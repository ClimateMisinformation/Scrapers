"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Walk‐​In Cooler and Freezer Refrigeration Systems proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide—two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties, including uncertainties within the critical physical parameters upon which their simplified climate model are built. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.



Until this entire situation can be properly rectified, the SCC should be barred from use in this and all other federal rulemaking. It is better not to include any value for the SCC in cost/​benefit analyses such as these, than to include a value which is knowingly improper, inaccurate and misleading.
"
"
Share this...FacebookTwitterRoland Tichy’s site here reports that although car traffic in the German city of Stuttgart has decreased significantly due to COVID-19, the drop in NO2 content has only been “slight”.
 So it cannot be the evil diesel engine cars that are “choking” our cities.

Image: NASA here (public domain) 
“The Corona crisis is bringing it to light: car traffic has decreased significantly, but the air quality in city centers has hardly changed,” Tichy comments.
Environmental claim exposed as false
Recall that a major reduction of diesel engines was supposed to improve air quality. After all, experts at the Baden-Württemberg State Institute for the Environment (LUBW), for example, have attributed a large 80 percent share of air pollutants especially to diesel vehicles – so they have to be banned soon.
Yet Tichy notes: “If this is true, the ‘shutdown’ would have to have a drastic effect. But it does not.”
It turns out that the involuntary “corona experiment” with its widespread stop of car traffic has exposed bare the false claim made by environmental activists: Diesel cars are responsible for polluting the air of German cities.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Diesel car bans “pointless”
According to Tichy, the “Corona Experiment” exposes just how pointless driving bans issued by the green transport ministers can be. “They obviously have no effect on the NO2 concentrations in the air.”
“The measured values, for example, at the Am Neckartor station in the Stuttgart city center were already below the limit value of 40 µg/m3 in February and March, ” writes Tichy. “At that time, traffic was still flowing and ‘shutdown’ had not yet been announced.”
Other larger factors
Tichy adds that engineer and measurement expert Martin Schraag accuses proponents of car bans of data “manipulation” and reminds that today’s “newer vehicles and those retrofitted with software updates hardly emit any exhaust gases. This should also have been reflected in the results.”
Schraag notes that NO2 values fluctuate strongly and depend heavily on Stuttgart’s weather conditions and wintertime  heating can be the cause.
“The weather, if you look at the data, has a decisive influence,” Tichy comments. “The experts of the LUBW environment office obviously did not care about these influences and officially know nothing about them. They still assume that traffic accounts for 80 percent of air pollutants.”
At a loss for words
Now they are facing difficulties in explaining the situation. As Schraag suspects, the 80 percent figure cannot be correct if there are significantly fewer cars on the road and yet the values have not changed.
The Bavarian State Office for the Environment also confirmed that the air pollutants in the city of Würzburg – hometown of former NBA superstar Dirk Nowitzski – had hardly changed either although traffic has decreased significantly.


		jQuery(document).ready(function(){
			jQuery('#dd_b0ff816632eacdd59517875595db342e').on('change', function() {
			  jQuery('#amount_b0ff816632eacdd59517875595db342e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitter The radiative impact of CO2 on the ocean’s thermal skin layer cannot penetrate deeper than 0.01 mm. This effectively eliminates the potential for CO2 to be a driver of global warming.
According to mainstream anthropogenic global warming (AGW) science, 93% of global warming is manifested in the 0-2000 m oceans. Just 1% of global warming is manifested as a change in atmospheric temperature.

Image Source: IPCC (ipcc.fandom.com)
Consequently, for anthropogenic CO2 emissions to be a driver of global warming, CO2 concentration changes must drive changes in the Earth’s ocean heat content.
Oceanographers Wong and Minnett (2018) point out that total CO2 forcing can only radiatively exert an impact on the top 0.01 mm of the ocean’s thermal skin layer. (Human hair is about 0.06 mm thick.)

Image Source: Wong and Minnett, 2018
Problematically, the amount of solar radiation absorbed in the upper 0.01 mm layer of the ocean is just 4.9 W/m².
Thus, CO2 concentration changes may, at most, affect 0.049% of the global oceans’ thermal skin layer.
This is the total extent of the radiative impact for CO2 in global ocean heat content changes.
CO2 may therefore be ruled out as a driver of global warming.
Share this...FacebookTwitter "
"
The recent photo submissions at surfacestations.org have demonstrated that many NOAA/NWS climate monitoring stations feature convenient close-by vehicle parking.
Not to be outdone, the Paso Robles USHCN Climate Station of Record features freeway on-ramp access to California’s Highway 101. The weather station is just feet from the street, with the temperature sensor placed just high enough to catch full view of vehicles over the fence.
 My thanks to surfacestations,org volunteer Ed Hahn for this photo. His complete photo essay is available here
Here is the NASA GISS plot for Paso Robles:

Curiously the GISS database still classifies this station as a “rural area”.
I find it interesting that the temperature was trending down in the 70’s then a huge offset occurred just about 1980. I wonder if that was when the freeway access was added? Nothing in the MMS records seem to indicate a station move or other change at that time. Or maybe that’s when somebody got the bright idea to pour a concrete slab under the the station?
From NOAA’s own siting specs: “The sensor should be at least 100 feet from any paved or concrete surface.”
Close enough for government work…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea547028d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

You may recall that back in May I did a simple preliminary experiment to give me guidance on a hypothesis: That changes in paint on Stevenson Screens over time make a measurable difference on the temperatures recorded inside them. This stems from the fact that when the Weather Bureau commissioned the design in 1892, whitewash was specified. But whitewash is no longer commonly available, and the National Weather Service changed the specification in 1979 to be semi-gloss latex paint.
But, cured whitewash is composed of Calcium Carbonate, while latex paint uses Titanium Dioxide as a pigment. While they both appear “white” in visible light, they have vastly different properties in infrared.
My first simple experiment used thermistors in boreholes into 3 wood slats; 1 bare wood as a control, the other two painted with whitewash and latex, showed me that there was a measurable difference in the temperature of the wood by as much as 2-4 degrees at times. I needed to do that experiment before I embarked on the full scale test, since each of the Stevenson Screens you see here in the pictures cost me about $1000.00 Since I’m doing this out of pocket, with no funding or grants, I had to try a small scale test first.
The photos show 3 standard Stevenson Screens as used today in the United States. One is bare wood, unpainted, as a control, the middle one is latex, as sent from the supplier, and the third is painted with a historically accurate (for early 20th century) whitewash mixture that I obtained both materials and formula from the head chemist at the National Lime Company.

The device on the tripod is a stacked plate IR shield with a small fan to pull air through, commonly called an aspirated shield. It is the air temperature reference and placed at the same exposure height as the thermistiors in the screens. Also nearby but not shown is a pyranometer to measure solar insolation and wind speed/direction sensors that are being datalogged as well.
Each Stevenson Screen and the air temperature reference sensor are fitted with matched, calibrated thermistors, NIST traceable with certificates, that are connected to a calibrated data-logger, also with a certificate. The resolution is .01 degree Fahrenheit with an accuracy of +/- 0.1 degree over the range.
I expect that the air temperature differences inside the screens will be less than the 2-4 degrees I observed in the paint slat test. It’s possible that there will be no significant difference at all. I won”t know until I run about a months worth of datalogging.
The site, while not ideal due to the trees, is the best I could get permission to use.  Fortunately the trees do not directly shade the screens except for a short portion of the day. It’s also out of the way, so vandalism will not be likely. Since it had to be an unwatered grass field, concerns over fire danger were raised from some I asked because of the electronics package, so I had limited choices. Perhaps later I’ll be able to find a better site but for now it will have to do.
The whitewash on the third Stevenson Screen is still curing, as the chemical reaction is not yet complete to convert Calcium Hydroxide to Calcium Carbonate. In about a week, I’ll make the data available via a web link in near real-time.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea536d1ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman climate blogger Snow Fan here presents some background on Australian bush fires. 
It turns out that the 1974/75 bush fires were considerably larger in area than the 2019/20 bush fires we have been witnessing.

The Australian bush fires of 2019/20 have seen an area as big as southern Germany (see above). But in 1974/75, they covered an area as large as France and Spain combined! Source: www.wetteronline.de. 
Snow fan writes:
On the completely exaggerated climate alarm in the German media on the current bush fires in Australia, a pleasantly objective report from WetterOnline: ‘In the summer of 1974/1975, an area in Australia burned to the tune of about the size of Spain and France. For the sake of perspective: Bush fires are generally nothing unusual in the Australian summer. Often large areas are affected. The last time a huge fire raged was in February 2009. The so-called Black-Saturday-bush fires killed over 170 people and destroyed 1800 houses. […] Since the beginning of the great bush fires in October 2019, more than 100,000 square kilometres of land burned throughout Australia, which is roughly the size of Bavaria and Baden-Württemberg combined. Thousands of houses were destroyed.'”
Bavaria and Baden-Württemberg have a combined area of around 105,000 square kilometers, so there’s no doubt this season’s bush fires have been devastating.
But WetterOnline reminds Australia has seen much worse:
 In the summer of 1974/1975 the flames burned over an area of about one million square kilometers. This corresponds to an area about three times the size of Germany.”
That means an area that is nine times greater than what has been affected this year! Back in 1975, however, atmospheric CO2 concentrations were BELOW the “safe” 350 ppm.
Share this...FacebookTwitter "
"
It appears that in the quest to save our planet from dangerous chemicals, people will blindly sign anything.  Read more about this dangerous chemical here: http://www.dhmo.org/ It’s “an odorless, tasteless chemical” that can be deadly if accidentally inhaled.

This chemical is so dangerous that a local city council in Aliso Viejo, CA put it on the agenda to ban foam containers made with it. I expect our liberal city council may soon address this danger like we’ve already done for nuclear weapons in the city limits.
Penn and Teller provide an entertaining look into mindless activism.

with a hat tip to Mark…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea66a15b8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Anybody who wonders what we are fighting for in the middle east should read the article below in today’s Boston Globe. In case you didn’t know, there are places where you could go to jail for just reading this.

Iran bloggers test regime’s tolerance
Push boundaries of political dissent
By James F. Smith and Anne Barnard, Globe Staff  |  December 18, 2006
TEHRAN — By day, Alireza Samiei covers banking and insurance for an industry newspaper. By night, he writes a daring online blog about Iran’s social and political ills.
In a recent blog entry, he described a scene he saw while talking to a greengrocer about soaring prices: A young child was pleading, ” ‘Mommy, I want watermelon.’ The woman, shy and sorrowful, singled out one broken, small watermelon from the spoiled fruit bin and told the grocer, ‘Just this one, please.’ She put 20 cents on the counter and hurried away.”
Samiei, 27, is among the growing ranks of Iranian bloggers who are relentlessly pushing the boundaries of free expression, making Farsi one of the 10 most popular languages for blogs. The bloggers are testing just how much political and social dissent the nation’s rulers will tolerate on the Internet.
The authorities are pushing back. They have blocked access to thousands of websites in recent years that are deemed to threaten Iran’s Islamic revolution, including the BBC’s Farsi-language site. A trial began this month against four bloggers on charges including propaganda against the state. And in October, the government barred high-speed Internet service in private homes.
Especially threatening, it appears, are sites that create online communities that might allow Iranians to assemble virtually. The government banned the hugely popular Orkut site, an online Iranian social club. The latest casualty this month: YouTube.com, the American site for sharing videos online. Click on it in Iran and the screen reports, “Access denied.”
The Paris-based rights group Reporters Without Borders includes Iran on its list of 13 countries designated “enemies of the Internet.” That organization’s website is also blocked in Iran.
The organization said repression of bloggers has eased somewhat in 2006. But in a report in November, the group said Internet filtering has accelerated, with two political sites, tik.ir and meydaan.com, closed down in recent weeks. Both had criticized the government of President Mahmoud Ahmadinejad.
Bloggers agree that they have found some latitude in recent months. Many have developed a feel for the boundaries, and some are trying to stretch them rather than break them.
Farzana Sayid Saidi, a 29-year-old reporter and colleague of Samiei, has two blogs, one political and the other showcasing her poetry. She has been blogging in her spare time for two years. Her first blog was shut down within three days, she said, after she wrote that school officials were providing access to abortions in clinics for young students.
Now she’s back at it. She blogged a few days ago that while Ahmadinejad wants people to have more children, his economic policies make it difficult for many families to do so. She said she received some obscene and abusive replies. In a poem on her other blog, she compared Iran’s leaders to the pharaohs of Egypt.
She estimates her readers at only in the hundreds, but adds: “We just want to express ourselves. We don’t know how many people are reading.”
Farzana’s colleague Samiei admires her courage.
“Of the things she writes in her blog, only 1 percent would be acceptable in print,” Samiei said.
She and hordes of other Iranian bloggers are pushing the envelope of the permissible. Technorati, a Silicon Valley search engine for blogs, said in October that Farsi has moved into the top 10 languages worldwide for bloggers. Most estimates put the number of active blogs in Iran at 70,000 to 100,000, and growing fast.
Iran has a long tradition of controlling the airwaves and the print media, banning papers and jailing journalists who criticize official policies.
But Iran’s online activists have proved harder to quash. They have used fast-changing Web addresses, proxy sites, and other technological tricks to get around the restrictions.
“They block us and we evade the blocks,” Samiei said. “It goes on every day. They code, we decode.”
The Ministry of Information periodically sends lists to Internet service providers saying which keywords to filter out so that users can’t get access to websites or blogs that contain them. The government contends that the principal target is pornography and other morally offensive material. The word “sex” is among those blocked.
That has some odd consequences. At one point, an Internet café owner said, the word “hot” was blocked. And that briefly prevented access to Hotmail, the popular e-mail program.
Amirhussein Jaharuti, the manager of a major Internet service provider in Tehran, said the government’s restrictions focus on pornography, and he feels that filtering is appropriate.
“This is the demand of Iranian families, that they don’t want their children to use these kinds of sites,” he said. Asked about the political restrictions, he said: “All governments have ways to control their societies. . . . It’s natural that when we see that someone wants to destroy us, we limit them.”
Jaharuti said his client base has doubled in the past two years, to nearly 70,000. He provides dial-up and digital-subscriber line service to home and business customers at a cost of 20 to 40 cents an hour, or about $20 per month.
Internet use in Iran has exploded in recent years, with about 7.5 million users in 2005 in a country of nearly 70 million people.
Some journalists say the Internet has become even more vital in Iran as the government has suppressed other, more easily controllable forms of expression. Several opposition newspapers have been shut down since September, including the prominent paper Shargh.
The editor, Mohamed Atrianfar, said in an interview that the closure of Shargh and other publications and renewed pressure on critical websites reflects the government’s concern that “the more challenges we have, the more agile and fresh the society becomes.”
“All the hard-liners have mustered all their strength to fight this war. I am proud that we have invoked this reaction in them,” he added.
Despite its closure, Shargh has maintained a website to continue coverage of elections last week.
Atrianfar estimated that about 70 to 80 Iranian journalists have their own blogs.
“Websites and blogs have real impact,” he said. “They have been very powerful in forming a word-of-mouth culture, especially for those between 17 and 35.”
An Internet café owner in central Tehran who gave only his first name, Shariar, said the filtering of keywords rather than individual sites often blocks legitimate websites that people need for academic research. He also said limits on credit cards resulting from US financial sanctions against Iran have all but eliminated e-commerce on Iran’s Internet, a major obstacle to economic growth.
Shariar said that while the government contends it is aiming its restrictions at pornography, “I think they are worried about politics. . . . I think they fear everything. They don’t want people to make connections overseas. They are worried about information.”
The authorities also close Western media sites temporarily. Both The New York Times and Los Angeles Times sites were blocked briefly this month.
A 22-year-old university student, Morteza Yeganeh, said the state-owned broadcasters and newspapers “brainwash people, so we need to find ways to educate ourselves.”
But he said the filtering of sites is effective because “even though people can get around the filters, it is difficult and time-consuming and people give up.”
Most Iranian blogs are apolitical, and government members — including Ahmadinejad himself — have their own blogs to convey their views. But those with blogs that challenge the government know they are taking a risk.
Niloufar Taslim, 24, said that three years ago, she was one of Iran’s first bloggers, writing about social and political problems. But she started receiving e-mails signed by a group calling itself the Army of God, listing her name, telephone number, and address and threatening to kill her.
She shut down that site, but now has two new blogs. One talks about social problems without crossing what she also considers political red lines: transportation and environmental problems.
Another blog features her poems. One laments that she has lost her voice, that in “a situation without possibilities” her hands are “in pain because they cannot write.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9745970',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Have you ever wondered why the vast majority of plants and trees have green
leaves and not some other color?
It’s always been a bit of a mystery why plants absorb red and blue light, but
reflect green, allow us to see the leaves as green. It seems inefficient of
nature when the sun emits the peak energy of its visible spectrum in the
yellow-green areas. A new theory offers one possible answer: that the first
chlorophyll-utilizing microbes evolved to

exploit the red-and-blue light that older green-absorbing microbes didn’t use,
eventually out-competing them through

greater efficiency and the rise of oxygen.
If that were the case, plant life long ago may have had purple leaves to
catch both the red and blue portions of the spectrum. For those whom don’t know
this, RED + BLUE =
MAGENTA (purple)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6e7304d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Global Warming is a hot topic here on Earth, but it may be the issue will be settled not here on Earth, but on Mars.  A study of the ice caps on Mars suggests it is also experiencing a warming trend. A story about new data from NASA cites a six year study by researchers at Duke University showing that Mars may also be seeing a Global Warming trend and that both the Earth and Mars are seeing changes related to solar output.


You can click on this link to get a time-lapse image of the Mars ice cap changes between 1999 and 2001. It may take awhile to load as its 1.6 megabytes in size. Those with very high speed connections can get an even larger and more detailed time lapse here which is 6.2 megabytes in size
Planetary scientists have been watching melting of deep, wide pits in the southern Martian ice caps. The melting is substantial. According to Michael Malin, principal investigator for the Mars Orbiter Camera, the polar ice cap is shrinking at “a prodigious rate.”
Now where have we heard that before? Oh, seems Al Gore said in his movie An Inconvenient Truth that our own ice caps are melting and that we’ll see a 20 foot rise in sea level as a result. Here’s a transcript of the movie if you are interested.
The scientists believe this observed melting on Mars means that there is a layer of dry ice that is evaporating off of a thicker layer of water ice. The yearly increases in evaporation may be caused by a global warming trend happening on Mars.
The most recent images in the NASA sponsored study show changes from 1999 to 2005, suggesting the climate on Mars is presently warmer, and perhaps getting warmer still, than it was several years or decades ago.
Another recent NASA announcement, that recent water flows have also been discovered on Mars, also lends credence to the idea that Mars is getting warmer. One of the mechanisms suggested is that liquid water is subsurface, and that patches of dry ice acting as “plugs” are melting, releasing the water which moves enough surface material around before it freezes again to show up on photographic comparisons.
If both Mars and Earth are experiencing global warming, then maybe there is a larger phenomenon going on in the Solar System that is causing their global climates to change, like changes in the Sun. There’s correlating evidence showing sunspot trends match temperature trends on Earth. Mars may have a similar linkage.
But perhaps there will be those arguing it’s because we’ve landed two tiny SUV’s (Space Utility Vehicles) named Spirit and Opportunity on Mars surface.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea962d53c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

A “computer glitch” when the reporter sent my story to copy editing added an extra “o” to the word “Outlook” in the title, sending my entry into “etherland”. 
You can view the entire Outlook Special online at:
http://www2.chicoer.com/specialSections/Outlook_2007/Outlook_2007.pdf (takes awhile to download, my article on Page45, which they added afterwards)
Or you can read it below. If you have been thinking about putting solar on your home, here is a guide. Enjoy.
ER Sustainability Outlook 02/27/07
Sustainability is a trend that is growing not only here, but also throughout the world.
It is an attempt to provide the best outcomes for the human and natural environments both now and into the future.  Essentially you could think of it as balanced use of the planet, where the use doesn’t outstrip regeneration.
Locally there have been a number of movements towards this goal, particularly with solar power. Butte County is particularly well suited for solar power. Climate records show that we have 219 sunny days and 57 partly cloudy days per year on average, which makes solar power viable. It wasn’t always that way, and it’s only now that solar power is becoming economically viable due to increased electricity costs, increased solar cell efficiency, and state rebate programs to help home and business owners kick-start the process.

There’s three reasons to do solar power on your home or business:
1 – You want the economic benefit of reduced power costs.
2 – You want to do something environmentally sound.
3 – You have no other power options available, such as a summer cabin.
Most often it’s the first two, but there are some limits you should be aware of related to economics. Solar power can be an expensive proposition to install, even with rebates. Thus unless you have money to burn you have to plan it carefully to ensure that you get payback on your investment. You also need an unobstructed view of the southern sky.
I myself have placed two solar power systems into use, one on my home, and another for Chico Unified School District on Little Chico Creek School, which is the largest solar power system for a school north of Sacramento.
In both cases, there were high power uses going on, which made the economics easy. My home had a deep well, a pool, and an upper and lower A/C unit, making my power bills hit as much as $500 per month in the summer! I’m studying a design for a third solar power system on my new home, purchased just last year, but its more energy efficient, making the planning task more detailed.
Typically, you’ll need to have a power bill of at least $150-200 per month or more to make solar viable for your home as a retrofit. However, if you are building a new home, planning solar into the building process is less expensive.
Some forward thinking developers are now offering turnkey solar built into new homes, such as is being done in Fresno. So far, I haven’t seen Chico developers offer such an option, but I think the time is right for our Building Industry Association, Chamber of Commerce, and City Government to work together to make such an offering practical.
The way solar power works for homes and businesses is by a reverse metering scheme based on Time Of Use (TOU). During peak power need times of noon to 6PM on weekdays, electricity is far more valuable than during off-peak times. PG&E will credit any power generated during those peak times as much as 4 times the value of electricity used during off-peak times.
It’s sort of like the stock market, sell high and buy low.
To achieve this, your home or business has to outfitted with a TOU Meter, so that PG&E can track when you use power. Then when you connect a solar power system to that, it will log when you are generating power during midday peak times, and when you are drawing power during off-peak times. The trick is to generate exactly enough power to result in a net-zero energy use, because PG&E does not pay you back for any excess power generated.
A solar power system generates DC voltage from the solar panels, and when they are working at peak you can expect a 15% solar to electricity power conversion efficiency. The DC power from the solar cells must then be converted and phased to match the 60 cycle AC power grid. This is done with DC to AC inverters, usually mounted near your mains breaker box. There’s about a 10% conversion loss in that process.
If you are planning to do solar, there are a few things you should know:
· Pick a reliable contractor experienced with the process, particularly with the California Energy Commission rebate process, because a mistake there can cost you a lot of time and money.
· Be prepared to spend money or to seek financing. There are low cost state-sponsored finance programs available.
· Be patient. The process takes time, often more than you think, especially in a retrofit. There are applications, permits, tests, and government interactions involved.
· Solar will immediately add to the resale value of your home, that value never decreases. So when you get a state rebate, say for $10,000 towards the purchase, you get to keep that as equity.
· Financing should be balanced in such a way so that it is equal to or less than your average existing electricity bill, so that you are paying yourself back. When the system is paid off, then you’ll have zero payments for energy.
· You’ll be switched to a yearly billing system rather than a monthly. If your solar system doesn’t produce enough electricity to cover all your use, at the end of the year you’ll have what’s called a “true-up” bill, which could be large, but averaged over the year will be much smaller. Be sure to plan for that.
· Right now, solar isn’t for everyone as its still a rather expensive and complicated process to install as a retrofit. However, as solar panel efficiencies increase, and more companies get online producing solar cells, the costs will come down, as happens with any new technology.
· There are state and federal tax credits for any solar installation which when figured in with rebates, can make the project quite attractive, and in some cases, a very low cost.
Given that energy demands are only going to go up, and prices will naturally follow that demand, if you have high electric bills or have a business that could benefit, solar power is certainly worth looking into.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7d4c3e0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA new study finds rising CO2 concentrations (and warming) have driven the rapid increase in Earth’s photosynthesis processes, or greening. CO2-induced planetary greening leads to an enormous expansion of Earth’s carbon sink. By 2100 this greening-sink effect will offset 17 years of equivalent human CO2 emissions. This easily supersedes the effect of Paris Agreement CO2 mitigation policies.
In a break from the deflating global news of viral infections and rising death rates, a groundbreaking new study (Haverd et al., 2020) affirms the “beneficial role of the land carbon sink in modulating future excess anthropogneic CO2 consistent with the target of the Paris Agreement” via the fertilization effect of rising CO2.
There has been a 30% rise in global greening since 1900. CO2 fertilization is the “dominant driver” of these greening trends, with an additional positive contribution from climate warming.
When CO2 levels double (to 560 ppm), this CO2-fertilization-greening effect is expected to increase to 47%.
Growth in the land’s carbon sink – absorbing excess CO2 emissions – will reach 174 PgC by the end of the century.
This is the equivalent of eliminating 17 full years of human CO2 emissions.

Image Source: Haverd et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_fba188469f13af433635491d7989d885').on('change', function() {
			  jQuery('#amount_fba188469f13af433635491d7989d885').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA 2006 peer-reviewed scientific paper (Inglesby et al., 2006) on pandemic mitigation policy said the efficacy of 3-feet social distancing is unknown, surgical masks do little to prevent virus droplet inhalation, and closing schools, restaurants, stores…have seriously adverse community consequences.
The paper refers to what policymakers should recommend if there is an influenza pandemic.
Keep in mind that the flu kills about 500,000 people a year worldwide and there have been some years when it has reached pandemic proportions, killing over a million people (i.e., 1957-’58, 1968-’69).
In 1918 the flu pandemic killed 50 million out of 500 million infections worldwide. At the time, that was one-third of the world’s population.
And yet at no time have we ever responded to a pandemic the way we have with COVID-19.

Image Source: Inglesby et al., 2006
Share this...FacebookTwitter "
"

Today I had to do a round trip drive to San Jose to inspect some video transmission equipment and back to Chico all in a few hours. Coming back, I was in stop and go traffic coming across the Benicia-Martinez Bridge (which is being rebuilt)which carries I-680 across Suisun Bay and had a fair amount of time to look at the Ghost Fleet kept by the Navy there.
Officially known as the National Defense Reserve Fleet (NDRF) they have all sorts of ships there including the WWII battleship USS Iowa, merchant ships, and an aircraft carrier. There are also some WWI steam ships there too, many in a state of decay.
I was reminded of a boat trip I took down the Delta a few years ago where I got up close and personal with these ships. Some were impressive, others downright spooky. I also remeber finding the crossing of the old Sacramento Northern Electric Railway which went all the way to Chico, and up the Esplande. I’ll tell that story another time.
In the meantime here are some pictures and links of the Ghost Fleet.

Several destroyers and merchant ships, plus a tug.


The USS Iowa BattleShip BB61, soon to be moved to Stockton for restoration and display

Another view of the USS Iowa

Carrier USS New Orleans and merchant ships


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7b3e327',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
As you may or may not know, this blog has taken off with big traffic increases as of late.
While the traffic has been an indicator of success, unfortunately, keeping that success gets to be more and more time consuming. This blog platform is hosted on Moveable Type version 3.2, which is about as close to being crippled as blog software can get. For example. it doesn’t even have a spell checker. The spam comment filter doesn’t work much anymore, and email notifications are also broken. The host has promised for months now to upgrade the platform, but so far has been unable to do so.
Working with MT in it’s current state requires a lot of extra effort compared to other software, and I find myself spending an inordinate amount of time just dealing with the limits of Moveable Type and trying to work around them. It has become quite frustrating as I want to offer better quality content and find myself unable to easily do so. I can’t even put in fully rich HTML into MT because of the way it deals with formatting. I’ve tried several add on programs to aid in blog generation, all of which have been thwarted by the MT platforms non standards compliance. Of course, some of my more snarky readers would likely suggest that standards “don’t matter” and that any problems with the content can be “adjusted” 😉
So the question is this, should I:
1) Close this blog and give up blogging altogether on this platform
2) Move someplace else and link back to this location
3) both
I welcome any input.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3b81b69',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDr. Knut Wittkowski, the former head of the epidemiology department at Rockerfeller University, says doing nothing would have been more effective – and ultimately cost fewer lives – than the “containment” strategy now in operation across the world.
By restricting movement and confining people in their homes we are unnecessarily prolonging or widening the curve instead of just flattening it.
The only way to eliminate any respiratory virus is not by developing vaccines or with pharmaceutical intervention, but by natural herd immunity. This means we should be allowing children to attend school.
When 80% of the population becomes infected – and the vast majority of the population won’t even know it because they won’t have symptoms – a common coronavirus like this one can be exterminated within about 4 weeks.
By trying to contain the virus, we are practically ensuring there will be a “second wave” of infections in the Northern Hemisphere fall, as not enough people will have been infected in recent months to exterminate this particular coronavirus strain.
Dr. Wittkowski asserts he is able to talk candidly about what should have been done in response to this COVID-19 outbreak because he is not paid by the government and therefore he is able to “actually do science”.

Transcribed commentary (and image) from a YouTube interview

Share this...FacebookTwitter "
"

The diagram above is central to the paper’s examination of the “spiral” nature of the earth to sun distance relationship, which affects noit only seasons, but longer term climate cycles.
Every once in awhile some thing comes along that really “clicks” with a  lot of people in the science community.
A new paper from New Zealand titled: Linkages between solar activity, climate predictability and water resource development is one of those that has “clicked” with a lot of people recently. It is the first scientific paper I’ve ever seen that pulls all the interdisciplinary fields of solar physics, astronomy, meteorology, hydrology, and climatology together to prove that in fact the sun is the major driver, even with its “small” fluctuations often ignored by climate scientists as being too small to matter.
It does matter, I’ve written about it many times, and this paper really has strong evidence supporting it. This is not just another paper talking about sunspots and the maunder minimum, no this one has some strong empirical evidence that directly links climate changes on earth to a myriad of changes in the sun-earth relationship.
What’s even better, this paper is readable. It’s not written in techno-speak with accents on using words 99% of the general population doesn’t use. It’s refreshing. Read it here (Adobe PDF)
The abstract reads: “This study is based on the numerical analysis of the properties of routinely observed
hydrometeorological data which in South Africa alone is collected at a rate of more than
half a million station days per year, with some records approaching 100 continuous years
in length. The analysis of this data demonstrates an unequivocal synchronous linkage
between these processes in South Africa and elsewhere, and solar activity. This confirms
observations and reports by others in many countries during the past 150 years.
It is also shown with a high degree of assurance that there is a synchronous linkage
between the statistically significant, 21-year periodicity in these processes and the
acceleration and deceleration of the sun as it moves through galactic space. Despite a
diligent search, no evidence could be found of trends in the data that could be attributed
to human activities.”
My hat’s off to these scientists: W J R Alexander, F Bailey, D B Bredenkamp, A van der Merwe and N Willemse


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5c2b952',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

An airliner traveling from Chile to New Zealand early today was in for an near miss from something you wouldn’t expect.

Flaming space debris — the remains of a Russian satellite — came hurtling
back to Earth not far from a passenger jet on its way to Auckland, New Zealand.
Here’s further proof for the growing concern of the  increasing amounts of space junk orbiting our planet. From the article: ‘The pilot of a Lan Chile Airbus A340 … notified air traffic controllers at Auckland Oceanic Center after seeing flaming space junk hurtling across the sky just five nautical miles in front of and behind his plane…’
Yikes!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea747bc4f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   




The new paper’s lead author is Jonathan Gregory of the U.K.’s University of Reading, and the other authors are a who’s who of sea level researchers (repeating my professions nauseating belief that putting a large number of authors (most of whom have—at best—just read the manuscript) somehow makes it more persuasive). The paper concludes that the causes of sea level rise, and its temporal variations, across the 20th century were many, and that a link to anthropogenic global climate changes has been weak or absent over this period. Basing future sea level rise projections on a presumed historical relationship between anthropogenic global warming and corresponding sea level rise turns out to be a bad idea.   
  
Here is how Gregory et al., 2012 put it:   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



And here are the main conclusions, now seriously questioned, from the semi-empiricical citations included in the above quote:   
  
Rahmstorf (2007):   




When applied to future warming scenarios of the Intergovernmental Panel on Climate Change, this relationship results in a projected sea-level rise in 2100 of 0.5 to 1.4 meters above the 1990 level [by 2100].



Vermeer and Rahmstorf (2009):   




For future global temperature scenarios of the Intergovernmental Panel on Climate Change’s Fourth Assessment Report, the relationship projects a sea-level rise ranging from 75 to 190 cm for the period 1990–2100.



Jevrejeva et al. (2010):   




With six IPCC radiative forcing scenarios we estimate sea level rise of 0.6–1.6 m, with confidence limits of 0.59 m and 1.8 m.



Seems like three strikes against projecting those high rates of sea level rise.   
  
For a little reality check, the current rate of rise is somewhere in the range of 1.8 to 3.5 mm/yr (0.07 to 0 .14 in/yr) depending on the time period over which you calculate the trend.   
  
Further, as we have previously written, it doesn’t look as if the recent increased rates of ice loss from Greenland and Antarctica are sustainable—much less going to linearly increase to the end of the century. All of this strongly argues that the 21st century sea level rise is not a problem that we can’t keep up with.   
  
**References:**   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , doi:10.1175/JCLI-D-12-00319.1, in press.   
  
Hansen, J.E., 2007. Scientific reticence and sea level rise. _Environmental Research Letters_ , **2,** doi:10.1088/1748-9326/2/2/024002   
  
Jevrejeva, S., et al., 2010. How will sea level respond to 1019 changes in natural and anthropogenic forcings by 2100? _Geophysical Research Letters,_ **37** , L07703, doi:10.1029/2010GL042947.   
  
Rahmstorf, S., 2007. A semi-empirical approach to projecting future sea-level rise. _Science_ , **315** , 368–370, doi:10.1126/science.1135456.   
  
Vermeer, M. and S. Rahmstorf, 2009. Global sea level linked to global temperature. _Proceedings of the National Academy of Sciences,_ 106, 51, 21527–21532, doi:10.1073/pnas.0907765106.


"
"
Share this...FacebookTwitterThe “Streetscooter”: Electric Mobility’s First Large Bankruptcy

Image: From Superbass – own work, CC BY-SA 4.0, commons.wikimedia.org
By AR Göhring, European Institute for Climate and Energy (EIKE)
(Text translated and summarized by P. Gosselin)
Manufacturer “Streetscooter”, purchased by Deutsche Post (German Post) in 2014, will be scrapped. The German media of course blame it on “bad management” by the large company.
Which city dweller doesn’t know the small, yellow electric scooters of the German post office that the postmen and women deliver letters and small packages to citizens comfortably and efficiently? Not long ago I received news via Facebook on how the e-delivery-vehicles just barely made it back to the post office, especially in winter, and only when the heating is off.
Now the management of the Swiss Post is also following suit and ending the experiment with delivery street scooters.
The company used to be a small startup, a young dynamic private company in a “sexy” field – just like artificial intelligence or climate protection technology. Deutsche Post bought the company with the benevolent support of the eco-loving press and used it to polish up its otherwise staid image a bit.
However, any PR coup based on electro-chemistry ultimately has to prove itself in everyday life over years. The post office scooters obviously couldn’t. Pushing an electric vehicle still loaded with letters back to the local depot when the battery is empty is not possible: the scooter is too big and heavy for that. Or you have to plan shorter routes (in winter), which reduces efficiency. Since letters are only delivered during the day, the scooters can be conveniently charged at night. But if you have to reload during working hours, it takes hours, and you don’t have the time for that.
Berlin e-bus failure


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Take, for example, the Berlin E-bus experiment: the lithium buses run from 8 to 12 a.m., then the diesel vehicles take over. Our speaker Prof. Alt talks in this context about a double infrastructure, which is of course also roughly twice as expensive. Presumably Deutsche Post had to manage a similarly inefficient double fleet of about 13,000 street scooters. The scooters broke down more often and then soon had to be repaired, and replaced by diesel-powered delivery vans.
A commentator from ntv television, however, blames it on the slow management of Deutsche Post: A project like an electric fleet of electric cars has to be run by flexible start-up managers with heart and attitude, then it would work.
The Streetscooter deserved a dynamic, creative and risk-taking management – and the opportunity to obtain the necessary funds independently on the capital market.”
This claim is not convincing. Whether it’s a startup or the Deutsche Post, both must adhere to the main laws of physics and economics. One thing must never be forgotten: Deutsche Post is a business group that has to make money.
The city administration of Berlin, on the other hand, can waste money at will with misguided planning. They work with funds from taxes levied by force. And Berlin’s eco-socialist politicians, who are poor in arithmetic, are elected and are not held accountable for their failures with their own private assets.
Of course some will claim that Tesla has achieved what the N-TV quote above calls for. But this is not true: Elon Musk is an eco-media darling who has already received billions of dollars in US subsidies. Without these billions he would have long since gone bankrupt or become a mini-manufacturer for a niche.
We Germans are now experiencing the same thing in Brandenburg: because Merkel’s “grand coalition” wants to have a share in the media sexiness of Tesla, the “Gigafactory” is being heavily subsidized there.
The fact that an entire forest is being cut down and cheaper Polish workers have to be hired is of no consequence to someone like Federal Economics Minister Peter Altmaier. The press as well.


		jQuery(document).ready(function(){
			jQuery('#dd_3a04978ab8a5ce78325435c2a3e12065').on('change', function() {
			  jQuery('#amount_3a04978ab8a5ce78325435c2a3e12065').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Polychaetes are a class of segmented worms that live under a wide range of oceanic conditions. Often, they are the dominant organisms found living in the sea floor, but they also thrive in the open ocean. According to Ricevuto _et al_. (2015), although knowledge of the potential response of these organisms to ocean acidification is growing, much remains to be learned, including “how their trophic behavior might change in response to low [less basic, or more acidic] pH.” In an effort to fill this informational void, Ricevuto _et al_. thus set out to examine food-chain interactions of three polychaete species ( _Platynereis dumerilii_ , _Polyophthalmus pictus_ and _Syllis prolifera_ ) and their organic matter (food) sources (macroalgae, seagrass and epiphytes) in a naturally acidified region of the Mediterranean Sea.   




The location for their study was a shallow water reef area on the north-eastern coast of Ischia, an island off the coast of Italy known for volcanic features, including underwater vents that release copious quantities of CO2. The vents produce a pH gradient in the area that provides “a natural laboratory for ocean acidification studies,” which the researchers further describe as “an ideal model system to conduct experiments investigating the effect of climate changes (particularly ocean acidification) on benthic community composition and structure, as well as on functional aspects, such as tropic interactions,” which was the focus of this study. And what did the study show?   
  
After collecting data and conducting a series of complex analyses, the three Italian researchers report “increased pCO2 did not alter the trophic interactions dramatically,” adding “there seems to be a resilience in the trophic pattern, possibly due to the tolerance of the target species to acidification and potential local acclimatization and/or adaptation (see Calosi _et al_., 2013).” Such “phenotypic plasticity” (the ability to alter biochemical reactions based on environmental changes such as increasing temperature or acidity) observed in the three polychaete species studied, according to Ricevuto _et al_., “may allow them to respond well to alterations in the environment and eventually offset near-future ocean acidification scenarios.” Thus, as the researchers ultimately conclude, “for some species, like the ones considered in this study, ocean acidification may not represent a dramatic stress.” And that’s good news worth reporting.   
  
  
  
**References**   
  
Calosi, P., Rastrick, S.P.S., Lombardi, C., de Guzman, H.J., Davidson, L., Jahnke, M., Giangrande, A., Hardege, J.D., Schulze, A., Spicer, J.I. and Gambi, M.C. 2013. Adaptation and acclimatization to ocean acidification in marine ectotherms: an _in situ_ transplant experiment with polychaetes at a shallow CO2 vent system. _Philosophical Transactions of the Royal Society of London B Biological Sciences_ **368** : 20120444.   
  
Ricevuto, E., Vizzini, S. and Gambi, M.C. 2015. Ocean acidification effects on stable isotope signatures and trophic interactions of polychaete consumers and organic matter sources at a CO2 shallow vent system. _Journal of Experimental Marine Biology and Ecology_ **468** : 105-117.


"
"

A potentially informative and constructive debate about the costs and benefits of global warming has been lost to “political dramatization,” argue the authors of a new Cato Institute book.



In _The Satanic Gases: Clearing the Air about Global Warming,_ climatologists Patrick J. Michaels and Robert C. Balling, Jr., trace the development of global warming, writing that politicians blame the latest thunderstorm, flood, or change in the weather on global warming. They also assert that global treaties, protocols, and other policies are being signed and negotiated despite shoddy science.



Michaels, senior fellow in environmental studies at the Cato Institute and professor of environmental sciences at the University of Virginia, and Balling, director of the Laboratory of Climatology at Arizona State University, analyze the politics of global warming and provide a primer on the science. Acknowledging that industrial emissions of greenhouse gases have warmed the planet and will continue to do so over the next several decades, Michaels and Balling argue that future warming will be moderate, not catastrophic, and will have benign economic and ecological effects. They point out that the effects of climate change are already positively affecting mortality and agriculture, citing data that show the “greening” of the earth may be enhancing plant growth. The year 1998, during which temperatures warmed as a result of El Niño, produced record agricultural output. The authors expect that the earth’s average surface temperature will warm 0.65 to 0.75 °C (1.17 to 1.35 °F) by 2050, resulting in a decline in temperature‐​related mortality and a rise in crop yields that alone would feed one‐​quarter of today’s world population



The authors find that government funding of research has corrupted the scientific process as scientists compete for funding in a politically charged environment. Total federal spending on global climate change research has ballooned from a few million dollars to $2.1 billion annually in the last 15 years.



The book has already received much praise. Frederick W. Seitz, past president of the National Academy of Sciences, says it “should be read by every scientist and layman who has an interest in the topic.”



 _The Satanic Gases: Clearing the Air about Global Warming _can be purchased through the Cato Institute’s online bookstore.



 **Mises, Hayek Examined in _Cato Journal_** __



The latest issue of the Cato Journal (vol. 19, no. 2) commemorates the 50th anniversary of the publication of _Human Action_ by Ludwig von Mises (Yale University Press, 1949) and the 100th anniversary of F. A. Hayek’s birth. Editor James A. Dorn writes, “These two giants of market‐​liberal thought exposed the fallacies of central planning, pointed to the importance of private property rights and limited government in promoting a spontaneous market order, and explained the role of institutions in shaping incentives and behavior.”



Papers by Vernon Smith, Israel Kirzner, Kenneth Elzinga, and George Selgin (along with comments by Lawrence H. White, Gordon Tullock, Frank Machovec, and Richard Timberlake) were first presented at the 1999 meeting of the Western Economics Association in a session titled “Mises’ _Human Action_ : A Critical Appraisal after 50 Years.” Smith discusses how the experimental economics in which he is a pioneer has confirmed Mises’ analysis of cooperation. Selgin and Timberlake examine Mises’ views on the role of gold in the monetary system. Elzinga and Kirzner both note Mises’ understanding that the market is a constantly evolving process, not a path to a particular endpoint.



Stephen Macedo of Princeton University discusses three themes in Hayek’s work: his critique of political utopianism, his emphasis on the interdependence of law and liberty, and his faith in the power of ideas and institutions. Ronald Hamowy, a student of Hayek’s at the University of Chicago, offers some personal reminiscences and an examination of Hayek’s history of liberalism. Those papers were delivered at the Cato Institute on May 8, 1999, the centenary of Hayek’s birth.



Other papers in the Cato Journal discuss the regulation of addictive substances, politics and the IRS, and women’s sports. _Cato Journal_ is published three times a year. Most articles are available at www​.cato​.org; subscriptions and single copies are also available.



 _This article originally appeared in the May/​June 2000 edition of_ Cato Policy Report.
"
"
Share this...FacebookTwitterDonations now welcome!
Before we jump into the data, readers may have noticed that NoTricksZone is now accepting donations – after close to 10 years of not doing so. Hosting services and the hourly rates specialists charge for troubleshooting are painfully expensive, and so any support, no matter how small, is very much appreciated.


		jQuery(document).ready(function(){
			jQuery('#dd_a992b20773ff8dfb5303892e42d2859f').on('change', function() {
			  jQuery('#amount_a992b20773ff8dfb5303892e42d2859f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
I’ve gotten some donations since I’ve installed the donation button, and so it’s very encouraging. Thanks to those who have done so.
And no, I’m not nor have I ever been funded by Big Oil, The Heartland Institute, Big Coal, the Koch Brothers or anything of the sort. Everything has been out of my own pocket. Thanks, Pierre
Falling Canada mean February temperatures
By Kirye
and Pierre Gosselin
Today we look at the data from the Japan Meteorological Agency (JMA) mean temperature for Canada. Examined are the 9 stations that have almost complete data going back to 1983.
Nine of the 9 stations show February mean temperatures have had a cooling trend since 1996: 



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA 
Annual Canada temperatures steady since 1994
Looking at the JMA data for mean annual temperatures for Canada, we see that no significant warming has taken place at the 9 stations since 1994!

Data: JMA.
Yes, the climate is changing, but nothing unusual is happening. The changes are well within the natural range of variability and so cannot be wholly attributed to man’s activities.
In fact, using the data and results that are available, one can easily argue that the recent warming is related to ocean and solar cycles. There’s nothing to be alarmed about.
Worry about Corona and political threats, and not climate.
Share this...FacebookTwitter "
"

OK, _The New York Times per se_ has not weighed in with harsh criticism, but Prof. Hal Varian of U. Cal. Berkeley, a contributor for the NYT’s excellent “Economic Perspectives” column, weighs in today with a nice summary of the problematic assumptions made by Sir Nicholas Stern in the oft‐​quoted Stern Review on the Economics of Climate Change. For those who don’t recall, Stern argued that it makes sense to spend 1 percent of the world’s GDP to reduce greenhouse gas emissions because the costs associated with those emissions might total anywhere between 5–20% of global GDP some time down the road.



Regular readers here will notice that Prof. Varian’s arguments closely mirror those I made earlier on this page (for the curious, here and here, with a minor correction to the latter here).



So it’s not just me folks .…. 
"
"
Share this...FacebookTwitterInternational and NASA solar scientists find their Total Solar Irradiance reconstruction extending to 1700 can “correlate well” with Earth’s global temperature records, including a positive net TSI trend during 1986-2008. A new Grand Solar Minimum is expected to commence during the 2030s.
Surface climate records that have been uncorrupted by coastal (ocean-air)/urbanization biases suggest there has been a long-term oscillation in temperature since 1900, with peaks during the 1920s-1940s and again during recent decades (Lansner and Pepke Pedersen, 2018).


Image Source: Lansner and Pepke Pedersen, 2018
An analysis by Soon et al. (2015) (full paper) indicated Northern Hemisphere surface temperatures from rural locations (unaffected by artificial urban heat) aligned well with trends in solar activity since the 19th century. However, models of greenhouse gas forcing did not correlate well with the long-term hemispheric record.

Image Source: Soon et al. (2015)

Image Source: Soon et al. (2015)


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A new paper (Scafetta et al., 2019) also finds the global temperature record aligns well with trends in TSI when using the observation-based ACRIM satellite data rather than the model-based (and IPCC-preferred) PMOD data for trends in recent decades.

Image Source: Scafetta et al., 2019
“By adjusting the TSI proxy models to agree with the data patterns before and after the ACRIM-gap, we found that these models miss a slowly varying TSI component. The adjusted models suggest that the quiet solar luminosity increased from the 1986 to the 1996 TSI minimum by about 0.45 W/m² reaching a peak near 2000 and decreased by about 0.15 W/m² from the 1996 to the 2008 TSI cycle minimum. This pattern is found to be compatible with the ACRIM TSI composite and confirms the ACRIM TSI increasing trend from 1980 to 2000, followed by a long-term decreasing trend since.”
“This model was extended using the ACRIM composite since 1981 and an average between VIRGO and SORCE TIM since 2013. This particular TSI model appeared to correlate well with the Earth’s global surface temperature records since 1700 [Hoyt et al., 1993, . … The TSI data from 1978 to 1981 appeared too corrupted because of uncorrected degradation of theNimbus7/ERB sensors during the solar maximum of cycle 21. For this reason, it was more appropriate to dismiss the data from this period because modifying TSI data using proxy models, as done by PMOD, would be arbitrary. We proposed that any reliable TSI composite should begin from late 1980 with the ACRIM1 record.”
“The same harmonic solar model suggests that the sun may now be heading toward a new grand solar minimum in the 2030–2040 time frame. Final evidence that TSI may have increased from 1980 to 2000 comes from Earth’s climate studies. Secular climate records correlate well with TSI curves such as the one depicted in Figure 13 and on longer ones covering the entire Holocene [1,23,60,64]. In particular, the warming observed from 1970 to 2000, followed by a temperature standstill since 2000, is a good fit for a natural 60-year cycle prediction superimposed to other contributions [20]. This pattern correlates better with a TSI evolution similar to the ACRIM composite [17–21,62,65] than with the CMIP5 general circulation climate model predictions of continuous anthropogenic warming [22]. The CMIP5 climate models use a high climate sensitivity to CO2 forcing and low secular TSI variability proxy models, such as the one proposed in [3], which was calibrated using the PMOD TSI composite model after 1980.”

Image Source: Lansner and Pepke Pedersen, 2018
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently I wrote here how Germany’s now infamous record-setting weather station in Lingen was producing readings that were 2-3°C hotter than surrounding stations, yet the German DWD weather service refused to acknowledge the station was likely producing bad data. Today they admit the station has problems and that they will be moving it to a better location.
Last year’s all-time record high is now in question.
Lingen’s heated readings
Last summer the Lingen station, located in northwest Germany near the Dutch border, smashed the country’s all-time record high when the ‘mercury’ rose to a scorching 42.6°C during a late July heat wave. The previous all-time high for Germany was a comparatively cool 40.3°C.

Lingen’s readings of late July 2019 compared to other stations in the surrounding region (July 23 – July 27).
Today, t-online.de reports that Germany DWD national weather service has now reversed and realized that something may be very wrong with the Lingen station after all, and so will relocate it and examine its recorded data.
“The weather station in Lingen: The DWD will not publish the temperatures measured here anymore – there are doubts about the data,” T-online reports. “Now it is being relocated.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moreover, the temperature measurements previously recorded at the station will be checked, and there are now doubts whether last year’s all time record will be allowed to stand.
“Astonishing temperature values”
“We don’t know yet whether the value will stand,” admitted a DWD spokesperson on Friday. “Again and again and more and more frequently astonishing temperature values” had been coming from Lingen.
The DWD experts believe the distorted values are linked to “certain weather conditions,  especially on hot summer days when there is little wind. The station in Lingen is located in a depression and is now surrounded by trees, which means the air gets stuck in place and so heats up.
The decision to stop using data from the current Lingen station and to relocate it represents a position reversal by the DWD. Earlier the DWD had told Bild daily how it planned to stick to the Lingen readings, deeming them to be of good scientific quality and that an earlier site assessment had found that the station conditions had “no serious influence on the temperature measurements” and therefore “did not contradict the WMO standards.”
The DWD now acknowledges the station has siting issues and its data are suspicious after a number meteorologists criticized the station’s poor siting.
“The quality of our measurements has the highest priority,” said Jürgen Schreiber, DWD’s Chief Technical Officer. “We have decided to no longer publish the observational data from the Lingen station, but to use them internally for scientific tests only”.
The DWD spokesman said a second sensor would be used to measure the temperature at another location in parallel. Though after two days it is still too early to make scientific statements, but already there have been noticeable deviations in the temperature measurements. Now the tests are to be carried out with meticulousness.
T-online.de confirms that construction work for a new location has begun and that by next spring there could be temperature data coming from Lingen again.
Share this...FacebookTwitter "
"

The image above is from Google Earth, which had its photography for Chico updated just within the last two weeks. The picture is the closest zoom available and shows the new City Plaza under construction. Based on other construction landmarks around town and the colors of foliage, plus the fullness of Lake Oroville at the time, I’m estimating the image was snapped by satellite sometime in late June 2006.

Just looking at it I immediately found out something I didn’t know; That the four corners of City Plaza are compass points. The North corner, which points to Duffy’s Tavern is almost exactly true North, with the other corners being East, South and West. I added the letters to the image for your convenience, they don’t exist on the sidewalks.
The previous imagery for Chico on Google Earth was taken in 2003, and quite fuzzy. Partially due to the image being taken while smoke and haze from a wildfire somewhere covered the town. Part of that imagery remains in the Nord Avenue section on the west side and when you load images with that view, you can see a sharp cutoff where the smoke ends and fresh image begins.
Another thing thats been updated is integration of vegetation colors at lesser zoom levels, which clearly show where agriculture exists. See the image below.

But the coolest thing is the 3D capability, and in the image below, you can take in all of Chico and the foothills in one image. This view is looking Northeast from about 23,000 feet altitude. The Chico Airport is in the upper left. Click the link below the picture for a larger more detailed image.

Click to view larger image
If you haven’t tried Google Earth yet, its really a lot of fun. Free too. See http://earth.google.com/ It’s a great tool for figuring out things or for just looking around at things you’ve never seen from the air. The Pro version, which I use, allows integration of overlays and drawings, and allows measurements. When I was running for County Supervisor this past year I used Google Earth partly to devise a solution to the Keefer Slough flooding problem. I’ll post that solution in a future blog.
Another thing that can be seen in Google Earth is the city owned stormwater basin just south of the Chico ER building that was the subject of problems this summer related to mosquitos and a West Nile virus outbreak. That was a hot topic on Jack Lee’s blog this summer.
But the most odd new image in Chico is the one below. Can anybody guess what it is?



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea922807c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterUPDATE: Here’s an even earlier version of the NASA GISS data plot of Hokitika station before all the data altering began (hat-tip Iggie). It shows temperatures had been COOLING:

Source: NASA
By Kirye
and Pierre Gosselin
We continue to hear warming horror stories coming from the island of New Zealand, and the socialist by sales pitch how “climate change is the biggest challenge of our time“.
Yet this doesn’t seem to be the case in New Zealand. For example, we learned from Electroverse here that the Pacific island country “just recorded its coldest June temperature in 5 years”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moreover, when we examine NASA GISS data, we uncover where all the warming rumors come from: alterations to the recorded historical data.
The following chart shows the data from Hokitika Aerodome, going back more 130 years (It’s the only NASA station that has data going back over 100 years). Plotted are the unadjusted Version 3 data and the Version 4 unadjusted:

Data source: NASA GISS V3 and V4
The old data set showed no warming, until NASA GISS went back and rewrote it Orwellian style, and made up a warming trend and called it Version 4.
Tony Heller also reported earlier: “NASA  didn’t like the fact that New Zealand wasn’t warming, so they simply changed the data.”
In summary: There really hasn’t been that much change at many locations around the globe. In fact the real changes are taking place in the NASA GISS datasets.


		jQuery(document).ready(function(){
			jQuery('#dd_2092e503990bc1bcda4ec06e5b01627d').on('change', function() {
			  jQuery('#amount_2092e503990bc1bcda4ec06e5b01627d').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The new US stealth fighter, the F-22 Raptor, was deployed for the first time to Asia earlier this month. On Feb. 11, twelve Raptors flying from Hawaii to Japan were forced to turn back when a software glitch crashed all of the F-22s’ on-board computers as they crossed the international date line.
The delay in arrival in Japan was previously reported, with rumors of problems with the software. CNN reported that every fighter completely lost all navigation and communications when they crossed the International Date Line. They reportedly had to turn around and follow their tankers by visual contact back to Hawaii. According to the CNN story, if they had not been with their tankers, or the weather had been bad, this would have been serious.
I have to think there’s going to come a time when wars are fought by warrior hackers, each trying to take down the other sides computers. Or there may come a day when an airliner falls out of the sky because software failed on all the redundant systems. I sure hope not.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7c4cb19',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report is a weekly feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Whenever the topic of rising seas comes up, we point out that Antarctica is expected to gain mass through enhanced snowfall in a warmer climate, and therefore its contribution to global sea level rise should be negative—that is, the water locked up in the added snowfall there will act to reduce the level of the globe’s seas. The models used by the Intergovernmental Panel on Climate Change (IPCC) in their 2007 Fourth Assessment Report project the sea level reduction from this mechanism by the end of the 21st century to amount to somewhere between 2 cm and 14 cm (roughly 1 to 6 inches). While this is not a lot, the main point is that Antarctica is not expected to be a contributor to rising seas as the climate warms. Without a large contribution from Antarctica, we will not approach alarmist projections of a meter-plus of sea level rise by century’s end.   
  
Up to now, though, Antarctica has not exactly been with the program.   
  
Instead of gaining mass through increased snowfall, there have been indications that Antarctica is losing ice (contributing to sea level rise) as ice discharge from its coastal glaciers exceeds gains from snow increases (which have been hard to find). One has to wonder whether Antarctica, contrary to expectations, will continue to lose mass and become an important contributor sea level rise, or whether the projected increases in snowfall have just not yet reached a magnitude sufficient to offset the loss from glacial discharge.   
  
Things are starting to change down there.   




The research that has gotten the most attention on the subject of Antarctic mass balance has been based on observations made by the Gravity Recovery And Climate Experiment (GRACE) satellite. This orbiter senses changes in gravity (i.e., mass) which can be caused by increasing snow and ice loads over the continent. One key piece of information which must be factored into the calculations of ice mass change is the change in the underlying geologic formations, which are still rebounding from enormous amounts of ice lost after the end of the last ice age. This geologic motion, known as the glacial isostatic adjustment (GIA), is largely modeled rather than directly observed. Our level of knowledge (or lack thereof) of the true GIA adds a sizable amount of uncertainty to GRACE-based estimates of the ice mass changes over time in Antarctica (and Greenland, the northern hemisphere’s cheap imitation of Antarctica).   
  
In a widely cited finding, Velicogna (2009) reported that Antarctica was losing ice at a rate of about 104 gigatons per year (Gt/yr) during the period 2002–2006, increasing to a loss rate of 246 Gt/yr during 2006–2009 (about 374 Gt of ice are equivalent to 1 mm of sea level). Rignot et al. (2011) also found an acceleration of ice loss there, increasing from a loss of about 209 Gt/yr (in 2003-2007) to about 265 Gt/yr from 2007 to 2010. However, Wu et al. (2010) argued that the GIA model used in these previous studies is incorrect, and that when a more accurate GIA model is incorporated in the GRACE-based ice mass change calculations, Antarctica was only losing about 87 Gt/yr during the period 2002–2008.   




Support for the GRACE-based calculations comes from the general agreement between the GRACE numbers and those calculated from studies of changes in the grounding lines of coastal glaciers and the ice flow across those grounding lines in association with the other aspects of the mass balance. This method is known as the Input-minus-Output Method (IOM). The IOM estimates of the average ice loss from Antarctica over the past several decades (1992–2007) lie somewhere around 136 Gt/yr, in rough agreement with the GRACE-based estimates. However, the IOM is also subject to a lot of uncertainty. An attempt by Zwally and Giovinetto (2011) to reduce the uncertainty and increase the accuracy resulted in an IOM-based estimate of a loss of only 13 Gt/yr over the same 18-yr period and led the researchers to conclude that: 



Although recent reports of large and increasing rates of mass loss with time from GRACE-based studies cite agreement with IOM results, our evaluation does not support that conclusion.



It seems that as the calculations and derivations are improved, the amount of ice mass that Antarctica is supposedly losing gets less and less.   
  
Or perhaps it isn’t losing any mass.   
  
Using a set of observations from a series of satellites that have been in orbit since 1992 and that measure changes in the height of the surface of the ice (ICESat), NASA’s Jay Zwally and colleagues (2012) report that Antarctica is gaining mass. Zwally recently presented his findings to a workshop of the Ice-Sheet Mass Balance and Sea Level expert group of the Scientific Committee on Antarctic Research and the International Arctic Science Committee. According to his abstract, Zwally reported that “During 2003 to 2008, the mass gain of the Antarctic ice sheet from snow accumulation exceeded the mass loss from ice discharge by 49 Gt/yr (2.5% of input), as derived from ICESat laser measurements of elevation change.”   
  
Zwally further added, ""A slow increase in snowfall with climate warming, consistent with model predictions, may be offsetting increased dynamic losses.""   
  
So the ""global warming, leading to increased snowfall, leading to a drawdown of global sea level"" mechanism may be operating after all.   
  
A paper to soon appear in _Geophysical Research Letters_ give us another enticing look at recent snowfall changes in Antarctica. In “Snowfall driven mass change on the East Antarctic ice sheet,” Carmen Boening and colleagues from NASA’s Jet Propulsion Laboratory report that extreme precipitation (snowfall) events in recent years (beginning in 2009) have led to a dramatic gain in the ice mass in the coastal portions of East Antarctica amounting to about 350 Gt in total (Figure 1).   






Figure 1. Timeseries of snow accumulation in coastal East Antarctica (shaded region in inset).   
(Source: Boening et al., 2012)Boening et al. reported that the increase in ice mass in East Antarctica has not completely offset the loss of ice mass during the same time in West Antarctica, but as this comparison is made using GRACE data, it is hard to know just how accurate it is.   
  
Also note that a few years with a lot of snowfall does not mean that a change in the long-term snowfall rate has occurred. Nevertheless, the situation bears careful watching.   
  
Putting everything together, we conclude that many of the claims that Antarctica is rapidly losing ice and increasingly contributing to a rise in global sea levels must now be, at the very least, tempered, if not overturned entirely. Time will certainly tell. And time will also tell just how much we need to worry about future sea level rise. Currently, the answer seems to be “not overly much.”   

"
"

 _The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels and Paul C. “Chip” Knappenberger, from Cato’s Center for the Study of Science, review interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
  
  
With all the stern talk about global warming and widespread concern over climate change, you would think that we humans would have a propensity for cooler temperatures. Everywhere you look, the misery that rising temperatures (and the associated evils) will supposedly heap upon us seems to dominate reports about the coming climate. But do patterns of population movement really support the idea that we prefer cooler locations?   
  
**Increased Mobility**   
  
Since 1900, the population of the United States increased from about 76 million people to about 309 million people in 2010. Accompanying that population growth were major advances in technology and industry, including vast improvements in our nation’s system of transportation. As planes, trains, and automobiles replaced the horse and buggy, Americans became more mobile, and where we live was no longer connected primarily with proximity to where we were born. Instead, we became much freer to choose our place of residence based on considerations other than ease of getting there.   
  
Where has our new-found freedom of mobility led us? Figure 1 shows the rate of population change from 1900 to 2010 for each of the contiguous 48 states. Notice the increases in states with warm climates such as Florida, Texas, and California, and also in states with big industry (that is, jobs), such as New York, Michigan, and Ohio for example.   
  
  






  




_Figure 1. The state-by-state population trend (people/year) from 1900 to 2010 (data fromU.S. Census Bureau)._



Which states are people less likely to choose to live in? States such as North Dakota, South Dakota, Montana, Maine, Vermont—all of which have harsh climates and low temperatures.   
  
Comparing a map of the change in population (Figure 1) with a map depicting the average temperature of each state (Figure 2) reveals a pretty strong indication that people seem to be seeking out warmer states.   
  
  






**  
**_Figure 2. The state-by-state average annual temperature for the period 1900-2010 (statewide temperture data available from the U.S.National Climatic Data Center)._   
  
**Experiential Temperature**   
  
Another way of looking at human temperature preferences is to calculate what we’ll call the “average experiential temperature”—that is, the annual temperature that the average person living in the lower 48 states experiences each year. We can calculate this value by first multiplying the average temperature in each state during a particular year by the state’s population in the same year. Then we sum this product across the 48 contiguous states, and finally divide this sum by the total population of the country. In other words, the temperature in states with larger populations weigh more heavily on the national composite experiential temperature than does the temperature in those states with sparser populations. As the population of the country redistributes itself over time, we can track how the average person’s climate changes.   
  
When we do that for each year from 1900 to 2013, we get the result shown in Figure 3—a steadily rising temperature. In fact, the average experiential temperature has risen by a total of about 3.85ºF over the course of the last 114 years (a rate of 0.34ºF per decade).   








_Figure 3. The average experiential temperature of the population of the United States, 1900 to 2013._



But the history of experiential temperatures alone can’t tell us whether the increase has been unwillingly forced upon us by a large-scale warming of the climate from, say, an enhanced greenhouse effect, or whether the change results from Americans seeking out warmer locales on their own accord.   
  
**U.S.** **Average Temperature**   
  
To answer this question, we must calculate the area-weighted average temperature of the United States—that is, the combination of the yearly average temperature within each state weighted by that state’s total area. In this case, it is the size of the state, rather than the size of its population, that matters—the bigger the state, the bigger its contribution to the nationwide average.   
  
The result of this calculation is a quite different looking temperature history. In Figure 4, we included the annual U.S. average temperature history along with the annual U.S. “experiential” temperature from Figure 3. We see that, while the United States actual temperature has fluctuated a bit, experiencing warm decades such as the 1930s and 1990s and cold ones such as the 1910s and 1970s, it has increased only slightly during the 20th century—about 0.90ºF (a rate of 0.08ºF/decade).   






_Figure 4. Average temperature of the United States, 1900 to 2013._



For what it’s worth, when you calculate the national temperature this way (using the state-by-state temperature data from the National Climatic Data Center, NCDC), you get a heckuva lot less warming than is in the “official” NCDC record put out by the U.S. Department of Commerce. The difference lies in the “adjustments” plastered on to the original data. Both records are adjusted for a bias known as “time of day” when the previous 24-hour highs and lows recorded. It’s complicated, but it also does slightly alter the data.   
  
But the official version is additionally massaged more than—well, we can’t say in polite company. A laundry list can be found here. The sum of all of those adjustments is to put about twice as much warming in the record as is in our state-averaged plot.   
  
**Seeking the Heat**   
  
Although there has been a slight warm-up of the actual temperature, that rise is nowhere near the increase in the _experiential_ temperature. In fact, the average experiential temperature has climbed at a rate more than four times that of the U. S. average temperature—which is the experiential temperature had the population distribution not changed at all. That means that Americans have actively been moving to warmer climates. And there is every indication that they are continuing to do so, as evidenced by the strong rise in experiential temperatures during the past 20 or 30 years.   
  
While climatologists have not generally appreciated this fact, it has been long recognized and appreciated by sociologists. As both people's mobility and their ability to select the climate they prefer have increased throughout this past century, the core of the U.S. population has moved southward—into warmer climates. The overall migration of people into the southern ""Sunbelt"" states has created a temperature change over time for the ""average American"" that far outstrips the most pessimistic measurements of global warming for the past century, and rivals the projections for the next!   
  
Apparently, people--or Americans at least--seem to prefer a warmer climate to a cooler one. Next time climate prognosticators warn of the perils of rising temperatures, remember this: when given the means and a choice, some (or rather, most) like it hot!   
  
_(Special thanks to Robert C. Balling Jr. and Randy Cerveny, who assisted with early versions of this research.)_


"
"
Share this...FacebookTwitterOf good trees and bad trees: an unimaginable story
By Die kalte Sonne
(Text translated by P Gosselin)
We have already reported about the very different views on trees in this blog. Perhaps this phenomenon has something to do with the fact that the words environmental protection and nature conservation are slowly but surely disappearing from our language and being displaced by climate protection. Everything has to subordinate itself to this, also environmental and nature protection. Sometimes this has has had disastrous consequences.
The value of trees is in the eye of the observer or his agenda
Trees are extremely valuable carbon stores. They are true CO2 sinks. It is estimated that a large tree removes and stores about 12.5 kg of CO2 per year from the atmosphere. Actually, one would have to think, we should not only reforest massively, as Professor Werner Sinn suggested in his lecture “How we save the climate and how not“, but also preserve existing tree populations.
Of course, trees are protected, sometimes with drastic means such as in the Hambach Forest. There, however, not for CO2 storage reasons but because the activists want to prevent lignite mining. Such actions are spectacular and get through the media. So this is about good trees.
Much less attention is paid to protests by residents of Grünheide in Brandenburg, who are mobilizing against the deforestation of an area the size of 420 football pitches, which are to make way for Tesla’s new megafactory. Here too, nature is losing carbon stores, and no activist is really itching because they are bad trees. Or were there demos of Fridays For Future (FFF) or Extinction Rebellion in Grünheide?
Weird swaps in Scotland
Just as little interest in Scotland. There it has now been discovered that almost 14 million trees have had to be felled since 2000 to build wind turbines (WTGs). According to the above calculation, Scotland has thus “given up” 175,000 tonnes of CO2 reduction per year in order to save the climate. Even planting 100,000 trees, as in Scotland, is of little use, as they only replace the lost capacity to a very limited extent. Trees simply need time until they are stately and can absorb the above-mentioned amount of CO2 annually.
Foundations and access
The areas for the foundations are still the least of the evils, although in Schleswig Holstein alone, a sealed area of 3 million square meters was assumed in 2018. Approximately 1300 cubic meters of concrete and 180 tons of steel disappear in such foundations.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image: By Mussklprozz (Own work) [CC-BY-SA-3.0 ( http://creativecommons.org/licenses/by-sa/3.0 )],via Wikimedia Commons
It is not even clear how such colossuses are to be removed from the forest later on, or how they can be removed at all. Anyone who has ever spent a holiday in France on the Atlantic Ocean knows that concrete remains, i.e. bunkers, of the 3rd Reich bunkers stubbornly refuse to decay on the coasts. Ina 1000 years probably only the bunkers of the Nazis or the foundations of wind turbines will remain.
Access to wind turbines is much more serious than the foundations, which only cover a relatively small area. Wind turbines are getting ever taller and the rotors ever bigger. The radius that the special transport vehicles now is so large that a massive quantities of trees have to make way for access roads. And since the wind turbines only have a limited lifetime, the access roads have to remain, because at some point they will have to be dismantled or maintained. The forest at this point is lost and chopped up.
German conservative CDU now poised to play along
The CDU Lower Saxony is now poised to go along with a proposal that more wind turbines in forests should be approved. Whether an impact assessment has been made here? Especially in forests, the population of birds of prey is high and one can only guess what will happen if huge rotors rotate over birds or their breeding grounds and habitat. These rotors are, as studies show, a considerable hazard for birds of prey.
Indeed the wind power lobby is trying to invalidate such studies, for example by pointing out the large number of songbirds and garden birds that are killed annually by windows, cars or cats. But if you use your common sense, here you see whataboutism in its purest form. Birds of prey very rarely die from windows, cars or cats and songbirds and garden birds rarely die from wind turbines. At the latest, when the census of seabirds in the Irish Sea – an area with a lot of wind turbines – shows that the population is declining massively, the windscreen/cat/car argument falls apart.
The same outcome, but completely different reactions
But it gets really crazy when we look at the situation in places like the Reinhardts Forest in the state of Hesse. This forest is very valuable, because it still has a virgin forest character in parts. Nevertheless, wind turbines are to be built there with all the consequences described above. Residents’ protests are being dismissed as an obstacle to technology and energy production transformation.
Yet, at the same time, the people go into collective outrage when the Amazon becomes smaller through slash-and-burn clearing. In both cases forests, biotopes and very same carbon reservoirs are lost, but with completely contrary reactions. Good and bad trees.
Used to be tranquility above the tree tops
But forests are much more. Many people pursue various activities there. A climate activist from Berlin Kreuzberg or Hamburg Ottensen may find this hard to imagine, but there are actually people who visit the forests enjoy tranquility or the sounds of nature. If the plans of wind power advocates are anything to go by, then in many forests this will soon be lost forever.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterVictoria Falls ignoring IPCC science: Sometimes more water, sometimes less
By Die kalte Sonne
(German text translated by P. Gosselin)
German Spiegel Online on 7 December 2019:
Victoria Falls in Zimbabwe and Zambia: “It’s the longest dry period that we have ever had.”
The Victoria Falls are considered to be the widest waterfall in the world. But instead of the usual quantities that plunge into its depths, there is a drought – tourists have also gone absent. The mood there is gloomy.”
And, of course, it is being immediately attempted to explain the water flow with man-made climate change. The Guardian wrote on the same day:
Data from the Zambezi River Authority shows water flow at its lowest since 1995, and well under the long-term average. Zambian president, Edgar Lungu, has called it ‘a stark reminder of what climate change is doing to our environment’.”
Spiegel calls it the worst drought ever. The Guardian calls it the worst drought since 1995. That’s a small difference.
As always, it’s best to look at the hard data. Here we look into a report by Richard Beilfuss from 2012 (pdf here), which has the following exciting title:
A Risky Climate for Southern African Hydro: assessing hydrological risks and consequences for Zambezi River Basin Dams “
The report checks whether dams along the Zambezi River are always supplied with sufficient water. In the event of water shortages, the turbines would quickly stop and the production of electricity would fail. Therefore, Beilfuss looks back into the hydrological past of the Zambezi region to better understand the variability of rainfall. He embeds his research in the usual climate change narrative, which we can overlook.
We are particularly interested in the facts presented by Beilfuss. From his summary:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Zambezi River Basin has one of the most variable climates of any major river basin in the world, with an extreme range of conditions across the catchment and through time. Average annual rainfall varies from more than 1,600 mm per year in some far northern highland areas to less than 550 mm per year in the water-stressed southern portion of the basin.
Runoff is highly variable across the basin, and from year to year. The entire Zambezi River Basin is highly susceptible to extreme droughts (often multi-year droughts) and floods that occur nearly every decade. Droughts have considerable impact on river flows and hydropower production in the basin. For example, during the severe 1991/92 drought, reduced hydropower generation resulted in an estimated US$102 million reduction in GDP, $36 million reduction in export earnings, and the loss of 3,000 jobs.”
Oh, man. The Zambezi area is already well known as a highly variable rain area. Can a drought be surprising at all? Why is this drought man-made in 2019, when there have apparently always been droughts in the past?
Not CO2 related
A little later in the report we also find a flow diagram for the Victoria Falls:

Figure: Water flow rate-volume at the Victoria Falls an den 1907-2006. Source: Beilfuss 2012 (immediately pdf)
We see a strong variability from year to year. On a scale of several decades, the period 1940-1980 is characterized by particularly high flow rates. The early 20th century was rather dry. A coupling to the 60-year-old ocean cycle offers itself. The Pacific is far away, but the wet Zambezi phase fits quite well into the negative PDO:

Figure: The Pacific Decadal Oscillation (PDO). Source: By Giorgiogp2 – Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=13297650
Working hypothesis: Whenever the PDO is positive, the flow at the Victoria Falls decreases. And what is PDO doing right now? It is positive. It fits!

Abbildung: PDO bis Ende 2019. Quelle: daculaweather.com
It’s a pity that so few take the trouble to first check the natural precipitation dynamics and possible correlations.
It’s so much easier to simply hold the universal rogue CO2 reflexively responsible for every observed rain anomaly. Naturally, this quick fix is not sustainable. In this specific case, it was probably just a matter of dark climate alarmist background music for the climate conference in Madrid.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe two recent dry summers seen in Europe have led to alarmists believing that the climate doomsday has arrived. But The European Institute for Climate and Energy (EIKE) looks at the past to see if this sort of thing is really unusual.
=======================
German forests growing much faster today than 1000 years ago. Photo: NTZ
=================
Dry summers as a doomsday scenario – are they really something new?
By Axel Robert Göhring
(Text translated/edited by P. Gosselin)
Drought completely normal during High Medieval Ages.
Researchers from the German Department of Ecology and Ecosystem Dynamics at the University of Greifswald have shown that drought in the High Middle Ages was completely normal during the summer. Even if hardly any real scientist dares to say anything against the climate madness, many do their work properly and deliver many small mosaic pieces for dismantling all the fraud.
Last spring, however, one could hear “top physicist” Harald Lesch at the Markus Lanz’s ZDF show claim how climate change would hit quite badly in summer, how the drought of the “record summer” 2019 would have violent effects, especially on the holy German Forest (forest die-off scare came knocking again…).
So what about drought in the holy German Forest 2019? Is it real, or “interpreted”?
Well, it’s probably real. But why not? In summer it is hot and dry even in the temperate climate zone of Europe. Mr. Lesch & Co. showed a heat peak and claimed it is man-made climate change. And when a cold peak appears, then it is only weather – or even proof of climate change. Weather extremes are somehow more frequent today.
Dry summers in Europe not uncommon, new study


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Biologist Martin Wilmking and his team from the University of Greifswald in the German state of Vorpommern now have shown that dry summers a thousand years ago were not uncommon in northern Germany. In fact, they were much warmer than today – and this without combustion engines, industry and motor traffic.
Prof. Wilmking and his biologists evaluated so-called proxy data, i.e. verifiable effects of climate in animate or inanimate nature. Specifically, the team worked on annual rings in living beech trees and thousand-year-old archaeological timber: the long established field of expertise is dendroclimatology (Greek: dendron – the tree).
Trees growing faster today
The authors prove once again that our forests are growing much faster today than in the past, because agriculture (also traffic & industry) provides them with a lot of fixed nitrogen (ammonium salts).
The modestly increased CO2 content of today’s air also allows the trees to open the stomata of the leaves for a shorter period of time, thus limiting water losses. In other words, our industrial civilization is considerably HELPING the forest by supplying it with building materials and  even water indirectly. This is nothing new for avid EIKE readers, as we have pointed out more than once that the planet has become much greener in recent decades.
Often dry in the prosperous High Middle Ages
If one includes the faster growth of today’s trees, one can conclude in comparison using the annual ring curves of the historical woods that it was often dry in summer in the High Middle Ages. Even the Rhine, the largest river in Europe, became dry near Cologne. Yet the High and Late Middle Ages were not a phase of decline like the Early Middle Ages. On the contrary, the courtly knightly culture flourished in Western and Central Europe. There are tens of thousands of stone castles in Germany, Switzerland, Bohemia and Austria – several in the Saale valley near EIKE’s home city of Jena.
And if you take a boat trip on the Rhine or Moselle rivers – ruins of stone castles can be seen everywhere. They all date back to the time after 1000 AD. The stone castles are a testimony to a significant increase in Europe’s economic performance, which can be traced back to the warmth of the Climate Optimum. The development lasted until the late Middle Ages, when it then became much colder again.
Politically correct theories assume that the booming economy overtaxed nature and thus undermined itself. Certainly not wrong, but without the cold, nature could have recovered faster from the overexploitation.
Warmer is better
Also findings from graves, for example near Berlin, prove that Brandenburg citizens from the Renaissance period were significantly sicker than their ancestors of the High and Late Middle Ages. The saying rings true: “Cold is bad, warm is good.”
Share this...FacebookTwitter "
"
In my previous post, NOAA Throws a roadblock my way I talked about how NOAA/NCDC has thrown a roadblock into the work being done to survey weather stations citing “privacy concerns” of observer’s name being included in station data being used to locate stations.
Alert blog reader Gerald Ingle passed this info on to me.
It appears that NOAA does not follow their own edicts, as they have a web page dedicated to cooperative observer newsletters and awards.

http://www.nws.noaa.gov/om/coop/2002-Awards.htm
On this web page you can find names of the observers, the station name, their PHOTOS in front of their stations, and in some cases their partial life history!
They also have a gallery of images in addition to the newsletters about COOP observers.
For example:

http://www.nws.noaa.gov/om/coop/2002/2002-15.htm

Britt, IA, Cooperative Observers Dianne and Keith Hansons show
off their 10 Year Length of Service Award.
This blows the NOAA/NCDC “privacy concerns” out of the water. They were worried about names appearing with MMS station data, well here we have names, photos, and more on NOAA’s own website.
They can’t have it both ways. Here is the link for the NOAA newsletters page.

http://www.nws.noaa.gov/om/coop/coop_newsletter.htm
Note the link where ANYBODY can sign up their email and get the newsletter chock full of names, stations, and photos of observers

Get on the free newsletter mailing list
It’s not even a confirmation email signup, just type in anybody’s email and it appears to accept it.
No confirmation email was received when I signed up, so apparently having somebody getting spammed isn’t an issue either.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5742f5f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Global warming alarmists like claiming that a certain place is seeing more warming and climate change than everywhere else. Remarkably, they say that about almost everywhere, which of course makes no sense.
Today we look at Canadian temperature trends using the data from the Japan Meteorological Institute (JMA) for stations where they have data available going back to at least the mid 1990s.
First we look at December mean temperatures. What follows is a chart depicting the results of 9 stations across Canada:

Of the 9 examined stations, seven show no warming taking place at all in Canada over the past quarter century for the month of December. Data: JMA. 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The data hardly show the trends you’d expect from a place that is supposed to be “warming faster than anywhere else”.
Canada mean annual temperatures show no warming 
Okay, those are only data for December. How about the annual mean temperatures?
What follows are plots for the mean annual temperatures for the 9 stations: 
Data source: JMA
The plots speak clearly enough: we have been seeing more cooling than warming.
Though the surface of the globe may be have warmed modestly as a whole, nothing unusual is going on. What we are likely seeing are mainly natural oceanic cycles at work, which we still know very little about.
Share this...FacebookTwitter "
"

No thats not the title of a new Godzilla movie, but “Deep Fried Rodan” could be.
My friends in journalism say news goes in cycles. If that is so, this must be the year of the creepy crawly restaurant.
Today I see on the TV news the shocking video (a frame of which is shown above) of the Kentucky Fried Chicken combo Taco Bell in New York City’s Greenwich Village that has been taken over by rats and closed down by the health department.
What’s in those buckets anyway? Just kidding, and the trademark bucket in the picture above had a little help using Photoshop. But it makes you wonder just how many restaurants in America are as bad as this?
Oddly, it was exactly one year ago today that we had the China Star meltdown, where police and fire responders to a burglar alarm found a restaurant so incredibly filthy and pest ridden, it defied description.
In his ER article last year,  reporter Ari Cohn and Chico PD officer Melody Davidson’s incident report did an admirable job in conveying the heebie jeebies via the written word to anyone whom ever ate there. Today reading the news reports online and then seeing the videos, it was “like Deja Vu all over again”.
I wrote a letter to the editor last year suggesting we need to have color coded health inspection reports posted in the entrance of every restaurant showing its last inspection status. Green for Pass, Yellow for some minor violations, and Red for get the heck outta there ! I still think its a good idea.
Some progress has been made, as now you can get inspection reports online at  Butte County’s Health Department. Here is the link: http://www.buttecounty.net/Default.aspx?tabid=312
Reading through the list of inspection reports at the Butte Health Dept website, I was surprised to learn that even some well known and considered “classy” Chico restaurants had some major violations in the last year. If you eat out a lot, this website is worth a look. Any enterprise that sells packaged food, serves food or food samples, including school cafeterias, coffee houses, country clubs, fraternal clubs, and even liquor stores get inspected by the County Health Department.
Here’s a surprising fact: Indian Casinos and their restaurants are exempt from inspections, because tribal operations are considered their own sovereign nation. That may be so, but I think any place that could potentially make people sick through sloppy food handling shouldn’t get a free pass on a legal technicality.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea83ee422',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn Berlin today, some 200 scientists shivered at a scientists4future demonstration in front of the Chancellor’s office in order to protest government inaction on combatting global warming.
German activist scientists silently demonstrated before the Chancellor Merkel’s office to protest inaction over warming. Cropped image: ScientistsForFuture.
One prominent attendee was FridaysForFuture activist Prof. Volker Quaschning, who proudly took the day off from lecturing on taxpayer expense. Here’s what he tweeted just before attending the modest demonstration:

Auf dem Weg zur #scientists4future  Schweigedemonstration heute 12:30 vorm Kanzlerinnenamt. Zur #Klimakrise ist alles gesagt. Handel, liebe Regierung!#FridaysForFuture pic.twitter.com/LNycIksdXa
— Volker Quaschning (@VQuaschning) November 15, 2019

Quaschning, a HTW Berlin professor, is seen above at a bus stop on his way to the demonstration, holding the propaganda temperature stripe chart to protest the German government’s inaction on fighting global warming. Unfortunately the professor looks rather silly all dressed up for winter cold in gloves, knit hat, scarf and coat – to protest warming!
200 scientists shiver to protest warming
The activist Berlin professor wasn’t the only scientist trying to stay warm today while protesting climate warming. Two hundred other scientists also showed up in front of Chancellor Merkel’s office, all bundled up in winter clothes, demanding a stop to the warming and that they be listened to – instead of the working class taxpayers.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Low IQ scientists in Berlin outside freezing, dressed in winter clothing, protesting warming! https://t.co/dOwWfFHoEy
— Pierre L. Gosselin (@NoTricksZone) November 15, 2019

The science-is-settled scientists held up signs declaring, “Everything has been said! Act now!” or: “Decades of climate research: Ignored!”
The question is whether they will be taken seriously by the hundreds of millions of Northern Hemisphere inhabitants who are getting socked by a premature frigid winter this year.
Silent protest
The scientists4future all appeared with their mouths taped shut in order to symbolize a “silent protest”. Or perhaps the tape was to keep their teeth from chattering as they shivered in the bitter November cold.
Of course, the scientists didn’t stick around too long. Reportedly they left for an early start to the weekend – in the warm comfort of their homes.
Today the German government voted to pass measures that among other things will make heating and fuel more expensive for ordinary citizens starting next year.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI feel obligated to upgrade a reader comment by Jim Lakely, Communications Director, The Heartland Institute, to a post. He says I was “being very unfair” yesterday.
My main gripe is that these two very dishonest Correctiv “journalists” should have never been allowed to get as far as they did. Now we independent skeptics here in Germany have to deal with being smeared nationally again.
In Germany, the skeptics are outnumbered by like 20 to 1; it’s not a 50-50 deal like in USA. German MSM journalists like those at ZDF are very nasty, unfair and dishonest to legitimate climate science critics.

Jim Lakely, Director of Communications, The Heartland Institute
I have other comments, but will hold them for another time.
I’d love to see a point-by-point rebuttal from Heartland to the nonsense made by ZDF Frontal 21 and Correctiv so that we can defend ourselves here.
============================================================



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Pierre,
You are being VERY unfair to James Taylor and Heartland, and Naomi Seibt — who is not “distraught,” but angry (as is Heartland) at how this has been able “to weave a highly distorted, one-sided, yet believable hit piece.” You had it right on “distorted,” and not so right with the rest.
First: James is surely flattered by your battlefield promotion, but he is not the president of The Heartland Institute. He is the Director of Heartland’s Arthur B. Robinson Center on Climate and Environmental Policy.
Second: They “worked” James spottily over several days in two cities over two weeks before he sat down for a brief talk. That’s a lot of squeeze for no juice. Their big “get” was a big nothing. They tried to bait James into agreeing for a “pay to play” arrangement, and the reporters say … “he didn’t say no.” Really? After two weeks of work, they finally get around to asking their “money question” and that’s it?
There’s a reason James didn’t say “yes”: Because “pay to play” is not what Heartland does, unlike many think tanks on the left. You’re seriously faulting James for steering a (fake) potential donor away from that “gotcha” dishonorable idea into discussing honorable like-minded work with a donor? By that standard, no one in the climate realist movement would ever be able to raise any funds to support our work.
Ask yourself: If these “journalists” had a genuine “smoking gun,” wouldn’t they have presented that instead of that lame “gotcha”?
Third: To compare this lame, days-long caper to the work of Project Veritas is a massive insult to the latter’s fantastic work.
I hope, Pierre, that you will consider giving Heartland some space on your site to more fully rebut for your readers your unfair and uncharitable take on this. Considering all Heartland has done to promote climate realism over many years, I hope you don’t consider that an unreasonable request.
Jim Lakely
Director of Communications
The Heartland Institute”
Share this...FacebookTwitter "
"

Irregardless of our political situation at home, our troops overseas and their families, always deserve the best support we can provide. For the past two years, this event gave comfort to many local families whose loved ones are serving overseas in the cause of freedom. Thanks to a great outpouring of community support, it was hugely successful. This yearâs event promises to be even better.
While the sacrifice of the men and women in our nationâs armed forces is clear, what many in our community donât know is that families of our servicemen and service women suffer even beyond the separation and the worry. Many families lose their breadwinner, and often take a large pay cut in service to our country. Sometimes it takes months for military pay to even be received as training, locations, and deployments change, leaving families to subsist on savings, borrowings, and kindness. Some families find themselves behind in rent, or leaving important bills unpaid while their loved ones selflessly serve our country.
Proceeds of this event will replenish an assistance fund used to help local military families in their time of need.
It also will provide entertainment, moral support, and inspiration. Most importantly it will provide cheer and personal contact via an Internet Video Conferencing System that will allow our troops stationed overseas to see, hear, and interact with their loved ones left behind.
On behalf of our local military families, and the members of the Chico National Guard, I ask for your support.
You can support our troops 3 ways:
1. Buy tickets to the event
2. Make a donation of goods and services for the silent auction
3. Make a cash donation.
Details are below.  Thank you for your consideration. Anthony

Support Our Troops
A Red White and Blue Christmasâ¦The 3rd Annual Northern California Holiday Fundraiser to assist the Chico National Guard Families
â¢ Emceed by Bruce Sessions, KPAY Radio
â¢ Tri Tip dinner with the Soldiers families, catered by Larry Juanarena (Pat and Larryâs Steak House)
â¢ Fundraising Silent Auction
â¢ Guest Speakers – Entertainment, and more.
When and Where:
Chico Elks Lodge, 1705 Manzanita Ave. Chico Tuesday December 19th 2006 6PM -11PM
Tickets are $30 per person
Tickets are available at:
â¢ California Water Service 222 Whitman Ave. Chico (Next to Costco)
â¢ Wittmeier Auto Center, Forest Ave next to Wal-Mart in Chico
â¢ Diamond W. Western Wear 181 E. 2nd, Street, Chico
â¢ Coldwell Banker, 7030 Skyway, Paradise
Donations can be made out to âChico National Guard Family Assistanceâ? and dropped off at California Water Service Company 222 Whitman Ave Chico or at Tri Counties Bank, All Chico Locations.
If you have a donation for the silent auction, they’ll be happy to come by and pick it up. Email Sgt. Douglas Shaw at:
Chicotroops@yahoo.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea98de6e2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Boulder is home to National Institute of Standards and NOAA’s research lab…big government facility and probably the most secure weather station in the USA, I had to go through metal detectors, have mirrors run under my vehicle, be photographed, and my drivers license verified.
Took 2 hours…on the road at the moment to get another station in Colorado, blogging via WiFi from Starbucks
Will post new pix soon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea41b293b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterTwo new papers use tree ring proxy evidence to suggest modern European temperatures are neither unusual nor higher than they were during the Medieval Warm Period.

Image Source: Esper et al. (2020)
Esper et al. (2020) have produced a new temperature reconstruction for Southern Europe to complement past reconstructions for Northern and Central Europe.
They find “the warmest 30-year period since 730 CE occurred during high Medieval times (876–905 CE=+0.78 °C w.r.t. 1961–1990) and has been slightly warmer than the recent period from 1985–2014 (+0.71 °C)“.
The proxy evidence and instrumental record also show there has been no obvious net warming in Southern Europe since the 1940s.
Past reconstructions for Northern and Central Europe also show no unusual warming has occurred over the last century, with as-warm or warmer temperatures during the 1940s.
Ljungqvist et al., 2020  cite tree ring temperature studies from Scandinavia, Scotland, Continental Europe, and the Pyrenees that also show the 1930s and 1940s were as-warm or warmer than recent decades.

Image Source: Ljungqvist et al., 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterTo all the morons out there who think we ought to defund the police and drag them through the mud. How stupid can you be?







<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->









Readers are welcome to add your own videos of police saving lives.
Share this...FacebookTwitter "
"
I’m surveying climate stations of record around California and documenting their condition as part of a larger project I’m doing. You’ll see more about it here in the near future.
Today I visited Marysville’s Fire Station, just off Hwy 70 at 9th and B Street, where they have the station of record for the city using the MMTS electronic sensor installed by the National Weather Service. The data from this station is part of the USHCN (US Historical Climatological Network) and is used in the computer modeling used to predict climate change.
The Marysville station is located behind the fire department building on a patio and is probably the worst site visited so far. In addition to the sensor being surrounded by asphalt and concrete, its also within 10 feet of buildings, and within 8 feet of a large metal cell tower that could be felt reflecting sunlight/heat. And worst of all, air conditioning units on the cell tower electronics buildings vent warm air within 10 feet of the sensor. Oh and lets not forget the portable BBQ the firefighters use a “couple times a week.” The area has been constantly added to, what was once a grass rear yard was turned to a parking lot, then more buildings added, then a cell tower with one, then two electronics buildings and the air conditioners…no report on how long the firefighters were BBQ’ing back there, when they figured out why I was asking all the questions they clammed up.
I can tell you with certainty, the temperature data from this station is useless. Look at the pictures to see why, and is it any wonder the trend for temperature is upward?
 



Above: Vehicles with hot radiators park within 6 feet of the temperature sensor!

Now compare Marysville to Orland, just 50 miles away, where there’s not been any significant change in the last 100 years at the measuring location. Its obvious that Marysville is measuring UHI (Urban Heat Island) effects.

So the question is, how does bad data like this slip into the NASA GISS model database?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea65b785c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Multiple news sites are reporting that 
levels of the second most important greenhouse gas, methane, have stabilized.
From Scientific American: ""During the two decades of measurements, methane
underwent double-digit growth as a constituent of our atmosphere, rising from
1,520 parts per billion by volume (ppbv) in 1978 to 1,767 ppbv in 1998. But the
most recent measurements have revealed that methane levels are barely rising
anymore — and it is unclear why.""
From NewScientist: ""Although tis is good news,  it does not mean that methane levels will not rise again, and that carbon dioxide remains the 800-pound gorilla of climate change.""
Actually, NewScientist is wrong. CO2 is not the biggest ""gorilla"" of
greenhouse gas on planet earth. It’s water vapor. Our earth would be much colder without water vapor in the atmosphere…it would be much like Mars.
So many of the climate models focus solely on CO2, but they leave out water vapor in the equations, or assume its ""static"".
CO2 is far from being the most potent greenhouse gas. Chloroflourocarbons
(CFC’s) commonly used as refrigerants as far worse at trapping infra-red in our
atmosphere.
Of naturally created GHG’s, Methane is 23 times more effective at warming the
atmosphere than CO2. Nitrous Oxide is even worse at 296. So far no emergency
legislation has been authored to eliminate the effect of cows or dental
surgeons. The Kyoto treaty does not address these other gases either.
Global Warming Potentials 
(100 Year Time Horizon) 
GAS GWP 
========================
Carbon dioxide (CO2) 1 
Methane (CH4) 23 
Nitrous oxide (N2O) 296 
Hydrofluorocarbons 
HFC-23 12,000 
HFC-125 3,400 
HFC-134a 1,300 
HFC-143a 4,300 
HFC-152a 120 
HFC-227ea 3,500 
HFC-43-10mee 1,500 
Fully Fluorinated Gases 
SF6 22,200 
CF4 5,700 
C2F6 11,900 
C4F10 8,600 
C6F14 9,000 
The concept of the global warming potential (GWP) was developed to compare the
ability of each greenhouse gas to trap heat in the atmosphere relative to
another gas. In this case, CO2 is the reference gas. Methane, for example, has a
GWP of 23 over a 100-year period. This means that on a kilogram for kilogram
basis, methane is 23 times more potent than CO2 over a 100-year period.
The interesting thing here is that this stabilization of methane levels in
our atmosphere happened all by itself, and the scientists are clearly baffled as
to an explanation. As I’ve always said, the earth’s atmosphere is such a complex
system, that pinning its change on just one thing is not good science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9ceefb1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
It’s been awhile since I updated this series, and its not for lack of material. But I got busy with the UCAR conference, publishing a slide show, and other things. But this morning, über volunteer Don Kostuch sent me a note on his latest survey in Titusville, FL near Cape Canaveral and KSC. I’d like to point out that Don has traveled further and surveyed more stations in the USA than anyone. He is a surveying machine. He wrote this in his email to me:
  “On your scale of 1 to 5, this is an 8. Peace, Don Kostuch”
Ok in the past we have seen stations on rooftops, at sewage treatment plants, over concrete, next to air conditioners, next to diesel generators, with nearby parking, excessive nighttime humidity, and at non-standard observing heights.
Imagine a USHCN station that embraces all of that. I give you the Titusville, FL USHCN station:



Ever thorough, Don also provided photographs of the Climate Reference Network site, just 7 miles east at KSC, which demonstrates the correct environment for measurement of near surface air temperature:

Now I know there will be the usual critics who will jump in and say “This can be adjusted for!”. Ok here is your chance, show me the equations to untangle Titusville’s temperature record from microsite bias. Personally, it looks FUBAR to me.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3cc7da6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Above: Tifton, GA Sewage Treatment Plant – a good place to measure climate?
There have been some claims on the blogosphere of limited or no value to the taking of pictures for the www.surfacestations.org project. This is my view of why pictures are vitally important to an assessment of the accuracy of the near surface temperature record gathered by USHCN and other weather stations, where the data gathered is used in climate studies.
Photography is well established as a diagnostic tool in many fields. Take astronomy for example. If data and computer models of the universe is all that was needed to move the science forward, we certainly wouldn’t need the Hubble Space Telescope.
Do pictures work very well in illustrating problems that need correction?  Well I say ask any doctor who uses xrays, or MRI images, or ultrasound. Do you think doctors can define an illness solely on chart data such as BP and body temperature? No of course not, they need pictures. They DEMAND pictures.
Or how about the NASA’s loss of the space shuttle Columbia in 2004? The spacecraft is covered in sensors, yet after a photo showed foam striking the shuttle during booster burn, engineers pleaded to get photos under the wing from Department of Defense DOD. NASA Engineering made three separate requests for DOD imaging of the shuttle in orbit to get photos to determine if there was damage. NASA management did not honor the requests for DOD photos and in some cases intervened to stop the DOD from assisting.
On reentry, sensors on the shuttle started showing problems, and flight controllers struggled to understand what was happening. Photos and video taken by amateurs on the ground showed clearly what had happened. I don’t recall CNN showing pictures of sensor data in announcing this failure to the world.
Given NASA’s unwillingness to listen to engineers first with Challenger (frost and o-rings) and Columbia (possible wing damage – just get us a picture so we can be sure) I have even less respect for the NASA armchair UHI analysis called “lights = x” ironically done by counting the number of streetlights near weather stations using DOD nighttime photos. This method can give an approximation of the urbanization around a weather station, but it can’t possibly discern the nearby microsite effects like asphalt and air conditioners that have seen so far.
The worlds of science, engineering, medicine, forensics, astronomy, biology, and many more use photos to cross check gathered data or to confirm observations or theory. Climatology shall be no exception.
We are getting pictures of stations, lots of them, and we’ll get every one if possible. Then we are going to analyse them against existing published standards, and then we will publish the results of that analysis. And unlike some prominent climatologists, the pictures, the methods, the code, and the results will be publicly available to anybody, be it scientist, layman, or citizen. And, it will be done without wasting once cent of taxpayer money.
Then after that, critics can determine just how useful the pictures are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea50af6b3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
One of the really odd discoveries that I’ve made while surveying climate monitoring stations around the USA is the fact that many of the official stations are located at sewage treatment plants. For example, the one in Colusa, CA is at their sewage treatment plant. I’ve visited it.
A couple of volunteers for www.surfacestations.org have been going around Washington and Oregon locating stations there and have also reported a number of stations at waste-water treatment facilities. I’ll get to why locating a temperature monitoring station at these facilities is a really bad idea later, but first I want to tell you why many of them are located at these places.
It has to do with the fact that somebody must read the thermometer once a day, write down the max and min temperatures for the last 24 hours in a logbook, then send in the page of the logbook to the National Climatic Data Center (NCDC) once a month. When stations were assigned to cities, they needed to locate them at a place where there was somebody 7 days a week. Sewage is a 24/7 operation. Police and fire stations have some stations for the same reason, somebody is always there.
Ok this picture comes in today from from surfacestations.org volunteer Steve Tiemeier, who visited the climate station of record located at the Urbana, Ohio Waste Water Treatment Plant:

The small item in the center of the picture labeled “MMTS” is the temperature sensor that is used to submit monthly climate reports to NCDC.
Now in case you don’t see some of the obvious problems with this location and why its a terrible place to measure temperature, I’ll list them one by one:
– Sensor is attached to the building, just mere inches away from brickwork
– Sensor is near windows, which radiate heat from heated interior rooms in winter
– Sensor is directly above effluent grates for waste-water, Waste-water is often warmer than the air many months of the year
– Sensor is between three buildings, restricting wind flow
– Sensor is between three buildings, acting as a corner reflector for infrared
– Several exhaust fans near sensor, even though one is disable, there are two more on the walls (silver domes)
– Air conditioner within 35 feet of sensor, enclosed area will tend to trap the exhaust air near sensor
– Sensor is directly over concrete slab
– Refrigeration unit nearby, exhausts air into the enclosed area
– Shadows of all buildings create a valley effect related to sunlight at certain times
– There are two nearby digester pools, which release heat and humidity in the sensor vicinity
– Heat and humidity plume over the site from digesters is often tens of degrees warmer than the air in the wintertime
Here is wider view that shows the temperature sensor in relation to the digester tank:

More picture on my image database here: http://gallery.surfacestations.org/main.php?g2_itemId=5322
I don’t know if any readers of this blog have ever driven by a sewage treatment plant in the winter, in the midwest, as I have, but I can tell you from experience it looks like a hot springs with steam rising into the air.
Talk about your urban heat island effect…not only that, sewage treatment plants effluent volume is a direct indicator of population growth. So as more water is treated, more local effects from the heat/humidity plume occur, which can affect the temperature readings.
There are dozens, possibly hundreds of USHCN climate monitoring stations sited at sewage treatment plants around the USA. I’ll have more reports on this in the future.
Who knew? I’ve been working in meteorology 25 years and I didn’t until this week.
here are some other stations at a sewage treatment plants:
http://gallery.surfacestations.org/main.php?g2_itemId=1489
http://gallery.surfacestations.org/main.php?g2_itemId=4658
http://gallery.surfacestations.org/main.php?g2_itemId=4388


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5d45db9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMonster carbon footprint: Six German researchers fly all the way to Ecuador to study how humans are impacting the earth during the Anthropocene – and do lots of hiking at the expense of the public – on a 17-day “expedition”. Some of them, including a musicologist, are of questionable scientific disciplines. 

Chimborazo in Riobamba, Ecuador, Photo David Torres Costales, CC BY-SA 4.0, via Wikipedia here. 
In spring 2020, six members of Die Junge Akademie from the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina from a range of disciplines – departed on “Expedition Anthropocene”.
“One of the focal points of the expedition,” the website says, “is climate change and its consequences for the environment as well as the transition of a region over the past 200 years.”
“In this, humans are consistently viewed as the instigators, those affected and the observers of these events.”
Lots of hiking
Their research took them to Ecuador and the volcanic mountain Chimborazo. Due to its location close to the equator, the summit of Chimborazo is the highest point on the planet when measured from the Earth’s center.
Impacts from “advancing climate change”
According to the six German researchers, “Together with our local partners, we go in search of traces of human activity in this environment” by using “methods from glaciology, biology, chemistry, acoustic ecology, computer science and medicine, we will investigate the human impact on Chimborazo at different altitudes – from advancing climate change and its consequences for humans, glacial retreat and biodiversity, to acoustic ecological changes and the question of whether microplastics can be detected in the snow and ice.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A junket disguised as science?
But already some are criticizing the expedition. Not only because of the carbon footprint the long travel and extensive accommodation, but because of it has the appearance of a junket disguised as a scientific expedition.
“The first stop on our expedition is Quito, the capital of Ecuador and the world’s highest capital city. We will spend a few days in the city to give ourselves time to acclimatize to the altitude, and set out from here on our first day trips,” the site explains.

Locations to be studied by among others,  a musicology professor. Chart source: Expedition Anthropocene.
The team were planned to hike to the active volcano Pichincha and the inactive volcano Chimborazo – the highest point above Earth’s center – as well as to the Ambato and the Llanganates National Park where they would “spend a few days in a complementary vegetation zone to the mountainous regions of the Andes.”
Not a single climate-related scientist – one musicologist
Of course long hikes to sites are all part of many expeditions and thus perfectly legitimate. But controversy swirls concerning the background of members of the German team.
Many have nothing or little to do with climate science. German science site Die kalte Sonne here noted: “No single geologist, geographer or glaciologist is involved. Instead there is a musicologist, a medical doctor (okay, maybe because of the altitude), a computer scientist and (EVEN!!) a physicist. The physicist comes from the PIK!”
“Six young people simply claimed that they would study the consequences of climate change and without further ado they jetted off to Ecuador. A nice example of a real-life satire: young up-and-coming artists (some of them really artists) are “researching” climate change in South America in Humboldt’s footsteps,” Die kalte Sonne writes.
Share this...FacebookTwitter "
"

Fans of Cato@Liberty may have noticed two new features from the Center for the Study of Science. These are a weekly _Global Science Report_ and a monthly _Current Wisdom_.   
  
  
While the _Wisdom_ has been a monthly feature that can be found under my publications, _World Science Report_ is new and is modelled after my original blog, _Global Climate Report_ , which is the Web’s longest running climate change blog. Our first release was September 11, 1995. The enormous archive at http://​www​.world​cli​matere​port​.com is cross‐​referenced by subject and date, and can provide valuable information on virtually any climate question. We also reserved the right to write in a humorous fashion.   
  
  
As the Center adds new affiliates, you will see much more in the new _World Science Report_ than mere climate.
"
"
Share this...FacebookTwitterIt’s been acknowledged by mainstream scientists for years now that at certain locations on planet Earth, rising carbon dioxide levels cause cooling. It’s now been determined that rising CO2 also causes “negligible” cooling (or warming) depending on the season.
A few years ago a seminal paper (Schmithüsen et al., 2015) was published in Geophysical Research Letters that indicated raising the concentration of CO2 causes a negative greenhouse effect, or cooling, in central Antarctica.
The forcing from the CO2 greenhouse effect ranges from -2.9 W/m² to +1 W/m², and the forcing for the Arctic (central Greenland) is said to be “comparably weak”.

Image Source: Schmithüsen et al., 2015
Now scientists have found that CO2 – to the extent that it has a “negligible” influence on temperature – causes the climate to cool from winter to summer and to warm from summer to winter.

Image Source: Lightfoot and Mamer, 2018
For the most part, CO2 varies due to temperature and water vapour level changes. The variance can range from 403 ppm during the drier winter to 377 ppm during the summer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Lightfoot and Mamer, 2018
Similar seasonal CO2 variability can be found in pristine cave environments.
A paper published earlier this year (Al-Manmi et al., 2019) also finds CO2 rises to 756 ppm in winter but drops to 484 ppm in summer.
So observations indicate higher CO2 concentrations are linked to cooler temperatures, not warmer temperatures.

Image Source: Al-Manmi et al., 2019
Nowhere do these observations support the paradigm that says real-world temperature (and water vapour) changes are driven by variations in atmospheric CO2 concentrations.
If anything, it’s the other way around.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn an interview with Punkt.Preradovic, finance Prof. Dr. Stefan Homburg of the Leibniz University of Hanover said Germany’s lockdown has “amounted to nothing”, has had no effect on the spread of the corona virus and that the spread had already slowed down below a reproduction number of 1.0 before the lockdown.
Citing data from Robert Koch Institute (RKI) 
In the interview, the prominent professor, once an adviser to former chancellor Gerhard Schröder, cited a chart from the Robert Koch Institute (RKI) that was issued on April 15th:

As the RKI chart shows, in early March the reproduction number had risen rapidly before reaching a peak on about March 10. By March 21st, the reproduction number dropped below 1.0.
“Ineffective”, “completely unnecessary”
It wasn’t until March 23 that the German government decreed a lockdown. As the chart shows, since the lockdown was enacted, the reproduction number did not change at all. It’s had no effect.
“It is not the case that the reproduction number went down after the lockdown”, Professor Homburg says. “There are two points we can draw from this: First, the lockdown was not necessary because the number was below 1, and secondly, the lockdown was not effective because the number didn’t drop afterwards.”
“Enormous economic damage”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Homburg agrees that the lockdown led to “enormous economic damage” and was “completely unnecessary”. In view of the data, Homburg does not know why the lockdown continues even today. Currently the reproduction rate stands at 0.7.
Homburg tells Preradovic that the politicians issued the lockdown in panic, came too late and thus so served no purpose. “It was not only unbelievably damaging for the economy, but also for other human factors. It’s about suicides and delayed operations.”
Panic fanned by absurd numbers
Citing the RKO numbers, Homburg also says: “There is not going to be any terrible epidemic. All the panic was fanned by the Robert Koch Institute, who said on March 20th that in the best case we will see 300,000 dead, and maybe 1.5 million dead.” Today the number is well under 5000.
One single alarmist paper
Homburg explains that the origin of the alarming death projections were adopted by the RKI from one single alarmist paper and that “it’s unbelievable that the government allowed itself to be so misled.”
Financial ruin for no reason at all
On the overall situation, Homburg comments: “It’s completely unimaginable. Huge damage is being caused. People and businesses are being financially ruined without any reason at all.”
On why it’s happening, Homburg says that German politicians, such as Angela Merkel, have gotten full cover from the media, and today enjoy high approval ratings because of the appearance of being competent crisis managers. And as long as the polls remain so, Homburg says, there’s little incentive for politicians to relax the lockdown.
Trump may wish to take a look at what’s happening in Germany, and move fast to end the destructive lockdown in USA. 
Share this...FacebookTwitter "
"

On Tuesday the Supreme Court will hear arguments in _South Dakota v. Wayfair,_ which will force it to decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state. Should it effectively reverse a pair of previous Supreme Court decisions and permit states to do such a thing, it will constitute a significant change in our economy—but those changes won’t include rescuing embattled terrestrial retailers or filling states’ coffers with new tax revenue.



The main outcome will be slower economic growth.



The Supreme Court’s 1992 ruling in _Quill v. North Dakota_ imposed a prohibition on states taxing the sales of remote retailers that exists to this day. The Court found at the time that it was impossibly complex for a remote retailer to know or compute the sales tax owed in thousands of different jurisdictions, and determined that if a retailer had no operations–such as a store or warehouse—in a state, then it did not make sense for it to pay taxes on sales made in that state.



The timing of the _Quill_ case proved to be propitious: Less than two years after its decision Jeff Bezos introduced Amazon to the world. Today, of course, internet retail is enormous, and many of the terrestrial retailers that are struggling these days blame the company for their woes.





The Supreme Court will decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state.



The states have been enthusiastic in having the court—or Congress—end that ban as well: They have blamed the expansion of internet retail for everything from their budget woes to climate change.



However, the notion that states will reap a revenue bonanza by taxing remote retailers is as dubious as the Glengarry leads. Every estimate that has previously been made of the revenue to be gained from such a tax vastly overstated reality, simply because most sales on the internet are already taxed (roughly two thirds, according to a recent study) and internet sales still comprise a small fraction of all retail sales—roughly 11 percent in 2016.



The Government Accountability Office estimates that ending _Quill_ would raise an additional $8 to $13 billion for the states, or roughly 1–2 percent of the $700 billion of revenue the states anticipate collecting in 2018. Or, put in a different context, it roughly amounts to Walmart’s annual tax bill.



And that revenue comes at a significant opportunity cost to the economy. A plethora of research over the last two decades has found that it has been the hyper‐​competitive retail economy of the U.S. that has driven much of the productivity gains that the country has achieved since the mid‐​1990s. Both Walmart and–next–Amazon have ruthlessly pursued methods to reduce costs and increase worker productivity, and they have forced their suppliers to follow suit.



Productivity growth is important because it ultimately determines a nation’s standard of living, economists believe, and the retail sector plays an outsized role in its determination. William Lewis, the founding director of the McKinsey Global Institute, and former Obama CEA chair Jason Furman have both concluded that the nation’s competitive retail sector distinguished us from most other developed countries, and that this is an important reason–if not _the_ main reason–that we have had greater productivity growth the last three or four decades.



Right now, Amazon is winning that productivity race hands down–its sales per employee exceeds $100,000, or twice that of Walmart. It is likely that as Amazon expands its terrestrial offerings and Walmart beefs up its online presence that this gap will shrink, but no one thinks it will disappear anytime soon.



If Amazon can withstand any competitive threat from Walmart, Target, or other big‐​box retail stores, then who _is_ the next retailer who has a chance of beating it at its own game? We can safely venture that it will originate as an internet retailer, much like Amazon did.



However, taxing remote retail constitutes a barrier to future internet retail startups. Amazon undoubtedly benefited in its early years from not having to pay sales taxes in most states when it was competing remotely from a relatively small number of distribution centers and faced more difficult shipping and return logistics, and that ultimately came to benefit consumers–that advantage helped Amazon to hasten the internet marketplace.



Today, Amazon now has a presence in all fifty states in the form of its logistics network and therefore pays sales taxes in each state that has one—to provide the level of service it feels compelled to offer in order to compete against Walmart and Target it has to have warehouses and stores and pickup facilities near its customers.



The retail sector of the economy has, of course, changed dramatically in the last quarter century and in ways that no one anticipated then, or even a decade later. Until the last decade or so retailers were, generally, either solely remote retailers or else sold their wares only via their stores. Today, almost all retail sales are completed by an entity that is an omnichannel retailer, with both an internet presence as well as one or more physical stores.



The notion—implicit in the plaintiffs’ argument—that there are distinct internet and distinct non‐​internet retailers does not at all reflect the reality today. Most existing internet‐​only retailers are small mom and pop businesses selling goods that aren’t readily found elsewhere–a chia pet in the shape of Jerry Garcia’s beard or vintage Buffalo Braves shirts, for instance. A sales tax on these operators reduces the breadth of goods available to consumers without increasing demand for anything from their terrestrial “competitors.”



And for all of the bluster and talk of Amazon acting like a monopoly, it is worth remembering that its annual sales are only one third that of Walmart and behind CVS as well, and it is unclear whether its retail operations—which it doesn’t break out from its other operations—have ever turned a profit for the company; its most lucrative division appears to be its cloud services.



The history of retail shows that a company can dominate the field for only a short period of time. Before Walmart and Amazon there was Sears, Kmart, JC Penney, Montgomery Ward, and Woolworths, each of which innovated the retail environment in some way and became the top dog in the market before being supplanted by a competitor. Today, these companies are defunct or nearly so.



It is a history that Bezos is keenly aware of: He constantly refers to his company being at Day One of its history, explaining that his goal is to forever maintain that same competitive culture so that it can resist the new competitors that will inevitably arise. It is a noble goal but an impossible one.



But one way Bezos can put off its future rivals is to make it more difficult for small companies to grow—and imposing a retail sales tax on even the smallest remote retailer presents a significant barrier for retail start ups. That is undoubtedly the reason why Amazon now _advocates_ for a sales tax on all internet retail.



Imposing an internet sales tax on remote sales won’t make us like France, which carefully circumscribes the retailers’ hours, prices, number of sales, and other behaviors in order to keep at bay the nonexistent bogeyman of unfair competition, but it’s a step in that direction.



State governments with a fiscal problem—and there are many, despite the fact that we are in the ninth year of an economic expansion and unemployment rates are nearing record lows—agitate for the right to tax out of state retailers because they need to place blame somewhere besides their profligacy and inability to coherently govern. Taxing out‐​of‐​state retail sales won’t fix anything–and it will hurt the economy to boot.



Despite the ancient principle of _stare decisis_ , the betting seems to be that this court will indeed rule with the plaintiffs and effectively overturn _Quill._ If so, the U.S. economy would be worse off for it in the long run.
"
"
Share this...FacebookTwitterOne highly visible person often seen at Greta Thunberg’s side at public Fridays For Future appearances is German 23-year old climate activist Luisa Neubauer of Hamburg.

Climate activist Luisa Neubauer at the center of infighting between FFF organizers. Image: Andol – own work, CC BY-SA 4.0
The Green Party member was a driving force behind the launch of the Fridays For Future movement in Germany and Europe. However, it appears other FFF organizers have had enough of Neubauer constantly hogging the spotlight amid signs of growing discord and infighting among FFF organizers.
MOPO.de here reports how Neubauer has been “disqualified as permanent speaker” by FFF organizers and “will also not speak at the controversial citizens’ meeting in Berlin in June.”
Moreover, according to MOPO: “Neubauer already was replaced” by 17-year-old schoolgirl Helena Marshall at the Siemens Annual Shareholders’ Meeting on February 5.
Jet-setting Greenie “Longhaul Luisa”
The problem, MOPO writes, is that “Neubauer is too much in the public eye” and that she may not be “suitable as a representative” because it was reported how she had flown often. Neubauer was recently dubbed “Longhaul Luisa” after having posted images of herself on foreign trips in social media.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Over the last years, climate activist Luisa made it around the world by plane several times, via Austria, Switzerland, Italy, Belgium, the Netherlands, Sweden, Poland, England, Scotland, France, Canada, China, Hong Kong, Nepal, Morocco, Namibia, Tanzania, Indonesia, etc.
Once in a talk show, Luisa was asked what she personally would do to protect the climate. She answered: “Fly as little as possible”.
Open discord between Greta and Luisa
Evidence of open discord among the FFF movement organizers also surfaced in a video showing Greta Thunberg expressing her displeasure at comments made by Neubauer during the World Economic Forum earlier this year.

Strife is undeniable as Greta openly shakes her head as Neubauer speaks.
MOPO reports the “first trouble” began “already at the beginning of spring 2019” when co-organizers felt “the distribution of speeches and appearances was too one-sided.”
Greta skips France
Today Greta tweeted she was skipping appearances in the marches in Grenoble and Paris this Friday, citing “family reasons”.


		jQuery(document).ready(function(){
			jQuery('#dd_2eb4fb5288a5586c6e0d291efb22ada4').on('change', function() {
			  jQuery('#amount_2eb4fb5288a5586c6e0d291efb22ada4').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterUnusual cold and snow are expected to sweep across North America and Europe over the coming days, thus threatening crops. 
Not that mid May is approaching, global warming alarmists tell us we should already be expecting heat waves. But right now the opposite is in the forecast: snow and extreme cold! Who would have thought?
Snow for Boston and new York?
Meteorologist Dr. Ryan Maue tweeted that both New York City and Boston might see snow on Saturday as a “rare & powerful May ‘bomb cyclone’ Nor’easter” is projected to develop off the east coast.
Over a foot of snow might fall in Maine, Maue asserts:

While snow won't stick on the ground for long, if at all, parts of Maine might see more significant accumulations over a foot. ❄️
Flurries for NYC and Boston perhaps on Saturday as rare & powerful May ""bomb cyclone"" Nor'easter develops off New England coast. pic.twitter.com/aOHT4APBfp
— Ryan Maue (@RyanMaue) May 7, 2020



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Maue earlier had tweeted that the snow and cold could act as a “triple whammy” because “hard freezes limit agriculture, forces people to remain indoors w/central heating & provides outdoor environment favorable for coronavirus.”
Snow to blanket parts of northern/central Germany 
Not only the Northeast has to worry about winter striking so late in the season, but also an intense cold front will be sweeping across a vast swath of northern Europe, reports German weather site daswetter.com here.
One can also call it an unusual cold snap for almost mid-May. The air in the north will warm up to only 7 to 12 degrees. […] With the polar air, the temperatures will plunge, and so will the snowfall line. When the cold air reaches the south in the night from Monday, it will snow slowly until the middle of the day. Around 400 to 600 m, up to 10 cm of fresh snow is possible in the middle of May. Even up to 300 m wet snow can fall.”
“Five to 6 nights of frost warnings” in UK
At Twitter David Birch tweeted a GIF animation showing the projected movement of the cold front, writing that Britain might see 5 or 6 nights of frost warnings next week:

Likely the UK could see 5/6 consecutive frost warnings throughout next week. pic.twitter.com/paLeD4vvOK
— DavidIBirch 🇬🇧 (@dbirch214) May 7, 2020



		jQuery(document).ready(function(){
			jQuery('#dd_91b09e226f3683c039c0d47c6040e31f').on('change', function() {
			  jQuery('#amount_91b09e226f3683c039c0d47c6040e31f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe onslaught of paleoclimate evidence for warmer-than-now Mid-Holocene climates – when the Earth’s sea levels were meters higher than they are today –  stormed through 2019.
There were 107 scientific papers published this past year indicating today’s warmth isn’t even close to being unusual or unprecedented when compared to the climates of the last centuries to millennia.
As illustrated below, there were also 19 papers affirming today’s sea levels are among the lowest of the last ~8000 years.
This is added to the list of nearly 100 scientific papers published in the last handful of years indicating Mid-Holocene sea levels were multiple meters higher than they are today due to the much more extensive glacier and ice sheet melt occuring during these millennia.

Oliver and Terry, 2019  Thailand, +2.0 to 3.8 m higher than present
“~6000 cal yr B.P. old oysters can be found from between 3.8 ± 0.1 m to 2.5 ± 0.1 m above present day mean sea level. … Dead (fossil) oysters were collected from between 1 and 3 m above the centre of the live oyster band in a more sheltered cleft inside the notch. The oldest sample with an age of 5270–4950 cal yr B.P. was collected at an elevation of 3.01 ± 0.1 m above the apex of the notch. The ages decrease with elevation down to 920–710 cal yr B.P. at 1.03 m. … In all the sites, the 14C age of the dead oysters inside the notches increases with increasing elevation above present day MSL. Clearly, relative sea level was 2 to 3 m higher than present between 6000 and 3000 B.P. and has steadily fallen since.”


Brooke et al., 2019  Queensland (NE Australia), +1-2 m higher than present
“Indicator data for Queensland have been assessed for their accuracy and robustness by Lambeck et al. (2014), who identified a number of coastal and inner shelf island sites in the northeastern region, in which Cowley Beach is located (Fig. 1), where accurately dated in situ fossil coral, coral microatolls and sediment core samples provide robust sea-level records (Chappell, 1983; Chappell et al., 1983; Horton et al., 2007; Yu and Zhao, 2010; Zwartz, 1995; Fig. 3). Here, relative sea level reached a Holocene highstand between 6770 and 5520 yr BP approximately 1–2 m above the present level (Lewis et al., 2013; Fig. 3). Following the highstand, the data record a gradual fall in sea level to the present position (Perry and Smithers, 2011; Lambeck et al., 2014). … Local and regional records for the Holocene at far-field sites may also reflect the influence of climatic variations on sea level, such as shifts in the El Nino Southern Oscillation (ENSO), that can induce minor (<0.5 m) changes in sea level (Duke et al., 2017; Leonard et al., 2018; Sloss et al., 2018) on annual to multi-decadal, rather than millennial, timescales.”

Yamano et al., 2019 SW Japan, +1.1 to 1.2 m higher than present
“Evidence from the core samples and fossil microatolls suggests sea level reached its present position before 5100 cal yr B.P., and a relative sea-level highstand of 1.1–1.2 m above the present sea level occurred from 5100 to 3600 cal yr B.P. This was followed by a gradual fall in relative sea level. The tectonically corrected sea-level curve indicates a stable sea level after 5100 cal yr BP., with a sea-level highstand of up to 0.4 m between 5100 and 3600 cal yr B.P.”
Makwana et al., 2019 Western India, +2 to 3 m higher than present
“The BB trench site is located at an elevation of 2 m above present day msl, where it shows evidences of dominant marine processes at depth of 2 m with a horizon of clay at depth of 3.2 m. In coastal environments, clayey horizons get deposited in calmer and non turbid conditions with depth > 3 m, which explains the clay horizon at BB trench site that would have been deposited with the water level depth of 3.2 m at > 2.5 ka period.”

Loveson and Nigam, 2019 Eastern India, +4 m higher than present 
“The continuous rise in sea level ever since late Pleistocene has reached the present sea level during 6800 years 100 BP and the highest sea level of about ~4m above the present sea level is observed during 6050 BP. Since then, the sea level started fluctuating in lesser magnitudes (between +4.0m to -2.0m), responding to the cycles of global ice melting and climate thereof. … It is also observed that the magnitude of all five high stands in between 7,200 to the recent has a decreasing trend from +4m to 0m. It obviously indicates that the most of the present day coastal plains were once under the sea as evidenced by the presence of many inland leftover paleo delta signatures in the East Coast of India.”

Oliver et al., 2019  South Australia, +2 m higher than present
“Raised beach strata imaged with Ground Penetrating Radar (GPR) at Rivoli Bay suggest a sea-level highstand of +2 m above present ~3500 years ago, steadily falling and reaching the present ~1000 years ago.”
Kylander et al., 2019  Scotland, +9 m higher than present
“At present, the Laphroaig bog is edged by a dune system, but this sand source may have looked very different at the time peat accumulation started 6670 cal. a BP. A primary control on dune building is RSL. Glacial isostatic modelling, supported by radiocarbon-dated sea-level index points, show that the RSL on Islay was about 9 m higher at 6000 cal. a BP, and fell in a linear fashion to 2.2 m higher than present at 2000–1000 cal. a BP (Fig. 7C;Dawsonet al. 1998; Shennan et al. 2006a,b).”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Meeder and Harlem, 2019  Southeast Florida (USA), +1-1.3 m higher than present
“Sea level was at ca 8 m above present during the last interglacial ca 120,000 yr bp inundating the entire platform during deposition of the Miami Limestone strata (Moore, 1982) …  The marls form a leaky seal on the Everglades floor (Figure 14B) slowing water infiltration and storing water, increasing the hydroperiod and providing an environment suitable for peat deposition which started ca 4,500 yr bp (Gleason & Stone, 1994) at elevations between 1 and 1.3 m above present sea level (Wanless et al., 1994). … The historic high‐water stage occurred prior to drainage when the water stage was between 0.6 and 2  m higher than present in the study area (McVoy et al., 2011; Parker, 1975; Parker et al., 1955).”
Cuttler et al., 2019  Western Australia, +1-2 m higher than present
“Ningaloo Reef grew over the last ~8,000 years (Twiggs and Collins, 2010) with rapid reef build up ceasing ~5.8 ka BP when sea level was approximately 1 to 2 m higher than present. During this phase of development, benthic cover was dominated by reef-building corals (Collins et al., 2003; Twiggs and Collins, 2010). After this sea level highstand, reef evolution at Ningaloo was characterised as ‘detrital build-up and aggradational’ as sea level fell to present levels and the reef back-stepped (seaward) to its present location (Twiggs and Collins, 2010).”
Bondevik et al., 2019 Western Norway, +8.2 to +9 m higher than present
“We conclude that the maximum sea level of the Tapes transgression lasted 2000 years from 7600 cal yr BP and extended into the Early Neolithic, to about 5600 cal yr BP (Fig. 13), with an uncertainty of about 100 years. We estimate that the highest spring tide during the Tapes transgression maximum phase was between 8.2 and 9.0 m above the present mean sea level. … To account for additional uncertainties, we suggest that the spring tide sea level at Longva would have been 8.6 ± 0.4 m above present day mean sea level during the Tapes transgression maximum.”

Yamada et al., 2019   Japan, +1 m higher than present
“Post-glacial sea level reached about 1 m higher than today around 6000 years ago and then started to fall (Yokoyama et al., 1996). As such, a sudden appearance and increase of marine and brackish diatoms just below PL-b cannot be explained by eustatic sea-level change.”
Montaggioni et al., 2019 French Polynesia, +0.8 m higher than present
“The foundations of islets (motus), namely conglomerate platforms, started to form with deposition of patchy, rubble spreads over the upper reef-rim surfaces from ca 4,500 yr BP as sea level was about 0.80 m above its present mean level. On these platforms, islets started to accrete not before ca 2,300 yr BP, from isolated depocentres located midway between outer-reef and lagoon margins. At that time, sea level at about +0.60 m above present mean sea level was starting to slowly decrease to its present position.”
Brouwers et al., 2019  Dubai, +1.6 to 2.5 m higher than present
“During Pleistocene glaciations, global sea level was 100–120 m below the present level and resulted in most of the Arabian Gulf occurring as a dry basin (Purser 1973; Gunatilaka 1986) … Since late Pleistocene to early Holocene times, the sea level rose gradually until a maximum sea level stand 1.6– 2.5 m higher than today (Gunatilaka 1986).”
Haryono et al., 2019  Indonesia, +4.5 to 6 m higher than present
“[I]n 5000 BP, sea level increases up to +5 m from the present time; it means it was warmer than the present day. … Sealevel change started in 6,000 BP and rose to reach the highest sea level in 4,500-3,600 BP as +4.5 m above present sea level. Then moderate sea level lasted for 600-700 years until 2,200 BP reached +2.8 m. Low sea level peak occurred in 3,000 BP (+4.5 m above present sea level). Meanwhile, present sea level is lower than sea level peak during the middle period, that reached 2m above mean sea level. … Marine terrace also found in +6 m above present sea level.”

Williams et al., 2019 North Vietnam, +2 to 4 m higher than present
“A freshwater coastal marsh near the mouth of the Cam River in Northern Vietnam stands 2–3 m above mean sea level and is bordered by a coastal barrier that reaches about 6 m above mean sea level. A core from the marsh contains a 14-cm-thick sand and shell layer. The presence of abundant shell fragments suggests inland transport of littoral sediment, and the sand layer is tentatively identified as a washover deposit. The coast of the study area contains a beachrock standing above the modern beach and reaching to ∼4 m above mean sea level. A tentative explanation of this beachrock is that it represents a beach that formed during a mid-Holocene 2–3-m highstand, evidence for which has been reported from Thailand, Malaysia, Singapore, and Vietnam.”
Rivers et al., 2019  Northern Qatar, +1.6 m higher than present
“The Al Ruwais area of northern Qatar has been the site of shallow water carbonate sedimentation since the mid-Holocene. Two distinct depositional packages have been identified. Between ca 7000 and 1400 years ago, when sea-level was up to 1.6 m higher than today, a barrier/back-barrier system was active in an area immediately landward of the modern shoreline. During the same period, a laterally-continuous coral reef flourished in the open waters approximately 3 km to the north. Towards the end of this period sea-level fell to its current position, and the reefal system died, perhaps due to exposure or the influx of detrital sediment. Between 1400 and 800 years ago a new barrier island was established directly on top of the moribund reef, and the old barrier to the south was exposed to the meteoric realm. Over the past ca 800 years the new barrier has retreated landward as much as 1 km to its current position.”

Fachbereich, 2019  Antarctic Peninsula, +14.5 to 16 m higher than present
“Raised beaches along the coasts of Maxwell Bay, located at 7.5 to 4 m amsl (locally termed “6-m-beaches”), interfinger with terminal moraines of the last glacial-readvance (LGR), which occurred between 0.45 and 0.25 ka cal BP (John and Sugden, 1971; Sugden and John, 1973; Clapperton and Sugden, 1988; Yoon et al., 2004; Yoo et al., 2009; Simms et al., 2012). It is therefore likely that these beaches developed during the LGR (John and Sugden, 1971; Sugden and John, 1973; Hall 2010). Recent uplift of KGI was 0.4 mm a-1 during the last decade (Rülke et al., 2015). Average uplift during the entire Holocene, however, is 2.8 to 3 mm a-1 (Bentley et al., 2005; Fretwell et al., 2010). Fall of relative sea level on KGI accelerated during the last 500 years (Bentley et al., 2005, Hall, 2010; Watcham et al., 2011). This was most likely the result of a short-term acceleration in glacio-isostatic rebound after the LGR, with a modeled peak uplift rate of 12.5 mm a-1 between 1700 and 1840 CE (Simms et al., 2012). …  Bentley et al. (2005) show that an initial post-glacial sea-level fall was interrupted by a mid-Holocene highstand at about 14.5 to 16 m amsl from 5.8 to 3.0 ka cal BP. In contrast, data presented by Hall (2010) show a continuous sealevel fall, which becomes accelerated between 1.5 and 0.5 ka cal BP.”
Nirgi et al., 2019  Baltic Sea, +10 m higher than present (rate: +3.5 meters per century)
“Considering the elevations of the pre-Ancylus Lake palaeochannel sediments in the Pärnu site and the highest coastal landforms in the area, the water level rose at least 17.5 m at an average rate of 35 mm per year, which is 5–6 m more than proposed by earlier studies in this area (Rosentau et al., 2011; Veski et al., 2005). Similar fast transgression (40 mm/yr), about 21–22 m, has been documented inthe Blekinge area between 10.8 and 10.3 cal. ka BP (Hansson et al., 2018a). … At about 8.2–7.8 cal. Ka BP, the rising Litorina Sea flooded the palaeochannel in the Pärnu site and floodplain in Reiu at an elevation of 1–2 m b.s.l., around 7.6–7.8 cal. ka BP Rannametsa site at an elevation of 4 m a.s.l. and around 7.6–7.4 cal. ka BP Sindi BOM layer at an elevation of 7 m a.s.l. (Figure 7). The Litorina Sea reached its maximum transgressional RSL ca. 10 m a.s.l. [meters above present sea level] just after 7.6 cal. ka BP, most probably around 7.3 cal. ka BP (Veski et al., 2005), as also determined in Narva-Luga region at the south-eastern coast of Gulf of Finland (Rosentau et al., 2013). Thus, during the transgression, the sea level rose by about 14 m at an average rate of 12 mm per year.”

Rasmussen et al., 2019  Denmark, +3 m higher than present
“Full marine phase (c. 7700–3700 cal. a BP). – The appearance of a high salinity demanding fauna in this phase (several mollusc species, echinoids and Quinqueloculina seminulum) indicates a change to full marine conditions (Figs 4, 11). This marked environmental change coincides with a rapid and significant sea-level rise documented in both the Danish and the Baltic area dated to around 7600 cal. a BP (Fig. 11; Morner 1969; Christensen 1995, 1997; Yu et al. 2007; Lampe et al. 2011; Sander et al. 2015) and probably of global extent related to the so-called ‘global meltwater pulse 3’ documented in Caribbean-Atlantic coral sea-level records c. 7600 cal. a BP (Blanchon & Shaw 1995; Blanchon et al. 2002; Bird et al. 2010; Blanchon 2011a,b). Based on data from a recent study on the island of Samsø in the central Kattegat, Sander et al. (2015) estimated a relative sea-level rise of ~4.5 m between 7600 and 7200 cal. a BP. A high sea level in Aarhus Bay at this stage is supported by an almost complete absence of terrestrial plant macrofossils (Fig. 5) testifying to an increased distance between the core site and the shore. … In the period of greatly increased sedimentation (c. 7700–6300 cal. a BP), the average rate is ~2.8 mm a1 (Fig. 11). The extensive coastal erosion during this sea-level highstand period is manifested in today’s landscape in the form of numerous fossil coastal cliffs situated above present-day sea level that formed during the Mid-Holocene when the relative sea level was ~3 m higher than present along the coasts of the Aarhus Bay area (Mertz 1924). … In a study of the island of Anholt in the central part of the Kattegat, the drop in absolute sea level was estimated to 2.6 m over a 700-year period between 4300 and 3600 cal. a BP (with most of the sea-level fall taking place between 4250 and 3740 cal. aBP; Clemmensenet al. 2012).”
Share this...FacebookTwitter "
"

Last week, the _New York Times_ delivered the worrisome news that a team of scientists has concluded that maximum hurricane winds will increase 6 percent by the 2080’s, thanks to global warming. I was very upset to read that news, but not because I’m afraid my great‐​grandchildren will get blown away. My concern is what those scientists’ work says about the state of climate science.



The researchers reached their conclusions using a series of climate models called General Circulation Models. They assumed that the concentration of atmospheric carbon dioxide–the main global warming gas–will increase by 1 percent per year, compounded yearly. That would warm the ocean, which would create slightly stronger storms.



But there’s a problem: Any atmospheric scientist who is worth his or her salt knows that atmospheric carbon dioxide is not increasing at that rate and has not been doing so for decades. And that makes a real difference in the modeling results.



The increase has been about four‐​tenths of a percent per year, averaged over the last 30 years–not 1 percent. Charitably, throw in another tenth of a percent because of other human “greenhouse” emissions (though the two major ones, chlorofluorocarbons and methane, are declining or holding steady). That means that the researchers’ models are envisioning twice the actual increase in carbon dioxide as has been occurring for decades. 



The reason that carbon dioxide is growing so slowly is because the world is gradually becoming more energy‐​efficient as its people become more affluent. That results in both a reduction in per‐​capita emissions and a reduction in the number of “capits” that are born, as rich folks have fewer kids. Among big countries, this trend started in the United States. It is now spreading globally as the enriching world buys more‐​efficient cars and power plants.



This trend isn’t going to change anytime soon. That means the growth rate in carbon dioxide over the next few decades is likely to be the same as the rate for the last few. Using the more realistic rate delays the time that hurricane winds will increase by 6 percent from the 2080’s to the 2180’s–175 years from now.



And it’s pretty hard to speculate what impact humanity will have on nature over nearly two centuries in time. To understand that, let’s go backwards in time 175 years, to 1830, and think about the changes in energy and technology that have occurred since then.



The fact is that, just as folks in 1830 could not possibly imagine the many technological changes we have today (cars, planes, rockets, nuclear bombs, computers and Viagra come to mind), so can we have absolutely no vision of 2185. The only reasonable bet is that it will be dramatically different than today, and our fossil fuel‐​powered society will seem as remote in the future as one driven by horses and slavery seems remote to us today. So why would anyone make a prediction of what effects humanity will have on the environment some two centuries from now, based on what we’re doing today?



Or, in the case of the researchers’ exaggerated percentage increase in carbon dioxide, what we’re not doing today? That leads to an interesting question: Because carbon dioxide increases have been bouncing around four‐​tenths of a percent per year for three decades, why do climate modelers insist on using the wrong number? It seems peculiar that people who have the equivalent of doctorates in applied physics (which is what climate science is) would somehow be perfectly happy to do something they know is wrong.



I began asking that question at scientific meetings a decade ago. At that time, I asked Kevin Trenberth, a highly visible atmospheric scientist from the U.S. National Center for Atmospheric Research, who often testifies to Congress on climate issues. He told me it was done because it was “convention.” 



That answer doesn’t set well with me, because it’s awfully easy to program a computer to increase a variable by half a percent instead of 1 percent per year.



That leads to the final, nagging question. There are literally hundreds of scientific papers out there in which climate models use this wrong number. Each of those papers gets sent to three outside peer‐​reviewers. The fact that 1 percent continues to be used only means one thing: when it comes to global warming, hundreds of scientists must prefer convention to truth.



But why? Is it because, when the real numbers are put in, there’s no story for the _New York Times_ to report? 
"
"

The picture above is of the official USHCN climate station of record in Quitman, GA and comes to me via www.surfacestations.org volunteer Joel McDade.
It is located at a residence, the observer has consented to having this NOAA weather equipment at his home.
Besides the usual problematic close-by parking of vehicles that we’ve seen before, and buildings less than 100 feet from the temperature sensor, we have a new issue to contend with: inoperable vehicles and abandoned appliances near the temperature sensor. Such big chunks of metal have thermal retention, which means that heat is retained past sunset and re-radiated near the sensor. This may bias overnight lows.
I thought the old washing machine was a nice touch though. It illustrates how little quality control of the temperature measuring environment is being done with the US Historical Climatological Network.
Additional pictures of the site are available at the surfacestations.org online database.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5b237d1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I found the full page color advertisement on page 5E of the Sunday Enterprise Record quite interesting.
It lists a number of environmental reasons why the M&T Baldwin Gravel mine would be a good thing, not the least of which is the reduction of the number of truck miles traveled in Butte County due to trucking in building gravel from outside the county, and the reduction in gasoline burned and GHG’s avoided helping “Global Warming”.
And then there’s the angle that this mine pit will fill with water, and create an animal habitat just like the Teichert Ponds have done when it was used as a borrow pit to construct Highway 99 overpasses. There we have a clear example of how a lowly gravel mine got turned into a nature habitat, and there was no help or “kickstart” to nature as the M&T operators are proposing for their pits destined to be ponds.

It will be interesting to see how opponents argue against the project with these environmental assets it offers.
Here’s how Chico Creek Nature Center described the Teichert Ponds for a walking tour they sponsored of them:

April 8, Sunday – Teichert Pond/Birding By Ear – Trip co-leaders: Scott Huber and Dawn Garcia. Time: TBD. Chico’s Hidden Wetland – the view from Rte 99 is enticing; a large pond surrounded by tules and ringed with willows and oaks. Trip leader, Scott Huber, will direct you through the maze of streets that lead to the heart of Teichert Pond(s). Once in, you\’ll delight in the diversity of avian life found in this \’secret wetland\’ just blocks from downtown Chico. Co-leader Dawn Garcia, an expert at identifying local bird species by ear, will point out audio clues for ID\’ing species seen and perhaps some that are only heard! Expect at least three woodpecker species, a number of flycatchers, numerous sparrow species, a few raptors (possibly a Great Horned Owl), at least three warbler species, some ducks, geese and shorebirds and with any luck, some surprise migrants! Consider picking up one of the great “birding by ear” CD sets to prepare you for this trip: Bird Songs of California (Keller – Cornell Lab of Ornithology) or Western Birding by Ear (Peterson Field Guides). 
So what’s all the fuss about over this gravel mine?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea687dd57',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

That’s the topic of my _Washington Examiner_ column this week. In it, I discuss last week’s budget battle and the failure of “policy riders” designed to rein in the Obama EPA’s attempts to regulate greenhouse gases without a congressional vote specifically authorizing it. The Obama team believes it has the authority to implement comprehensive climate change regulation, Congress be damned. Worse still, under current constitutional law–which has little to do with the actual Constitution–they’re probably right. Thanks to overbroad congressional delegation, “the Imperial Presidency Comes in Green, Too.” At home and abroad, the legislative branch sits on the sidelines as the executive state makes the law and wages war, despite the fact that “all legislative powers” the Constitution grants are vested in Congress, among them the power “to declare War.”   
  
  
Yet, as I point out in the column, Congress retains every power the Constitution gave it–powers broad enough that talk of “co‐​equal branches” is a misnomer. Excerpt: 



The constitutional scholar Charles Black once commented, “My classes think I am trying to be funny when I say that, by simple majorities,” Congress could shrink the White House staff to one secretary, and that, with a two‐​thirds vote, “Congress could put the White House up at auction.” (I sometimes find myself wishing they would.)   
  
  
But Professor Black wasn’t trying to be funny: it’s in Congress’s power to do that. And if Congress can sell the White House, surely it can defund an illegal war and rein in a runaway bureaucracy.   
  
  
If they don’t, it’s because they like the current system. And why wouldn’t they? It lets them take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the bureaucracy when it imposes costs in the course of deciding what those statutes mean.



Last year, in the journal _White House Studies_ [.pdf], I explored some of the reasons we’ve drifted so far from the original design: 



_Federalist_ 51 envisions a constitutional balance of power reinforced by the connection   
between “the interests of the man and the constitutional rights of the place.” Yet, as NYU‘s Daryl Levinson notes, ―beyond the vague suggestion of a psychological identification between official and institution, Madison failed to offer any mechanism by which this connection would take hold.… for most members, the psychological identification with party appears greatly to outweigh loyalty to the institution. Levinson notes that when one party holds both branches, presidential vetoes greatly decrease, and delegation skyrockets. Under unified government, “the shared policy goals of, or common sources of political reward for, officials in the legislative and executive branches create cross‐​cutting, cooperative political dynamics rather than conflictual ones.”



Individual presidents have every reason to protect and expand their power; but individual senators and representatives lack similar incentive to defend Congress’s constitutional prerogatives. “Congress” is an abstraction. Congressmen are not, and their most basic interest is getting reelected. Ceding power can be a means toward that end: it allows members to have their cake and eat it too. They can let the president launch a war, reserving the right to criticize him if things go badly. And they can take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the executive‐​branch bureaucracy when it imposes costs in the course of deciding what those statutes mean.   
  
  
In David Schoenbrod’s metaphor, modern American governance is a “shell game,” with We the People as the rubes. That game will go on unless and until the voters start holding Congress accountable for dodging responsibility.
"
"
Share this...FacebookTwitterSeveral new studies use evidence from temperature-sensitive plant species and megafauna remains to reconstruct an Arctic climate that was 6°C to 22°C warmer than today when CO2 concentrations lingered near 300 ppm.
Navigating the Arctic Ocean
William Barentsz discovered Arctic Svalbard as he sailed through an open-water Arctic Ocean using a wooden boat in early June, 1596.

Image Source: Wikipedia
In September, 2019 (the month of the year with least extensive sea ice), 16 scientists needed to be rescued by helicopters because the massive ship they were using to study climate change couldn’t cut through the ice-covered waters near Svalbard.
In the 1500s, the Western Arctic was sea ice free for about 4-5 months of the year. Today – and steadily since 1800 – the Western Arctic is sea ice free only about 2 weeks of the year (Porter et al., 2019).

Image Source: Porter et al., 2019
In fact, according to Rosel et al., 2018, Arctic sea ice was actually thicker in 2015 (1.56 m) and 2017 (1.65 m) than it was in 1955 (0.94 m).

Image Source: Rosel et al., 2018
The paleoclimate Arctic record
1. Voldstad et al., 2020  A much broader distribution of thermophilous (warmth-dependent) plant species suggest the sea surface temperatures near Svalbard were as much as 6°C warmer than they are today earlier in the Holocene, which effectively means the Arctic was sea ice free throughout the year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Voldstad et al., 2020
2. Kirillova et al., 2020  About 115-100,000 years ago, “semi-dwarf” wooly mammoths thrived in an Arctic Siberia teeming with lakes, rivers, aquatic plants, grassy meadows, and forests. July temperatures were 8-10°C warmer than today’s. CO2 levels ranged between 260-280 ppm.

Image Source: Kirillova et al., 2020
3. Schenk et al., 2020  Plant species’ thermal tolerance limits (Finland) suggest July temperatures had already reached “near-modern” (~16°C vs. 17°C) levels 15,000 to 11,000 years ago, or during the last late glacial, when CO2 hovered near ~220 ppm CO2. By about 10,000 to 9,000 years ago, Greenland was “4-7°C warmer than today”.

Image Source: Schenk et al., 2020
4. Fedorov et al., 2020  The Eurasian Arctic was up to 8°C warmer than today during the last interglacial (130,000 to 110,000 years ago). Extensive forests lined the Arctic Ocean coast from 10,000 to 3,000 years ago, when the region was also much warmer than today. Today’s Eurasian Arctic coast is treeless tundra. The cold-preferring collared lemming has 25% more habitat now than it did earlier in the Holocene due to today’s tundra expansion and cooler climate.

Image Source: Fedorov et al., 2020
5. Rybczynski et al., 2013  Giant camels and horses were hedged by plants, forests, and wetlands in a balmy High Arctic until at least the late Pleistocene (~79k yrs ago). CO2 ranged from 190 to 280 ppm back then (Pleistocene) and averaged about 300 ppm during the Pliocene, but yet the Arctic climate was 18.3°C warmer than today.

Image Source: Rybczynski et al., 2013
6. Fletcher et al., 2019  From 2 to 4 million years ago, as CO2 pivoted around 300 ppm, the Arctic was 15-22°C warmer than it is today.

Image Source: Fletcher et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_bf2a70f196bcfaac008638389ee41e60').on('change', function() {
			  jQuery('#amount_bf2a70f196bcfaac008638389ee41e60').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMany of us have been noticing there’s a not-so-subtle hate-campaign against the elderly going on.
Not long ago we saw a German production which featured kids singing “my mother is old environmental scumbag“or how Brexit voters were portrayed as incontinent scum.
At a recent performance in Dortmund, Leftist hip-hop band K.I.Z. told a cheering crowd of predominantly women:
People are scared of some stupid virus. The truth is, only old white men die!”
“Corona is practically healing the planet”
Now we have a production again by ARD/ZDF German Public Television, Browser Ballet, that tops it all. The satire is titled: “Corona is rescuing the planet”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





In the skit the moderator begins by telling viewers:
We here at Browser Ballet say ‘yes’ to Corona because this virus is practically healing the planet by itself. Interesting is how fair this virus is. It’s ravaging the elderly, but the youth are withstanding this infection almost without effort. That’s only just because it is the generation 65 and over that has run this planet into a wall over the past 50 years.”
Especially responsible, says the moderator, are the overweight and ill adult Americans, who have been recklessly practicing earth-destroying “turbo capitalism”:
Maybe the Corona virus is merely a response to turbo capitalism, and it is working. Air travel has collapsed, production has been cranked back, consumption is declining. There couldn’t be better news for this planet. Air pollution in China, thanks to Corona, has decreased in a very short time. If that continues, then we may experience a new green paradise!”
He adds:
And isn’t it the problem that there are just too many of us? Less people means less shortages of resources, which means less hunger, which means less war, and that means less causes for refugees. So, probably the Corona virus is simply a nice reflex of nature to tell people who’s the boss here. That’s why: enough with this silly egoism. Corona is here simply because we don’t deserve anything better. “


		jQuery(document).ready(function(){
			jQuery('#dd_c281537b1a33a0b9fbc65907b25ba34a').on('change', function() {
			  jQuery('#amount_c281537b1a33a0b9fbc65907b25ba34a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea69672d7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Another parking lot being measured for climate change: Newport, TN
My surfacestations.org project has reached an important milestone.
With the submission of #222, Lexington, VA, submitted by John Goetz, we are now below the 1000 mark (out of 1221) stations left to survey. It was a 3 -way race to #222 between power surveyors John Goetz, Kristen Byrnes, and Don Kostuch.
Thanks to ALL of the wonderful volunteers for helping to reach this important benchmark! We currently stand at 231 surveyed stations and 990 left to go.
I still need help in the midwest and the south, particularly Kansas, Nebraska, Montana, the Dakotas, Oklahoma, Mississippi, and Alabama. If you live in the areas want to make a lasting contribution to science, please visit www.surfacestations.org and sign up as volunteer. Its easy to do, and it makes for a fun science learning experience.
To see more weather stations like this one, see my “How not to Measure Temperature” series on this blog. This is only a small sample of the 231 surveyed to date, but it will give you an idea of the problems that have been seen so far.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4d334ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Tuesday on page 7A of the Enterprise Record there was a full page ad for the Oriental Buffet at the corner of East Avenue and Esplande that touted a copy of the most recent Butte County Health Department inspection with the words in bold “Compliance Achieved” on the newspaper ad.
You may remember the previous restaurant left an indelible mark in the minds of many Chicoans when it was closed down over a year ago due to massive health violations. Here is the ER Article.
Everybody deserves a chance to succeed, but I have to wonder about the wisdom of opening a door like this by putting your health report in a newspaper ad because it invites people to take a further look. It was the topic of my morning discussion group on Tuesday, so I decided to look for myself.
You can see Food Facility Inspection Reports for the Chico Area online here
And the inspection reports starting 5/07/07 for the Oriental Buffet are here:
( you’ll need Adobe Acrobat PDF reader to view these – its free here )

Oriental Buffet, 2539 Esplanade, Chico 05/07/07 Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/08/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/09/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/11/07 Re-Inspection 
On the first inspection on 5/07/07 there were 7 major violations and 14 minor ones, for a total of 21 violations. The inspector made 22 notations on the issues filling two pages. The next day on 5/08/07 they were down to 4 major violations and no minor ones. On 5/09/07 they were down to 3 major violations. On 5/11/07 they finally achieved “compliance”. The restaurant has been open since April 8th.
But I have to wonder, compliance for how long? You have to wonder that when a restaurant runs a full page ad touting “compliance” given the visually dramatic stigma the building has attached to it maybe the owners don’t fully understand what they are up against. Like I said, everybody deserves a chance to succeed, but perhaps a different theme would be the way to do it in this case.
To be fair though, I’ll point out that the inspection reports show that Egg Roll King on Palmetto needed 4 attempts to reach compliance this year , as did Gen Kai on Pillsbury, and Big Al’s needed 4 last year and so did Rice Bowl this year, and so did Sin of Cortez. Thai House on Broadway needed 5 inspections this year.
The all time high was Happy Garden on Cohasset with six consecutive inspections required last year before compliance was acheieved.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea61e5f6c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Carbon dioxide regulations promulgated by the EPA are based upon the assumption that they will actually _do_ something about climate change in the U.S., and that the rest of the world, which had been needling the U.S. for decades of inaction, will now follow our virtuous lead.   
  
Neither is going to happen.   
  
This _Report_ is based upon just-released data from the U.S. Energy Information Administration showing that the amount of carbon dioxide emitted from the U.S in the last year was the about the same as was emitted in 1994—nearly two decades years ago. During that time, emissions grew steadily for 14 years, peaking in 2007, and then fell dramatically (Figure 1). The emissions in 2012 were 12% less than those of 2007.   






**Figure 1. U.S. annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration).** Given this non-trivial decline in carbon dioxide emissions, let’s see how the government’s assumptions are holding up.   




**Assumption 1: Reducing U.S. carbon dioxide emissions will do something about global warming-related weather/climate impacts in the U.S.**   
  
The U.S. National Oceanic and Atmospheric Administration (NOAA) compiles the number of “Billion Dollar Weather/Climate Disasters” in the U.S. The higher this number is the more media attention they get and the more global-warming-is-making-the-weather-worse-and-we-must-immediately-act-to-stop-it furor it engenders.   
  
With U.S. emissions on the decline, so too, certainly, are the number of billion dollar weather disasters, right?   
  
Figure 2 shows the annual tally of the number of billion dollar “disasters.” Despite the rapid U.S. emissions decline for the past 5 years, the number of weather disasters in the U.S. is increasing.   






**Figure 2. Number of billion dollar weather/climate disasters in the U.S., 1994-2012 (data compiled by the U.S. National Oceanic and Atmospheric Administration).** There are two primary reasons why this is the case. The first is that the number of people and the amount of stuff that we all have continues to increase (i.e., there is more stuff in harm’s way), and the second, _there is no relationship between U.S. carbon dioxide emissions and extreme weather events in the United States_. U.S. emissions could have been _zero_ and the result would have been even worse—because rebuilding infrastructure in the extremely expensive energy economy that would ensue would cost much, much more.   
  
  
  
**Assumption 2: The rest of the world will follow our lead and reduce emissions.**   
  
As we like to point out, the magnitude of future global warming (and accompanying climate change) rests not on the future carbon dioxide emissions pathway of the U.S., but rather on that taken by the rest of the world. In the typical mid-range emissions scenario employed by the UN’s Intergovernmental Panel on Climate Change (IPCC), the amount of warming projected to occur between 1990 and 2095 is 2.8°C (5.0°F). Of this amount, only about 0.2°C (0.38°F)—about 7%—is expected to result from U.S. emissions. Big deal.   
  
Further, these forecasts are likely way too high. We’ve experienced 0.33°C of warming since 1990, or about 12% of the forecast total, even as 22% of the forecast period is has already passed.   
  
So then why is there so much focus by President Obama on regulations aimed at reducing U.S. carbon dioxide emissions when they will result in no meaningful climatic consequence?   
  
Because our emissions declines are supposed to set an example and other nations of the world will follow suit.   
  
That’s not happening, either.   
  
Figure 3 shows the carbon dioxide emissions from the rest of the world over the same time period as the U.S. emissions in Figure 1. When U.S. emissions increased 14% from 1994-2007, emissions from the rest of the world increased. When U.S. emissions then declined 12% from 2007-2012, emissions from the rest of the world increased even faster.   
  




**Figure 3. Global (less U.S.) annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration updated through 2012 according to Peters et al., 2013).** Why? Because the emissions growth in the rest of the world is primarily being driven by China and other developing countries that are more interested in growing their economies then they are in emulating the U.S.   
  
All which has to make you wonder. As the data show that the premises upon which U.S. greenhouse gas regulations have been promulgated are wrong, why do we persist with such silly notions?   
  
**Reference:**   
  
Peters, G.P., et al., 2013. The challenge to keep global warming below 2°C. _Nature Climate Change_ , **3** , 4-6, doi:10.1038/nclimate1783.  

"
"
Share this...FacebookTwitterIn her latest panic attack, teenage Swedish climate activist Greta Thunberg – citing the Guardian –  once again appeared to be proclaiming the end of the world was a step closer when she tweeted Antarctica has set a new record high temperature:

20,7°C on Seymour Island off Antarctica… https://t.co/OiIdlQIl6A
— Greta Thunberg (@GretaThunberg) February 13, 2020

Two new warm records
According to the Guardian, “The 20.75C logged by Brazilian scientists at Seymour Island on 9 February was almost a full degree higher than the previous record of 19.8C, taken on Signy Island in January 1982.”
That reading, the Guardian reports, follows the February 6 record of 18.3C recorded at the Argentinian research station, Esperanza measured.
As is the case with most alarmists, every warm single datapoint anomaly gets uncritically accepted with open arms as solid evidence of man-made global warming while cold trends get dismissed or downgraded as “natural variability”.
Seymour Island has been cooling for over a quarter century
So we have two recent warm records set at and near the Antarctic peninsula over the past week or so and that means the region there is heating up, alarmists like Greta and the Guardian want us to believe. But what are the real TRENDS there? Do the 2 recent warm records mean the region is heating up.
Looking at official data from NASA, it turns out that warming isn’t true. And because climate is always changing, the temperature in the region in question has also not remained completely steady. The only possibility left? COOLING.
Seymour Island, also known as Marambio Island is an island in the chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. What follows is a plot of the mean annual temperature measured at Seymour Island – based on NASA data – going back to the time all the global warming predictions began in earnest:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Contrary to that implied by The Guardian and Greta, the island has in fact cooled a bit during the period, despite the warm spike of 2016.
13 of 13 Antarctic Peninsula/island stations cooling
Next we look at the Antarctic Peninsula, which global warming alarmists also like to have us believe is teetering on the brink of meltdown. Not long ago Japanese climate blogger Kirye posted a chart showing the mean annual temperatures of 13 stations located there – going back two decades.
For alarmists, the results turn out to be terribly inconvenient. The following map shows the location of the stations:

The following chart shows the plots of the mean annual temperature of the 13 stations, using NASA Version 4 unadjusted data:

13 of 13 Antarctic Peninsula and nearby island stations show cooling over the past 21 years. There hasn’t been any warming there so far this century. Data source: NASA GISS, Version 4 unadjusted. 
Natural ocean cycles
Buried near the end of the Guardian article is mention of the real reason behind Antarctic temperature trends:
Scientists on the Brazilian Antarctic programme say this appears to be influenced by shifts in ocean currents and El Niño events: “We have climatic changes in the atmosphere, which is closely related to changes in permafrost and the ocean. The whole thing is very interrelated.”
Indeed it is. Very likely in ways the climate alarmists prefer not to mention.
Share this...FacebookTwitter "
"

_The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
Could President Obama have picked a worse time to announce his Climate Action Plan?   
  
Global warming has been stuck in neutral for more than a decade and a half, scientists are increasingly suggesting that future climate change projections are overblown, and now, arguably the greatest threat from global warming—a large and rapid sea level rise (SLR)—has been shown overly lurid (SOL; what did you think I meant?).   
  
You hardly need an “action plan” when there is so little “action” worth responding to.   
  
As I frequently discuss the lack of warming and the decreases in the estimates of future climate change, I’ll focus here on new scientific findings concerning the potential for future sea level rise, interspersing a little travelogue.   
  
Projections of a large sea-level rise this century depend on rapid ice loss from Greenland and/or Antarctica. Yes, as ocean waters warm, they expand, but this expansion-induced rise is pretty well constrained and limited to being about 6 inches plus or minus a couple of inches by century’s end. And the contribution from melting glaciers/ice in other parts of the world (not counting Greenland and Antarctica) is even smaller, maybe 2-4 inches. So that adds up to about 8-12 inches of sea level rise by the year 2100—not much different than that which has already occurred over the past century. This is hardly catastrophic.   




So getting a good handle on the contributions from Antarctica and Greenland is essential if you want to develop a reasonable expectation for the future. Lacking a good handle leads to unreasonable projections.   
  
Here is an example of the latter.   
  
A breathless passage from the book version of Al Gore’s _An Inconvenient Truth_ :   




I flew over Greenland in 2005 and saw for myself the pools of meltwater covering large expanses on top of the ice. …These pools have always been known to occur, but the difference now is that there are many more of them covering a far larger area of ice. …In Greenland, as in the Antarctic Peninsula, this meltwater is now believed to keep sinking all the way down to the bottom, cutting deep crevasses and vertical tunnels that scientists call “moulins.”   
  
When water reaches the bottom of the ice, it lubricates the surface of the bedrock and destabilizes the ice mass, raising fears that the ice mass will slide more quickly towards the ocean.   
  
…If Greenland melted or broke up and slipped into the sea—or if half of Greenland and half of Antarctica melted or broke up and slipped into the sea, sea levels worldwide would increase by between 18 and 20 feet.   
  
Tony Blair’s advisor, David King, is among the scientists who have been warning about potential consequences of large changes in these ice shelves. At a 2004 conference in Berlin, he said: THE MAPS OF THE WORLD WILL HAVE TO BE REDRAWN. [all caps in original]



Gore went on to include page after page of now and then maps of the world’s major cities after a sea level rise of 20 feet (of course, assuming no adaptive measures put in place).   
  
But Gore’s disaster mechanism has been shown to be impotent and ineffective. In fact, a collection of recent papers published in the peer-reviewed scientific literature basically dispels all myths foretelling a large sea level rise this century coming from ice loss on Greenland. Recent research on Antarctica largely does the same.   
  
First off, research by Sarah Shannon and 18 co-authors takes direct aim at Gore’s mechanism in their paper “Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise.” Here is what they conclude, in direct opposition to Gore’s claims:   




Although changes in lubrication generate widespread effects on the flow and form of the ice sheet, they do not affect substantial net mass loss; increase in the ice sheet’s contribution to sea-level rise from basal lubrication is projected by all models to be no more than 5 percent of the contribution from surface mass budget forcing alone.



And “no more than 5 percent” turns out to be, by the year 2100, somewhere between 0 and 3 millimeters, or in English units, a tenth of an inch or less. Some disaster. Certainly “18 to 20 feet” is a lot scarier, but it is just plain wrong.   
  
Another new study looks at (among other things) the sea-level rise effect of the acceleration of the discharge rate of those glaciers across Greenland, which directly empty out into the sea. Heiko Goelzer and fellow researchers found that after an initial bump in the contribution to sea level rise as these glaciers retreat, once they draw back to the grounding line—the point where the outlet glaciers stop floating and instead rest on the bedrock—the loss rate slows dramatically. They conclude that the contribution from dynamical changes to the flow rate of outlet glaciers may contribute between 8 to 18 millimeters of sea level rise by the year 2100. That is about a quarter to three-quarters of an inch. Again, not even close to a disaster.   
  
Here’s your climate news scoop of the day: The highest discharge-volume glacier in the entire Northern Hemisphere—Greenland’s Jakobshavn—has grounded, which is really going to put the kibosh on the Greenlandic myth. Here’s a picture I took from my own Greenland sojourn* earlier this summer. It shows the southern end of Jakobshavn glacier, on June 24.   






  
  
_Looking south along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
You can see that it is grounded over most of its humongous 10-kilometer face. The calved ice drops off in smaller chunks, dramatically reducing the size of the bergs that will eventually float down the spectacular Ilulissat Icefjord.   
  
A small portion of the glacier was perhaps still floating when I was there, right near the north end, as indicated by a reduction in the height of the calving face, as shown in this photo.   






  
  
_Looking north along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
As a tidewater glacier, Jakobshavn regularly calves some tremendous icebergs that take a couple of years to make their way down the 35-mile fjord, only to ground on the terminal moraine near Ilulissat (and conveniently located in view of the Hotel Arctic’s live webcam, here). Because the glacier has largely grounded, these bergs are not the giants that they once were (although some sizeable icebergs continue to be produced in the early summer as the floating ice tongues established in the winter break up). _Hie thee to Ilulissat! The sooner the better!!_ Presumably some views through the webcam (which was near my room) will convince you!   
  
(The terminal moraine near Ilulissat dates to the end of the Little Ice Age—meaning that the productive fishery at the mouth of the fjord was probably inaccessible. Farther south, such an expansion of ice no doubt covered much of the Viking pastureland, chasing them to places elsewhere (including North America?)).   
  
A third new study examined the direct contribution of changes in the surface mass balance (SMB) of Greenland (that is, total run off from ice melting minus total gains from enhanced snowfall) to future sea level rise (they did not consider ice loss from glacier speed). In their study “Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR,” Xavier Fettweis and colleagues found that declines in the SMB by the year 2100 led to somewhere between 2 centimeters and 13 centimeters of sea level rise, depending of the carbon dioxide emissions scenario used in their model. That’s somewhere between 1 and 5 inches (and these projections are based on climate models which, according to the latest science, overestimate future warming by some 70 percent).   
  
So adding all of these effects up—basal lubrication, glacial dynamics, and enhanced melting—the total global sea level rise by the end of the 21st century originating from Greenland projected by the latest, greatest scientific studies averages out to be maybe 3 to 4 inches. Ho hum.   
  
Like I said, sea level rise disaster scenarios that are dreamed up by Greenland shedding large volumes of ice ( _a la_ Al Gore, Jim Hansen, etc.) are SOL.   
  
  
  
**References:**   
  
Fettweis, X., et al., 2013. Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR. _The Cryosphere_ , **7** , 469-489.   
  
Goelzer, H., et al., 2013. Sensitivity of Greenland ice sheet projections to model formulations, Journal of Glaciology, 59, 733-749, doi:10.3189/2013JoG12J182   
  
Shannon, S., et al., 2013. Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise. _Proceedings of the National Academy of Sciences_ , doi:10.1073/pnas.1212647110   
  
  
  
*Get that ticket to Greenland pronto! Travel hint: the shortest route is through Reykjavik on Iceland Air and then on Air Iceland to Ilullisat. Reserve in advance and you can get a Saga Class (business) seat for pretty cheap compared to the Majors (which will take you all the way to Copenhagen and then backtracking on Air Greenland’s A330 to Kangerlussaq (Sondre Stromfjord) and an additional connection to Ilulissat, i.e. $$$$).


"
"

Our lovely model above appears to be boarding up her cabana in preparation for the next hurricane.
Nails, as inventions go, have a legacy back to before the time of Christ. And after two milliennia, they are still pretty much the same; a piece of iron wire with a wider top to be driven into two pieces of wood to hold it together.
Sure there’s been improvements in steel, in manufacturing, and in making better hammers, but the nail itself hasn’t really changed much.
So its with a surprise that I read in Popular Science that a new nail became one of the ineventions lauded in 2006. Popular Science is naming its Best of What’s New.
It’s not your average nail though, the HurriQuake nail spent six years in development. Its designed to help building withstand hurricane force winds (over 71MPH) and earthquakes that rock wood structures apart. I expect our own local best hardware store, Colliers, to start carrying this sometime soon…I mean they HAVE to, they have EVERYTHING.
From the article:
“As the Bostitch team tweaked the head-to-shank ratio, Sutt and metallurgist Tom Stall worked on optimizing high-carbon alloys, trying to find the highest-strength trade-off between stiffness and pliability — the key to preventing snapped nails. ‘Meanwhile,’ Sutt says, ‘we were focusing on how to keep the nail from pulling out.’ The team machined a series of barbed rings that extend up the nail’s shaft from its point, experimenting with the size and placement of the barbs. ‘You want the rings to have maximum holding power,’ he says, ‘but if they go up too high, it creates a more brittle shank that shears more easily.'”
Now if they can just invent the thumb-proof hammer, we’ll really have something.
I guess you could file this invention under “global warming” as the company references the recent increased frequency of hurricanes as an impetus to the invention. Personally I think the linkage between global warming and hurricanes doesn’t wash, as do many hurricane experts like Dr Neil Frank, former director of the National Hurricane Center and Dr. William Gray, a world renowned predictor of hurricanes. The 2006 hurricane season ends December 1st, and so far this year has been an.. er. wash out in big hurricanes with only 7 in the Atlantic, compared to 2005’s hurricane season with 7 major storms, including hurricane Katrina. Katrina became the media poster child for the global warming to hurricane link, among other things.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9c0cb1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterScientists (Syring et al., 2020) find almost sea ice-free conditions pervaded a much warmer northern Greenland region during the Early Holocene.  Arctic sea ice extent has “continuously” grown for ~4800 years, with modern conditions a bit lower than the peak of the last few centuries.

Image Source: Syring et al., 2020
In a new paper (Syring et al., 2020), scientists rely on biomarker evidence – (a) the presence of warmth-demanding species Armeria scabra and Mytilus edulis, and (b) IP25, a proxy for the presence or absence of sea ice – to suggest not only were there much warmer (4 to 5°C) northern Greenland temperatures 10,000 to 8500 years ago, but effectively sea ice-free conditions pervaded the region during this time.
The sea ice in the region has been growing “continuously” for the last 4800 years, reaching its peak during the last millennium.
The authors also find decadal- and centennial-scale periodicities in solar activity have coincided with variability in Arctic sea ice (IP25) throughout the Holocene.

Image Source: Syring et al., 2020
Share this...FacebookTwitter "
"

Asked recently when the Senate might vote on cap‐​and‐​trade, Senate Majority Leader Harry Reid, D-NV, demurred, muttering about “a busy, busy time the rest of this year.” And yet last week, the Obama administration quietly moved forward with a plan to regulate power plants and other large stationary sources of greenhouse gases.



The Obama team appears to believe it has the authority to implement comprehensive climate change regulation, Congress be damned. Worse still, under current constitutional law–which has little to do with the actual Constitution–they’re probably right.



In a democratic country, you’d think that before the executive branch could regulate CO2–a ubiquitous substance essential to life–the legislature would have to vote on the issue. But you’d be wrong.



In 2007, the Supreme Court ruled that the 1970 Clean Air Act’s definition of air pollutant was broad enough to allow regulation of CO2 emissions from new cars, and that the EPA was required to regulate once it issued a finding that CO2 contributes to global warming. In fact, once the EPA rules that CO2 is a dangerous pollutant–as it did in April–regulation of industrial sources likely becomes mandatory as well.





As Obama’s popularity erodes, he may come to like the idea of being the ‘decider.’



But existing law still leaves the executive branch enormous discretionary power–and thus a hammer to hold over Congress’s head. A report issued in April by the New York University Law School argues that “if Congress fails to act, President Obama has the power under the Clean Air Act to adopt a cap‐​and‐​trade system.”



James Madison believed that there could be “no liberty where the legislative and executive powers are united in the same person.” And yet, here we are, with those powers united in the person of a president who has pledged to heal the planet and stop the oceans’ rise.



This constitutional nightmare is the culmination of a trend many years in the making. The first sentence of the Constitution’s first article says that “all legislative Powers herein granted” are vested in Congress.



The Supreme Court once took that language seriously, as when, in 1935, it struck down a key New Deal program for delegating legislative power to the executive. Yet the Court eventually made its peace with statutes that allow the executive branch to both make and enforce the law.



That paved the way for the modern administrative state, which looks a lot like the situation complained of in the Declaration of Independence, in which “a multitude of New Offices… harass our people and eat out their substance.”



After 9/11, the phrase “unitary executive theory” (UET) came to stand for the idea that the president can do whatever he pleases in the national security arena. But it originally stood for a humbler proposition: UET’s architects in the Reagan administration argued that the Constitution’s grant of executive power to the president meant that he controlled the executive branch, and could therefore rein in aggressive regulatory agencies.



In an era when Republicans held a virtual lock on the Electoral College, that idea had some appeal. But as Elena Kagan, now President Obama’s Solicitor General, pointed out in a 2001 Harvard Law Review article, there’s little reason to think that “presidential supervision of administration inherently cuts in a deregulatory direction.”



How far will Obama push in the other direction? He may be reluctant to stretch his authority as far as the law will allow, in a political climate where even green‐​leaning Democrats scream bloody murder every time gas prices rise.



But as Kagan notes, after the Democrats lost control of Congress in 1994, President Clinton used his regulatory authority unilaterally to show progress, pushing “a distinctly activist and pro‐​regulatory agenda.” As Obama’s popularity erodes, he may come to like the idea of being the “decider.”



Will liberals who decried George W. Bush’s unilateralism object to this staggering concentration of executive power? Don’t hold your breath.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
It’s been a particularly mild winter in Europe this year. But that hasn’t changed the long-term trend over the past 30 years.
Now that the February 2020 data have been coming in, we plot the mean February temperatures for some countries in Europe.
Sweden
Three of 5 stations show February mean temperature in Greta Thunberg’s Sweden have had a cooling trend since 1988! The real data will probably make the climate alarmists upset.

Data source: JMA. 
Great Britain
Twelve of 14 stations in UK show February mean temperatures have had a cooling or no warming trend since 1992! (Note: those 14 stations have data dating back at least to 1980s):

Source: JMA
Finland


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As a whole this northern European country has seen very little warming over the past 3 decades for the month of February:

Three of six stations in Finland show no warming since the 1980s. Data: JMA.
Ireland
Ireland, situated in the North Atlantic, also shows no warming for the month of February since 1987:

Four of 6 stations with data going back to the 1980s show no February warming. Data source: JMA
Netherlands
The story is the same in the Netherlands for the month of February. All 5 of 5 stations there with data going back to at least 1995 show February temperatures having a cooling trend:

Mean February temperatures have been falling at stations in the Netherlands since 1995. Data source: JMA. 
So it remains a mystery as to why global warming alarmists claim rapid warming is happening.


		jQuery(document).ready(function(){
			jQuery('#dd_78e475dfa13960a3458e73b79c2c48f9').on('change', function() {
			  jQuery('#amount_78e475dfa13960a3458e73b79c2c48f9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Beginning with the Carter “hit list” and continuing with the fiscal conservatism of the Reagan administration, westerners have been obliged to realize that the days of concrete and steel solutions to water problems are gone. In stressing the West’s need to adjust to the new realities of water, Gov. Richard D. Lamm of Colorado described the change that has taken place as follows:



When I was elected governor in 1974, the West had a well‐​established water system. … Bureau [of Reclamation] officials and local irrigation districts selected reservoir sites and determined water availability. With members of the western congressional delegation, they obtained project authorization and funding. Governors supported proposals, appearing before congressional committees to request new projects, and we participated in dam‐​completion ceremonies.



In 1986, the picture is quite different. The boom in western resources development has fizzled, though tourism remains an economic mainstay.… Congress, including members of the western delegation, has to worry about how to cut spending, not which [water] projects to fund.… Farmers are trying to stay in business and are recognizing that their water is often worth more than their crops. Policymakers recognize that the natural environment must be protected because it is a major economic asset in the region.[1]



The current political, social, and economic climate is ushering in a whole new era in western water. In the face of efforts to curtail runaway government spending and protect the environment, water institutions must foster the conservation and efficient allocation of existing supplies. They must also take water’s growing recreational and environmental value into account. The crucial question is, can the current water institutions meet today’s requirements?
"
"
Share this...FacebookTwitterClimate alarmist scientists refuted
Distinguished climate expert Roger Pielke Jr. tweeted on recent findings contradicting alarmist claims that tropical storms have slowed down (thus stick around longer and wreak more devastation) or are more frequent and intense.
First, lets look at frequency and intensity.
No detected upward intensity/frequency trend at all
In an article appearing at Forbes, Pielke writes together with atmospheric scientist Dr. Ryan Maue how they and University of North Carolina-Wilmington professor Jessica Weinkle used datasets available around the world on tropical cyclones to create a historical record of storms of at least hurricane strength that made landfall.
Fifty years of global landfalls of tropical cyclones of hurricane strength, based on the Saffir-Simpson hurricane scale, were analyzed.
According to the findings published earlier here:
The analysis does not indicate significant long-period global or individual basin trends in the frequency or intensity of landfalling TCs of minor or major hurricane strength. The evidence in this study provides strong support for the conclusion that increasing damage around the world during the past several decades can be explained entirely by increasing wealth in locations prone to TC landfalls, which adds confidence to the fidelity of economic normalization analyses.”
Shown below is an updated chart from the Pielke et al 2012 paper, which was extended to 2019. It shows global tropical cyclone landfalls at hurricane strength from 1970 to 2019:





<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Pielke and Maue at FORBES: “There are a lot of ups and downs in the data, but no obvious trends.”
Tropical storm translation speeds have not slowed down
Pielke also tweeted about a new study appearing in Nature here. The University of Colorado scientist commented:
He added that the claim that tropical cyclones have now slowed down are “supported by both observations and modeling” and that “there is no reason to expect a slowdown” in the future:

And, from this study under RCP8.5 (!!) there would be no reason to expect a slowdown in hurricane translation speeds in the future pic.twitter.com/RE1aG9OCBC
— Roger Pielke Jr. (@RogerPielkeJr) January 9, 2020

The distinguished professor also says it’s: “Time to retire the notion that hurricanes are slowing down (much less the attribution claims).”
Share this...FacebookTwitter "
"

This week’s report, by Elizabeth Thomas and colleagues from the British Antarctic Survey, that snowfall has been increasing in Antarctica is hardly surprising. What is different that it is much more comprehensive than previous studies, which were largely limited by a virtual lack of pre‐​1957 data. That was the “International Geophysical Year”, in which systematic observations of Antarctica’s climate began.   
  
  
The new study looks at the last 200 years of snowfall trapped in 79 ice cores taken from around the continent. It supplements other recent findings that also made headlines.   
  
  
Determining Antarctica’s overall ice balance has been, well, slippery. One favored method has been to look at gravitational data measured by satellite. Thicker ice means more mass, which means greater gravity. These studies usually come up with a net loss, translating to from 6/1000 of an inch of sea level rise per year to 12/1000 (both values being rather small beer). But different measurements show otherwise. Three years ago, Jay Zwally and his colleagues at NASA used satellite‐​based altimetry and concluded Antarctica was undergoing a net gain in ice.   
  
  
Common sense dictates that it should be snowing more in Antarctica. Think of it as Buffalo on steroids when it comes to snow. In the fall, when Lake Erie isn’t frozen, cold air passing over it from the west picks up evaporated moisture and dumps it on the land in the form of snow squalls. The warmer the water and/​or the colder the air is, the more is snows. Unlike a mere Great Lake, Antarctica is surrounded by a largely unfrozen ocean, and when any atmospheric disturbance sends moisture onshore, it snows too.   
  
  
Around Antarctica, there’s been a slight—meaning a couple of tenths of a degree—warming of the surrounding ocean, which means that the air blowing over it picks up a bit more moisture than it used to. Unlike Lake Erie, the Southern Ocean is huge, and any atmospheric disturbance that shoves more oceanic air up onto the continent is going to be pushing a substantial stream inland with ever more moisture, even for a very slight ocean temperature rise.   
  
  
The “surface mass balance” of a glacier or an ice sheet is the difference between accumulated snowfall and what either melts or evaporates. In anticipation of increased snowfall, the last (2013) scientific summary by the United Nations’ Intertgovernmental Panel on Climate Change shows that the projected 21st change in the Antarctic mass balance to be weakly _positive_. That’s why it’s perplexing that the new finding is so newsworthy.   
  
  
But now we know that the snow has been increasing down there for the past 200 years…and that the increase started before the major emissions of atmospheric carbon dioxide.
"
"

 _Here we introduce a new feature from the Center for the Study of Science, “On the Bright Side.”_ _OBS will highlight the beneficial impacts of human activities on the state of our world, including improvements to human health and welfare, as well as the natural environment. Our emphasis will typically focus on the oft-neglected positive externalities of carbon dioxide emissions and associated climate change. Far too often, the media, environmental organizations, governmental panels and policymakers concentrate their efforts on the putative negative impacts of potential CO 2-induced global warming. We hope to counter that pessimism with a heavy dose of positive reporting on the considerable good humans are doing for themselves and for the planet._   
  
_**—**_   
  
According to Piao _et al_. (2015), the reliable detection and attribution of changes in vegetation growth are essential prerequisites for “the development of successful strategies for the sustainable management of ecosystems.” And indeed they are, especially in today’s world in which so many scientists and policy makers are concerned with what to do (or not do) about the potential impacts of CO2-induced climate change. However, detecting vegetative change, let alone determining its cause, can be an extraordinarily difficult task to accomplish. Nevertheless, that is exactly what Piao _et al_. set out to do in their recent study.   
  
More specifically, the team of sixteen Chinese, Australian and American researchers set out to investigate trends in vegetational change across China over the past three decades (1982-2009), quantifying the contributions from different factors including (1) climate change, (2) rising atmospheric CO2 concentrations, (3) nitrogen deposition and (4) afforestation. To do so, they used three different satellite-derived Leaf Area Index (LAI) datasets (GLOBMAP, GLASS, and GIMMIS) to detect spatial and temporal changes in vegetation during the growing season (GS, defined as April to October), and five process-based ecosystem models (CABLE, CLM4, ORCHIDEE, LPJ and VEGAS) to determine the _attribution_.   




With respect to _detection_ , this work revealed that most regions of China experienced a greening trend indicative of enhanced growth across the time period studied (see Figure 1). Overall, 56 percent of the area studied experienced a significant increase in greening (95% level) when using the GLOBMAP dataset, compared with 54 and 31 percent using the GLASS and GIMMIS datasets. Those regions with the largest greening trends include southwest China and part of the North China Plain.   
  
_  


![Figure 1. Spatial distribution of the trend in LAIGS over the period 1982–2009 as calculated by the GIMMS dataset \(a\), GLOBMAP dataset \(b\) and the GLASS dataset \(c\). The frequency distribution of the significance level \(P value\) of the trends calculated for the three LAIGS datasets is shown in panel d.](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/chinaidso.jpg?itok=fhC2To8Y)

_



  




**_Figure 1._** _Spatial distribution of the trend in LAI GS over the period 1982–2009 as calculated by the GIMMS dataset (a), GLOBMAP dataset (b) and the GLASS dataset (c). The frequency distribution of the significance level (P value) of the trends calculated for the three LAIGS datasets is shown in panel d._



With respect to _attribution_ , Piao _et al_. report that “the combined effect of CO2 fertilization and climate change with the effect of nitrogen deposition, leads to the conclusion that these three factors are responsible for almost all of the average increasing trend of LAIGS observed from the satellites” (see Figure 2). They also report that “at the country scale, the average trend of LAIGS attributed to rising CO2 concentration is estimated to be ... about 85% of the average LAIGS trend estimated by satellite datasets,” while noting secondarily that the enhanced nitrogen deposition driven by _fossil fuel combustion_ and _agricultural fertilization_ is likely the source of the remaining portion of China’s enhanced vegetation growth, citing the findings of Reay _et al_. (2008), Thomas _et al_. (2009), Fleischer _et al_. (2013) and Yu _et al_. (2014).   






**_Figure 2._** _Trend in China’s LAI GS over the period 1982–2009 at the country scale for the three satellite remote sensing datasets and five process models described in the text above. Significance levels of 95 and 99 percent are denoted with one and two asterisks, respectively. See the authors' original text (Piao et al., 2015) for additional explanation of this figure._



In considering the researchers' several findings, it is clear that the fossil fuel combustion that has resulted in the rise in atmospheric CO2 and enhanced nitrogen deposition over the past three decades has provided a great benefit to Chinese vegetation. As illustrated in Figure 2, led primarily by the increase in CO2, that benefit has been _more than sufficient_ to compensate for the negative effects of climate change that also occurred over that time period. Thus, it would seem far more prudent to _celebrate_ CO2 instead of _demonizing_ it, like so many people incorrectly do these days; for atmospheric CO2 is truly the _elixir of life!_   
  
  
  
**References**   
  
Fleischer, K., Rebel, K.T., Molen, M.K., Erisman, J.W., Wassen, M.J., van Loon, E.E., Montagnani, L., Gough, C.M., Herbst, M., Janssens, I.A., Gianelle, D. and Dolman, A.J. 2013. The contribution of nitrogen deposition to the photosynthetic capacity of forests. _Global Biogeochemical Cycle_ s **27** : 187-199.   
  
Piao, S, Yin, G., Tan, J., Cheng, L., Huang, M., Li, Y., Liu, R., Mao, J., Myneni, R.B., Peng, S., Poulter, B., Shi, X., Xiao, Z., Zeng, N., Zeng, Z. and Wang, Y. 2015. Detection and attribution of vegetation greening trend in China over the last 30 years. _Global Change Biology_ **21** : 1601-1609.   
  
Reay, D.S., Dentener, F., Smith, P., Grace, J. and Feely, R.A. 2008. Global nitrogen deposition and carbon sinks. _Nature Geoscience_ **1** : 430-437.   
  
Thomas, R.Q., Canham, C.D., Weathers, K.C. and Goodale, C.L. 2009. Increased tree carbon storage in response to nitrogen deposition in the U.S. _Nature Geoscience_ **3** : 13-17.   
  
Yu, G., Chen, Z., Piao, S., Peng, C., Ciais, P., Wang, Q., Li, X., and Zhu, X. 2014. High carbon dioxide uptake by subtropical forest ecosystems in the East Asian monsoon region. _Proceedings of the National Academy of Sciences USA_ **111** : 4910-4915.


"
"
I decided I’d drop some more fun with entropy your way. Here is the USHCN
station of climate record in Redding, CA GISS number # 425725920010 and used in
the climate modeling database
It is now operated by the US Forest Service at their HQ located at the
Redding Airport. It used to be operated by the National Weather Service, but
that WSFO closed in the mid 90’s.
Like Marysville, the site is surrounded by asphalt, and the surface is
unnatural – its wood chips over weedmat, and I’ll have to say it was hot as heck
to walk on during mid-day..
But the kicker is the “accessories” they’ve added for convenience of running
the hygrometer and for night observations. Yes it is another fine high-quality
USHCN climate recording site. I wonder how many times they forgot to turn off
the light? It looks like there might be room for a hot plate to keep your coffee
warm while making observations.



The blower is used to run air past the wet bulb hygrometer…its not the
correct way
to do it (manual aeration by rotation is specified).

Here is the satellite picture from Google Earth



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea62cfaea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman online site Stromreport writes that since the year 2000 the average electricity price for private households has risen from 13.94 to 30.43 euro cents per kilowatt hour (2019).
German electricity prices for households are among the highest worldwide.

Image: Statista.com
The price increase has little to do with demand or markets, but almost everything to do with government interference. According to Stromreport, “Taxes, charges and levies have tripled since 2000 [from 5.19 to 16 cents]. In total, German government charges now account for more than half of the electricity price [52.5%].”
Electricity becoming a luxury
Annually hundreds of thousands of German households see their power cut off due to unpaid power bills. For example in 2018, the Tagesspiegel here reported: “In the past year, almost 344,000 households in Germany had their electricity turned off. This is according to the monitoring report of the Federal Network Agency on the electricity market.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of course the high prices hit the poor the hardest.
More price hikes in 2020
And things are not going to improve for Germany’s overburdened power consumers in 2020. Stromreport writes that 403 suppliers have already raised electricity prices by an average of 5.3% this year already, bringing the price to a whopping 30.43 cents per kilowatt-hour. “A 3-person household currently pays almost 89 euros for its electricity. That is 27% more than 10 years ago [69.09 euros].”
Now comes the CO2 charge
The price in 2020 is expected to reach 31.47 cents per kilowatt-hour. Also the wholesale prices for electricity are expected to rise in 2020, due to “rising CO2 prices”…”which will make electricity from coal and gas more expensive on the electricity exchange,” says Stromreport.
Another major component of the German power price are the green energy feed-in tariffs for power coming from, for example, wind and sun. German consumers pay 6.756 cents for kilowatt-hour.
Hat-tip: Die kalte Sonne.


		jQuery(document).ready(function(){
			jQuery('#dd_79cf92e8191e2141d3be52723b8df435').on('change', function() {
			  jQuery('#amount_79cf92e8191e2141d3be52723b8df435').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEven NASA says it:
Without the Earth’s greenhouse gases (GHG) in the atmosphere, the planet would be on average a frigid -18°C.
But because of the preindustrial 280 ppmv CO2 and other GHGs in our atmosphere, the average temperature of the Earth thankfully moves up by 33°C to +15°C (see chart below), based on the Stefan Boltzmann Law.

 
Global warming theorists say the Earth’s surface preindustrial temperature was supposed to be 15°C. And today CO2 is supposed to have warmed it another degree, to 16°C. Image: www.klimamanifest.ch.
And because CO2 has since risen to about 410 ppmv today, the global temperature supposedly should now be about another 1°C warmer (assuming positive feedbacks) bringing the average earth’s temperature to 16°C.
And once the preindustrial level of CO2 gets doubled to 560 ppm, later near the end of this century, global warming alarmists insist the Earth’s temperature will be near 18°C, see chart above.
So we are now supposed to be at 16°C today and warming rapidly. But what is the globe’s real average temperature today? 15.8C? 16.0C? 16.5°C?
Answer: astonishingly the official institutes tell us it is only 14.7°C!
For example, data from the World Meteorological Organization (WMO) shows us the global absolute temperatures for the previous 5 years:

Image: www.klimamanifest.ch, data source: WMO in Geneva.
As the image above shows, the global absolute temperature last year was just 14.68°C.
This is 0.32°C COOLER than the 15°C we are supposed to have with 280 ppmv, and a whopping 1.32°C cooler than the 16°C it is supposed to be with the 410 ppmv CO2 we have in our atmosphere today.
So why are we missing over 1.3°C of heat? Why is there this huge discrepancy between scientists?
German scientists say Earth temperature was 15.5°C – in 1990!
In May, 1990, even the German government stated in its major report on climate (BT-DRS 11/8030, p. 29) that the global average temperature back then was 15.5°C and that the natural temperature was supposed to be 15°C — like NASA says. The German government reiterated that figure in 1992.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In 2003, renowned German climate scientist Prof. Mojib Latif also confirmed in his book that 15°C was the “optimum temperature” for the Earth.
And in his 2003 doctorate dissertation, Tim Staeger wrote that the natural Earth’s temperature without man-made impacts is “about 15°C”.
In fact, practically all German textbooks used at schools today say that the “natural temperature” of the Earth due to the greenhouse effects of the atmosphere is a “life-friendly average of 15°C” instead of -18°C.
PIK scientist: “15°C in 1850”
Moreover, Potsdam institute for Climate Impact Research (PIK) scientist Anders Levermann testified before the German Parliament last year, and confirmed that the global mean temperature of the Earth back in 1850 was 15°C, which means today it is supposed to be well over 16°C. So why is the WMO telling us it’s only 14.68°C and others like the PIK and NASA saying it’s about 16°C?
This massive discrepancy needs to be explained.
IPCC 2007 report in line with WMO
The IPCC indeed contradicts all the scientists and media who claim the global average temperature was 15°C back in 1850. Here’s the IPCC chart from the 2007 report:

How can 14.68°C be “too hot”?
As the chart above shows, the IPCC stated that the global mean temperature in 1850 was a relatively frigid 13.7°C – i.e. well below the “life-friendly” average of 15°C.
Today we are still below the 15°C, and so how can it be too hot?
Summary
There’s no doubt it’s gotten warmer since 1850, the peak of the Little Ice Age. But it’s clear nobody knows what the globe’s real average temperature is. Figures are being wildly tossed around. If we are to believe the IPCC’s 14.7°C figure, then we are still too cool and there is absolutely no warming crisis.
Scientists need to answer this question behind this huge discrepancy quickly. Would the real global temperature please stand up!
============================
Hat-tip: Das Klimamanifest von Heiligenroth
Share this...FacebookTwitter "
"

Hillary Rodham Clinton, the secretary of state who no doubt thinks of herself as “fourth in the line of succession,” tells a European audience how the Obama administration will pass an agenda that Americans have previously rejected: “Never waste a good crisis … Don’t waste it when it can have a very positive impact on climate change and energy security.”   
  
  
As I’ve written several times, governments throughout the decades have taken advantage of wars and economic crises to expand their size, scope, and power. Bob Higgs wrote about “Crisis and Leviathan” long before Naomi Klein called it “The Shock Doctrine.”   
  
  
But the striking thing about the Obama administration is that they openly acknowledge that’s what they’re doing — using a crisis to ram through their entire policy agenda while people are in a state of panic. Projects like national health insurance, raising the price of energy, and subsidizing more schooling — the three prongs of President Obama’s speech to Congress — have nothing to do with solving the current economic crisis. But the administration is trying to push them all through as “stimulus” measures. And they keep proclaiming their strategy.   
  
  
First it was Rahm Emanuel: “You never want a serious crisis to go to waste. And this crisis provides the opportunity for us to do things that you could not do before.” Then Joe Biden: “Opportunity presents itself in the middle of a crisis.” Not to mention Paul Krugman and Arianna Huffington. And now Hillary.   
  
  
Not since George Bush the elder told the media that his campaign theme was “Message: I care” has a president been so open about his political strategy. But these people are displaying a contempt for the voters. They’re telling us that we’re so dumb, we’ll go along with a sweeping agenda of economic and social change because we’re in a state of shock. They may be right.   
  
  
But voters and members of Congress should remember Bill Niskanen’s sobering analysis of previous laws passed in a panic.
"
"
Share this...FacebookTwitter
Image cropped from Met Office here. 
By Die kalte Sonne
(German text edited by P. Gosselin)
On June 3, 2020, npj Climate and Atmospheric Science published a study by Athanasiadis et al. 2020, in which the authors investigated the question of whether changes in the frequency of blocked weather situations in the North Atlantic and Central European region are predictable.
“Quite some nonsense”
Previously, scientists inclined towards climate alarmism had told us that CO2 would lead to more and more blocked weather situations. Quite some nonsense as it now turns out, because the blockings are more likely to be due to the 60-year AMO ocean cycle, which in turn affects the NAO. These are exciting results.
Here’s the abstract:
Decadal predictability of North Atlantic blocking and the NAO
Can multi-annual variations in the frequency of North Atlantic atmospheric blocking and mid-latitude circulation regimes be skilfully predicted? Recent advances in seasonal forecasting have shown that mid-latitude climate variability does exhibit significant predictability. However, atmospheric predictability has generally been found to be quite limited on multi-annual timescales. New decadal prediction experiments from NCAR are found to exhibit remarkable skill in reproducing the observed multi-annual variations of wintertime blocking frequency over the North Atlantic and of the North Atlantic Oscillation (NAO) itself. This is partly due to the large ensemble size that allows the predictable component of the atmospheric variability to emerge from the background chaotic component. The predictable atmospheric anomalies represent a forced response to oceanic low-frequency variability that strongly resembles the Atlantic Multi-decadal Variability (AMV), correctly reproduced in the decadal hindcasts thanks to realistic ocean initialization and ocean dynamics. The occurrence of blocking in certain areas of the Euro-Atlantic domain determines the concurrent circulation regime and the phase of known teleconnections, such as the NAO, consequently affecting the stormtrack and the frequency and intensity of extreme weather events. Therefore, skilfully predicting the decadal fluctuations of blocking frequency and the NAO may be used in statistical predictions of near-term climate anomalies, and it provides a strong indication that impactful climate anomalies may also be predictable with improved dynamical models.”

Share this...FacebookTwitter "
"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Small, Large, and Very Large Air‐​Cooled Commercial Package Air Conditioning and Heating Equipment proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide — two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.
"
"

 _Global Science Report is a feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Last fall, the press pounced on the results of a new study that found that global climate change was leading to an increasing frequency of heat waves and thus resulting in greater heat-related mortality. Finally a scientific study showing that global warming is killing us after all! See all you climate change optimists have been wrong all along, human-caused global warming is a threat to our health and welfare.   
  
Not so fast.   
  
Upon closer inspection, it turns out that the authors of that study—which examined heat-related mortality in Stockholm, Sweden—failed to include the impacts of adaptation in their analysis as well as the possibility that some of the temperature rise which has taken place in Stockholm is not from “global” climate change but rather local and regional processes not at all related to human greenhouse gas emissions.   
  
What the researchers Daniel Oustin Åström and his colleagues left out of their original analysis, we (Chip Knappenberger, Pat Michaels, and Anthony Watts) factored in. And when we did so, we arrived at the distinct possibility that global warming actually led to a reduction in the rate of heat-related mortality in Stockholm.   
  
Our findings have just been published in the scientific journal Nature Climate Change as a Comment on the original Oustin Åström paper (which was published in the same journal).   
  
We were immediately skeptical because the original Oustin Åström results run contrary to a solid body of scientific evidence (including our own) that shows that heat-related mortality and the population’s sensitivity to heat waves was been declining in major cities across America and Europe as people take adaptive measures to protect themselves from the rising heat.



Contrarily, Oudin Åström reported that as a result of an increase in the number of heat waves occurring in Stockholm, more people died from extreme heat during the latter portion of the 20th century than would have had the climate of Stockholm been similar to what it was in the early part of the 20th century—a time during which fewer heat waves were recorded. The implication was that global warming from increasing human greenhouse gas emissions was killing people from increased heat.   
  
But the variability in the climate of Stockholm is a product of much more than human greenhouse gas emissions. Variations in the natural patterns of regional-scale atmospheric circulation, such as the Atlantic Multidecadal Oscillation (AMO), as well as local impacts associated with urbanization and environmental changes in the direct vicinity of the thermometer are reflected in the city’s temperature history, and the original Oudin Åström et al. publication did not take this into account. This effect is potentially significant as Stockholm is one of Europe’s fastest growing cities.   
  
But regardless of the cause, rising temperatures spur adaptation. Expanded use of air conditioning, biophysical changes, behavior modification, and community awareness programs are all examples of actions which take place to make us better protected from the dangers associated with heat waves. Additionally, better medical practices, building practices, etc. have further reduced heat-related stress and mortality over the years.   
  
The net result is that as result of the combination of all the adaptive measures that have taken place over the course of the 20th century in Stockholm, on average people currently die in heat waves at a rate four times less than they did during the beginning of the 20th century. The effect of adaptation overwhelms the effect of an increase in the number of heat waves.   
  
In fact, it is not a stretch to say that much of the adaptation has likely occurred because of an increased frequency of heat waves. As heat waves become more common, the better adapted to them the population becomes.   
  
Our analysis highlights one of the often overlooked intricacies of the human response to climate change—the fact that the response to climate change can actually improve public health and welfare.   
  
Which, by the way, is a completely different view than the one taken by the current Administration.   
  
References:   
  
Knappenberger, P., Michaels, P., and A. Watts, 2014. Adaptation to extreme heat in Stockholm County, Sweden. Nature Climate Change, 4, 302-303.   
  
Oudin Åström, D., Forsberg, B., Ebi, K. L. & Rocklöv, J., 2013. Attributing mortality from extreme temperatures to climate change in Stockholm, Sweden. Nature Climate Change, 3, 1050–1054.


"
"

You may have heard or seen that I donated the equipment and continue to provide the high bandwidth server to carry the City Council Meetings, School Board, Planning Commission and other public meetings via live Internet Webcast as a public service.
Councilman Larry Wahl and I worked together on this project to make it become a reality, and I was pleased to announce its operation in September of 2005. It was a fun and useful project since many people can’t get cable channel 11 to see public meetings.
Well tonight as I blog this over a glass of wine, there’s some new personal satisfaction in that I’ve sucessfully completed a major test that will make this medium even more valuable for the citizens of our fair city.

It all started last week when Kris Koenig of the Chico Observatory asked me if I could run a live webcast to cover this weeks Northern California History Museum Cosmic Hike lecture series done in conjunction with the Chico Community Observatory. This weeks topic was about the Sun and Global warming, so naturally I said “absolutely”.
Now if you have ever said yes to something before fully understanding what you just committed to you’ll realize this is why parents tell their kids “don’t volunteer” when they go into the Army.
I figured, “hey no sweat”. I’ve done it before and I can do it again. Well, in the world of Internet connectivity, thats a whole different animal. I should have known better because last week I was knee deep in another computer problem when somebody suggested the Occam’s Razor solution, (the simplest answer is the most likely solution) to which I replied: “Occam never owned a computer!”.
Enter the CARD Center on Vallombrosa. They have a broadband network, should be easy to connect up my streaming video software and away we go…instant live broadcast right? Wrong.  Like many organizations, they have a firewall. A big one, and…it requires login to even do web browsing. No connect and go here.
My previous setup for the City Council Chambers used a fixed public IP address…the simplest most direct way to connect. But its also dangerous, as its like setting up a lemonade stand on Highway 99 and 149 interchange. You are likely to get run over just sitting there unless you know what you are doing. In my case it was designing an “invisible” server to connect the Cable 11 video feed to the Internet. Firewalls are designed to protect the foolish from the “raw” Internet and its vagaries of hackers, viruses, spyware, and trojans… but they also make life miserable when you want to do something other than simple web browsing and email.

So anyway, to make a long story short after four days of email, support calls, testing, alternate testing, testing again, reporting results, trying new things, etc I still didn’t have a working solution for Thursday night’s CARD center event. But I was getting close. I’d solved one Microsoft induced problem, that of a network card driver that didn’t like certain types of network traffic, but my network engineer and I were still butting our heads up against the CARD firewall problem.
Today, with help from a programmer, we tried a new setup and voila’ …all was right with the world again. We got it working at the CARD Center.
Not only can I now stream video from wired connections like the CARD center, but now I can stream video from almost ANYPLACE that has wired or wireless “WiFi”connectivity.

The little picture above of color bars may not say much to you, but to me it speaks volumes, because it was captured at the Market Cafe restaurant that has a firewall, PLUS a Wireless Encrytion Protocol. They use a WEP key that they give customers to logon. AND its traveling all the way to Arlington Texas where I have a rented high bandwdith server and back to the laptop on the bar again. it’s the worst case scenario. I brought in my laptop and set it up on the bar with my portable NTSC Test signal generator and a USB video capture device. And by golly…it works! I’m blogging this entry from the bar too.
So what does this mean?
Well it means that I can now broadcast ANY live event in Chico or wherever, as long as there is some kind or wired or wireless Internet connection. For example, I could broadcast concerts in the new City Plaza, I could broadcast from Laxon auditorium, I could broadcast from Starbucks, Bidwell Perk, Moxies, local schools, courtrooms, backyard BBQ’s, concerts, …you name it.
But wait…there’s more. Not only that, but now I have the ability to simultaneously record the live webcast and make it available for playback later. Did you miss last night’s City Council meeting where somebody suggested pushing conservative counselors out of Enloe’s Flightcare helicopter because “they are going to lose the next election anyway”…no problem, log on and play it back. Just joking, that never happened though something like it once did at a planning commission meeting.
The Internet world just got a whole lot bigger, look for fun stuff to come.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea891ad7d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Yes, climate change is real.
But what they don’t tell us is that in many places that change has gone in the opposite direction of what alarmists like to have us think.
Moreover, that change is obviously driven far more by natural causes, such as solar and oceanic cycles, and has very little to do with man-made CO2.
Today we look at the untampered temperature datasets of the Japan Meteorological Agency (JMA) that go back to 1988 and which are mostly complete.
Here’s the plot of the 6 stations with adequate data:

Data: JMA
Five of the 6 stations show cooling or no upward trend. Earlier predictions of rapid warming are proving to be false.
Next is a plot of 8 stations using data from NASA, which show notable cooling trend over the past 25 years:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Deep blue Vermont says no to Big Wind
When it comes to wind energy, we’re seeing strong signals against it coming from Vermont, a state that is as politically blue, and thus pro-green, as any state could possibly be.
One would think that the Green Mountain State would welcome “clean”, renewable wind energy and happily make its contribution to rescuing the climate and environment. Vermont, after all, is home to Bernie Sanders and Bill McKibben.
Ironically, Vermont’s strong environmental streak is backfiring on industrial wind. Vermont citizens are realizing industrial wind parks are not green after all, and aren’t worth defacing the rural landscape. Vermonters now more than ever want them the hell out.
For example, Windpower Engineering Development site here reports how Vermont’s political environment now “is hostile to wind energy”.
“In 2012, there were over a dozen wind projects in development. Now there are none. This is truly a sad state of affairs for Vermont,” stated David Blittersdorf, CEO and founder of AllEarth Renewables.
Large wind facilities banned
Meanwhile voters in the tiny Vermont village of Grafton have endorsed a new town plan that prohibits industrial and commercial wind, reports the Brattleboro Reformer here. “On a 95-66 vote during an all-day ballot Monday, residents approved the new town plan, which bans any large wind facility, and includes other planning updates.”
Also neighboring Windham has said “no” to industrial wind parks by developer Iberdrola, which has since “dropped its plans to build what would have been the largest wind project in the state of Vermont,” writes the Brattleboro Reformer.
Good to see Vermonters are finally waking up to the green energy madness and landscape blight and zero benefit it all leads to.
Share this...FacebookTwitter "
"

Sky of Jerusalem at 7BC-11-12 at about 7:30PM local time – Click to View Larger image
With all the hullabaloo about politically correct “Happy Holidays” greetings, as done up in electric lights on top of the Sierra Nevada Brewery, I thought the Christmas Star would be an appropriate topic.

About 2,006 years ago, according to a widely accepted historical and biblical accounts, a star rose in the east and guided three eminent thinkers, known as the Magi,  to the scene of an event that was to change the face of the world.
Since that time, astronomers and theologians have been baffled as to the precise nature of the star which, as told in the Gospel of St Matthew, led the Magi to the stable in Bethlehem where Christ was born.
Was it a miracle, a divine intervention to herald the birth of Christ? Was there a star at all, or was it simply added to the Bible to fulfil an Old Testament prophecy? Or was there some actual astronomical event that gave rise to the story of the Star of Bethlehem?
These questions have intrigued scores of scientists, writers, and artists ever since.
Evidence drawn from modern Biblical scholarship, recent findings in space and ancient Chinese history to suggest that evidence of the star’s existence could be at hand.
A British astronomer, Mark Kidger suggests that the Nativity may well have taken place at some time in March or April rather than in December.
Christ’s birth is said to have taken place while shepherds were watching their flocks at night, he notes, something that takes place at lambing-time in the spring rather than in the depths of winter. If the local inns were full, as the Gospel of Matthew insists, this would be because of the Jewish Passover, which also occurs in the spring.
Kidger concludes that Christ was born some time around March in 5 BC, taking account of the generally accepted fact that the inventor of the Christian calendar, the 6th century monk Dionysius Exiguus, was five years out in his calculations.
Ther have been several theories, including  the “star” could have been an unusual sighting of Venus, or perhaps Halley’s Comet, a supernova, or a meteor shower.
More plausible is the popular theory that what the Magi saw was a planetary conjunction, which occurs when two planets pass very close to each other in the sky, often producing a very striking configuration.
As shown in the picture above, generated by a computer program known as Starry Night, one such conjunction took place in 7 BC when Jupiter and Saturn came close to each other three times in seven months and were then joined by Mars, an event known to have been observed in Babylonia, well to the east of Bethlehem.
A more recent idea is that the Star of Bethlehem may have been an occultation of Jupiter by the moon that occurred in 6 BC, the re-emergence of the royal planet from behind the moon’s disc suggesting a royal birth.
However Kidger points out that the event would have taken place so low in the twilight sky of the region it would have been impossible to observe directly.
For his “best guess” at solving the Star of Bethlehem riddle Kidger looks to an ancient Chinese chronicle called the Ch’ien-han-shu which states that an object, probably a nova, or new star, was observed in March in 5 BC and remained visible for 70 days.
The object would have appeared in the east and remained in the sky long enough to have guided the Magi — Babylonian astrologers, according to some scholars — across the desert to Bethlehem.
“It’s hard to believe the Star of Bethlehem could have been anything else,” Kidger says of the nova, citing the coincidence in date, the duration of visibility and its position in the sky. And proof of its identity may soon be possible by looking for its telltale remains when the successor to the Hubble telescope goes online in 2011.
When a star goes nova, or supernova, if it has any planets, those planets usually become toast in the process. It may be that our birth of Christianity was heralded in by the destruction of another planet, possibly an entire civilization. As they say, God does work in mysterious ways.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9509c38',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Whew! It has been a busy week, with lots of things to report. So rather than
making a blog entry for each one I thought I’d condense them all into one entry
with links.
The emphasis this week seems to be on the sun, and the fact that maybe its
really the sun which has been driving climate change after all. That’s what I’ve
been saying for years, because its just unrealistic to ignore the largest and
single most important contributor to our planets energy balance and to only
focus on made-made CO2 and nothing else.
Here are some headlines and links to the reports:
Former Tennessee Senator Fred Thompson

comes out against Gore – cites the sun – from the National Review

Sun Blamed for Warming of Earth and Other Worlds – from LiveScience
Gore testifies on Global Warming before congress –
video from
C-SPAN – free RealPlayer required,

download here
Greenlands

Ice pack measured accurately, shown to be shrinking, but alternate cause
suspected – from NASA
NASA Finds

Sun-Climate Connection in Old Nile Records – Pharoahs apparently made some
accurate records

Sun’s Output Increasing in Possible Trend Fueling Global Warming – From a

Duke University paper and Space.com
 

Global Warming expedition to north pole called off due to extreme cold

Biggest solar storm in fify years is expected – from NASA


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7929fda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

_The_ _Washington Post_ writes about how President Obama became obsessed with grabbing our complex energy systems by the scruff of the neck and shaking them into something more appealing to Ivy League planners. I was struck by this vignette: 



But even before the late‐​night session in July, Obama had begun to educate himself about energy and climate and to use those issues to define himself as a politician, say people who have advised him. He read a three‐​part New Yorker series on climate change, for instance, and mentioned it in three speeches.



It’s great that he read a three‐​part series in the New Yorker. But has the president ever actually read anything by a climate change skeptic? Actually, a better term would be “a climate change moderate.” Leading “skeptic” Patrick J. Michaels, for instance, of Cato and the University of Virginia, isn’t skeptical about the reality of global warming. His summary article in the Cato Handbook for Policymakers begins: 



Global warming is indeed real, and human activity has been a contributor since 1975.



But he also notes that climate change is complex, and its policy implications are at best unclear. “Although there are many different legislative proposals for substantial reductions in carbon dioxide emissions, there is no operational or tested suite of technologies that can accomplish the goals of such legislation.” The flawed computer models on which activists rely cannot reliably predict the future course of world temperatures. The apocalyptic visions that dominate the media are not based on sound science. The best guess is that over the next century there will be very slight warming, without serious implications for our environment our society. Michaels’s closing appeal to members of Congress would also apply to President Obama and his advisers: 



Members of Congress need to ask difficult questions about global warming.   
  
  
Does the most recent science and climate data argue for precipitous action? (No.) Is there a suite of technologies that can dramatically cut emissions by, say, 2050? (No.) Would such actions take away capital, in a futile attempt to stop warming, that would best be invested in the future? (Yes.) Finally, do we not have the responsibility to communicate this information to our citizens, despite disconnections between perceptions of climate change and climate reality? The answer is surely yes. If not the U.S. Congress, then whom? If not now, when? After we have committed to expensive policies _that do not work_ in response to a misperception of global warming?



Please, President Obama — in addition to the lyrical magazine articles on the apocalyptic vision that you read, please read at least one article by a moderate and widely published climatologist before rushing into disastrously expensive policies.
"
"

Suppose a commenter posts a libelous comment here at NorCalBlogs. It’s been known to happen. Can the blogger, Enterprise Record, and its corporate owners be sued for defamation? A federal appeals court just held that no, they cannot. The court noted that a federal law was designed to ensure that ‘within broad limits’, message board operators would not be held responsible for the postings made by others on that board,’ adding that, were the law otherwise, it would have an ‘obvious chilling effect’ on blogger free speech.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8011936',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 
In Today’s Chico News and Review, the cover story is about Internet Radio and all
the trouble the Copyright Royalty Board recently caused with a draconian ruling
on the cost to Internet Radio Stations. Regular over the air
broadcasters don’t have such limits because they are seen to ""promote the music
industry"" Its an alliance as old as payola.
But good news comes today. A bill introduced in Congress today could nullify
the new rates set by the Copyright Royalty Board (CRB) which advocates say would
put Internet Radio webcasters out of business, such as our own local
Radio Paradise.
Rep. Jay Inslee (D-WA) and Rep. Don Manzullo (R-IL) have presented the ""Internet
Radio Equality Act"" which aims to negate the controversial March 2nd
decision which puts royalty of a .08 cent per song per listener, retroactively
from 2006 to 2010 on internet radio.
Advocates of Internet Radio have dreaded the CRB ruling, which they say could
raise rates between 300 to 1200 per cent for webcasters. Earlier this month, the
CRB threw out an appeal by commercial webcasters, National Public Radio and
others to review the new rates and postpone a May 15 deadline for the
introduction of the royalty schedule.
If passed, today’s proposed bill would set new rates at 7.5 per cent of the
webcaster’s revenue — the same rate paid by satellite radio. Alternatively,
webcasters could decide to pay 33 cents per hour of sound recordings transmitted
to a single user.
This bill is a critical step to preserve this new growing medium, and would
present a level playing field where webcasters can compete on the same royalty
terms with satellite radio. It would also reset royalty rules for non-profit
radio such as NPR. Public radio would be required present a report to Congress
on how it should determine rates for their internet streaming media.
I hope this passes, not so much because local radio needs more competition,
but because this insane CRB ruling makes it nearly impossible for local broadcasters to
compete on the Internet at all. This would give everybody a fair chance and at
the same time bring in millions, perhaps billions in royalties for artists.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6ce3daa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterLarge regions of the globe have  been cooling or not warming in recent decades according to several new scientific papers.
A new paper shows the West Antarctic Ice Sheet (WAIS), sea surface temperatures near southern Chile, and the entire region between 50-70°S have cooled or not warmed since the early 1980s (Collins et al., 2019).
The region was more than 2°C warmer 1000 years ago and today’s temps (12.1°C) are the coldest of the last 2300 years (Collins et al., 2019).
Other new papers indicate the North Atlantic sea surface temperatures (SSTs) between southeast Greenland and Denmark have cooled at a rate of 0.78°C per decade since 2004 (Fröb et al., 2019).
The North Atlantic region (Labrador Sea to Icelandic Basin) hasn’t warmed overall (net) since the 1950s (Buckley et al., 2019), which includes no net warming of winter temperatures in Northern Europe and North America since the 1980s (Chen and Luo, 2019, Gan et al., 2019).
Should it be called “global warming” if it isn’t actually global?

Image Source: Collins et al., 2019


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Collins et al., 2019

Image Source: Collins et al., 2019

Image Source: Collins et al., 2019

Image Source: Fröb et al., 2019

Image Source: Buckley et al., 2019

Image Source: Chen and Luo, 2019

Image Source: Gan et al., 2019
Share this...FacebookTwitter "
"

It’s not often that I get to have a glass of wine at a restaurant in Chico and have WiFi Connectivity at the same time. Having both of these at the newly opened Market Cafe where Highway 32 meets 99 I decided I’d do my first “Live” blog entry.
Restaurants come and go in Chico, some don’t ever rise above the level of having an “open” sign. Finding new ones with ambiance and class is a treat. Finding one that gives every customer a free appetizing plate after 5 is even better.
This restaurant used to be “The Bean Scene” which was started by a couple of people I used to call “The Evil Blonde Ladies” because they’d come into Bidwell Perk and take notes on that business just before opening up their own gig. They didn’t make it, partly because it took them 20 minutes to toast a bagel.
So seeing a new restaurant in Chico is always a good thing, as on Friday and Saturday nights it’s often impossible to find a classy place to eat that isn’t fully booked. So while I’m never mentioned any Chico business before, I thought this one was worth a mention not only because they are new, but because its run by locals, Bob and Patricia Johansen.
I’m partial to wine blends, as they tend to take the edge off of the aftertaste. Tonight I sampled one of the best blends I’ve had in a very very long time. Its called Falling Star Merlot-Malbec (shown above), and I gotta tell you it beats my former favorite “Clous du Bois Marlstone” by a long shot, and costs about 66% less. ($19/bottle -vs- $60)
It’s worth checking out, if nothing else for the free appetizer and WiFi.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8aef9e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This mornig s session is all about drafting a set of suggestions to forward to other key members of the climate research community using the group knowledge gained from this conference. I have submitted my suggestion, and it has been accepted for inclusion in the publication. It reads:
 It has become clear that many surface weather stations, possibly a
significant number, may have undocumented biases that may or may not
be correctable using data analysis and data adjustment techniques.
After completion of weather station surveys for USHCN and other
networks, Why not identify the known good stations that have long term
records, few station moves, and no obvious microsite biases and
separate their data into a subset. Study the data and trends the known
good station subsets produce separately to see what can be learned.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea427e598',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Here are some notes on the tax proposals in the new federal budget: (See Table S-6; All figures are 10‐​year totals) 
"
"
Share this...FacebookTwitterBy Die kalte Sonne
The past winter in central and northern Europe was quite warm. Why is that? The Norwegian Centre for Climate Research CICERO explains it in an article from 6 January 2020:
Unseasonal temperatures for Norway
The unusual warm temperatures this winter and forecasts indicating milder winter conditions for January, February and March in Europe are partly due to an atmospheric circulation pattern called the North Atlantic Oscillation, or NAO. This atmospheric circulation pattern explains well the weather we get in Europe, especially in winter.
As explained in a CICERO-article from November 2019, seasonal forecast models are sometimes able to correctly forecast the phase of the NAO. 6 different seasonal forecast models are run at the beginning of each month by their respective weather centres from around the globe. The October simulations gave us a hint that we might get a positive phase of the North Atlantic Oscillation for November, December and January. The signal was quite strong, and 4 out of the 6 models were clearly in that direction, while 2 suggested normal winter conditions. The November simulations gave similar results, the majority of the models showed a positive NAO for December, January and February. Experts were a bit puzzled, as at the same time the snow cover over Siberia was already quite extensive, and the Arctic was very warm, two things that usually suggest a cold winter for Europe.
And then came the December simulations, the most recent ones, where all six models hinted to a positive NAO for January, February and March, and therefore milder conditions than normal for northern Europe in particular.
Read more at CICERO. An excellent report, which also applies to Central Europe.
For those who don’t know it yet: The NAO (North Atlantic Oscillation) controls the winter temperature in Northern and Central Europe. Positive NAO brings warm winters, negative NAO brings cold winters. Please note!
Now some of you will ask, what is this NAO actually? Well, it is the difference in air pressure between Iceland and the Azores. If the difference is big (pronounced low and pronounced high), then the NAO is positive (NAO+). If low and high are a bit thin, so the difference is smaller, then the NAO is negative. It’s as simple as that.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Why is it important? This pressure difference pushes the westerly winds a little bit more to the north or south. In an NAO+, the westerly wind belt lies further north and meets central Europe. This is where the humidity (wet February 2020, does anyone remember?) and the relative warmth of the Atlantic occur.
In a negative NAO (NAO-), the westerly winds blow further south and discharge their humidity as rain in Portugal and Spain.
For those who want to know more, the NAO website of the British MetOffice is recommended. We take the liberty of reproducing the two most important graphs of the website here. And this is how it looks with a positive NAO:

And this is how a negative looks:

And now, of course, you want to know where to check the current NAO status. To do so, simply google NAO and NOAA, or click on this NOAA page. There you can follow the last months of the NAO in high resolution. There is also a forecast for the next 2 weeks.
We see: In fact, the NAO was mostly positive during the winter. The forecasts for the next 2 weeks are not consistent. Bad luck. But if all models show a sharp downward trend in winter, you urgently need to buy road salt.
If you understand the NAO, you will get along better in life and in the climate change labyrinth. Finally we allow ourselves the question:
Why can’t the DWD German Weather Service explain such contexts to us?

Source: NOAA
Stay healthy!


		jQuery(document).ready(function(){
			jQuery('#dd_16efd3924a8804ec558ac63db78e3d5e').on('change', function() {
			  jQuery('#amount_16efd3924a8804ec558ac63db78e3d5e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Pictures have been coming in to www.surfacestations.org from many places. This one is from Fort Morgan, Colorado’s USHCN climate station of record. Fort Morgan is in the eastern plains of Colorado, about 100 miles northeast of Denver.
In such a place, with all that open space, you’d think it would be an easy matter to place something as important as an official NOAA temperature sensor used to contribute measurements to the national climatic database in some of that open space.
No such luck. In fact, the sensor recording the wide open plains has four air conditioners near it!

But lets not forget, in keeping with current observed trends, that any weather station with air conditioning also needs close-by parking.

It’s not like there’s no other open space to put the sensor in Fort Morgan.

The pictures above, courtesy of the Pielke Research Group shows an electronic Min/Max Temperature Sensor placed near a grain elevator office. Cable length limitations on this sensor have caused hundreds of similar placements in the USHCN network where Stevenson Screens used before could be placed a good distance away from such influences.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea556c7ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Image: Average temperatures warmed in nearly all parts of California between 1950 to 2000. Image credit: NASA/JPL/Cal State L.A Click for Larger Image
Average temperatures in California rose almost one degree Celsius (nearly two degrees Fahrenheit) during the second half of the 20th century, with urban areas leading the trend to warmer conditions, according to a new study by scientists at NASA and California State University, Los Angeles. Results of the study appeared in the journal Climate Research.
But 50 years of temperature trends hardly proves anything relevant about climate change, other than its gotten warmer in the past fifty years. 50 years in terms of our planet and the suns processes is a blink. I have to think that because NASA chose to co-author this paper with researchers at California State University, that some of the statewide “global warming as man-made problem bias” crept into the thinking for the purpose of this paper, i.e. “we need another study to show that its getting hotter so action is justified”.
What is troubling about this study is that many of California’s historical climatological stations, when done on a 100 year trend, rather than a 50 year trend, show a net cooling over the period, or a reversal of trend. The northern Sacramento Valley has very few reporting stations that go back 100 years, so I only have 4 data points, but it makes me wonder just what data the NASA/CSU study used to come to the conclusion that our area has warmed 1.1 degrees F over the last 50 years.
I’ve prepared some side-by-side graphs below of Sacramento Valley stations to illustrate that point:

My data source: U.S. Historical Climatology Network (USHCN) Data Set
Yet the NASA/CSU paper claims “The only area to cool was a narrow band of the state’s mainly rural northeast interior“. None of the stations above are in that area, but are in the North Sacramento Valley.
Even odder than that, cold and snowy Mt. Shasta, where you’d expect to hear about depleted snowpack, it’s melting glacier on the side of the mountain, and other “signatures” of “global warming” shows a significant drop in temperatures over the last 50 years. yet the NASA/CSU study for that area concludes that a 2.1 degree F rise in temperature occurred.

Granted a few data points don’t equal a complete study, but the fact that I’ve been able to find and plot in a couple of hours, several places that don’t match the trends in the NASA/CSU study calls their methodology into question. Note the cities I used are all small rural cities, but the NASA/CSU study plotted major, medium, and minor cities in California to draw their conclusions. From their own paper they admit that the areas that have grown the most have shown the greatest temperature increases:
Southern California had the highest rates of warming, while the NE Interior Basins division experienced cooling. Large urban sites showed rates over twice those for the state, for the mean maximum temperatures, and over 5 times the state’s mean rate for the minimum temperatures. Average temperatures increased significantly in nearly 54 percent of the stations studied, with human-produced changes in land use seen as the most likely cause. The largest temperature increases were seen in the state’s urban areas, led by Southern California and the San Francisco Bay area, particularly for minimum temperatures.
For example, look at Pasadena, CA once a small city itself,  but in the last 100 years it became a dot in the sea of the second largest American City, Los Angeles. It’s temperature trend, unsurprisingly, is sharply upward, for both the 50 and 100 year trends. Its drowning in a sea of asphalt and concrete, is it any wonder it shows a temperature increase?

The inescapable conclusion is that the NASA/CSU study is plotting the effects of urban heat islands, and applying that trend to the entire landmass of California to reach the conclusions they have mapped onto the state map of temperature trend they present.
A simple filtering based on urban growth factors would yield a temperature map with a far different result.
To their credit though, they recognize this fact:  “If we assume global warming affects all regions of the state, then the small increases our study found in rural stations can be an estimate of this general warming over land. Larger increases would therefore be due to local or regional changes in land surface use due to human activities.” 
For the most part, “urban warming” has dwarfed “global warming” in its magnitude, a fact that is lost on some who look at temperature data from weather stations worldwide and treat them all equally in the quest to prove a theory.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea757c22d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

There’s an article in The Oil Drum that focuses on electricity production; or rather how or what we will need to do to keep pace with people’s demands while balancing that with environmental and economic impact. It is lengthy but well-reasoned and good reading.
From the article: “One of the biggest threats the USA faces today is a serious shortage of energy. Vulnerabilities in our system have been made glaringly obvious several times; since the 1970’s the USA has had social and economic upheaval due to the actions of foreign oil producers, and two hurricanes in 2005 showed just how fragile our remaining domestic supplies of oil and natural gas are.”
The president recently reiterated a commitment to reducing our national oil consumption, and I hope that gets implemented as its really a good idea. Hybrids and electric vehicles are looking better and better. Chances are my next car will be one of these.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea86f23c6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterNesting red kite shot dead because of wind energy?
By Die kalte Sonne
(Text translated by P. Gosselin)

Red kites have little chance against wind turbines. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
Red kites and wind power just do go well together. These predatory birds can find good prey especially where farmers mow meadows or plow fields. Lethal are cases such as the one in Baden-Württemberg, where areas with green fodder have been planted in the immediate vicinity of a wind park.
When these fields are mowed, the red kites search for food within the hay. It is ideal for them, but also possibly deadly because they cast their view downward when hunting, and not forward. The Hilpensberg wind farm was even approved in a red kite area. Now one of the beautiful animals has fallen victim again, as the Nature Conservation Initiative reports:
According to biologist Immo Vollmer, the conclusion can only be that we should not build any more wind turbines in areas where red kites nest or where buzzards often seek food. Otherwise the red kite, which has its largest distribution center in the world in Germany, will have no future here, because the loss rate is already almost in the same order of magnitude as the rate of offspring.”
And another sad case has just been reported in North Rhine-Westphalia. A female, nesting red kite was shot dead near Paderborn.
In an earlier trial, a judge even gave the controversial wind projects approval – precisely where the shot bird was found – under the condition that no protected species be proven to exist there. Now that the animal has been executed, this condition has been met. Probably just a coincidence, or maybe suicide, to make the wind turbines possible and to get out of the way?


		jQuery(document).ready(function(){
			jQuery('#dd_9aefd3a91f352f2c5132dfecb2a10391').on('change', function() {
			  jQuery('#amount_9aefd3a91f352f2c5132dfecb2a10391').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
The picture below is from Oregon State Climatologist George Taylor. You may have heard of him, the Governor of Oregon tried to get him fired for not jumping on to the global warming bandwagon because he doesn’t see enough supporting evidence.

The picture is of Forest Grove, Oregon, and the temperature plot below shows how it is warming. But George says:
“Yes, it’s a window air conditioning unit to the east and the edge of a large asphalt parking lot to the north, northwest, and west. The pic is shot looking northeast. For those of you that may not immediately realize this, air conditions exhaust hot air to the outside.

Not only that, but Forest Grove is located in Washington County, Oregon’s fastest-growing county (in terms of population growth, not percentage) for the last 40 years. No wonder it’s seeing unprecedented high temperatures…”
It looks like the air conditioner may have been installed around 1985, notice the sustained 1 degree jump that started about then and sustained a plateau.
And this is a station of record, a US Historic Climatology Network station that is used in global climate models by NASA, in fact the plot is from that database.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea603b6f9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGrand media deception
So typical of climate science. Everything in and around it gets wildly exaggerated in order to feed media consumption and deceive the public. Reports are emerging that the “500,000 people” crowd awaiting Greta in Madrid was in fact as low as 15,000, according to Spanish federal police.
That would make the 500,000 claimed figure a 3000% exaggeration!

Ms. Thunberg, media, claim crowd in photo was 500,000 people. But Spanish federal police say the crowd was closer to 15,000. Image: Greta Thunberg.
=========================================
Greta demonstration in Madrid: Crowd number – 97% is exaggeration?
By A. R. Göhring
(German text translated/edited/supplemented by P Gosselin)
After Greta Thunberg’s arrival in Madrid, there was an “unprecedented demonstration” for climate protection in the Spanish capital, supposedly involving 500,000 participants.
But the federal police say there were about 15,000 (!) demonstrators.
At German news outlets, people could read that Greta Thunberg at first had to interrupt participating at the demonstration in Madrid – because of so many people and security could not be guaranteed. Later she stood on a stage with famous Hollywood actor Javier Bardem and gave a speech.
There were, as usual, no scientific facts to be found, but a lot of feeling, self-righteousness and general demands to an ominous elite, who officially stand behind Thunberg’s FFF.

The leaders are betraying us. Enough is enough. Change comes whether you like it or not. We want to see action.” [translated from the German]

Bardem seconded: “We only have ten years to mitigate the worst effects of climate change.” He also hit the Mayor of Madrid, José Luis Martinez-Almeida, and President Donald Trump for their “stupid” measures against climate change.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sources revealing much smaller numbers
The Spanish media, but also the Austrian ORF, are now discussing falsified numbers of participants. The mass media had spoken of 500,000 demonstrators. That would mean that Greta caused one of the largest climate protection demonstrations on the planet in Spain.
Largely empty seats in Katowice in 2018
Those who remember the coverage of the world climate conference COP24 in Katowice, Poland, know that Thunberg’s “breakthrough”, an emotional and panic-stricken speech in front of a “large” audience, had already been manipulated. In fact, the 15-year-old Swede spoke in front of largely empty rows of chairs, which was not visible in the media due to the skillful selection of photographic and film perspectives.
Police figure: only 15,000
Therefore the brash falsification of the numbers is not surprising. The organizers of the demonstration spoke of half a million participants, but the large newspaper El País spoke of 25,000 to 35,000, and the Spanish federal police of only about 15,000 people with the help of helicopter images.

Excerpt screen shot from El Pais.
Experience has shown that the police always give quite low numbers of demonstrators, but the unusual use of aerial photographs makes this figure seem convincing.
If one calculates the real number percentage of the FFF fantasy figure, then you come up with three percent. 97% of the claimed figure is presumably a lie. Demonstration organizers like to give slightly higher figures, but 30 times above reality is already extraordinary, especially in a globally important political PR campaign filmed by numerous cameras.
The knowledgeable climate sceptic is also quite familiar with the 97% figure – with respect to ex-President Barack Obama and climate psychologist John Cook. One sees how huge and audacious exaggerations / counterfeits are part of the climatic profiteering business. Isn’t that risky? Wouldn’t it be better to manipulate more cautiously so that there’s less to be accused of?
Obviously, those who don’t panic enough achieve less. Moreover, even critical citizens often would not imagine that someone would publicly falsify in such a brazen manner.
Share this...FacebookTwitter "
"

In an historic event next week, Pope Francis will make his first visit to the United States. It is expected to generate as much political interest as it will religious concerns. On Thursday, he will address a joint session of Congress, and on Friday he will speak to the United Nations General Assembly. He is widely expected to focus on climate change, a topic on which he shares much political ground with President Obama.



This will not be the first time Pope Francis has ventured into the global warming debate. The June 2015 release of his encyclical “Laudato si” marked his initial foray into the discussion. Therein, Pope Francis echoed President Obama’s tune, claiming there exists “solid scientific consensus” that human activities are causing a “disturbing warming” of the climate, which left unchecked will result in a type of planetary Armageddon manifested by escalating temperatures, melting polar ice caps, rising seas, more frequent and more severe weather, ecosystem degradation, and plant and animal extinctions, all of which he claimed will severely affect humanity.



Given that this was the pope’s stated position on global warming a mere three months ago, look for a familiar refrain to accompany his remarks in Washington and New York next week. He will likely repeat a challenge first issued in his June encyclical, which called for humanity to “recognize the need for changes of lifestyle, production and consumption, in order to combat this warming,” which he believes is “aggravated by a model of development based on the intensive use of fossil fuels.”





Contrary to what Pope Francis says, fossil fuels are good for the poor and the Earth.



 **The Consensus Isn’t**



But are the pope’s concerns over potential global warming based upon the best available science? Or are they significantly overinflated? Is the biosphere rapidly spiraling downward toward planetary Armageddon? Or is it marching forward toward biospheric rejuvenation? Is limiting fossil fuel use a policy prescription panacea? Or is it a recipe for social and economic disorder and regress?



With respect to the science, those who promulgate a fear of planetary Armageddon often conveniently fail to disclose that literally _thousands_ of scientific studies have produced findings that run counter to their view of Earth’s climatic future. As just one example, and a damning one at that, _all_ of the computer models upon which this vision is based failed to predict the current plateau in global temperature that has continued for nearly two decades now. That the Earth has not warmed significantly during this period, despite an 8 percent increase in atmospheric CO2, is a major indictment of the models’ credibility in predicting future climate, as well as the assertion that debate on this topic is “settled.”



Numerous other problems with the apocalyptic vision of our future climate have been filling the pages of peer‐​reviewed science journals for many years now, evidenced most forcefully by the work of the Nongovernmental International Panel on Climate Change, which has highlighted the results of thousands of scientific studies challenging the alarmist and model‐​based vision of the planet’s future. This large and well‐​substantiated alternative viewpoint contends that rising atmospheric CO2 emissions will have a much smaller, if not _negligible_ , impact on future climate, while generating several biospheric _benefits_.



 **Global Warming Could Be Good**



Concerning such benefits, it is a well‐​established _fact_ that atmospheric CO2 is the major building block of nearly all life, as it is used by plants in the process of photosynthesis to construct their tissues and grow. As numerous scientific studies have conclusively demonstrated, the more CO2 there is in the air, the better plants grow. They produce greater amounts of biomass, become more efficient in using water, and are better able to cope with environmental stresses such as pollution, drought, salinity, and high temperatures.



The implications of these benefits to society are enormous. One study, for example, calculated that over the 50‐​year period of 1961 to 2010, the direct monetary benefits atmospheric CO2 enrichment conferred on global crop production amounted to a staggering $3.2 trillion. Projecting this positive externality forward in time reveals it will likely bestow an additional $9.8 trillion in crop production benefits between now and 2050.



By ignoring these realities, policy prescriptions calling for a reduction in fossil fuel use are found—on this basis alone—to be ill‐​advised. Yet there are still other important reasons to reject them.



 **The World Needs More Energy, Not Less**



We live in a time when approximately half the global population experiences some sort of limitation in accessing the energy they need for the most basic of human needs, including the production of clean water, warmth, and light. One‐​third of those thus impacted are children. An even greater portion finds its ranks among the poor. How can a society turn its back on these individuals and deny them the right to increase their energy and fossil fuel use so that they can increase their living standards? It is reprehensible to even consider such an action and it is certainly morally wrong to do so. The world needs _more_ energy, not less.



Taxing or regulating CO2 emissions is an unnecessary and detrimental policy option that should be shunned. Why would any government or religious institution advocate to increase regulations and raise energy prices based on flawed computer projections of climate change that will never come to pass? Why would any government or religious institution advance policy that seeks to destroy jobs, rather than to promote them? Why would any government or religious institution want to deny increasing energy access to those in the world who are in most need of it? And why would national governments or religious institutions actually “bite the hand that feeds them”?



It is high time for our world leaders to recognize and embrace the truth. Contrary to misguided assertions, political correctness, and even government or religious edicts, carbon dioxide is _not_ a _pollutant_. Its increasing concentration only minimally affects our climate, while offering great benefits to the biosphere. Efforts to regulate and reduce CO2 emissions will economically burden society and yield little to no measurable impact on Earth’s climate.
"
"
Today I visited my friend Jim Goodridge, former California State Climatologist and the man with a garage full of data going back to before the Gold Rush.
He’s been quietly toiling away in his retirement on his computer for the last 15 years or so making all sort of data comparisons. He gave me two CD ROMS full of data that I’m just now wading through. One plot which he shared with me today is a 104 year plot map of California showing station trends after painstakingly hand entering data into an Excel spreadsheet and plotting slopes of the data to produce trend dots.
He used every good continuous piece of data he could get his hands on, no adjusted data like the climate miodelers use, only raw from Coopertive Observing Stations, CDF stations, Weather Service Offices’s and Municipal stations.
The results are quite interesting. Here it is:

Squint hard and you can see a pattern emerge.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea610a013',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

An article today in BRIDGES Weekly Trade News Digest ( _What? You don’t subscribe??_ ) contains an explicit rejection by India’s trade minister of the idea that carbon border tax adjustments belong in the WTO’s agenda. Border tax adjustments in this context refers to _de facto_ tariffs that would “level the playing field” for domestic producers competing with foreign producers not subject to climate change policies of an equivalent rigour, also called “border carbon adjustments” or variations on that theme.   
  
  
While Minister Khullar predicts that these sorts of measures will be in place in 2–3 years time, he rejects that the WTO is the forum to deal with environmental issues.   
  
  
Furthermore, countries introducing such measures can expect litigation: 



India and other developing countries will undoubtedly challenge the true impetus behind the [border carbon adjustment] measures.



“Such measures imposing restrictions on imports on the grounds of providing a ‘level playing field’, or maintaining the ‘competitiveness’ of the domestic industry, etc are likely to be viewed as mere protectionist measures by the developed world to block the exports of the poorer nations,” [a recent report from an Indian think‐​tank closely connected with the Indian government] reads. “This is because there is little empirical evidence that companies relocate to take advantage of lax pollution controls.”   
  
  
The [report] argues that such unilateral trade measures will inevitably lead to tit‐​for‐​tat trade retaliation that could spiral into an all‐​out trade war. Such warnings have also been raised by China and several think tanks following the issue.



I’ve written before on the dangers of introducing climate change issues into the WTO (and Dan Griswold has written more broadly on why labor and environmental standards don’t mix well with the aim of freeing trade) but this is yet another firm, unequivocal warning to developed countries that their proposals (and they are still just proposals at this stage) will have consequences. Developed country politicians who insist on forcing rich‐​world standards on the poor world should listen carefully.
"
"
Share this...FacebookTwitterClouds regulate Earth’s climate. New studies suggest uncertainty in clouds’ surface radiative effects reach 17.4 W/m² per year (±8.7 W/m²/year). Total CO2 climate forcing is said to be just 0.02 W/m² per year. The difference in these magnitudes preclude detection of a CO2 signal in climate forcing.
According to Feldman et al. (2015), a 2 ppm increase in CO2 per year (22 ppm over the 11 years from 2000-2010) results in a surface radiative forcing influence of 0.02 W/m², or 0.2 W/m² per decade. This is said to be just “ten per cent of the trend in downwelling longwave radiation” when clouds and water vapor are considered.
In contrast, the influence of clouds in total longwave forcing is substantially larger, with radiative forcing trends reaching ±4 W/m² per decade.
From 1978 to 2010, the total longwave anomalies reached amplitudes of about ±2 W/m² per year (Loeb et al., 2012, below image). These anomalies dwarf the 0.02 W/m² per year radiative influence from CO2, and thus factors other than CO2 must be driving the variability.
Further, the overall trend in longwave or greenhouse effect forcing appears to have been flat during this 30-year period. CO2’s 0.2 W/m² per decade contribution may have had no net impact on the trend.

Image Source: Loeb et al., 2012
Even if a CO2 forcing signal was detectable in an overall trend over the last few decades, it would be lost amid the uncertainty and noise of the radiative effects of clouds.
According to L’Ecuyer et al., 2019, the global annual net cloud radiative effect (CRE) at the Earth’s surface is estimated to be -24.8 ±8.7 W/m². In other words, when cloud cover increases, surface temperatures cool because the net shortwave effects of cloud (-51 W/m²) exceed the net longwave effects of cloud (+26.3 W/m²). The uncertainty value associated with this overall surface forcing estimate, ±8.7 W/m², has a range of 17.4 W/m².
If CO2’s net radiative effect in surface forcing is 0.02 W/m² per year and the uncertainty the radiative effects of clouds is ±8.7 W/m² per year, this means that uncertainty is 870 larger than the CO2 influence.
A CO2 forcing signal is therefore not detectable in the Earth’s energy balance.

Image Source: L’Ecuyer et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_717fa080377e4740ab19829655e23af7').on('change', function() {
			  jQuery('#amount_717fa080377e4740ab19829655e23af7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOn May 26, the World Meteorological Organization (WMO) issued a press release warning of “another record-breaking heat season” for the northern hemisphere this summer, along with the potential of the COVID-19 pandemic amplifying the health risks of the hot weather.
Media outlets picked up the WMO warnings and spread panic stories of mayhem and climate breakdown among the public.
But veteran Swiss meteorologist Jörg Kachelmann, citing models from the ECMWF, doesn’t see any evidence of another “record breaking summer”. He tweeted:

Dear @WMO I've wondered since you wrote thishttps://t.co/2JEahOzwIi
What forecast data led you to the conclusion that we could expect the hottest summer for the Northern hemisphere ever in this year. Wherever I look, I can't see any evidence.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Can you help me? Thanks. pic.twitter.com/zMrlZRtCTC
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 12, 2020

So far Kachelmann has yet to receive an explanation from the WMO.
Like the ECMWF model for the next 45 days shows, the northern hemisphere has extensive cool patches, and so no signs of a “record breaking northern hemisphere summer this year.
Whether it’s the World Health organization (WHO or the WMO, global institutions set up to guide policy are doing a lousy job and are in need of extensive reform, as some leaders have already called for.
Share this...FacebookTwitter "
"

From Slashdot.org The Wall Street Journal has a sobering piece describing the research of
medical scholar John Ioannidis, who showed that in many peer-reviewed research
papers ‘most
published research findings are wrong.’ The article continues: ‘These flawed
findings, for the most part, stem not from fraud or formal misconduct, but from
more mundane misbehavior: miscalculation, poor study design or self-serving data
analysis. […] To root out mistakes, scientists rely on each other to be
vigilant. Even so, findings too rarely are checked by others or independently
replicated. Retractions, while more common, are still relatively infrequent.
Findings that have been refuted can linger in the scientific literature for
years to be cited unwittingly by other researchers, compounding the errors.’


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3ddae1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFew places virtue signal green as much as Germany.
So not surprisingly a number of cities led by socialist/green governments have attempted to implement electric public transportation buses, declaring they are the future of clean mobility.

Electric powered buses still struggling to be successful. Image: Flixbus
But Tichy’s Einblick just recently reported on the results of attempted electric bus fleets across Germany. They are not pretty.
Electrically driven public transport by bus is still a long way off.
FlixBus suspends electric bus after “repeated technical problems”
One example, Tichy’s Einblick cites, is German intercity bus carrier FlixBus, which worked with Greenpeace to promote the electric bus on the route between Mannheim and Frankfurt as a showcase project – all accompanied by ample fanfare and slogans such as “sustainable travel” and “the mobility of the future is green”.
But last April Greenpeace reported the discontinuation of the first nationwide electric long-distance bus line: “The long-distance bus provider announced on Wednesday that there had been repeated technical problems during the pilot project between Mannheim and Frankfurt with the vehicle of a Chinese manufacturer. These problems with the bus of Chinese manufacturer BYD must have been so massive that the project was suspended.”
Tichy’s Einblick reports the “only one thing that was really sustainable about the project was the disappointment of the travelers.”
Wiesbaden: 45 million euros, only on flat routes
Tichy’s Einblick also looked at the city of Wiesbaden where city bus operator ESWE Stadtwerke ordered 56 electric buses in April of this year and the first five were to run in October, with another five to follow in November. But so far none have “made any further progress yet”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Wiesbaden plans a total of 140 electric buses, all to be supported by 45 million euros in taxpayers’ money from the Federal Environment Ministry. Yet Tichy’s Einblick reports that only the “flat inner city stretches are to be served” and that “they do not dare to venture onto the steeper streets on the outskirts.”
Also: “A battery charge should last 150 kilometers, but less if the temperatures are freezing and the bus is full: 100 kilometers.” That would mean about 3 hours of service before a recharge becomes needed.
Nürtingen electric bus pilot “a flop”…battery 80,000 euros!
In the city of Nürtingen, “The electric bus pilot project is a flop,” says Tichy’s Einblick. “The battery on a bus was broken after two and a half years of operation, and a new one costs 80,000 euros. Too much – that’s why the operation is stopped.”
Trier: buses taken out of service after just 2 weeks
Tichy’s Einblick also sarcastically reports that since July, “The electric buses in Trier have proven to be truly quiet and environmentally friendly: They are idol.”
“Already after two weeks the first electric bus had to stay in the workshop. Reason: Problems with the battery. An end of the problems is not in sight,” according toTichy’s Einblick.
Bremen backs off electric bus plans: “many disadvantages”
The failures of the electric buses on German streets has not gone unnoticed. Even the Green/Socialist government of the northern city of Bremen has made “a 180-degree turn”.
Earlier the city had planned to purchase five electric buses, 40 percent of which were to be funded by the federal government. But the Bremen city government opted out of the plan. “The reason: It is still unclear whether the electric drive really is the technology of the future.”
According to Tichy’s Einblick: “Bremen Mayor Maike Schaefer (Green Party) says that e-mobility has many disadvantages: ‘The batteries need cobalt, which comes from mines in the Congo. Exploitative child labor prevails there.'”
“Currently, the electric vehicles also have such a short service life that their carbon footprint does not represent any real progress compared with conventional technologies,” writes Tichy’s Einblick.
Share this...FacebookTwitter "
"
In the UK Channel 4 produced a new documentary titled:
The Great Global Warming Swindle  This is well worth watching, especially if you’ve ever doubted the veracity of such claims, no matter which side you find yourself on. 
Through interviews with prize-winning climate experts and others, this masterful documentary explains the origins of global warming alarmism; factually addresses claims of man-made global climate change; exposes the motivations of organizations, scientists and activists sounding the alarm; and explains why it’s been extremely difficult, if not downright career killing, for scientists to question global warming orthodoxy publicly.
While presenting hard facts, it is artfully done, making it watchable for the layman and scientist alike.
You can watch the video here. Its about 75 minutes. You can press the Play button and Pause button if you need a break. If the video player below doesn’t work, here is a <a href=”http://www.youtube.com/watch?v=XttV2C6B8pU”direct link
&nbsp


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7a37040',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
By Kirye (photo)
and Pierre Gosselin
Today we look at the mean annual temperatures of western USA stations that have a Brightness Index (BI) of 0, meaning they are not subjected to urban heat island impacts.
Many people are claiming that temperatures worldwide are rising due to greenhouse gas emissions from human activities.
First we begin with the station located at the town of Fort Bragg in California. Using NASA data, we plot the annual temperatures going back to 1935!

Data source: NASA GISS 
Above we plot the V4 unadjusted versus the V4 adjusted data. Neither show any warming since 1935. The adjusted data, however, turns a cooling trend into one of no cooling.
Recently I tweeted an animation that compares the v4 unadjusted data to the V4 adjusted data for the Beowawe station in the state of Nevada:

GHCN V4 Unadjusted data show Beowawe, State of Nevada has had a cooling trend since 1891!Needless to say, NASA changed the data by a large margin.https://t.co/XfEZRXWoFl~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/bDIleXLYuB
— キリエ (@KiryeNet) May 10, 2020

Note how the Beowawe data of the past was substantially altered (reduced) in order to create a warming trend from a previously cooling trend. Here the warming is man made – but statistically by researchers at NASA.
The story is similar for 4 other stations located in the western US.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the Manti station in Utah, modest warming was adjusted to created more warming:

Data source: NASA GISS
The same is true for the Seligman, Arizona station:

Data source: NASA GISS. 
The mean annual temperatures measured by the Cheesman, Colorado station used to show a cooling trend since 1903, before NASA tampered with the data and changed them into a warming trend: 

Data source: NASA GISS.
Finally we look at the data from the station for Hachita, New Mexico:

Data source: NASA GISS.
Here for Hachita, NASA changed the data so that modest warming was changed to produce greater warming.
Why do the new, adjusted data plots always end up warmer and never cooler? This seems to be Deep State science, and not real science which the public expects to get and is owed.


		jQuery(document).ready(function(){
			jQuery('#dd_016dc3091d95560b9168dfc61dda50bc').on('change', function() {
			  jQuery('#amount_016dc3091d95560b9168dfc61dda50bc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAt his rally in Hershey, Pennsylvania, President Donald Trump was at the top of his game, doing what he does best: trolling his floundering opponents. The US president is ripping his Democrat rivals like a lion mauling hyenas.
At his Hershey speech, 31:00 mark video below, Trump brings up the climate and energy issue, ridiculing wind energy and alarmist climate science just as COP25 takes place in Madrid amid declarations of a state of emergency in Europe and pledges to go carbon neutral.

“You’d have windmills all over the place if you had Crooked Hillary. They’d be knocking off birds left and right,” said Trump before a packed crowd.
President sees through global warming sham
“Darling, I wanna watch television tonight, and there’s no damn wind. What do I do?” Trump mocked wind energy before a laughing audience, who applauded enthusiastically. Trump even imitated the noisy, unreliable contraptions that litter the landscape.
“I wanna watch the election results. Darling, there’s no wind, the damn wind just isn’t blowing like it used to because of global warming, I think,” Trump mocked. “I think it’s global warming. Global warming – no more wind. No more life!”
The President then trolled the climate alarmists, sarcastically warning: “The oceans are gonna rise an eighth of an inch within the next 250 years,” said Trump, feigning breathlessness. “We’re gonna be wiped out!”
“Clean air, clean water”
The President then conveyed to the audience that the focus needs to be on clean air and clean water, and without shutting down industries and causing job losses.
Share this...FacebookTwitter "
"
I just finished a 150+ mile round trip from Boulder to get Dillon, CO and Cheesman Reservoir USHCN sites in addition to the Boulder NIST/NOAA site.
Cheesman had recently been flooded due to heavy runof from forest fire, the roads were mudpits, and even with 4WD I rented couldn’t get there before sunset. So gave up and returned to hotel at DIA for flight out tomorrow.
Had Vietnamese food with Pielke’s group last night, and that didn’t help my day either. I’m pretty toasted. But it was a heckofa good day even so.
So I’m signing off for a couple days for travel back home and some R&R.
The good news; While driving back on US285 I had another citizen science project idea to disprove Parker’s  2004 and 2006 papers essentially saying “UHI is minimal or doesn’t exist”, which I believe is unsupportable. I think it will work. Got to mull it over. Check back in a day or two.  Pictures and presentation coming when I get back to normal schedule.
Anthony out


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea40bc5a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
Within the last few years, over 50 papers have been added to our compilation of scientific studies that find the climate’s sensitivity to doubled CO2 (280 ppm to 560 ppm) ranges from <0 to 1°C. When no quantification is provided, words like “negligible” are used to describe CO2’s effect on the climate. The list has now reached 106 scientific papers.


Link: 100+ Scientific Papers – Low CO2 Climate Sensitivity


A few of the papers published in 2019 are provided below.
Krainov and Smirnov, 2019  (2X CO2 = 0.4°C, 2X anthroCO2 = 0.02°C)
“The greenhouse phenomenon in the atmosphere that results from emission of its molecules and particles in the infrared spectrum range is determined by atmospheric water in the form of molecules and microdrops and by carbon dioxide molecules for the Earth atmosphere and by carbon dioxide molecules and dust for the Venus atmosphere. The line-by-line method used the frequency dependent radiative temperature for atmospheric air with a large optical thickness in the infrared spectral range, allows one to separate emission of various components in atmospheric emission. This method demonstrates that the removal of carbon dioxide from the Earth’s atmosphere leads to a decrease of the average temperature of the Earth’s surface by 4 K; however, doubling of the carbon dioxide amount causes an increase of the Earth’s temperature by 0.4 K from the total 2 K at CO2 doubling in the real atmosphere, as it follows from the NASA measurements. The contribution to this temperature change due to injections of carbon dioxide in the atmosphere due to combustion of fossil fuel, and it is 0.02 K. The infrared radiative flux to the Venus surface due to   CO2 is about 30% of the total flux, and the other part is determined by a dust.”

Image Source: Krainov and Smirnov, 2019

Ollila, 2019 (2XCO2= 0.6°C)
“If a climate model using the positive water feedback were applied to the GH effect magnitude of this study, it would fail worse than a model showing a TCS value of 1.2°C. If there were a positive water feedback mechanism in the atmosphere, there is no scientific grounding to assume that this mechanism would start to work only if the CO2 concentration exceeds 280 ppm, and actually, the IPCC does not claim so. The absolute humidity and temperature observations show that there is no positive water feedback mechanism in the atmosphere during the longer time periods. … The contribution of CO2 in the GH effect is 7.3% corresponding to 2.4°C in temperature. The reproduction of CO2 radiative forcing (RF) showed the climate sensitivity RF value to be 2.16 Wm-2, which is 41.6% smaller than the 3.7 Wm-2 used by the IPCC. A climate model showing a climate sensitivity (CS) of 0.6°C matches the CO2 contribution in the GH effect, but the IPCC’s climate model showing a CS of 1.8°C or 1.2°C does not.”

Varotsos and Efstathiou, 2019
“The enhancement of the atmospheric greenhouse effect due to the increase in the atmospheric greenhouse gases is often considered as responsible for global warming (known as greenhouse hypothesis of global warming). In this context, the temperature field of global troposphere and lower stratosphere over the period 12/1978–07/2018 is explored using the recent Version 6 of the UAH MSU/AMSU global satellite temperature dataset. Our analysis did not show a consistent warming with gradual increase from low to high latitudes in both hemispheres, as it should be from the global warming theory. … Based on these results and bearing in mind that the climate system is complicated and complex with the existing uncertainties in the climate predictions, it is not possible to reliably support the view of the presence of global warming in the sense of an enhanced greenhouse effect due to human activities.”

Image Source: Varotsos and Efstathiou, 2019Share this...FacebookTwitter "
"
Share this...FacebookTwitter
Image: NASA Earth Observatory. Public Domain
Prof. Fritz Vahrenholt’s Monthly Solar Report
The global mean temperature in April 2020 was again significantly lower than in February and March, at 0.38°C above the average from 1981 to 2010. The average temperature increase on the globe from 1981 to February 2020 was 0.14°C per decade. The further development promises to be interesting, especially since a number of research institutes expect a higher probability of a cooling La Nina in the Pacific towards the end of the year. March’s solar activity was very low with a sunspot number of 1.5.  Activity in April rose slightly to 5.4. The first sunspots of the new cycle are showing.
What causes the sun to have an 11-year cycle?
Since the Dessau pharmacist Heinrich Samuel Schwabe discovered in 1843 that the sunspots of the sun increase and decrease in an 11-year cycle, science has been puzzling over the reason why this cycle lasts 11 years and why the solar magnetic field also changes its polarity in this rhythm: the north pole becomes the south pole and vice versa.
In July last year, scientists at the Helmholtz Centre in Dresden Rossendorf made a little-noticed but exciting discovery. Every 11.07 years, the planets Venus, Earth and Jupiter are aligned quite precisely. At this point in time, their gravitational force acts jointly in one direction on the Sun.
“The agreement is amazingly accurate: we see a complete parallelism with the planets over 90 cycles,” explains Frank Stefani, one of the authors of the publication published in Solar Physics. Just as the gravitational pull of the Moon causes the tides on Earth, planets could move the hot plasma on the surface of the Sun. But the effect of a simple gravitational force is too weak to significantly disturb the flow in the Sun’s interior, so the temporal coincidence has long been ignored.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now the researchers assume that the layers of the plasma are subject to a Taylor instability. The Taylor instability is known from the behavior of liquids of different densities at their interface (we know the turbulence that occurs when milk is poured into a cup of tea).  Taylor instability is sensitive to even very small forces. A small burst of energy is enough for the polarity of the solar magnetic field to swing back and forth every 11 years. The necessary impulse for this could be provided by the tidal action of the planets – and thus ultimately determine the rhythm in which the sun’s magnetic field reverses its polarity.
The tidal forces of the planets could have other effects on the Sun in addition to their role as pace-setter for the 11-year cycle. For example, it would be conceivable that they could change the stratification of the plasma in the boundary area between the inner radiation zone and the outer convection zone of the Sun, the tachocline, in such a way that the magnetic flux could be more easily dissipated.
Under these conditions, the strength of the activity cycles could also change, just as the “Maunder Minimum” once caused a significant decrease in solar activity over a longer period, the researchers write on the Helmholtz Center website. It is an unusual idea that the activity of the sun is controlled by the planets, including the earth itself. This sounds like astrology – but it is the latest in solar research.
One of the first researchers who assumed an influence of the solar activity by the planets was Theodor Landscheidt, who already in 1988 in his book “Sun-Earth-Man” predicted the decreasing strength of the solar cycles 22 and following. However, he assumed a different mechanism, according to which the planets cyclically move the sun out of the center of gravity (barycenter) of our solar system. Landscheidt died in 2004.
And also in our book “The Forgotten Sun” we had invited Prof. Nicola Scafetta for a separate chapter, who already then interpreted the conjunction of Saturn and Jupiter as the cause of a 60-year cycle. In a publication published in Solar Physics in February 2020, he also relates the longer-term oscillations (Hallstatt -2400 years ,Eddy – 1000 years, Suess-de Vries – 210 years) to influences of the large planets of Jupiter, Saturn, Uranus and Neptune. The long version is accessible here.
Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_51502fcc0d8cc70a6aed31f4b468444e').on('change', function() {
			  jQuery('#amount_51502fcc0d8cc70a6aed31f4b468444e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

This picture comes to me via www.surfacestations.org courtesy of Dr. Roger Pielke Sr. of the University of Colorado.
It is the US Historical Climatological Network (USHCN) Station of Record for Hopkinsville, KY. The NOAA provided Max/Min Temperature Sensor is located at the observers home. The nearby air conditioner is just 10 feet from the temperature sensor. Then there’s the chimney. The contribution of the portable BBQ grill to the temperature record is unknown.
The MMTS temperature sensor wasn’t always mounted on the tower next to the house, it used to be in the yard, but the observer made some “improvements” over time. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5a35352',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
An astute letter to the editor writer in Arkansas has found the reason for global warming:

This actually happened, as attested to on the rumor/urban legend verifcation website Snopes.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6b2c1e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I don’t know why I’m posting this other than its how I feel today.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7356857',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’ve long argued that enviros don’t have anywhere near the electoral clout most people think and that no one is going to gain much political capital donning the garb of “Mr. Green Jeans.” Today, the trade publication Greenwire (subscription required) agrees. And believe me, these are the last people who want to make this argument. 



**CAMPAIGN 2006: Voters cool to climate issue in torrid midterm races**   
  
  
**Darren Samuelsohn, _Greenwire_ senior reporter**   
  
  
Five Northeastern Republicans facing fierce re‐​election battles turned just before the latest congressional recess to global warming in hopes the issue would boost their chances in their suburban House districts.   
  
  
But the lawmakers apparently got little traction from climate change in a campaign dominated by voter concerns about the Iraq war, President Bush’s unpopularity and overall dissatisfaction with Republican leadership.   
  
  
“It’s been very difficult for any of these incumbents whose problems are bigger than themselves, or whose problems have been themselves,” said Bernadette Budde, a senior vice president for the Business and Industry Political Action Committee. “They have had a hard time changing the subject.”   
  
  
The five — Reps. Curt Weldon (Pa.), Mike Fitzpatrick (Pa.), Christopher Shays (Conn.), Nancy Johnson (Conn.) and Rob Simmons (Conn.) — cosponsored in September what some consider the most aggressive bill to date aimed at limiting heat‐​trapping greenhouse gas emissions. The bill’s lead sponsor is Rep. Henry Waxman (D‐​Calif.), the presumed new chairman of the House Government Reform Committee if his party wins a majority of House seats.   
  
  
“Doing it before Congress goes off to campaign is telling,” said Howard Reiter, chairman of the political science department at the University of Connecticut. He added that global warming is a nuanced subject that comes with an important caveat: It may require constituents to make sacrifices in their day‐​to‐​day lives.   
  
  
“The problem with global warming is its incremental,” Reiter said. “It’s not as if there’s an immediate crisis people can see.”   
  
  
Massie Ritsch, spokesman for The Center for Responsive Politics, a nonpartisan organization that tracks campaign spending, said the recent media frenzy over climate change — from Hollywood‐​style documentaries to mainstream press coverage — did little to stir voters this year. “For all of the attention Al Gore’s movie got, it hasn’t stayed a major election issue,” he said.   
  
  
The lack of voter interest in climate change is not due to a lack of effort from environmental groups .…   
  
  
_Reporter Michael Burnham contributed to this report._
"
"

New York Attorney General Eric Schneiderman demands out-of-state charities disclose all donors for his inspection. He does not demand this of all charities, only those he decides warrant his special scrutiny. Schneiderman garnered national attention for his campaign to use the powers of his office to harass companies and organizations who do not endorse his preferred policies regarding climate change. Now, it seems he seeks to do the same to right-of-center organizations that might displease him. Our colleague Walter Olson has cataloged Schneiderman’s many misbehaviors.   
  
He’s currently set his sights on Citizens United, a Virginia non-profit that produces conservative documentaries. While Citizens United has solicited donations in New York for decades without any problem, Schneiderman now demands that they name names, telling him who has chosen to support the group. Citizens United challenged this demand in court, arguing that to disclose this information would risk subjecting their supporters to harassment and intimidation.   
  
These fears are not mere hyperbole. If the name Citizens United rings a bell, it’s because the organization, and the Supreme Court case of the same name, has become the Emmanuel Goldstein of the American left, complete with Democratic senators leading a ritualistic two minutes hate on the Senate floor. In 2010, the Supreme Court upheld its right to distribute _Hillary: The Movie_, and ever since “Citizens United” has been a synecdoche for what Democrats consider to be the corporate control of America. Is it unwarranted to think that their donors might be subjected to the sort of targeted harassment suffered by lawful gun owners, or that Schneiderman might “accidentally” release the full donor list to the public, as Obama’s IRS did with the confidential filings of gay marriage opponents?   
  
The Supreme Court has long recognized the dangers inherent in applying the power of the state against the right of private association. The cornerstone here is 1958’s _NAACP v Alabama_ _._ For reasons that hardly need be pointed out, the NAACP did not trust the state of Alabama, in the 1950s, to be good stewards of its membership lists. “Inviolability of privacy in group association may in many circumstances be indispensable to preservation of freedom of association, particularly where a group espouses dissident beliefs,” wrote Justice John Marshall Harlan II, who went as far as to compare such demands to a “requirement that adherents of particular religious faiths or political parties wear identifying arm-bands.” More recently, Justice Alito pointed out in a similar context that while there are undoubted purposes served by reasonable, limited disclosure requirements, the First Amendment requires that “speakers must be able to obtain an as-applied exemption without clearing a high evidentiary hurdle” regarding the potential harms of disclosure.   
  
But the Second Circuit Court of Appeals has decided it knows better than the Supremes. On Thursday, it ruled that Citizen United’s challenge should be thrown out without even an opportunity to prove their case. In the process, it effectively turned _NAACP_ into a “Jim Crow” exception to a general rule of unlimited government prerogative to panoptic intrusion into citizen’s political associations. While there can be no doubt that the struggle for civil rights presented a unique danger for its supporters, this should not mean that _only_ such perils warrant First Amendment protection.   




The marketplace of ideas is often fraught with contention, and those who support controversial causes must shoulder some risk. As the late Justice Scalia argued, “running a democracy takes a certain amount of civic courage.” But anonymity in such pursuits serves important purposes, and the premise that concealment of one’s identity is a sign of ill-will would have surprised James Madison, who published numerous defenses of the new constitution, convincing his fellow citizens of the virtue of the endeavor; he signed them “Publius.”   
  
In our schismatic political climate, many people could suffer if their political views were made widely known. This could include everything from adverse employment actions to outright violence. Some groups, such as those in the “antifa,” have openly advocated violence against political opponents. It’s odd that some on the modern left find themselves on the same side as the state of Alabama in 1958: arguing that those who support some political views should be disclosed to the state, even if violence might result. Although an appeal has not yet been filed, the Supreme Court should take the case and reverse the Second Circuit, making it clear that a compelling government interest is required before the government can force the disclosure of people’s political affiliations. 


"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Climate change is a moral, non‐​partisan and pragmatic issue which can be addressed by solutions with multiple co‐​benefits. We urge legislators to join global business, faith, scientific, health and military leaders in acknowledging that climate disruptions are real, happening now, and requiring our nation’s leaders to act.



It is interesting that they juxtapose a “moral issue” with calls for “policies to reduce national and global greenhouse gas emissions.” Interesting, we say, because there is a soon‐​to‐​be released and incredibly compelling book written by the Center for Industrial Progress’s Alex Epstein titled _The Moral Case for Fossil Fuels_. Its main premise is that both the short‐ and long‐​term benefits of using fossil fuels greatly outweigh the risks of any climate change that may occur as the result of the accompanying carbon dioxide emissions. Epstein argues that the “moral” thing to do is to continue (and expand) the use fossil fuels:   




If we look at the _big picture_ of fossil fuels compared with the alternatives, the overall impact of using fossil fuels is to make the world a far better place. We are morally obligated to use more fossil fuels for the sake of our economy and our environment.



The primary case against expansion of current fossil fuel use involves the risk from anthropogenic climate change. However, here, the threats are overstated—especially by organizations (like many of those behind The People’s Climate March) that favor centralized government control of energy production (and most everything else).   
  
  
The sea level rise concerns that are to be described in the Hill briefing will undoubtedly fall into the “overstated” category. According to the briefing’s flier:   




“The U.S. National Climate Assessment projected that sea levels will rise 1 to 4 feet by 2100, affecting 39 percent of the U.S. population and impacting the very futures of many coastal communities and small island nations.”



We imagine that the focus will be on the high end of the 1 to 4 foot range (and beyond), even as a plethora of new science argues for an outcome nearer to the low end.   
  
  
The current decadal rate of sea level rise is about 3 mm (.12 in) per year, which would result in about a foot of sea level rise during the 21st century. There is a lot of recent research that concludes that a large increase in this rate of rise as a result of the melting of Greenland’s and/​or Antarctica’s glaciers is unlikely.   
  
  
The statistical models most responsible for the high‐​end sea level rise projections used have been shown to be questionable and thus unreliable. And finally, and perhaps most importantly, the future projection of temperature rise made by climate models (upon which the sea level rise projections are based) have been shown by a growing body of scientific research to be overestimated by about 40 percent.   
  
  
Taken together, the latest science argues that the case for rapid and disruptive sea level rise is flimsy at best.   
  
  
Undoubtedly, sea levels will continue to rise into the future, in part, from the earth’s temperature increase as a result of human carbon dioxide emissions resulting from our use of fossil fuels. Appropriate adaptations will be necessary. However, signs point to a rather modest rise in sea levels accompanying a rather modest rise in temperature—a pace at which our adaptive response can keep up.   
  
  
So long as this is remains case, the continued use of fossil fuels to power the developed world and the expanded use to help provide safe, reliable, and cheap electricity to the more than 1 billion people in the underdeveloped world that currently live without any (or very minimal) access to it is a no‐​brainer. That’s where the moral imperative should lie.
"
"
I’ve been involved in meteorology in one way or another since 1976, and while I knew of the vast number of COOP stations around the USA, I never knew that a good number of them are at sewage treatment plants until I started my surfacestations.org project. It seems to me, that given the physical makeup of these facilities, they are one of the worst possible environments to measure air temperature. But like many historical stations, they weren’t chosen with the environment in mind, but rather if there was a human being present 7 days a week whom could take the high/low temps and rainfall and write it down on an NCDC B44 form.
This week I visited a few stations in southern California, and Santa Barbara is one of those USHCN stations that is also a sewage treatment plant. Conicidentally, a few other USHCN stations that are also WWTP’s were posted by www.surfacestations.org volunteers. So I thought I’d give you the grand tour.

Above: aerial view of Santa Barbara WWTP and USHCN climate station of record

Above: Placement of Santa Barbara’s MMTS Temperature Sensor – looking NW

Here’s one from Tifton, GA taken by Joel McDade:

more pictures here
Cheraw, SC taken by L. Nettles:

more pictures here
Albany, GA from Joel McDade:

more pictures here
Zumbota, MN from Don Kostuch

more pictures here
And let’s not forget Urbana, OH, by Steve Tiemeir

more pictures here
There’s lots more, but you get the idea.
surfacestations.org volunteer Don Kostuch wrote this to me about WWTP’s recently:
“I spoke with the curator in New Hampton IA. He gave me these figures for his plant last January:
780,000 gal/day
Incoming temp 55F
Outgoing temp 43F
I calculate this heat loss is about 3 million btu/hr.
The population is about 3500 so each person releases about 1000 btu/hr at the plant on a cold day.
The effect on the sensor depends on the placement, temperature, wind, location of the tanks, etc. which I have not attempted to analyze, but it seems to be worth some careful attention.
The worst example I saw was in Winnebago, MN where the  sensor is above and in the middle of four large tanks all huddled together in about a 100
ft square. The population there is about 1500 so the heat released would be about 1.5 million btu/hr in an area of about 10000 sq.ft.”
And, as population grows in a city so would waste water volume. So it stands to reason the a temperature sensor at a WWTP would be directly sensing waste heat produced by population growth, and the amount of waste heat would grow proportionately with population.
Perhaps we should call the WWTP effect “P-UHI”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4fe67e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The California legislature may want to revisit the wording of their proposed ban on incandescents (AB 722). California assemblyman LLoyd Levine, a Democrat from Van Nuys in Los Angeles, wants to make California the first to ban incandescent light bulbs (by 2012) part of its new initiatives to reduce energy use and greenhouse gases blamed for global warming. But somebody hasn’t thought this through completely.
Why do I suggest a change? Two reasons: 1- There’s a new efficient challenger to the old tungsten filament light bulb. 2- The Compact Flourescent Lamps touted as “Eco Bulbs” have a small amount of mercury an other heavy metals in them, making disposal a problem. Some landfills won’t take them!
GE has announced an advancement in incandescent technology that promises to increase the efficiency of lightbulbs to put them on par with compact fluorescent lamps (CFL).

The new high efficiency incandescent (HEI(TM)) lamp, which incorporates innovative new materials being developed in partnership by GE’s Lighting division, headquartered in Cleveland, Ohio, and GE’s Global Research Center, headquartered in Niskayuna, NY, would replace traditional 40- to 100-Watt household incandescent light bulbs, the most popular lamp type used by consumers today.
The new technology could be expanded to all other incandescent types as well. The target for these bulbs at initial production is to be nearly twice as efficient, at 30 lumens-per-Watt, as current incandescent bulbs. Ultimately the high efficiency lamp (HEI) technology is expected to be about four times as efficient as current incandescent bulbs and comparable to CFL bulbs. Adoption of new technology could lead to greenhouse gas emission reductions of up to 40 million tons of CO2 in the U.S. and up to 50 million tons in the Eeropean Union if the entire installed base of traditional incandescent bulbs was replaced with HEI lamps.
So take note California assemblymen and assemblywomen, how about mandating a level of lighting efficiency for bulbs rather than assuming that innovation of older technology can’t happen?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7e64554',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In an abstract presented at the 26th PACLIM Conference that was published in a recent issue of Quaternary International, Verosub (2015) writes about the challenges of maintaining and utilizing water supplies in California. However, the geologist from the University of California notes that what is often missing from discussions of water security is a consideration of the effects of natural climate variability beyond the historical record. As an example of such variability, Verosub cites the fact that river flow and lake measurements during the 20th century “document the occurrence of several multi‐​year droughts in the past 100 years while tree ring records show that 20‐​year and 70‐​year droughts occurred during the last 300 years.”   
  
  
On an even _longer_ time scale, the scientist reports that “at least once and probably several times in the last few thousand years, there have been droughts severe enough to drop the level of Lake Tahoe by several tens of meters, which allowed Douglas fir trees to grow to maturity on exposed lake beds.” Furthermore, other data indicate episodes of extreme flooding, such as the water year of 1861–1862 that brought extensive rainfall from Oregon down through southern California.   
  
  
In consequence of these realities, Verosub concludes that “the paleoclimate history of California suggests that even in the absence of climate change due to anthropogenic greenhouse gases, decadal, multi‐​decadal, or even century‐​long droughts are a real possibility in the future for California as is flooding on a greater scale than was seen in the twentieth century.” According to Verosub, if such _natural_ events were to occur today, they would easily “wreck havoc with California’s delicately balanced water delivery system” in the case of drought, and “overwhelm the levee system and destroy California’s ability to transfer water from north to south” in the case of flooding. No doubt, such events would quickly be labeled by climate alarmists and advantage‐​seeking politicians as “human‐​caused.” Yet, given the historic periodicity of these events, there would be no way to prove that they weren’t natural. In fact, their mere occurrence would simply confirm that they _are_ natural, recurring over and over again throughout history, human influence notwithstanding. As such, the title of the author’s work provides some good advice for Californians: _Don’t worry about climate change; California’s natural climate variability will probably “get us” first_.   
  
  
  
  
  
**Reference**   
  
  
Verosub, K. 2015. Don’t worry about climate change; California’s natural climate variability will probably “get us” first. _Quaternary International_ **387** : 148.
"
"
Share this...FacebookTwitterStefan Rahmstorf on the IPCC modelling breakdown: Reason to breathe a sigh of relief, new climate models are far too sensitive.
By Die kalte Sonne
(Translated by P. Gosselin)
DER SPIEGEL provides a regular platform for the controversial climate scientist Stefan Rahmstorf. On 12 May 2020 he was allowed to:
Stronger temperature rise: Why the climate models are running hot
A guest article by Stefan Rahmstorf
New calculations have alarmed the scientific community – they suggest the earth could be more sensitive to greenhouse gases. Will global warming be stronger than previously thought?
Here the quick reader will suspect one of the usual Rahmstorf climate alarm pieces. And this is exactly how the beginning of the article reads. However, it deals with a tricky topic that will certainly hit the Potsdam scientists quite hard to the stomach.
Huge mishap
In the course of the preparation of the 6th Climate Status Report, the IPCC has again run a large number of climate models. This time, however, a huge mishap has occurred. Several of the models have delivered far too much warming, which is not compatible with the measured data of the last decades. This fundamentally casts the models into question. They suggest that the warming effect of CO2 is far too high. A scandal that should actually cast everything into question.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Rahmstorf plays it dumb at the beginning of the article, luring his readers into the alarm trap. Will everything get much worse than expected? This is the typical Rahmstorf narrative.
“Models are crap”
But if you can make it to the end of the article, you will be surprised. Rahmstorf actually admits quietly that the models are crap, running way too hot.
In reality it’s all not so bad. Rahmstorf writes literally in his article:
The comparative study by researchers from the University of Exeter now shows that in particular the warming since 1975 – i.e. most of the modern global warming – is clearly too strong in the sensitive models. More recent analyses by ETH Zurich, for which more models have already been evaluated, confirm this conclusion. This is a reason to breathe a sigh of relief: there is currently some evidence that these models are not better than the old ones, but are simply too sensitive.“
Did SPIEGEL force its guest author to write this article? Was this a prerequisite for him to continue writing there? A balanced presentation with a fair evaluation of all opinions represented in science has never really been Rahmstorf’s strength.
Obvious failure
Or was it a flight to the front because the modelling failure was all too obvious and Rahmstorf feared complete professional isolation? It’s hard to say.

Stefan Rahmstorf must have struggled for several months before deigning to admit this mishap. This certainly could not have been easy for him.
By the way, here in the blog we have already reported on the topic several times: “The sun in February 2020, science against doom and gloom” and “The sun in November 2019 , when models exaggerate” and “The sun in December 2019, advances in climate science“.


		jQuery(document).ready(function(){
			jQuery('#dd_a5e15193542096fb84730e25e385a8c3').on('change', function() {
			  jQuery('#amount_a5e15193542096fb84730e25e385a8c3').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"
Share this...FacebookTwitterCoastal history analyses increasingly suggest sea levels are lower today than at any time in the last 7000 years – even lower than the 1600s to 1800s.
Recently we compared cartology from the 17th to 19th centuries to direct aerial images of coastal positions today. Rather surprisingly, there seemed to be more land area below sea level a few hundred years ago.
For example, an 1802 nautical map of New York City and Long Island shows there may have been more open waters in this region during the Little Ice Age than in 2019.

Image Source: Amazon.com
Shoreline analysis from India also suggests the coasts were further inland during the 1600s than they are today (Mörner, 2017).

Image Source: Mörner, 2017
In another new study, the borehole sea level history for the Italian port city of Salerno reveals the coast was hundreds of meters further inland compared to today’s 7000 years ago. Even 300 years ago the coast was still much further inland (Amato et al., 2020).

Image Source: Amato et al., 2020
Citing previous studies, another new paper has today’s sea levels about 2 to 3 meters lower than they were 4000 to 5000 years ago along the coasts of Brazil (Martins et al., 2020). And, again, today’s relative sea levels seem to be the lowest of the record – lower than the Little Ice Age.

Image Source: Martins et al., 2020
Share this...FacebookTwitter "
"

This year’s installment of the U.N.‘s annual holiday party has come and gone from Cancun, with little to show for it except the massive carbon footprint of thousands of attendees — official delegates of member nations, plus representatives of Greenpeace, the World Wildlife Fund, and their ilk.



Technically, what we have just witnessed is the 16th Conference of the Parties to the United Nations’ Framework Convention on Climate Change, a.k.a. “COP16.” These conferences always take place at this time of year, and often in the tropics or in the Southern Hemisphere, where it is now summer. Next year’s confab will be in Durban, South Africa.



It’s really poor planning to throw these parties in December. The weather in the Northern Hemisphere lately has been downright uncooperative. Cancun witnessed a 100‐​year record low temperature one day as the delegates got down to “business” for two weeks, actually accomplishing little of substance. Last year, when COP15 was held in Copenhagen, the weather was miserable, with frequent snow and unseasonably cold temperatures. The quintessential images from that conference were of Barack Obama pronouncing the meeting a great success, and then rushing back to Washington, only to land in a blinding snowstorm.



Copenhagen was an abject failure. The great “success” was a requirement that each nation submit a schedule for reductions in carbon‐​dioxide emissions. But that requirement was waived by the executive secretary of the Framework Convention, Yvo de Boer. Then he quit.



He was replaced by Costa Rica’s Christiana Figueres, who welcomed the crowd to Cancun, invoking Ixchel, the Mayan goddess of weaving and creativity. Rather appropriate for an organization whose last climate report was made up out of whole cloth.



Predictably, Figueres pronounced the festivities a roaring success. “Cancun,” she said, “has done its job — the beacon of hope has been reignited.”



Sure — as in, Third World nations hope that the developed world’s governments will magically decide to donate them a trillion dollars over the next decade to “cope” with climate change.



Indeed, the delegates did agree to this “green fund,” but they failed to explain where the money will come from. All they agreed upon was how the nonexistent moneys are to be distributed.



Nor did they agree to anything that would commit any nation to reductions in carbon‐​dioxide emissions. However, you are free to submit a schedule that you don’t need to adhere to.



I’m sure that the U.N. can’t wait for the U.S. response on this one. It will appear in the form of some directive from the executive branch, specifically the EPA. But let us not forget — as I’m sure he has not — that the president’s party just got pummeled in the midterm elections. The more stringent the directive is, the more likely it is that Mr. Obama will leave Washington a private citizen in January 2013.



Even if he approves big (and impossible) cuts, it won’t do a measurable thing about global warming unless China and India agree to similar reductions. In fact, they have already informed the world — at both Copenhagen and Cancún — of their intent to raise emissions. China’s are on track to double in the next decade, and India’s look to increase threefold. Together, those two countries could easily be responsible for half of global emissions over the next two decades. The U.S. is currently responsible for about 20 percent of the total, and that percentage is steadily dropping. In 2009, China emitted a whopping 27 percent more than the United States.



The Cancun partiers couldn’t agree on any treaty or protocol to replace the failed Kyoto Protocol, which expires at the end of 2012. That one was supposed to “legally bind” the industrialized world to reduce its emissions to about 5 percent below 1990 levels by now. That language really worked, didn’t it? Emissions rose by more than they were supposed to fall. And even if all nations met their “obligations” under Kyoto, they were so insignificant that their effect could never be found by thermometers.



Kyoto was so unpopular that it was never brought up for ratification by the U.S. Senate. After the unceremonious death of cap‐​and‐​trade, are there going to be the 67 votes necessary to ratify something even more politically damaging?



It’s not easy to see the need for all these annual gatherings. In this fiscal climate, the developed world isn’t going to send a trillion bucks to Africa and a few tropical islands. Nor is any agreement going to be enforceable.



If the U.N. delegates were serious about global warming, at least they could meet by Skype and GoToMeeting instead of burning hundreds of thousands of gallons of Jet A in pursuit of perennial failure.
"
"
Share this...FacebookTwitterPaleoclimate evidence shows there is little to no link between atmospheric CO2 concentration and relative sea level.
Venice, the treasure of Italy, is a city built on a mud swamp. Consequently, in the last 100 years it has sunk about 25 cm, or 2.5 millimeters per year (Munaretto et al., 2012).
The lagoon city had its worst flooding event ever recorded in 1966, when CO2 concentrations were still about 320 ppm.
Last week Venice flooded again, and, as expected, journalists blamed climate change and rising atmospheric CO2.
However, when we consider sea level rise rates for Venice averaged 2.6 mm/yr during 1872-1969, but then decelerated to 0.7 mm/yr for 1970-2000 (Munaretto et al., 2012), these trends are the opposite of what would be expected if CO2 emissions were driving sea level rise.

Image Source: Munaretto et al., 2012
Pisa’s history provides a sea level perspective
Italy’s Pisa is famous for its leaning tower.
The city was originally built on the coast of the sea about 13 centuries before the common era (~3300 years ago). At that time, sea levels were meters higher than they are now despite the low (~270 ppm) CO2 concentrations.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




During the Roman Warm Period, Pisa was still close enough to the sea coast (~4 km) to be a busy harbour (Huissen and de Graauw, 2019), accessible by canal. Dozens of ships dating to Roman times have been found buried beneath the city in recent decades.
In the last 2000 years, however, sea levels have retreated so thoroughly that Pisa now sits 9.7 km from the sea coast.

Image Source: Huissen and de Graauw, 2019
Italy’s sea level during the last interglacial
About 130,000 to 120,000 years ago, or during the last interglacial, CO2 levels peaked at 280 ppm.
Yet along the coasts of central Italy there are marine mollusck shells buried in silty sand and clay 12-35 m above today’s sea levels dating to this time period (Marra et al., 2019).

Image Source: Marra et al., 2019
In sum, the record of coastal changes throughout both ancient times and in the modern era do not support the conclusion CO2 levels are a driver of sea level change.
Share this...FacebookTwitter "
"
This post is an outgrowth of comments I made on Commission Impossible on the new proposed strengthened tree ordinance.
I like trees, and I recently planted four, but at the same time I’ve had to remove a couple of trees from my home and business property. In the latter case, the City did the work because they agreed with me that the tree was unsafe and posed a public hazard.
This tree ordinance thing is taking on overtones of the abortion battle, except that the roles seem to be reversed, with the “right to life” being on the left. Lately, it seems that meadowfoam, garter snake habitat (see Sundays letters to the editor) and beetle habitat Elderberry bushes are more important than the rights and lives of people.
Case in point – how many people have died at the Highways 70/149/99 interchanges in the 10+ years that environmentalists have placed roadblocks in front of that project? I remember one little boy, about two years ago, who died when the car he and his mom were riding in was broadsided by a car on 70 as she turned onto 149. If the road had been improved on schedule, that never would have happened.
Was that worth 10 years of delay to protect some Meadowfoam and beavers? I think not. Meadowfoam is being grown in quantities at reserves near Vina and commercially in Oregon, and the Limnanthes Flococcus Californica aka Butte County Meadowfoam can just as easily be grown with it. Beavers relocate with ease too. Anybody who tells you otherwise is just pushing an agenda.
Now we have the City saying there’s a delay in authorizing a bid to fix drainage problems for a man made stormwater retention basin near south Chico street Paseo Campaneros that becomes a West Nile hotspot. Two people have died on that street from West Nile in the past year…yet the “garter snake habitat” aka man-made retention basin gets hands-off priority according to what the city said recently.
It’s lunacy and its morally wrong. Public health, be it an accident prone intersection or a festering man-made mosquito pool should always trump protecting bugs, plants, snakes, and the occasional beaver. If you think these things are more important than the health of the community, then you have your social priorities reversed.
Environmentalists digging in their heels on this only hurts their cause, because it makes them look unreasonable, and maybe they are. But most people I know, on either side of the political spectrum actually want to protect the environment, me included, but they want to protect their children and grandparents more.
Making them choose through obstruction is a no-win polarizating situation. We CAN have it both ways.
In the case of trees, the folks pushing this strengthened law act as if the tree, once cut down, could never be replaced. We’re not talking giant Redwoods here…more like Dogwoods and Pines, available at Home Depot.
Compromise folks…compromise.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6f63713',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Of course all we hear from the media nowadays is that weather extremes have been getting worse (over the past decades) and the planet is warming rapidly. But when we look at the untampered data, we see that many places have been cooling.
Today I present to you some examples, now that untampered November data for 2019 have become available at the JMA.
Iceland
First we look at November mean temperatures of three stations Iceland, isolated in the middle of the North Atlantic. The source of the data is the Japan Meteorological Agency (JMA):

Three Icelandic stations with sufficiently available data from the JMA show a cooling November trend since the early part of the century. Data JMA.
In Iceland, the start of winter in fact has been getting colder, and not warmer like AGW theory would suggest. So there’s been lots of deception coming from activists.
Netherlands
The data tell a similar November story at the North Sea country of Netherlands: The start of winter is not getting delayed in the sense of warming. Quite to the contrary, it’s been cooling in November:

No real warming to be seen in November from the five examined stations in the Netherlands. Data: JMA.
Ireland
Over at the North Atlantic island of Ireland, we find 6 stations with sufficient data from the JMA to allow the plotting of November trends. Here’s the result of the six stations observed:

Warming? Ireland is hardly behaving like the alarmists and Greta claim. Early winter in Ireland has been cooling! Data: JMA. 
Norway (Greta’s neighbor)
Greta Thunberg’s western neighbor, Norway, has also been defying claims made the teenage Swedish activist, and, for the most part, has been seeing cooling in November for two decades:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





November in Greta Thunberg’s neighbor, Norway, has been cooling for some two decades now. Data: JMA.
Seven of 11 stations in Norway show November temperatures have had a cooling trend since 1999 (20 years)!
This year TROMSO/LANGNES and VARDO saw their coldest November in the last 17 yrs, BODO VI, ORLAND III, BERGEN/FLORIDA and OSLO/GARDERMOEN were the coldest in 9 years.
Sweden
Next we move to Greta’s Sweden, whose future she says we are stealing. But looking at her home country as a whole, 3 of 6 stations in Sweden show November temperatures have had a cooling trend since 1999:

No significant Swedish warming during Greta’s lifetime. Data: JMA.
Powerful ocean cycles, not trace gas CO2
So how can these northern European countries examined above be cooling in times of “rapid global warming”? Why is winter coming earlier, and not later? Isn’t CO2 greenhouse gas supposed to be trapping heat there and making things warmer?
Obviously it isn’t, and so there has to be other natural explanations.
The explanation for this of course has long been suspected, but has been kept hidden from the public. The major reason for the cooling of November over the recent years very likely has something to do with the North Atlantic sea surface temperature (SST).
As the following chart shows, SSTs are cyclic, and they have been trending down for the past 15 years.
 

Chart: NOAA, via Climate4you.
So it’s no surprise that November has been cooling over northern, coastal Europe since the start of the century. Claiming that CO2 drives climate is a sham.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNow that severe restrictions concerning mobility and social distance have been put in place and led to a shut down of a large part of the economy, climate activists claim that already we are seeing huge environmental benefits, among them: cleaner air.
One person here in Germany even tweeted that he had not seen such clean air since his childhood, and attributed it to scale-down of human activity.
But German wetteronline.de here reports the clean air central Europe has seen recently since the COVID 19 crisis began is not the result of the shutdown, but is mostly due to the current weather pattern over Europe.
Wetteronline.de reports:
The lower volume of traffic and the largely idle economy certainly has an impact on the concentration of dust and dirt in the air. However, the current weather situation is much more important. On the verge of a powerful high over the Baltic, dry and cold air is being carried from Siberia to Central Europe. It is very clear and pure. In addition, there is a gusty easterly wind, which leaves no chance for a so-called inversion. Under such an inversion the concentration of dust, soot and dirt would increase rapidly.”
Wood burning the biggest threat
Moreover, a heated debate has been unleashed by Swiss meteorologist Jörg Kachelmann, who says the biggest threat to clean air in Germany is the now increasing use of wood burning for home heating.

Das ist immer die grösste Lüge. Nichts verbrennt dreckiger als Holz mit so vielen Zusatzsubstanzen.
Siehe den #Thread hierhttps://t.co/gp9ZexphxD


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Und auch auf der Tabelle die Folgen zu sehen im Vergleich.
Jeder Holzofen ist eine Umweltkatastrophe für sich.@davidermes https://t.co/QDgtshMd9g pic.twitter.com/O6XUjboWLq
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) April 5, 2020

40 times dirtier than natural gas
According to the veteran meteorologist, “nothing burns dirtier than wood and its additives” and “every wood stove is an environmental catastrophe”. The following chart shows the fine particle emissions from various heating fuels:

Wood pellets are 40 times worse than natural gas.
With governments moving to restrict fossil heating fuels and encouraging “renewable” wood, more and more Germans are opting for wood heat. Thus climate activists have to expect the air to become dirtier – much dirtier – and not cleaner should restrictions be enacted against the fossil fuel economy.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNot long ago one (right wing) politician warned before the German Parliament that the bicycle as a means of transport was extremely dangerous – especially for children – and thus ought not be promoted.
“Highly impractical and dangerous”
This of course brought ridicule from the infallible leftists and greens – and yes, even from German centrists who have long become all drugged up on green and “climate protection”.

AfD parliamentarian Dr. Dirk Spaniel told before the Parliament: “Soberly considered, bicycles are highly impractical and dangerous.”
According to Spaniel, a transportation expert, a child transported on a bicycle is exposed to greater danger than in a car. On the Green Party’s vision of a bicycle utopia in Germany and the world, Spaniel mocked: “They want to draw an ideal fairy tale world here with bicycles, which do not exist in this form.”
Twice as likely to die on a bike
As much as the left and greens like to ridicule Dr. Spaniel’s claim, it is backed up by most studies.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




For example, the Washington Post here writes that “bikes are the most dangerous way to get around with the exception of motorcycles” and that in the USA, “you’re more than twice as likely to die while riding a bike than riding in a car, per trip” and riding a bicycle is “about 500 times more fatal than riding in a bus”. Here the WaPo cited according a 2007 study led by Centers for Disease Control and Prevention epidemiologist Laurie Beck.
Why so many German politicians are now striving to transport children using such a dangerous mode of transport remains a mystery. It’s one of the side effects of being drugged on green. In their doped minds, addicts dismiss all the risks and amplify the promised benefits.
Bicycle deaths rising in Germany
As bike riding increases in Germany, so do the accidents and fatalities. According to Spiegel, citing the Federal Statistical Office in Wiesbaden: “445 people died in accidents on a bicycle – 63 cyclists more than in the previous year and the highest number since 2009.”
“A total of 88,850 cyclists were involved in accidents on German roads in 2018,” Spiegel wrote earlier in 2019.  “That is around 11 percent more than in the previous year.”
25 times higher risk of injury
According to AfD Parliamentarian Dr. Dirk Spaniel: “Parents who transport their children on bicycles increase the risk of injury to them 25 times more than those who transport them by car.”
Time for parents to be responsible for their kids and to stop pretending they can be responsible for the climate.
Share this...FacebookTwitter "
"
This is the USHCN climate station of record for Bainbridge Georgia. It comes to me by way of surfacestations.org survey volunteer Joel McDade. Joel wins the award for finding the USHCN station closest to an air conditioner, at 8.9 feet. That honor was previously held by Oregon State Climatologist George Taylor at just over 10 feet in his picture of Forest Grove Oregon.

In addition to the air conditioner, this USHCN climate monitoring station sports several other features:
– A building just 14.3 feet away
– Convenient close-by radiator forward parking for your vehicle within feet of the MMTS sensor
– An asphalt road within 10-15 feet of the sensor
– A mature shade tree that changes shade patterns with the season
– A station move of about 150 feet closer to the building to accommodate the new MMTS sensor cable length
The station is operated by the International Paper Company. The plot of temperature below illustrates some data gaps and jumps that may be related to station moves.

Full details on this site are at the surfacestations.org online image database


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5e3246c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the third of a century since its founding, the Cato Institute’s scholars have issued a wealth of predictions about the likely effects of government policies and programs. While sometimes ignored or belittled, these predictions have often proved prescient.



Most famous was Joe Stilwell’s Policy Analysis published in 1982. In “The Savings & Loan Industry: Averting Collapse,” Stilwell warned that, “regardless of changes in the economic climate, numerous S&Ls will be unable to meet their financial obligations.” Few in government listened then. Through the remainder of the decade, Americans would have been better off if they had, before the taxpayers had to come up with a $500 billion rescue plan.



In 1982, Cato founder and president Edward H. Crane wrote about his recent visit to the Soviet Union. “It is a society that appears to be crumbling from within,” Crane wrote. He added, “If we can avoid confrontation with the Soviets over the next 20 years, their system should collapse of its own bureaucratic weight.” Such a prediction sounded crazy at the time. And indeed Crane’s estimate was off target.



The Soviet Union vanished, in not 20 years, but 9.



Stanley Kober, a research fellow in foreign policy studies at the Cato Institute, warned in a 1996 paper that “the terrorist attacks in Saudi Arabia, Israel, and other countries suggest that the trend in the Middle East is not nearly as hopeful as it appeared just a few years ago,” and he identified Osama bin Laden as a particular terrorist threat to the United States.



In a study he published in February 2001, Daniel Griswold wrote, “A domestic recession would reduce the trade deficit, as it has in the past, but at great cost to U.S. workers and their families.” A month later, the U.S. economy slipped into recession and the trade deficit declined in 2001 compared to 2000, after having risen in each of the previous five years. Then came the Great Recession, beginning in 2008. The trade deficit in 2009 was $300 billion smaller than in the pre‐​recession year of 2007.



In few areas have Cato scholars been more consistently correct and more consistently outside the mainstream consensus than the Iraq war. In 1999, Ted Galen Carpenter argued that “removing a thug like Saddam … is extremely ill‐​advised. It will make Washington responsible for Iraq’s political future and entangle the United States in an endless nation‐​building mission beset by intractable problems.” William Niskanen wrote in the _Chicago Sun‐​Times_ in December 2001, “Another war in Iraq may serve bin Laden’s objective of unifying radical Muslims around the world in a jihad against the United States.” In 2002, Doug Bandow warned that, “If Iraq’s forces don’t quickly crumble, the U.S. might find itself involved in urban conflict that will be costly in human and political terms.” And in March 2003, Christopher Preble argued America’s experiences with nation‐​building in Germany and Japan advise against attempting the same with Iraq. “If these ‘success’ stories reflect the model for post‐​war Iraq,” Preble wrote, “we should expect the U.S. to remain in this troubled region for many years.” Returning to domestic affairs, in March 2007, Jim Harper said in congressional testimony: “Mr. Chairman, the REAL ID Act is a dead letter. All that remains is for Congress to declare it so.” More than three years later, REAL ID, an attempt by the federal government to establish a national personal identification system, has gone nowhere, and two major implementation deadlines have passed.



In February 2009, when President Obama’s approval rating was in the mid‐​60s and most political opinion makers thought he was on the cusp of radically remaking America, Gene Healy published his first weekly column in the _D.C. Examiner_. Healy wrote, “When he fails to fully heal our financial troubles, fix health care, teach our children well, provide balm for our itchy souls, and so forth, his hopeaddled rhetoric will seem all the more grating, and the public will increasingly come to see him as the source of all American woes.” By July 2010, according to Gallup, President Obama’s approval rating had fallen to 44 percent, the lowest of his presidency, and his party was fearing considerable losses in the upcoming congressional elections.



As Healy predicted, President Obama did fail to fix health care. Instead, he ushered through Congress the ill‐​considered legislation known as ObamaCare.



Michael Cannon predicted in September 2009, six months before the bill’s passage, that ObamaCare’s individual mandate would force as many as half of all Americans with private insurance to switch to a more expensive plan. At the time, the administration insisted this was fantasy. In June, it all but admitted Cannon was right, prompting the _New York Times_ to write that “the rules appear to fall short of the sweeping commitments President Obama made while trying to reassure the public in the fight over health legislation.” Even earlier was Michael Tanner’s 2006 paper, “Individual Mandates for Health Insurance: Slippery Slope to National Health Care.” Later that same year, Massachusetts enacted health care legislation that included an individual mandate. The results have followed Tanner’s script exactly. RomneyCare’s individual mandate took effect in 2006, along with health insurance exchanges. Subsequently, 16 mandates have been added to the original list of benefits that health insurers must provide in the Bay State. Massachusetts now has the most rapidly increasing premiums in the nation. The most recent attempt to control costs, as Tanner predicted, was to simply prohibit insurers from increasing premium rates, leading insurance companies to predict that they will suffer from hundreds of millions of dollars in losses this year. In addition, wait times have increased to see both primary‐​care physicians and specialists, just as Tanner’s paper said they would.



The fact that policymakers failed to take Cato scholars’ warnings of the last 30 years to heart, makes it only more crucial that they do so in the next 30.
"
"

What do the numbers 923, 930, 935, 941 and 944 have in common? Answer: They’re different names for the same sunspot, this one shown above.
Greg Piepol of Rockville, Maryland, took the picture yesterday using a Solar Max Solar telescope/camera. It shows sunspot 944 coming around the sun’s eastern limb–for the fifth time! Usually sunspots form and dissolve in a matter of weeks, but this spot has endured for more than five 27-day solar rotations. By long and idiosyncratic tradition, a sunspot receives a new number each time it reappears and is visible to earth.
Sunspot 944 may not seem impressive now, but one month ago as “941” it was a lovely spiral. Three months ago as “930” it produced one of the strongest solar flares of the past 25 years and Northern Lights as far south as Arizona. What will it do this time?
Even though we are in between peaks in our 11 year sunspot cycle, we still seem to have quite an active sun. The trend over the last century has been that our solar cycle has had more activity than centuries before.

Of course, that couldn’t possibly have anything to do with global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea821d20f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"


An erupting solar prominence photographed by the Solar and Heliospheric Observatory (SOHO). [More]
In a post a few days ago I mentioned scientists discovering that global warming appears to be happening on Mars in its polar ice caps and that this was likely evidence of a solar linkage that also affects Earth’s climate. Today NASA announced in an article shown below that the next solar sunspot cycle due in 2010 is likely to be one of the historically largest in 400 years of sunspot records.

What does this mean? Well if you follow sunspots and temperature trends on earth you’ll be able to see clear correlations between the ebb and flow of sunspots and Earthly temperature. While global warming proponents brush this off as inconsequential, the fact is that when sunspots happen in larger numbers, Earth warms up, when they disappear, the earth cools, as evidenced by a 50 year cold period in Medieval history with virtually no sunspots known as the Maunder Minimum. Its also called The Little Ice Age.
So, with a big sunspot cycle in the next few years, we can expect many record high summer temperatures and warmer than normal winters. We’ll see melting sea ice, retreating glaciers, and wailing of those saying “We told you so, CO2 is killing the planet!”. Al Gore will probably get elected President by a panicked nation, and general worry and angst will reign supreme. Emergency CO2 emissions measures may be enacted. Perhaps a rationing on driving our cars?
And then, when solar cycle 25 hits ten years later, which will likely be much smaller, the “crisis” will subside and those whom enacted those emergency measures will pat themselves on the back and bask in their “heroism”. Except, it won’t have anything to do at all with changes in emissions. It’s all about the sun. Just take a look at the picture above and notice just how small earth is compared to the sun, or even a large solar flare. Anybody whom thinks the human race has more effect on our global energy balance than an active sun does is just deluding themselves.
There’s a monetary bet out there: two Russian solar scientists are so certain that its the sun driving climate change and nothing else, they have put down a $10,000 bet with a prominent climate change scientist saying we’ll see a cooler of the earth by about 2015.
I want some of that action. Bets anyone?

From NASA, Dec. 21, 2006: Evidence is mounting: the next solar cycle is going to be a big
one.
Solar cycle 24, due to peak in 2010 or 2011 ""looks like its going to be one of the most intense cycles since record-keeping began almost 400 years ago,"" says solar physicist David Hathaway of the Marshall Space Flight Center. He and colleague Robert Wilson presented this conclusion last week at the American Geophysical Union meeting in San Francisco.
Their forecast is based on historical records of geomagnetic storms. Hathaway explains: ""When a gust of solar wind hits Earth’s magnetic field, the impact causes the magnetic field to shake. If it shakes hard enough, we call it a geomagnetic storm."" In the extreme, these storms cause power outages and make compass needles swing in the wrong direction. Auroras are a beautiful side-effect.
Hathaway and Wilson looked at records of geomagnetic activity stretching back almost 150 years and noticed something useful:. ""The amount of geomagnetic activity now tells us what the solar cycle is going to be like 6 to 8 years in the future,"" says Hathaway. A picture is worth a thousand words

Above: Peaks in geomagnetic activity (red) foretell solar maxima (black) more than six years in advance. [More
In the plot, above, black curves are solar cycles; the amplitude is the sunspot number. Red curves are geomagnetic indices, specifically the Inter-hour Variability Index or IHV. ""These indices are derived from magnetometer data recorded at two points on opposite sides of Earth: one in England and another in Australia. IHV data have been taken every day since 1868,"" says Hathaway.
Cross correlating sunspot number vs. IHV, they found that the IHV predicts the amplitude of the solar cycle 6-plus years in advance with a 94% correlation coefficient.
“We don’t know why this works” says Hathaway. “The underlying physics is a mystery. But it does work.”

According to their analysis, the next Solar Maximum should peak around 2010 with a sunspot number of 160 plus or minus 25. This would make it one of the strongest solar cycles of the past fifty years—which is to say, one of the strongest in recorded history
Left: Hathaway and Wilson’s prediction for the amplitude of Solar Cycle 24. [More]
Astronomers have been counting sunspots since the days of Galileo, watching solar activity rise
and fall every 11 years. Curiously, four of the five biggest cycles on record have come in the past 50 years. “Cycle 24 should fit right into that pattern,”says Hathaway.
These results are just the latest signs pointing to a big Cycle 24. Most compelling of all, believes Hathaway, is the work of Mausumi Dikpati and colleagues at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. “They have combined observations of the sun’s ‘Great Conveyor Belt’ with a sophisticated computer
model of the sun’s inner dynamo to produce a physics-based prediction of the next solar cycle.” In short, it’s going to be intense.
Details may be found in the Science@NASA story Solar Storm Warning
“It all hangs together,” says Hathaway.

Picture above – Sunpot numbers have been increasing for the last 150 years and have been at their highest average levels during the last 20 years, which could explain much of the global warming conditions observed on earth.
Note that during the 1970s, sunspot numbers decreased, we had some severe winters, and many scientists and popular press at that time talked of a coming ice age.  You can read a June 24th, 1974 article about a coming ice age in the TIME Magazine archive here:
http://www.time.com/time/magazine/article/0,9171,944914-1,00.html
Even as recently as 1994, TIME was concerned about a possible ice age coming as we see in this article:
http://www.time.com/time/magazine/article/0,9171,980050,00.html
Chances are, we’ll see another dramatic dip in sunspots by 2015 through 2022 and global cooling will set in again as it did in the 1970’s.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea93ed743',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Image above: Dubbed the “Swan” this X-ray image shows massive energy releases from the sun’s magnetic field, even while we are at the solar minimum in between sunspots cycles.
Last week, on the same day Al Gore was giving testimony to congress on made-made CO2 being the sole cause of Global Warming, NASA called a press conference in Washington DC to announce some spectacular new findings about the sun. Of course everybody in the press was so busy covering Gore’s big day, there was hadly any mention of what NASA announced.
What they announced was that a new X-ray imaging satellite called HINODE, launched in September 2006, has seen the first images that  explain one of the biggest mysteries of the sun: why the corona is hotter than the suns surface. Magnetic reconnection seems to be the key, and these images go a long ways towards proving the theory.
But even more importantly, scientists expected to see a very quiet sun with the new x-ray imager, since we are at solar minimum right now. NASA announced we’d reached solar min on March 6th. The fact that the HINODE scientists saw huge explosive energy bursts even while the surface of the sun is nearly devoid of sunspots tells them that the suns magnetic field is still tremendously active. The suns magnetic field has been getting more active for the past hundred years, coincidentally at the same time CO2 on earth has been increasing along with the global mean temperature.

But it seems that coincidence makes CO2 a Red Herring.
The linkage between changes in the suns magnetic field and earths climate has been well documented. Global temps closely track solar cycles as measured by sunspot intensity. Sunpots are proxy indicators of changes in the suns magnetic field. The Danish Meteorological Institute first reported the correlation in a study going back centuries. Historic data reveal that whenever the sun got more active, the earth heated up, and vice versa. The best correlation was the Maunder Minimum.

But until now, we could not see energy being transported away from the sun via its magentic field, which is why many in the environmental community doubt the role of the sun in climate change. We couldn’t visualize the sun’s magnetic output. This new tool is going to open a whole new era of understanding how the sun works, and more importantly how changes on the sun link to climate changes on earth.
Of course I’m sure Mr. Gore will find a way to explain this away, since we can’t have any new science getting in the way of a “consensus” and a “debate thats over”.
Inconveniently, NASA also announced last week a new study that shows a clear sun-earth linkage in records kept by Eqyptians of the Nile river, rainfall, and auroral activity which is a direct indicator of solar activity. It seems the sun-earth climate linkage has been around way before SUV’s.
So what’s easier to believe as the cause of climate change? That a trace gas called CO2 that has increased on earth from about 280 PPM to 380 PPM in the last 100 years is the cause, or that the giant nuclear fireball a thousand times bigger than earth a mere 8 light-minutes away has been getting more active during the same period is the reason?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea783b761',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This picture, taken by www.surfacestations.org volunteer Don Kostuch is the Detroit Lakes, MN USHCN climate station of record. The Stevenson Screen is sinking into the swamp and the MMTS sensor is kept at a comfortable temperature thanks to the nearby A/C units.

The complete set of pictures is here
From NASA’s GISS, the plot makes it pretty easy to see there was no discernible multi-decadal temperature trend until the A/C units were installed. And it’s not hard to figure out when that was.

But hey, thy can “fix” the problem with math and adjustments to the temperature record.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4e14bff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

For those of us that hate having Winnie the pig presented to us as part of our local weather report, I’d like to offer this solution that cuts all of us annoying weather middlemen and weather forecasting pigs right out of the picture and give you total control over your weather report.
Its called the ViziFrame – now you can program your own local weather channel at home, or at the office, or at the marina, or the golf course, your school, a truck stop, gas station, or wherever there may be an interest in weather to make a go/no go decision. You can view it on your own terms, and unlike the Weather Channel, you don’t have to wait a half hour to get the info you need.
And the graphics, look at good as anything on TV. For those of you with a profit in mind, it can have advertising and other information too. Its way cool, inexpensive, and trouble free. It works with any TV, big screen, or computer monitor. It updates its information via WiFi or a regular Internet cabled connection to a home DSL/cable router or T1 router.
The Chico Chamber of Commerce is going to put a bunch of them (the premium model that also does video and audio clips with touch screen interactivity) around town at hotels, restaurants, city hall and other public places that cater to visitors. Local artist Gregg Payne worked out a cool design for the front fascia that looks like the Hooker Oak tree…a concept view is below along with the current weather page and forecast…which are live content links soon to be on the Chamber of Commerce web page. Kris Koenig and Anita Berkow of Interstellar Studios are doing the interactive kiosk presentation for it. Look for these around town soon!



Note the current conditions page – it has solar irradiance on it – I figured if we were going to become a solar powered city, getting a real-time indicator that people can use to calculate solar panel efficiency would be a good first step, so I invested in the equipment to do that. I’ll have an entire blog entry on this service later.
The graphics are made in my rendering system as weather data arrives at my office here in Chico, there’s actually about a hundred plus graphics that are available.
But you can get a weather channel for your home or business too. See www.viziframe.com I’ve sold several of these already and people at home just connect them up to a spare video port on their big screen TV, and when they want weather, just switch to it. No waiting, no pigs, no hassle.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7156f18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This morning, I took my children out front, and we placed three flags in our front yard. Each child got one little flag on a wooden stick to plant in the front garden, while mommy and daddy got the big flag to hang from the porch.
After a little discussion on why we did this on Memorial Day,” to remember those who keep us free”, my son William remarked, “ok…can we wash the car now?” (that was our next project).
Well maybe it’s a little early at nearly 4, to install some patriotism. But later when William and I drove to the hardware store together he said “Daddy, how come those houses don’t have flags? We have flags”. It was then I realized we were the only house on our entire street displaying flags today.
Good question son, good question.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea63cd550',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHat-tip: Die kalte Sonne
In 2019, weather-related events in Germany caused insured damage to houses, household contents, commerce, industry and motor vehicles amounting to 3.2 billion euros. This is the result of preliminary figures published in a press release by the German Insurance Association (GDV).
The level is thus at the previous year’s level and below the long-term average of around 3.7 billion euros.
“Despite the storm and hail damage to motor vehicles, the overall natural hazard balance is slightly below average”, said GDV President Wolfgang Weiler.
What follows is the GDV annual chart for weather-related damage (in 2019- based euros):









Source: GDV
Insured damage has been below the average for 6 consecutive years, despite, the alarming tones one reads in the GDV press release.
There were also fewer losses due to storms and heavy rain in property insurance. Windstorm and hail and other natural hazards such as heavy rain caused damage amounting to EUR 2.2 billion, which is below the long-term average of EUR 2.7 billion.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The below-average balance should not hide the fact that there have been repeated heavy local rains with high damages”, Weiler said. “All in all, the year 2019 stands for a number of severe storms, great heat and severe local flooding and is therefore characteristic of extreme weather in Germany as well.”
Experts: Central Europe weather “not more extreme”
Meanwhile Die kalte Sonne site here comments:
Fact: The weather in Central Europe has NOT become more extreme. The only exception is heat waves. The Austrian Central Institute for Meteorology (ZAMG) states that a trend towards more extreme weather in Austria is generally not noticeable:
‘It should be anticipated from the detailed discussion of the development of extreme values in the following sections heat (air temperature) heavy precipitation (precipitation) and storms (wind) that all in all the climate has not become more extreme in the last 200 years. According to the only suitable basis for this assertion – long and quality-checked measurement data – climate variability in Southern Central Europe remained the same or even decreased’.
A similar assumption can be made for the neighboring country Germany. For transparency reasons, the German Insurance Association (GDV) should finally admit this to its customers. Instead, the press release concludes with an advertising message that citizens should please insure themselves even more comprehensively against extreme weather.”
!!!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman broadcaster RTL here reported how Northern Hemisphere snow mass has reached the highest level in years.

Image: Finnish Meteorological Institute
The Finnish Meteorological Institute (FMI) reports the total amount of snow in the northern hemisphere this winter season has been well above the long-term average from 1982 to 2012.
This will come as a surprise to Europeans, who have seen one of the mildest winters on record. According to RTL, “In those places where it was cold enough for snow at all, there was a lot of snow. The snow is meters high, higher than usual.”
Snow cover trending upwards since 1990


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Looking at the northern Hemisphere snow cover charts from Rutgers University Global Snow Lab, northern hemisphere snow cover (area) has been on the rise since reaching a low in 1990.

Chart: Rutgers University Snow Lab.
Record-breaking snow across Montana and South Dakota
Meanwhile weather site Electroverse reports of “record breaking February snowfall” burying Montana and South Dakota. According to Electroverse, “The cold times are returning” due to reversing natural cycles.
Record Breaking February Snowfall Buries the U.S. States of Montana and South Dakota



		jQuery(document).ready(function(){
			jQuery('#dd_51c08b0b0f83667272b40339623e8ea9').on('change', function() {
			  jQuery('#amount_51c08b0b0f83667272b40339623e8ea9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBoth during the last interglacial (~120,000 years ago) and from roughly 2000 to 7000 years ago, relative sea levels were from 6-10 meters to 1-3 meters higher than they are today, respectively.
For a list of over 100 other scientific papers indicating sea levels across the world were multiple meters higher when Earth’s CO2 concentrations were about 150 ppm lower than they are today (~260 ppm), see our database here.
The Mid-Holocene, 2000-7000 years ago
Lopez-Belzunce et al., 2020 (Mediterranean)
“Regarding the stabilization of the RSL [relative sea level], our data show it to be 1.20 m above the present-day level at 3000 cal yr BP and 1 m higher at 2000 cal yr BP.”
Burley et al., 2020  (Polynesia)
“At the time of first Lapita arrival at Nukuleka, sea levels were 1.2–1.4 m higher than present (Dickinson 2007).”
Lopes et al., 2020 (Brazil)
“The late Pleistocene-middle Holocene post-glacial marine transgression (PMT) that started around 18 ka b2k in response to the melting of ice caps and glaciers, together with increased precipitation, would have led to another lake highstand (Figure 3A). Sea-level curves obtained from several sites along the Brazilian coast show that a mean sea level (m.s.l.) equal to the present one was reached at ~7 ka b2k, and continued to rise until reaching up to +5 meters between 6 and 5 ka b2k (Martin et al., 2003; Angulo et al., 2006). In the CPRS the PMT formed the Barrier IV, and the estimates based on geologic and fossil records indicate that it reached amplitude of about 2-3 meters above the present m.s.l. (Barboza and Tomazelli, 2003; Caron, 2007; Lima et al., 2013; Dillenburg et al., 2017).”
“The altitude of the terrace T3 above the fossils of Toxodon found in situ indicates this was cut by the Holocene sea-level highstand that reached a maximum altitude of 3 meters [above present] between 6 and 5.1 ka b2k. At that time Mirim Lake was invaded by the Atlantic Ocean through Taim and São Gonçalo channel, becoming a large paleo-lagoon with conditions suitable for its occupation by marine organisms, including sharks, rays, teleost fishes and whales. The coastal waters were warmer than today, as indicated by the presence of fossils of the shark Carcharhinus leucas, common in tropical areas.”

Image Source: Lopes et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Brocx and Semeniuk, 2020 (Western Australia)
“The Holocene stratigraphy in the Walpole–Nornalup Inlet Estuary shows that mean sea level was 1 m higher than present some 2900–1200 years BP (Semeniuk et al., 2011).”
Helfensdorfer, 2020 (Australia)
“This study presents a well-constrained model of the geomorphic evolution of the lower Murray River and Murray estuary with a specific focus on the response of the system to the Holocene sea-level highstand. Hydrodynamic modelling of the lower Murray River and Murray estuary was conducted to evaluate the primary drivers of palaeo-environmental change during the Holocene and constrain the plausible response of the Murray estuary to the +2 m higher-than-present sea level of the Holocene sea-level highstand.”
Martin et al., 2020 (Western Australia)
“Sea level high stands (~2 m higher than present) occurred at ~7 and 4 ka (Gouramanis et al., 2012) that likely caused seawater intrusion events into the aquifer”
The Last Interglacial (LIG), ~120,000 years ago
Muh et al., 2020  (Bahamas, Bermuda)
“Corals with closed-system histories collected from patch reefs on NPI have ages of 128-118 ka and ooids/peloids from beach ridges have closed-system ages of 128-116 ka. Elevations of patch reefs indicate a LIG paleo-sea level of at least ∼7 m to ∼9 m above present. Beach ridge sediments indicate paleo-sea levels of ∼5 m to ∼14 m (assuming subsidence, ∼7 m to ∼16 m) above present during the LIG. …. Results of this study show that at the end of the LIG paleo-sea levels could have been as high as 11-13 m above present (at localities close to North American ice sheets) to as little as 5-8 m above present (at localities distant from North American ice sheets).”
Helm et al., 2020  (South Africa)
“Around 126 ka, sea levels were 6.6-8 m higher than present levels on the Cape south coast [of South Africa]. … Chronological context11 suggests an age of MIS 5e (the Last Interglacial). As sea levels during MIS 5e in this area were up to 6-8 m higher than at present, a warmer climate capable of supporting large reptiles on the Cape south coast can be inferred.”
Share this...FacebookTwitter "
"



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea89e324b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe Corona crisis has reduced car traffic, yet air quality has not improved. This suggests that the automobile’s role in air pollution has been vastly exaggerated. 

Image: NASA (public domain)
There are fewer cars on the road in major German cities due cities to the massive COVID-19 restrictions and “green zones”, “and yet it apparently does not look as if this will significantly improve air quality,” reports the online German Nordkurier here.
“Despite existing driving bans in large cities and the corona protection measures, nitrogen oxide pollution remains the same and is even increasing in some cases, according to the FDP in Mecklenburg-Western Pomerania.”
Environmentalists and climate activists like to claim that modern cars and industry have been polluting the air with dangerous particulate matter or nitrogen dioxide, but since restrictions were put in place 3 weeks ago, no improvement has been detected thus far.
“Less of an impact than previously assumed”
“The only conclusion that can be drawn from this is that air pollution from internal combustion engines has less of an impact than previously assumed,” said René Domke, regional chairman of the Free Democrat Party (FDP), in the north German city of Schwerin.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“He demanded that the future of internal combustion engines must be managed objectively again after these figures become known,” report Nordkurier.
Skeptics claim that there’s been way too much hysteria and baseless activism surrounding automobile traffic and the pollution they allegedly cause.
“Correlation broken, causality clearly refuted. Any further discussion about driving bans in view of these undeniable facts is completely unnecessary,” said Mr. Domke.
“Renewable” wood-burning the culprit
The real air quality problem now in Germany, critics say, is caused by the ever-increasing use of wood-burning for home heating, especially in the wintertime.
Often viewed  as a renewable source of heating energy, wood-burning increasingly has been shown to be a major cause of pollution in German cities. The leftist Guardian here reported in 2018, for example, that wood-burning has in fact been “suffocating cities” in the UK.
Since traffic has been reduced over the past weeks, air quality has  not improved, indicating that pollution from automobiles has long been over-hyped.
According to Domke, “The downright hysteria which the Deutsche Umwelthilfe and other NGOs have created  combustion engines, which at times dominated everyday political life, was and is completely unfounded.”
“In order to ensure mobility in the mainly rural Mecklenburg-Western Pomerania, we urgently need individual transport,” said Domke.


		jQuery(document).ready(function(){
			jQuery('#dd_3c88cf748d15ad302ac45b5484cb6404').on('change', function() {
			  jQuery('#amount_3c88cf748d15ad302ac45b5484cb6404').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Last week I was informed that the MBNA credit card I’ve had for years was “acquired” by Bank of America. Ok no big deal, mergers go around all the time as big corporations get even bigger by swallowing other corporations whole.
But I was shocked to discover that my interest rate had soared. It was around 12% previously, but now, thanks to the merger and corporate greed used to pay for that merger, my interest rate was raised to: (drum roll, and please sit down while reading) 24.97%  !!  The friendly note from the “BofA customer satisfaction center: said  “I could of course pay off the balance and avoid the rate change”. Gee, thanks.
WTH? I have excellent credit, no late payments on this card, and I’ve been with BofA since 1994 when Jolene Francis signed me up. I’ve had business loans, home loans, car loans, and my savings, personal, and business checking account with BofA since then, and thanks to the same sort of corporate weasel thinking where the Bank is more important than the customer…one by one, all of these accounts where transferred to other more sensible banks when BofA announced some amazingly stupid new “plan” to improve “customer satisfaction”.
Here’s some insight into the national credit card problem by SF Chronicle columinist  David Lazarus
Now, I’d point out that an interest rate of 25% generally makes it impossible to pay off a loan if the consumer pays the minimum payment listed on the bill. So it became clear to me that BofA was financing their shiny new merger with MBNA. Despite layoffs at MBNA designed to sweeten BofA’s bottom line, they just couldn’t resist sticking consumers with the bill for their merger.
So today marks my end of my 12 year relationship with Bank of America. Hello Discover Card. Hello WaMu.
It amazes me that banks keep pulling these kind of Enronesque stunts and still keep customers. They certainly lost me, and my company business, because they simply got too greedy. The only way consumers can fight back against these sort of practices is to cancel accounts and do balance transfers to more reasonable companies. For example, Discover offered me a balance transfer at a very VERY low interest rate, a bargain compared to 25% from BofA!
From David Lazarus column I found this nugget of wisdom:
“All in all, the world of plastic is an uneven playing field. This is something that should never be far from mind as you spend the next month or so probably running up your biggest credit card bills of the year. “
Corporate mergers never seem to do anybody any good. Debt is acquired with the mergers, and workers get laid off to finance it and the customers get stuck with the bill over the long term. Customer service usually takes a nosedive. Shareholders may earn dividends, and the inner sanctum of corporate weasels that structured the deal usually make out like bandits. But the customer usually suffers at their expense.
I think on the whole, corporate mergers are bad for America, as is excessive credit card debt.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9a63721',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In a nod to alternate energy, Ford annouced a new sports sedan with roots in the Mustang design that will run on E85 ethanol fuel. It’s called the Ford Interceptor. It is shown above. The fuel it can burn, E85,  is a mixture of 85% ethanol and 15% gasoline.
Ford says: âOur customer target for this powerful masculine sedan was a man with a family,â? Horbury said. âHeâs essentially a good guy, but a bit mischevious. He loves power and performance. But ultimately, heâs responsible. When he has his family on board, he values new safety technology as well as a powerful engine that runs on E-85 ethanol.â?
You may have read in the ER where Rick Keene went to South America to view how ethanol is produced and used as an alternative to gasoline. In Brazil, ethanol is produced from sugar cane and fuels a major portion of their automobiles. It was Brazils response to the Opec induced oil crisis of the 1970’s, and it’s a good idea.
In the USA, ethanol is produced from corn, but its production efficiency is reportedly not as good as with sugar cane. Nonetheless, demand for ethanol continues to grow, with new production facilities coming online each year.

Ford has issued the following press release:
Building on its legacy of bold muscle cars, Ford is introducing a modern, all-American sedan concept that combines âBuilt Ford Toughâ? attitude with the sporty elegance of its iconic 1960s sedans.
The Ford Interceptor concept comes equipped with a manual six-speed gearbox mated to a Ford Racing 5.0-liter V-8 Cammer engine that delivers 400 horsepower and runs on E-85 ethanol.
âThis concept celebrates the best of American muscle, showing customers what âmodern muscleâ is all about,â? said Peter Horbury, executive director â Design, The Americas. âThe Interceptor concept is much like a Marine in dress uniform. He looks smart and elegant but you can see the raw power that lies beneath.â?
Flexing Modern Muscle
The Ford Interceptor conceptâs exterior design features substantial, sometimes brutish, surfaces and sections that give the concept its modern, powerful look.
The Mustang-based concept features a traditional rear-wheel drive proportion that includes a short front overhang, long rear overhang and extended dash-to-axle ratio.
The Ford Interceptor also has a low cabin and higher beltline, adding to the vehicleâs attitude and sense of mystery.
âThe Ford Interceptor concept is a pure sedan that speaks to performance car lovers everywhere,â? said Freeman Thomas, director, North American Strategic Design. âThese people might need more space, but they still appreciate the power and attitude that cars like this represent,â?
Painted a deep blue, the Ford Interceptor conceptâs strength exudes from its strong, high shoulders. And much like on last yearâs Ford F-250 Super Chief pickup concept, a single character line runs the length of the body side, slightly sloping downward as it reaches the back of the sedan.
This adds wedge to the car, making it dynamic, without detracting from its smooth, clean design.
Signature Ford touches include the horizontal three-bar grille, which has been structurally integrated into the bumper beam, as well as âsquirclesâ? â or professionally square circle-shaped graphics â inside and out.
As a nod to performance purists, the ultimate muscle lies under the powered clamshell âshakerâ? hood, which caps a thoroughly detailed engine compartment that houses a 5.0-liter V-8 Cammer engine.
This is an upgraded variant of the 4.6-liter engine under the hood of the current production Mustang GT. The Cammer modular engine powered Ford Racingâs FR500C race car to the top of the Grand Am Cupâs GS class, achieving five victories on its way to the Drivers, Manufacturers and Team Championships in its first season of competition.
The Interceptor conceptâs Cammer engine is mated to a manual six-speed transmission. The car, equipped with 22-inch wheels, also features a solid rear axle for more hard-core performance feel.
Attitude Within
Inside, the Ford Interceptor concept is sleek and thoroughly modern, completed in contrasting black leather and metal finishes.
The dash, headliner and thick steering wheel are leather-wrapped. Plus, the Interceptor conceptâs four low-back bucket seats are wrapped in thick black belt leather with exposed-edge seams and contrasting caramel stitching. The seats are accented with Ford GT-inspired squircle grommets finished with Titan Metal painted inserts.
Squircle accents are repeated in the conceptâs door trims, floor, console and instrument panel.
Designed within a pair of squircles, the speedometer and tachometer are eye-catching. The needles for both start at center and move opposite each other as the speed and RPM climb.
Other clever touches include retractable headrests that deploy from the roof when the car is parked. They adjust fore and aft, as well as up and down for each occupant. Audio control panel and climate controls also are stowable.
On the other hand, the gated six-speed shifter is exposed, just waiting to be thrown into gear.
âThe Interceptor concept is a sedan â but with the heart and soul of a performance car,â Thomas said. âThis car is about restraint â and not clouding the driving experience with too much technology. There arenât a lot of layers between the driver and the road with this car.â?
Safer travels
For safety, the Interceptor concept incorporates Fordâs patented four-point âbelt and suspendersâ? safety belt design in all four seats and inflatable seat belts in the rear.
While current three-point safety belts are extremely effective in reducing the risk of injury in a crash, Ford Motor Company is researching these two potential safety belt technologies as possible ways to further reduce injury risk in vehicle crashes.
A number of technical challenges still need to be overcome before such restraint systems could ever be used, but these technologies might one day further enhance safety belt effectiveness.
The four-point belt showcases a possible next-generation safety belt that is more comfortable and easier to use than traditional three-point belts, according to consumer research. Additionally, inflatable belts have been included in the rear seat of the concept to help better protect occupants in a variety of crashes.
Ford Interceptor Concept
Powertrain
5.0-liter Cammer V-8
Chassis lengths
Overall length…………………………………….201.6 in.
Wheelbase………………………………………….120.8 in.
Overall width……………………………………….76.4 in.
Overall height at curb…………………………….54.8 in.
Track width
Front…………………………………………………..66.5 in.
Rear……………………………………………………67.8 in.
Suspension
Front………………….. Double wishbone-independent
Rear…………………….3-Link Design with Panhard Rod
Headroom
Front…………………………………………………..37.5 in.
Second Row…………………………………………35.9 in.
Legroom
Front…………………………………………………..42.3 in.
Second Row…………………………………………35.6 in.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea903ff34',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe leading media worldwide cranked up the volume when it spread the news of how a statement had been published in the journal BioScience. The statement was a collaboration of “over 11,000 from 153 nations”.
The Guardian, for example reported: “The world’s people face ‘untold suffering due to the climate crisis’ unless there are major transformations to global society.”
Like most major media outlets around the world, the Guardian handled the “statement” as if it were the final confirmation needed to finally end any further discussion and hesitation on rapidly moving to a new, transformed “global society”.

Slick sales job. Dr. Thomas Newsome falsely claiming over 11,000 “scientists” support the climate statement. Image cropped from video by University of Sydney. 
The statement more hoax than scientific declaration
Days later, after a more careful scrutiny of the list – which the media failed to carry out, it was uncovered that the list of signatories was a declaration of scientific and media sloppiness and deception. One of the signatories was even cartoon character “Mickey Mouse”. But it gets worse than that.
11,224 list analyzed by Japanese blogger
Since then Japanese climate science skeptic and blogger, Kirye, spent dozens of hours thoroughly compiling and evaluating the 11,224 signatories using an Excel spreadsheet. Her findings have added greater clarity and exposed the true extent of the once media ballyhooed statement now turned hoax.
Kirye’s spreadsheet here.
5 of 11,224 a “climate scientist”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of the 11,224 signatories, JUST FIVE (5) claimed to be a “climate scientist”.
Only 4 were meteorologists.
A vast number did not even state PhD or professor as their professional title/discipline. Only 2,796 (24.9%) had “professor” in their title. 1,481 (13.2%) of the signatories stated some form of PhD, including PhD “candidate”.
A total of 1,021 had “doctor in their title, i.e. only 9.1%. Many in an unrelated field.
303 of the signatories listed no professional title at all!
34 names had to be discarded altogether because they were invalid.
New climate experts: nephrologists, philiologists, pharmacists!
The vast majority were active in fields totally unrelated to climate science, such as “philiogist”, psychologist, CEO, political scientist, pharmacist, medical doctor, primatologist, physiopathology of the mitochondria, sociologist, industrial systems, nanoscientist, genetics, nephrologist, economist. biotech engineer, foreign language teacher, etc.  In other words, it’s a list hyperinflated by unqualified climate activists. Others were affiliated with environmental activist groups.
“Disservice” to science …”blow to credibility”
Thie list and media handling were in fact so sloppy that it compelled German geologist and hjournalist Axel Bojanowski to write at Cicero here how the statement and list of signatories were “a disservice” to climate science and “a deep blow to the credibility of research (and the media), not only because the list of signatories has apparently been published without verification.”
“Mocks media quality control”
The former Der Spiegel science journalist added: “The fact that numerous representatives of environmental associations are among the signatories and many others without a professional title makes one doubt their scientific character” and that it “mocks” the “media’s quality control function.”
Share this...FacebookTwitter "
"

The first Earth Day, in 1970, was celebrated after a wave of environmentalism swept the nation. Many give credit to Rachel Carson’s 1962 book, _Silent Spring_ , which popularized the notion of large‐​scale chemical pollution, for igniting the movement.



But she was really feeding off of a concept developed a few years earlier. The “precautionary principle” was conceptualized when the National Academy of Sciences proposed a radical change in the risk assessment of exposure to radiation and carcinogens. It recommended changing the regulatory paradigm from a “threshold dose” model to a linear one.



The threshold paradigm was what one might call common sense. It held that humans could tolerate small doses of things that, in larger doses, could be harmful.



Sunlight is a perfect example. Low doses are actually required for survival, as ultraviolet radiation — the same general type that causes sunburn — catalyzes the formation of Vitamin D. But, as is obvious to anyone who lives in a sun‐​drenched area, excessive exposure can lead to death in the near term (from dehydration) or the longer term (from skin cancer).



The “linear model” assumes that just a single molecule of a carcinogen or a single ionization from an X‐​ray can induce cancer. The enthusiasm spawned by Earth Day soon gave us brand new regulatory agencies such as the Environmental Protection Agency and the Occupational Safety and Health Administration. The EPA routinely applies the linear model to carcinogens.





Environmental regulations based on the “linear model” are having a negative impact, not only on societal costs, but on our health as well.



The linear model is a case study in the unintended consequences of the desire to do good. In this case, an ideologically driven scientist, Nobel Prize laureate Herman Muller, whose research formed the basis for EPA’s model, led the charge. A very powerful figure in health physics, he is now known to have marginalized and obstructed the publication of any research that provided evidence counter to the linear model.



If that sounds like the way senior climate scientists were found to behave in the famous 2009 “Climate‐​gate” emails, it should.



The regulatory agencies fell in line, as did a compliant scientific community and a media that was afraid to dig deeper. Every country followed the U.S.’ lead.



The linear model is rigid, absolute and wrong. We now know that there are so many flaws or holes in the linear dose response model that it looks more like Swiss cheese. The resulting environmental regulations are having a negative impact, not only on societal costs, but on our health as well.



Over the past several decades, considerable research has revealed a plethora of life‐​saving adaptive processes that can be used to enhance the quality of life and to extend life. Our cells are flexible, adaptive and can actually be strengthened via low‐​level exposure to a large number of compounds that the EPA would like to regulate down to the last molecule.



Instead of preventing harm, the precautionary principle actually causes harm. The entire therapeutic model is built around the notion that certain compounds that are highly toxic in large doses can be life‐​enhancing and life‐​extending in low ones.



How can the regulatory community accept the linear model when so many of its senior practitioners are living lives that prove the opposite? Many of these aging regulators are taking ACE (angiotensin converting enzyme) inhibitors to control blood pressure. The original ACE inhibitor, Captopril, is the active substance in the venom of the Brazilian viper. A lot will kill you very quickly. A little could extend your life for decades.



We need a new Earth Day. It should be dedicated to righting the past deceptions and correcting the ongoing errors in environmental regulation. It should be one that acknowledges our adaptive responses to what, in high doses, can cause cancer, but, in low doses, can improve our well‐​being.
"
"
Share this...FacebookTwitterIn recent decades there have been “notable cooling trends” throughout many regions of the globe according to several new studies.
A year ago NoTricksZone (NTZ) announced Greenland Has Been Cooling In Recent Years – 26 Of Its 47 Largest Glaciers Now Stable Or Gaining Ice.
Six months ago NTZ cited several scientific papers indicating The Region From 50-70°S Has Cooled Since The 1980s As North Atlantic SSTs Have Cooled 1°C Since 2004.
Three months ago we reported A Massive Cooling Of 2°C In 8 Years (2008-2016) Has Jolted Large Regions Of The North Atlantic.
A few days ago we shared a New Study Finds The Larsen Ice Shelf (Antarctic Peninsula) Has Cooled More Than 2°C Since 1991.
Now we shine the light on 3 more studies that assess “Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends” from 2002 to 2013 (Xu et al., 2020), and both West and East Antarctica have been rapidly cooling since the mid-2000s (Hrbáček and Uxa, 2020 and Fatras et al., 2020).
At some point the question may need to be asked: Just how global is recent “global warming”?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Xu et al., 2020
“Concurrent with the slowdown of global warming during 2002–2013, the wintertime land surface air temperatures over Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends. … The slowdown concurs with a negative phase of the Pacific Decadal Oscillation (PDO), indicating that PDO plays an important role in modulating the global warming signal. Not all ensemble members capture the cooling trends over the continents, suggesting additional contribution from internal atmospheric variability.”

Image Source: Xu et al., 2020
Hrbáček and Uxa, 2020
“A significant air temperature decrease started around 2000 along most of the Western AP. The cooling triggered by natural variability of cyclonic activity and increasing sea‐ice concentrations near coastlines caused MAAT trends of −0.16 to 0.05°C y−1 in the period 2006–2015. In contrast, the MAAT on JRI was increasing at a non‐significant rate of 0.10°C y−1, which corresponds to observations from other sites of the north‐eastern AP where positive, but non‐significant, trends between 0.02 and 0.08°C y−1 have been reported.10 Unlike MAAT , there was a non‐significant negative trend of −0.05°C y−1 for MAGT 5. Interestingly, the MSAT and MSGT 5 trends were positive only in autumn (MAM), at 0.30 and 0.13°C y−1, respectively, while they were negative in the other three seasons. Yet, the north‐eastern AP region exhibited a MAAT more than 1°C lower in the period 2006–2015 compared to 1996–2005, and autumn (MAM) air temperature was even about 1.5°C lower.”

Image Source: Hrbáček and Uxa, 2020
Fatras et al., 2020
“The mean annual Sea Surface Temperature (SST) variations from ECMWF ERA interim database are displayed on Fig. 4b. They present no particular trend for the 1979–2018 period, with variations contained between -1°C and +0.15°C. Nevertheless, the mean temperature between 1979 and 2000 is -0.45°C and decreases to -0.64°C during the 2000–2015 period.”

Image Source: Fatras et al., 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSo far no signs of another super hot-dry summer for Europe, which media have been alarming about. 
Veteran Swiss meteorologist Jörg Kachelmann tweeted the 45-day projections for Europe.
In terms of precipitation, Europe saw drought conditions over the past two summers (2018 and 2019) and climate alarmists claimed this would be the new normal. And recently the European public got bombarded by media reports stemming from the World Meteorological Organization (WMO) of a blistering super hot and dry summer this year.
But look what the ECMWF now projects for the next 45 days (upper chart). Kachelmann comments: “Even according to the latest 46-day trend of the ECMWF, which runs to mid-July, the drought summer seems to be completely called off for the time being.”

Ein ""Hitzesommer"" war noch nie in den Vorhersagen drin und das scheint zumindest bis Mitte Juli auch so zu bleiben, auch wenn es später im Juni wieder sommerlicher wird. pic.twitter.com/6G97p1NJmY
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 5, 2020

And in terms of temperature, see the lower chart, nothing unusual is projected to happen over the next 45 days. The Swiss meteorologist notes: “A ‘hot summer’ was never in the forecasts, and it seems it’ll stay that way at least until mid-July, even if it gets more summery again later in June.”
Keep in mind these long range forecasts come with much uncertainty, and change with every run. But right now it looks like all the recent doomsday projections of a scorched euro-summer were overblown.


		jQuery(document).ready(function(){
			jQuery('#dd_ffb24e69e1916c9a909c9e37aa3ef08a').on('change', function() {
			  jQuery('#amount_ffb24e69e1916c9a909c9e37aa3ef08a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA new study (Stallinga, 2020) assesses the climate sensitivity to rising CO2 concentrations is just 0.0014°C per ppm. 
Dr. Peter Stallinga has published a comprehensive analysis of the Earth’s greenhouse effect. He finds an inconsequential role for CO2.
Doubling CO2 from 350 to 700 ppm yields a warming of less than 0.5°C (500 mK).
Feedbacks to warming are likely negative, as adding CO2 may only serve to speed up natural return-to-equilibrium processes.
As for absorption-reemission perturbation from CO2, “there is nothing CO2 would add to the current heat balance in the atmosphere.”

Image Source: Stallinga, 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A portion of Dr. Stallinga’s paper worth highlighting – which he mentions only in passing – refers to the early history of the Earth’s greenhouse effect paradigm.
K. Ångström receives little attention as a pioneer of the conceptualization that warming and cooling resul from radiative imbalances within a planetary greenhouse effect.
About 120 years ago, Ångström (1900) contradicted the oft-cited Arrhenius (1896) – the atmospheric physicist referred to by proponents of anthropogenic global warming.
Ångström suggested Earth’s greenhouse effect is already saturated in its current (1900) state, and therefore increasing CO2 will have “no effect whatsoever” on climate (Stallinga, 2020).
Ångström’s conclusions were largely ignored.

Image Source: Arrhenius, 1896 and Stallinga, 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGreenland’s largest glacier (Jakobshavn) has quite abruptly thickened since 2016. The thickening has been so profound the ice elevations are nearly back to 2010-2011 levels. The nearby ocean has cooled ~1.5°C – a return to 1980s-era temperatures.
The world’s glaciers have not been following along with the CO2-driven catastrophic melting narrative.
Alaska

For example, in a study of 50 Alaskan glaciers for the warming period between 1972-2012, researchers (McNabb and Hock, 2014) found there was

“…no corresponding change in the number of glaciers retreating nor do we see corresponding acceleration of retreat rates. To the contrary, many glaciers in the region have advanced…”

Image Source: McNabb and Hock, 2014
Antarctica
In the Southern Hemisphere, an accumulating collection of (29) referenced studies (Lüning et al.,2019) indicate that not only has the Southern Ocean, Antarctic Peninsula, West Antarctica, and East Antarctica been cooling or not warming in recent decades, but many regional glaciers have begun advancing again.

Image Source: Lüning et al.,2019
Greenland
Greenland’s ice sheet mass losses have significantly decelerated since 2013 – a reversal from the rapid retreat from the 1990s to 2012 driven by cloud forcing and the NAO (Ruan et al., 2019).
The 47 largest Greenland glaciers also experienced a “relatively stable” period of rather insignificant retreat from 2013 to 2018 (Andersen et al., 2019).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Only 21 of the 47 Greenland glaciers retreated in 2018, 12 advanced, and the other 14 showed no trends in either direction (Polar Portal, 2019).
Greenland’s largest glacier, Jakobshavn, earned headlines in 2019 for it’s surprising and non-predicted rapid thickening in recent years.


Image Source: BBC, 2019
New Study
A new study (Joughin et al., 2020) finds that the Jakobshavn glacier thickening that began in 2016 has continued apace, and ice elevation has now nearly completely returned to 2010/2011 amplitudes.
The authors attribute much of the glacier advance to the rapid 1.5°C ocean cooling impacting the region in recent years.
Ocean temperatures have returned to 1980s-era levels.

Image Source: Joughin et al., 2020
Share this...FacebookTwitter "
"

 ** _Editor’s note_** _:_ _In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change. _



_As part of ourProject on Threat Inflation, Cato will be republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. _



_The first response comes from Brendan Rittenhouse Green, an assistant professor at the University of Cincinnati, and a recently namedCato adjunct scholar. _



——-



Many world leaders today could tell you, earnestly and genuinely, that their country faces major security threats. Historically, such threats have been endemic to the international system, and they have tended to consume most of the time, attention, and social resources of national policymakers. Moreover, statesmen from the past and present alike could probably adopt a common definition of what a “security threat” is: the possibility of outside actors using large scale violence to menace a state’s sovereignty, territorial integrity, or the physical safety of a substantial portion of its populace; or the emergence of a state that could obtain enough material power to do these things.



But the modern United States does not have this kind of problem. To be sure, its foreign policy discourse has been suffused with the language of security threats for a hundred years. The regnant American grand strategy, which I term primacy, is justified largely—though not exclusively—on security grounds. Yet no state with enough military power to reach inside the Western Hemisphere is likely to emerge any time soon. In short, there is a major disjunction between the language sometimes used to explain and justify American foreign policy commitments and the actual purpose of its strategy.



This, at any rate, was the premise of my essay “Security Threats in Contemporary World Politics.” In it, I made three basic arguments. First, I tried to show that America’s most powerful rival, China, looks nothing like the most plausible past security threats faced by the United States—the Nazi and Soviet empires. Indeed, China would have to jump over a series enormous hurdles before it even came within shouting distance of such dangerous states. Second, I claimed that the political commitments entailed by primacy had only a small prospect of reducing competition in China’s backyard below what it otherwise might be. That is, primacy has a “goldilocks problem”: the highly revisionist states that would propel any East Asian competition are likely to be either absent, or too highly motivated for American power to discourage them from risky behavior. Third, I argued that American political commitments were themselves the most plausible sources of threats to national security. Though unlikely to successfully depress regional competition, primacy’s political connections provide several mechanisms by which America could become involved in a major war.



Looking back on this essay from nearly a decade’s distance, I continue to endorse its major claims. Though I might make a few marginal changes here and there, my views are still roughly the same. But national security discourse, recent history, and my own intellectual temperament have all been altered in important ways. These changes would make for a very different essay, were it written today.



For one thing, the essay’s overwhelming focus on security issues seems less necessary today. Over the past decade, national security discourse has increasingly centered on the defense of the “liberal (or rules‐​based) international order” as the key object of American foreign policy. I think the idea of the “order” borders on conceptually incoherent. But it does have a key virtue: it has enabled more and more analysts to admit that American grand strategy is concerned with something other than traditional security problems. It has therefore made the trade‐​off at the heart of American grand strategy more obvious: American leaders are risking major war, and thereby making the American people less secure, for the purpose of shaping the international environment in ways they consider favorable. Was I re‐​writing this essay today, I would devote more attention to examining the supposed benefits of the international order. Essays by Daniel Drezner and Eugene Gholz from _A Dangerous World?_ provide excellent examples of this kind of analysis.



Another idea I would emphasize more is the idea of “tail risk.” The world today is living through a global pandemic, which will probably kill hundreds of thousands of people and induce the worst economic crisis since the Great Depression. This turn of events was unexpected, even though the potential for a devastating global pandemic has been well‐​known for decades. Nevertheless, most countries were underprepared.



Many rare phenomena pose a similar problem. Society lacks the data that would justify the assumption that certain kinds of apparently rare events are in fact extreme outliers on a bell‐​shaped curve of event frequencies, rather than merely uncommon results of some other kind of frequency distribution.. In fact, it turns out that many rare events—for example, earthquakes, rogue waves, and importantly, war—do not follow a normal distribution. In many cases, the statistical likelihood of such events is far greater than the traditional bell‐​shaped curve would imply—the tail ends of the actual distribution of events are “fat.”



Societies are therefore likely to underestimate the risk associated with rare events. I suspect that the probability that America’s primacy strategy will produce a major war is similarly underestimated. The probability may be relatively low, but the scale of disaster would be very large. Over the long‐​term I worry that the chances of such a war would exceed the tolerance threshold of even the most aggressive strategist. Considering and analyzing this possibility seems like an especially salient task in light of recent events.



Finally, if I wrote the essay today, I would focus more attention on the idea of “second best” strategies. Early in the last decade, I still had something of the zeal of youth about me. I retained hopes that normal politics might produce non‐​trivial change in American grand strategy. After all, the country had been somewhat chastened by its exhausting wars in Southwest Asia. The Tea Party, whatever its faults, was a live political force that had managed to achieve temporary restraint in the defense budget, a feat whose last occurrence had required the collapse of the Soviet Union. Obama was pursuing a second‐​term foreign policy that, if not exactly worth defending, at least challenged the elite consensus on grand strategy in a couple of respects.



Well, there is nothing that the world likes better than nice, tasty hopes. The forces enumerated at the end of Christopher Preble and John Glaser’s lead essay turned out to be significantly stronger than I estimated. American power has proven so extensive that a grand strategy explicitly justified in terms of many varied goals like the liberal order is now plausible to the foreign policy establishment. The material and ideological consensus in favor of primacy among the national security elite has proven so robust that American commitments have been able to resist the election of a president like Donald Trump, who is no one’s idea of an internationalist. The American people turned out to give even less of a damn about foreign policy than I expected.



Today I believe that the probability of normal politics producing a genuinely restrained grand strategy is exceedingly slight. The best hope for a major change is probably a crisis that exposes the unexpected risks and costs of primacy. For this reason alone, the task of making the case for restraint remains vital: policymakers will need to have good ideas lying around if and when the bankruptcy of primacy is revealed.



However, I increasingly believe that more effort should be devoted among partisans of restraint to “second‐​best” policies, in case my pessimistic political assessment proves out. And I am not confident that the standard answer—that the second‐​best policy is “less of whatever primacy is proposing”—is always true.



For instance, if we are not going to abandon American alliances, I am not certain that loosening those ties is worthwhile, as it may encourage bad behavior among allies and adversaries alike. If we are going to retain American political commitments, then I suspect that will require more robust military capabilities than I would like as a matter of first preference. I worry that grand strategies may best be plotted on a U‐​shaped curve, where the tail strategies of primacy and restraint both produce reasonably coherent and stable outcomes, but where the strategies in the middle—“off-shore balancing,” “selective engagement,” and “liberal internationalism”—turn out to be ineffective and destabilizing to world politics.



But working out whether there is anything to these concerns would be the subject of a completely different essay. And the present essay, I believe, retains real value. Its fundamental conclusion is still true: “the United States spends hundreds of billions of dollars a year—and risks war—largely to stop other people from fighting among themselves. The common story that reducing regional competition abroad makes America more secure at home is close to being backwards.”



My essay is not the most original or brilliant exposition of this basic point — but as bottom line conclusions go, I think one could do a lot worse.



–Brendan Rittenhouse Green



Cincinnati, OH
"
"

The major criticism that East Asian officials would make of the outgoing Bush administration’s foreign policy would be Washington’s focus on the geostrategic problems in the broader Middle East in the past eight years, and the resulting sidelining of China and most of East Asia on the US global agenda. This neglect of China needs to change. 



Secretary of State Condoleezza Rice’s recent trips to South Asia (to try to defuse Indo‐​Pakistani tensions in the aftermath of the Mumbai terrorism) and to the Middle East (to attempt to re‐​energise Israeli‐​Palestinian negotiations) have been highlighted in leading US newspapers. Treasury Secretary Henry Paulson’s meetings in Beijing, as part of the ongoing Strategic Economic Dialogue, have, however, only been minor news as far as the US media is concerned. 



After president‐​elect Barack Obama recently unveiled his national security team, most of the discussion among Washington’s pundits centred on how the selection of Hillary Rodham Clinton as secretary of state and the retaining of Robert Gates as defence chief would affect US policies in the Middle East. China and East Asia were largely ignored. 



Earlier, in the foreign policy debates during the presidential election campaign, when China was mentioned, it was mostly in the context of criticising its trade policies, and warnings of its rise as a geoeconomic “threat” to US interests. 



Certainly Mr Obama needs capable people in his administration to manage the challenges in the Middle East. But he and his foreign policy aides must realise that all the major geostrategic and geoeconomic problems facing the US in the next four years, including energy policy, climate change, nuclear proliferation — and the current global economic crisis — will require co‐​operation with Beijing. 



During his discussions with officials in Beijing, Mr Paulson expressed concern that lowering the value of the yuan to stimulate the Chinese economy could worsen the US slowdown by keeping Chinese export prices relatively low and import prices high, which would hurt US exporters. In the past, US lawmakers have threatened to punish China if it refuses to change its exchange rate policies. But it is unlikely that Congress will risk a trade war now, given that America’s effort to spend itself out of recession will depend so much on the willingness of the Chinese to continue financing the US deficit. 



This dilemma highlights the need for a long‐​term strategy to manage the Sino‐​US relationship in a way that encourages China to assume greater leadership in multilateral economic organisations like the International Monetary Fund. 



During the cold war, the first question on the minds of America’s allies and rivals following the election of a new president was: “How is he going to handle Moscow?” Today, and in the future, America’s friends and adversaries should be more concerned about the approach a new White House occupant will take towards Beijing. 
"
"

I just completed my first meeting of the City of Chico Sustainability Task Force today and here are a few observations.
First, it seemed to be pretty well rounded, we had public and private sector, business, building industry, CSUC, and regular citizens represented by the 15 appointees.
Second, so far the focus seems to be doing things better, more efficiently, and at less cost. I’m all for that.
Third, everybody seemed to get along, no shouting matches or fistfights broke out.
While the group is still feeling their way, I expect that given the makeup of it, we’ll get some useful suggestions and ideas from it that may very well get implemented as policy someday. I was worried that we might have a group of folks who were so focused on the goal of “green” that we’d see odd policy come from it like our famously silly nuclear weapons ban in the city limits.
I’ll keep you posted. I have a few ideas of my own that I’ll discuss here.
Some folks ask me how I can be against the idea of man-made global warming but for alternate energy. Its simple really, if its more efficient, pollutes less (on any venue) has no social cost, and has a lower operating cost, I’m for it. Mostly I’m for alternate energy becuase California has essentially legislated out the ability to build any traditional forms of energy generation, such as coal, hydro, and nuclear. That leaves wind, solar, and conservation as the future of energy in California. whether you beleive in man-made global warming or not, our future energy needs have to come from some source, so we’d better get started now. If they have beneficial side effects, all the better.
One thing I’m not for is a carbon credits/trading programs. I think the whole idea is simply a cop out and designed to benefit the few that setup these programs. See why in this post.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7711e26',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Lon Glazner, a fellow blogger and local electronics engineer made some comments about my post on the NASA/CSU study on California temperatures. Well that got me started…so below are Lon’s comments and my reply along with a fun technical challenge. For those of you that read this blog, but disagree with my views, I invite you to read this carefully.
Anthony,
You make a number of good points.  Particularly in the fact that the writers may have applied changes in urban temperature measurements over large regions for graphical impact.
As someone who has designed and built electronic temperature sensors I have certain concerns about the data itself.
Unless temperature sensors are regularly calibrated I think it is unreasonable to expect accuracy of greater than a couple of degrees.
Even some that are calibrated may not have good accuracy.  The LM34 which is a commonly used semiconductor for measuring temperature is +/-2 degrees F.  This is pretty typical of analog or digital semconductor sensors.  The temperature error for this part is also non-linear, and so it’s not a simple offset that you have to account for during data collection.  Furthermore, there are lots of additional errors that can creep into a temperature measuring device beyond the sensor itself.
http://www.national.com/pf/LM/LM34.html
One could argue that numerical analysis done on data points would tease out errors.  But if a scientist doesn’t know the exact accuracy of a temperature sensor then they couldn’t account for errors in their system.
Some of the temperature sensing stations may be  very accurate and regularly calibrated.  But maybe they’re not?
I have a hard time trusting that the data is accurate to the level of identifying 1 or 2 degree changes over decades.  This is especially true since the techniques of making these measurements have changes over that time frame.
Lon

Lon, thank you for the comments. FINALLY somebody who understands the kind of biases that creep into temperature measurements!
I’m innately familiar with National Semi’s LM34 and it’s accuracy problems. One of my early jobs at my university as a research assistant was to create remote electronic weather stations. I soon learned how inaccurate many electronic devices can be in temperature measurement.
The problem with the National Weather Service temperature data sets (and world data sets too) is that they are full of biases and errors that I’m not sure have been accurately accounted for. People such as Jim Price, from CSUC who is on the IPCC say they have been, yet nobody has shown me any hard evidence of such. I’d be a lot less skeptical if I could see how the IPCC accounted for temperature measurement biases. But they won’t share.

Some people that I try to explain this to accuse me of splitting hairs. But these bias problems in temperature measurement are quite real.
What works against my arguments about the difficulty in getting accurate temperature records is the everyday simplicity of temperature and its common measurement. We live by temperature, we have it reported constantly, we all have thermometers at home, we measure our childrens fevers with thermometers, we barbeque with thermometers.
Measuring temperature is easy right? You just stick the thermometer in whatever gas, liquid, or solid you want to measure the temperature of and voila’  there it is. People tend to think of thermometers as perfect devices. Some very expensive calibrated thermometers, are close to perfect, especially when taking measurements in a closed system, like a fermenatation vat at Sierra Nevada.
But in an open system in our atmosphere, there are many many more biases that can affect the measurement within a few inches or feet of the thermometer. Here’s just a few:
– Reflected sunlight from nearby building or objects
– Re-radiated infrared from nearby cement or asphalt surfaces or the ground itself (which is why airports make terrible places for temperature measurement)
– The structure that the thermometer is mounted to, can conduct heat to the thermometer
Now add to that:
– Accuracy of the thermometer itself
– Linearity of the thermometer over its measurement range
– Long term repeatability of the thermometer’s accuracy
– Long term repeatability of the thermometer’s linearity
And then we have urban effects such as:
– Localized vegetatation removal or addition over time
– Localized building changes over time
– Localized asphalt or concrete surfaces addition or removal
And finally within the global temperature records data set we find instances of:
– Changing the location of the weather station and/or its thermometer
– Changing the thermometer itself at some point – i.e. repair/replace
– Changing the thermometer type, from mercury, to electronic (thats been done at thousands of weather stations worldwide)
– Variations in temperature measurement devices from country to country, even though the World Meteorological Organization has specifications, they are not always followed.
– Changes in thermometer shelter, different types of paint over time, all which have different absorptive and reflective properties.
– Changes in the observer recording the temperature, some may round up, others round down numbers. BTW for about 75 years, all temperature records were manually recorded.
Ok with all these biases and possible errors that you have to account for to make long term temperature measurement reflect the true temperature of the location, can you be absolutely sure of the data integrity? Especially when you are looking for trends that may be 1 degree or less over 50-100 years? I can tell you that I’ve looked at these climatological data sets, and NONE of them come with a calibration record for the thermometer, or even a description of the make/model used at that location. There are notations in the records that say things like “station relocated to accomodate construction” or “thermometer replaced” which can give clues to the data integrity possibly changing but the climate researcher is left to make a judgement call on the viability of the data without anything to gauge the sensor or its local environment.
Or lets try a thought experiment Lon, you’ve been commissioned by the IPCC to make a new thermometer for use around the world at climate measurement stations. As an electrical engineer, could you design an air temperature thermometer that is:
– Linear to within 0.1% over a temperature range of -20F to 120F
– Accurate to within 0.1 degree F over that same range
– Repeatable in linearity and accuracy defined above for a period of 20 years. Or even 10 years.
– Identical withing the specs above, so that if one fails, it can be immediately swapped with another one from parts stock with no worry about introducing bias
Ok there’s your challenge. Could you do it?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7248471',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterCorona shows the stark attitude differences between the sciences of climate and virology. While one arrogantly claims to monopolize the truth, the other acknowledges the great uncertainties.

Image: CDC
By Dr. Sebastian Lüning, Die kalte Sonne
(Text translated, edited by P Gosselin)
The corona virus with all the effects is currently pushing all other issues completely to the sidelines. This also includes the climate topic.
Climate activists like Professor Volker Quaschning or Professor Stefan Rahmstorf know it. Currently they fear for their livelihoods as they try to resist this with all their might, sometimes with absurd tweets or, as in the case of Professor Stefan Rahmstorf, with an article in Spektrum der Wissenschaft: Denial of science in times of corona“.
In his article, Professor Rahmstorf (Oceanography and Paleoclimatology!) also fancies himself a corona expert, and he puts those who do not agree with him in climate research in the same bag with the scientists who are critical of the corona crisis and approach. This is done completely without any basis because these are two completely different issues.
Climate activists seeking attention
It is a contemptible attempt to desperately link corona and climate, no matter what. This is being done simply because corona is the top issue right now. And before his own issue gets completely washed away, Rahmstorf is stooping to such means to try to get some attention. This blog here admits that it lacks the expertise to properly judge the corona debate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Beholders of the truth?
But something completely different is crucial here. Anyone who looks at or listens to the regular statements or podcasts of experts such as Berlin virologist Professor Christian Drosten will see the pleasant difference to climate alarmists like Professors Quaschning or Rahmstorf. Unlike them, Professor Drosten does not consider himself to be the sole beholder of the truth. On March 20, 2020, he explained in his podcast:
“There is no research data on long-range curfews. Caution is also called for when dealing with numbers.” …”Summer can have at least a small effect on the virus.”
Uncertainty is acknowledged
This has been the case since the beginning of his podcast series. Professor Drosten has stated more than once that science does not yet know certain things or that certain findings have since become obsolete. Imagine if he applied the popular killer argument “the science is settled” to corona in the same way as Professors Rahmstorf or Quaschning do with climate.
Models have failed
Yet, climate models that fail to reflect reality still remain the basis for future scenarios. They are taken at face value, even though their calibration fail when past data are applied. And even worse, people are being told that there is a control over the climate that is now being lost.
Is this a lack of knowledge, a lack of respect for nature, or just their own blunt agenda?
Alarmist scientists don’t contribute to healthy science
The climate debate will also fail because of the fact that the aforementioned people are sitting in ideological trenches and hurling grenades in the direction of the other side. This is something you can do in a war, but unfortunately it does not lend to a social discourse.


		jQuery(document).ready(function(){
			jQuery('#dd_99d26721b518a0ab9826796b8202153e').on('change', function() {
			  jQuery('#amount_99d26721b518a0ab9826796b8202153e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

A recent investigation by the Financial Times says that the new Carbon Credit Industry may already be rife with fraud. Hmmm…now where have we heard that before?
Among the findings:
■ Widespread instances of people and organisations buying worthless credits that do not yield any reductions in carbon emissions.
■ Industrial companies profiting from doing very little – or from gaining carbon credits on the basis of efficiency gains from which they have already benefited substantially.
■ Brokers providing services of questionable or no value.
■ A shortage of verification, making it difficult for buyers to assess the true value of carbon credits.
■ Companies and individuals being charged over the odds for the private purchase of European Union carbon permits that have plummeted in value because they do not result in emissions cuts.
From the article:
Some companies are benefiting by asking “green” consumers to pay them for cleaning up their own pollution. For instance, DuPont, the chemicals company, invites consumers to pay $4 to eliminate a ton of carbon dioxide from its plant in Kentucky that produces a potent greenhouse gas called HFC-23. But the equipment required to reduce such gases is relatively cheap. DuPont refused to comment and declined to specify its earnings from the project, saying it was at too early a stage to discuss.
The burgeoning regulated market for carbon credits is expected to more than double in size to about $68.2bn by 2010, with the unregulated voluntary sector rising to $4bn in the same period. 
Seems like the “green” here is not about Gaia…but all about Benjamins.
There’s no mention of how much these companies pay gamers to have virtual trees planted in video games.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6dacc7b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA new study assesses a reduction in tree cover via urbanization or by clearing forests for cropland can warm up a locality by 1°C within 10 years. In contrast, transitions from croplands and urban centers to forests leads to cooling. Europe has been cooling recently (1992-2015) from land cover transitions to forests.

Image Source: Huang et al., 2020
Urbanization adds multiple degrees of warming over decades
A few years ago a compelling analysis (Levermore et al., 2018) found the urban heat island effect can reach intensities of 8°C warmer temperatures than nearby rural sites.
Further, reducing the green (trees and vegetated areas) in an urban center by as little as 11% can lead to a 0.21°C per decade (non-climatic) warming trend in the local thermometer record.

Image Source: Levermore et al., 2018
Global warming can be reversed via land cover changes
While forest losses can heat up local temperatures by as much as 1°C within 10 years (Alkama and Cescatti, 2016), a new study (Huang et al., 2020) has assessed the opposite can occur too.
From 1992 to 2015, there were about 70 million hectares (Mha) of land cover changes (LCCs) occurring across the European continent.
A substantial portion of these LCCs were “cropland-to-forest” transitions due to agricultural abandonment.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




When a region returns to forest and tree cover, cooling ensues.
And with a growing percentage of European forested areas returning, a “predominant regional biophysical cooling” with “an average temperature change of −0.12 ± 0.20 °C, with widespread cooling (up to −1.0 °C) in western and central Europe in summer and spring” has swept across Europe due to LCCs in recent decades.

Image Source: Huang et al., 2020
The substantial impact of land cover changes
The implications of this study are profound.
First, the human effect on CO2 concentration changes appears to have minimal effect on local and regional temperatures relative to the much larger impact from land use changes.
More importantly, if reducing global warming is indeed the goal of policy makers, then denuding forests so as to install hundreds of steel-and-concrete wind turbines would appear to achieve the opposite of what it is claimed to do (reduce warming).
So why on Earth are we doing this?

Image Source: The Telegraph
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThat’s right, last week a panel, made up of 4 pompous linguists and one journalist, chose “climate hysteria” as Germany’s taboo word (un-word) of 2019.

Image: PatriotRetort.com
Discriminatory, disguising or misleading
The Unwort des Jahres (un-word of the year) is a new or recently popularized term used in Germany which a panel deems “violates human rights or infringes upon Democratic principles”.
According to Wikipedia, “The term may be one that discriminates against societal groups or may be euphemistic, disguising or misleading. The term is usually, but not always, a German term. The term is chosen from suggestions sent in by the public.”
Over the years, like so many other institutions, the volunteer panel has leaned to the left and has been choosing words that tend to cast conservatives and the right political spectra in a negative light. The panel’s announcement of the un-word of the year gets broad media coverage.
Last week the panel selected “climate hysteria” as the un-word of the year.
Taboo because it “defames climate protection efforts”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Wikipedia, the panel – which has no scientific expert on it at all, chose “climate (change) hysteria” as the un-word of 2019 because it “defames climate protection efforts and the climate protection movement, and discredits important discussions about climate protection.”
Climate science dissent is no longer welcome, the panel wants to tell us.
According to Wikipedia:
The expression [climate hysteria] was used by many in politics, economics, and the media in 2019 – by the Frankfurter Allgemeine Zeitung as well as by entrepreneurs and especially by politicians of the Alternative for Germany party. It dismisses the increased commitment to climate protection as some kind of collective psychosis. Moreover, in light of scientific findings regarding climate change, this word is misleading and irresponsibly supports anti-scientific tendencies.”
Yet, thankfully, some media have grown critical of the panel of volunteer linguists and single journalist, and all the media attention it gets. For example, Bild newspaper wrote:
As if it were the decision of an important institution, the decision of a privately organised group is reported: Four linguists and a journalist who volunteer once a year to play linguistic police. According to the motto: Listen up, citizens, the language committee has decided, this word is taboo from now on!
Ironically, in 2011 the panel chose “alternativlos” (no alternative) as the un-word of 2010 in politics because they claimed it was “undemocratic”, as any discussion on a subject “would be deemed unnecessary or undesirable”.
Today the panel appears to have forgotten about that earlier choice.
In any case, skeptics and dissenters should instead ramp up the use of the term “climate hysteria” to describe the FFF and XR movements, and all the nutty doomsday scientists who like telling us there’s no alternative to decarbonization.
Share this...FacebookTwitter "
"

UPDATE: The national website www.junkscience.com has referenced this blog entry.
From the waaaayyyy over the top department:
The Weather Channel’s climatologist, Dr. Heidi Cullen who hosts the program “The Climate Code”, is advocating that broadcast meteorologists be denied certification (or re-certification) if they express skepticism about predictions of manmade global warming. She posted this revelation in the blog she runs on the Weather Channel website and you can read it here: http://climate.weather.com/blog/9_11396.html
She writes: “If a meteorologist has an AMS Seal of Approval, which is used to confer legitimacy to TV meteorologists, then meteorologists have a responsibility to truly educate themselves on the science of global warming.” “Meteorologists are among the few people trained in the sciences who are permitted regular access to our living rooms. And in that sense, they owe it to their audience to distinguish between solid, peer-reviewed science and junk political controversy.” “If a meteorologist can’t speak to the fundamental science of climate change, then maybe the AMS shouldn’t give them a Seal of Approval.”
Them’s scientific fightin’ words lady.
So, apparently any free speech, scientific debate, and public dialog that doesn’t agree with the peer reviewed popular scientific opinion is grounds for denying an AMS Broadcast certification?
This reminds me of Galileo and his fight with the Roman Catholic Church in 1632. Galileo wanted to publish a book Dialogue Concerning the Two Chief World Systems which totally revised the earth centric view of the universe favored by scientists, scholars, and clergy of the time and built on the work of the earlier astronomer Copernicus. Galilieo was tried and imprisoned for daring to speak out against the “consensus” of the time for what he saw as a scientific truth.
I think we would all do well to follow this maxim: “People who live in greenhouses shouldn’t throw stones”.

Open scientific debate is essential to the scientific process, to call for castigating and silencing TV weathercasters who see other evidence is not only against American free speech values, it’s unprofressional for a scientist. Freedom of speech is the concept of the inherent human right to voice one’s opinion publicly without fear of censorship or punishment.
I support Cullen’s freedom of speech to make the claim that Global Warming is entirely an affliction caused by humanity, but I don’t support her call for decertifying of proponents of alternate theory
Despite receiving over 1000 blog comments by the public, most of them harshly critical of Cullen’s call for suppressing the voices of manmade global warming skeptics Cullen has refused to retract her call for AMS decertification of broadcasters who may also be global warming skeptics but instead blamed the whole mess on “spin.‿ Here is her latest post on the controversy. No mention of the word “sorry” or mea culpa in that post.
The Weather Channel has yet to officially comment on the matter. They are most likely being very careful as they are now in the middle of a scientific and political firestorm.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8d3e6bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
German Coal Power Plants To Be Converted: To Burn Trees

Millions of trees to be shipped from around the world to Europe to be burned as “green coal”. Image cropped from “Planet of the Humans”
By Die kalte Sonne
(Translated/edited by P. Gosselin)
On May 2, 2020, we reported on the movie Burned. In the USA, the focus is on biomass.
However, they do not ferment fast-growing plants into gas as is the case in Europe, rather they cut down trees and burn them in power plants – often together with other things like car tires or soaked railway ties.
The issue is controversial because it is about pure ideology. Climate organisations such as 350.org, which in the USA is like Fridays For Future (FFF) in Europe, have given their blessing to this type of power generation.
The film Planet of the Humans by Michael Moore also denounces this.
Converting CO2 sinks instantly into atmospheric CO2
And so the USA is losing valuable carbon sinks and biotopes, destroying its environment and lying to itself about sustainability and the climate. A tree that takes 50 – 100 years to become big and stately, but then is burned up in a few minutes, can never have a favorable climate balance, no matter how you calculate it. Trees are the new coal, it seems.
But anyone who thinks that this is only done in the USA, where huge forests and thus carbon sinks are destroyed, is mistaken.
“Madness”: German coal plant to be converted to burn trees
The online daily Weserkurier reports on a coal-fired power station in Wilhelmshaven (North Germany) that is to be converted to burn wood. This made Germany’s most famous forester, Peter Wohlleben (book “The Secret Life of Trees“) flash with anger on Twitter.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Der Wahnsinn geht weiter: Obwohl hunderte Wissenschaftler vor der Holzverbrennung als Klimakiller warnen, setzen Politik und Wirtschaft in Deutschland auf Waldzerstörung und wollen Kohlekraftwerk umrüsten. https://t.co/gHJvNGi4YR @GrueneBundestag @SvenjaSchulze68 @spdbt
— Peter Wohlleben (@PeterWohlleben) June 13, 2020

 
Wohlleben’s tweet in English:
The madness continues: although hundreds of scientists are warning against burning wood as a climate killer, politics and industry in Germany are backing forest destruction and want to convert coal-fired power plants.”
What Wohlleben means by madness could be the statements of Social Democrat Party member of parliament Siemtje Möller. Her slogan on her own website: “Think about the climate too!”
“Green coal”
Siemtje Möller is already thinking ahead. After all, the Wilhelmshaven site could eventually also produce hydrogen with the green coal. The stimulus for the technology, worth billions of euros, which has just been ratified, should also come to Wilhelmshaven.
“I’d like a fair share here,” says the Siemtje Möller about the budget. In general, she sees the hydrogen initiative, the coal phase-out law and the structural transformation law as “a huge opportunity for the Northwest to enter the future”.
She calls trees “green coal” in all seriousness and then wants to use the energy from burnt trees to produce hydrogen. Does the federal hydrogen initiative mean something like that? Probably not. Destroying carbon sinks cannot possibly be a huge opportunity for the future.
Why does Ms. Möller take her own slogan so little seriously?

Share this...FacebookTwitter "
"
You know your presentation was successful when:
1) Nobody threw rotten fruit
2) People came up to me afterwards and said “I have photos I can get to you”
3) A high level official at NCDC requests a copy of my presentation “as soon as you can get it to me”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea43650d4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The picture below comes to me via my website www.surfacestations.org from volunteer site surveyor Bob Meyer. It is the USHCN climate station of record for Waterville, Washington.
In addition to the now commonly seen attempts at measuring the temperature of parking lots, this station sports another new feature: volcanic cinder rock under the station to complement the tidy sidewalk. Note the convenient drive through teller window nearby so that you can cash your paycheck while on the way to the Post Office to mail in your COOP observer form to the National Climatic Data Center.

There’s also a nearby building about 10 feet away, and of course, convenient close-by parking just a few feet from the MMTS temperature sensor. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.

The USHCN “high quality” set of climate monitoring stations keeps getting curiouser and curiouser.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea594507e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterScientists suggest relative sea level changes are well-correlated with natural variability and accelerated sea level rise is a “recurring feature” of what has been observed for over 300 years. Five of six studied regions along the North American Atlantic coast show declining sea level rates (mm/yr) in recent decades.
After retreating into the sea until about 1960, for the last five decades the Atlantic coast of North America has, on net, reversed course, expanding at a rate of about 5 centimeters per year (Armstrong and Lazarus, 2019).
This is likely the exact opposite of what would be expected given the reports of accelerated sea level rise for this region in recent decades.

Image Source: Armstrong and Lazarus, 2019
The lead author of a new study, Professor Roland Gehrels, has previously found much more rapid rates of sea level rise prior to 1950 than in recent decades in Southern Hemisphere locations, such as along the coasts of Tasmania and New Zealand (Gehrels et al., 2012).

Image Source: Gehrels et al., 2012
In a new study, Gehrels et al. (2020) also found rapid rates of sea level rise reaching up to 3 millimeters per year during the 1700s along the Atlantic coast of North America. He suggests “those rapid episodes of sea level rise on the north east coast of North America in the 18th Century have a natural cause”.
Interestingly, of the 6 locations chosen for the study, only 1 (Connecticut) indicates sea level rise rates have been steadily accelerating throughout the second half of the 20th century and in recent decades. The 5 others (Nova Scotia, Maine, New Jersey, North Carolina, and Viðarhólmi) all show the millimeters-per-year rates of sea level change have either not been rising or even rapidly falling.
This would not appear to be consistent with a driving anthropogenic influence in sea level rise trends since the 1950s, or since CO2 emissions have risen dramatically.

Image Source: Gehrels et al. (2020)
Share this...FacebookTwitter "
"

As many readers know, the www.surfacestations.org effort has been gaining a lot of attention, and also volunteers. I’m now at over 130 volunteers nationwide.
The results of the effort attracted national attention. I never went seeking it, but when Bill Stiegerwald of the Pittsburgh Tribune stumbled across it, he wrote a column about it. Little did I know his column was nationally syndicated. Last week I found myself being asked to give radio interviews. One interview, at KIRO in Seattle surprised me when I found myself being co-interviewed with Dr. Thomas Peterson of the National Climatic Data Center (NCDC) the keeper of weather records, including weather station records. The exchange was congenial and stuck to science. That was Thursday June 21st. I am certain NCDC is aware of the effort that is going on to document the stations. Part of the reason the effort exists is that NCDC has been pressed to do this by scientists that want to do exactly what I’m doing, studying the measurement environment, and NCDC has failed to do it. We’ll come back to that.
Part of the method I and volunteers are using to do this project relies on a database of weather station information provided by NCDC. In some cases stations are at airports, fire stations, sewage treatment plants, and ranger stations. In other few cases, they are at the residences of observers that have volunteered to record weather data and submit it to NCDC. Since the latitude and longitude provided in the database is fairly coarse, volunteers have to rely on a database entry called “Managing Parties” to find the name of the location, be it a fire station of the name of the volunteer observer.
You can access the database yourself, its a public record: http://mi3.ncdc.noaa.gov/mi3qry/login.cfm
Use the “Guest Login” button
I last used the NCDC database system this way to locate stations on Sunday evening, June 24th it went down Monday Morning June 25th and displayed a message:
“You are not authorized to view this information. Your IP address has been logged”
When it came back up Monday afternoon, the “managing parties” field identifying the location of the weather station was gone. I would note that I shared a radio interview with Dr. Thomas Peterson of NCDC last week, so I am certain NCDC is aware of the effort.
No notification was given, nor even a professional courtesy to advise of the change, nor any notice on the website. The records were simply removed from public view where they existed before. Given the timing, and because the this same data had been visible on the same system for years It seemed this was a response to the efforts to photograph and document the USHCN network.
Without this information, its is very difficult to locate the stations, and in some cases where the official climate station is in some one’s backyard, completely impossible. For example, fellow blogger and surfacestations.org contributor Russ Steele had a very difficult time locating the official station for Ft. Bragg, CA. The observer did consent to having photos posted by the way. Had Russ not been able to contact the observer, the station would likely never have been found as it’s surrounded by trees and garden.
One of my volunteers wrote a query to NCDC and got this back:
Your inquiry was forwarded to me by our webmaster. I’m glad you’ve found
MMS to be a useful tool in your research.
MMS is our primary source of station metadata for National Weather Service
Cooperative Observer and several other networks, and we are
actively working to provide increased detail for a larger number of stations.
It sounds as though you’ve used the system enough that once you’ve located
a station using the search, you’re clicking on the station name hyperlink
and opening a separate station details window. The managing party for a
station has always been visible by clicking on the “Other Parties” tab. In
the case of NWS Coop stations (the USHCN research network relies upon a
subset of stations in the NWS Coop program), this is usually the NWS office
that administers the site. This information was previously included at the
bottom of the Identity tab’s “form view,” but was removed from that view
early this week because in some cases it also revealed the name of the
Cooperative observer.
Cooperative observers are volunteers who donate their time in the interests
of the public good with a reasonable expectation that their personal
information will remain private. It is the NCDC’s policy to protect
observer details, based upon Freedom of Information Act (FOIA) Update, Vol.
X, No. 2, 1989, which exempts the application of FOIA in certain cases and
establishes privacy protection decisions in accordance with the Privacy Act
of 1974 (2004 edition). This exemption applies when the personal privacy
interest is greater than any qualifying public interest for disclosure.
If you have other questions regarding MMS, please feel free to contact me.
I am often away from my desk, so my response may not be immediate.
I was shocked to say the least. So were others in the scientific community.
Data which was once public for years, has now been removed, and the timing is very suspect.
The claim that it was done to protect the privacy of observers doesn’t stand up to certain tests:
1) COOP weather observers are gathering climate data which is published and publicly available. The program is publicly funded. Data and methods from a publicly funded program that is not classified for national security reasons should be available for public inspection. Clearly results from surefacestations.org so far show some problems with the climate measuring network.
2) That published data is used in a multitude of publicly funded research. Some of that research guides policy decisions. The effects of a public policy decision based on data gathered by a volunteer individuals can affect millions of people. The right of the individual to FOI privacy is trumped by the greater need of the general public’s right to know if the data produced by that observer is accurate.
3) The data has been publicly available for years, removing it now is clearly in response to the effort to examine a public program given the timing of it having been removed four days after an NCDC official became aware of my efforts.
4) The data that has been removed also includes locations of public entities such as fire stations, police stations, sewage treatment plants, park headquarters, state run agricultural experiment farms, and many more. These locations are public entities and have no expectation of privacy whatsoever.
I can understand wanting an individual volunteer’s privacy protected. But the method used so far has been to contact the observer ahead of time, tell them what the project is about, and ask for consent. If consent has not been given, no visit is made, and no photographs are taken. See the rules that each volunteer to surfacestations.org must follow
So you have to wonder this: Is NCDC asserting that the privacy interests of police and fire stations, park headquarters, waste water treatment plants, and a handful of individuals, outweighs the public interest in examining quality of data produced in NCDC records and subsequent NOAA reports and publicly funded research? 

Does this waste water treatment plant measureing temperatures for the climate record really need privacy protection?
I said earlier we’d get back to something.
Dr. Roger Pielke, a senior climate researcher, of the University of Colorado, posted on his blog, his outrage at this action, calling it a “cover up”. Those are strong words coming from a congenial scientist. He also posted something even more shocking:
Pictures of these weather stations already exist, but they are being held from public view. Apparently some time ago weather service offices were issued digital cameras and told to do this work. The pictures were submitted to NCDC, and an archiving process begun, then stopped again for “privacy concerns”.
This is my position:
Given what has been seen so far at weather stations that have been inspected by myself and volunteers, it is clear that parts of the USHCN climate monitoring network are out of compliance with published siting standards and in disrepair. Given that the output of this network drives in part NOAA’s climate assessment, the public should demand a full and open accounting of the condition and data accuracy. If volunteer observers using NOAA equipment at private residences do not wish to have their location and the data it produces scrutinized by quality control methods, they have that right. But the data [produced by these stations should be removed from the climatic dataset because it will be unverifiable.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea58423f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

From the France surrenders just to be safe department :
Some “experts” think we should put the U.N. in charge of our space defense against large meteors or asteroids that could wipe out Earth. Ok, let me ask you a question.
Can you name one thing the U.N. has been able to accomplish with complete success? …..Yeah, I thought so.
If the world needs to deflect an asteroid, or even practice doing it, failure is not an option. So rather than leave the fate of the world in the hands of this, ahem, “capable” diplomatic organization, who you gonna call? (Hint, they have headquarters in Florida and Texas). Please, leave space work to space agencies, and the hand wringing to diplomats.
SAN FRANCISCO (Feb. 18) – An asteroid may come uncomfortably close to Earth in 2036 and the United Nations should assume responsibility for a space mission to deflect it, a group of astronauts, engineers and scientists said on Saturday.
Astronomers are monitoring an asteroid named Apophis, which has a 1 in 45,000 chance of striking Earth on April 13, 2036.

Although the odds of an impact by this particular asteroid are low, a recent congressional mandate for NASA to upgrade its tracking of near-Earth asteroids is expected to uncover hundreds, if not thousands of threatening space rocks in the near future, former astronaut Rusty Schweickart said.
“It’s not just Apophis we’re looking at. Every country is at risk. We need a set of general principles to deal with this issue,” Schweickart, a member of the Apollo 9 crew that orbited the earth in March 1969, told an American Association for the Advancement of Science conference in San Francisco.
Schweickart plans to present an update next week to the U.N. Committee on Peaceful Uses of Outer Space on plans to develop a blueprint for a global response to an asteroid threat.
The Association of Space Explorers, a group of former astronauts and cosmonauts, intends to host a series of high-level workshops this year to flesh out the plan and will make a formal proposal to the U.N. in 2009, he said.
Schweickart wants to see the United Nations adopt procedures for assessing asteroid threats and deciding if and when to take action.
The favored approach to dealing with a potentially deadly space rock is to dispatch a spacecraft that would use gravity to alter the asteroid’s course so it no longer threatens Earth, said astronaut Ed Lu, a veteran of the International Space Station.
The so-called Gravity Tractor could maintain a position near the threatening asteroid, exerting a gentle tug that, over time, would deflect the asteroid.
An asteroid the size of Apophis, which is about 460 feet long, would take about 12 days of gravity-tugging, Lu added.
Mission costs are estimated at $300 million.
Launching an asteroid deflection mission early would reduce the amount of energy needed to alter its course and increase the chances of a successful outcome, Schweickart said.
NASA says the precise effect of a 460-foot object hitting the Earth would depend on what the asteroid was made of and the angle of impact.
Paul Slovic, president of Oregon-based Decision Research, which studies judgment, decision-making and risk analysis, said the asteroid could take out an entire city or region.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8507042',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAn observational analysis of photometric evidence suggests solar forcing of Earth’s atmosphere could vary by as much as ±4.5 W/m² since 1750, which is “far larger than the IPCC estimate of −0.30 to +0.10 W/m²” (Judge et al., 2020).
A 2017 study suggested the solar activity during the “modern maximum period from 1940 to 2015” is a “relatively rare event, with the previous similarly high levels of solar activity observed 4 and 8 millennia ago” (Yndestad and Solheim, 2017). Variations in solar activity since the 18th century were shown to have ranged between about 1357.5 W/m² and 1362 W/m² (~4.5 W/m²).
In contrast, the total radiative forcing due to the increase in the CO2 concentration since 1750 is suggested to be 1.82 W/m² (Feldman et al., 2015).

Image Source: Yndestad and Solheim, 2017
A new study (Judge et al., 2020) also affirms our highly uncertain estimations of solar forcing variations since 1750 may be “of the order of 3 W/m², far larger than the IPCC estimate of −0.30 to +0.10 W/m²” and also greater than the uncertain IPCC estimates of total anthropogenic forcing (+2.2 ± 1.1 W/m²) since 1750.
Large estimate ranges for solar forcing variability should reduce the certainty that Earth’s radiative forcing has been dominated by anthropogenic activity in recent centuries.

Image Source: Judge et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_d215e380a9bc673ec0197eb9c75076b2').on('change', function() {
			  jQuery('#amount_d215e380a9bc673ec0197eb9c75076b2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLest anyone has wondered why the climate movement has shifted its focus over to children, it is because too many adults just refused to buy into the climate-Armageddon hoax.
It’s a fact: Children are very easy to manipulate and deceive.
Children – and adults with stunted intellectual development – are much easier to convince than adults who have been around the block of life a few times. Children are naive, inexperienced, lack insight and highly impressionable. This makes them vulnerable and thus really easy targets for climate radicals.
88 victims of sadism
Nothing illustrates this better than a recent story appearing in the Daily Mail here, where it is reported how a German sadist, via Skype, was able to successfully convince 88 young women to give themselves potentially lethal 230-volt shocks!
If one sicko is able to convince people to practically electrocute themselves, then imagine how easy it is for the media/organized activists to convince kids a climate doomsday is coming. It’s all based on the same bloody. The approach is the same in both cases:
1) First there’s an offender who derives pleasure through power over the victims
2) The offender claims to have great authority
3) The offender requests that their victims submit and obey
4) The offender claims it’s for a good cause
5) The offender promises the victims great reward for submitting
6) disaster results
1. Pleasure from sense of power
Just as the German ‘socket sadist’ derived his pleasure from the sense of power over his victims and sexual gratification, the climate radicals derive their pleasure from the control they have over today’s children.
2. Claim of authority
While the German socket sadist claimed to be a researcher conducting an experiment that would advance science and thus the common public good, climate radicals falsely claim to possess the scientific truth, and that they have everything under control and can be trusted.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The victims believed he was a scientist and there was no danger to them to carry out the experiment, that’s why they agreed,” the Daily Mail quoted prosecutors. “But he appeared so serious,” one victim later said.
The socket sadist refused to be challenged. If his victims resisted cooperating, then they were made to feel guilty and inadequate. With climate radicals, they label dissenters as deniers and villains. The climate radicals also do not tolerate any questioning or dissent.
3, Request to sacrifice
The German socket sadist asked his young female victims to hurt themselves – all in the name of science. The climate radicals demand that their followers collectively subject themselves to hardship and accept going without the amenities we enjoy, while exempting themselves.
4. Do it for a good cause
The socket sadist promised his victims it was in service of science, a good cause they could feel good about. Likewise, the climate radicals falsely promise kids they will see a much brighter future-  but only if they submit and do as they’re told. It’ll save the planet if they do, they are told.
5. False promises of reward
While the “socket sadist” allegedly made false promises of money (up to €3,000) to his victims and assured them they were participating in the noble cause of advancing science, the highly organized and authoritarian climate radicals promise the kids that if they do as they are told, the planet will be rescued, will become a green paradise, and peace will reign.
And they won’t have to school on Fridays.
6. Will turn into a mess
Just as the socket sadist case turned into a disaster, so will the extreme climate movement of zero carbon emissions by 2050.
Ironically, defense lawyers for the socket sadist, Klaus W Spiegel and Matthias Bohn, are now claiming their client had diminished responsibility for his actions as he suffers from Asperger Syndrome and autism. Sound familiar?
Share this...FacebookTwitter "
"
I’m sitting in a presentation by William R. Cotton, of Colorado State University where he’s talking about the effect of Urban Heat Islands (UHI) on precipitation. He’s making a convincing pitch showing how the UHI factors into downwind delayed convection initiated by the city UHI along with a significant contribution of aerosols and ice nuclei that seed the precipitation. He’s been able to demonstrate that in St. Louis, downwind from the city (typically NE to SE based on prevailing winds) there are increased precipitation from thunderstorms by as much as 160% during the life cycle of the storm.
Yesterday, I saw a very similar study done by Indiana State Climatologist, Dev Nyogi, where he studied Indianapolis, IN and came to similar conclusions. The midwestern cities make good case studies because they are singular islands of urbanization (as opposed to sprawling cities like Los Angeles and Chicago) that essentially become point heat sources at the mesoscale level.
The summary is this: Urban and-use has the biggest control on locations and amounts of precipitation and that condensation nuclei added by the city also have a significant effect. Heat and particles contributed by the city can make bigger, more precipitating thunderstorms.
Of course studies by Parker tells us there is no significant UHI effect, so this presents yet another challenge to what is looking ore and more like a flawed study by Parker.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4780e8a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My friends at coffee this morning got a huge laugh out of Chico Peace and Justice Center member Sherri Quammen’s claim in a vitriol filled letter to the editor that I’m the “real WMD”.
For somebody who professes “peace and justice”, she sure seems to have a lot of anger to vent. She’s sent letters to all three newspapers, the ER, Chico Beat, and you’ll see the same letter come Thursday at the Chico News and Review I’m sure. Lately, the message of “peace on earth” seems to have lost the accessory clause of “goodwill towards men”. Though its hard to tell through her rant just what she dislikes about me most, it appears that my views and research into climate change must be the main factor.
I sent her a nice note last week, offering to meet and get aquainted over coffee or tea someday, (since we’ve never met) after the letter appeared in the Chico Beat, so far no response.
But that’s OK, being a public person, criticism comes with the territory. It’s an occupational hazard. I guess I should be honored that my threat level has been elevated. Poor Al Gore takes all sorts of flak daily.
Sooo….since I’ve been labeled a WMD, I think that I’ll have to look over my shoulder a lot to make sure I’m not being followed by police officers intent on giving me a ticket in case I go off in the Chico city limits. That’s a $500 fine you know.
To make it easier for people to spot me, I think I’ll get a T-shirt that says simply “BOOM”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6794214',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Stephanie Kelton’s _The Deficit Myth_ is quite the talk of the town. To quote Amazon’s webpage:



It’s an attractive vision, but it doesn’t work.



I am reminded of Einstein’s time at the Swiss Patent Office where he used to check applications to patent perpetual motion machines. They don’t work, but the fun is working out why. The same applies to proposals to bring about prosperity that depend on loosening the monetary spigots. MMT is a perfect example.



MMT is a macroeconomic school of thought in the post‐​Keynesian tradition. Its central tenets: fiscal deficits don’t matter; monetary policy should be subordinate to fiscal policy; and the monetary authorities should be willing to issue base money to finance government spending. MMT is associated with large‐​scale government spending, a focus on ending involuntary unemployment, and programs to alleviate poverty and fight climate change.



Kelton’s book builds on earlier work by Warren Mosler and Randall Wray but has its roots in Abba Lerner’s system of “functional finance,” which goes back to the 1960s. She builds on Lerner primarily by adding a federal job guarantee that would eliminate involuntary unemployment and provide an automatic economic stabilizer.



MMT makes _big_ promises. It would “build a more just economy that works for the many and not just the few” and put “people and planet first.” “MMT’s lens enables us to see that another kind of society is possible, one in which we can afford to invest in health care, education, and resilient infrastructure. In contrast to narratives of scarcity, MMT promotes a narrative of opportunity.”



But does MMT deliver? Let’s see what she says.



“The idea that taxes pay for what the government spends is pure fantasy,” writes Kelton. Really? Let’s go back to basics. The government must finance all its expenditures. In a world in which it does not issue debt and does not issue currency, and assuming away any gifts it might receive, all its expenditures must be financed by current taxation.



If the government can issue debt but not issue currency, then it can finance its expenditures by current taxation or by issuing debt. But to issue debt is to pass on the obligation to repay that debt to future taxpayers. If that debt is to be repaid, then it must be repaid out of future tax proceeds.



If the government can issue its own currency and monopolizes the issuance of currency, then it can also pay off its debt obligations as they come due by issuing additional base money (“printing money”). Does this mean that printing money allows the government to avoid the need to raise taxes? No, because printing money lowers its value against goods and services, and so operates as a tax on money holdings and other holdings of wealth that are fixed in nominal terms (such as level annuities). So, barring gifts, all government expenditures must be financed by taxation in one form or another.



Kelton explains:



This sounds great: involuntary unemployment eliminated and everyone willing to work gets a high federal minimum wage or more, to the extent that market wages are forced higher to compete. But hold on. If it is such a good idea, why not raise the minimum wage beyond the $15 an hour she suggests? Why not $30 an hour? Or $50? The problem is that there are a raft of jobs that are profitable to provide at existing wages but would disappear at higher wages.1 It is not just the existing unemployed who would end up on federal payrolls but these newly unemployed too, and many of their employers. Think of the restaurant sector. That sector and others in the same position could only survive by hiking their prices: dining out would become a lot dearer. Ordering in, too. The federal government, the employer of last resort, would find itself with the problem of what to do with all these people also turning up for guaranteed jobs. The feds would have crowded out much of the labor market and wiped out the lower paid sectors of the economy.



“Why does the financing have to come from Uncle Sam?” asks Kelton. “Simple. He can’t run out of money.” Imagine that the government pays debts coming due by handing over dollar bills that it has in a chest in the basement. If it runs out of bills, then it will default the next time a payment comes due. But then imagine if it can also print money. If it runs out of bills, it can avoid default when the next payment comes due by printing more. It does not follow, however, that the government can _always_ meet its payment obligations by printing more money.



Suppose the government prints money at an accelerating rate and we end up with an accelerating hyperinflation. The traditional tax‐​collection apparatus will break down because the tax revenue will be worth almost nothing by the time it comes in. Similarly, the government will effectively be unable to borrow in its own currency because the borrowed funds would also be worth next to nothing by the time they come in. As the hyperinflation accelerates further, the real value of the revenue from printing money also goes to zero. The government then faces the prospect of default despite being able to print any amount of its own money. To give an example, by the end of the Hungarian hyperinflation of 1946, the total value of all Hungarian notes in circulation was a thousandth of a U.S. cent (see Judt 2006: 87). The Hungarian government didn’t have a cent, let alone a dime. The Hungarian government would have been unable to make repayments denominated in other currencies or make inflation‐​linked payments in its own.



The mistake is to presume that what is correct at the margin (i.e., that the government can avoid default by issuing a few extra dollar bills) is also correct under any circumstances, that is, at any scale. There is also the related point that issuing a small amount of money will have a negligible impact on prices but issuing a lot of money will not.



It is a “myth,” writes Kelton, “that deficits will burden the next generation.” This claim is also wrong. Suppose Congress passes a Boomers Boomtime Act to provide for a humongous 75th birthday payout to each surviving member of the first Boomer cohort born in 1946. They will reach 75 in 2021. These payments are to be financed by a zero‐​coupon bond with a 40‐​year maturity. Since none of the beneficiaries will be around to pay taxes when the bond is due to be repaid, they get a free handout.



Who bears the burden of paying for it? When the Boomer bond comes due in 2061, the government faces the following choices: (a) pay it off by raising taxes, (b) pay it off by issuing money, (c) default, (d) pay it off by rolling over, that is, by issuing a new bond.



If (a), then the burden is borne by taxpayers in 2016.



If (b), the subsequent price level is higher, so the burden takes the form of a tax on money holdings and other instruments of fixed nominal value.



If (c), default, the burden is borne by those who suffer the adverse consequences of default.



If (d), then the rollover will mean that there will more debt after 2061 than there would otherwise have been and we have the same choices again when the new payments come due. If the decision is to roll over each time, then the debt/​GDP ratio will hit a level at which the government defaults sooner than otherwise.2 Thus, however the government responds when the Boomer bond matures, some group born after 1946 bears a burden from it.



More generally, any arrangement that involves one group issuing a debt that another group is expected to pay for _necessarily_ burdens the second group. The injustice is all the worse because the second group has no say in the matter.



There are also the government’s entitlement programs, Social Security, Medicaid, etc. This takes us to Kelton’s “myth” that “entitlements are propelling us toward a long‐​term fiscal crisis.… There is absolutely no good reason for Social Security benefits, for example, to ever face cuts. Our government will always be able to meet future obligations because it can never run out of money.”



These programs however are just another form of debt insofar as they create obligations on the government’s part to make future payments. Consequently, my earlier argument, that programs that create future obligations burden future generations, applies here also.



Entitlements are large, so the corresponding burdens would be large as well. To illustrate, there are perhaps $210 trillion in entitlements, and possibly more.3 If these entitlements are to be paid for by future taxation, then that is a lot of future taxation. If they are to be paid for by rolling over, then we would anticipate the ratio of debt (including entitlements) to GDP rising considerably, possibly to default levels.



Then there is the option of meeting those obligations by printing money. Given that the current stock of base money is just over $5 trillion, that response implies a possible 42‐​fold‐​plus expansion of the monetary base. That, in turn, implies a considerable increase in prices. Making entitlement payments is one thing, but the purchasing power of those payments is another.



We have here another instance of the margin vs. scale issue. The government can increase entitlements a little with next to no impact on their real value. But if the government creates huge entitlements to be financed by printing money, then those entitlements are going to be greatly devalued in purchasing power terms. And what the government must absolutely _not_ do is create huge entitlements that are inflation‐​linked and then rely on printing money to finance them. If it does that, it will produce both hyperinflation and default. The “myth,” that the national debt is a problem, is not a myth.



We can break down MMT into a set of policy _ends_ (what the government spends _on_ ), and a set of policy _means_ (how the government finances its spending), which in the case of MMT would involve large deficits and a lot of borrowing and money printing. The expenditure and the financing of that expenditure are two different issues. Bernie Sanders might use MMT to advance a more right‐​wing version of the Kelton agenda, but Donald Trump might seize upon the spending opportunities promised by MMT to advance his own, even more right‐​wing, agenda, for example, to promote policies that work for the few and not the many.



My point is that it is short‐​sighted and potentially counterproductive to promote a particular policy package such as MMT because _you_ can use it to finance projects that _you_ like, because someone else might use it to finance projects that you do _not_ like. To the extent that MMTers persuade people that MMT‐​based government finance is a good idea, they can hardly restrict that message to people who share their own political views. If you think of MMT as a government‐​financing package, then that financing package can be used to finance any government spending program, whatever its political hue.



A deeper issue is that any policy that gives policymakers the appearance of being able to spend a lot without having to bear the unpopularity of the high taxes needed to finance that spending is a dangerous one and invites abuse. Any such policy entails a major shift in power away from the legislative branch to the executive branch, because it gives the latter additional means of finance that bypass constitutional constraints against government overspending. To elaborate, the Constitution says that fiscal policy, the power to tax and spend, is constrained by the need to obtain congressional approval. If there were no Fed, or if the Fed were genuinely independent, then Congress could deny appropriations for spending projects of which it does not approve. But if the executive branch has the power to print money, then it has a potential means to circumvent Congress. If Trump wants his wall and Congress denies the appropriations, he can then order the Treasury secretary to print the necessary money instead. From this perspective, MMT is something of a constitutional abomination.4



It is a fundamental principle of constitutional political economy that economic policymakers operate under rules that constrain the decisions they make, and that these rules should be designed in ways that prevent undesirable behavior on their part. Under this way of thinking, the rules operate as bulwarks that constrain policy makers in order to protect everyone else from the misuse of the powers entrusted to those policymakers.



For proponents of MMT and for many other advocates of big government, however, those rules serve no real purpose and merely constrain policymakers from achieving the lofty ends that they seek to pursue. Yet they fail to appreciate that lofty ends do not justify giving those in power unrestrained discretionary powers. As Adam Smith observed:



Should MMT be adopted, then the Fed would become subservient to the Department of the Treasury, but in a more nakedly obvious way than it was during the years before the 1951 Treasury‐​Fed Accord, and without operating under the constraints of the Bretton Woods watered‐​down gold standard. Personally, I would suggest that, if MMT were adopted, then the government should make the new monetary policy arrangements transparently obvious. The Fed’s independence, such as it was, would be history and there would be no point pretending otherwise. The government should nationalize the Fed and make it a division of the Treasury, whose responsibilities would then be to issue currency and manage the national debt. Nationalizing the Fed would highlight the underlying chartalism of MMT (i.e., the idea that money is a creature of the state),5 and would also simplify analysis going forward because it would cut out the need to consider Fed/​Treasury interactions that only mask the underlying reality. We could then talk openly about the _government printing money_. If the United States is going to embark on a monetary policy worthy of a banana republic, then it should look the part.



Suppose then that the government goes full throttle MMT à la Kelton. This spending must be financed, however, and the government would have to do so by some combination of levying taxes, borrowing, and printing money. In essence, she proposes high government spending and a big deficit financed by borrowing (“debt finance”) and printing money (“monetary finance”).



If such a policy were launched at a time when there is considerable unemployment, then one would suppose that unemployment would fall. But there must eventually come a time when the economy returns to more or less “full” employment, whether because of those policies or despite them being a separate question. What happens then? To examine this question, I built a model along “unpleasant monetarist arithmetic” (see Sargent and Wallace 1981) lines and got some interesting results.



Let’s consider the following three possible MMT policies: (1) the government pursues pure debt finance; (2) the government pursues pure monetary finance; and (3) the government pursues debt finance for as long as possible, up to the point where it is about to default, and then switches to monetary finance.



Let _d_ be the long‐​term growth rate of the deficit and _g_ the long‐​term rate of economic growth. It is reasonable to suppose that MMTers would want _d_ considerably in excess of _g_ , so let us assume that this is so. Under a policy of pure debt finance, the debt/​GDP ratio would grow relentlessly and must at some point reach a level at which the government defaults. Hence, debt finance makes government default inevitable if pursued for long enough—that is, pure debt finance is unsustainable.



Under a policy of pure monetary finance, _d_ becomes the key driver of the inflation rate. If _d_ is steady in the long‐​term, then the inflation rate will converge to a long‐​term steady state. But if _d_ itself grows, so the deficit grows at an _accelerating_ rate, then long‐​term inflation will also accelerate.



Under the third policy, the debt/​GDP ratio rackets up to the brink of default, then the government switches to monetary finance with similar long‐​term outcomes as pure monetary finance.



It is important to emphasize that under monetary finance there is no way in which the government can simultaneously pursue an inflation target. Instead, the inflation rate becomes a residual outcome from the government’s fiscal policy and the government loses all control over the inflation rate. To make matters worse, the inflation rate also becomes the macroeconomy’s main shock absorber, so any shocks that produce unexpected increases in the deficit will lead to unexpected increases in inflation, which makes inflation highly uncertain too. A policy of monetary finance is therefore dangerous, because it can easily lead to runaway inflation or even hyperinflation. Externally, these effects on prices and inflation will be reflected in a falling, volatile and uncertain exchange rate.



In sum, we have a variety of possible long‐​term consequences, ranging from merely bad (highish and uncertain inflation, loss of control over inflation, a volatile exchange rate) to positively catastrophic (huge levels of national debt, high future taxation, national default and all that might entail, runaway inflation, hyperinflation). I could go into a long discussion of why any other fiscal‐​monetary policy mix would produce better long‐​term outcomes. On the fiscal side: a balanced budget or lower deficit financing. On the monetary side: a monetarist rule, a nominal GDP target, a Taylor Rule, a gold standard, whatever.



Suffice to say that the poor performance of MMT is not a coincidence. On the fiscal side, it encourages a much more rapid run‐​up of the debt/​GDP ratio than any alternative, which has got to be the worst possible fiscal policy in the long term. On the monetary side, it throws away any attempt at controlling inflation or maintaining monetary stability by making monetary policy subservient to runaway government spending. MMT performs so badly _precisely because_ it represents the extremes of fiscal and monetary excess.



Indeed, MMT does not even work on its own terms. Kelton herself indicates that an MMT policy package is constrained by the requirement that inflation should not rise. Unfortunately, she hasn’t thought it through.



Current U.S. inflation is 2.3 percent. Is she suggesting that the government should pursue MMT subject to the constraint that the inflation rate should not rise above 2.3 percent? If so, she should advertise the fact to help dispel the concerns of those who might have gotten the impression that MMT is some sort of funny money scheme. If she did so, much of the knee‐​jerk opposition from sound‐​money people would dissipate. The problem, however, is that a commitment to maintaining inflation at no more than its current rate would severely constrain the ability of MMT to deliver on its promises.



A looser interpretation of her “inflation shouldn’t rise” constraint would apply to inflation in the long run. But then consider my unpleasant monetarist arithmetic results. Since pure debt finance is fiscally unsustainable, any MMT package must involve some element of monetary finance. But, _ex hypothesi_ , the monetary finance must be constrained by the need to avoid rising inflation in the long run. A necessary condition to prevent rising inflation is that the rate of growth of the deficit should not itself increase. This constraint is not as severe as requiring that current inflation should never rise, but it is a severe constraint nonetheless. The problem is that it is not at all clear how much of what she promises can be delivered while satisfying this constraint. That she does not address this issue is the central failing of her book.



In effect, she offers us the prospect of a bunch of goodies but doesn’t explain why those goodies fall within the economy’s production possibility frontier, that is, are actually attainable—an intriguing oversight for an economist.6



She offers a revealing anecdote when she recalls a discussion between James Tobin and President Kennedy:



“Everything else is just talk” captures it perfectly. Indeed, it undermines the entire book.



We shouldn’t forget what subsequently happened. Government spending on the Vietnam War and the Great Society overdid it, and the United States ended up with rising inflation and the monetary troubles of the 70s and early 80s. MMT’s fiscal proposals are akin to the Great Society and its monetary proposals are akin to relying on the pre‐​NAIRU, or fixed, Philips Curve that was discredited by Milton Friedman. From this perspective, MMT has a distinct sixties feel to it.



In both cases, the root problem is the same: the absence of a coherent theory of inflation. The Keynesians of the sixties didn’t have one and neither does Kelton. Now, as then, the solution to that problem is the same: their macro model needs some version of the quantity theory of money to connect the money supply to the price level, and thence to the inflation rate. If the central bank or the government pursue policies that lead to too much monetary growth, then the result will be a higher than desired inflation rate. The solution to that problem is also the same as it was then: to rein in the rate of monetary growth. In short, MMT is not particularly modern and the monetary theory has too much money and not enough theory.



Kelton might object that these negative outcomes would not occur under MMT because counter measures would be taken once (or even before) inflation started to rise. So, what counter measures would she pull from her MMT toolbox? The answer is a rise in taxes.7



One can imagine the howls that would follow a proposal to raise taxes “merely” because inflation had gone over some “arbitrary” threshold. Why abandon The Project? Why stop policies that were on the verge of making the world a better place just as the going gets tough? MMTers attempting to stick to the “if inflation rises” script on which The Project was predicated would be cast into the role of fiscal conservatives. Don’t they know that deficits don’t matter, etc.? They then reap the downside of overpromising.



If and when taxes were increased, it is doubtful that doing so would get inflation back down again. We know from monetarism that the inflation rate will only come down once the underlying monetary growth rate has slowed. But since the MMTers lack a decent model of inflation the likely policy responses would be some muddle akin to what we experienced in the late 1960s and much of the 70s, and with similar results: rising inflation followed by stagflation, a new Keynesians vs. monetarists controversy, and inflation only being brought under control again when policymakers relearn the lessons learned then—namely, the importance of the quantity theory of money.



The poor long‐​term performance of MMT under my simulations is a perfect illustration of why policymakers need to be constrained by rules. To illustrate the benefits of such rules, consider the following. Recall that we can think of MMT as a set of policies that break down into two subsets:



(1)MMT = {MMT spending program; MMT financing program}.



The former is about what goes out of the government’s coffers and the latter is about what goes in.



It seems to me that for most people inclined toward MMT, the big attraction is the MMT spending program. For the sake of argument, let’s hypothetically agree with that spending program and then ask if we can replace the MMT financing program with something better.



The MMT financing program consists of a combination of high deficits, tax, borrowing, and monetary finance. This financing program is a key reason why MMT performs so badly, so let’s replace it with an alternative financing program that is a combination of, let’s say, more restrained deficits, tax and borrowing, and no monetary finance. The monetary side of this program would be taken care of by some monetary policy or rule that focuses on a stable inflation rate or something similar. We then come to:



(2)Alternative to MMT = {MMT spending program; alternative financing program}.



My models indicate that this “Alternative to MMT” would produce considerably better outcomes, including a less rapid run‐​up of national debt and lower inflation.



Why then would you not prefer the “alternative” to MMT? The “alternative” still delivers the spending goodies you want, but in a less damaging way in the long‐​term. But the “alternative” is old‐​fashioned tax and spend!



My point is that even if you support MMT because of its spending program, there is no good reason to support MMT in preference to some tax and spend policy mix with the same spending program. Whatever your preferred government spending program, MMT is a poor way to finance it.



So, if you are a hard‐​left socialist who supports the Kelton government spending platform, you should support tax and spend, not MMT. And if you do not support her spending platform, say because you are not politically hard left or because you support sound money, then you would also not support MMT. Whatever your politics, MMT is not for you; MMT is just bad economics.



In the end, MMT comes down to this: the government spends a lot, issues a lot of debt, and prints a lot of money. It is not as if it hasn’t been tried before.



Haskins, R. (2015) “The Federal Debt is Worse Than You Think.” Brookings Institution blog (April 8).



Judt, T. (2006) _Postwar: A History of Europe Since 1945_. New York: Penguin.



Sargent, T. J., and N. Wallace (1981) “Some Unpleasant Monetarist Arithmetic.” _Federal Reserve Bank of Minneapolis Quarterly Review_ (Fall): 1–17.



Smith, A. (1994 [1776]) _An Inquiry into the Nature and Causes of the Wealth of Nations_. New York: Modern Library.



Kevin Dowd is an adjunct scholar at the Cato Institute, and a professor of finance and economics at Durham University in England.



ShowHide

Endnotes



1 “There’s no reason every job—all the way down to retail clerk or fast food worker or janitor in a luxury Chicago hotel—can’t be a good job, with dignified pay, hours, security, and benefits,” she says. The question however is how many of these jobs would still exist.



2 I implicitly assume, as seems reasonable in this (MMT) context, that the rate of growth of the national debt, including entitlement commitments (see below), exceeds the economic growth rate. In that case, the ratio of debt (including entitlements) to GDP will keep growing, and default is then inevitable unless the government resorts to (a) taxation or (b) printing money.



3 This figure comes from Laurence Kotlikoff’s testimony to Congress in 2014 (see Haskins 2015).



4 Few presidents would have the self‐​restraint to refrain from taking advantage of such powers, but the point is that if the Constitution were properly followed, we wouldn’t have to rely on his or her self‐​restraint in the first place.



5 One might even go as far as to say that MMT is the apotheosis of chartalism and I do not mean that as a compliment. Chartalism maintains that the state is entitled to monopoly privileges regarding the issue of currency. In response: a government monopoly is always a bad idea, period.



6 As she says, “The real challenge lies in managing your available resources—labor, equipment, technology, natural resources, and so on—so that inflation does not accelerate.” This passage describes the problem nicely but does not give the solution to it.



7 Wrong again. The only way to reduce inflation is to rein in the excessive monetary growth that is the proximate cause of rising inflation. So, Kelton’s statement that “MMT … offers a more sophisticated array of techniques for managing inflationary pressures than what we have today,” does not instill much confidence.
"
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. In this special issue, we focus on the climate implications of a carbon tax._   






We calculate, you decide.   
  
Once you make your selections, the calculator will return the amount of global temperature rise that will be averted as a result of your choices by the year 2050 and also by the end of the century.   
  
Try it using this example. Choose a 100% reduction of carbon dioxide emissions from the United States and the IPCC’s sensitivity value of 3.0°C. Hit “Submit.” The amount of temperature savings that results is 0.052°C by the year 2050 and 0.137°C by the year 2100. (Why we are using three significant digits is in the fine print at the end of this article.)   
  
[block module=""carbontaxform"" delta=""carbontaxform""][/block]   
  
Sorry, Major Kong (h/t to “Dr. Strangelove”), those _are_ the figures. That’s the right answer. Assuming the IPCC’s value for climate sensitivity (i.e. disregarding the recent scientific literature) and completely stopping all carbon dioxide emissions in the U.S. between now and the year 2050 and keeping them at zero, will only reduce the amount of global warming by just over a tenth of a degree (out of a total projected rise of 2.619°C between 2010 and 2100).   
  
If you think that a rise of 2.482°C is vastly preferable to a rise of 2.619°C then all you have to do is set the carbon tax large enough to drive U.S. emissions to zero by mid-century—oh yeah, and sell that tax to the American people.   
  
To explore other alternatives, use our handy-dandy calculator.   
  
Have fun!   




*********



 **The fine print:**   




The results from our calculator are produced from climate change calculations performed using the MAGICC climate model simulator (MAGICC: Model for the Assessment of Greenhouse-gas Induced Climate Change). MAGICC was developed by scientists at the National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency.   
  
We are not creative enough to have made that acronym up. MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models to efficiently emulate the output of complex climate models. MAGICC produces projections of the global average temperature and sea level change under user configurable emissions scenarios and model parameters. MAGICC is run using its default model parameter settings except for climate sensitivity, which you can choose from between 1.5°C and 4.5°C.   
  
The baseline emissions scenario against which all climate dioxide reductions were measured is scenario A1B from the IPCC’s Special Report on Emissions Scenarios (SRES). Scenario A1B is a middle-of-the-road emissions pathway which assumes rapid carbon dioxide emissions growth during the first half of the 21st century and a slow CO2 emissions decline thereafter. Emissions are prescribed by country groups. Our “Industrialized Countries” group is the OECD90 countries (which includes North America, Western Europe, and Australia, New Zealand and Japan.) In order to obtain the baseline emissions from the United States to which the emissions reduction schedule could be applied, the U.S. emissions were backed out from the OECD90 country grouping. To do so, the current percentage of the total group emissions that are being contributed by the United States was determined—which turned out to be right around 50%. We assume that this percentage will be constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100. In this way, the future emissions pathway of the U.S. was developed from the group pathway defined by the IPCC for the A1B scenario. From these baselines (either the U.S. baseline or the OECD90 baseline), carbon dioxide emissions reductions were applied linearly from 2005 to 2050 to obtain the user-specified total reduction. The new (reduced) emissions were recombined with the other (unadjusted) IPCC country groupings to produce the global emissions total. It is the total global emissions that are entered into MAGICC to yield global temperature projections. The results using the reduced emissions pathway were then compared to the results using the original A1B pathways as prescribed by the IPCC, with the baseline against which temperature changes were calculated set to the year 2010.   
  
We assume that a carbon tax would only be applied to reduce carbon dioxide emissions. In practice however, the only way to reduce carbon dioxide emissions is to reduce the burning of fossil fuels. Reducing the burning of fossil fuels will have co-impacts such as reducing the emissions of carbon monoxide (CO), volatile organic compounds (VOCs), nitrogen oxides (NOx), and sulfur oxides (SOx). The first three chemical compounds generally enhance warming while the latter generally retards it. Sensitivity tests using MAGICC indicate that for the OECD90 countries under the A1B pathway, the effect of collective changes in these co-emissions is largely compensative.   
  
Additional fine print on precision: The temperature savings are presented to three significant digits in order to tell the results apart. In the real world, the impacts from the emissions reduction pathways are not nearly so precise and, in fact, the temperature savings from most of the different carbon dioxide emissions reduction pathways are scientifically impossible to tell apart from each other, and in many cases, are impossible to tell apart from the original A1B scenario, i.e., they are same thing as doing nothing.


"
"
Share this...FacebookTwitterAt COP 25 in Madrid, Princeton physicist and former President Trump advisor Prof. em. William Happer spoke on the “false pretenses of a climate emergency” and called the climate protection movement a “bizarre environmental cult”, “absurd” and “madness”.

Princeton University, Professor Emeritus, Dr. Will Happer Discusses the False Pretenses of a ""#ClimateEmergency""https://t.co/Xl1kn0wWcG#ClimateBrawl
— CO2 Coalition (@CO2Coalition) December 6, 2019

The distinguished professor said, “It’s too bad we are here on false pretenses, wasting our time talking about a non-existent climate emergency.”
“Bizarre environmental cult”
Some 25,000 delegates have flown into Madrid to express their panic over a perceived climate emergency and to pressure governments to take radical actions to profoundly alter human behavior.
Happer added: “I hope sooner or later enough people will recognize the phoniness of this bizarre environmental cult and bring it to an end.”
In his talk, Happer warned leading politicians against viewing combatting CO2 as a religion, and cautioned it could end up badly when millions of people become obsessed with a single delusion and become stark-raving mad at climate. He said there’s been “so much brainwashing that it’s going to be difficult to bring people back to reality.”
The distinguished Princeton professor said the focus needs to be on pollution, and not CO2, and that solar energy and wind energy blight the environment and don’t work very well.
Only very little impact on climate


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the physics of CO2 trapping heat, Happer presents a CO2 chart and suggests that doubling the CO2 concentration in the atmosphere will have very little impact on climate and that he “can guarantee that no one who knows anything about science can dispute this curve. That’s the truth.”
“Absolute madness”
Taking action based on the curves that show CO2 has little effect, Happer says this is “absolute madness.”
The Princeton physicist also believes the climate models greatly exaggerate the warming and that the trace gas is in fact beneficial to the planet. Today atmospheric concentrations are extremely low compared to previous geological times:

The extra CO2 recently added into the atmosphere has in fact led to a greening of the planet, Happer shows.
“Phony consensus”
Next Happer called the often claimed 97% consensus among scientists “phony” and that science is determined by observation, “not votes”. “Scientific consensus is often wrong,” Happer showed.
Happer summarized that CO2’s impact has been exaggerated “by a factor of 2 to 4”, and that overall a little extra CO2 in the atmosphere is beneficial to vegetation and agriculture.
The whole CO2/climate worry “is absolutely absurd. It’s a cult,” Happer concludes.
Follow CO2 Coalition at Twitter.
Share this...FacebookTwitter "
"
Warren Meyer, one of the first surfacestations.org volunteers, delivered Tucson for us Saturday. It was discovered during an analysis of climate stations around the USA on the Climate Audit blog that Tucson had the greatest positive temperature trend for any USHCN station after the TOBS adjustment was applied. The TOBS adjustment corrects for differences in local times of observation of temperature by the observer. The picture says it all:

Yes folks, this is an official climate station of record, the temperatures it measures go into our National Climatic Database and are used in research such as the graph produced by NASA Goddard Institute for Spaceflight Studies here:

There’s a British word that has been bandied about to describe the reaction to pictures like this one: “gobsmacked”. The word applies even more so since this station is operated by science faculty members at the University of Arizona.
They are so proud of this station they even had a sign made for it to hang on the chain link fence enclosure:

The complete photo essay is available at the Tucson album at www.surfacestations.org The satellite and aerial photo images there are telling of the environment being measured.

Besides the obvious questions like “why is it in the middle of a parking lot?” and “why would scientists who should know better allow such a bizarre siting for a USHCN climate station of record?” Then there is this burning question: “Why did they go to the trouble of installing a precision aspirated temperature sensor and then not even bother to place it at the standard observing height?”. 

It appears that the Stevenson Screen serves no other purpose except as an equipment holder, as Warren Meyer reports the Stevenson Screen to be empty. Originally the inside standard mounting board for the mercury max/min thermometers were mounted about 1.5 foot higher than the air inlet of the precision aspirated temperature sensor. So the lower mounting height for the precision sensor adds a positive bias.
Is there no diligence left in basic measurement? Is this what they teach in college science departments these days?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4ef3691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

9/29/07 UPDATE: We are still waiting on Mr. Steve Bloom to answer this question: “Why is positive bias imparted in USHCN adjustments?”
He incorrectly asserts that he has been “banned” from this blog. Not true. Once he answers this question, that answer along with whatever else he has to say after that will be posted here. Otherwise we’ll continue to wait.
What say you, Mr. Bloom?
——————————————-
Given what NASA GISS has recently done with posting a change to the data methodology on the heels of an error which was embarrasing to them, (see Raising Walhalla)  I think this review of a relevant paper might bear some examination:
An Introduced Warming Bias in the USHCN Temperature Database Reference
Balling Jr., R.C. and Idso, C.D. 2002. Analysis of adjustments to the United States Historical Climatology Network (USHCN) temperature database. Geophysical Research Letters 10.1029/2002GL014825.
Abstract http://www.agu.org/pubs/crossref/2002/2002GL014825.shtml and the full paper Download file
What was done:
The authors examined and compared trends among six different temperature databases for the coterminous United States over the period 1930-2000 and/or 1979-2000.
What was learned:
For the period 1930-2000, the RAW or unadjusted USHCN time series revealed a linear cooling of 0.05°C per decade that is statistically significant at the 0.05 level of confidence. The FILNET USHCN time series, on the other hand – which contains adjustments to the RAW dataset designed to deal with biases believed to be introduced by variations in time of observation, the changeover to the new Maximum/Minimum Temperature System (MMTS), station history (including other types of instrument adjustments) and an interpolation scheme for estimating missing data from nearby highly-correlated station records – exhibited an insignificant warming of 0.01°C per decade.
Most interestingly, the difference between the two trends (FILNET-RAW) shows “a nearly monotonic, and highly statistically significant, increase of over 0.05°C per decade.” With respect to the 1979-2000 period, the authors say that “even at this relatively short time scale, the difference between the RAW and FILNET trends is highly significant (0.0001 level of confidence).” Over both time periods, they also find that “the trends in the unadjusted temperature records [RAW] are not different from the trends of the independent satellite-based lower-tropospheric temperature record or from the trend of the balloon-based near-surface measurements.”
What it means:
In the words of the authors, the adjustments that are being made to the raw USHCN temperature data “are producing a statistically significant, but spurious, warming trend in the USHCN temperature database.” In fact, they note that “the adjustments to the RAW record result in a significant warming signal in the record that approximates the widely-publicized 0.50°C increase in global temperatures over the past century.” It would thus appear that in this particular case of “data-doctoring,” the cure is worse than the disease. In fact, it would appear that the cure IS the disease.
From the paper: Our analyses of this difference are in complete agreement with Hansen et al. [2001]
and reveal that virtually all of this difference can be traced to the adjustment for the time of observation bias. Hansen et al. [2001] and Karl et al. [1986]
The reviewer notes: “Our prescription for wellness? Withhold the host of medications being given and the patient’s fever will subside.”
Originally from CO2Science


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3ed9891',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGlobal warming should mean that the period of May 11-15 – known as the Ice Saints in Europe – when late spring frosts often occur, would become less frosty over the years. But the opposite has been the case since 1995. 
===============================================
Why have the Ice Saints gotten colder over the past 25 years?
By Die kalte Sonne
Authored by Josef Kowatsch
(Translated and edited by P. Gosselin)
In Germany the five days from  the 11th to 15th May are the so-called Eisheilige (Ice Saints). Farmers used to understand “ice” simply as late spring frost, so these are days with frost.
Our question is how have the Ice Saints behaved at various locations across Germany the last 25 years?
POTSDAM
Let’s start with Potsdam, the capital of Brandenburg. We take the last 25 years as the period under consideration:

Figure 1: The five Ice Saints days in the state capital Potsdam. They are getting much colder. 2020 was the low point of the last 25 May months. On three days there were night frosts.
BAD KREUZNACH
At Bad Kreuznach in the Upper Rhine Valley we show a southern Germany station from from Palatinate, a warm sunny region. The DWD Germany National Weather Service weather station is located north, outside the town.

Figure 2: Bad Kreuznach in the Upper Rhine, the trend line of the present day is even slightly lower for the Ice Saints than in Potsdam.
DRESDEN


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The DWD weather station is located in the Klotzsche suburb, at the airport north of the Saxon state capital.

Figure 3: DWD station Dresden Klotzsche. Ice Saints in the present. The trendline is a little bit lower than Potsdam and the Ice Saints 2020 were among the coldest since 1997.
GOLDBACH
Goldbach near Bischofswerda in eastern Saxony, a small suburb with about 500 inhabitants.

Figure 4: The Ice Saints 2019 were clearly the coldest, source of data: Station manager Dietmar Pscheidt.
SCHNEIFELFORSTHAUS
This DWD weather station is located in the Eifel region, near the Belgian border.

Figure 5: Even in the far west of Germany, the Ice Saints outside the cities have become significantly colder. The average of the five calendar Ice Saints days in 2020 was 4.77°C – the second lowest.
NUREMBERG
The DWD Station Netzstall is located near Nuremberg, well outside the city. The missing value of the year 2000 was interpolated using the neighboring stations at Nuremberg and Nuremberg-Roth.

Figure 6: The village near Nuremberg shows a falling trend line of the five Ice Saints days. 2020 was slightly colder than 2019 and this year was clearly the coldest in the last 25 years.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn a new paper, atmospheric physicist Dr. Richard Lindzen summarizes the “implausible” claims today’s proponents of dangerous anthropogenic global warming espouse.
Dr. Richard Lindzen retired several years ago, and yet his immense contribution to the atmospheric sciences lives on. His research is still cited about 600 times per year.
Lindzen recently published another scientific paper (Lindzen, 2020) in The European Physical Journal criticizing the current alarmism in climate science.  Here are a few of the highlights.
1. Doubling the atmospheric CO2 concentration from 280 ppm to 560 ppm results in just a 1-2% perturbation to the Earth’s 240 W/m² energy budget. This doubled-CO2 effect has less than 1/5th of the impact that the net cloud effect has. And yet we are asked to accept the “implausible” claim that change in one variable, CO2, is predominatly responsible for altering global temperatures.
2. A causal role for CO2 “cannot be claimed” for the glacial-to-interglacial warming events because CO2 variations follow rather than lead the temperature changes in paleoclimate records and the 100 ppm total increase over thousands of years produce “about 1 W/m²” of total radiative impact.
3. Climate science didn’t used to be alarmist prior to the late 1980s. Scientists were instead sufficiently skeptical about claims of climatically-induced planetary doom. That changed during the years 1988-1994, when climate research centered on CO2 and global warming received a 15-fold increase in funding in the US alone. Suddenly there was a great financial incentive to propel alarming global warming scenarios.
4. Concepts like “polar amplification” are “imaginary”.
“The change in equator-to-pole temperature difference was attributed to some imaginary ‘polar amplification,’ whereby the equator-pole temperature automatically followed the mean temperature. Although the analogy is hardly exact, this is not so different from assuming that flow in a pipe depends on the mean pressure rather than the pressure gradient.”

Image Source: Lindzen, 2020
Share this...FacebookTwitter "
"

Many in the energy and environmental industries thought Donald Trump’s victory in November meant certain death for the Clean Power Plan (CPP), a piece of low‐​hanging fruit in Trump’s promise to revitalize coal country. This regulation, which many argue is one of the most expensive in American history, was key to Obama’s climate legacy and, indeed, the President’s Executive Order issued this week does kill the CPP. Until, that is, the environmental activists file for a stay, which could happen any day now.



As with Trump’s promises for the revitalization of coal country, all of this will be more complicated than suggested.



Legally, the Supreme Court’s 2007 decision, _Massachusetts v. EPA_, held that if the EPA determined carbon dioxide is a pollutant causing harm to human health and welfare, then it is empowered to regulate it under the 1992 amendments of the Clean Air Act.





The conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion.



Trump’s executive order cannot call on the EPA to cease and desist from its Clean Power Plan until it somehow determines that carbon dioxide, after all, does not cause endangerment, or that the science is simply not there to show that it does. As science moves slowly, and with the federal government itself providing a vast majority of all climate science funding, this will be a difficult battle.



Undoing regulations is typically more difficult than creating them. However, the selection of Scott Pruitt, who defended the rights of Oklahomans to set their own environmental standards, shows the Trump administration is serious. While many left‐​leaning environmentalists tend to believe Pruitt is “against” the environment, the truth is that most Republicans strongly value the environment — they just wish to regulate it at a state level, where local knowledge and values can be applied. Pruitt is not an anti‐​environmental zealot; as for the EPA, he’s said “Clearly the mission of the EPA is to protect our natural resources, protecting our water quality, improving our air.”



And, as many have noted, even the elimination of the Clean Power Plan will not itself bring coal back to anything like its former life. The major reductions that the US has made in its greenhouse gas emissions stem not so much from a war on coal (indeed, the previous administration was surely belligerent toward the industry), but from the market itself.



Dramatic advances in geolocation and hydraulic fracturing have made natural gas, which only emits half as much carbon dioxide as coal when used for power generation, and the equipment used to burn it, cheaper than coal. It also burns much cleaner, so the expensive scrubbers and bag houses required to capture coal’s bad residuals are not necessary.



This conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion. Instead it was accomplished by greed and genius.



It’s hard to predict the legal fate of Mr. Trump’s latest executive order. What we do know, though, is it will be a long time before the dust settles, and unless many fundamental changes occur legally, diplomatically, and scientifically, any new administration can bring Obama’s policies back to life with a pen and a phone.
"
"
This photo comes come to me from NOAA’s Weather Service Forecast Office in Monterey.
This is the official USHCN climate station of record for Livermore, CA. USHCN # 44997 The temperature sensor is located in a backyard of a residence within six feet of the swimming pool.

Here is the temperature trend from NASA GISS:

The question is: can an unbiased and accurate reading of temperature be obtained in somebody’s backyard next to their pool? With NOAA siting requirements saying a minimum of 100 feet from buildings, I would assume this would apply to pools too.
I couldn’t make this up if I tried.
You can see the picture without the annotations on the NWS website with this direct link:
http://www.wrh.noaa.gov/images/mtr/cpm/4997.jpg


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5653ce6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye (photo right)
and Pierre Gosselin
Today we post before-and-after mean annual temperature charts for 6 US stations in the midwest region with a low brightness index (BI), meaning low impact from the urban heat island (UHI) effect, which arises from widespread asphalt, concrete and infrastucture.
The low BI index tells us that the stations are sited in a rural-type environment. Five of the six stations have a BI of 0, while one (Thibodaux, LA) has a relatively low BI of 11.
Shown will be comparisons of NASA GISS Version 4 unadjusted, versus Version 4 adjusted. In each case the unadjusted data showed a cooling or little warming, while the adjusted data all ended up to show warming.
First we plot the station at Plainville, Kansas, which I already posted at Twitter. Shown is the plot going back over 100 years, before NASA adjustments and after adjustments.

It's obvious, as far as Plainville's temperature goes, NASA made the false warming trend.Why do many media pretend to know nothing about that?https://t.co/35Itt16sN0~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/WTD380BGxN
— キリエ (@KiryeNet) June 7, 2020

Data source: NASA GISS.
As the 2 plots show, the data from the past were changed by NASA and made cooler. The new result: a warming trend! In other words, a cooling climate was fudged into one that is supposedly warming.
Next we move to the station of Hobart, Oklahoma. A slight cooling trend there was transformed by NASA into a warming trend:

Data source: NASA GISS.
The third station we look at is Carrizo Springs, Texas:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: NASA GISS.
Originally in the V4 unadjusted, the past at Carrizo Springs was warmer than today. But NASA didn’t like that, and so they adjusted the temperatures from earlier in the 20th century downward. Again a modest cooling trend was changed into warming.
The story is the same at the station at Conception Missouri:

Data source: NASA GISS.
At Conception, Missouri, we originally saw no warming over the past 130. years. But then NASA fiddled with the data and now tell us there’s been warming at there as well.
Looking at the temperature charts for the station at El Dorado, Arkansas:

Data source: NASA GISS.
Note how warm it was in the 1920s. But NASA said that this couldn’t be right, and so cooled the mean annual temperatures in the early 20th century byalmost a whopping 2 degrees! Result: (fake) warming!
Finally we plot the NASA GISS data from the station located at Thibodaux, Louisiana – i.e. the U.S. South:

Data source: NASA GISS.
Above we see how the unadjusted V4 data were changed to create more warming.
NASA changed the data several times until they got the warming they want to us believe is taking place. The original data tell us there has been any real warming over the past century at these 6 stations.


		jQuery(document).ready(function(){
			jQuery('#dd_9484675fc1afd13abd685b1cca163d6c').on('change', function() {
			  jQuery('#amount_9484675fc1afd13abd685b1cca163d6c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
An odd twist has developed in the past week regarding some data sets that surfacestations.org volunteers have been using to look at individual stations. The data has changed on NASA’s GISS website with no notice whatsoever.
My first indication that something changed came from surfacestations.org volunteer Chris Dunn who wrote to me complaining that one of the sites he’d recently surveyed, Walhalla, SC had been greatly adjusted at GISS for no good reason that he could ascertain, since the site is pristine by climate monitoring standards, and has not gone through any significant changes in the past, and has been operated at the same location (by the same family) since 1916. He wondered why NASA would have to adjust the data for a “good” station. The way I view it, shouldn’t good data stand on it’s own? That was September 7th. He was using data from NASA GISS published on 8/28.
So he continued to look at the data, and the site. The on Sept 11th he noticed a change when he downloaded the data again. Something had changed, the data was different. Not only the adjusted data but the “raw” data too.
Steve McIntyre of Climate Audit has a complete review at: http://www.climateaudit.org/?p=2077 where he traces data back to Detroit Lakes, MN the station that started this all. See my original post on this: http://www.norcalblogs.com/watts/2007/08/1998_no_longer_the_hottest_yea.html
This set other people into motion looking at the NASA GISS data sets. The conclusion? NASA published new raw and adjusted data on their website with no formal or informal notice. I don’t know what to make of this, by I think perhaps this could be a breach of the Data Quality Act. At the least, it flies in the face of accepted scientific courtesy, where if you publish data sets being used by researchers worldwide, scientific courtesy would dictate that you at least place notice of such a change, otherwise there can be a domino effect for hundreds of research projects that use the data. Which would cause researchers to wonder why things don’t look the same anymore and begin searching for answers. Well that is exactly what happened here. We had a citizen trying to figure out why a climate site with good data was “adjusted”, and then the data changed right in the middle of him looking at it.
Whether this was accidental or intentional I cannot say, but it certainly does not look good coming on the heels of NASA GISS’s most recent issue of a mistake causing a revision of our temperature history on August 8th. We deserve better accounting than this when so much hinges on this data.
Let’s give NASA and Hansen the benefit of the doubt and see what they have to say about it.
UPDATE: NASA has posted today, their explanation which you can read here: http://data.giss.nasa.gov/gistemp/ Note that this notice appears a full week after the data changed (about 9/10) and only after there was discussion of the issue on blogs such as Climate Audit over the weekend. Why would NASA GISS not announce the change at the same time the data did, particularly when the announcment of the change ammounted to one small paragraph?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3fb385b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From the ""almost a
Darwin Award winner""
department
and from
WISN-TV
in Milwaukee…TV News Truck Breaks Through Ice:

Even though the temperatures have fallen, the ice on many bodies of water is
not thick enough to support vehicles.
A crew for a local television station drove its news truck onto a channel to
Big Muskeo Lake Sunday and broke through the ice. Crews for WDJT-TV in Milwaukee
were reportedly shooting a story about thin ice when the truck fell through.
The driver reportedly mistook the channel for a road when the accident happened.
The truck was approximately 150 yards off the boat launch, according to a
release issued by the City of Muskego Police Department.
The driver of the truck was the sole occupant and was able to get out without
injury, but the truck remains partially submerged.

Moderators note: Having worked in TV news myself, I’m
not that surprised. Often the only thing on reporters and producers minds is the
story deadline. Caution and common sense sometimes take a backseat in the news van.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea880e97d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe grieve the loss of Dr. S. Fred Singer
By Michael Limburg, EIKE
(Translated, edited by P. Gosselin)

Dr. Siegfried Frederick Singer

Yesterday, our mentor and good friend, the outstanding scientist S. Fred Singer passed away – peacefully and quietly at the age of 95. Without the constant encouragement we received from this outstanding scientist from the very beginning, the founding of EIKE and our commitment to the dissemination of the scientific facts on climate change would not have been possible here in Germany.


Dr. Fred Singer as the keynote speaker at the first European Climate and Energy conference on May 30, 2007.
Distinguished career spanning 7 decades
Dr. Singer was the keynote speaker at our very first Climate Change Conference in Berlin in 2007 at the premises of the Institute for Entrepreneurial Freedom (IUF) on May 30, 2007, immediately after our founding. And he remained loyal to us in all subsequent years, even though in recent years his physical condition made the long journeys from his home in Virginia increasingly difficult.
But his unrestrained desire not to let science degenerate into a water-boy for politics, which was particularly evident in the increasing appropriation of environmental science by politics, allowed him to marshal all the strength his body could muster.
Fortunately for us all, he was able to do so for almost one and half decades. No one would have been more predestined than him to see exactly this monopolization, because he came directly from science and always worked there in outstanding positions. A short and partial look at his extraordinary curriculum vitae shows.
Father of U.S. weather satellites
His scientific work has also been published over 200 times in leading scientific journals. In 1954, President Eisenhower even awarded him a special prize from the White House for his work.
Without any exaggeration it can be said that S.Fred Singer can be called the father of the US weather satellites. Atmospheric physics was his domain.
Politicization of science “highly dangerous for democracy”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Because he saw that the emerging environmental movement was striving for a symbiosis with politics in particular, which was highly dangerous for democracy, he founded the Science and Environmental Policy Project (SEPP) in 1990 and the Nongovernmental International Panel on Climate Change (NIPCC) in Vienna in 2008. Both institutions were active in the collection and dissemination of scientific facts, against the increasing ideologization of the environmental idea – and the emerging panic-mongering about supposedly man-made climate change.
Over 200 scientific publications, wealth of books
A wealth of scientific books (Climate Change Reconsidered, or Unstoppable Global Warming, Every 1,500 Years, together with Dennis Avery) and many works written during this fruitful period, many of them with the support of Heartland and CFACT, bear eloquent witness to this.

Fred Singer at the 5th Climate and Energy Conference in Munich in 2012.
Escaped Nazi Germany, “unspeakable cruelty”
Our friend, my good friend S. Fred Singer, was also a living example of the unspeakable division and cruelty our continent saw in the last century. Born in Vienna in 1924 as the child of a Jewish family, he left his home country at the early age of 14 after the annexation of Austria by Nazi Germany in 1938, fled first to the Netherlands, where he was apprenticed to an optician, and from there emigrated further via England to the USA.
After serving in the U.S. Navy, he studied physics at Princeton and received his doctorate in 1947. Later he also studied electrical engineering at Ohio State University, where he graduated with a diploma. In addition to English, Fred spoke German, Swedish and Dutch.
Defamed by environmental activists who spread lies
However, neither his resume nor his extraordinary scientific merits kept the growing opposition from the green-left camp from attacking and muzzling him using unspeakable defamation and lies instead of scientific debate. The German WIKIPEDIA issue (here) offers readers an example of this. Among other things, the lie is repeated repeatedly that Singer would have let himself be bought by the tobacco lobby because he – himself a lifelong non-smoker and chairman of a non-smoker’s association – had truthfully stated that the carcinogenic effect of passive smoking could not have been scientifically proven.
My last e-mail contact with him is dated October 8, 2019, when we, the board of directors of EIKE, congratulated him on his 95th birthday. We didn’t get an answer to that, his mind was still alert, as we know, but his body refused  to go on.
Farewell, good old friend, rest in peace. You have done so much for this society. I am very proud to have had you as my friend.
Michael Limburg
European Institute for Climate and Energy
===========================
Also this blog, NoTricksZone, was in large part inspired by Dr. Singer, his occasional emails to me and particularly his book: Unstoppable Global Warming Every 1500 Years. And I know other skeptics here who also say they were inspired by him.
I recall wishing hm a happy 90th birthday with this blog post. He’s going to be missed.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAn expert psychoanalyst appeared on German television to provide his view on Greta’s high-octane anger, see (German) video below:

At her speech at the UN, Greta’s voice was filled with worry and acute anger. When asked about the source of the intense emotions, German psychiatrist, psychoanalyst and book author Hans-Joachim Maaz commented:
What’s really bad about it, is the marketing, what is being made of her, how she is being, shall we say, used or misused for certain interests. […] I don’t find this is appropriate. I would surely critically ask the parents what they intend by all this, that they are tolerating this, that they are indeed promoting  Greta’s personal problem. I find this ethically problematic.”
Anger has other sources
About the pain and anger in her performance in New York that was accompanied by emotionally charged accusations that her youth and life have been destroyed by the climate situation, Maaz says: “That’s just not real. It’s not the case. She just hasn’t had any such really serious experience that would justify such anger. But she harbors such affects within herself and they certainly have a totally other source.”
The German psychiatrist adds:
That they fundamentally allow her to storm in such a direction, without taking her problems into account, without giving her help – that I find to be highly troublesome.”
Share this...FacebookTwitter "
"
I just watched a presentation Elsi Sertel from a university in Turkey showing how easy it is to introduce true land cover data into a climate model. Her study area was around the Black Sea near Istanbul, and used LANDSAT imagery along with a pixel by pixel truthing technique to determine the type of land cover, sea, forest, urban, etc and apply it to use in a GCM.
Her premise was that current GCM’s use land surface info that isn’t fully representative, out of date, and in some cases just plain wrong.
Her study showed a technique that allowed for a significant amount of automation to the process, to allow improved and current land surface types to be easily integrated into the grid cells of a GCM. Unfortunately, some GCM gridding schemes are too coarse to handle such data.
From what I’ve seen in this conference so far, and I’ve seen presentations now from Europe, Turkey, China, Australia and the USA, it is becoming more clear that land use is a major driver of climate change, and perhaps dwarfs even GHG effects. That’s just a hunch. One study from Australia showed the effects of removing a woody type bush over a large area over the past century, and the results on rainfall and temperature were profound.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4691079',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterProbably the most disturbing aspect of wind energy is its destructive impact the green energy source has on the environment, never mind its sporadic supply, adverse health impacts and high costs.
As Germany ignored all warnings about wind energy’s shortcomings and its threats, government officials charged ahead blindly and obstinately, without a plan, into the uncontrolled development of the power source.
Germany wrecks its landscape
What follows is a photo video montage of how a beautiful natural landscape near the (once) scenic central German community of Wartenberg was tragically transformed into an industrial eyesore.

The photos were taken by Herrmann Dirr
The “Vogelsberg” wind project was carried out by a cooperation consisting of utility HessenEnergie, “scrupelous” forest owners and the community of Wartenberg.
The video posted on YouTube starts by showing a once idyllic scenery the area offered before the wind project came along and stamped it out. At about the 1:20 mark we begin to see the destruction inflicted on nature to build the project and then how today it is an almost inhospitable area for much wildlife.
Ruining communities to “save the planet”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s a classic example of how people professing to have good intentions, yet lacking in vision and judgement, allow themselves to be hijacked by populist green ideology and end up ruining everything they touch around them. They tell us they will rescue the planet, but only end up ruining their communities. It’s a disgrace.
These “leaders” operate according to true populism. They make promises that sound good to the masses who cannot think one step ahead and see the true consequences.
Vermont’s, Bernie Sanders’s disaster
It all reminds me how a few years back my home state of Vermont (whose source of power was in large part green hydro to begin with) deforested and dynamited its green mountain tops which took nature millions of years to sculpture, and then installed large wind turbines. Also see here and here.

How Vermont protects the environment by installing wind turbines. Photos taken near Lowell. 

Silent spring for Vermont wildlife. Making way for green energies. Stupid as stupid gets. 
The source of above photos is mountaintalk.com.
 
Share this...FacebookTwitter "
"

Could it be the _Washington Post_? Bannered across the top of the _Post_ ’s op‐​ed page today is a piece titled “Copenhagen’s political science,” titularly authored by Sarah Palin. I’m delighted to see the _Post_ publishing an op‐​ed critical of the questionable science behind the Copenhagen conference and the demands for massive regulations to deal with “climate change.”   
  
  
But Sarah Palin? Of all the experts and political leaders a great newspaper might call on for a critical look at the science behind global warming, Sarah Palin?   
  
  
What’s even more interesting is that the _Post_ also ran an op‐​ed by Palin in July. But during this entire year, the _Post_ has not run any op‐​eds by such credible and accomplished Republicans as Gov. Mitch Daniels; former governors Mitt Romney or Gary Johnson; Sen. John Thune; or indeed former governor Mike Huckabee, who might be Palin’s chief rival for the social‐​conservative vote. You might almost think the _Post_ wanted Palin to be seen as a leader of Republicans.   
  
  
I should note that during the past year the _Post_ has run one op‐​ed each from John McCain, Bobby Jindal, Newt Gingrich, and Tim Pawlenty. (And for people who don’t read well, I should note that when I call the people above “credible and accomplished,” that’s not an endorsement for any political office.) Still, it’s the rare political leader who gets two Post op‐​eds in six months, and rarer still the _Post_ op‐​eds by ex‐​governors who can’t name a newspaper that they read.
"
"

Seven years ago, Rand Simberg, an adjunct scholar at the Competitive Enterprise Institute (CEI), wrote a blog post criticizing the work of Dr. Michael E. Mann, a climatologist at Penn State University and a noted global warming doomsayer. Mann helped create the famous “hockey stick graph,” which shows temperatures spiking in the 20th century, and he was implicated in the famous “Climategate” scandal, when hackers obtained emails from the Climate Research Unit at the University of East Anglia. One of those emails described “Mike’s nature trick,” referring to the splicing together of different temperature data to “hide the decline” in global temperatures. The emails created enough controversy that investigations into Mann’s work were launched by Penn State and the National Science Foundation, both of which cleared him of any wrongdoing.



Simberg wrote that many in the skeptic community regarded the Penn State investigation as a “whitewash” because “university circled the wagons and narrowed the focus of its own investigation to declare him ethical.” He then used unfortunate language to compare the investigation to the then‐​ongoing Jerry Sandusky scandal, writing that Mann could be said to be “Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science.” He raised the Sandusky comparison because Penn State’s investigation had concluded that “in order to avoid the consequences of bad publicity” the university’s top officials had “repeatedly concealed critical facts relating to Sandusky’s child abuse from authorities.” Simberg asked if the university could be expected to “do any less to hide academic and scientific misconduct, with so much at stake” in terms of reputation and funding.



Michael Mann sued CEI, Simberg, National Review, and Mark Steyn for defamation (Mark Steyn had linked and quoted Simberg’s post in a National Review post), claiming that the accusations of scientific misconduct and data manipulation and molestation were false statements of fact (rather than opinion) that defamed his reputation as a scientist. Surprisingly, the DC Court of Appeals, which is like the state supreme court for DC, allowed the case to go forward despite the clear and disturbing First Amendment implications. Now the case is on petition to the Supreme Court. Cato has filed for the fourth time in this case, now joined by the Individual Rights Foundation and the Reason Foundation, and we’ve asked the Court to stop this dangerous case from going forward.



Defamation is one of the categories of speech unprotected by the First Amendment, and it is very important that courts keep those categories narrow or a lot of protected speech could be censored or chilled. When speech is about an ongoing debate of significant public concern, as is the case here, courts should be wary of those who want to use defamation law to shut down public debate. Mann, who has described climate change “deniers” as “shills for the fossil fuel industry,” was exonerated by the investigations into his conduct, but Simberg disagreed. The DC Court of Appeals, however, put undue weight on the investigations into Mann and other climate researchers. In the court’s view, the investigations showed not only that the allegation of “scientific misconduct” was “capable of being proved true or false, but the evidence of record is that it actually has been proved to be false by four separate investigations.”



Questioning an investigation that purports to exonerate a controversial figure should not give rise to an actionable defamation claim. Calling O.J. Simpson a murderer is not defamation because he was acquitted by a jury. Saying that someone who was exonerated by the Warren Commission did in fact have role in the Kennedy assassination is not defamation either. If this case is allowed to stand if will be the law in the District of Columbia, the pulsing heart of our political discourse. Those who arrive at conclusions contrary to official reports or investigations will be too easily subject to possible defamation suits. The Supreme Court should take the case to ensure that people can’t use courts to shut down public debate.  

"
"
This afternoon there will be several presentations that embrace the measurement systems used for the near surface temperature and precipitation records.
Of great interest to me is a presentation outlining the new US CRN (Climate Reference Network) by Bruce Baker of NCDC. Another is by Glenn Conner, former Kentucky State Climatologist whose talk will be about the role of station histories in identifying biases in climate records.
My presentation follows those two – it should be a lively afternoon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea450bc7f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Russ Steele is out on vacation and doing several surveys while traveling. This one below is from St. George, UT. Here we see an MMTS measuring the temperature near the surface of an elevated parking lot. The effect of the asphalt and vehicles that park near it, engine forward, probably dwarfs the effect of the nearby a/c unit. The shading may help daytime temps some, but the asphalt likely biases Tmin the most. The complete photo survey is available on surfacestations.org




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4b945b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
We had ""yellow"" level geomagnetic activity on the sun last night, and more may
come tonight and tomorrow night. Its coming from Sunspot 953, which is about 3 times the size of the Earth.

Sunspot 953 is crackling with mild
B-class solar flares. Credit: SOHO/MDI 

Image of sunspot 953 taken today by Sebastien Kersten of Le Cocq, Belgium:
Here is the dispatch:
From: solarxactivity@bbso.njit.edu
Date: April 28, 2007 9:24:59 AM CDT
To: xxxx@rice.edu
Subject: BBSO Solar Activity Warning 28-APR-2007 14:19:18 UT
Region NOAA 10953 is currently beta-gamma magnetic class, and may increase in complexity.
The region is bright in H-alpha as well. This region has a chance of producing M-class
events.
NOAA 10953, S10 E41.  Beta-gamma region. Position as of April 28, 2007 at 13:30 UT.
And this is in the middle of our solar minimum, indicating our sun still has a few belches to pass out before completely settling down.
One of the best tools we have is the ACE Spacecraft, which monitors the sun 24/7 and provides us with a plethora of real-time data, of the magnetic
field, the solar wind, and  inter-galactic cosmic ray counts.

For the latest ""dial"" info (including our ""space weather stoplight"") go to
http://space.rice.edu/ISTP/dials.html
For the latest 10-minute averages of the Boyle Index from realtime ACE
spacecraft data, go to http://space.rice.edu/ISTP/wind.html
Some guides to interpret the gauges
If the hourly-average of the Boyle index exceeds 110, then Kp 4-6 storms
will likely occur within the next three hours
If the hourly average of the Boyle index exceeds 200, then major magnetic
storms will occur within the next three hours
If the hourly average of the Boyle index exceeds 250, major low-latitude
auroras will occur within the next three hours.
A magnetic storm generally occurs about an hour or two after the CME arrives at Earth, which is roughly 26-48 hours *after* a major solar flare. The Boyle Index is derived from real-time ACE spacecraft data, which gives about 45 minutes of warning before it hits the Earth.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6a441e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterToday, 30% of the globe’s CO2 emissions come from China. In 10 years, China’s emissions alone will match the rest of world’s emissions combined. China continues to build hundreds of coal plants today. So why are the rest of us spending $600 billion every year on CO2 emissions mitigation?
China overtook the United States as the world’s largest CO2 emitter in 2008 (Liu et al., 2019).

Image Source: Liu et al., 2019
It only took 7 years for China’s emissions percentage to double that of the USA’s. As of 2015, China accounted for 30% of global emissions (Shan et al., 2018) compared to the USA’s 15%.
Much of the reason for China’s emissions domination is because its citizens consume more than 50% of the world’s coal.
China is in the process of building 100s of new coal plants, with plans to add a new coal plant every 2 weeks for the next 12 years.
According to the People’s Daily, China, the country’s longest coal transporting railway, carrying 200 million tonnes of coal from north to east China every year, is now (October, 2019) in operation.

Menghua Railway, China’s LONGEST coal transporting railway line, is expected to be put in operation in Oct. The 1,837-km railway will carry 200 million tonnes of coal annually from N China's Inner Mongolia to E China's Jiangxi. pic.twitter.com/sFXpCjplaN
— People's Daily, China (@PDChina) July 23, 2019



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Due to its exponentially-growing energy demands, China will be responsible for 50% of the globe’s CO2 emissions within 10 years (Liu et al., 2019).
Why should the rest of us spend $89 trillion to reduce CO2 emissions?
According to proponents of CO2 mitigation policies, the cost of infrastructure changes required to reduce CO2 emissions to acceptable levels is $89 trillion by 2030.

Image Source: WorldBank.org
Per a scolding, we’re-not-spending-enough-on-climate article published in the journal Nature, we’re already spending about $600 billion annually on CO2 mitigation.
“[T]otal climate-related financing was $510 billion to $530 billion in 2017,” which is much higher than the $360 billion spent in 2012. “The UN’s Framework Convention on Climate Change (UNFCCC), put it at $681 billion in 2016” (Yeo, 2019).
So we’re spending 100s of billions to 10s of trillions to reduce CO2 emissions in Western countries.
Meanwhile, China continues to build hundreds of new coal plants and grow its carbon-intensive infrastructure, thwarting any and all efforts to reduce net global emissions.
Why are we doing this?
Share this...FacebookTwitter "
"

Anyone who knows much about me knows that I’ve been a strong advocate for alternate energy, and that I’ve put my money where my mouth is by putting solar power projects on my home as well as at Little Chico Creek School in my former role as CUSD Trustee.
Now I’m going to push an alternate way to do home or business computing.
A couple of months ago I wrote about the upcoming release of Windows Vista, and how I was disappointed that this new release from Microsoft and all of its Digital Rights Management (DRM) nonsense made the operating system turn your PC into a version of George Orwell’s Big Brother.
A friend of mine, school Trustee Rick Anderson recently dumped his PC and bought a Mac Mini because he said he was tired of all the viruses, spyware, upgrades and the like. I pointed out that if all he needed to do was do email, web browsing, word processing, some digital picture work and maybe some video editing, then the Mac Mini would be a good choice since it comes with all those things right out of the box.
It’s an important point, because what I described is what the majority of non technical people need in a personal computer. So why go through the expense and hassle that has become Windows? That got me to looking at an operating system that I once only thought of to be the domain of the uber Geeks – Linux.
For those who don’t know about Linux, or have the view that you have to be one of those people that stares at a computer screen until 3AM and then falls asleep on a stack of empty pizza boxes, that used to be the case. But Linux has grown up. While there are still a few distributions aka distros out there that cater to that some new ones have emerged recently that are as easy to use as Windows. They even come with applications like word processors, spread sheets, etc and the best part is they are free or low priced. Some work right out of the box, requiring only a simple install.

Since Microsoft had me so ticked off because of the expense and corporate control issues in Vista, I recently started experimenting with a Linux distro called Ubuntu. Now don’t let the weird name scare you, this is actually the most amazing thing I’ve seen in awhile. Ubuntu is as African word meaning “humanity to others” and the company that is distributing it bills the software as “Linux for Human Beings”. The company seems to be setup like a philantrophy, they want to bring computers to people whom can’t afford it. With Vista costing upwards to $500 for ll of its features, they are positioned well to do that.
I recently installed it on a blank PC, and was instantly impressed. Best of all it’s free if you know how to download it and burn it to a DVD. If not, they have lost cost media you can buy.
My installation experience was fast and easy. And I had Ubuntu up and running within 30 minutes. It was just as simple as putting in the DVD, answering a few simple questions about configuration, and off it went. It came up, updated itself automatically wiht the web connection and was ready to run.

It comes with everything a home or small office might need. Word processor and Open Office Application Suite, Firefox web browser, Email, and a bunch of other applications including a cool paint program called “gimp” that rivals Adobe PhotoShop. Ubuntu comes installed with a project management application called Planner. The tool allows users to create simple Gannt charts, tasks, and allocate resources. There’s even a Microsoft PowerPoint clone. And you can read or save Microsoft document formats for all their office applications.

There are hundreds of other free aplications available for download.
Ubuntu recently announced they are offering a free video/graphics/audio editor to make DVD’s and edit video from camcorders and sound/music tracks. You can see a preview of Ubuntu Studio here
If you can run Windows you can just as easily run Ubuntu Linux. It’s fast, relaible and virus/spyware free.
I gotta say that if you just want a simple and reliable student, home, or office PC at minimal cost, one that will actually run effectively even on older inexpensive hardware, Ubuntu is it.
Microsoft is going to lose some of their edge to unique companies like this.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8e63d13',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe current furor about an alleged connection between climate change, CO2 emissions, and Australian fires finds no support in the scientific literature. According to scientists, rising CO2 concentrations reduce fire ignition and burned area. Further, both global-scale and Australian fires were far more pervasive during the colder Little Ice Age.
Here’s what the scientific literature has to say about fires and their connection to climate and CO2 concentrations.

1. “Elevated CO2 and warmer climate promote global total tree cover” and higher CO2 “leads to reduced fire ignition and burned area” (Chen et al., 2019).
Image Source: Chen et al., 2019

2. Globally, fires were much more common during the colder Little Ice Age (Yang et al., 2007; Ward et al., 2018; Doerr and Santin, 2016), or before 1900. Fire frequencies have been rapidly declining as CO2 emissions began abruptly rising in the 1940s.

Image Source: Yang et al., 2007

Image Source: Ward et al., 2018

Image Source: Doerr and Santin, 2016

3. There has been a continued decline in global fire since the 21st century began (Earl and Simmonds, 2018).

Image Source: Earl and Simmonds, 2018



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




4. Australia’s mainland experienced far more pervasive fire during the 1800s to early 1900s, or the Little Ice Age. There has been an abrupt decline in fire activity for the entire Australasian region in the last 50 years (Mooney et al., 2011).
Image Source: Mooney et al., 2011

5. The “assumed positive relationship between drier climates and biomass burning” is not supported by wetter Little Ice Age climates coeval with more burned area in Australia (Tibby et al., 2018).

Image Source: Tibby et al., 2018

6. Australia, like the globe, has neither become wetter or drier over the last 3 decades. There is “no evidence” global precipitation patterns have been been altered by global or regional temperature changes (Nguyen et al., 2018).

Image Source: Nguyen et al., 2018

Image Source: Nguyen et al., 2018

To summarize, the scientific literature does not lend support to claims fires in Australia are connected to warming, rising CO2 emissions, dry climates, or wet climates.
If there were a potential climate linkage, it would be that enhanced fire activity arises in cooler climates.
In other words, there is no apparent link to anthropogenic global warming that can be supported by evidence found in the scientific literature.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHardcore German left-wing activist Tom Radke – citing selfish leaders, fraudulent science, psychological manipulation and a cult-like atmosphere within the German Greens – has had enough of the Fridays For Future (FFF) Germany movement and has announced his resignation.  

German left-wing activist, former FFF participant Tom Radke quits movement after experiencing its inner working. Image cropped from Tom Radke.de.
In a statement published at his site, Tom Radke wrote he wishes “to concentrate on left-wing patriotic politics: environmental protection, a strong welfare state and peace” instead of “Green voters and ‘green’ corporations”.
FFF not about the environment
Radke writes how he “found out through very negative practical experiences with Fridays for Future that many people never cared about environmental protection” and that he “misjudged the other ‘climate activists”.
He wrote: “Of all activists at FFF Hamburg, only a few were really interested in preserving our environment, clean air and healthy food.”
Radke blasts the FFF leader “Longhaul Luisa” Neubauer who he believes was “obviously in it for a career” and “her luxury life” while she called on others to save money.
Greta “taken advantage of”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Radke also had the impression that FFF movement leader Greta Thunberg was “being taken advantage of by her family but that she was “personally a nice person”.
Climate science “largely manipulation and fraud”
He recognized “the nonsensical and anti-people content of the climate movement” and that the movement’s “climate science is largely manipulation and fraud”.
Movement based on “pure emotions and blind faith”
Radke also explains how he came to realize that the FFF activists in fact had very little knowledge about climate science itself and that “their fanaticism is largely based on pure emotions and blind faith” and “also based on fear-mongering”.
“The young people are told that they, their families and everyone they love will die if we don’t act immediately.”
“Leaving the cult”
Radke accuses the German Green Party Using “a lot of psychological pressure” to coerce donations from followers despite the fact that “there are major donors in the background, who are completely unknown to the ordinary members.”
Radke also writes that the FFF movement “has nothing to to do with real environmental protection” and that “the CO2-tax serves to squeeze even more out of ordinary people.
He summarizes his departure as follows: “I feel a little like a person who is leaving the cult. It is a liberating feeling. Through my experience I will try to help other students to leave FFF and the climate religion.”


		jQuery(document).ready(function(){
			jQuery('#dd_47acf6f0d6d4644f09cfbc459ffd60db').on('change', function() {
			  jQuery('#amount_47acf6f0d6d4644f09cfbc459ffd60db').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
About two weeks ago I published this story about the loony idea that was proposed by some researcher in Europe about “cell phone radiation may be killing bees”. I pointed out that it was garbage then, as it is now. Here’s a portion of the original post I made:


There’s an article on UK’s The Independent website about a most unusual scientific theory: “Cell Phones kill bees.”

Well today in the LA Times,  it seems that UC San Francisco researchers have uncovered what they believe to be the real cause, and its not loony ideas like cell phones. Its fungus.
From the article:
A fungus that caused widespread loss of bee colonies in Europe and Asia may be playing a crucial role in the mysterious phenomenon known as Colony Collapse Disorder that is wiping out bees across the United States, UC San Francisco researchers said Wednesday.
Researchers have been struggling for months to explain the disorder, and the new findings provide the first solid evidence pointing to a potential cause.
Other researchers said Wednesday that they too had found the fungus, a single-celled parasite called Nosema ceranae, in affected hives from around the country — as well as in some hives where bees had survived. Those researchers have also found two other fungi and half a dozen viruses in the dead bees. 
The researchers caution that the results are preliminary, and data sampling represents just a fraction of hives, but they are encouraged by the findings. Hopefully they’ll be able to come up with a solution.
Yet it appears that the “Cell Phones kill bees” lunacy has caught on, since there’s a comment today in the ER’s “Tell it to the ER” that furthers that nutball idea. What a public disservice that column is.
Thanks to Lon Glazner for the tip.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6c06c18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 There’s an interesting thorn in the side of the recent planning commission approval of Meriam Park that nobody seems to have brought up or discussed. Maybe there are plans I’m not aware of, but given the issues being raised with a cell phone tower elsewhere in the city, it seems the issue would have been vocalized by now.

There’s a giant cell phone tower jutting into the Meriam Park property. Most people think its a standard radio station. It was, but not anymore. It’s purely a cell phone site. It’s at the intersection of Bruce and Picholine Way, as seen at left.

I’ve also provided an aerial view from Google Earth.

The former KHSL-AM radio towers on Bruce Road no longer broadcast on AM 1290 as they did for 50 plus years. That transmitter was removed a few years ago but the towers remained. The FCC license was and property was sold to McCoy broadcasting and KPAY 1050 was converted to 1290.
In the early 1990’s, one of Chico’s first cell phone services was placed on the East tower and it remains in service today. The West tower is not transmitting anything at all. A few years ago, the Bruce road property was sold to one of the cell phone companies when Clearchannel bought KPAY from McCoy.
The old KHSL Radio tower cell site is probably the best in Chico due to its location and height. I don’t think they’ll be easily persuaded to give it up.
So now, some city councilors that may want to vote against a cell phone tower at the Elks Lodge for “health and safety” issues raised by enlightened citizens may find themselves in a pickle when it comes to approving Meriam Park homes that will be less than 500 feet from those “dangerous cell phone emissions”.
In the graphic below from the Meriam Park planning website, I’ve added the pointer showing where the cell phone tower is in relation to the rest of the plan.

The neighbors right across the street on Picholine fit in that zone already.
I wonder if the neighbors on Picholine Way were ever told of the nature of that tower? I wonder if they even care? Since the cell phone transmitters have been there about 15 years now, I wonder how many of the neighbors are suffering from debilitating health issues as is claimed by some detractors of the Elks Lodge cell tower?
‘Tis a quandary for sure.
UPDATE– It turns out New Urban Builders has purhcased the tower property, see comments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea64c07c1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGermany’s onslaught on its famed automotive and production industries appears to be taking an economic toll as the country pushes ahead to go green by phasing out internal combustion engines and coal power plants.
Recently we reported how electricity prices are again slated to increase this year, and thus will continue to make German power among the most expensive worldwide.
A wave of green activism has led to tighter regulations against the internal combustion engines and to a planned phase-out of coal-fired power plants.
Teetering on recession
Just recently German online business daily Handelsblatt reported here that there are “new concerns about an economic slump in Germany” as “surprisingly weak figures are fueling new worries about a downturn”.
“Horrible numbers”…a “disaster”
“Experts spoke of ‘horrible numbers’, a ‘disaster’. Industry, construction, and energy providers produced a full 3.5 percent less in December than in the previous month,” the Handelsblatt reports.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




December production plummets 6.8%
The economic bloodbath was even worse in the production sector which “fell even more sharply, with output falling by 6.8 percent – the sharpest drop since the end of 2009,” writes the Handelsblatt. “Concerns are growing again that the German economy may be in more difficult waters than expected.”
For Germany, “2019 was not only the worst year for industrial orders since 2008, it was also the first time since 2002 that German order books shrank for two years in a row,” reports Yahoo here.
Massive automotive layoffs
The German auto sector has been hard hit. For example, car maker Opel recently announced 2,100 job cuts in Germany. Late last year Daimler, owner of Mercedes Benz, announced plans “to ax at least 10,000 jobs,” Volkswagen’s Audi said “it would slash up to 9,500 jobs or one in ten staff by 2025 and car suppliers Continental and Osram announced staff and cost cuts.”
The Financial Times reported today that Daimler suffered its “worst results in decade” and that its earnings “plunged 60% in 2019 amid ‘Dieselgate’ woes.” Daimler also “refused to deny reports” that an additional 5,000 jobs could be cut.
The Financial Times adds: “Daimler is being forced to spend heavily on electric vehicles and plug-in hybrids in order to avoid fines from Brussels for breaching new emissions regulations.”
Other reasons cited for the poor German economic results are the ongoing global trade disputes. Figures are expected to come under even greater pressure due to the spreading corona virus in China.
Share this...FacebookTwitter "
"

You know you’ve reached critical mass in an argument when you start having editorial cartoons drawn about you.
In this weeks Chico Beat, the editorial cartoon above appeared. While editor Tom Gascoyne would not admit to it being my caricature that was used, a call to artist Steve Ferchaud in Paradise confirmed he used me at Tom’s suggestion of my name.
I consider it high praise to be drawn by Ferchaud, but not so high to be in the Chico Beat.
In any event, by the end of the year 2017, ten plus years from now, we’ll know for sure who’s right. I think it will start to be cooler due to the solar cycle starting to dampen.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea812990b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The picture below is of the USHCN climate station of record for Newport Beach, CA When I first visited this site I did a double take. Then started searching for the “real” temperature sensor.


I couldn’t believe that NOAA allowed them to use consumer grade equipment. I was sure I just hadn’t located the MMTS sensor. It wasn’t until I looked up the MMS metadata entry for equipment for NB and saw “miscellaneous” listed for rain and temperature sensors, that I began to get concerned.

I then went back a second time to be sure I hadn’t missed the station, after checking lat/lon on my GPS…because I just didn’t think it possible NOAA would allow a consumer grade sensor in the USHCN dataset. Then I found somebody in the harbor patrol office to ask, and he confirmed that was the station they use to send readings to NOAA.
I was reminded of that famous quote from the movie “Treasure of the Sierra Madre” lampooned in the movie Blazing Saddles; “We don’t need no stinking badges!”. Except, what was playing in my mind then was “We don’t need no stinking homogeneity!”
Note to NOAA: standards exist for a reason.
Apparently the observer wanted wind too, (the wind sensors are on top of the tower, not shown in these pictures)and while I can appreciate that being located at the harbor patrol office, NOAA could have supplied standard equipment in addition to the shiny new consumer grade Davis station. In fact a standard rain gauge and MMTS did exist, but was removed in 1998 in favor of “miscellaneous” equipment.
Now don’t get me wrong, Davis makes a great weather station, but we can’t just replace sensors with other types willy-nilly and have a homogeneously rigorous data set.
But there are other issues too, such as the rooftop proximity, the diesel generator, and the parking lot it sets in the middle of. More pictures available on surfacestations.org


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5193595',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFour reconstructions from the central and western High Arctic reveal July temperatures were about 1-2°C warmer than today during most of the 1st millennium and Medieval period (Tamo and Gajewski, 2019).
A few years ago, a chironomid reconstruction of Boothia Peninsula in the Canadian Arctic (Fortin and Gajewski, 2016) revealed not only were today’s temperatures the coldest of the last 7000 years, but the last 150 years “do not indicate a warming during this time.”

Image Source: Fortin and Gajewski, 2016
The Canadian Arctic’s Baffin Island had 5°C warmer summer temperatures between 10,000 and 8000 years ago (Ilyashuk et al., 2011). Somehow the polar bears managed to survive this sea-ice-free period.

Image Source: Ilyashuk et al., 2011
Earlier this year, another Canadian Arctic reconstruction (Bajolle et al., 2019) was published indicating temperatures were about 2-4.5°C warmer than today throughout the last 8500 years. Only 3 records mark temperatures colder than they are now.

Image Source: Bajolle et al., 2019
Another new reconstruction of Arctic Canada temperatures cites 4 records indicating the Medieval Climate Anomoly (MCA) was about 1-2°C warmer than today (Tamo and Gajewski, 2019).

Image Source: Tamo and Gajewski, 2019
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Prof. Fritz Vahrenholt at Die kalte Sonne
(Translated by P. Gosselin)
June 7, 2020
Dear ladies and gentlemen
First, the global mean temperature of satellite based measurements was surprisingly much higher in May 2020 than in April. In contrast, the global temperatures of the series of measurements on land and sea decreased. The difference can be explained by the fact that under warm El-Nino conditions the satellite measurements lag about 2-3 months behind the earth-based measurements.
From November 2019 to March 2020 a moderate El-Nino was observed, which has now been replaced by neutral conditions in the Pacific. Therefore, it is to be expected that also the satellite based measurements, which we use at this point, will show a decrease in temperatures within 2-3 months.
The average temperature increase since 1981 remained unchanged at 0.14 degrees Celsius per decade. The sunspot number of 0.2 corresponded to the expectations of the solar minimum.
The earth is greening


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In August 2019, I reported on a remarkable publication by the Max Planck Institute for Meteorology in Hamburg: “Our main finding,” said Aexander Winkler’s researchers at the time, “is that the effect of CO2 concentration on terrestrial photosynthesis is greater than previously thought and therefore has important implications for the future carbon cycle.
According to this, the CO2 attenuation effect of plants is 60% higher than the average of climate models had assumed.
“In the last two decades, an average of 310,000 km² of additional leaf and needle area – roughly the size of Poland or Germany – has been created every year,” the researchers say. I had shared this important finding with the members of the German Bundestag at the time, which led Stefan Rahmstorf to conclude that I was “trying to fool the German Bundestag“. This assessment was taken up by some media such as the TAZ and ultimately led to my dismissal as sole director of the German Wildlife Foundation.
New confirmation: CO2 uptake by plants is increasing
In April ,2020, a research group led by the Australian scientist Vanessa Haverd published a paper in Global Change Biology which more than confirmed the findings of the Max Planck Institute. The researchers describe that plants have absorbed 30% more CO2 since 1900. The previous estimates were 17%. In their calculation for a mild increase in CO2 in this century (IPCC scenario 2.6), the researchers led by Vanessa Haverd arrived at a net uptake of 528 billion tonnes of CO2 by plants by 2100, compared to the 238 billion tonnes of CO2 previously calculated by climate models.
According to Adam Riese, this is more than twice as much. By way of comparison: In scenario 2.6, a total of 1000 billion tonnes of CO2 (IPCC, Chapter 6, p. 468) will be emitted in this century. Today the plant world absorbs about 30% of the anthropogenic CO2 annually, the oceans another 24%.
In contrast, the statement of the IPCC in its last report from 2013 (p.26 of the Summary for Policymakers) is diametrically different: “Based on Earth system models, there is a high confidence that the feedback between climate development and the carbon cycle in the 21st century is positive. As a result, more of the anthropogenic CO2 emitted will remain in the atmosphere. Maybe I need to write to the German Bundestag again.


		jQuery(document).ready(function(){
			jQuery('#dd_702a1f6ffcce5ac21c82418cd887a235').on('change', function() {
			  jQuery('#amount_702a1f6ffcce5ac21c82418cd887a235').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
News from Antarctica: how’s the ice?
By Kalte Sonne
(German text translated/edited by P. Gosselin)
The ice in Antarctica, how is it doing? Is it melting, is it growing? In the following we wishto present the latest literature on the subject. There is a lot to report.
Fasten your seat belt, there’s a lot to cover.
Let’s start with the temperature development because along with snowfall, this is the most important control factor for Antarctic inland ice.
At NoTricksZone, Kirye shows ten coastal stations of Antarctica. None have been warming over the past 10 years. An example followws

And here’s the temperature development of the entire Antarctic according to UAH and RSS satellite measurements (from Climate4You, via NoTricksZone):



According to Clem et al. 2018, East Antarctica has cooled over the last 60 years, while West Antarctica has warmed. The authors establish a connection with the SAM ocean cycle, the Southern Annular Mode. Euan Mearns also deals with the temperature development of Antarctica during the last decades.
Increased ice
Based on height and gravity field measurements by satellite and GPS measurements on the ground, Martin-Español et al. 2017 determined an increase in ice mass in the East Antarctic and a reduction in ice mass in the (much smaller) West Antarctic for the interval 2003-2013. NASA researcher Jay Zwally also interprets an increase in the East Antarctic ice mass. However, a paper announced in mid-2018 still seems to be stuck in review…
Will the ice of East Antarctica be dragged along by the melting West Antarctic at some point in the melting vortex? No, this will not happen, Indiana University said in a press release of 2017:
New study validates East Antarctic ice sheet should remain stable even if western ice sheet melts
A new study from Indiana University-Purdue University Indianapolis validates that the central core of the East Antarctic ice sheet should remain stable even if the West Antarctic ice sheet melts. The study’s findings are significant, given that some predict the West Antarctic ice sheet could melt quickly due to global warming.
If the East Antarctic ice sheet, which is 10 times larger than the western ice sheet, melted completely, it would cause sea levels worldwide to rise almost 200 feet, according to Kathy Licht, an associate professor in the Department of Earth Sciences in the School of Science at IUPUI. Licht led a research team into the Transarctic Mountains in search of physical evidence that would verify whether a long-standing idea was still true: The East Antarctic ice sheet is stable.
The East Antarctic ice sheet has long been considered relatively stable because most of the ice sheet was thought to rest on bedrock above sea level, making it less susceptible to changes in climate. However, recent studies show widespread water beneath it and higher melt potential from impinging ocean water. The West Antarctic ice sheet is a marine-based ice sheet that is mostly grounded below sea level, which makes it much more susceptible to changes in sea level and variations in ocean temperature. „Some people have recently found that the East Antarctic ice sheet isn’t as stable as once thought, particularly near some parts of the coast,“ Licht said.
Recent studies have determined that the perimeter of the East Antarctic ice sheet is potentially more sensitive and that the ice may have retreated and advanced much more dynamically than was thought, Licht said. „We believed this was a good time to look to the interior of the ice sheet. We didn’t really know what had happened there,“ Licht said. The research team found the evidence confirming the stability of the East Antarctic ice sheet at an altitude of 6,200 feet, about 400 miles from the South Pole at the edge of what’s called the polar plateau, a flat, high surface of the ice sheet covering much of East Antarctica.
To understand how an ice sheet changes through time, a continuous historical record of those changes is needed, according to Licht. The team found layers of sediment and rocks that built up over time, recording the flow of the ice sheet and reflecting climate change. Finding that record was a challenge because glaciers moving on land tend to wipe out and cover up previous movements of the glacier, Licht said.
The big question the team wanted to answer was how sensitive the East Antarctic sheet might be to climate change. „There are models that predict that the interior of the East Antarctic ice sheet wouldn’t change very much, even if the West Antarctic ice sheet was taken away,“ Licht said. According to these models, even if the ice sheet’s perimeter retreats, its core remains stable. „It turns out that our data supports those models,“ she said. „It’s nice to have that validation.“
The team’s research findings are presented in a paper, “Middle to Late Pleistocene stability of the central East Antarctic Ice Sheet at the head of Law Glacier,” that was published today online in the journal Geology.  The research presented is in collaboration with Mike Kaplan, Gisela Winckler, Joerg Schaefer and Roseanne Schwartz at Lamont-Doherty Earth Observatory in New York.”
A Nature Editorial also dealt with the current growth of the East Antarctic ice in January 2018. Of course, the ice in this region has also been worse at times, so it continues to heat up. However, one would have to go back to the warm Pliocene (5.3-2.6 million years before today):
A history of instability
The East Antarctic ice sheet may be gaining mass in the current, warming climate. The palaeoclimate record shows, however, that it has retreated during previous episodes of prolonged warmth.
The phrase “at a glacial pace” once invoked a sense of slow and unchangeable movement, an almost imperceptible motion. But decades of remote sensing and seafloor observations have shown that glaciers and ice sheets can respond to disturbances much more dynamically than once thought. But as satellites captured the surges and retreat of Greenland’s maritime glaciers in the past decades the Antarctic ice sheets — east and west of the Trans-Antarctic mountains — were at least assumed to be stable. But this, too, turned out to be wrong. First came sediment1 and model2 evidence that the West Antarctic ice sheet collapsed during previous interglacial periods and under Pliocene warmth. Then came erosional data showing that several regions of the East Antarctic ice sheet also retreated and advanced throughout the Pliocene3. An extended record4 of ice-sheet extent from elsewhere on the East Antarctic coast now paints a more complicated picture of the sensitivity of this ice sheet to warming.”
Curiously enough, half a year later, the tide turned when a paper by Shakun et al. 2018, also in Nature, saw no major problems for the Antarctic ice in the Pliocene:
Minimal East Antarctic Ice Sheet retreat onto land during the past eight million years
The East Antarctic Ice Sheet (EAIS) is the largest potential contributor to sea-level rise. However, efforts to predict the future evolution of the EAIS are hindered by uncertainty in how it responded to past warm periods, for example, during the Pliocene epoch (5.3 to 2.6 million years ago), when atmospheric carbon dioxide concentrations were last higher than 400 parts per million. Geological evidence indicates that some marine-based portions of the EAIS and the West Antarctic Ice Sheet retreated during parts of the Pliocene1,2, but it remains unclear whether ice grounded above sea level also experienced retreat. This uncertainty persists because global sea-level estimates for the Pliocene have large uncertainties and cannot be used to rule out substantial terrestrial ice loss3, and also because direct geological evidence bearing on past ice retreat on land is lacking. Here we show that land-based sectors of the EAIS that drain into the Ross Sea have been stable throughout the past eight million years. We base this conclusion on the extremely low concentrations of cosmogenic 10Be and 26Al isotopes found in quartz sand extracted from a land-proximal marine sediment core. This sediment had been eroded from the continent, and its low levels of cosmogenic nuclides indicate that it experienced only minimal exposure to cosmic radiation, suggesting that the sediment source regions were covered in ice. These findings indicate that atmospheric warming during the past eight million years was insufficient to cause widespread or long-lasting meltback of the EAIS margin onto land. We suggest that variations in Antarctic ice volume in response to the range of global temperatures experienced over this period—up to 2–3 degrees Celsius above preindustrial temperatures4, corresponding to future scenarios involving carbon dioxide concentrations of between 400 and 500 parts per million—were instead driven mostly by the retreat of marine ice margins, in agreement with the latest models5,6.”
Also read more at cato.org.
Antarctica stable
Eight million years ago, the earth’s atmosphere had a similar CO2 content as today. Investigations now show that the Antarctic ice sheet had hardly retreated at that time. The ice is apparently more stable than expected. Click here for the press release from the National Science Foundation. You can also read an article in Popular Mechanics.
University of Edinburgh press release from 2017::
Central parts of Antarctica’s ice sheet have been stable for millions of years, from a time when conditions were considerably warmer than now, research suggests.
The study of mountains in West Antarctica will help scientists improve their predictions of how the region might respond to continuing climate change. Its findings could also show how ice loss might contribute to sea level rise.
Although the discovery demonstrates the long-term stability of some parts of Antarctica’s ice sheet, scientists remain concerned that ice at its coastline is vulnerable to rising temperatures. Researchers from the Universities of Edinburgh and Northumbria studied rocks on slopes of the Ellsworth Mountains, whose peaks protrude through the ice sheet. By mapping and analysing surface rocks — including measuring their exposure to cosmic rays — researchers calculated that the mountains have been shaped by an ice sheet over a million-year period, beginning in a climate some 20C warmer than at present.
The last time such climates existed in the mountains of Antarctica was 14 million years ago when vegetation grew in the mountains and beetles thrived. Antarctica’s climate at the time would be similar to that of modern day Patagonia or Greenland. This time marked the start of a period of cooling and the growth of a large ice sheet that extended offshore around the Antarctic continent. Glaciers have subsequently cut deep into the landscape, leaving a high-tide mark — known as a trimline — in the exposed peaks of the Ellsworth range.
The extended ice sheet cooled the oceans and atmosphere, helping form the world of today, researchers say. Their study is among the first to find evidence for this period in West Antarctica. The research, published in Earth and Planetary Science Letters, was done in collaboration with the Scottish Universities Environmental Research Centre. It was funded by the UK Natural Environment Research Council and supported by British Antarctic Survey.
Professor David Sugden, of the University of Edinburgh’s School of GeoSciences, said: „These findings help us understand how the Antarctic Ice Sheet has evolved, and to fine-tune our models and predict its future. The preservation of old rock surfaces is testimony to the stability of at least the central parts of the Antarctic Ice Sheet — but we are still very concerned over other parts of Antarctica amid climate change.“
As the ice in West Antarctica melts, it rises isostatically, which in turn stabilizes the overlying ice, found a research team from Denmark and Colorado.
Again and again there are the climate stories about the Totten Glacier in the East-Arctic Wilkesland. Gwyther et al. 2018 was able to show that the basal melting of the glacier is subject to strong natural fluctuations (press release of the NSIDC here). There is no long-term melting trend.
Melting from volcanoes
Glaciers in the western Ross Sea are also stable (Fountain et al. 2017, press release here). The rapidly melting Pine Island Glacier in West Antarctica has a hot secret that has now been revealed: Beneath the glacier lies a previously unknown volcanic heat source. University of Rhode Island press release from June 2018 (via EurekAlert!):
Researchers discover volcanic heat source under glacier


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Plays critical role in movement, melting
A researcher from the University of Rhode Island’s Graduate School of Oceanography and five other scientists have discovered an active volcanic heat source beneath the Pine Island Glacier in Antarctica. The discovery and other findings, which are critical to understanding the stability of the West Antarctic Ice Sheet, of which the Pine Island Glacier is a part, are published in the paper, „Evidence of an active volcanic heat source beneath the Pine Island Glacier,“ in the latest edition of Nature Communications.
Assistant Professor Brice Loose of Newport, a chemical oceanographer at GSO and the lead author, said the paper is based on research conducted during a major expedition in 2014 to Antarctica led by scientists from the United Kingdom. They worked aboard an icebreaker, the RRS James Clark Ross, from January to March, Antarctica’s summer. „We were looking to better understand the role of the ocean in melting the ice shelf,“ Loose said. „I was sampling the water for five different noble gases, including helium and xenon. I use these noble gases to trace ice melt as well as heat transport. Helium-3, the gas that indicates volcanism, is one of the suite of gases that we obtain from this tracing method. „We weren’t looking for volcanism, we were using these gases to trace other actions,“ he said. „When we first started seeing high concentrations of helium-3, we thought we had a cluster of bad or suspicious data.“
The West Antarctic Ice Sheet lies atop a major volcanic rift system, but there had been no evidence of current magmatic activity, the URI scientist said. The last such activity was 2,200 years ago, Loose said. And while volcanic heat can be traced to dormant volcanoes, what the scientists found at Pine Island was new. In the paper, Loose said that the volcanic rift system makes it difficult to measure heat flow to the West Antarctic Ice Sheet. „You can’t directly measure normal indicators of volcanism — heat and smoke — because the volcanic rift is below many kilometers of ice,“ Loose said
But as the team conducted its research, it found high quantities of an isotope of helium, which comes almost exclusively from mantle, Loose said. „When you find helium-3, it’s like a fingerprint for volcanism. We found that it is relatively abundant in the seawater at the Pine Island shelf. „The volcanic heat sources were found beneath the fastest moving and the fastest melting glacier in Antarctica, the Pine Island Glacier,“ Loose said. „It is losing mass the fastest.“ He said the amount of ice sliding into the ocean is measured in gigatons. A gigaton equals 1 billion metric tons.
However, Loose cautions, this does not imply that volcanism is the major source of mass loss from Pine Island. On the contrary, „there are several decades of research documenting the heat from ocean currents is destabilizing Pine Island Glacier, which in turn appears to be related to a change in the climatological winds around Antarctica,“ Loose said. Instead, this evidence of volcanism is a new factor to consider when monitoring the stability of the ice sheet.
The scientists report in the paper that „helium isotope and noble gas measurements provide geochemical evidence of sub-glacial meltwater production that is subsequently transported to the cavity of the Pine Island Ice Shelf.“ They say that heat energy released by the volcanoes and hydrothermal vents suggests that the heat source beneath Pine Island is about 25 times greater than the bulk of heat flux from an individual dormant volcano.
Professor Karen Heywood, from the University of East Anglia in Norwich, the United Kingdom, and chief scientist for the expedition, said: ‘The discovery of volcanoes beneath the Antarctic ice sheet means that there is an additional source of heat to melt the ice, lubricate its passage toward the sea, and add to the melting from warm ocean waters. It will be important to include this in our efforts to estimate whether the Antarctic ice sheet might become unstable and further increase sea level rise.’
Does that mean that global climate change is not a factor in the stability of the Pine Island Glacier? No, said Loose. ‘Climate change is causing the bulk of glacial melt that we observe, and this newly discovered source of heat is having an as-yet undetermined effect, because we do not know how this heat is distributed beneath the ice sheet.’
He said other studies have shown that melting caused by climate change is reducing the size and weight of the glacier, which reduces the pressure on the mantle, allowing greater heat from the volcanic source to escape and then warm the ocean water. ‘Predicting the rate of sea level rise is going to be a key role for science over the next 100 years, and we are doing that. We are monitoring and modeling these glaciers,’ Loose said.
The scientists conclude by writing: ‘The magnitude and the variations in the rate of the volcanic heat supplied to the Pine Island Glacier, either by internal magma migration, or by an increase in volcanism as a consequence of ice sheet thinning, may impact the future dynamics of the Pine Island Glacier, during the contemporary period of climate-driven glacial retreat.’
In addition to Heywood, Loose worked with Alberto C. Naveira Garabato, of the National Oceanography Centre at the University of Southampton, United Kingdom; Peter Schlosser of Arizona State University’s School of Earth and Space Exploration and the Lamont-Doherty Earth Observatory at Columbia University; William Jenkins of the Woods Hole Oceanographic Institution in Massachusetts; and David Vaughn of the British Antarctic Survey, Cambridge, United Kingdom.”
University of California in Santa Cruz 2015:
Study finds surprisingly high geothermal heating beneath West Antarctic Ice Sheet
UC Santa Cruz team reports first direct measurement of heat flow from deep within the Earth to the bottom of the West Antarctic ice sheet
Read more here.
Article at Spiegel.de 2017:
Researchers discover 91 volcanoes under the ice 
Surprise in Antarctica: hidden under kilometres of ice, researchers have found dozens of previously unknown volcanoes. Eruptions threaten a strong melt – sea levels could rise.”
Read more at Spiegel.de (press release from the University of Edinburgh here).
The West Antarctic Kamb Ice Stream has always puzzled the researchers because here the ice thickened, in contrast to the general melting trend in West Antarctica. What could be the cause? Another volcano, as reported by the University of Washington in 2018: University of Washington 2018:
Volcano under ice sheet suggests thickening of West Antarctic ice is short-term
A region of West Antarctica is behaving differently from most of the continent’s ice: A large patch of ice there is thickening, unlike other parts of West Antarctica that are losing ice. Whether this thickening trend will continue affects the overall amount that melting or collapsing glaciers could raise the level of the world’s oceans.
A study led by the University of Washington has discovered a new clue to this region’s behavior: A volcano under the ice sheet has left an almost 6,000-year record of the glacier’s motion. The track hidden in the middle of the ice sheet suggests that the current thickening is just a short-term feature that may not affect the glacier over the long term. It also suggests that similar clues to the past may be hiding deep inside the ice sheet itself. ‘What’s exciting about this study is that we show how the structure of the ice sheet acts as a powerful record of what has happened in the past,’ said Nicholas Holschuh, a UW postdoctoral researcher in Earth and space sciences. He is first author of the paper published Sept. 4 in The Cryosphere.
The data come from the ice above Mount Resnik, a 1.6-kilometer (mile-high) inactive volcano that currently sits under 300 meters (0.19 miles) of ice. The volcano lies just upstream of the thickening Kamb Ice Stream, part of a dynamic coastal region of ice that drains into Antarctica’s Ross Sea. Studies show Kamb Ice Stream has flowed quickly in the past but stalled more than a century ago, leaving the region’s ice to drain via the four other major ice streams, a switch that glaciologists think happens every few hundred years. Meanwhile the ice inland of Kamb Ice Stream is beginning to bulge, and it is unclear what will happen next. ‘The shutdown of Kamb Ice Stream started long before the satellite era,’ Holschuh said. ‘We need some longer-term indicators for its behavior to understand how important this shutdown is for the future of the region’s ice.’
The paper analyzes two radar surveys of the area’s ice. One was collected in 2002 by co-authors Robert Jacobel and Brian Welch, using the ice-penetrating radar system at St. Olaf College in Minnesota, and the other in 2004 by co-author Howard Conway, a UW research professor of Earth and space sciences. Conway noticed the missing layers and asked his colleagues to investigate. “It wasn’t until we had spent probably six months with this data set that we started to piece together the fact that this thing that we could see within the ice sheet was forming in response to the subglacial volcano,” Holschuh said.
The study shows that the mysterious feature originates at the ice covering Mount Resnik. The authors believe that the volcano’s height pushes the relatively thin ice sheet up so much that it changes the local wind fields, and affects depositing of snow. So as the ice sheet passes over the volcano a section missed out on a few annual layers of snow. “These missing layers are common in East Antarctica, where there is less precipitation and strong winds can strip away the surface snow,” Holschuh said. “But this is really one of the first times we’ve seen these missing layers in West Antarctica. It’s also the first time an unconformity has been used to reconstruct ice sheet motion of the past.”
Over time, the glacial record shows that this feature followed a straight path toward the sea. During the 5,700-year record, the five major coastal ice streams are thought to have sped up and slowed down several times, as water on the base lubricates the glacier’s flow and then periodically gets diverted, stalling one of the ice streams. “Despite the fact that there are all these dramatic changes at the coast, the ice flowing in the interior was not really affected,” Holschuh said.
What the feature does show is that a change occurred a few thousand years ago. Previous UW research shows rapid retreat at the edge of the ice sheet until about 3,400 years ago, part of the recovery from the most recent ice age. The volcano track also shows a thinning of the ice at about this time. “It means that the interior of the ice sheet is responding to the large-scale climate forcing from the last glacial maximum to today,” Holschuh said. “So the long-timescale climatic forcing is very consistent between the interior and the coast, but the shorter-timescale processes are really apparent in the coastal record but aren’t visible in the interior.”
Holschuh cautions that this is only a single data point and needs confirmation from other observations. He is part of an international team of Antarctic scientists looking at combining the hundreds of radar scans of Antarctic and Greenland glaciers that were originally done to measure ice thickness. Those data may also contain unique details of the glacier’s internal structure that can be used to recreate the history of the ice sheet’s motion.
“These persistent tracers of historic ice flow are probably all over the place,” Holschuh said. “The more we can tease apart the stories of past motion told by the structure of the ice sheet, the more realistic we can be in our predictions of how it will respond to future climate change.” The research was funded by the National Science Foundation and NASA. The other co-author is Knut Christianson, a UW assistant professor of Earth and space sciences.
Blown soot apparently has no influence on the Antarctic glaciers in the McMurdo dry valleys, Khan et al. 2018 (press release).
Medley & Thomas 2019 documented an increase in snowfall in the Antarctic, which benefited the ice sheet (NASA press release here). The authors establish a connection with the SAM ocean cycle, the Southern Annular Mode. The University of Colorado in Boulder, however, blames the increase in snowfall on the ozone hole (press release, paper by Lenaerts et al. 2018).
Jenkins et al. 2018 pointed to decadal cycles in the melting of the West Antarctic ice sheet at the edge of the Amundsen Sea. The relationship between melting and ocean temperature is nonlinear:
West Antarctic Ice Sheet retreat in the Amundsen Sea driven by decadal oceanic variability
Mass loss from the Amundsen Sea sector of the West Antarctic Ice Sheet has increased in recent decades, suggestive of sustained ocean forcing or an ongoing, possibly unstable, response to a past climate anomaly. Lengthening satellite records appear to be incompatible with either process, however, revealing both periodic hiatuses in acceleration and intermittent episodes of thinning. Here we use ocean temperature, salinity, dissolved-oxygen and current measurements taken from 2000 to 2016 near the Dotson Ice Shelf to determine temporal changes in net basal melting. A decadal cycle dominates the ocean record, with melt changing by a factor of about four between cool and warm extremes via a nonlinear relationship with ocean temperature. A warm phase that peaked around 2009 coincided with ice-shelf thinning and retreat of the grounding line, which re-advanced during a post-2011 cool phase. These observations demonstrate how discontinuous ice retreat is linked with ocean variability, and that the strength and timing of decadal extremes is more influential than changes in the longer-term mean state. The nonlinear response of melting to temperature change heightens the sensitivity of Amundsen Sea ice shelves to such variability, possibly explaining the vulnerability of the ice sheet in that sector, where subsurface ocean temperatures are relatively high.
And here are even more temporally variable relationships. Wang et al. 2019: reported on temporally variable relationships of the surface ice mass balance in West Antarctica with the SAM cycle and ENSO:
A New 200‐Year Spatial Reconstruction of West Antarctic Surface Mass Balance
High‐spatial resolution surface mass balance (SMB) over the West Antarctic Ice Sheet (WAIS) spanning 1800–2010 is reconstructed by means of ice core records combined with the outputs of the European Centre for Medium‐Range Weather Forecasts “Interim” reanalysis (ERA‐Interim) and the latest polar version of the Regional Atmospheric Climate Model (RACMO2.3p2). The reconstruction reveals a significant negative trend (−1.9 ± 2.2 Gt/year·per decade) in the SMB over the entire WAIS during the nineteenth century, but a statistically significant positive trend of 5.4 ± 2.9 Gt/year·per decade between 1900 and 2010, in contrast to insignificant WAIS SMB changes during the twentieth century reported earlier. At regional scales, the Antarctic Peninsula and western WAIS show opposite SMB trends, with different signs in the nineteenth and twentieth centuries. The annual resolution reconstruction allows us to examine the relationships between SMB and large‐scale atmospheric oscillations. Although SMB over the Antarctic Peninsula and western WAIS correlates significantly with the Southern Annular Mode due to the influence of the Amundsen Sea Low, and El Niño/Southern Oscillation during 1800–2010, the significant correlations are temporally unstable, associated with the phase of Southern Annular Mode, El Niño/Southern Oscillation and the Pacific decadal oscillation. In addition, the two climate modes seem to contribute little to variability in SMB over the whole WAIS on decadal‐centennial time scales. This new reconstruction also serves to identify unreliable precipitation trends in ERA‐Interim and thus has potential for assessing the skill of other reanalyses or climate models to capture precipitation trends and variability.”

Share this...FacebookTwitter "
"
Share this...FacebookTwitterThough the media like to tell their audience that man-made climate change is leading to more extreme weather, the data don’t support it. In fact, one could easily argue that Japan’s climate is more agreeable today.
By Kirye in Tokyo
and Pierre Gosselin
No trend in long-term annual precipitation
Over the past 100 years, for example, annual precipitation has not trended in an particular direction over the long term, showing rather some cyclical attributes:

Data source: Japan Meteorological Agency (JMA). 
If anything, precipitation has been rather steady for the better part of the past 2 decades, and even resembles what was observed about 60 years ago, in the 1950s.
Note how the extremes in precipitation occurred in the 1970s and 1980s when most of the climate talk was about global cooling. But overall, there’s been no trend change in precipitation in Japan.
Typhoons trending downward modestly!
Typhoons forming, and those striking Japan, also show no worsening, as is otherwise often claimed by climate alarmists. What follows is a plot of typhoon landfalls for Japan and typhoons formed, since 1951:
Data: JMA here and here. 
The data suggest the number of typhoons forming and those striking Japan have declined modestly over the past 70 years, which is in line with the trends for global tropical storms. So there’s nothing alarming happening.
Japan sea level rise “no long-term trend”



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also nothing dramatic is happening with regards to Japan and sea level rise. This is the official conclusion of the JMA! Their site states:
A trend of sea level rise has been observed in Japanese coastal areas since the 1980s, but no long-term trend of rise is seen for the period from 1906 to 2018. Variations with 10- to 20-year periods (near-10-year variations) are seen for the period from 1906 to 2018.




 Time-series representation of annual mean sea level values (1906 – 2018) 
 The 1981 – 2010 average is used as the normal. 
Annual sea level anomaly time series (comma-separated value file: 3 KB)
The graph indicates annual mean sea level anomalies for each year averaged among the four tide gauge stations shown in the map on the left below for the period from 1906 to 1959, and among the four regions shown in the map on the right below for the period from 1960 onward. The solid blue line represents the five-year running mean of annual sea level anomalies averaged among the four stations, while the solid red line represents the corresponding value for the four regions. The dashed blue line represents the value at the four stations for the same period shown by the solid red line (from 1960 onward) for reference.”




Japan annual temps steady 80 years, before peculiar 1990 jump
Finally, we look at Japan’s mean annual temperature trend over the past 100 years. Though we see an overall rise – it had remained more or less steady for some 80 years, from 1918 to 1990. But suddenly in 1990, the mean temperature jumped to new plateau.

Data source: JMA. 
Perhaps this may have in part been due to a change over to electronic measurement systems and urban heat island effect, along with station siting. One thing can be ruled out: Any CO2 effect would not act so instantaneously.
Japan’s climate has not really worsened
Overall in terms of weather extremes, cold and storms, things in Japan have not gotten worse. In fact one could easily argue things have tamed just a bit.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMarch mean temperatures over Northern Europe showed an overall cooling trend and so with it a later start to spring, the data from the Japanese Meteorological Agency (JMA) show. 
By Kirye
and Pierre Gosselin
Europe saw a very mild winter – one of the mildest on record – and so people living here believe it’s warming and that every year spring is arriving earlier.
Yet mean the temperature data from the Japan Meteorological Agency (JMA) going back 2 decades and more tell us the opposite is in fact the case in northern Europe and elsewhere: March has been cooling.
First we look at the mean March temperatures at 14 stations across the United Kingdom.

Data Source: JMA
Most stations plotted above show a cooling trend for the month of March. Obviously CO2 is not the driving factor, rather likely it has to do with the North Atlantic oceanic cycles.
The story is the same across the Netherlands when we look at data going back to the time the Kyoto Protocol was adopted, in 1997.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA.
The northern European country of Finland shows no climate change, at least in march, over the past 30 years:

Data source: JMA. 
Moving to the Scandinavian country of Sweden, the home of teen activist Greta Thunberg, we see a trend that is more in line with United Kingdom, possibly being influenced by the North Atlantic cycles as well:

Data source: JMA. 
The six Swedish stations examined show cooling in March over the past 30 years.
Why everyone is still speaking about warming even though the planet is showing more a mixed bag remains a mystery. It probably has a lot to do with the global warming obsessed media.
Summary: Northern Europe early Spring (March) is not getting warmer, and is in fact cooling modestly, when we look at the untampered data from the JMA.
Plotted in this post were the stations for which the JMA had complete or almost complete data sets.


		jQuery(document).ready(function(){
			jQuery('#dd_1a2510940116542717cdb96031995e99').on('change', function() {
			  jQuery('#amount_1a2510940116542717cdb96031995e99').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Kirye 
and Pierre Gosselin
So what’s going on?
NASA and other government agencies keep telling us that the globe is warming and ice becoming more rare, yet when look out the window, things often appear to be going the opposite direction.
Rare cold, snow grip Greece
For example, the Greek Reporter here informed how a “rare spring snow” blanketed large parts of northern Greece. It reported: “Of course, Northern Greece is used to low temperatures and snow, but even for their standards, such an intense snowfall in April is rare.”
Moreover, the widely read Electroverse weather site here reported how southeast Europe had seen its “coldest April morning in a decade”, potentially causing widespread crop damage.
So why are such events happening when they aren’t supposed to be?
Altering: from cooling to warming
Today we look at the NASA data from two stations in Greece: Makedonia and Larissa, shown below:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




These two stations have data going back to 1892 and 1899 respectively.
As we know NASA has been busy over the last years adjusting and homogenizing data from stations around the world, again and again. Data which once stood for years, suddenly got altered – in many cases very substantially. Greece here is another example.
What follows are side-by-side plots of the two stations: V4 unadjusted and V4 adjusted:

The two GHCN V4 unadjusted mean annual temperature data plots of the respective stations clearly show a cooling trend.
So no wonder we are seeing “rare” snow and cold in April.
NASA’s claimed warming is statistically fabricated
But NASA altered the data for the two Greek stations and named the two new data sets “V4 adjusted”. Clearly the adjustments magically produce the warming they like to scream about.
The warming didn’t occur because the air warmed up, but rather because NASA changed the data. The warming is a fake.
Though lots of people, media and politicians are listening and believing it, the weather isn’t. Snow and cold continue to make their appearance, Greece and other places are showing.


		jQuery(document).ready(function(){
			jQuery('#dd_525a8a6412dd739f5bdb3ff44afa9033').on('change', function() {
			  jQuery('#amount_525a8a6412dd739f5bdb3ff44afa9033').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe COVID-19 pandemic has exposed how scientific dissent is not only being suppressed and marginalized in Germany when it comes to climate science, but with virology and public health.

The online German national daily Die Welt here writes, “Virologists and physicians fear for their freedom of expression in the Corona crisis.” Climate scientists are getting some company!
Scientific freedom “under threat”
Citing the results of a recent survey, to Die Welt reports “many experts believe that freedom of expression in science is under threat” and “virologists have begun to change their attitude towards the measures taken by the German government.”
As the economy reels from the stringent restrictions enacted by authorities across Europe, the discussion about how to respond to the spread of the virus has become bitter. Public health experts opposing the restrictions and lock downs have found themselves marginalized and attacked by the media, other virologists and most politicians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Shift: COVID-19 alarmism waning, frustration growing
Using a survey, “Researchers at the University Hospital Eppendorf in Hamburg (UKE), the Society for Virology (GfV) and the University of Tübingen have tried to determine the mood among experts,” Die Welt reports. “178 experts from the fields of virology, immunology, hygiene, internal medicine and intensive care medicine” were surveyed anonymously. The results show that “more than more than 70 percent of the participants support the distance rule of two metres and the prohibition of major events” but that the other restrictions were “far more controversial”.
One third feel freedom in science is “being threatened”
What’s surprising: “One third even see freedom of expression in science as being threatened” and today 63 percent think “it would be sensible to restore public and economic life”. Also: “social distancing” is apparently losing support, according to the survey.
Overall, the findings show that the pandemic skeptics are finally beginning to assert their views to the public.
Unfortunately this is not even close to happening in climate science, where in Germany the ultra-alarmists continue to control the message. That freedom to express science is under threat, “is probably what many climate scientists secretly think,” Die kalte Sonne site here commented.
Perhaps COVID-19 tells us how moods can change quickly once restrictions start eroding freedom and prosperity.
Share this...FacebookTwitter "
"

From the ""you’ve GOT to be freaking kidding me"" department:
Dell’s Virtual Plant a Tree for Me program into the computer game Second Life has many tech savvy people wondering if this represents a new low in Earth Day marketing tie-ins. It looks like in the rush to pander to green-ness, some Dell executives maybe didn’t think beyond the boardroom door.
You may wonder, too, after reading Dell’s invitation to its Earth Day Party at Dell Island in the Second Life game  where they say proudly “get your own tree sapling to plant in Second Life!”.
Yes that’s’ right, you can plant a virtual tree in a video game for Earth Day. And, Dell is only too happy to take a couple bucks from you in the process as well for their real tree planting program designed to assuage your guilt at using a computer that uses electricity.
You have to wonder just how hypocritically lazy some people might be to take this offer seriously, though with 5.7 million ""residents"" in the Second Life game, I suppose its hard to deny that this offer would have an impact.
Just how much electricity is used by PC’s in pursuing this pointless exploit in ""green-ness""? And with Dell soliciting and online Earth Day Party, that will tie up PC’s, routers, and Servers nationwide, using even more electricity. There’s no mention in Dell’s press release of the expected carbon footprint on this bogus promotion. Maybe Gore will fly in on his private jet to make a “virtual appearance” to preach to the faithful.
But since some people nowadays seem incapable of disconnecting themselves from the virtual world of gaming, it stands to reason that a virtual eco-delusional activity might very well appear valid to them.
Maybe next the researchers at Berkeley can tap into the seti@home background processing idea and instead of searching for intelligent life in radio-telescope signals, we could program our wasted CPU cycles to grow virtual trees on a screen-saver. It could boast onscreen counts of virtual carbon sequestered, and virtual O2 produced. I can smell the virtual fresh air already!
We’re doomed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea705a03b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

As the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish, a conclusion that can be derived from the recent work of Dias de Oliveira _et al_. (2015).   
  
The five-member Australian research team noted that “elevated CO2 and high temperature are climate change drivers that, when combined, are likely to have an interactive effect on biomass and grain yield,” leading to three possible outcomes: (1) a “reduced positive effect of elevated CO2,” (2) an “amelioration of the effect of high temperature, or (3) a “synergistic effect where high temperature increases the positive effect of elevated CO2.” They also note that the resultant response “may be influenced by [plant] genotypic differences.” In an effort to study these interactions and possibilities, Dias de Oliveira _et al_. designed a field experiment to determine the interactive effects of CO2 and temperature, as well as those of a third variable—drought—on two pairs of sister lines of wheat ( _Triticum aestivum_ L.) over the course of a growing season, where one of the contrasting pairs of wheat sister lines differed in tillering, or branching (free vs. reduced), while the other differed in early vigor (high vs. low). The experiment was conducted out-of-doors in Western Australia in poly-tunnels under all possible combinations of CO2 concentration (400 or 700 ppm), temperature (ambient or + 3°C above ambient daytime temperature), and water status (well-watered or terminal drought post anthesis). So what did it reveal?   




After presenting a very long list of findings, Dias de Oliveira _et al_. summarized their results as follows: (1) elevated CO2 “increased grain yield and aboveground biomass,” (2) terminal drought “reduced grain yield and aboveground biomass,” but elevated CO2 “was the key driver in the amelioration of [its negative] effects,” (3) “temperature did not have a major effect on ameliorating the effects of terminal drought,” and (4) although “the mechanisms by which [the CO2-induced] enhancements were brought about differed in each pair of sister lines,” there was “no difference in aboveground biomass or grain yield within each pair.” Thus, it would appear that the overall outcome the researchers observed in this study was one in which elevated CO2, acted alone, overpowered the negative effects of a debilitating environmental stress (drought). Consequently, as the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish. And that is good news worth celebrating!   
  
**Reference**   
  
Dias de Oliveira, E.A., Siddique, K.H.M., Bramley, H., Stefanova, K. and Palta, J.A. 2015. Response of wheat restricted-tillering and vigorous growth traits to variables of climate change. _Global Change Biology_ **21** : 857-873.


"
"

Dr. Gavin Schmidt, a lead researcher with NASA’s Goddard Institute for Space Studies (GISS) that does leading climate change studies, replied to one of my posts and made an assertion that the USHCN and GHCN stations and station data being discussed here in my blog are not used in validating climate models. This is surprising to me.
Here is the full correspondence:
Schmidt’s first post:
> Don’t let me get in the way of your efforts here, but please stop saying that “This data is in fact used in climate modeling to predict our climate future”.
>
> This is simply not so.
>
> You’ve downloaded the GISS model – perhaps you’d like to show me where these station data are used? You won’t be able to because they aren’t.
>
> Observational data at large scale (not individual stations) are used to evaluate the models after they’ve been run – but again generally only at the continental scale and above. The evaluation is not just with trends but
> with patterns of variability (El Nino responses, NAO etc.) and obviously, the better the data the more reliable the evaluation.
>
> Note that the climate model hindcasts for this area are around 0.5 over the 20th Century – significantly less than this individual station. Should this record therefore be shown to contaminated, it would actually improve our confidence in the models, not lessen it!
I responded to this on June 21st 2007 as follows:
> Gavin,
>
> I thank you for commenting on my blog, Watts Up with That? I’m honored
> that you would take the time. Rather than reply immediately, I thought
> I’d give some thought and research to my response, hence the delay. I
> also thought you’d appreciate a direct reply rather than a blog post.
>
> You wrote on the blog:
>
> “You’ve downloaded the GISS model – perhaps you’d like to show me where
> these station data are used? You won’t be able to because they aren’t.”
>
> I did some looking at a paper you authored, I found Schmidt et al 2006,
> from BAMS, which is also posted on your website:
> http://pubs.giss.nasa.gov/docs/2006/2006_Schmidt_etal_1.pdf
>
> You wrote on page 168 of the BAMS article:
>
> “We endeavor to compare the model simulations to as many suitable
> datasets as possible. … . Where useful gridded datasets exist of
> selected in situ data we use those.”
>
> After reading through your paper, I agree that you did not show any
> comparisons to GISS gridded data and I will withdraw any implication
> that you used GISS station data. However, I must say that I’m surprised
> to learn that GISS gridded data did not meet the standards of Schmidt et
> al 2006 of being either “useful” or “suitable”. Thank you for drawing
> this to my attention.
>
> However, later in the article, on page 176, you show comparisons of
> model output to CRU surface temperature data on two occasions:
>
> “Surface air temperatures (SATs; Fig. 17) show a general warm
> continental bias in comparison to the updated Climate Research Unit
> (CRU) data (Jones et al. 1999).
>
> Figure 23 on page 187 shows Taylor diagram comparisons among the
> selected models for the December-February (DJF) and June-July (JJA)
> extratropical NH CRU surface air temperature (SAT)”
>
> It is my understanding that CRU uses GHCN station data, which includes
> the USHCN sites discussed here in my blog. So, my answer to your
> question is that Figures 17 and 23 of Schmidt 2006 et al use the station
> data discussed here via the CRU gridded data. It has always been my
> understanding that adjusted GHCN and USHCN surface station data (also
> listed on the GISS webpage) including the ones I show plots of, is
> applied to a gridded data scheme for use in the computer models, such as
> model E. If I am in error in that assumption, I welcome you pointing out
> that error.
>
> If you felt that I was speaking of a specific station data being “used
> to predict our climate future” that of course is not my intent. If that
> was the case, I’ll revise the wording to make it clearer.
>
> Regarding your mention that “contamination of station data would improve
> your confidence in your model”, I must say that I’m a bit surprised at
> this. I’m not really in a position to dispute this yet, but would
> appreciate some additional clarification as why you are so certain of
> this without even seeing the impact of contaminated data. I surmise the
> opposite to be true, but I welcome further understanding.
>
> Again I thank you for your comments, and I welcome any correspondence or
> suggestions you may have.
>
> Best regards,
> Anthony Watts
Dr. Gavin Schmidt replied on June 22nd, 2007 with:
My comments stand. The station data are not used *in* climate models, and
they are not used to predict future climate. So yes, the sentence you have
is just wrong. I’m not sure how you could edit it to make it correct.
We compare the models to the gridded products that deal with individual
station problems as best they can. We have used the GISTEMP and CRU
products to do so. (Semantic note, ‘compare to’ is not the same as
‘include in’). For the specific station you have highlighted, the grid
point trends in the products (~0.5 deg – eveballing it, since I’m on
travel) are significantly less than the trend you show (2 deg or so).
Climate model results for the 20th C are similar (i.e. 0.5 deg). Thus
reductions of the trend at this station would actually improve the match
to the model – always being clear that you shouldn’t really compare
model grid boxes to individual stations…
If you are of the opinion that this station is contaminated, then you have
to admit that the process designed to remove artefacts in the GISS or CRU
products has in fact done so – (i.e. that grid box in the product does not
have a 2 deg/Century trend).
Improvements to that process and the data are always welcome, but do not
ascribe consequences to your project that clearly do not follow.
Gavin
*——————————————————————–*
| Gavin Schmidt NASA/Goddard Institute for Space Studies |
| 2880 Broadway |
| Tel: (212) xxx-xxxx New York, NY 10025 |
…| |
*——————————————————————–*
[email address and tel# removed by Anthony for privacy/spam purposes]
So one has to wonder.
If Dr. Schmidt’s point is only the observation that they do not reconcile their models with every individual station (as opposed to gridcell composites calculated by GISS and CRU), then there is no misunderstanding.
However, it is very clear that the NASA GISS and CRU (Climate Research Unit) use this station data in arriving at their gridcell values which are what is presumably used in testing the models.  From 53 USHCN site surveys done so far we know that a number of stations do not meet published WMO (World Meteorological or NOAA (National Oceanic and Atmospheric Administration published standards.
There is no evidence at present that NASA GISS or CRU have made any effort to verify quality control standards at these USHCN stations. Whether these quality control issues will have a significant impact on overall averages remains to be seen.  The only way to tell for certain is by examining individual stations though the site survey process as is being done on www.surfacestations.org and then doing an assessment of how pervasive the quality control problems are and what the potential impact of these problems may be.
But, any problems in individual USHCN stations will affect gridcell values. For non meteorologists, a gridcell is a box on a map that has been divided up into a x-y  lines and specific data applied to each box. This helps in computer modeling because with computer programs it is easier to divide into cells, then calculate and display. Below is an example  map that may help you visualize gridcells:

Whether it’s a big problem or a little problem remains to be seen, but it’s odd for Dr. Schmidt to pretend that it’s not a problem because they use the gridded version of the data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5f332f6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

From the “us” against “phlegm” department:
There’s a DailyMail article about the possibility of a revolutionary flu vaccine that could work against all strains of the Influenza A disease. This ‘holy grail’ of vaccines would work on everything from the annual ‘winter flu’ to the ‘bird flu’. The best part is that just a few vaccinations may provide complete immunity, unlike the annual boosters are current defenses require.

From the article:
“The new jabs would be grown in huge vats of bacterial ‘soup’, with just two pints of liquid providing 10,000 doses of vaccine. Current flu vaccines focus on two proteins on the surface of the virus. However, these constantly mutate in a bid to fool the immune system, making it impossible for vaccine manufacturers to keep up with the creation of each new strain. The universal vaccines focus on a different protein called M2, which has barely changed during the last 100 years.”
The brainchild of scientists at Cambridge biotech firm Acambis, working with Belgian researchers, the vaccine will be tested on humans for the first time in the next few months.
A similar universal flu vaccine, being developed by Swiss vaccine firm Cytos Biotechnology, could also be tested on people in 2007 – and the vaccines on the market in around five years.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9120cc2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

You know the old saying that “no two snowflakes are alike“? Well it may be possible for two snowflakes to be alike after all. There’s a fascinating article in LiveScience that details how this may be possible.
For anyone who studies probability, this seems reasonable, given that the article mentions that 10^24 snowflakes fall in any given year. The article also contains a photo gallery of fascinating snowflake pictures like the one shown above.
From the article: “A typical snow crystal weighs roughly one millionth of a gram. This means a cubic foot of snow can contain roughly one billion crystals … It is probably safe to say that the possible number of snow crystal shapes exceeds the estimated number of atoms in the known universe.”
Kenneth Libbrecht, a professor of physics at California Institute of Technology runs a website devoted entirely to Snow Crystals at www.snowcrystals.com which is also visually impressive.

Here’s an interesting graphic on the formation of Snow Crystals:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8c39c76',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterLast Thursday evening in Münster, Germany, amid an atmosphere of loudly protesting students and Extinction Rebellion activists outside shouting obscenities and beating drums, prominent SPD social democrat and climate science critic Prof. Fritz Vahrenholt spoke on why Germany was headed down the wrong path with its now flailing transition to green energies, dubbed “Energiewende“.

Prof. Fritz Vahrenholt. Image: GWPF. 
Vahrenholt called the Energiewende: “An impending disaster.”
According to the Westfälische Nachricten here, “Scientists for Future activists handed out leaflets to emphasize that in their opinion the climate models of the IPCC (‘Intergovernmental Panel on Climate Change’) accurately depicted climate warming and that only trace gas CO2 was responsible for it.”
Hat-tip: Die kalte Sonne


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But Vahrenholt, former environment senator of Hamburg, refuted the claims and showed why he thought CO2 is only half responsible for climate change today and that the rest was due to natural factors like sun and clouds.
In his 45-minute presentation, Vahrenholt showed those in attendance how Germany’s foray into green energies was doomed to fail. As leaders in Germany continue to insist wind energy is able to supply the country’s energy needs, Vahrenholt – an environmentalist and one of the founders of Germany’s modern environmental movement – pointed out the major technical obstacle: the inability to store wind energy for periods of low wind.
“Not even in the grid, like one well-known Green politician claimed,” said Vahrenholt, taking a shot at Green party leader Annalena Baerbock, who once famously claimed the power grid could store energy.
German electricity prices among world’s highest
Vahrenholt also reminded that the Energiewende has made Germany’s electricity prices among the highest in the world and that it would hit the poor especially hard. “I never understood the SPD here,” said Vahrenholt, criticizing his own party. The retired professor said it would take 90,000 wind turbines to supply Germany with electricity, a number that would lead to the country having a turbine every 2 kilometers.
The Westfälische Nachrichten sums up on whether the Energiewende is going to work:
At the end of the complex, 45-minute presentation, the majority in the hall were probably convinced: it can’t. The facts and figures presented by the environmentalist were too overwhelming.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterScientists find three Arctic (Svalbard) lakes were all ~7°C warmer than they are now about 10,000 years ago – when CO2 concentrations were only 260 ppm.
According to a just-published Geophysical Research Letters paper (van der Bilt et al., 2019), not only were surface temperatures 7°C warmer than today in High Arctic Svalbard due to the accompanying “high radiative forcing” during the Early Holocene, but sea ice limits were well north of the study area back then too.
The authors point out that model “simulations neither reproduce this reconstructed pattern nor its magnitude.” This is presumably because the model simulations are predicated on the assumption CO2 concentrations are a primary climate driver.

Image Source: van der Bilt et al., 2019
This paper is yet another in a “growing body of recent work” that uses the prevalence of warmth-demanding (thermophilious) species present in Arctic locations to reconstruct regional temperatures based on a requisite warmth limit for the species’ survival.
Earlier this year Leopold et al., 2019 assessed temperatures were 5-8°C warmer than today in Arctic Svalbard 8 to 10 thousand years ago due to the presence of Mytilus spp, a warmth-loving mussel.

Image Source: Leopold et al., 2019
Share this...FacebookTwitter "
"
This picture below comes to me via surfacestations.org volunteer Kristen Byrnes, a 15 year old budding scientist that has created a bit of a stir with her critique of Al Gore’s Inconvenient Truth. Her website,”Ponder the Maunder” also has more photos of weather stations.
It is the USHCN Climate Station of Record for Lewiston, Maine, placed at the Union Water Power Company there.

It features an air conditioner unit, a portable barbecue grill, pavement and a nearby building. No close-by parking though as we’ve seen with other stations.
It also features a curious non-standard instrument shelter, of a design I’ve not seen before. The observing height appears to be non-standard, and lower to the ground than usual.

In addition to the close by hard surfaces like concrete pavement, the shelter also is located on an up-slope. That’s a no-no according to NOAA siting specs for a good reason – hot air rises.
Ms. Byrnes found another interesting station in Eastport, Maine. Ms. Byrnes found another interesting station in Eastport, Maine. While it is not part of the USHCN climatic network it is worth looking at because it shows how something simple and obvious that was missed can skew any experiment.
This station is a state operated, NOAA funded special monitoring station with high accuracy, very expensive laboratory grade sensors. The temperature sensor is aspirated, meaning it has a powered fan to draw air in from the outside, and is considered the most accurate way to measure air temperature. The same temperature sensor is used in the US Climate Reference Network (USCRN) specs of which can be seen here and photos here.
The setup also has a portable electronics building to go with it, to house all the data logging and analysis electronics. All that electronics needs to be kept cool, so these building are fitted with an air conditioner.
But the scientists who placed the temperature sensor were apparently so transfixed on the goal, they didn’t notice the air conditioner for the electronics building:

Fortunately, the US Climate Reference Network sites I’ve seen are much better thought out than this station in Eastport Maine.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5298c60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Well I just finished Day1 at the conference at UCAR (University Corporation for Atmospheric Research) put together by Dr. Roger Pielke, and sponsored by the National Science Foundation titled: Detecting the Atmospheric Response to the Changing Face of the Earth: A Focus on Human-Caused Regional Climate Forcings, Land-Cover/Land-Use Change, and Data Monitoring.
The day started off bright and early with the shuttle to the NCAR headquarters, shown above. It’s a unique place, at over 6000 feet up right next to the “flatirons“. Once there, we learned that the conference had been moved to downtown Boulder (somebody forgot to tell the shuttle driver). So we had to wait for the shuttle to return. A new one arrived, and we piled in. Then we sat there and waited because others were coming. As we waited in the sun, someone remarked, “It’s getting hot in the van, open your window” to which I remarked “well, with all these windows, it’s a simple greenhouse experiment”. That brought a chuckle, then “no, really, open he window”. So 10 minutes later, we were on our way in a van that holds 12, we had 7.
The driver informed us he had two stops to make to pickup additional people. We added three at the first stop, and at the second stop, at the invitation of the driver (I don’t mind if you don’t ) we added 6 more people, for a total of 16, all crammed into a van that holds 12.  After that exercise I quipped: “well in addition to our earlier greenhouse experiment, now we are adding population growth in an urban setting” which drew a big laugh – inside joke for climate science, you had to be there.
At the conference we had a busy day, lots of papers on land use changes, urbanization studies, rainfall studies, and one statistical study which really caught my eye because I had lunch with the presenter and he gave me the real inside scoop on the “adjustments” process used to turn raw temperature data into “usable” data. More on that later.
I felt a bit out of place at first, because I’d been away from the scientific community for awhile, and this was the first presentation of this type (mine comes tomorrow) in about 25 years. So I was a bit nervous. That soon faded, as people whom I’ve never met saw my name tag, came up and introduced themselves, and said things like “I’ve been following your work, I’m really looking forward to seeing what you’ve found” “after what I’ve seen on your website, I’m beginning to think the surface temperature record is hopeless, and we should focus elsewhere”. So I started feeling a bit more confident. I didn’t see anybody packing rotten tomatoes, and everyone was very nice today, so I’m hoping for the best tomorrow.
Of course Roger Pelke Sr. was a most gracious host, as was his assistant, Dallas, and it was a comfortable and easy day thanks to their efforts.
Later I’ll have a short summary of some of the papers presented today.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4c6bd7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Who can forget the cartoon scene shown above where Wile E. Coyote has a little trouble controlling some tornados he seeds in yet another futile attempt to capture the elusive Road Runner?
About ten years ago I thought of creating a computer game where you could create and steer your own tornado, but then I quickly thought to myself “I’d instantly be laughed and ridiculed out of the TV weather business for making such a socially distasteful product” so the thought passed just as quickly as an F0 twister.
Today I’m wandering COMPUSA in LA and what do I see? A game called Tornado Jockey.
From the game’s website description: “Target different objects like: vehicles, baseball stadiums and drug stores. Steer your storm away from forces like ‘Ray Gun Trucks’ and ‘Radar Bombers’ that aim to kill your tornado. You’re at the helm of mother nature’s energy, so pick your path and have a little fun!”
Then they go on to add: * Percentage of proceeds are donated to the American Red Cross
Yeah…. surely that makes it far more… umm…socially correct?
Here’s a screenshot of what the game looks like in operation:


Some of the game features:
– In-game objects: dairy farms, gas stations, amusement parks, & more (what, no mobile homes?)
– Enemies like: ‘F-Killer rockets’, ‘Storm Chasers’, & ‘Ray Cannons’
– Educational facts for: funnels, tornado types, supercells, etc.
– Dynamic game-play, original settings, outstanding special effects!
There’s nothing like seeing “educational facts” while ripping apart an amusment park full of kids.
But the icing on the cake has to be the game summary, where a blonde “weather babe” does a live TV report and tallies up your damage score in dollars.

Please excuse me while I go practice some power hurling into a wastebasket.   Blllluurrrrch!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea85d9209',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterTemperature stations along the coast of the Antarctic Peninsula indicate “marked statistically signficant cooling” has occurred since 1991, with the Larsen Ice Shelf cooling at a rate of -1.1°C per decade.

Image Source: Bozkurt et al., 2020
Bozkurt et al., 2020
“Observed near-surface temperature trends indicate important contrasts between summer and autumn for the period 1991−2015. A notable summer cooling exists on the northern peninsula (Frei and Marambio stations) and leeward side (Larsen Ice Shelf station). The largest summer cooling trend is observed at the Larsen Ice Shelf station [−0.92°C (10 yr)−1, p < 0.05]. On the other hand, in autumn, San Martin station on the central windward coasts exhibits the largest warming trend [+0.64°C (10 yr)−1 , p < 0.05]. Autumn warming is also notable at the other stations except the Larsen Ice Shelf station. At the annual time scale, there is a clear warming trend at San Martin station [+0.52°C (10 yr)−1 , p < 0.05], whereas at a close latitude on the leeward side the Larsen Ice Shelf station exhibits a marked statistically significant cooling [−1.1°C (10 yr)−1].”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEurope storm leads to negative electricity prices
By Die kalte Sonne
(Translated/edited by P. Gosselin)
It almost hurts a little that “specialist for renewable energies” (own claim on Twitter) Prof. Volker Quaschning gets mentioned here so often. This is simply due to the absurd tweets the man continuously puts out.
His latest prank has to do with the storm Sabine, which earlier this week swept across Germany. It supplied a lot of energy in the form of wind, which made the wind turbines rotate strongly.
Even in the otherwise regulated electricity market, the laws of the market cannot simply be levered out. Supply and demand determine the price. If supply is higher than demand, the price falls.
In the case of electricity, even money might be included with the product when this electricity is purchased, meaning negative prices. Electricity is an extremely perishable “commodity”, it must be consumed at the moment of production. However, the “expert” Quaschning does not blame this oversupply and the negative prices on the volatile wind power plants, but rather on nuclear power and coal. They deliver very reliably and not wildly fluctuating like wind power.
Prof. Volker Quaschning tweeted in response to the negative electricity prices from overproduction which Germany saw during the recent storm:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sturm und viel Wind sorgen wieder für negative Börsenstrompreise. Ein klares Zeichen, dass Kohle- und Atomkraftwerke zu unflexibel sind und nicht als Backup für erneuerbare Energien taugen. Wir brauchen darum einen schnelleren #Kohleausstieg. #FridaysForFuture #Scientists4Future pic.twitter.com/w7CRrEruoq
— Volker Quaschning (@VQuaschning) February 10, 2020

In English: “Storm and lots of wind are again causing negative stock market electricity prices. This is a clear sign that coal-fired and nuclear power plants are too inflexible and are not suitable as back-up for renewable energies. We therefore need a faster #coal exit.
#FridaysForFuture #Scientists4Future.”
A logical train of thought actually would have been to realize that highly volatile power sources such as wind and the simultaneous provision of base load are difficult to reconcile. Unfortunately, the energy source gas is also being massively fought by people like Quaschning, although it is more flexible and at the same time more CO2-friendly. In any case, however, it is only a crutch that might have to supply a great deal of energy, namely when we have the well-known lulls in wind and sun.
Every wind turbine and every photovoltaic system needs a backup. And anyone who has ever wondered why the wind countries of Denmark and Germany have such high electricity prices knows the reason. We are paying for a double power infrastructure. The prices will not decrease with an increasing share of renewable energies, but rather will continue to rise.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIt’s certainly no misperception that global warming alarmists have become far more shrill over the past months with ever wilder, more sensational claims about how the climate is getting more dangerous.
Making up facts
For example, in his COP25 opening speech, many of the claims made by UN Secretary General Antonio Guterres outright flunked the reality test, were exposed to be outlandish and seemed to be completely made up.
At Twitterm, Timjbo tweeted some Sky News footage of a statement made by the UN Secretary General, followed by a fact-check:
 Climate related disasters are becoming more frequent, more deadly, more destructive with growing human and financial cost.”

Many of Mr Guterres’ assertions in his opening speech #COP25 didn't stand up to scrutiny, including his claim a record number of people are dying due to climate-related disasters. @CraigKellyMP #Outsiders 
Not true, the #UN is after cash.
Full interview https://t.co/vCrHGqmOuY pic.twitter.com/vriSgQb2Ej
— Timjbo 🇦🇺 (@pleaseuseaussie) December 9, 2019

“Incredible 98% reduction” in climate deaths since 1930
The footage of Guterres is then followed by a Sky News panel analysis at Sky News, which featured MP Craig Kelly, who said that Guterres told three untruths in a single sentence.
First, Guterres claims a greater number of deaths from climate-related disasters, but this is untrue as the data chart show:

Lie no. 1: More climate-related deaths. Reality: 98% reduction since 1930!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Guterres is saying the exact opposite of what is true
Kelly shows that climate-related deaths are “at the lowest level in history”.
“There’s been a 98% decline, an incredible 98% decline since the 1930s,” said Kelly. “And this year we’ll get the lowest number of deaths from climate-related  disasters, ever!”
Kelly slams the UN Secretary General for saying the “exact opposite of what is true.”
Lie number 2: increasing number of climate related disasters
Also shown by Sky News is a chart depicting the number of reported natural disasters since 2000.

It’s a lie that climate-related disasters have increased.
Lies for cash = fraud
Here as well climate-related disasters have fallen steadily since 2000.
Kelly believes that money is behind all the falsehoods. “The UN is after cash,” Kelly says. He adds: “Now when someone makes false and misleading statements asking for cash, that is called fraud.”
Lie no. 3: “The costs of disasters is increasing.”
The third lie in the Secretary General’s whopper statement was saying the costs of disasters was rising. But here as well the data tell the opposite story:

Watch the whole Sky News segment here.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMichel Gay at the French language Contrepoints here writes that “EU Commission President Ursula von der Leyen’s green madness is the best way to torpedo Europe”.

EU Commission President Ursula von der Leyen is all smiles as she proposes 1 trillion euros for “Green New Deal”. Photo: Bundesregierung.
1000 billion euros
The new EU President of the European Commission said she wanted to present a plan called The Green Deal that would cost 1000 billion euros over the next decade in order reduce the European Union’s (EU) greenhouse gas emissions by 55% by 2030 compared to 1990.
According to Ursula Von der Leyen, “The European Green Deal is Europe’s new growth strategy. It will reduce emissions while creating jobs and improving our quality of life. To do that, we need investments…. To do that, we will come up with a plan.”
Von der Leyen also announced a European climate law slated March 2020 which would make the transition to carbon neutrality… “irreversible”.
She added: “If some people talk about costs, we should always keep in mind the extra costs if we do not act now.”
Up to 300 billion euros per year
Frans Timmermans, Executive Vice-President of this Commission in charge of the Green Deal, told a press conference in Brussels that these investments would be “a combination of public money, loans and private money” estimated to run as high and 300 billion euros per year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Public services will be hit hard
But Contrepoints writes that no matter wehere the money comes from, it will have a “siphoning effect” and thus “will be missing elsewhere where it might be more useful (health, public services, national security…).”
The EU’s new leadership is calling on Europe to be climate-neutral by 2050.
Taxpayers will pick up the tab
As to who will pay, Contrepoints says “in the end taxpayers and consumers will be largely involved, as usual.” Then, everyone will benevolently ask themselves “Why are local services disappearing? Why are taxes going up? And why is my standard of living falling?”
Collusion capitalism
Contrepoints calls the new EU Green Deal “collusion capitalism”, which describes an economy where business success depends on close relations with representatives of power: governments, and various commissions.”
“The aim is to create a climate of favoritism, for example, when it comes to the allocation of permits, government subsidies or tax cuts. It appears when cronyism, including ideological cronyism, pervades politics and elected officials,” comments Contrepoints. “It leads to collusion between elected officials and market players, in particular to win public contracts, obtain subsidies and guide legislation. Some of these systems are formalized and dominate an entire economy, but they are generally more subtle.”
Contrepoints writes that with 1000 billion euros to distribute, it will be necessary “to have to have friends at the European Commission to prosper in the shadow of all this public money.”
And due to the high costs and burdens, Contrepoints warns that Europe, with Germany now leading the charge, “wants to accelerate into a dead end!”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterElsevier has accepted a new paper by Lüdecke et al, 2020, showing natural oceanic and solar cycles play a large role in modulating Europe’s climate. Offers new chances for robust midterm temperature prognoses. 
The paper, in press, journal pre-proof, analyzes natural variability in European monthly temperatures on decadal and multidecadal timescales and their possible drivers.
NAO, AMO, sun behind temperature variability 
The authors claim to have established characteristic correlations of temperature with ocean cycles, here NAO and AMO, and solar activity for many regions and seasons. This means it is likely that NAO, AMO and solar activity are the actual drivers of a lot of the temperature variability.
Figure 1 of the soon-to-be-published paper follows:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image: Decadal and multidecadal natural variability in European temperature, Lüdecke et al, Journal of Atmospheric and Solar-Terrestrial Physics (journal pre-proof).
The authors did NOT look at the anthropogenic component of the long term warming of the past 150 years and its attribution but feel the results will hopefully help to better attribute shorter-term temperature changes and their typical patterns.
Chance for midterm temperature prognoses
This is important because efforts by the scientific community are progressing to better predict NAO and AMO for months and a few years in advance, This opens up new chances for more robust mid term temperature prognoses that also includes natural climate variability which the  paper documents.
The paper’s preliminary abstract (emphasis added):




European monthly temperatures undergo strong fluctuations from one year to the other. The variability is controlled by natural processes such as Atlantic cycles, changes in solar activity, volcanic eruptions, unforced internal atmospheric variability, as well as anthropogenic factors. This contribution investigates the role of key natural drivers for European temperature variability, namely the Atlantic Multidecadal Oscillation (AMO), the North Atlantic Oscillation (NAO) and solar activity changes. We calculated Pearson correlation coefficients r for AMO, NAO and sunspots compared to monthly temperature data of 39 European countries for the period 1901–2015 in order to search for ‘fingerprints’ of the natural drivers. A cross correlation window of 11 months was chosen for AMO, NAO and of 120 months for SILSO to account for possible time lags or phase shifts. The r coefficients were mapped out across Europe on a monthly basis to document regional and seasonal changes of the correlation strength. The NAO dominates European temperature variability during the winter months, with strongest relationship in February. The AMO modulates temperatures in March to November, with best correlations occurring in summer, but also in April. Regions of strongest AMO and NAO impacts shift across the continent from month to month, forming systematic patterns. Direct correlation of the solar 11-year Schwabe cycle with temperatures was identified only in some countries in certain multidecadal intervals during February, March, June and September. Previous studies have suggested a significant solar influence on the AMO and NAO. It is likely that the greatest impact of solar activity on European temperatures is of a non-linear, indirect nature by way of interaction with Atlantic cycles.”






The paper’s graphs show an oscillation of temperature that challenge the sharply rising GISS-like narrative for Europe.


		jQuery(document).ready(function(){
			jQuery('#dd_d64e9e9adc2a2d68f814d56480aff4dc').on('change', function() {
			  jQuery('#amount_d64e9e9adc2a2d68f814d56480aff4dc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"

The good news is that the coming years hold out real promise for a new wave of market‐​oriented regulatory reforms here in the United States. Momentum on this front crested in the late 1970s and early ’80s and has been all but defunct for the past couple of decades, but that may well be about to change. The bad news is why: the reason we should be getting our political hopes up is that our economic hopes over the near to medium term are so likely to be dashed.



Let me spell out that bad news a bit. U.S. economic performance in the wake of the Great Recession of 2008-09 has been nothing short of dismal. More than three years passed before real output even returned to its prerecession level, and a return to the prerecession growth trend remains nowhere in sight. The story with employment is even worse: yes, the unemployment rate has gradually subsided, but only because the percentage of Americans actually in the work force has sunk to its lowest level since the late 1970s.



Debate still rages over how much of the economy’s continuing sluggishness reflects short‐​term, cyclical factors — in particular, a shortfall in aggregate demand or deleveraging in response to the financial crisis of 2008. It is becoming increasingly clear, however, that slower growth in output and a weak labor market are now the “new normal.” In other words, the ongoing economic slump is not just a matter of a temporary gap between current and “potential” output. Rather, the economy has suffered a decline in its potential or full‐​employment growth rate.



Consider the long‐​term trends for each of the four major components of economic growth: growth in labor participation, growth in labor skills, growth in investment, and growth in output‐​enhancing innovation. As I explained in my 2013 Cato paper “Why Growth Is Getting Harder,” those trends are now uniformly unfavorable. Average hours worked per capita have fallen since 2000. Growth in so‐​called labor quality (as measured by years of school completed) has slowed considerably. The net domestic investment rate has been trending downward for decades. And total factor productivity (TFP) growth, our best measure of innovation, has slumped again in recent years after an Internet‐​fueled surge between 1996 and 2004.



Consequently, there are strong reasons for believing that growth in the years ahead will fall well short of the long‐​term historical average. Between 1870 and 2010, growth in real (i.e., inflation‐​adjusted) GDP per capita averaged just under 2 percent a year. By contrast, recent long‐​term growth projections by top academic and government economists point to an average annual per capita growth rate in the range of 1.0 to 1.5 percent — a fairly dramatic decline from the historical trend line.



In a welcome bit of irony, such economic pessimism offers solid grounds for political optimism. Here’s the basic logic: there is an inverse relationship between the external conditions for growth, on the one hand, and the incentives for good economic policymaking on the other. When conditions for robust growth are favorable, politicians can indulge in the characteristic vices of their profession — a time horizon bounded by the next election cycle and an overriding focus on dividing the pie rather than making it bigger — and still preside over a thriving economy. When, however, times get tougher, politicians must up their game or else economic performance will suffer. In the latter event, the poll numbers of incumbents start to drop, those of their challengers start to rise, and thus opportunities for policy change improve.



Now, policy change could well proceed in the wrong direction and produce worse results than the status quo. But over the past several decades at least, the general pattern around the world — as documented in the annual _Economic Freedom of the World_ reports and similar sources — has been for economic policies to move toward less government control and greater reliance on market competition. And usually, progress in liberalization has been spurred by disappointing economic performance.



If this general pattern holds in the present case, then a protracted period of sluggish growth should open a window of opportunity for pro‐​market, pro‐​growth reforms here in the United States. In sunnier times, many bad policies widely understood to create obstacles for growth are left undisturbed because the political price to be paid for changing them doesn’t seem worth it: why borrow trouble by attacking policies with powerful defenders if things are going OK anyway? But when the economic climate worsens, politicians come under increasing pressure to do something.



 **OPPORTUNITY FOR REFORM**  
Here then is the challenge for supporters of free markets: how can we best take advantage of this window of opportunity? During good times, we are forced to argue that, even if the overall economy seems to be performing well, it could be doing even better with appropriate policy reforms. Now, by contrast, we can reframe our case with a much more compelling sense of urgency: if we do not make difficult but necessary reforms, the economy will perform much worse than in the past. Since people are typically loss averse (they are more concerned about losing what they have than not getting what they want), the growth slowdown makes our case much stronger and more persuasive. But once we reframe our argument, what policy agenda best fits the new frame?



There is no shortage of possible reforms to include. For evidence I refer you to _Reviving Economic Growth_, a new Cato ebook that I edited. The book is a collection of essays by 51 prominent economists and policy experts, all of whom were asked to offer suggestions for improving the U.S. economy’s long‐​term growth outlook. Although a number of reform ideas come up repeatedly, what is striking about the collection is just how wide‐​ranging and varied the proposals are. Which should not be at all surprising: fiscal and regulatory policies affect the allocation of resources and the climate for innovation along countless different margins, and thus the potential levers for improving overall economic performance are similarly numerous and diverse.



In particular, the major economic policy debates that have dominated Washington and the nation’s attention in recent years — the trajectory and composition of federal spending, the level and structure of taxation, health care policy, regulation of the financial sector, what to do about illegal immigration, and climate change and environmental regulation more generally — have important implications for growth. Yet precisely because these debates have already found the spotlight, they are unlikely to supply policy ideas that can take advantage of the political opportunity that the current growth slowdown affords. On all of these hard‐​fought fronts, battle lines are already clearly drawn and often reflect differences over goals and priorities other than growth. Moreover, opinions are highly polarized along partisan and ideological lines, which in the current political environment is often a recipe for stalemate and gridlock.



What we should be looking for, then, are policy ideas that are not already the subject of high‐​profile, politically polarized debate. America’s growth slowdown is a new problem, and policy responses that address that problem are more likely to gain traction if they are not recycled ideas originally put forward to address other problems. And if a policy idea is already clearly associated with either the left or the right, in today’s highly contentious environment it is all but guaranteed that the other side will fight tooth and nail against it — which makes progress of any kind difficult in the absence of large congressional majorities and unified partisan control of the White House and Congress.



Meanwhile, of course, the items on this new policy agenda need to be effective remedies for slow growth. Since innovation is the ultimate source of long‐​term growth in advanced countries at the technological frontier, we should focus especially on policy reforms that can facilitate the introduction and spread of new ideas. In other words, we should target policy barriers that inhibit entrepreneurship and the reallocation of resources through competition and “creative destruction.”



 **REFORMING REGRESSIVE REGULATION**  
In a new Cato White Paper with the curious title of “Low‐​Hanging Fruit Guarded by Dragons,” I take all these factors into account and propose a pro‐​growth reform agenda that focuses on regulatory policies whose primary effect is to inflate the incomes and wealth of the rich, the powerful, and the well established by shielding them from market competition. To apply a convenient label, let’s call these policies “regressive regulation” — regulatory barriers to entry and competition that work to redistribute income and wealth up the socioeconomic scale.



In the paper I identify four main areas of regressive regulation: excessive monopoly privileges granted under copyright and patent law, restrictions on high‐​skilled immigration, protection of incumbent service providers under occupational licensing, and artificial scarcity created by land‐​use regulation. Space constraints prohibit an in‐​depth discussion of those policies here; for details I refer you to the paper. Here I will simply offer some general observations about why targeting these policies seems to me to be the most promising strategy for reversing the growth slowdown — and for taking advantage of the growth slowdown to revive political momentum for economic freedom.



At first blush, these four policy areas seem completely unrelated. They cover highly disparate subject matters, they are administered at different levels of government, and they feature widely varying forms of regulatory apparatus. Notwithstanding all these obvious differences, there are also deep and important similarities. All the policy areas feature regulations that erect explicit barriers to entry — whether in the economist’s sense of barriers to market entry, or in the literal sense of barriers to geographic entry. Copyright and patent laws and occupational licensing limit who can engage in particular kinds of commercial activity; immigration laws and zoning regulations limit who can enter or do business within a designated geographic area.



All of these entry barriers undermine economic growth by restricting vital inputs to innovation. Excessive copyright and patent protections restrict the recombination of ideas that is the essence of innovation by making some ideas artificially inaccessible. Immigration laws restrict the inflow of highly skilled individuals who are disproportionately entrepreneurial and innovative. Occupational licensing restricts the formation of new businesses, which are frequently the vessels for new products or new production methods. And zoning restricts urban density, a vital catalyst for the innovative recombination of ideas.



Finally, all these policy domains have similar distributional consequences: all of them redistribute income and wealth to the well‐​off and privileged. Copyright and patent laws pinch consumers for the benefit of huge corporations. Immigration laws expose America’s lowest‐​skilled workers to intensifying competition from foreign‐​born workers while shielding high‐​skilled workers from equivalent competitive pressures. Occupational licensing boosts the earnings of protected incumbents by restricting supply, especially in higher‐​income professions. And zoning gives windfall gains to wealthy landowners.



 **LEFT-LIBERTARIAN SYNTHESIS**  
In all likelihood because of these underlying similarities, none of these policy areas have become zones of ideological or partisan conflict. To be sure, proper policy is vigorously debated in all these areas, but the contending sides are not divided along left‐​right or Republican‐ Democratic lines. In striking contrast to the polarization and gridlock that now dominate most national policy debates, opposition to regressive regulatory controls has brought together politicians and policy experts from across the political spectrum. Thus, in the field of intellectual property, Nancy Pelosi (D-CA) joined forces with Darrell Issa (R-CA) and Ron Paul (R-TX) to oppose the Stop Online Piracy Act, a failed legislative effort to toughen criminal penalties for copyright violations. Among policy experts, leading critics of copyright and patent law excesses include progressives Lawrence Lessig and Dean Baker and libertarians Tom Bell and Jerry Brito.



With regard to high‐​skilled immigration, a number of bipartisan reform bills have been introduced in recent years. To take a recent example, in January 2015 a group of six senators, including Orrin Hatch (R-UT), Mark Warner (D-VA), and Marco Rubio (R-FL), introduced the Immigration Innovation Act to boost the numbers of both temporary and permanent visas for highly skilled workers. And among policy experts, scholars from the libertarian Cato Institute and the progressive Center for American Progress supported the most recent comprehensive immigration legislation passed by the Senate in 2013. As to occupational licensing, the Obama administration’s latest budget contains a provision to nudge states toward reform; furthermore, this past summer the administration released an excellent report that makes the case for deregulation in this area.



Meanwhile, in July 2014, Rep. Paul Ryan (RWI) released a widely discussed plan for combating poverty. And in the section on regulatory reform, Ryan singled out occupational licensing laws as prime examples of the “regressive regulations” that too often constrict economic opportunity for the least advantaged. Among policy experts, Alan Krueger of Princeton University, who served as chairman of the Council of Economic Advisers under President Obama, is a leading critic of these regulatory restrictions, while the libertarian Institute for Justice has a long track record of challenging and overturning licensing rules in court.



Zoning is a local issue that has long been thought to have only local consequences, so to date it has not attracted much attention from Washington policymakers. Among policy experts, though, the pattern of support for reform across ideological dividing lines holds here as well. Edward Glaeser, who in addition to teaching at Harvard is affiliated with the libertarian‐​leaning Manhattan Institute, is among the nation’s leading critics of current land‐​use regulation. Another prominent critic is Matthew Yglesias of Vox, who wrote his book, _The Rent Is Too Damn High_ , while he was working for the Center for American Progress.



 **PUBLIC INTEREST VERSUS VESTED INTERESTS**  
It’s not simply the case that one can find policy experts on both sides of the ideological spectrum who support reform of these regressive regulatory policies. More than that, it’s very difficult to find disinterested policy experts anywhere on the spectrum who support the status quo. Certainly, there are strong defenders of both intellectual property protection and zoning, but even in their ranks you will find recognition that current policies are seriously flawed. Thus, the economist Carl Shapiro, a prominent supporter of patents generally, has written, “[While] there is no doubt that the patent system taken as a whole plays an important role in spurring innovation, the general consensus is that the U.S. patent system is out of balance and can be substantially improved.” In similar fashion, the economist William Fischel, who has written sophisticated defenses of zoning, acknowledges that its exclusionary impact has increased since 1970 and that the “social and economic costs” of contemporary land use regulation are “not trivial.” As far as high‐​skilled immigration restrictions and occupational licensing are concerned, it is difficult to find any scholar who has anything nice to say about the current state of either.



This combination of qualities — negative impact on entrepreneurship and innovation, absence of political polarization, and an intellectual consensus in favor of reform — makes regressive regulation an especially inviting target for any campaign to enact pro‐​growth policy reforms. For all who are interested in better long‐​term U.S. economic performance, this is the “low‐​hanging fruit.” Reforming these policies is something that we know will make a positive difference, and by “we” I mean the vast bulk of disinterested experts. Yes, it is true that plucking this fruit won’t be easy, because the interest groups that benefit from the status quo are politically powerful, well organized, and highly motivated. This is the “guarded by dragons” part of the story. But knowing clearly what needs to be done, however difficult it might be, is an advantage that should not be underestimated.



Pursuing an agenda of curbing regressive regulation would allow us to open a new front in the economic policy debate. Unlike the all too‐ familiar policy disputes now ongoing, a campaign against regressive regulation would feature issues new to the national policy spotlight — especially in the case of occupational licensing and zoning, because they occur at the state and local levels and thus are typically ignored by Washington. Meanwhile, the organizing rubric of regressive regulation packages together disparate issues in a novel way and can thereby impart new energy to reform efforts in each of its constituent policy domains. This new front would look very different from the other, ongoing policy debates. Instead of the opposing forces being arrayed along the left‐​right axis, here the contest pits an expert consensus across the political spectrum against the interest groups who profit from existing policy. Instead of yet another left‐​right fight, this time the contest could be framed as a choice between the public interest and vested interests.



The idea of a left‐​right coalition to push deregulation may sound far‐​fetched, but it is not without precedent. Consider the country’s last major episode of pro‐​market regulatory reform in the late 1970s and early 1980s. During that brief period, price‐​and‐​entry regulation of airlines, trucking, and railroads was systematically dismantled; price controls on oil and natural gas were lifted; interest‐​rate caps for checking and savings accounts were removed; and the AT&T monopoly was ended, paving the way for competition in long‐​distance telephony. Those too young to remember can be forgiven for associating all of this with Ronald Reagan, but in fact Democrats and progressives played a major role. Jimmy Carter signed the legislation that deregulated airlines, trucking, railroads, and natural gas. On Capitol Hill, Edward Kennedy led the fight for airline deregulation, ably assisted by his aide Stephen Breyer. Yes, the rise of Chicago‐​school economics and especially the law‐​and‐​economics movement supplied momentum for these sweeping policy changes, but so did the activism of Ralph Nader.



History never repeats itself, but sometimes it rhymes. As in the 1970s, the U.S. economy today is delivering disappointing results. Back then the problem was “stagflation”; today we worry about a “great stagnation.” And once again, the shifting currents of political debate are bringing together unlikely allies with a common interest in reviving prosperity and a common hostility to the entrenched interests that stand in the way. With luck, contemporary reformers can follow their predecessors’ good example.
"
"

If you live near the ocean, chances are high that your home is built over sandy soil. For example many places in San Francisco are built on sandy soil or fill. Many homes built on this type of soil were badly damaged during the 1989 Loma Prieta earthquake.
When an earthquake strikes, deep and sandy soils can turn to liquid by a process known as liquefaction, with disastrous consequences for the buildings above. In an odd application of biotech, researchers at UC Davis have found a way to use bacteria to steady buildings against earthquakes by turning these sandy soils into rocks.
“Starting from a sand pile, you turn it back into sandstone,” the chief researcher explained. It is already possible to inject chemicals into the ground to reinforce it, but this technique can have toxic effects on soil and water. In contrast, the use of common bacteria to “cement” sands has no harmful effects on the environment. The new process, so far tested only at a laboratory scale, takes advantage of a natural soil bacterium, Bacillus pasteurii. The microbe causes calcite (calcium carbonate) to be deposited around sand grains, cementing them together.
So far this method is limited to labs and the researchers are working on scaling their technique.
Below: Before and After electron micrographs of microbiollogically-induced calcite  precipitation in which B. pasteurii cells are embedded.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7f4c617',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In his May encyclical, Pope Francis captured reader interest with an appeal to the deep sense of awe stemming from our attempts to comprehend complex natural processes of geology, biology, climatology, and others that comprise our ecosystem.



Unfortunately, interwoven with the science are familiar Malthusian ideological themes: excessive consumption by wealthy nations is responsible for climate change and the world’s poor are negatively and disproportionately affected’.





It is now within our power to export energy to poor, friendly nations and trading partners to help them keep their lights on, warm their homes, refrigerate their food and power their industry.



Not to worry though, because the encyclical follows up with matching ideological solutions. It implies that wealthy Western nations need to: quickly restrict and eventually eliminate all fossil fuel consumption, rush to develop renewable energy resources to power the Earth and clean up the environment, enable third world nations to eventually begin using renewables in their own grid, and, of course, foot the bill for all of it.



Part of the encyclical’s theme actually mirrors EPA’s recently published Clean Power Plan (CPP). Both documents attempt to foist expensive, unreliable, and unworkable “renewable” solar and wind power upon situations where it will not deliver for either wealthy nations or third world countries.



Pope Francis should realize we are truly blessed with our abundant energy resources. Just this year, the United States became the world’s top producer of oil and gas, and there is no end in sight to the vast energy supplies contained within our country and in other nations. Oil and natural gas production, particularly from the enormous U.S. shale‐​bearing basins, are at record‐​breaking levels and prices are continuing to drop — an enormous help to the poor, who spend an inordinate amount of their earnings on energy.



Now, for the first time in many years, we are taking more control of our energy and environmental future, and that of our allies. Supplies of oil and gas appear to be abundant worldwide. Thankfully, using cleaner natural gas in place of coal for power generation has cut carbon emissions by over 50 per cent, and that number is continuing to fall.



It follows that other countries are slowly gaining the extensive knowledge required to produce from shale, and some undeveloped countries can now see themselves participating in the “shale revolution” and producing sufficient energy to build their economies and address their national dreams and desires, just as we have done in the U.S. and in other Western nations.



As the Pope surely knows, the scriptures (Psalm 104:24 for example) teach that mankind is endowed with the wealth from the earth — from the rocks, surface waters, lifeforms, oceans, and atmosphere — and that we derive various forms and quantities of energy from each of these environments to sustain our lives and our progeny. It would appear that the papal encyclical’s discussion on man’s use of energy resources contains a major contradiction that must be addressed.
"
"

Mila Zinkova of San Francisco who took this picture of the setting sun on Dec. 29, 2006
You have probably heard something about green flashes, but may not have seen one. If so, you’ll be happy to find that a number of pictures of green flashes are available on the Web like the one above. The one pictured above is special because its a TRIPLE green flash which is exceedingly rare.  Its explanation lies in refraction of light (as in a prism) in the atmosphere and is enhanced by layered atmospheric inversions and possibly fog.
There was a time when green flashes were thought to be fables. Jules Verne, of all people, fixed them as real in his 1882 novel “Le Rayon Vert” (The Green Ray). He described “a green which no artist could ever obtain on his palette, a green of which neither the varied tints of vegetation nor the shades of the most limpid sea could ever produce the like! If there is a green in Paradise, it cannot be but of this shade, which most surely is the true green of Hope.”
Green flashes are real (not illusory) phenomena seen at sunrise and sunset, when some part of the Sun suddenly changes color (at sunset, from red or orange to green or blue). The word “flash” refers to the sudden appearance and brief duration of this green color, which usually lasts only a second or two.
For an explanation along with some great pictures of the atmospheric optics involved in green flashes and other sorts of colorful atmospheric phenonmena, I recommend this website in the UK: http://www.atoptics.co.uk/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea82e805f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter

War on windpark-blocking red kites?
Authorities are offering a €1000 reward for information leading to solving 11 cases of dead red kite protected birds. Nine of the deaths were due to a long-banned poison. 

Protected red kites being poisoned in north Germany. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
The German Presseportal.de here writes that a total of eleven dead red kites have been reported to the LLUR (State Office for Agriculture, Environment and Rural Areas) since 2017 from the area south of Neumünster in northern Germany.
“Nine of these rare birds of prey died of a banned insect venom,” the Presseportal.de reported. Now the Hunting Association of Schleswig-Holstein e.V. (LJV) is offering a reward of 1,000 euros (1,100 US dollars) for information leading to solving the cases.
“Banned poison”
Authorities say “the 9 red kites died from an insect poison which had been banned for many years” and that four dead red kites with suspected poisoning have been reported since March alone.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Three birds were found close together by a local hunter in the community of Rendswühren in the Plön district. The police departments Segeberg and Kiel have taken over the investigation,” according to the Presseportal.de.
More than half of the worldwide population lives on the territory of the Federal Republic of Germany, where wind energy proponents have been actively lobbying to build wind parks. Red kites and other protected species have ling been obstacles against the construction of wind parks.
A few days ago we reported here how one red kite had been found shot dead in an area where a wind park could be built under the condition that “no protected species be proven to exist there”.
1000-euro reward
In 2008, authorities and wildlife clubs jointly signed the Kiel Declaration on the Protection of Birds of Prey, under which the costs of examining dead birds of prey can be borne by the state,” the Presseportal reports. “Anyone who finds a dead bird of prey where the circumstances of discovery indicate an illegal act is requested to contact the LLUR (+49) 4347-704-0 or the Lower Nature Conservation Authority of the respective district.”
The Presseportal.de also informs that persons may also contact the Landesjagdverband Schleswig-Holstein e.V., which is offering a reward of 1000 euros for information that would lead to solving the cases. Evidence from citizens are accepted under (+49) 4551/884-0 (Police Bad Segeberg UVS) or (+49) 431/160-1503 (Police Kiel UVS).
Your contact at the Landesjagdverband Schleswig-Holstein e.V (State hunting association of Schleswig-Holstein e.V.) is
located at Böhnhusener Weg 6, 24220 Flintbek. Telephone: (+49) 4347-9087-0.


		jQuery(document).ready(function(){
			jQuery('#dd_25ae9455058c240ae34b8e284e82d5e9').on('change', function() {
			  jQuery('#amount_25ae9455058c240ae34b8e284e82d5e9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security, _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change._



 _As part of ourProject on Threat Inflation, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. The first of these, by Brendan Rittenhouse Green, appeared ___here__ _last week._



 _Paul R. Pillar, a n_ _on‐​resident senior fellow at the Center for Security Studies of Georgetown University, and a non‐​resident fellow of the Quincy Institute for Responsible Statecraft, provides his thoughts below. His reflections on hischapter are informed by his 28‐​year career in the U.S. intelligence community, and his voluminous writing and research, including his most recent book, _Why America Misunderstands the World: National Experience and Roots of Misperception _(Columbia University Press, 2016), whichhe discussed at Cato in late 2016. _



—–



Prevailing American thinking about substate threats—and more specifically the thinking that shapes U.S. policy—exhibits at least as much of a disconnect between perception and reality as when _A Dangerous World?_ was published six years ago. The policy players and their principal bugbears have changed, but broader patterns my earlier essay identified persist. Perhaps the most glaring demonstration of this persistence is the continued presence of U.S. troops in Afghanistan—more than eighteen years after the original intervention, in what has become America’s longest war. A major impediment to withdrawing those troops continues to be the notion of Afghanistan as a unique “safe haven” for terrorists who, because of that haven, are supposedly more likely than they otherwise would be to inflict harm on Americans. The result is an interminable military expedition that in important respects is doing more harm than good.



The evolution of international terrorism during the last six years has challenged other common but flawed thought patterns about terrorism. The biggest development in that evolution has been the rise and, as a territorial entity, fall of the Islamic State or ISIS. This group’s split from, and competition with, Al Qaeda underscore the error of the earlier tendency to treat violent Sunni radicalism as monolithic, with the accompanying habit of applying the label “Al Qaeda” to the whole phenomenon. ISIS’s history also further refutes the thinking about terrorist safe havens. When ISIS had its mini‐​state in Iraq and Syria, it was focused primarily on running and maintaining that entity and less focused on international terrorism than it has been when lacking such a territory.



The Trump administration appears to have centered its threat perceptions more on states than on substate phenomena. Nonetheless, its foreign policies demonstrate some of the patterns identified in the earlier essay, including the tendency to divide the perceived world simplistically into competing camps of good guys and bad guys. A prime example is the administration’s idea of a NATO‐​like security alliance in the Middle East that would unite the United States, Israel, and some favored Arab states against a presumed bad guys’ bloc led by Iran. Nonstate actors such as Lebanese Hezbollah, the Houthi movement in Yemen, and some militias in Iraq are placed in the bad guys’ camp because of their association with Iran. The idea hasn’t gotten anywhere partly because it does not correspond to the more complicated lines of conflict and competition in the Middle East.



The administration’s obsession with Iran also illustrates a corollary to a pattern the earlier essay identified regarding perceptions of revolutionary violence and regime change. The pattern is the habitual assumption that regime change in any state the United States currently considers a friend or ally is assumed to be a threat to the United States. The corollary is that any regime change in a state the United States considers an adversary is assumed to be good. Thus, the Trump administration presses on with its “maximum pressure” campaign against Iran, which, in the absence of feasible demands or constructive diplomacy, can only be aimed at collapse of the current Iranian regime. It presses on—and in so doing raises the risk of escalation to a wider war—oblivious to the likelihood that a replacement regime, such as a Revolutionary Guard dictatorship, would be even worse than what Iran has now.



Now the United States and the world are confronting a nonstate threat, in the form of the COVID-19 pandemic, that is inflicting death and damage orders of magnitude beyond what was ever inflicted by the substate actors that for years have been the focus of American threat perceptions. Unlike with, say, terrorism, there certainly has been no problem of previously prevailing threat perceptions exceeding the reality. With terrorism, more sober voices have had to point out that in most years more Americans drown in bathtubs than fall victim to terrorism. Even after an outlier event such as 9/11, the casualties have been many times fewer than, say, the number of Americans who die in traffic accidents. But in only a couple of months, COVID-19 has left bathtub drownings in the dust and has killed more Americans than a year’s worth of traffic deaths.



One pattern applicable to other nonstate threats that does apply to the current pandemic is the tendency—a characteristically American tendency—to overstate the newness of a threat. The novel coronavirus may be novel in terms of virology, but infectious disease epidemics certainly are not. Plagues go back to ancient times. A failure to think in such terms is one factor underlying the inadequacy of preparations to deal with the likes of COVID-19.



Some of the U.S. responses to COVID-19 can be attributed to Trump’s habits, such as the flagellation of China as a way to deflect blame and attention away from the administration’s performance. But a more general American tendency is in play as well. COVID-19 is a nonstate threat, but it also is a nonhuman threat. As such, it does not conform well with the way Americans habitually think of their _bêtes noires_. Americans have long looked for monsters to destroy, but they expect the monster to have a face, in the form of a loathed leader, regime, or substate group. They have difficulty thinking ahead about meeting faceless threats such as a disease or a changing climate.



This is one reason to temper silver‐​lining hopes that the pandemic will get people and their government to think more about threats that are most likely to kill them and less about foreign regimes or groups that are unlikely to do so. Just look at how the Trump administration has continued with its maximum pressure campaign against Iran. As thoughtful and expert observers on both sides of the Atlantic have observed, any nation’s inability to get the virus under control impedes efforts to contain the pandemic globally and thus threatens other nations’ citizens. A prudent step, therefore, would be to ease the U.S. sanctions that are impairing Iran’s ability to contain COVID-19. At a time when tens of thousands of American deaths ought to make control of the pandemic an overriding priority, the Trump administration ignores this advice.



- Paul Pillar, Washington, DC
"
"

Scott Brown’s stunning upset in the Massachusetts special election may have done what the best policy arguments could not – defeat the Democrats’ plans for a massive government takeover of the U.S. health care system.



Democrats will undoubtedly offer a variety of excuses for Brown’s win. The Democratic nominee, Attorney General Martha Coakley, was a poor candidate. The “political climate” was bad. The dog ate their ballots. But in reality, there can be no denying that this election was a clear cut rejection of the Democratic health care bills.



There were no blurred differences on this issue. Scott Brown made his opposition to the bill a centerpiece of his campaign. He promised to be the 41st vote to sustain a filibuster and kill the bill, even signing autographs as “Scott41.” Coakley, on the other hand, pledged to vote for the bill.





[T]here can be no denying that this election was a clear cut rejection of the Democratic health care bills.



The issue was featured in ads, debates, and public discussions. In the end, according to polls, in the home of Ted Kennedy, more than half of voters opposed the version of health care reform being rushed through Congress. Voters knew what they were saying. And what they were saying was a resounding “No!”



What do Democrats do now? House Speaker Nancy Pelosi says that they will pass health care reform “one way or another.” Those “ways” are:



 **Hurry up and stall:** New York Democratic congressman Anthony Wiener both named and defined this strategy. Democrats would slow‐​walk certification of Brown’s victory, preventing him from taking his seat in the Senate. Massachusetts Secretary of State William Galvin has hinted that he won’t certify election results for at least the 10 days that local officials have to report on absentee and overseas ballots and has noted that state election law gives him as long as 50 days beyond that. Meanwhile, Pelosi and Harry Reid will rush their negotiations to merge the House and Senate bills, allowing the appointed interim Massachusetts Senator Paul Kirk to vote on the bill before Brown takes his seat.



Democrats legitimately fear that such a blatant disregard for the democratic process would spark an enormous backlash. There would be no way to pretend it was anything other than the most corrupt power politics. After previous special elections the winners have been seated within days. In fact, when Kennedy first won the seat in a 1962 special election, he was sworn in the very next day. Would Democratic moderates, already frightened by the election outcome, be willing to go along with such an approach?



 **The House Surrenders:** Democrats could try to avoid a Senate vote altogether by having the House simply pass the already Senate approved measure. But that would require the House to accept the Senate bill with no changes at all. Pro‐​life Democrats like Bart Stupak (D‐​Mich.) would have to accept much weaker Senate restrictions on government funding of abortions. Liberals would have to accept the so‐​called “Cadillac tax” on high value insurance plans — without the exemption demanded by labor unions. And what about nervous moderate and blue dog Democrats? Are the Massachusetts results going to make them more or less likely to go out on a limb for health care reform? Remember, the House only passed their bill by a three vote margin, and one of those, Rep. Robert Wexler (D‐​Fl.) has since resigned his seat.



 **Let It Snowe:** Democrats could try to reach a compromise with Republican moderates like Olympia Snowe of Maine. Snowe did vote for a version of health reform in the Finance Committee and has spoken positively of the need for reform. But she also voted in favor of a resolution declaring that the individual mandate was unconstitutional and has raised a number of other objections. Any bill she agreed to would have to be substantially different than the ones currently being considered. That would almost certainly jeopardize already tenuous support from liberals. If the current Senate version is hard for them to swallow, just imagine how they will react to one watered down even further.



 **Go for 51:** The last, desperate gasp would be to use an arcane procedure known as reconciliation to pass health care reform with just 51 votes. But doing so would require Senate Democrats to overcome all manner of procedural hurdles. Reconciliation cannot be used for policy as opposed to budgetary issues. That means Democrats would have to drop some of their more popular proposals like the ban on preexisting conditions. They would be left with a bill that did little more than expand Medicaid and other subsidies, raise taxes, and cut Medicare. How popular would that be?



So far Democrats have been willing to do almost anything, cut any deal, sacrifice any principle, to force this bill through. But they may be running out of options at last. And for that, we can thank Scott Brown and the voters of Massachusetts.
"
"

Given this gadget matches my blog namesake, you’d think maybe I invented it. Alas, though I’ve made lots of other inventions, this is not one of them.
For those of you interested in sustainability or renewable energy, your first and best defense against power waste is to look for energy that is being wasted in normal everyday use. You’d be surprised at how many of our modern electronic devices that appear to be “off” are actually wasting power and you don’t even know it. TV’s, radios, game consoles, some computers, and many rechargeable devices waste a huge amount of power.
There’s two places this happens:
1. Instant on devices: TV’s and stereos are especially bad. The convenience of having the device turn on immediately causes it to operate in standby mode, drawing a small amount of power 24/7 Some PC’s also operate this way.
2. Devices with AC plug transformers. Often called “wall-warts” these small transformers convert the 120 volts AC to a safer 6-15 volts DC to power the electronic device. Even if the device is unplugged from the transformer, the transformer continues to waste power!
The Watts Up meter can help you identify and quantify where power is being wasted and how much it is costing you. You’d be surprised.

A simple solution to the problem of home energy waste is a $5 power strip. For example I have a TV set and satellite receiver on my workshop which I use to keep abreast of news while tinkering. If I left these two devices plugged in 24/7, they’d waste about $12/year in electricity. I plugged both of them into an inexpensive power strip, enabling me to separate them from the AC power source. Since I don’t use my workshop every day, the small inconvenience of waiting for the satellite reciever to initialize (about 1 minute) is well worth the money I’ll save over the years.
Chargers are another place this could work. Cell phone chargers, MP3 player chargers, etc can all be placed on a single power strip. When the devices are fully charged or disconnected, simply turn of the power strip to end power draw.
For more info on alternate energy, see the website I designed for the North State Renewable Energy Group at www.nsenergy.org


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9b53788',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter[Correction: James Taylor is Director of Heartland’s Arthur B. Robinson Center on Climate and Environmental Policy, and not the President of The Heartland Institute]
Today I’m writing about a public relations disaster here in Germany by the Heartland Institute where its president, James Taylor, allowed himself to be fully duped by leftist journalists posing as industry lobbyists offering half a million dollars to his Chicago-based think tank.

Image: Correctiv
Just days ago, flagship ZDF German public television broadcast a segment here — created by “investigative” site Correctiv here — at its popular weekly Frontal 21 magazine. Frontal 21 describes how the two “investigative” journalists from the leftist Correctiv infiltrated Heartland Institute and exposed its “lobbying and climate disinformation campaign”.
The ZDF’s Frontal 21 magazine shows how Correctiv set up a phony PR agency, fake website and printed phony business cards so that two of its reporters could pose as lobbyists representing the German automotive and coal industry. Their Project Veritas-like mission: to infiltrate Heartland and uncover “what goes on in the climate denier scene, how it works, which strategies it’s pursuing in Europe and how influential it is.”
“That’s how we want to find out whether and how to question climate change in exchange for money and to buy influence,” reported Frontal 21.
Unfortunately, James Taylor fell for the entire ploy. The ZDF’s Frontal 21 (and Corectiv) took the material and later pieced together a major hit piece seen nationally on February 4th. A blow to the skeptic scene in Germany.
First meeting in Munich
The two undercover German journalists started by making their way from their base in Berlin to the Munich Climate Conference last November, where they had an easy time fooling James Taylor.
Equipped with hidden cameras, they sat down with Taylor for dinner and asked how he went about convincing people of his mission. ZDF’s Frontal 21 quotes Taylor: “The people cannot be motivated by logical things; you have to argue emotionally.”
$500,000 offer in Madrid


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Frontal 21, after dinner Taylor then personally invited the two “German lobbyists” to Madrid for a later conference, where they met again with Taylor in the Marriott Hotel lobby.
ZDF Frontal 21 reported: “Taylor boasted about his good contacts to the Trump administration. ‘The Trump administration often asked for advice and about what we could do. We worked closely together.'” Taylor even told the undercover journalists that Heartland’s budget was $6 million.”
When asked how a $500,000 donation could be made, Frontal 21 says Taylor told them to donate it directly to Heartland. “And if you want to remain anonymous, then give it to an organization. One of them is Donors Trust.”
“To keep it hidden?” one undercover Correctiv journalist asked.”
“Yeah right, exactly,” replied Taylor, alleges Frontal 21.
Untypical for interviews, Taylor’s voice is completely filtered out by Correctiv editors, so it’s impossible to ascertain whether Frontal 21 translated Taylor’s words accurately.
In the Marriott lobby meeting, Taylor discloses Heartland’s future plans for Germany, where he also brings up young German influencer Naomi Seibt – the “anti-Greta”.
Duped by the mean and nasty media
Weeks later, Taylor sends the two undercover journalists his proposal dubbed “Funding Proposal: Germany Environmental Issues”, outlining Heartland’s strategy for Germany and that Heartland was interested in the funding in order to inform the public about the minimal effects of diesel exhaust and to produce videos on the negative impacts of excessive environmental regulation.
The set-up succeeded, and Correctiv got all the footage they needed, and much more, to allow German ZDF public television to weave a highly distorted, one-sided, yet believable hit piece. All in all, a major PR blow for climate skeptics in Germany.
A little research and precaution on Heartland’s part could have avoided being duped to the extent they were. They were sloppy. And we also have to feel bad that a young, promising German influencer got caught up in it and was so inaccurately portrayed on national television.
In her latest video, a visibly distraught Naomi tells of her experience of being observed by impostors under the table by hidden cameras, yet bravely pledges to keep up the fight for what she believes to be the truth. Now she understands just how mean and nasty the German media really are.

Share this...FacebookTwitter "
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented at_ _a Cato conference_ _the previous year. In each chapter, experts assessed and put in context the supposed dangers to American security, from nuclear proliferation and a rising China to terrorism and climate change._



 _As part of our_ _Project on Threat Inflation_ _, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. You can view previous entrieshere, here, and here and on the Project on Threat Inflation homepage._



 _This week’s entry comes from Christopher Fettweis, a professor of political science at Tulane University, and the President of the Board of the World Affairs Council of New Orleans._



In “Delusions of Danger,” I made the case that fear in the United States is out‐​of‐​proportion to the dangers it faces. In the time since I wrote, I have tried to explain why this is so, where that fear comes from and how the nation can be reassured. Then my country elected Donald Trump to be its president, and I was tempted to delete the whole project and join my cousins in the drywall business.



Donald Trump is a manifestation of American fear. He is the nightmarish personification of everything that scares us, from immigrants to crime to job loss, and he stokes those fears on a daily basis. Even the COVID-19 crisis has provided opportunities for him to remind us of dangers, this time those emanating from China, the WHO and (apparently) arms control treaties. Trump’s entire existence is defined by his enemies, who for his supporters become the enemies of the nation. Voices of reason are easily drowned out by the cacophony of madness. Fear will be the defining feature of American politics as long as we are led by Donald Trump.



The COVID-19 crisis had the potential to mitigate American fears. We could have used this opportunity to recognize our common humanity, to realize that we faced the same challenges and work together toward solutions. Empathy toward the plight of others, whether in China or Iran, could have grown. A better, more rational, less fearful world could have emerged from this ongoing tragedy. Unfortunately we are led by a man incapable of thinking in such directions.



Perhaps it is not, however, time for despair and drywall. Surely it is possible for U.S. foreign policy to emerge stronger and wiser after Trump leaves office. It may prove useful for this country to have its fundamental assumptions challenged, in order to examine its most sacrosanct beliefs and evaluate their value. Trump is the equivalent of what political scientists call a systemic shock, and there is no reason to believe that what follows him will be worse. Perhaps the wreckage he leaves behind can be reconstructed in new ways; perhaps the final chapter in the story of this most pathological of administrations will be a positive one, a happy ending in which the assumptions that drive our decisions can be re‐​thought. The first few post‐​Trump years will be crucial: Do we return to the same fearful delusions that led to war after war (and to Trump), or do we seek to improve U.S. policymaking? Can we learn from our hideous national mistake?



Cato’s Project on Threat Inflation could not have come at a better time. With it and the establishment of the Quincy Institute, momentum is building toward more rational foreign policy, and perhaps a better post‐​Trump world. I worry that restraint will be wrongly tainted with the Trump stench – an odor that will not wash off easily – but that is the subject for another essay. Much of what ails American foreign policy in the short term can be cured with one election. Long‐​term trends, however, will not change unless acted upon by a force. Such a force may be growing in DC policy circles, but whether it can overcome the power of fear remains to be seen.



-Christopher J. Fettweis
"
"

China’s born‐​again planners, led by President Jiang Zemin, are addicted to the idea that social and economic order areimpossible without the firm hand of the state. The Ninth Five‐​Year Plan (1996–2000) reconfirms the Chinese CommunistParty’s (CCP’s) faith in socialism and distrust of capitalism. The plan embraces the illusion that it is possible to revitalizestate‐​owned enterprises by a change in management rather than ownership. 



To think that China’s ruling elite can control an economy of 1.2 billion people is a fatal conceit. The failure of central planningand of communism throughout the world is testimony to the ill‐​fated desire to engage in social engineering. Indeed, recentstudies have shown that those countries that have fostered economic freedom have experienced greater wealth creation thanhave those that have failed to protect economic liberties. And as people have acquired greater economic freedom, they havedemanded other significant rights, including the right to participate in political life. That demand is exactly what worries China’shard-liners. 



Institutional innovation in China since 1978 has produced the fastest economic growth in the world and enlarged opportunitiesfor many people, but it has not eliminated the main obstacle to China’s future prosperity — the CCP. The party’s monopoly ofpower has been eroded by the market‐​based reforms of China’s paramount leader Deng Xiaoping, but with the Deng eraending, China is at a crossroads. The nation must decide whether to deepen reform and risk pressures for political change orslow reform and risk alienating China’s newly emerging middle class and fomenting social unrest. The dilemma is complicatedby the advent of democracy in Taiwan and the return of Hong Kong in 1997. 



**China at a Crossroads**



In less than two decades China has become one of the top 10 trading nations in the world. Over the past decade nearly $90billion in direct foreign investment has flowed into China’s emerging markets. If the Chinese economy continues to grow at anaverage annual rate of 9 percent, it could soon become the world’s largest. 



The challenge for China is to develop the hard institutional infrastructure of a market economy. The centerpiece of thatinfrastructure is a rule of law that protects property rights and limits the power of government. Therein lies the difficulty, for theCCP is unlikely to give up its power and let freedom reign. Nevertheless, there are internal and external forces at work thatmay push China in the direction of greater liberalization and democratization. Internally, China’s reformers have created aneconomic space that allows individuals the freedom to improve their living standards outside the state sector. Externally,China’s “open‐​door” policy has allowed foreign competition and know‐​how to help the nonstate sector grow. In the process,new business practices have evolved along with legal norms associated with a market‐​liberal order. 



The Ninth Five‐​Year Plan may slow China’s march toward the market, but it won’t stop it. The forces for change are toostrong. And the further the market advances, the more costly it will become for the CCP to try to reverse it. Yet it is clear fromChina’s saber rattling on the eve of Taiwan’s ascension to democratic rule that China’s authoritarian rulers are willing to incurconsiderable economic losses to protect their positions of power and privilege. That willingness is further evidenced by China’svow to crush any democratic movement in Hong Kong after 1997. 



The dilemma facing Western leaders is whether to contain China and risk military confrontation or to peacefully engage Chinawith the knowledge that the best way to foster human rights is by opening markets and cultivating exchanges — not by using theblunt instrument of economic sanctions or the rhetoric of China bashing. 



In evaluating their options, Western leaders should heed the lesson of Taiwan — a police state that turned into a free‐​marketdemocracy because economic liberalization led to political liberalization. Taiwan’s leaders were willing to experiment withinstitutional change and had the courage to let the people speak — in the market and in the polity. 



China, too, has created a new economic space but has resisted political change. Even so, there is reason to believe China mayfollow Taiwan’s “quiet revolution.” According to Lee Teng‐​hui, Taiwan’s first democratically elected president, “Vigorouseconomic development leads to independent thinking. People hope to be able to fully satisfy their free will and see their rightsfully protected. And then demand ensues for political reform.” Thus, he predicts, “The model of our quiet revolution willeventually take hold on the Chinese mainland.” 



**Deng’s Experiment**



The key to China’s progress has been its willingness to allow institutional change on a trial‐​and‐​error basis and to promotesuccess. Like Taiwan, China has reduced the relative size of the state sector by cultivating the nonstate sector, not byprivatizing large state enterprises. State‐​owned firms now account for only 25 percent of China’s total output (includingagriculture and services), and their share of industrial output has fallen from 80 percent in 1978 to 40 percent today. 



On the heels of the failure of central planning, Deng had no grand vision for institutional change, but he was willing toexperiment. His guiding principle was, “Once we are sure that something should be done, we should dare to experiment andbreak a new path.” 



Deng began to break a new path in 1978, when he launched his agricultural reform. Communal ownership of land wasabolished and a system of contractual relations was introduced through the “household responsibility system.” Rural familieswere allowed to hold long‐​term leases and acquired the right to use the land at their disposal. They could sell their crops in theopen market, provided they first satisfied the state quota. Under the new incentive structure, farmers increased production andbegan to invest their profits in town and village enterprises (TVEs), which are beyond the reach of state ministries. 



Although TVEs are legally owned by local governments, individual households are allowed to share profits, hold(nontransferable) shares, and receive dividends. Wages are tied to profits, and the managers of TVEs face hard budgetconstraints, unlike the politically motivated managers of state‐​owned enterprises (SOEs). As a result, TVEs have mushroomedwhile SOEs continue to whither on the vine of state subsidies. 



In addition to creating new ownership arrangements, Deng’s reforms decontrolled prices, opened China to the outside worldthrough trade liberalization and the establishment of Special Economic Zones, devolved power from the central government tolocal governments, and instituted a system of fiscal contracts that limited Beijing’s share of tax revenue and provided localofficials with an incentive to promote markets — a system Yingyi Qian and Barry Weingast have called “market‐​preservingfederalism.” Those institutional changes resulted in a parallel economic structure to compete with the SOEs, reduced thecentral government’s share of tax revenue from 60 percent in 1978 to 40 percent in 1993, and helped weaken the centralgovernment’s grip on everyday life. 



Those reforms, however, have failed to create a genuine market system founded on the principles of private ownership andfreedom of contract. The goal of China’s born‐​again planners is not market liberalism but market socialism. The resultant lackof clear rules at the enterprise level and attempts to plan the market are, in the absence of a constitution that protects propertyand contracts, reflections of what F. A. Hayek aptly called the “fatal conceit” of socialism. 



**Revitalizing Civil Society**



The quiet revolution that has been taking place in China’s economy since 1978 is combining with the information revolution tostrengthen the fabric of civil society and weaken the CCP. As China has expanded the freedom to earn a living outside thestate sector, individuals have gained greater control over their lives. In its 1994 report on human rights, the U.S. Department ofState noted the connection between economic rights and human rights: “A decade of rapid economic growth, spurred bymarket incentives and foreign investment, has reduced party and government control over the economy and permitted everlarger numbers of Chinese to have more control of their lives and livelihood.” 



People are learning how markets work by participating in the growing nonstate sector and by engaging in foreign trade. As themarket has replaced Marx, newly acquired ideas and wealth have given rise to a spirit of independence and to a rebirth of civilsociety, especially in China’s southern coastal provinces. Commenting on China’s cultural transformation, Jianying Zha writes inher book _China Pop_ , 



The economic reforms have created new opportunities, new dreams, and to some extent, a new atmosphereand new mindsets. The old control system has weakened in many areas, especially in the spheres of economyand lifestyle. There is a growing sense of increased space for personal freedom [so long as people stay out ofpolitics].



Anyone who has visited China and seen the vibrancy of the market, the dynamism of the people, and the rapid growth ofurban areas will concur with Zha’s cautious optimism. 



New towns and cities are evolving naturally as people flee the countryside for improved living conditions and the chance tostrike it rich in the nonstate sector. Villages that were once small fishing centers along the southern coast are now booming withthe flow of trade and people. The new urban centers, such as Shishi in the province of Fujian, are characterized by the market,not the plan. Their model of development, writes Kathy Chen of the _Wall Street Journal_ , is “small government, big society[ _xiao zhenfu, da shehui_ ] — which advocates less involvement by cash‐​strapped governments and more by society.” 



Ambitious young people want to become capitalists, not communists. A recent survey found that young people ranked beingan entrepreneur first among 16 job choices and employment with the national government eighth. Freer labor markets have ledto a growing demand among college students for business courses, and universities are responding. The CCP has lost much ofits credibility and is no longer the major route to success. 



The freedom to trade is an important human right in China. As trade expands, there will be a growing middle class with a largestake in China’s future. Moreover, China’s high savings rate gives all those who sacrifice current consumption and invest theirearnings in the nonstate sector a strong incentive to further depoliticize economic life. The formation of economic and civilsociety will lead to a natural call for greater participation in political life. Yet as long as the CCP stands in the way of thespontaneous market order, controls the flow of information, and prevents free association, the future of China’s civil societywill be in jeopardy. 



**Institutional Change and Democratization**



If democratization is to proceed in China, the government must continue to allow experimentation and new forms ofownership. Yuan Mu recently articulated the key role of ownership reform in the Beijing press: “We should discover the bestmodel for ownership by the whole people [notice the bias against privatization], so that they will genuinely become the mainbody of market competition and operate with vigor and vitality in accordance with the rules of the market economy.” 



Those rules will evolve as individuals grope for ways to lower the costs of exchange and expand markets. In _China Pop_ , Zhaquotes Liu Ge, a lawyer trained in both China and the United States, as saying, 



Gradually, there will be more laws and rules; the market will be more mature, more compatible withinternational standards, the competition more fair and open. Then, China will have been structurallytransformed! Political change will come after that.



According to Zha, “A lot of the educated urban Chinese … echo this way of thinking.” There is reason to believe, therefore,that institutional change in China will bring about what Princeton University professor Pei Minxin has called “creepingdemocratization.” 



Pei points to the upward mobility of ordinary people, occasioned by the deepening of market reform, and to the positiveimpact of China’s “open‐​door” policy on political norms. In his view, public opinion and knowledge of Western liberaltraditions, such as the rule of law, “have set implicit limits on the state’s use of power” and have promoted the democratizationof the legal system. People are starting to use the court system to contest government actions that affect their lives, liberty, andproperty. There has been a sharp rise in the number of civil lawsuits against the state, and individuals are beginning towin — perhaps as many as 20 percent of — their cases, according to official sources. 



The opening of the legal system is important because it paves the way for the transition from “rule by law” to “rule of law.“Marcus Brauchli of the _Wall Street Journal_ writes, 



The state’s steel‐​clad monopoly on the legal process, which makes the courts just another arm of government,is corroding. China’s economic liberalization … has spawned a parallel legal reform that raises the prospect ofrule of, not merely by, law.



Nevertheless, Brauchli recognizes that “legal ambiguity” remains “a ruthless weapon” for harassing the population. Until thatfacet of China’s institutional structure changes, no one’s rights will be secure. 



**China’s Future**



The challenge for China is to get out of the way of the market and let it grow naturally along with civil society. Doing so,however, requires an understanding of the institutional infrastructure that makes the market system tick and an appreciation ofthe spontaneous order that emerges when private property and freedom of contract are protected by a rule of law.Democracy is neither necessary nor sufficient for a market system — as the experience of Hong Kong has illustrated. What isnecessary is a stable legal framework that protects life, liberty, and property. If China is to prosper in the global economy, thenation will have to adopt common‐​law practices and abide by international commercial codes and customs. Old habits arehard to break, but the forces for change are strong, and there is reason to believe that China will “creep along in the rightdirection.” 



China has been willing to experiment, but it has not yet provided the climate of freedom necessary for growing market‐​liberalinstitutions. In fact, there is an effort to give the central government greater power by ending the system of fiscal federalism.Putting more money into the pockets of Beijing bureaucrats by recentralizing the tax system, however, is not the answer toChina’s problems. Nor will improving the management of SOEs do anything to solve the problems of loss‐​ridden enterprisesthat have no real owners. 



Real stability will come to China only when its leaders abandon their fatal conceit and realize that it is impossible to plan themarket or society. Although the leadership is willing to tolerate gradual reform to keep the economy strong, there is noindication that they will tolerate political reform. The crackdown on dissidents, especially the arrest of Henry Wu and WeiJingsheng, is a clear signal to Hong Kong and the rest of the world that democratic rule is unacceptable. The West should notconfuse economic liberalization with a desire for democratization. 



Foreign pressure is unlikely to foster positive political change in China. Indeed, such pressure is likely to be counterproductive.Beijing’s frosty attitude toward the United States and our confrontational approach to China will do little to promote stability inEast Asia or to advance human rights in China. Economic sanctions and partial removal of most‐​favored‐​nation trade status forChina would surely damage China, but they might damage the wrong people. Sanctions or higher tariffs could inflict harm onthose who are fleeing the state sector for greater opportunities and freedom in the market sector, and protectionist measuresclearly would harm U.S. consumers and Americans who do business in China. 



To depoliticize economic life, China needs constitutional change and new thinking. As Chinese scholar Jixuan Hu writes, “Bysetting up a minimum group of constraints and letting human creativity work freely, we can create a better society withouthaving to design it in detail. That is not a new idea, it is the idea of law, the idea of a constitution.” Ultimately it is up to theChinese people to shape their own institutions and to secure their fundamental rights, including the right to self‐​government. 



The United States, as the world’s leading constitutional democracy, should spread its ethos of liberty by keeping its marketsopen and extolling the principles that made it great. It should not play the dangerous game of pitting human rights activistsagainst free traders. China should be admitted to the World Trade Organization as soon as possible and be givenmost‐​favored‐​nation status unconditionally. 



It may take another generation for China’s quiet revolution to succeed, but with patience and foresight China may yet joinTaiwan in a mutually beneficial alliance based on free markets and free people. 



_James A. Dorn is vice president for academic affairs at the Cato Institute. He has lectured at Fudan University in Shanghai and is coeditor of_ Economic Reform in China: Problems and Prospects _._
"
"

 **Introduction**  
President Obama’s major trade initiatives, the Trans‐​Pacific Partnership, the Transatlantic Trade and Investment Partnership, and obtaining fast‐​track trade negotiating authority from Congress, have run into a buzz saw of opposition, which has derailed prospects for U.S. trade liberalization for the time being.



What began as the usual objections from the usual suspects—labor unions blaming trade for manufacturing decline and job loss; environmental groups blaming trade for climate change; anti‐​globalization activists sparing the developing world from development–has grown into a populist backlash against the TPP, which is portrayed as a secretive, corporatist plot to circumvent democratic processes and usurp national sovereignty. The nascent TTIP negotiations have been smeared with a similar taint.



Characterizations of the TPP as a scheme to boost the fortunes of tobacco, oil and gas, banking, and pharmaceutical companies at the expense of worker protections, the environment, public health, and food and product safety have gone viral. And without so much as a single public repudiation of these claims by the president those perceptions are sticking.



As is true of most populist causes, buried beneath the enabling mythology and hyperbole are some kernels of truth. One such truth, which this paper seeks to distill from the vacuous, anti‐​capitalist hyperventilation surrounding the trade agenda, is that the so‐​called Investor‐​State Dispute Settlement (ISDS) mechanism, which enables foreign investors to sue host governments in third‐​party arbitration tribunals for treatment that allegedly fails to meet certain standards and that results in a loss of asset values, is an unnecessary, unreasonable, and unwise provision to include in trade agreements. Although detractors may not know it by name, ISDS is a significant reason why trade agreements engender so much antipathy. Yet, ISDS is not even essential to the task of freeing trade. So why burden the effort by carrying needless baggage?



Purging both the TPP and the TTIP of ISDS makes sense economically and politically, would assuage legitimate concerns about those negotiations, splinter the opposition to liberalization, and pave the way for freer trade.



 **What’s Troubling the Trade Agenda?**  
President Obama has failed to make an affirmative case for his trade agenda, and his disinterest in rebutting the flood of damaging portrayals of the TPP has permitted germinating dissent to metastasize into a problem much worse. Meanwhile, the nature of trade, the nature of protectionism, and the substance of trade agreements have changed with the proliferation of cross‐​border investment and transnational supply chains. As companies establish operations in foreign markets, where they are engaged in direct and more intense competition with incumbent firms, concerns about protectionism are no longer confined to the border.



Protectionism manifests in more subtle ways today. Accordingly, ensuring nondiscrimination against imports, foreign investment, and the operations of foreign companies requires rules that sometimes burrow into areas that were once the exclusive purview of domestic legislatures and regulators. Agreements nowadays include provisions affecting domestic intellectual property laws, environmental and labor standards, data flow and storage requirements, banking regulations, and food and product safety requirements, to name some.



The interplay of domestic governance and trade agreement obligations has raised questions about jurisdiction, sovereignty, and the separation of powers. That some perceive the TPP as secretive has heightened sensitivity about its objectives and implications. So, instead of seeing negotiations on “regulatory coherence” as a commonsense way for businesses to reduce the costs of compliance without compromising public health or safety objectives, some suspect it is a path to gutting compliance obligations altogether. Others see regulatory harmonization as another step toward global governance. Efforts to include provisions extending protection of patents, copyrights, and other forms of intellectual property are perceived by some as attempts to impose through treaty what was unachievable through domestic processes. Negotiations of rules that would help ensure that financial‐​sector regulations are promulgated in manners that are nondiscriminatory are painted as attempts to weaken domestic safeguards against recurrence of a financial meltdown.



The hallmarks of tighter global economic integration—cross-border investment, transnational supply chains, and intensifying competition—have created tension between the imperative of domestic sovereignty and the growing demand for rules to guard against protectionism and discrimination. One area where this debate has gotten especially heated is tobacco regulation.



Anti‐​tobacco advocates have been demanding a “carve out” provision, which would excuse lawmakers and domestic agencies from their obligations to craft and enforce tobacco regulations in manners that do not discriminate against imports. The rationale for the safe‐​harbor provision is that tobacco poses special known risks to public health and human safety and that trade obligations should not interfere with the capacity to regulate such a dangerous product. Recently, 42 of the 50 U.S. state attorneys general signed a letter insisting that such a safe‐​harbor provision is essential to protecting public health.1



But as trade experts have explained, there is nothing about the TPP or any other trade agreement that impedes a government’s capacity to protect human life or health. Trade agreements do not prohibit regulating. They merely require that such measures be based on sound science and that discrimination against similar products on the basis of national origin be avoided. The states can ban cigarettes, for example, but not cigarettes “from Indonesia.” As Cato trade policy analyst Simon Lester puts it: “Although there may be valid concerns about some of the more recent additions to trade and investment agreements … the core of these rules constrains domestic regulation only to the extent that such regulation discriminates against imports and does not preclude legitimate domestic policymaking.“2



But if one listens closely to the arguments of anti‐​tobacco advocates (or reads the letter from the 42 attorneys general), what most oppose is the possibility of tobacco companies suing the U.S. government in third‐​party tribunals. Creating a tobacco carve out would reiterate a right that governments already possess and would do nothing to safeguard against suits by tobacco companies—or any other companies.



The real ire of anti‐​tobacco advocates is the Investor‐​State Dispute Settlement mechanism.



 **What is Investor‐​State Dispute Settlement?**  
The ostensible purpose of ISDS is to ensure that foreign investors—usually multinational corporations (MNCs)—are protected against host government actions or policies that fail to meet certain standards of treatment and that cause the investor economic harm. The ISDS confers special legal privileges on foreign‐​invested companies, including the right to sue host governments in third‐​party arbitration tribunals for failing to meet those standards.



Investor‐​State Dispute Settlement dates back to the era following World War II, when previous European colonies were achieving independence and seeking to attract Western investment. It was borne as an expedient to overcome concerns about expropriation by new governments lacking experience with property rights and the rule of law.3 But ISDS procedures were rarely used. In fact, from the inception of ISDS in 1959 through 2002, the number of known ISDS claims worldwide stood at fewer than 100.4 However, during the 10 years between 2003 and 2012, the cumulative total increased to 514 cases.5 In 2012, claimants initiated 58 ISDS cases worldwide, which was the greatest number of initiations in any year, surpassing the previous record set in 2011.6



Provisions for ISDS are included in the 41 U.S. bilateral investment treaties in effect, as well as most of the U.S bilateral trade agreements.7 American TPP and TTIP negotiators are seeking ISDS rules in those agreements. Proponents argue that ISDS provides assurances against unfair treatment from host governments, strengthens the rule of law, and helps bring otherwise reluctant investors to capital‐​hungry jurisdictions. But looking more closely, ISDS arguably weakens the rule of law, forces the public to subsidize the risk of MNC investment abroad, and effectively encourages outsourcing.



 **Eight Good Reasons to Drop ISDS from TPP and TTIP**  
There are practical, economic, legal, and political reasons to expunge ISDS from current trade negotiations.



First, _ISDS is overkill_. Governments are competing to attract productive investment to keep their citizens employed and their economies growing. Accordingly, it is imperative to maintain smart, transparent, predictable policies that are administered fairly and nondiscriminatorily. Asset expropriation or other forms of shabby treatment of foreign companies is not likely to be rewarded by new investment.



Of course, that doesn’t guarantee that policies will never go astray. Sometimes they will. But investment is a risky proposition. Foreign investment is usually more risky. But that doesn’t necessitate the creation of institutions to protect MNCs from the consequences of their business decisions. Multinational companies are among the most successful and sophisticated companies in the world. They are quite capable of evaluating risk and determining whether the expected returns cover that risk. Although MNCs may want assurances, they don’t need them.



Multinational companies can mitigate their own risk by purchasing private insurance policies. Alternatively, they can condition investment on the host government’s agreeing to other protections, contractually. Whether the host agrees would be influenced by the supply of potential investors and the strings they would attach.



Second, _ISDS socializes the risk of foreign direct investment_. When other governments oppose, but ultimately concede to, U.S. demands for ISDS provisions, they may be less willing to agree to other reforms, such as greater market access, that would benefit other U.S. interests. That is an externality or a cost borne by those who don’t benefit from that cost being incurred. In this regard, ISDS is a subsidy for MNCs and a tax on everyone else. Taking the argument one step further, ISDS not only subsidizes MNCs, but particular kinds of MNCs. What may be too risky an investment proposition without ISDS for Company A is not necessarily too risky for Company B. By reducing the risk of investing abroad, then, ISDS is a subsidy for more risk‐​averse companies. It is a subsidy for Company A and a tax on Company B.



Third, ISDS encourages “discretionary” outsourcing. In the global competition to attract investment from the world’s best companies, the United States has some enormous advantages. For many decades, the United States has been the world’s premier destination for foreign direct investment. But in recent years, the United States has been slipping in a number of important investment‐​location decision criteria and, accordingly, its share of global foreign direct investment has declined from 39 percent in 1999 to 17 percent in 2011.8



While ISDS may benefit U.S. companies looking to invest abroad, it neutralizes what was once a big U.S. advantage in the competition to attract investment. Respect for property rights and the rule of law have been relative U.S. strengths, but ISDS mitigates those U.S. advantages. Access to ISDS could be the decisive factor in a company’s decision to invest in a research center in Brazil, instead of the United States. Why should U.S. policy reflect greater concern for the operations of U.S. companies abroad than for the operations of U.S. and foreign companies in the United States? Why should ISDS effectively subsidize outsourcing, and not insourcing?



To be sure, success abroad and success at home are closely correlated. Companies must be able to invest abroad to compete there, and the success of those foreign affiliates tends to be reflected in the performance of the parent companies at home.9 But there is a crucial distinction between “discretionary” and “nondiscretionary” outsourcing.



“Discretionary” outsourcing is investment that goes abroad, but doesn’t really have to. It is investment in activities that could be performed competitively in the United States, but is chased away by policies that make U.S. investment relatively more expensive. “Nondiscretionary” outsourcing is investment in activities that requires a foreign presence.



While we should not denigrate, punish, or tax foreign outsourcing, neither should we subsidize it, and ISDS subsidizes discretionary outsourcing.



Fourth, ISDS exceeds “national treatment” obligations, extending special privileges to foreign corporations. An important pillar of trade agreements is the concept of “national treatment,” which says that imports and foreign companies will be afforded treatment no different from that afforded domestic products and companies. The principle is a commitment to nondiscrimination. But ISDS turns national treatment on its head, giving privileges to foreign companies that are not available to domestic companies. If a U.S. natural gas company believes that the value of its assets has suffered on account of a new subsidy for solar panel producers, judicial recourse is available in the U.S. court system only. But for foreign companies, ISDS provides an additional adjudicatory option.



This inequality of treatment seems to run afoul of the investment provisions in the Baucus‐​Hatch‐​Camp legislation (to extend fast‐​track trade promotion authority to the president), which state that the principal U.S. negotiating objectives regarding foreign investment are to: “[R]educe or eliminate artificial or trade distorting barriers to foreign investment, while ensuring that foreign investors in the United States are not accorded greater substantive rights with respect to investment protections than United States investors in the United States…“10



Foreign investors having recourse to the U.S. legal system and then, if that produces unsatisfactory results, to third‐​party ISDS procedures arguably constitutes greater substantive rights for them than for domestic investors, whose options are confined to the U.S. legal system.



Fifth, _U.S. laws and regulations will be exposed to ISDS challenges_ with increasing frequency. The number of cases is on the rise. Most claims have been brought against developing countries—with Argentina, Venezuela, and Ecuador leading the pack—but the United States is the eighth‐​largest target, having been the subject of 15 claims over the years.11



As the percentage of global Fortune 500 companies domiciled outside the United States continues to increase, U.S. laws and regulations are likely to come under greater scrutiny. The specter of foreign companies prevailing in challenges of U.S. laws outside the U.S. legal system would frustrate further the task of selling trade to a skeptical public and would reward trade critics who have been warning of just such an outcome for many years.



Investor‐​State Dispute Settlement raises concerns about domestic sovereignty. Among recent cases highlighting these tensions is a suit brought by Philip Morris, Inc. against the Australian government for a law requiring that cigarettes be sold in plain packaging. Philip Morris claims that the requirement deprives it of its property (trademarks, logos, and labels), which is important for brand recognition and without which its revenues will decrease. Philip Morris may have a legitimate claim, but the optics will not be favorable for trade agreements if the company prevails.



Meanwhile, growing concerns in Europe about the vulnerabilities of environmental and public‐​safety laws to challenges by foreign corporations—sparked, in part, by a case brought by a Swedish energy company against Germany for its decision to abandon nuclear power—have led the EU to suspend ISDS negotiations in the TTIP for a period of three months, as it collects and evaluates public comments and reconsiders its position. Realistically, it is difficult to conceive of any benefits to including ISDS provisions in the TTIP, given the advanced legal systems in the United States and Europe, unless the wave of the economic future is expected to arrive in a tsunami of international litigation.



Sixth, _ISDS is ripe for exploitation by creative lawyers_. There is a lot of latitude for interpretation of what constitutes “fair and equitable” treatment of foreign investment, given the vagueness of the terms and the uneven jurisprudence. Thus, ISDS lends itself to the creativity of lawyers willing to forage for evidence of discrimination in the arcana of the world’s laws and regulations. Among the complaints worldwide in 2012 were challenges related to “revocations of licenses, breaches of investment contracts, irregularities in public tenders, changes to domestic regulatory frameworks, withdrawals of previously granted subsidies, direct expropriations of investments, tax measures and others.“12



Meanwhile, some agreements are attempting to expand the definition of a breach of the obligation of host governments to provide fair and equitable treatment to include: “targeted discrimination on manifestly wrongful grounds, such as gender, race or religious belief.” This attempt to broaden the scope for complaints—included in the EU‐​Canada trade agreement—should be a cause for concern.13



Seventh, _ISDS reinforces the myth that trade primarily benefits large corporations_. A persistent myth that has proven hard to dispel permanently is that trade benefits primarily large corporations at the expense of small businesses, workers, taxpayers, public health, and the environment. The fact is that trade is the ultimate trustbuster, ensuring greater competition that prevents companies from taking advantage of consumers. Lower‐​income Americans stand to benefit the most from trade liberalization, as the preponderance of U.S. protectionism affects products and services to which lower‐​income Americans devote higher proportions of their budgets.



But by granting special legal privileges to multinational corporations, ISDS reinforces that myth and is a lightning rod for opposition to trade liberalization. It is effectively a subsidy that mitigates risk for U.S. multinational corporations and enables foreign MNCs to circumvent U.S. courts when lodging complaints about U.S. policies. Ultimately, ISDS is unimportant to the task of trade liberalization and its inclusion in trade agreements only strengthens trade’s opposition.



Eighth, _dropping ISDS would improve U.S. trade negotiating objectives_ , as well as prospects for attaining them. Recently, a group of business associations joined in a statement of opposition to the requests for a tobacco carve‐​out provision, arguing that it would be superfluous and set a dangerous precedent that would undermine the settled view that governments are already entitled to regulate in the interest of protecting human life or health.14 Given their concern for the rule of law and the traditions of the trading system, the statement’s signatory organizations should be amenable to a compromise that would include purging ISDS from the TPP and the TTIP in exchange for a denial of the carve‐​out language.



Such a deal would assuage thoughtful critics of the trade agenda, who do not oppose trade, but who believe trade agreements should be more modest and balanced. Meanwhile, what now appears to be an angry mob protesting trade generally will be thinned out, exposing the unsubstantiated arguments of the professional protectionists who benefit by impeding Americans’ freedom to trade.



 **Conclusion**  
For practical, economic, legal, and political reasons, ISDS subverts prospects for U.S. trade liberalization. Yet it is tangential, at best, to the task of freeing trade. Any benefits to availing MNCs to third‐​party adjudication are all but totally overwhelmed by the additional costs. In the proverbial airplane that is down one engine and losing altitude, throwing ISDS out of the cargo hold to reduce unnecessary weight is the best solution.



At this point, it remains unclear whether the president is genuinely committed to doing what it will take to advance his trade agenda. But should he convince himself of the efficacy and righteousness of freeing trade and become interested in putting the necessary pieces together to bridge political divides, jettisoning ISDS and explaining how doing so liberates us from legitimate concern that corporations will run roughshod over domestic laws could go a long way toward selling these agreements to the public.



 **Notes**  
1\. Inside U.S. Trade’s World Trade Online, “Attorneys General From 42 States Call For Tobacco Carveout In Trade Deals,” February 6, 2014, www​.insid​e​trade​.com.  
2\. Simon Lester, “Free Trade and Tobacco: Thank You for Not Smoking (Foreign) Cigarettes,” Cato Free Trade Bulletin no. 49, August 15, 2012.  
3\. For a good history of ISDS, see Simon Lester, “Liberalization or Litigation? Time to Rethink the International Investment Regime,” Cato Policy Analysis no. 730, July 8, 2013.  
4\. United Nations Conference on Trade and Development (UNCTAD), “Recent Developments in Investor‐​State Dispute Settlement (ISDS),” IIA Issue Notes No. 1, May 2013.  
5\. Ibid   
6\. Ibid.  
7\. Shayerah Ilias Akhtar and Martin A. Weiss, “U.S. International Investment Agreements: Issues for Congress,” Congressional Research Service, April 29, 2013.  
8\. Daniel Ikenson, “Reversing Worrisome Trends: How to Attract and Retain Investment in a Competitive Global Economy,” Cato Policy Analysis no. 735, August 22, 2013.  
9\. Ibid., Figures 4, 5, 6, 7, 8, and 9; Matthew J. Slaughter, “American Companies and Global Supply Networks: Driving U.S. Economic Growth and Jobs by Connecting with the World,” Business Roundtable, the United States Council for International Business and the United States Council Foundation, January 2013.  
10\. United States Senate, Bipartisan Congressional Trade Priorities Act of 2014.  
11\. United Nations Conference on Trade and Development. To date, investors have not prevailed in any of their complaints against the United States.  
12\. United Nations Conference on Trade and Development.  
13\. Simon Lester, “An Equal Protection Clause for Investment,” International Economic Law and Policy Blog, February 6, 2014.  
14\. U.S. Chamber of Commerce, et al., “Joint Statement on Proposal to Modify Longstanding “General Exception” Provision and Dispute‐​Settlement Provision in the Trans‐​Pacific Partnership (TPP) Negotiations,” February 2014, http://​www​.amcham​.com​.au/​D​o​w​n​l​o​a​d​s​/​9​9​a​9​e​9​a​3​-​1​e​1​7​-​4​1​8​0​-​a​6​a​6​-​7​2​d​d​3​8​0​a65ca….
"
"
Share this...FacebookTwitterWinter has not even officially arrived, but already large areas of the northern hemisphere are seeing “historic snowfalls”, frigid temperatures and even avalanche alarms.
The Northern Hemisphere has certainly caught a major cold, one certainly not caused by the human CO2 virus. Instead of fever, parts of the northern hemisphere are in hypothermia!
Alarmists, media desperate
Though global warming scientists will never admit it, they are really surprised and stunned. All that is left for them is to make up some cockamamie warming-causes-cold explanations and hope there are enough severely stupid among the media and masses to believe it.
“United States — Rewrite the Record Books”
Beginning in North America, “sub-zero temperatures are now blasting” millions of Americans following “the three historic snowstorms which buried parts of the U.S. last month,” reports weather site electroverse.net here.
Electroverse writes that “lows throughout the week will be more like January temperatures” with readings below zero for many U.S. states and “temps down into the teens are even forecast as far south as Texas.” Yesterday, 97 records toppled.
“It’s a big deal,” Electroverse writes in its headline.
Solar activity suspected
It’s not the sort of thing we are supposed to be expecting from a “warming planet”.  Some climate experts blame natural factors, like solar activity, for the cold, and that these warnings have long been known since the sun has entered a new period of calm.
Freeze watches and warnings also extend as far south as Florida. And it’s only early November. And don’t expect to see many FFF activists to show up at rallies protesting hot weather any time soon.
Polar Bear Science site here also reports that the Hudson Bay in Canada has started freezing up earlier than normal three years in a row!
Europe starting to get clobbered by snow, 2m in Alps
Meanwhile cold has also spread across Europe, though not quite as brutal as what we’ve been seeing across North America.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In central Europe, the Austrian online heute here reports that “huge amounts of snow” are on the way for the Alps. German site Wetteronline.de reports here of “new, severe snowfalls in the Alps” with “up to two meters of fresh snow are possible in places up to the weekend” in Switzerland, Austria and Northern Italy, “This is good news for winter sports enthusiasts – but the danger of avalanches is increasing.”
Biggest November snowstorm in 40 years
Even global warming child activist Greta Thunberg’s Sweden is getting hard hit by extreme cold and snow. Electroverse reports the Nordic country is suffering “its biggest November snow storm in 40 years.”
On November 10th, Mika tweeted that temps in northern Sweden fell 10 -34.5°C.

Today is the coldest morning so far during the ongoing winter season:
-34.5°C in Sweden, -31.1°C in Norway and -30.6°C in Finland (not shown on the map). pic.twitter.com/PoH2Ddnde4
— Mika Rantanen (@mikarantane) November 10, 2019

Most snow in 60 years
The German Ruhrkultur site reports how also Finland just saw “the coldest autumn temperature and the highest snow depth in at least 60 years” and that ” the temperature in Enontekiö, a municipality in Finnish Lapland, dropped to 28.2°C on Tuesday 5 November.”
Deepening cold across Siberia as well
“On November 11 in Yakutia, the daily temperature never rose above −30°C (-22F),” reported SOTT site here. “Some parts of Siberia were even colder: In Evenkia and the northern regions of the Krasnoyarsk Territory, the temperature dropped to −41 … −44°C.”

SOTT comments (sarcastically): “I wonder how much ice will melt at −44°C (-47F).
With all the early winter weather, it’s ridiculous to claim the globe is burning up. So it’s no wonder the alarmists have taken their climate ambulance to the far side of the globe, NSW Australia, and kept their narrow focus on brush fires.
Hat-tip: Yota at Twitter
Share this...FacebookTwitter "
"

The Bush Administration caught unshirted hell from environmentalists last month for ordering a 20 percent increase in energy efficiency standards for air conditioners sold after 2006. The Clinton administration, you see, had proposed a 30 percent increase in standards, so the Bush administration is once again being hammered for relaxing environmental standards, promoting wasteful energy consumption, accelerating global climate change, and risking California‐​style blackouts during hot summer months for years to come.



While the Greens would have us believe that Bush was once again carrying water for corporate America, the president was actually doing us all a small favor. 



First, the Clinton standards would have imposed huge costs on consumers. The U.S. Department of Energy (DOE) estimates that the Clinton standards would have increased the price of your typical air conditioner by a whopping $332 to $435. The Bush administration standards would increase prices by only (!) $144 to $213. 



Second, the DOE had failed to adequately consider the possibility that some manufacturers would be driven out of business by the Clinton standards, thus reducing competition that helps restrain prices. So even those expected astronomical price increases were almost certainly low‐​ball estimates. 



Third, the DOE concluded that over 40 percent of consumers buying air conditioners that meet Clinton’s standards would never recover the higher costs through energy cost savings. Under the Bush administration standards, only about 25 percent of consumers would be net losers. 



The Greens are right, however, to point out that the Bush standards will accomplish little on the environmental front, but the Clinton standards would not have achieved much either. Consider: The DOE estimates that the Bush standards will reduce growth in energy consumption by “about 3 quads” between 2006–2030, a figure deemed by the department as “significant.” The nation, however, is expected to use about 3,200 quads of energy from 2006–2030, so “3 quads” equals but 9/100 of 1 percent of projected energy use. The DOE’s claims about avoided emissions of carbon dioxide and nitrogen oxides are equally overblown.



While we should be grateful that the Bush administration has adopted efficiency standards that are somewhat less costly than those planned by the Clinton administration, the unwarranted economic burden and restrictions on consumer choice remain too great. Consumers, not bureaucrats, should have the final say about what’s in the home.



The chance of recovering the higher cost of air conditioners through reduced energy consumption, after all, depends heavily on electricity prices and the frequency with which the air conditioner is used. Given that electricity prices vary widely throughout the United States (highest unit prices are more than double the lowest), and that consumers’ usage patterns vary for dozens of reasons (e.g., geographic location, family size, tolerance for warm temperatures), paying more up‐​front at the cash register to reduce operating costs by a small amount for years to come makes sense for some but not for others. 



While that observation should be rather obvious, it is — for practical purposes — ignored by appliance efficiency standards that apply to everyone whether it makes sense or not. The result is that many consumers are forced to incur higher costs than they would if allowed to base their decision on factors affecting their energy bills. The requirement is roughly equivalent to an attempt to restrain hat costs by requiring that everyone wear a size seven hat. 



Moreover, the entire regulatory exercise is fraught with uncertainty. The government, for instance, assumes that it can accurately forecast the conditions affecting future appliance manufacture, sales, and use during the next 30 years, including changes in technology, markets, energy prices, consumer and appliance manufacturer behavior, raw material, labor and overhead costs, and wholesale and retail markups. How likely is that, particularly when much of the data comes from appliance manufacturers with a direct financial interest in the DOE’s decisions? 



The upshot is that all analytic uncertainties in the exercise are resolved in favor of tighter standards because only the well organized — typically those positioned to gain the most from government action — can affect the rulemaking process. Big appliance manufacturers, for instance, are happy to have the government impose regulatory barriers to entry into their market, barriers that serve to cartelize the industry. Bureaucrats are always on the lookout to expand their power and reach. Green political lobbies consider energy conservation a religious virtue regardless of economic realities and are in the business of delivering such mandates in return for contributions from the faithful. 



Everyone wins but the poor consumer who is for the most part oblivious to these regulatory machinations undertaken at his expense. The administration did us a favor by minimizing the hunk of flesh taken out of our economic hides. But it would be better to junk these standards and make the consumer, once again, king of his own pocketbook. 
"
"
Ok the next session is starting in a few minutes, less than 2 hours from now I’m going to know if the work I and all of the volunteers at www.surfacestations.org has been scientifically fruitful, or if I’m going to get pelted on the stage with rotten fruit.
My presentation is updated with some late breaking photos Russ Steele got yesterday from St. George, UT, loaded into the presentation laptop, and my remote control has been tested. I’m as ready as I’ll ever be.
At the very least, after sitting through a bunch of Powerpoint presentations, my use of the same software I use for doing TV weather presentations should break the mold.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea442c16c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In politics, timing is everything, Howard Dean being a wonderful example. So is Al Gore, who chose to give a completely paranoid speech about global warming in New York two weeks ago, on a day when the temperature was 22 below normal. In a remarkably Dean‐​like rant to the Democratic organization MoveOn, he said that the reason Americans reject his vision of climate‐​Armageddon has more to do with what he called “a massive and well‐​organized campaign of disinformation” on the part of me and my few friends, than it does with the thermometer. 



When it comes to disinformation about climate change, Al’s got competition in the principal beneficiary of Howard Dean’s rhetorical largesse, John Kerry, who looks to me like a cinch for the Democratic nomination. On May 17, 2000, Kerry said:





I’m offering a night of free beer to the first journalist who can come up with a picture of John Kerry wearing a coat in November (and expect to have to pay off within one minute of this column’s publication). But what about that whopper about northern New Hampshire’s ponds?



One lesson in climate hype that Gore never learned (and which may have cost him the presidency) is that people can look up facts pretty quickly now. Gore lost normally Democratic West Virginia because of his hype on global warming and his resultant vitriol against the coal industry. Miners, who he would have put on unemployment, stayed home or voted for Bush. Now Gore’s venting about planetary heating in howling blizzards.



So should Kerry beware. There’s lots of data on the Internet, including a study by the U.S. Geological Survey of “ice‐​out” dates on lakes in northern New Hampshire. That’s the day of the year when you can no longer play hockey.



John Kerry is 60 years old, so it’s safe to say he was playing hockey in northern New Hampshire, his home, from the ages of 7 to 17, or 1950 through 1959, near First Connecticut Lake. The average date of ice‐​out for that period was May 1. From 1991–2000, when, according to Kerry, “you are lucky if the ponds freeze,” the average ice‐​out date is later, on May 5. 



A year later, on May 1, 2001, Kerry said, “This summer the North Pole was water for the first time in recorded history,” a story that was originally carried by the _New York Times_ in September 2000. It was retracted three weeks later as a barrage of scientists protested that open water is common at or near the pole at the end of summer. Further, it’s common knowledge in the scientific community that there has been no net change in Arctic temperatures in the last 70 years.



He went on: “In 1995, after a period of unusual warming, a 48 by 22 mile chunk of the Larsen Ice Shelf in Antarctica collapsed.” Disregarding that ice shelves don’t “collapse,” the fact, as accessible as the nearest _Nature_ magazine, is that Antarctica shows a slight cooling trend in recent decades.



Voters need to stay tuned to Kerry on global warming for the Arizona primary on Feb. 3. John McCain, who will do anything to defeat George Bush, has been on a merciless campaign of badgering the president about climate change, including shepherding the first Senate vote to restrict energy use because of global warming, which only failed by eight votes last fall. You can bet Kerry is going to feed off of McCain’s Arizona popularity. He may even entreat him into the Veep slot, claiming to be the ultra‐​centrist and spelling sure defeat for President Bush. 



Anyway, now that he’s the front‐​runner, he’s going to have to watch what he says. Or what he wears. Again, free beer for that picture of him wearing a coat in November.



If Kerry doesn’t check his facts better, he’ll soon be sharing the platform with Al and Howard, trapped in the living hell of the formerly relevant. 
"
"

Have you ever worked for an employer that was cheap to the point of making you want to find another job? So miserly and humiliating at Christmas that you had visions of fun revenge instead of sugerplums dancing in your head?
Slate Magazine decided to have a contest for the worst corporate scrooge, and over 200 entries were submitted.

Some of the stories are pretty humorous, others are just downright depressing. You can read it here.
I have a couple of stories of my own. Both were TV stations I worked for. WLFI-TV in Lafayete Indiana where I got my start was owned by The Shively Brothers, and Harold, the younger brother, was the General Manager. They were newspapermen and this was their first TV venture. Being that, they were both clueless on how to run a TV station or treat employees that had been in the business awhile.
One Christmas ‘bonus’ was quit memorable. Each employee received a brass key chain with a giant brass letter signifying their last name. Seeing how mine was one of the largest letters, a “W”, and it was about 5 inches long making it impossible to even fit into my pocket. One employee suggested we all get together, spell out an appropriate message (you guess what it was) and present it to the our clueless GM.
Another one happened right here at KHSL-TV in Chico, soon after the corporate weasels from Catamount Broadcasting bought the station, and turned it from a pleasant family run business, to a place that was all about the bottom line.
Mickey McClung, the former owner, and a kind woman, always made sure each employee had extra money or a gift card to buy Christmas dinner for the family, and we could always count on that, and sometimes more in good years. The next owner, Howard Brown, kept the tradition and was even more generous.
After Catamount took over, we wondered what would become of that tradition. We soon found out.
The next Christmas we were presented with a $25 gift certificate to Holiday Markets. They were an advertiser, and the gift certificates were “trade outs” for airtime, so they cost the corporation nothing.
And…surprise, there are no Holiday Markets in Chico, the company had closed its Chico store that year. They had stores in Redding and Paradise, but with the price of gas, it was hardly worth the trip.
What I learned from these experiences was to treat my own employees well. And hand out real, significant cash bonuses at Christmas instead of something that will make them think less of you. Employees make the company, not the other way around.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea932335f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAt his German news commentary site, Gabor Steingart reports on the results of the latest ARD German public broadcasting trend analyses. Here it’s clear that German citizens are speaking loud and clear on the topic of climate protection: not so fast!
Rush to Green Deal without support
Lately the media and politicians have been pushing hard to start a fundamental change of society to a low-carb, de-industrialized organic garden society. For example, EU Commission President Ursula von der Leyen just announced a 1 TRILLION euro plan to decarbonate the continent.
Yet, public support for such an ambitious and adventurous transformation project appears to be rather soft at best in Germany.
According to Steingart:
Only 27 percent of Germans regard climate protection as a political priority. 73 percent have other concerns. Outside of the green voters (currently 23 percent in surveys), the alarmism of politicians (“climate emergency”) and scientists (“ecocide”) meets with discontent. The people want to say to the grand coalition government: Don’t throw the baby out with the bath.”
Greenland sets new record low
On another note, according to the Austrian www.wetter.at site, Greenland recorded a new all time record low temperature. The site reports:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




While 2019 is the warmest year in Australia since records began, as the Office of Meteorology stated in its annual climate report, Greenland has set a new record for cold temperatures: minus 66 degrees C.”
Mother Nature tricks Europe
Meanwhile, parts of central Europe have been experiencing record warm January temperatures, and thus reinforce the collective public’s impression that the global climate is out of whack. It’s not.
How unfortunate that Mother Nature would play such a mean trick on Europe. Everywhere else it’s pretty much winter as usual:

Image: Ventusky, via Pure Climate Skeptic. 
Now European policymakers are going to have an easier time getting the public to accept the EU “Green Deal”. Boy, are they in for a cold awakening.
Share this...FacebookTwitter "
"

For all those folks worried that a cell phone tower at the Elks Lodge or Hooker Oak Park is going to give rise to a legion of cancer ridden mutants, here’s another study that says “not likely”
March 26, 2007 (Reuters) —
Cell phone use does not appear to be associated with an increased risk of glioma, the most common type of brain tumor, according to a new study.
“Public concern has been expressed about the possible adverse health effects of mobile telephones, mainly related to [brain] tumors,” Dr. Anna Lahkola, a researcher at the Radiation and Nuclear Safety Authority in Helsinki, and colleagues explain in the International Journal of Cancer.
The researchers examined the relationship between mobile phone use and risk of glioma by studying 1,521 glioma patients and 3,301 controls.

The vast majority of both groups reported using cell phones. Overall, 92% of glioma patients and 94% of controls reported using mobile phones.
Overall, there was no evidence of increased glioma risk related to regular mobile phone use.
There were no significant associations observed with duration of use, years since first use, cumulative number of calls or cumulative hours of use.
No increased glioma risk was observed when analog and digital phones were analyzed separately.
There was, however, a trend toward increased risk of glioma in people who used a cell phone for more than 10 years exclusively on one side of the head, which was on the same side as the tumor. The association reached “borderline statistical significance.”
“This may be due either to chance or causal effect or information bias, i.e., overreporting of mobile phone use on the affected side by the cases with brain tumors,” the investigators wrote


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea764b045',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Yes, Virginia, there is a sales tax. With Republicans no longer in control of the House, Forbes says this may be the last Christmas you’ll be able to dodge sales tax by buying that $350 iPod or $1,200 laptop online.
The Internet was just coming into its infancy in 1994 when Republicans took control of the House and Senate. Republicans have been steadfast in their resistance to taxing the Internet, but they may no longer be able to prevent it.
From the article:
“With Democrats in charge… ‘The stars are lined up better,’ says Harley Duncan, executive director of the Federation of Tax Administrators, which represents state tax officials… [But] this is hardly a done deal. The 4,700-member Direct Marketing Association is fighting any new authority for the states.”
It remains to be seen if the Internet will become the next tax revenue source. One thing’s for sure, Al Gore won’t come out and say he invented Internet taxation.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9986e9c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Northern Europe and the Arctic show signs of winter cooling over the past decades. Could the global warming theory be in for an upset?
Looking at January data over the northern Europe, we see no real warming trend for the month, according to data from the Japan Meteorological Agency (JMA).
When plotting mean January temperatures for Norway for stations where data are available since 1988, we see that 7 of 11 stations show a cooling trend since 1988, despite rising CO2:

Data: JMA. 
January cooling in Finland
The story is the same in Finland, a country that stretches into the Arctic:

All stations with data going back to 1988 show a cooling trend in Finland for the month of January. Data: JMA
Ireland cooling more than warming


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Ireland, situated in the North Atlantic, we also see signs of cooling mid winters. warming has been AWOL for some time now:

 
In Ireland 4 of 6 stations with data going back to 1988 show a cooling trend for the month of January. Data: JMA
Arctic sea ice rebound
Next we move to the Arctic. This year’s winter is seeing an impressive rebound in sea ice, tweets meteorologist Chris Martz, reaching the 3rd highest level in 15 years:

Arctic sea ice extent is currently 3rd highest in the last 15 years, the only years higher are 2008 and 2009. It just surpassed 2013 and is about to pass 2004 within the next day or two. It isn't too far behind 2009 either. This isn't good news for the narrative, I will say that. pic.twitter.com/RyCRMnI8tK
— Chris Martz Weather (@ChrisMartzWX) February 11, 2020

“Dramatic recovery” for Arctic sea ice
Obviously the situation in the Arctic is nowhere as dire as alarmists like to deceive others into thinking. According to meteorologist Justin Berk here, “Arctic Sea Ice has made a dramatic recovery and expansion this winter.”

Image: National Snow and Ice Data Center
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman online weekly FOCUS here reports how cuts by wind energy giant Enercon will lead to 3000 layoffs. According to Enercon chief executive Hans-Dieter Kettwig, “politicians have pulled the plug on wind energy.”

German wind energy industry in turmoil. Photo: By Pierre Gosselin
Subsidies cut
Once lavished with huge incentives, the German wind industry is being hit hard after the government recently ended the huge subsidies that were once aimed at expanding the installation of wind energy capacity. Power grid operators had been struggling to keep the grid stable due to erratic feed-in and the subsidized feed-in of wind energy caused German electricity prices to become among the most expensive worldwide.
Fierce opposition from hundreds of protest groups
Moreover, hundreds of citizen protest groups have sprouted and since become a formidable force pushing for the stop of proposed wind projects. Not only have wind parks scarred the German landscape and destroyed biotopes nationwide, they have also been shown to be a real health hazard to humans living in their proximity through the low frequency infrasound they emit. Enough is enough, citizens say.
3000 job cuts in the works


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




FOCUS reports: “The crisis in the German wind energy industry is worsening. According to the ‘Süddeutsche Zeitung’, hard cuts at the largest German manufacturer Enercon will cost 3000 jobs.”
Next year Enercon will also cut contracts with suppliers, sending a wave of job losses across the industry. “If supply contracts are terminated as planned, many of these companies are threatened with extinction,” FOCUS reports.
FOCUS notes that the layoffs will hit regions that are already economically weak. “At the Aurich and Magdeburg locations, 1500 jobs will be cut, according to the company. At the company headquarters in Aurich, 250 to 300 jobs are affected.”
Stricter regulations for wind parks, greater setback distances
Not only have the subsidies for German wind parks been cut back, but also setback rules will become more strict in order to protect homes and residents from landscape blight and infrasound. In the future, wind parks will need to keep a greater distance away from residential areas. The current  CDU/CSU/SPD government wants to keep at least one kilometer between wind power installations and residential areas in the future. This will make many proposed projects impossible.
German Greens demand the industrialization of scenic landscape
The stricter setback rules have been sharply criticized by the wind industry, and particularly by German Green party leader, Annalena Baerbock: “The planned distance rules for wind turbines are devastating,” she told network Germany RND.
“Contrary to all public announcements, the federal government is thus making further expansion of wind power impossible. This is tantamount to a boycott of the Paris Climate Treaty and its own climate targets.” Baerbock also told RND: “The federal government claims to want to get out of coal but at the same time is stopping the expansion of wind power.”
Share this...FacebookTwitter "
"

It was with sadness and surprise that I learned today that Bombers Baja Grill off East Avenue has closed their doors forever. They served their last Mexican food ordnance today, 12/15/06.
Bombers was known for the biggest burrito ever made (at least in Chico). There was the “missile”, the “bomber” and the “atomic bomb” which between the habeneros, beans, and the calories, was a nearly 2 pound explosive combination.
I’m not sure where they got the idea to mix bombs and burritos, but here is an early picture from the experimental days of the restaurant where they were loading planes at the Chico Army Air Base (now the Chico Airport).

Bombers was an original, so original that those corporate franchisers at Chipotle knocked off the idea I think. But unlike Chipotle, which has food served in prison cafeteria style with steel and glass ambience, Bombers food was great, and the ambience had history you could feel.
I suppose it was only a matter of time though, they had a terrible location, and competition was springing up around them. They were also nearly invisible, which goes to show it pays to advertise.
I’ll miss Bombers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea980e1fb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’ve been saying this all along…the sun is the Big Kahuna of climate change on earth. CO2 effects pale in comparison to the effects of the sun. I’ll have more on this in part 3 of my series on 2006’s Record setting temperature year.
WASHINGTON (AP) — The brightening and dimming of the sun may account for a 1,500-year cycle of cooling and warming on parts of the Earth, a study of ice in the North Atlantic suggests.
Researchers found that a very slight difference in the amount of solar energy reaching the Earth can have a powerful chilling effect on the climate: ice builds up in lands bordering the North Atlantic, the average temperature drops in Europe and North America.
see the full story here from USATODAY
This goes hand in hand with another study by the University of Main Climate Change Center as reported by SPACEREF
And just in case that’s not enough light reading for you, here is a study from Harvard that talks about “Chaos” and the sunspot cycle.
Part of the abstract is quite telling: “…by examining 1500 years of sunspot, geomagnetic, and auroral activity cycles. Sub-harmonics were found of the fundamental solar cycle period during the years preceding the Maunder minimum and loss of phase of the subharmonic on emergence from it. These phenomena are indicative of chaos. They indicate that the solar dynamo is chaotic and is operating in a region close to the transition between period doubling and chaos.”
Translation: The sun can easily tip from one state to another, with resultant changes in solar output.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8f61fb0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Sen. Dianne Feinstein has introduced the Bot Disclosure and Accountability Act, a proposal to regulate social media bots in a roundabout fashion. The bill has several shortcomings.   
  
  
Automation of social media use exists on a continuum, from simple software that allows users to schedule posts throughout the day, to programs that scrape and share information about concert ticket availability, or automatically respond to climate change skeptics. Bots may provide useful services, or flood popular topics with nonsense statements in an effort to derail debate. They often behave differently across different social media platforms; Reddit bots serve different functions than Twitter bots.   
  
  
What level of automation renders a social media account a bot? Sen. Feinstein isn’t sure, so she’s relinquishing that responsibility to the Federal Trade Commission:   




The term ‘‘automated software program or process intended to impersonate or replicate human activity online’’ has the meaning given the term by the [Federal Trade] Commission



If Congress wants to attempt to regulate Americans’ use of social media management software, they should do so themselves. Instead, they would hand the hard and controversial work of defining a bot to the FTC, dodging democratic accountability in the process. Moreover, the bill demands that the FTC define bots “broadly enough so that the definition is not limited to current technology”, virtually guaranteeing initial overbreadth.   
  
  
While the responsibility of defining bots is improperly passed to the FTC, the enforcement of Feinstein’s proposed bot disclosure regulations is accomplished through a further, even less desirable delegation. The Bot Disclosure and Accountability Act compels social media firms to adopt policies requiring the operators of automated accounts to “provide clear and conspicuous notice of the automated program.” Platforms would need to continually “identify, assess, and verify whether the activity of any user of the social media website is conducted by an automated software program”, and “remove posts, images, or any other online activity” of users that fail to disclose their use of automated account management software. Failure to reasonably follow this rubric is to be considered an unfair or deceptive trade practice.   
  
  
This grossly infringes on the ability of private firms, from social media giants like Facebook to local newspapers that solicit readers’ comments, to manage their digital real‐​estate as they see fit, while tipping the balance of private content moderation against free expression. Social media firms already work to limit the malicious use of bots on their platforms, but no method of bot‐​identification is foolproof. If failure to flag or remove automated accounts is met with FTC censure, social media firms will be artificially incentivized to remove more than necessary.   
  
  
The bill also separately, and more stringently, regulates automation in social media use by political campaigns, PACs, and labor unions. No candidate or political party may make any use of bots, however the FTC defines the term, while political action committees and labor unions are prohibited from using or purchasing automated posting software to disseminate messages advocating for the election of any specific candidate. It is as if Congress banned parties and groups from using megaphones at rallies. Would that prohibition reduce political speech? No doubt it would. How then can the prohibitions in this bill comport with the constitutional demand to make no law abridging the freedom of speech? They cannot.   
  
  
Feinstein’s bill attempts to automate the process of regulating social media bots. In doing so, it dodges the difficult questions that attend regulation, like what, exactly, should be regulated, and foists the burden of enforcement on a collection of private firms ill‐​equipped to integrate congressional mandates into their content moderation processes. Automation may provide for the efficient delivery of many services, but regulation is not among them. Most importantly, the bill does not simply limit spending on bots. It _prohibits_ political (and only political) speech by banning the use of an instrument for speaking to the public. Online bots may worry Americans, but this blanket prohibition of speech should worry us more.
"
"
Share this...FacebookTwitterEarth’s atmosphere is made of 78% nitrogen (N2) and 21% oxygen (O2). The “consensus” view is N2 and O2 are not greenhouse gases (GHGs) and don’t absorb infrared radiation (IR). But scientists have been saying N2 absorbs and radiates IR since 1944 and more recent (2012, 2016) studies have found N2 and O2 are “radiatively important” greenhouse gases with IR temperature absorption capacities similar to CO2.
It’s been known for 75 years that nitrogen – the Earth’s most prevalent atmospheric gas – absorbs and “strongly” radiates infrared energy (Stebbins et al., 1944)

Image Source: Stebbins et al., 1944
Methane (CH4) is thought to be an 84 times more potent greenhouse gas than CO2.

Image Source: Environmental Defense Fund
Nitrogen, oxygen are “natural greenhouse gases”
Scientists (Höpfner et al., 2012) publishing in Geophysical Research Letters dispute the “common perception” that nitrogen and oxygen – accounting for 78% and 21% of the Earth’s atmospheric gases – do not contribute signficantly to the Earth’s greenhouse effect.
They assert N2 and O2 are “radiatively important” “natural greenhouse gases” primarily because their concentration is “about 2000 (550) times higher than that of CO2 and about 4.4 × 105 (1.2 × 105) times more abundant than CH4.”
Nitrogen, oxygen combined are more potent GHGs than methane
The atmospheric abundance of N2 and O2 compensates for their relatively weaker IR function (when directly compared to CH4).
For example, “the natural greenhouse effect of N2 and O2 would be larger than that of CH4 by a factor of 1.3” when considering their combined isolated GHE influence.
Further, the reduction in the atmosphere’s infrared transmission amounts to 25.7% for N2, 14.2% for O2, and only 6.9% for CH4.
Nitrogen’s greenhouse gas influence also rivals CO2’s
Höpfner and colleagues also suggest N2 reduces outgoing longwave radiation (OLR) by 4.6 W/m² compared to CO2’s 5.1 W/m² when assesing their solo absorption capacity. This would appear to be a rather minor difference.
If the number of N2 molecules in the atmosphere were hypothetically doubled, it would produce a 12 W/m² longwave greenhouse effect forcing.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Doubling CO2 from 280 ppm to 560 ppm only yields a 3.7 W/m² radiative forcing.
The authors reject the “view that the radiative forcing of N2 increase operates only indirectly by broadening the absorption lines of other gases.” Instead, N2 has a “direct impact” (as well as an indirect impact) within greenhouse effect forcing.

Image Source: Höpfner et al., 2012
Experiment: nitrogen, oxygen absorb IR to about the same limiting temperature as CO2
A real-world experiment (Allmendinger, 2016) assessing the efficacy of CO2’s IR-absorption temperature capacity relative to air (N2, O2) and Argon (Ar) further establishes CO2 is not the “special” GHG it is commonly thought to be.
Twin styrofoam Saran-wrap-sealed tubes exposed to sunlight were used, one with pure (1,000,000 ppm) CO2 and the other with air (N2, O2) and/or Ar.
The results were admittedly “surprising” given expectations CO2 would operate as a radiatively distinct GHG.
The tube absorbing IR with N2 and O2 (air) and Ar warmed to a temperature limit quite similar to (55°C to 58°C) the temperature limit in the 100% CO2 tube (58°C).
There was no remarkable or  “special” heat absorption capacity for CO2 relative to air observed. And Argon – not considered a greenhouse gas – absorbed IR to the same temperature limit as CO2. With a concentration of 9300 ppm, Ar is the third-most abundant gas in the Earth’s atmosphere.
Because there is so little to distinguish CO2 from the most abundant gas molecules in Earth’s atmosphere, Dr. Allmendinger assesses “a significant efect of carbon-dioxide on the direct sunlight absorption can already be exluded.”
Further, “the greenhouse theory has to be questioned.”

Image Source: Allmendinger, 2016
Share this...FacebookTwitter "
