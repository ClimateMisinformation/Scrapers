"Few sights and sounds are as emblematic of the North American southwest as a defensive rattlesnake, reared up, buzzing, and ready to strike. The message is loud and clear, “Back off! If you don’t hurt me, I won’t hurt you.” Any intruders who fail to heed the warning can expect to fall victim to a venomous bite.  But the consequences of that bite are surprisingly unpredictable. Snake venoms are complex cocktails made up of dozens of individual toxins that attack different parts of the target’s body. The composition of these cocktails is highly variable, even within single species. Biologists have come to assume that most of this variation reflects adaptation to what prey the snakes eat in the wild. But our study of the Mohave rattlesnake (Crotalus scutulatus, also known as the Mojave rattlesnake) has uncovered an intriguing exception to this rule. A 20-minute drive can take you from a population of this rattlesnake species with a highly lethal neurotoxic venom, causing paralysis and shock, to one with a haemotoxic venom, causing swelling, bruising, blistering and bleeding. The neurotoxic venom (known as venom A) can be more than ten times as lethal as the haemotoxic venom (venom B), at least to lab mice.  The Mohave rattlesnake is not alone in having different venoms like this – several other rattlesnake species display the same variation. But why do we see these differences? Snake venom evolved to subdue and kill prey. One venom may be better at killing one prey species, while another may be more toxic to different prey. Natural selection should favour different venoms in snakes eating different prey – it’s a classic example of evolution through natural selection. This idea that snake venom varies due to adaptation to eating different prey has become widely accepted among herpetologists and toxinologists. Some have found correlations between venom and prey. Others have shown prey-specific lethality of venoms, or identified toxins fine-tuned for killing the snakes’ natural prey. The venom of some snakes even changes along with their diet as they grow.  We expected the Mohave rattlesnake to be a prime example of this phenomenon. The extreme differences in venom composition, toxicity and mode of action (whether it is neurotoxic or haemotoxic) seem an obvious target for natural selection for different prey. And yet, when we correlated differences in venom composition with regional diet, we were shocked to find there is no link. In the absence of adaptation to local diet, we expected to see a connection between gene flow (transfer of genetic material between populations) and venom composition. Populations with ample gene flow would be expected to have more similar venoms than populations that are genetically less connected. But once again, we drew a blank – there is no link between gene flow and venom. This finding, together with the geographic segregation of the two populations with different venoms, suggests that instead there is strong local selection for venom type.  The next step in our research was to test for links between venom and the physical environment. Finally, we found some associations. The haemotoxic venom is found in rattlesnakes which live in an area which experiences warmer temperatures and more consistently low rainfall compared to where the rattlesnakes with the neurotoxic venom are found. But even this finding is deeply puzzling.  It has been suggested that, as well as killing prey, venom may also help digestion. Rattlesnakes eat large prey in one piece, and then have to digest it in a race against decay. A venom that starts predigesting the prey from the inside could help, especially in cooler climates where digestion is more difficult.  But the rattlesnakes with haemotoxic venom B, which better aids digestion, are found in warmer places, while snakes from cooler upland deserts invariably produce the non-digestive, neurotoxic venom A. Yet again, none of the conventional explanations make sense. Clearly, the selective forces behind the extreme venom variation in the Mohave rattlesnake are complex and subtle. A link to diet may yet be found, perhaps through different kinds of venom resistance in key prey species, or prey dynamics affected by local climate. In any case, our results reopen the discussion on the drivers of venom composition, and caution against the simplistic assumption that all venom variation is driven by the species composition of regional diets. From a human perspective, variation in venom composition is the bane of anyone working on snakebite treatments, or antidote development. It can lead to unexpected symptoms, and antivenoms may not work against some populations of a species they supposedly cover. Anyone living within the range of the Mohave rattlesnake can rest easy though – the available antivenoms cover both main venom types.  Globally, however, our study underlines the unpredictability of venom variation, and shows again that there are no shortcuts to understanding it. Those developing antivenoms need to identify regional venom variants and carry out extensive testing to ensure that their products are effective against all intended venoms."
"For nearly 40 years, black coal has been mined at Myuna, an underground operation a short drive south-west of Newcastle. Each year about 2 million tonnes is dug up, dropped on to an overland conveyor and sent to the Eraring power plant next door to be burned. Although the New South Wales mine isn’t new, its operation under owner Centennial Coal has changed over the past couple of years, leading to a dramatic increase in greenhouse gas escaping its coal seams.  Emissions at the mine in 2017-18 were 65% above the government-agreed limit for the site. New data published just before Christmas show Centennial was also in breach last financial year, with carbon pollution at Myuna 47% above its limit. In an era in which political battles are fought over how to meet climate targets, an emissions rise of this proportion – nearly half a million tonnes at one site over just a couple of years – is noteworthy. Along with similar examples at other industrial sites, including those owned by BHP, Chevron and a range of other fossil-fuel companies, it helps explain why official data says the Morrison government will fail to cut emissions to 5% below 2000 levels in 2020 as it claims. It was not supposed to be this way. Among the issues on the agenda for the Morrison government this year as it considers where to head on climate policy is whether this trend in industrial emissions can continue. How did we get here? Back in 2015 when Tony Abbott was still prime minister, the Coalition released details of what it called a “safeguard mechanism”, a policy it said would deal with the problem of rising pollution from big industry. Greg Hunt, the environment minister who designed the policy, told ABC Radio National it would put “a limit on the emissions that individual firms can have”. Specifically, it would set a limit – a baseline – for about 140 industrial facilities that emitted more than 100,000 tonnes each year. While the government chose not to emphasise the point, companies that breached their baseline at a particular site without federal approval would have to pay by buying carbon credits created through cuts elsewhere. As Hunt described it, the safeguard mechanism was half the Coalition’s direct action climate policy (a name long since dropped). The first half of direct action was the emissions reduction fund, a $2.5bn incentive scheme under which the government would pay for pollution cuts, mostly from landowners who signed up to plant or look after native vegetation. The second half would “safeguard” those cuts – ensure they were not just wiped out – by preventing “significant increases in emissions above business-as-usual levels” elsewhere. In practice, the scheme has run quite differently. In the case of Centennial Coal, the Australian Conservation Foundation found it could reasonably have been expected to pay more than $6m to offset its extra emissions, based on the price the government pays for carbon credits. Instead it followed rules set up by the government that allowed it to retrospectively apply for a change to its baseline arrangements. Most people paying attention to the scheme have been left to wonder: why have a policy to limit emissions that routinely allows companies to ignore their limit? Suzanne Harter, a climate campaigner with the ACF, is among those who says it makes no sense. “If the government keeps increasing the pollution baselines, there is no point to the safeguard mechanism,” she says. Tennant Reed, who runs climate, energy and environment policy for the Australian Industry Group, which represents the interests of more than 60,000 businesses, agrees. “If it never has to do something to actually reduce emissions it will have been a waste of time for everyone involved,” he says. The numbers tell a pretty basic story. In the first two years of the scheme, analysts at energy and carbon consultancy RepuTex found the regulator had approved changes that allowed big industry to emit up to 32% more without penalty than when the safeguard was introduced. Companies used only some of this additional headroom. Actual emissions under the scheme rose 12% over those two years. Based on this, RepuTex found the safeguard was likely to lead to an extra 280m tonnes of pollution over the next decade, more than six months’ worth of Australia’s total carbon pollution. It would more than eclipse the 193m tonnes of cuts contracted under the emissions reduction fund. RepuTex’s executive director, Hugh Grossman, said it meant taxpayers’ dollars spent on storing carbon dioxide in vegetation was “effectively money going down the drain”. The rise in emissions under the safeguard mechanism reflects a longer-term trend. In a separate report, RepuTex found industrial emissions had risen 60% since 2005, the year against which the government has pledged at least a 26% cut by 2030. The surge has been driven by the creation of a $50bn liquefied natural gas export industry across northern Australia and increases in direct combustion at mining sites, venting of fugitive emissions in fossil fuel extraction and pollution from metals, chemicals and minerals processing. The rise in industrial emissions is the primary reason national emissions have stubbornly refused to fall while there has been a historic drop in pollution from power plants and a significant dip from farming due to the drought. That could start to change this year as more of the recent record investment in solar and wind power, spurred by the now-reached national renewable energy target, is comes online. But analysts say Australia cannot hope to meet its climate targets while major industry is left unchecked. The industrial sector is expected to pass power generation to become the country’s most polluting sector within two or three years. A key question for the government is whether it intends to address this as it considers new climate policies this year. One option likely to be before it will be a recommendation in a review of its climate policies led by the businessman Grant King, that quietly submitted its report earlier this month. In a discussion paper sent to some interest groups late last year, King’s panel floated changing the safeguard mechanism so companies that emit less than their baseline would be rewarded with carbon credits they could sell to the government or business. The scheme was clearly was designed with that in mind. In 2015 Hunt said it would be used to cut emissions by 200m tonnes over the decade to 2030. It implied limits would be enforced and tightened so companies had to either reduce pollution over time or trade in carbon credits to offset it. In other words, a return to a form of carbon pricing. That idea was dropped before the last election - the safeguard mechanism was not included among policies that would be used to meet Australia’s commitments under the Paris climate agreement. The current minister for emissions reduction, Angus Taylor, says the mechanism is “designed to support growth while encouraging businesses to lower their emissions intensity”. The shift proposed by King in last year’s discussion paper would come with challenges – for example, working out how to guarantee that businesses were rewarded for changes in practice that cut pollution, and not for things over which they had no control, such as a downturn in the economy. It would also require the government to commit to forcing industry to reduce pollution over time, a shift seemingly at odds with Morrison’s “technology over taxation” mantra. But the shift could have the support of the Australian Industry Group, which backs turning the safeguard mechanism into a meaningful emissions policy as long as it includes protection for export industries so they are not disadvantaged against overseas competitors. Reed says if there is an intention to reduce industrial emissions, ratcheting down baselines under the safeguard mechanism is an obvious option. “That would be a substantial step, and a substantial change,” he says. “The safeguard mechanism could be part of how we get there, but right now it’s not doing much other than creating paperwork for industry and the government.” The government has already begun to make changes to the safeguard that would allow that sort of shift. When the scheme began, emissions baselines were initially based on either a facility’s historic emissions or an independent forecast of future pollution. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on emissions intensity – how much they emit each unit of production. In one sense this just locks in what is already happening in cases such as the Myuna colliery – if companies lift production they will be able to put out more carbon pollution without risking a penalty. But emissions intensity baselines could also be more obviously reduced over time to drive a shift to cleaner practice without putting pressure on production levels. This is part of what Labor proposed to do – without releasing much detail – when it said before last year’s election it would cut emissions under the safeguard by 45% by 2030. Erwin Jackson, policy director with the Investor Group on Climate Change, believes the government can delay action on industrial emissions for only so long. The longer it waits, the greater the risk of a more “dramatic and draconian” shift as the pressure to act – from investors, from other countries, from the planet – escalates. He says the Australian climate debate remains stuck on the idea that pumping emissions into the atmosphere does not have a cost that will be borne eventually, one way or the other. “To address climate change you have to reduce emissions, and someone has to pay for that. Investors are already pricing this risk,” he says. “The longer we delay action the higher the cost will be.”"
"
Share this...FacebookTwitterWhile German politicians, alarmist scientists, activists, and media are staying super-glued stuck on stupid, i.e. remaining mired in the stupidity of dogmatism and closed-mindedness, the climate debate and controversy in Germany is, well, shall we say, heating the hell up.Mark the following time and place on your calender:
Wednesday, 25 May 2011, 10 pm.
http://www.mdr.de/mdr-figaro/
Once facing a hostile climate of intolerance and threats, German climate-catastrophe-skeptic scientists are increasingly coming out and choosing to exercise their human right to express themselves freely, without fear of mobbing and bashing. Good for them I say. It’s past high time.
“Do politicians really know what they are talking about?”
Hans von Storch’s Klimazwiebel site here informs us that German MDR public radio station will broadcast a special on climate change, Wednesday evening at 10 p.m. The show is produced by Kai-Uwe Kohlschmidt and according to the MDR website here, its description (emphasis added):
The show looks at just how much is climate change caused by man, or is it more a cyclic phenomena? The science is everything but in agreement when it comes to the interpretation of the huge number of facts, theories and model predictions of weather. There are plenty of loud and serious voices out there claiming climate swindle. The mainstream media are purveying a clear picture of coming catastrophe that is freshened up on a daily basis. When politicians call on us to prevent climate change, do they really know what they are talking about?

Kai-Uwe Kohlschmidt provides a look into the jungle of science, the media focus and political correctness, and invites you on a playful science expedition to Spitzbergen, Masdar, the Brandenburg Lindenberg and other locations.
Directed by: Kai-Uwe Kohlschmidt; Holger Kuhla
Production: RBB 2011


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany, on the subject of climate change, “do they really know what they are talking about” is one of the most provocative questions that’s been posed in a long time. Let’s hope that this show is really serious about being fair and balanced. I have no reason to doubt that they won’t be, and look forward to listening in.
I can almost hear Messieurs Schellnhuber and Rahmstorf screeching, at high pitches, to MDR in a bid to get the station to drop the skeptics and to stop spreading Big Oil’s and Fred Singer’s “disinformation”. But what else should we expect from zealot dogmatists who are hopelessly super-glued on stupid.
Klimazwiebel has a list of scientists who will be featured on the show:
Spitzbergen
Prof. Hauke Trinks (Sea Ice Researcher)
Prof. Steve Coulsen (UNIS Terrestial Ecology)
Prof. Jørgen Berge (UNIS Marine Biology)
Andreas Umbreidt (Terra-Polaris)
Abu Dhabi / Masdar City
Joachim Kundt (CEO Abu Dhabi Siemens)
Rene Umlauft (CEO Renewable Energies Siemens)
Dolf Gehlen (CEO International Renewable Energy Agency)
Germany
Christoph Hein (Writer)
Mike Kess / Udo Schulze (Citizens’ Initiative “CO2 Endlager”)
Dr. Franz Berger (Weather Station Lindenberg )
Prof. Hartmut Grassl (Hamburg Max Planck Institute)
Dr. Wolfgang Thüne (Meteorologist)
Prof. Dr. Werner Kirstein (Institute for Geography, University of Leipzig)
Prof. Friedrich Wilhelm Gerstengarbe (Potsdam Institute For Climate Impact Research)
Michael Limburg  (European Institute for Climate and Energy)
Dr. Joachim Bublath (Science Publicist)
Prof. Hans von Storch (Institute for Coastal Research, Geestacht)
Prof. Jan Veizer (Evolution Geologist, University of Ottawa)
Dr. Nico Bauer  (Potsdam Institute For Climate Impact Research)
Prof. Dr. Claudia Kemfert (German Institute for Economics)
Plenty of warmists, but still with a number of skeptics. MDR has framed the description in a way that tells listeners that not all is well in climate science. It’ll be interesting to see if they deliver on this. Let’s hope so.
Hope to have interesting results to report on Thursday.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Ed Caryl
The California Independent System Operator (ISO) manages the high-voltage wholesale power grid for the state of California. On their web site, they have several links including one to yesterday’s hourly breakdown of power usage and sources. The figures below are for May 25 2011.

Figure 1. The hourly breakdown of renewable power fed to the California grid. Source: California ISO.
 http://www.caiso.com/outlook/SystemStatus.html
In California, most wind power is generated in three high wind locations: Altamont pass east of San Francisco, Tehachapi pass east of Bakersfield, and San Gorgonio pass near Palm Springs. Notice the huge drop in the wind farm output centered on 10 AM local time. The wind power output at all the wind farms dropped from over 1800 Megawatts to less than 200 Megawatts in less than six hours. Solar power picked up about 400 Megawatts of that, but solar had a glitch of it’s own at about 5 PM, when a cloud obscured the sun at the major solar plant in the Mojave Desert. The grid had to replace this power, just when the load was reaching maximum in the middle of the day. Where did the backup power come from? As you can clearly see in figure 1, none of the backup power came from a renewable source.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2. The hourly breakdown of all electrical power production in California on May 25th 2011. Source: California ISO, same link as above.
California must import up to 33% of its power, most from thermal and nuclear plants in other southwest states, and some from hydroelectric dams in the Pacific Northwest. As you can see from the plot in figure 2, most of the slack when the wind died came from thermal power plants in California and imported power from other states. Nuclear plants are difficult to throttle up and down, so most of that make-up power came from thermal sources (fossil fueled).
To be available on a moments notice, (or even on a few hours notice) thermal power plants are on “standby” status. In most cases this means that they have turbines already turning, feeding minimal power to the grid, so that they can be “throttled up’ quickly. The fastest responding are the natural gas powered plants. Coal powered plants take a bit longer to be fired up.
Planning is a big part of managing a power grid. The load can be predicted with good accuracy. Even fluctuations in temperature affecting load are predicted more than 24 hours in advance. But wind is more difficult, and clouds over solar plants more difficult yet. Today’s wind, for instance, is fluctuating over a scale of minutes, giving power output fluctuations of 200 Megawatts in 30 minutes. This must be giving the operators headaches.
California plans to build renewable power resources to the tune of 33% by 2020. The Pacific Northwest already has grid problems with Oregon and Washington wind farms. California will need three to five thousand Megawatts of reserve fossil fueled or hydroelectric plants to back up the renewable power resources. Given the May 25th 2011 wind power glitch, that may be low.
Share this...FacebookTwitter "
"
From CNN An earthquake with a magnitude of 7.9 struck in the Samoan Islands region Tuesday, the U.S. Geological Survey said.

The temblor generated a nearly 10-foot (3-meter) tsunami — measured from crest to trough — according to preliminary data, said Chip McCreery, the director of the Pacific Tsunami Warning Center in Ewa Beach, Hawaii.
BULLETIN
TSUNAMI MESSAGE NUMBER   2
NWS PACIFIC TSUNAMI WARNING CENTER EWA BEACH HI
857 AM HST TUE SEP 29 2009
TO – CIVIL DEFENSE IN THE STATE OF HAWAII
SUBJECT – TSUNAMI WATCH SUPPLEMENT
A TSUNAMI WATCH CONTINUES IN EFFECT FOR THE STATE OF HAWAII.
AN EARTHQUAKE HAS OCCURRED WITH THESE PRELIMINARY PARAMETERS
NOTE MOMENT MAGNITUDE INCREASE TO 8.3
ORIGIN TIME – 0748 AM HST 29 SEP 2009
COORDINATES – 15.3 SOUTH  171.0 WEST
LOCATION    – SAMOA ISLANDS REGION
MAGNITUDE   – 8.3  MOMENT
MAGNITUDE   – 8.0  RICHTER (MS)
MEASUREMENTS OR REPORTS OF TSUNAMI WAVE ACTIVITY
GAUGE LOCATION        LAT   LON    TIME        AMPL         PER
——————-  —– ——  —–  —————  —–
APIA UPOLU WS        13.8S 171.8W  1832Z   0.70M /  2.3FT  08MIN
PAGO PAGO AS         14.3S 170.7W  1812Z   1.57M /  5.1FT  04MIN
LAT  – LATITUDE (N-NORTH, S-SOUTH)
LON  – LONGITUDE (E-EAST, W-WEST)
TIME – TIME OF THE MEASUREMENT (Z IS UTC IS GREENWICH TIME)
AMPL – TSUNAMI AMPLITUDE MEASURED RELATIVE TO NORMAL SEA LEVEL.
IT IS …NOT… CREST-TO-TROUGH WAVE HEIGHT.
VALUES ARE GIVEN IN BOTH METERS(M) AND FEET(FT).
PER  – PERIOD OF TIME IN MINUTES(MIN) FROM ONE WAVE TO THE NEXT.
EVALUATION
BASED ON ALL AVAILABLE DATA A TSUNAMI MAY HAVE BEEN GENERATED BY
THIS EARTHQUAKE THAT COULD BE DESTRUCTIVE ON COASTAL AREAS EVEN
FAR FROM THE EPICENTER. AN INVESTIGATION IS UNDERWAY TO DETERMINE
IF THERE IS A TSUNAMI THREAT TO HAWAII.
IF TSUNAMI WAVES IMPACT HAWAII THE ESTIMATED EARLIEST ARRIVAL OF
THE FIRST TSUNAMI WAVE IS
0111 PM HST TUE 29 SEP 2009
MESSAGES WILL BE ISSUED HOURLY OR SOONER AS CONDITIONS WARRANT.
h/t to Hotrod “Larry”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93191e82',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A sliver of hope against a backdrop of gloom: 18 countries showed a sustained decline in their carbon emissions from fossil fuel use over the past decade. This trend, averaging 2.2% a year over the period 2005-2015, is evident in less than 10% of the world’s countries, mostly in the EU, but accounts for 28% of global emissions. Our new research published in Nature Climate Change explains why these 18 countries have downward emission trends. This work involved an international team of researchers led by Corinne Le Quéré from the Global Carbon Project and the Tyndall Centre at the University of East Anglia. We found that part of the answer was country specific. For instance, in the US, a fracking boom meant coal was replaced by gas, while Eastern European states joined the EU and cleaned up their inefficient infrastructure. However, there are three common elements shared across these distinct national histories: the declining relevance of fossil fuels, a fall in energy demand, and strong national policy frameworks. We began by unpicking the drivers of falling CO₂ emissions in the sample of 18 “peak-and-decline” countries. We looked at four factors: lower energy use, such as more efficient vehicles, appliances or homes; lower share of fossil fuels in energy generation, thanks to new renewable or nuclear power; improved fossil fuel utilisation rate, for instance through leaking less electricity from cables or overhead lines; and lower carbon-intensity of fossil fuels, typically caused by switching from coal to gas. We found that a declining share of fossil fuels was responsible for about half of the fall in emissions, with a further third attributable to a decrease in energy use. The relative emphasis of these two factors varied across countries. Whereas Austria, Finland and Sweden relied more heavily on decarbonising their energy mix, Ireland, the Netherlands and the UK saw a stronger effect from reducing energy consumption. In general, though, both factors were important across the board. To check our claims were robust, we examined two other factors that may have influenced the results. The first was the global finalcial crisis, which we found did cause a slowdown in economic growth in the peak-and-decline countries, enough to partly explain some of the decline in energy use. Next was the well-documented effect of consumption in developed economies driving up emissions in industrialising economies – are Europeans who buy clothes or TVs made in China simply “outsourcing” their emissions? In our 18 peak-and-decline countries we found this process had slowed and largely ended prior to 2005, so had no significant effect on our results. Next, we tested whether falling carbon emissions in our 18 countries were associated with policies. We collected data on the numbers of energy efficiency, renewable energy, and climate policies (including frameworks and targets) adopted in law per country during the 2005-2015 study period. In each case, we found that these policy count statistics were strongly and significantly correlated with corresponding energy or emission trends.  To understand whether these findings were unique to the peak-and-decline countries rather than part of a more general phenomena, we repeated our analysis for two “controls”: a group of 31 countries with rising emissions and slow economic growth (such as Japan, Brazil and South Africa), and a group of 30 countries with rising emissions and fast economic growth (such as Turkey, India, and China). As expected, we found that none of the policy variables in the control group countries were significantly correlated with energy and emissions trends. Correlation is not causation. We cannot claim that national climate policies are directly responsible for falling emissions. And counting policies does not account for their stringency, enforcement, and credibility, all of which are important. However, there is some precedent in the literature for using count statistics to assess the effect of policy on emission reductions both in Europe and the US. Our findings are also consistent with political science, innovation studies, and energy assessments, which repeatedly emphasise the importance of stable policy environments for low-carbon innovation and clean energy deployment.  Another important takeaway from our research is the importance of energy use. Renewables, nuclear, fracking, coal, and carbon capture and storage tend to hog the headlines as well as the attention of policymakers. But our analysis found that even double digit growth rates for renewables did not make a dent in rising emissions in those countries with rapidly expanding energy systems dominated by fossil fuels as new solar panels or wind turbines were simply being added at the margins. In contrast, downsizing the entire energy system by reducing demand makes the whole process of decarbonisation much more manageable."
"Every budget is billed in advance as the most crucial in recent times, but then most are instantly forgotten. The one Rishi Sunak will deliver in just over two weeks’ time may be one of the few that justifies the hype. The reason so much is resting on the shoulders of the tyro chancellor is that the budget needs to satisfy a number of different audiences: the voters in the Midlands and the north of England who gave Boris Johnson his 80-seat majority; traditional Conservative voters; the financial markets; and foreign governments looking to see whether the UK will take a lead before the Cop26 climate change conference in Glasgow in November. A package that pushes all the necessary buttons is not going to be easy. To take one example, making tax relief on pension contributions less generous for those on higher incomes would help the chancellor’s sums add up and win credibility with the financial markets but antagonise the Tory party’s natural supporters. Sunak’s immediate task is to announce targets for the public finances that are easier to hit than the ones currently in place, but not so weak that the markets take fright. Moving the goalposts will give the government more scope to borrow for infrastructure projects that need to be underway soon if they are to be completed in time to deliver a political dividend for Johnson at the next general election. But unless he can also find a way of making the budget consistent with the government’s 2050 net zero carbon target for the economy a diplomatic failure of catastrophic proportions looms at the end of the year. The Cop26 is the most important summit the UK has hosted since the G8 met at Gleneagles in 2005 – and the task facing the government is much more daunting than it was then. The Gleneagles summit was all about the rich countries of the west agreeing to provide debt relief and higher levels of aid for poor nations. Most of the debts would never have been paid anyway and the doubling of aid was easily affordable at a time when the global economy was booming. Even so, it took a lot of time and effort to chisel out a deal. The then prime minister, Tony Blair, and the then chancellor, Gordon Brown, both lobbied hard to overcome resistance to their plan, expending plenty of political capital in the process. Public opinion – channeled through the Make Poverty History campaign – was effectively mobilised. Crucially, the Labour government showed leadership by committing to the UN target to spend 0.7% of national income on aid. A deal in Glasgow is going to be immensely more difficult than it was up the road in Perthshire 15 years ago. For a start, there are many more countries involved. For another, some of the biggest players are actively hostile to the idea of setting tougher emissions targets. The contrast between George W Bush – who was interested in Africa – and the climate emergency denier who currently occupies the White House is stark. But the US is not going to be alone in Glasgow: Brazil, Australia and Saudi Arabia will all prove hard to break down. After failing to persuade David Cameron to do the job, Johnson has put the business secretary, Alok Sharma, in charge of summit preparations. But Sharma does not have the heavy-hitting international reputation that is going to impress other governments. That will require Johnson to demonstrate his personal commitment to making Glasgow a success. All of which brings us back to the budget, which provides an opportunity for the government to announce measures that will accelerate the UK’s progress towards a decarbonised economy. These need to be more than the mooted increase in fuel duty. The Green New Deal Group (of which I am a member) has estimated it will cost about £100bn a year for 20 years to make the transition to a net zero carbon economy. Investment on that sort of scale would be necessary to make the UK’s 30m buildings energy efficient, turn buildings into power stations through the use of solar panels, and invest in renewable energy. So where’s the money going to come from? One answer would be a form of green quantitative easing – money creation by the Bank of England that would pay for the decarbonisation of the economy rather than, as was the case during and after the financial crisis, being pumped into the banking system. The government doesn’t seem keen on this approach, even though there are plenty of economists who think it is wholly feasible. Another possibility would be for the government to borrow the money in the usual way, but this doesn’t appeal to ministers either. There is, though, a third option. At present about £100bn year is paid into pension schemes, all of it eligible for tax relief currently worth £54bn a year. There is also tax relief on the £70bn a year invested in Isas. The GND proposal is that 25% of pension contributions should go into green new deal investment in exchange for that tax relief and that all new Isa contributions – which currently go into cash or shares – should be invested in green new deal bonds issued by the government at a guaranteed rate of interest. The idea is to provide a stream of income to transform the economy as well as offering a new secure investment vehicle for savers. Insurance companies and pension funds no longer risk being left with stranded fossil fuel assets and the City would be the place to do green finance. Above all, a strong signal of intent would be sent to the rest of the world."
"**Christmas is coming but how can you celebrate it without giving the unwelcome gift of coronavirus?**
Cosy rooms packed with people, chatting, laughing, even singing, and sharing food and drink for hours are the norm for the festive season.
Unfortunately, almost everything that's great for lifting our spirits at Christmas is also ideal for fuelling the pandemic.
So here are the key questions to ask about any festivities.
No-one will be popular for saying this, but the evidence is clear - the larger the group, the greater the risk.
If it was summer and we could meet outside, where the virus gets dispersed in fresh air, it would be less of a problem. But it's winter, so everyone's inside.
And the more people who are involved, the greater the likelihood that someone may be a carrier of the virus - maybe without realising.
A study by Sage, the government's science advisory panel, concludes that if you double the number of people getting together, you get a fourfold increase in the odds of infection.
It also matters how many different households are meeting - the fewer the safer - because the more different homes which are mixing, the greater the potential for the virus to spread.
Passing around dishes and bottles, encouraging everyone to tuck in, is one of the most natural of instincts at Christmas.
But the coronavirus can survive on surfaces, possibly for several hours, so plates and cutlery can become contaminated, which means you could be handing round the virus as well as the sprouts.
In the US, the official advice for this year's Thanksgiving dinners is to break totally with tradition by asking guests to bring their own food and drinks.
It's also recommended that you control who's allowed in the kitchen, with one lucky person doing all the serving.
Amid the excitement of reunions, it's perfectly normal for voices to be raised.
Add a little alcohol, and maybe have a TV or music on as well, and things get even noisier.
But if someone is infected, the louder they speak, the more virus they release.
A lot of research shows that when voices are projected, people emit more tiny droplets of the kind that can carry the coronavirus.
That's why for Thanksgiving gatherings, the US government advises: ""Encourage guests to avoid singing or shouting, especially indoors.
""Keep music levels down so people don't have to shout or speak loudly to be heard.""
Maybe the safest option is to hum Silent Night.
Popping in for a quick visit is safer than lingering over dinner for several hours.
Researchers say an event's duration has a big impact on the infection risk.
In March, more than 50 members of a choir in the US were confirmed or suspected of being infected, after a two-and-a-half-hour rehearsal session.
Scientists reckoned that if it had lasted less than an hour, the number of infections would have been reduced by more than half.
Prof Cath Noakes, one of the study's authors, says the problem is that tiny particles carrying the virus, known as aerosols, can accumulate in the air.
""There is growing evidence that if you're in a poorly-ventilated space for a long period of time with people who are infected, you may breathe in those aerosols and that might be one of the routes of infection,"" she said.
The obvious answer is: ""Of course not, it's too cold outside.""
But fresh air dilutes any virus that might be lingering in a crowded room.
A Sage report says infection risks can be increased by four times without proper ventilation.
And in this context, ""ventilation"" doesn't mean having fans blowing the air around, but a flow of air from outside.
And if people feel too cold? Wear another layer.
According to Prof Noakes, people must dream up creative new options for Christmas
That could range from meeting virtually on Zoom, going for a walk, braving the weather for a picnic, or even delaying big gatherings until next summer.
If you are planning a meal indoors, she says, make sure you keep everyone as far apart as possible and be careful to keep everything clean.
Also try to avoid having people from different households sitting opposite each other because speaking face-to-face is a route of transmission.
Any social setting increases the risk of infection, says Prof Noakes, and we will have to compromise, reduce our contacts with people and do things in a different way.
""The virus doesn't know it's Christmas,"" she says. ""It's just a virus and it thrives on human contact."""
"

Now that the election’s finally over, the Clinton administration has a last chance to do some real damage to George W. Bush’s economy.



President Clinton believes passionately that part of his legacy will be to put in place a mechanism that will forever mire America in the United Nations’ infamous Kyoto Protocol on global warming. Last month, the signatories met at The Hague, where Clinton proposed that we meet almost 90 percent of our obligations to reduce net emissions of major greenhouse gases by cutting energy use. Originally, the United States had proposed to lock up 50 percent of such emissions through trees and soil management — a relatively inexpensive proposition — but the European Union insists on emissions reductions, a course that will cause us grave economic harm.



So we caved all the way to a 90 percent reduction, and the EU still said no, we need more. Then, last week, the Clinton administration tried again in a closed‐​door meeting in Ottawa, Canada. Still no agreement. Finally, on Dec. 13, Norway’s Environment Ministry invited everyone to Oslo — before Christmas — for a third try.



This will be the last go‐​round, and Clinton has every incentive to give away the store. The result is a twofold “legacy” — being the first U.S. leader to commit to major reductions in greenhouse gases, and saddling the incoming president with a massive political and economic burden that will have absolutely no detectable effect on global weather and climate.



The political gains are obvious. Bush either gets clobbered in 2004 or the Republicans suffer in 2008. While Kyoto agreements go into force in 2008, major taxes and infrastructural changes have to begin long before then to meet these massive reductions in energy use. First, say good‐​bye to affordable electricity. Currently 56 percent of our juice is produced by burning coal, but because it emits a bit more greenhouse gas per unit energy than natural gas (which costs more), well, coal’s gotta go.



California, as usual, is leading the way here. Thanks to a moratorium on production of fossil fuel power plants, they’re out of power. It is a sad day when our Grinch‐​green friends compel us to turn off the Christmas lights, but that is the case right now in Los Angeles.



Second, we hope you like your new hybrid automobile. The technology’s really cool. My Honda Insight really does get 70 miles per gallon on a good day, and it is an engineering marvel. The only problem is that Honda’s losing at least $8,000 per car, and the company only sold 3,502 through November. It seats two comfortably.



So either we’re going to have to pay about 50 percent more for a mid‐​range hybrid car, or we’re all going to have to make it up in taxes to subsidize those who do buy them. And it might require quite a subsidy, too. Insight sales in November, at 291 units, were down 40 percent from August, despite giveaway prices.



Third, the $2‐​a‐​gallon gas of spring 2000 will be just a fond memory, thanks to the taxes required to discourage enough consumption to make you buy that subsidized hybrid.



High gas prices, tax‐​mandated technology, and dark Christmas trees are not the correlates of political popularity. But that is exactly where Clinton could force Bush to go if he gives away the store in Oslo.



All this for an agreement, the Kyoto Protocol, that is not the law of the land. It hasn’t been ratified by the Senate, and it stands little chance. And even if it were in force, the Clinton administration’s own scientists say it would only change global temperature by seven hundredths of a degree in 50 years. That’s too small to measure.



There will almost certainly be some weather disaster during the Bush administration. Right now, the insured value of property along the East Coast is almost equal to our annual Gross Domestic Product. We haven’t had a Category 5 hurricane hit since 1969. Even a lower Category 4, well‐​aimed, will cause unimaginable destruction. Federal scientist Christopher Landsea (the most appropriately named hurricanologist in the world) has shown that even this class of hurricane, if it hits Miami/​Fort Lauderdale, will be good for about $70 billion. On the high end, $100 billion from a Category 5 isn’t out of the question. People will blame global warming rather than admit it’s pretty stupid to sink one’s life savings in a sand dune on a hurricane‐​prone beach.



In Bush Sr.‘s administration, the Senate was adamantly opposed to a different climate treaty — the Montreal Protocol to ban chlorofluorocarbons (CFC) refrigerants. NASA scientist Bob Watson — now the powerful head of the U.N.‘s Panel on Climate Change — announced an imminent ozone hole over North America, and five days later, the Senate passed a ban on CFCs, 99 to 1. A senator by the name of Al Gore whipped up the troops with an impassioned speech about an “ozone hole over Kennebunkport,” Bush’s home.



Never mind that the predicted disaster never happened. NASA had made a measurement error. But Bob and his friend Al had correctly calculated the political trajectory that would bring in the ban on CFCs.



So it can happen, and next week in Oslo, the Clinton administration may sow the seeds that trash the future of George W. Bush.
"
"**The total number of deaths occurring in the UK is nearly a fifth above normal levels, latest figures show.**
Data from national statisticians show there were almost 14,000 deaths in the week ending 13 November.
Some 2,838 of the deaths involved Covid - 600 more than the preceding week, according to the analysis of death certificates.
The North West and Yorkshire have seen the most excess deaths.
The number of deaths in both regions were more than a third above expected levels.
By comparison, the number of deaths in the South East was just 2% above the five-year average.
But there is hope the rise in the number deaths may soon start slowing.
The daily figures published by government - which rely on positive tests - show deaths are not rising as quickly as they were, and may be levelling off.
And unlike in the first wave, when the lack of testing meant the government figures underestimated the number of Covid deaths, the two sets of data are mirroring each other.
Sarah Scobie, of the Nuffield Trust health think tank, said: ""Despite the end of the second national lockdown in England coming into focus, today's figures are a sobering reminder of the dreadful impact of this virus.
She said the high number of deaths was ""piling on the pressure"" on the NHS.
""For some hospitals, particularly in Covid hotspot areas, it will feel as if they are in the depths of winter already."""
"**P &O will not resume cruise sailings until at least April next year.**
The company ended its cruises in March in response to the coronavirus pandemic and has not resumed any of its voyages since.
The Southampton-based firm, which is part of the Carnival group, said the continued pause in operations was because of ""the current uncertainty around European ports of call"".
The latest round of cancellations affects 19 planned cruises.
P&O President Paul Ludlow said: ""With hopeful news headlines clearly we do not want to extend our pause in operations any further than absolutely necessary, but given the ever-changing guidance around international travel and the varying regulations in many European ports of call we felt it prudent to cancel these additional dates.
""In addition, as the final payments are due for these cruises very soon we felt it was the right thing to do for our guests.
""We are so sorry to disappoint those who were due to travel but really hope they will rebook for later in the year or for our new programme of 2022 holidays which went on sale earlier this month with strong demand, showing great confidence in cruising in the future.""
Guests with bookings on cancelled cruises will be offered credit on a future cruise or a refund.
In March, P&O Cruises brought its ships back to Southampton as the pandemic worsened.
Two months later Carnival UK said it planned to cut 450 jobs across P&O Cruises and its other cruise line, Cunard, to ""ensure the future sustainability"" of the business.
In July, P&O Cruises announced it was selling one of its oldest vessels, Oceana.
The sight of idle cruise ships anchored off the south coast became an unusual tourist attraction over the summer."
"
WUWT readers may remember when Bishop Hill wrote Caspar and the Jesus paper. It was a wonderful narrative of the complex subject of tree rings and Steve McIntyre’s quest with debunking the Mann MBH98 paper, which created the original hockey stick. Now Bishop Hill has done it again with another great narrative. – Anthony

The Yamal implosion
September 29, 2009   Climate
There is a great deal of excitement among climate sceptics over Steve McIntyre’s recent posting on Yamal. Several people have asked me to do a layman’s guide to the story in the manner of Caspar and the Jesus paper. Here it is.
The story of Michael Mann’s Hockey Stick reconstruction, its statistical bias and the influence of the bristlecone pines is well known. McIntyre’s research into the other reconstructions has received less publicity, however. The story of the Yamal chronology may change that.
The bristlecone pines that created the shape of the Hockey Stick graph are used in nearly every millennial temperature reconstruction around today, but there are also a handful of other tree ring series that are nearly as common and just as influential on the results. Back at the start of McIntyre’s research into the area of paleoclimate, one of the most significant of these was called Polar Urals, a chronology first published by Keith Briffa of the Climate Research Unit (CRU) at the University of East Anglia. At the time, it was used in pretty much every temperature reconstruction around. In his paper, Briffa made the startling claim that the coldest year of the millennium was AD 1032, a statement that, if true, would have completely overturned the idea of the Medieval Warm Period.  It is not hard to see why paleoclimatologists found the series so alluring.
Keith Briffa
 
Some of McIntyre’s research into Polar Urals deserves a story in its own right, but it is one that will have to wait for another day. We can pick up the narrative again in 2005, when McIntyre discovered that an update to the Polar Urals series had been collected in 1999. Through a contact he was able to obtain a copy of the revised series. Remarkably, in the update the eleventh century appeared to be much warmer than in the original – in fact it was higher even than the twentieth century. This must have been a severe blow to paleoclimatologists, a supposition that is borne out by what happened next, or rather what didn’t: the update to the Polar Urals was not published, it was not archived and it was almost never seen again.
Read the rest here at Bishop Hill’s blog, and be sure to leave a nice comment if you like his writing.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92b07887',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"It’s now possible to predict how likely an endangered species is to go extinct, with mathematical models acting as windows into the future. These models help scientists foresee how a population is likely to react to changes in the environment and therefore how likely it is to die out.  As the sixth mass extinction event rumbles on, this represents a powerful tool in the arsenal of conservationists. However, accurate information about species in the wild is still crucial to inform these models and divine the fate of wildlife populations. A species’ risk of extinction depends on how many individuals in its populations can reproduce and how long they can survive. To design management programmes that can prevent extinction, it’s essential to understand how survival and reproductive rates change within a population as the environment changes. We know that the number of individuals surviving and reproducing – known collectively as a species’ demographic rates – vary each year in response to environmental conditions. These include the availability of food and water, rainfall or extreme temperatures. As global temperatures increase and disrupt local weather, how variable these environmental conditions become will increase the extinction risk for many species. To predict if a population is likely to go extinct, we need to predict how changes in these environmental conditions will affect the population’s demographic rates and how the number of individuals in the population will change annually. 


      Read more:
      Climate change: effect on sperm could hold key to species extinction


 Models can take the likelihood of individuals in the population to survive and reproduce at different ages, and “shock” them in the same way the environment does.  It would be impossible to directly model how species respond to changes in rainfall or other conditions in the environment, as different species thrive under different conditions.  However, these “shocks” in the model – which disturb a species’ survival and reproductive rates – reproduce the random nature of the environment. By running the model many times to generate multiple outcomes, we can calculate what percentage of the populations will go extinct as the environment changes, and how long it will take them to die out.  In our recent study we found that simplifying the information used to construct these models can distort the predictions. Survival and reproductive rates vary in individuals as they age and each species has a very particular trend associated with their age. Until recently, researchers and wildlife managers assumed it was enough to represent these rates as constant as individuals age. This would assume that juveniles and adults all respond uniformly to changes in the environment. However, in our study we showed that reducing a population into general age classes can greatly distort the population’s predicted growth rate and our understanding of a population’s chances of avoiding extinction. Grouping individuals into general age classes can give the illusion that a population’s chances of extinction are slim when in reality, it will fast go extinct. We also found that it’s important to consider trade-offs between survival and reproduction. A good year for reproduction may result in a bad year for survival. This is because more energy invested in reproducing means there’s less left over for the healthy upkeep of the body. Despite increasing evidence of these trade-offs in nature, models tend to ignore them. We showed that ignoring these seemingly minor issues can harm a model’s accuracy in predicting the fate of populations in their natural environment. This reduces their capacity to anticipate the impacts of climate change, invasive species or habitat loss.  There is still hope for many endangered species, but preparation needs to be made. For this, we need to continue developing more sophisticated models with more accurate information about each species and their environment. With reliable foresight, we can give them the fighting chance they need in an uncertain future."
"Sometimes it has felt as if the rain might never stop. These storms have gone beyond the point of simply being storms now, each blurring into the next to create a strangely end-of-days feeling. Everything is freakishly sodden and swollen, and while the rural flood plain on which I live fortunately hasn’t flooded anything like as badly as some, the rivers are rising alarmingly. Yet still the lashing winds and biblical downpours keep coming. Suddenly the 40 Days of Action campaign that Extinction Rebellion (XR) will launch on Ash Wednesday (26 February), encouraging people to reflect on the environmental consequences of their actions in a kind of green Lent, feels ominously well named.  This week’s stunt in Cambridge, where XR activists dug up Trinity College’s lawn in symbolic protest at the college’s plans to build on land it owns in rural Suffolk, may be just the beginning. Some ask why these activists aren’t out stacking sandbags for the poor householders of the Wye valley, or canoeing through the streets of Mytholmroyd, West Yorkshire, highlighting the risks of a climate crisis that can only mean more freak flooding. Yet in some ways that was the point of targeting Trinity in the first place. Of all the Cambridge colleges, it’s the one identified by student journalists – using freedom of information requests – as the biggest investor in fossil fuel companies blamed for aggravating the climate crisis. Activists blockaded a research building run by the oil exploration company Schlumberger as well as making holes in the lawn. The clear aim is to make it toxic for institutions to maintain ties to polluting industries; and what makes universities tempting targets is that they’re already being hammered from inside by students raging against what they see as dirty money. But universities are not alone. This week Amazon chief Jeff Bezos announced he was giving $10bn (£7.7bn) to fight the climate crisis, provoking much the same complaints that greeted BP’s recent vow to go carbon-neutral by 2050: it’s not enough, it’s too vague, it’s just greenwashing. And yes, obviously Bezos should tackle his own company’s carbon footprint first, not to mention treating staff better and paying more tax if he has billions to spare. But until governments have the guts to legislate for all of that, then we are where we are, which is in danger of missing a sea change in corporate life. Executives in polluting industries haven’t quite reached the nadir of bankers after the financial crash, cold-shouldered at school gates and berated in the street, but the more enlightened can see something similar coming if they’re not careful. When BP’s new chief executive, Bernard Looney, made his carbon neutral announcement, following a similar pledge from British Airways, one key factor cited was pressure from staff. It hurts when your company is spurned by the likes of the Royal Shakespeare Company, which ended its sponsorship deal with BP last year amid climate protests, not long after the actor Mark Rylance compared the firm to an arms dealer or tobacco company. At Amazon, too, Bezos had felt the heat internally with hundreds of staff protesting publicly against the company’s links to oil and gas exploration. And if younger staff are making waves now, then the climate will be an even harder red line for the graduates these companies need to recruit in future. Two-thirds of American teens now think oil and gas companies create more problems in the world than they solve, according to a report from management consultants EY. Generation Z want to work for ethical companies that make them feel good about themselves, and increasingly see jobs that fuel climate change as morally suspect. Who wants to spend a first date plaintively explaining why working in Big Oil doesn’t make them a bad person? It may sound ridiculous to their parents’ generation, for whom energy companies were the ones keeping the lights on, but even those with no such qualms must wonder if there’s much future with fossil fuel companies – squeezed between the political rock of legal commitments to hit zero emissions by 2050 and a public hard place that gets harder with every flood or bush fire. It’s not consumer boycotts driving this, so much as social stigma. It’s tough to go without these companies’ products – there was outrage when a bursar at St John’s College, Oxford, responded to student demands to divest immediately from fossil fuels with a tongue-in-cheek offer to turn their heating off if they were that worried – although the intention was to make the students think, not freeze. But noisy public disapproval costs absolutely nothing, which makes it a powerful weapon. Add in shareholder pressure, driven by new government rules requiring pension funds to take account of climate risk, and the heat is really on. Suddenly Bezos’s gesture starts to look positively cheap in comparison with being forced to change his business model. Yet, whatever the motivation, it’s still one of the biggest philanthropic donations in recent history, and it shows which way a howling wind is blowing. How to spend those billions? Bill and Melinda Gates argued this month that private philanthropists should be “swinging for the fences”, taking the big risks governments can’t take with public money. So perhaps the Bezos fund will simply go on a few high-profile scientific gambles. But the radical choice would be to spend some of it funding movements within corporate and institutional life, pushing the foot-draggers to act while they’re still in control of the situation. Better to move fast than wait for activists to dig up your lawn; better to act now, before the river of public anger bursts its banks. • Gaby Hinsliff is a Guardian columnist"
"Imagine “carbon emissions”, and what springs to mind? Most people tend to think of power stations belching out clouds of carbon dioxide or queues of vehicles burning up fossil fuels as they crawl, bumper-to-bumper, along congested urban roads. But in Britain and many other countries, carbon emissions have another source, one that is almost completely invisible. In the UK, these overlooked emissions come from our most extensive semi-natural habitat, yet it is a habitat which is almost invisible within the national consciousness. The source of these emissions can be seen in the rich black peat soils of the East Anglian Fens, the Lancashire lowland plain, the Somerset Levels, the Forth Valley and indeed many lowland river flood plains, as well as in the hugely damaged peat soils of the UK’s uplands. The common thread here is “peat”, a soil derived almost entirely from semi-decomposed plant remains which have accumulated over thousands of years because the ground is waterlogged. Such peat soils are immensely carbon-rich because they largely consist of organic matter. Globally, peatlands contain more carbon than all the world’s vegetation combined.  Despite this, peatlands rarely feature in our cultural consciousness other than as areas of struggle – “stuck in the mire”– or as places of despair or danger. In the uplands, beyond the boundary of cultivated land, extensive peat bogs are lost in the all-embracing term “moorland”, which is more of a cultural term than anything ecologically meaningful. At lower altitudes, living peatland has all but vanished. Britain has drained its fens and converted the land into highly productive fields. Much of East Anglia was once a vast fen peatland, for instance, but just 3% of the original habitat remains today, in small scattered fragments. Such losses are mirrored throughout Europe, while much of the debate about palm oil and forest fires in South-East Asia is actually about the draining and conversion of peatland swamp forest. When peat soils are drained, the ground surface sinks, which is why large parts of East Anglia and the western Netherlands now lie below sea level. This is partly because peat shrinks and becomes more compact when it dries out, but there is also another key reason. Carbon in the now-dry peat reacts with oxygen to form carbon dioxide so each year some of the soil simply vanishes into the atmosphere as a greenhouse gas. While a sinking ground surface does pose ever-increasing flood risk, it is the release of CO₂ that has far wider implications. Every hectare (one and a bit football pitches) of tilled peat soil with a water table lowered to 50 cm or more below the ground surface emits somewhere between 12 and 30 tonnes of CO₂ equivalent (that is, all greenhouse gases, including CO₂) per year. To put this into context, that’s ten times the emissions of an average modern car travelling 10,000 miles per year. In fact, the total CO₂ emitted each year from just the East Anglian Fens and the UK’s damaged upland peat soils may be equivalent to around 30% of the country’s annual car emissions. The irony here is that, although these peat soils were created precisely because they were wetlands, and wetlands are some of the most productive ecosystems on Earth, farming tends to celebrate dryness. Our agricultural system is based on ideas that spread from the dry semi-desert conditions of the Middle East during the Neolithic shift from hunter-gathering to settled farming. Farming has thus been dominated for the past 5,000 years by the principle that dry land is good and wet land is bad – indeed, a farmer who tolerates significant areas of wet ground on the farm is still widely regarded as a poor farmer. Change is in the air, however. International climate obligations mean that countries are having to reduce their greenhouse gas emissions, and in many parts of the world there are also increasing concerns about the spiralling costs of flooding. No wonder many researchers are now looking at the agricultural possibilities of re-wetting former wetlands in order to establish new forms of farming based on productive wetland species.  In Germany, for instance, a type of “bulrush” is already being used to produce fire-resistant building board. At the University of East London we are currently testing two potential crops: sphagnum bog moss as a replacement for peat in garden-centre “grow bags”, and “sweet grass” as a food crop.  In only a few decades, traditional dryland farming on drained peat soils will be increasingly difficult as the rich organic soils vanish and flood prevention becomes too costly. By instead re-establishing wetland conditions, farms could reduce the risk of floods and retain the existing reservoirs of soil carbon but also potentially add new carbon to these long-term stores. Indeed, the longer-term vision of farming for carbon as well as food, and all the other ecosystem benefits that come from healthy peatland ecosystems, may already be upon us. It is part of the UK government’s 25 year environment plan, and environment secretary Michael Gove has pointedly signalled his support.  Such a longer-term vision is also deftly expressed in a film titled “The Carbon Farmer” by Andrew Clark, which looks at what life might be like for a carbon farmer three or four generations from now:  Everything in the film is already at least possible in one form or another. Our task is now to make it probable."
"A worldwide wave of school climate strikes, begun by the remarkable Greta Thunberg, has reached the UK. Some critics claim these activist-pupils are simply playing truant, but I disagree. Speaking as both a climate campaigner and an academic philosopher, I believe school walkouts are morally and politically justifiable. Philosophy can help us tackle the question of whether direct action is warranted via the theory of civil disobedience. This states that, in a democratic society, one is justified in disobeying the law only when other alternatives have been exhausted, and the injustice being protested against is grave. In the case of the climate school strikes, it is without question that the injustice – the threat – is grave. There is none graver facing us. It appears reasonable to claim furthermore that other alternatives have indeed been exhausted. After all, people have been trying to wake governments up to the climate threat for decades now, and we are still as a society way off the pace set out even by a conservative organisation such as the IPCC. But if that claim was strongly contested, and it was suggested that climate activism should continue to focus on conventional electoral politics, then attention might revert to the assumed premise that society is democratic. Do people in Britain and elsewhere really live in “democracies”, given (for instance) the vastly greater power of the rich, and of owners of media, to influence elections, compared to everyone else?  I don’t want to adjudicate whether we really live in a democracy. But what of course makes this a particularly salient question for school strikes is the simple fact that in any case children have no voice in this democratic system. And yet the climate crisis and the perhaps equally catastrophic biodiversity crisis will affect children much more than adults.  Our “democratic” system seems to have a built in present-centricness, and a weakness in relation to issues of long-term significance, that seriously undermines its claims to democratic legitimacy. Thus philosophers have sometimes argued, beginning with Edmund Burke in the 18th century, that to make the system truly democratic we would need to somehow include – and give real power to – the voices of the past and the future in that system. Most especially, for they are at risk of suffering the worst: the voices of children and indeed of unborn future generations. 


      Read more:
      Why don't teenagers have a greater say in their future?


 So, a forceful argument could be made that it must be legitimate for children to take part in climate actions, for they do not even have recourse to the democratic channels (such as they are) that adults take for granted. This is especially true once we add that it seems reasonable for children to object to schooling that may well be rendered irrelevant by a climate-induced catastrophe. For example, much of the way that economics, business studies and IT are taught presupposes a world that will probably soon cease to exist. If you are convinced by this, then all well and good. However, at this point, I want to pull the rug slightly from under the argument that I’ve made so far. I put it to you that, if you are an adult, as I am, then your view in any case is somewhat beside the point.  For the brutal fact is that, try hard though some of us have done, we adults have categorically failed our children. This is a grievous wrong, perhaps the worst thing that mammals, primates, such as ourselves, can do: to have let down those who we claim to love more than life itself. We have set our children on a path to a “future” in which society as we know it may have collapsed. And even if we accomplish an unprecedented societal transformation over the next decade, the massive time-lags built into the climate system mean things will still get worse for a long time to come. And so on this occasion we adults ought to humbly realise that it is no longer for us to tell our children what to do. We ought rather to take up the role of supporting them in their uprising, asking how we can help them in their struggle for survival. They are inspiring us, now. The ultimate reason why we should support these school strikes, as I and hundreds of other UK academics have just declared we will do, is that, through our inaction that has led the world they will inherit to this pretty pass, we adults have forfeited the moral right to do anything else. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
click for larger image
From environmentalist Jennifer Marohasy’s blog in Australia, please pay her a visit here – Anthony
There has been criticism of the potential for official weather stations in the USA to record artificially high temperatures because of the changing environments in which they exist, for example, new asphalt, new building or new air conditioning outlets.   Meteorologist, Anthony Watts, has documented evidence of the problem and Canadian academic, Ross McKitrick, has attempted to calculate just how artificially elevated temperatures might be as a consequence.
A reader of this blog, Michael Hammer, recently studied the official data from the US official weather stations and in particular how it is adjusted after it has been collected.   Mr Hammer concludes that the temperature rise profile claimed by the US government is largely if not entirely an artefact of the adjustments applied after the raw data is collected from the weather stations.
 
 
Does the US Temperature Record Support Global Warming?
By Michael Hammer
IN the US, the National Oceanic and Atmospheric Administration (NOAA) collects, analyses and publishes temperature data for the United States.   As part of the analysis process, NOAA applies several adjustments to the raw data.
If we consider, the above graph, which shows, their plot of the raw data  (dark pink) and the adjusted data (pale pink), it is obvious that the adjustments have little impact on data from early in the 20th century but adjust later temperature readings upwards by an increasing amount.  This means that the adjustments will create an apparent warming trend over the 20th century.  [Click on the above chart for a better larger view, this chart can also be viewed at http://cdiac.ornl.gov/epubs/ndp/ushcn/ndp019.html .]
NOAA state that they adjust the raw data for five factors.  The magnitude of the adjustments are shown in Figure 2.

 
Figure 2.  Form of individual corrections applied by NOAA. The black line is the adjustment for time of observation.  The red line is for a change in maximum/minimum thermometers used.  The yellow line is for changes in station siting. The pale blue line is for filling in missing data from individual station records. The purple line is for UHI effects (this correction is now removed).  [Click on the chart for a better larger view or visit the same website as for Figure 1.]
It is obvious that the only adjustment which reduces the reported warming is UHI which is a linear correction of 0.1F or about 0.06C per century, Figure 2.  Note also that the latest indications are that even this minimal UHI adjustment has now been removed in the latest round of revisions to the historical record.  To put this in perspective, in my previous article on this site I presented bureau of meteorology data which shows that the UHI impact for Melbourne Australia was 1.5C over the last 40 years equivalent to 3.75C per century and highly non linear.
Compare the treatment of UHI with the adjustments made for measuring stations that have moved out of the city centre, typically to the airport.  These show lower temperatures at their new location and the later readings have been adjusted upwards so as to match the earlier readings.  The airport readings are lower because the station has moved away from the city UHI.  Raising the airport readings, while not adding downwards compensation for UHI, results in an overstatement of the amount of warming. This would seem to be clear evidence of bias.  It would be more accurate to lower the earlier city readings to match the airport readings rather than vice versa.
Note also the similarity between the shape of the time of observation adjustment and the claimed global warming record over the 20th century especially the steep rise since 1970.  This is even more pronounced if one looks at the total adjustment shown in Figure 3 (again from the same site as Figure 1).  As a comparison, a recent version of the claimed 20th century global temperature record downloaded from  www.giss.nasa.gov is shown in Figure 4.

Figure 3.  Magnitude of the total correction applied by NOAA  
 
[Click on the charts for a larger/better view.]

Figure 4.  Temperature anomaly profile from NASA GISS
Since the total corrections for the US look so similar to the claimed temperature anomaly, it begs the questions as to what the raw data looks like without any corrections.  Does it show the claimed rapidly accelerating warming trend claimed by the AGW advocates?  To determine this I took the raw data from the USHCN graph shown in Figure 1 and plotted this using  a 5 year mean (blue trace), matching the smoothing in the NASA GISS profile shown in Figure 4.  The result is shown in Figure 5.  Please note that while the plot is one that I generated, the data comes directly from the raw data from Figure 1 published by NOAA.

Figure 5  Plot of raw temperature data versus time (from fig 1) 5 point smoothing. Vertical axis degrees Fahrenheit.  Red line is a linear trend line. Green line is a 2nd order (parabolic) trend line. 
Clearly the shape of this graph bears no similarity at all to the graph shown in Figure 4.  The graph does not even remotely correlate to the shape of the CO2 versus time graph.  The warming was greatest in the 1930’s before CO2 started to rise rapidly.  The rate of rise in 1920, the early 1930’s and the early 1950’s is significantly greater than anything in the last 30 years.  Despite the rapid rise in CO2 since 1960, the 1970’s to early 1980’s was the time of the global cooling scare and looking at the graph in Figure 5 one can see why (almost 2F cooling over 50 years).
A linear least squares trend line, created using the Excel trend line function (Red trace)  shows a small temperature rise of 0.09C per century which is far less than the rise claimed by AGW supporters and clearly of no concern.  However, the data shown in figure 5 bears little if any resemblance to a linear function.  One can always fit a linear trend line to any data but that does not mean the fitted line has any significance.  For example, if instead I fit a second order trend line (a parabolic) the result is extremely different.  That suggests a temperature peak around 1950 with an underlying cooling trend since.  Which trend line is the more significant one?  If there was really a strong underlying linear rise over the time period it should have shown up in the 2nd order trend line as well.  This suggests that it is questionable whether any relevant underlying trend can be determined from the data.
It would appear that the temperature rise profile claimed by the adjusted data is largely if not entirely an artefact arising from the adjustments applied (as shown in Figure 3), not from the experimental data record.  In fact, the raw data does not in any way support the AGW theory.
Based on this data, the US temperature data does not correlate with carbon dioxide levels.  The warming over the last 3 decades is completely unremarkable and if present at all is significantly less than occurred in the 1930’s.  It is questionable whether any long term temperature rise over the 20th century can be inferred from the data but if there is any it is far less than claimed by the AGW proponents.
The corrected data from NOAA has been used as evidence of anthropogenic global warming yet it would appear that the rising trend over the 20th century is largely if not entirely an artefact arising from the “corrections” applied to the experimental data, at least in the US, and is not visible in the uncorrected experimental data record.
This is an extremely serious issue.  It is completely unacceptable, and scientifically meaningless, to claim experimental confirmation of a theory when the confirmation arises from the “corrections” to the raw data rather than from the raw data itself.  This is even more the case if the organisation carrying out the corrections has published material indicating that it supports the theory under discussion.  In any other branch of science that would be treated with profound scepticism if not indeed rejected outright.  I believe the same standards should be applied in this case.
*********************
Notes and Links
Interestingly, there was an earlier version of the NASA GISS data shown in Figure 4 which was originally published at http://www.giss.nasa.gov/data/update/gistemp/graphs/FigD.txt While this site has now been taken down the data was apparently archived by John Daly and available at his website http://www.john-daly.com/usatemps.006.  The data is presented in tabular form rather than graphical form but appears to be either identical or extremely similar to that shown in my Figure 5.
Other contributions from Michael Hammer can be read here: http://jennifermarohasy.com/blog/author/michael-hammer/
[scroll down, click on the title for the full article]
Ross McKitrick, Ph.D. – Quantifying the Influence of Anthropogenic Surface Processes on Gridded Global Climate Data
http://www.heartland.org/events/NewYork08/newyork2008-video.html
Anthony Watts – http://wattsupwiththat.com/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e953748b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The way we measure global temperature is once again facing scrutiny for over‐​estimating the planet’s warming trends. Our government _homogenizes_ weather data so that all nearby weather stations are all singing the same tune. It’s done to weed out bad stations or failing weather equipment. We discovered such a thing earlier this year when we found that perhaps the nation’s most politically iconic weather station — Washington DC’s Reagan National Airport — was reading temperatures that were far too hot to be plausible.



Now it turns out that the homogenization _itself_ is suspect and also producing way too much warming. Anthony Watts, a prominent climate blogger without any external financial support, revealed this in a blockbuster presentation at the fall meeting of the American Geophysical Union in San Francisco, a few days before Christmas. Along with three colleagues, he may have invalidated much of the warming in recent years in the U.S. temperature history from our National Oceanic and Atmospheric Administration.



For years, Watts and a team of volunteers set about to photograph, or obtain photos or satellite imagery, of just about every weather station that forms NOAA’s “Historical Climate Network” (HCN), which our government claims was pretty much free of nagging problems like temperature sensors being close to parking lots or, even worse, heat sources like air‐​conditioning exhaust.



It turns out they weren’t, and after assiduously poring over all the pictures, Watts and his crew classified the stations into two general groups, well‐​sited, “compliant” stations, and poorly‐​sited “non‐​compliant” ones. From the compliant group, Watts’ team further selected only those stations which had no changes whatsoever in location or observation timing during their analysis period, 1979–2008, leaving 92 of the best quality stations distributed across the U.S.





‘Homogenization’ adds systematic errors to the data rather than accounting for them.



Watts then plotted up the average temperature from the government network’s homogenized data compared to his ultra‐​clean stations.



Around 1979, the second warming trend of the 20th century began in both US and global records. The first one, from 1910 to 1945, is about the same magnitude as the second one, but couldn’t have been from dreaded carbon dioxide because we had not emitted very much for most of that period.



The second warming is important because there’s likely to be some human component to it. If you want to know what that really means, here is a shameless plug for our new book, _Lukewarming: The New Climate Science that Changes Everything_. It’s very important to see how much of it is human, or, to put it more indelicately, if a substantial fraction of it is measurement error caused by compromised weather stations, there’s likely to be quite a bit less warming in our future than has been forecast by some computer models.



Watts’ findings are spectacular. Averaged across the U.S., the government’s homogenized stations are warming at a rate _more than 50 percent greater_ than Watts’ clean ones. A whopping difference.



There’s more. Average U.S. temperatures warmed from 1979 through 1997 and then levelled off. There was also a cooling period earlier in this century. The compliant data shows less warming _and_ less cooling than the homogenized data. In other words, “homogenization” adds systematic errors to the data rather than accounting for them. Basically, the government’s procedures adjust the observations of well‐​sited stations to be closer to that of poorly‐​sited stations, rather than the obviously preferable and more scientifically appropriate _vice versa_. Thus, the government’s procedure results in an enhanced warming signal.



The U.S. surface record turns out to be in many ways representative of the behavior of the entire Northern Hemisphere, and it turns out that NOAA does the same thing to their global land records, which means that there is the very real probability that not only has the global warming been overestimated by computer models, it has been over‐​measured by homogenized data. This is yet another piece of strong evidence that the Earth is not warming as much as the UN says it should have.



For much of the last year, Washington has been abuzz with rumors that NOAA manipulated the global temperature records to get them to “disappear” the “hiatus” in global warming since the mid‐​1990s, a phenomenon that is obvious in global satellite data. Congressman Lamar Smith (R-TX), chair of the House Committee on Science, Space and Technology, seems to smelling smoke over this. It appears that Anthony Watts has found the fire.
"
nan
"**Remote jury centres for sheriff court trials are to be created in Odeon cinema complexes in Ayr, East Kilbride, Dundee and Dunfermline.**
The facilities are in addition to two remote jury centres already identified in Edinburgh and Glasgow, where trials are set to get under way next week.
Arrangements for jury venues in Aberdeen and Inverness are currently being finalised for a February launch.
The pandemic has caused a major backlog of cases across the country.
David Fraser, chief operations officer for the Scottish Courts and Tribunals Service, said: ""The commencement of trials in Edinburgh and Glasgow next week sees the restart of sheriff court jury trials.
""There has been exceptional progress to secure remote jury centre venues required and we intend to move as quickly as possible to the pre-Covid number of sheriff court jury trials proceeding in Scotland.""
Under the plans, published on Tuesday, jurors will be based as follows:
The Odeon in Braehead, Renfrewshire, is currently operating as a remote jury centre for Glasgow High Court.
Last month the SCTS confirmed that six juries would be based remotely in the Odeon at Glasgow Quay for Glasgow Sheriff Court trials.
For trials running initially in Edinburgh Sheriff Court and then Livingston Sheriff Court, three juries will be based remotely in the Odeon complex on Lothian Road, Edinburgh."
"
From the U.S. Senate Committe on Environment and Public Works
Democrats Delay Global Warming Bill – Again


Obama Agenda In “Disarray”
Washington, D.C. – U.S. Senator James Inhofe (R-Okla.), Ranking Member of the Environment & Public Works Committee, today said that he was not surprised to learn that Senate Democrats were forced once again to delay introduction of their global warming cap-and-trade bill. Throughout hearing after hearing in the EPW Committee this summer, it became apparent that Democrats were a long way off from reaching the votes necessary in the Senate to pass the largest tax increase in American history.
“The news today-that Sen. Boxer and Sen. Kerry will delay introduction of their cap-and-trade bill-came as no surprise.  The delay is emblematic of the division and disarray in the Democratic Party over cap-and-trade and health care legislation-both of which are big government schemes for which the public has expressed overwhelming opposition.  With the climate change debate on Capitol Hill, it’s safe to report that bipartisanship is nowhere in evidence.  Cap-and-trade has pitted Democrat against Democrat, or, put another way, it centers on those in the party supporting the largest tax increase in American history against those in the party who oppose it.  As to just who will win this intra-party squabble, I put money down on those representing the vast majority of the American people, who are clear that cap-and-trade should be rationed out of existence.”
In the last hearing before the EPW Committee before the August recess,  Senator Inhofe spoke directly to the mounting concerns raised by Senate Democrats to cap-and-trade legislation:
Full opening statement provided below:
Climate Change and Ensuring that America Leads the Clean Energy Transformation
August 6, 2009
Madame Chairman, thank you for holding this hearing today. This is the last hearing on climate change before the August recess, so I think it’s appropriate to take stock of what we’ve learned.
Madame Chairman, since you assumed the gavel, this committee has held over thirty hearings on climate change. With testimony from numerous experts and officials from all over the country, these hearings explored various issues associated with cap-and-trade-and I’m sure my colleagues learned a great deal from them.
But over the last two years, it was not from these, at times, arcane and abstract policy discussions that we got to the essence of cap-and-trade. No, it was the Democrats who cut right to the chase; it was the Democrats over the last two years who exposed what cap-and-trade really means for the American public.
We learned, for example, from President Obama that under his cap-and-trade plan, “electricity prices would necessarily skyrocket.”
We learned from Rep. John Dingell (D-Mich.) that cap-and-trade is “a tax, and a great big one.”
We learned from Rep. Peter DeFazio (D-Ore.) that “a cap-and-trade system is prone to market manipulation and speculation without any guarantee of meaningful GHG emission reductions. A cap-and-trade has been operating in Europe for three years and is largely a failure.”
We learned from Sen. Dorgan (D-N.D.) that with cap-and-trade “the Wall Street crowd can’t wait to sink their teeth into a new trillion-dollar trading market in which hedge funds and investment banks would trade and speculate on carbon credits and securities. In no time they’ll create derivatives, swaps and more in that new market. In fact, most of the investment banks have already created carbon trading departments. They are ready to go. I’m not.”
We learned from Sen. Cantwell (D-Wash.) that “a cap-and-trade program might allow Wall Street to distort a carbon market for its own profits.”
We learned from EPA Administrator Lisa Jackson that unilateral U.S. action to address climate change through cap-and-trade would be futile. She said in response to a question from me that “U.S. action alone will not impact world CO2 levels.”
We learned from Sen. Kerry (D-Mass.) that “there is no way the United States of America acting alone can solve this problem. So we have to have China; we have to have India.”
We learned from Sen. McCaskill (D-Mo.) that if “we go too far with this,” that is, cap-and-trade, then “all we’re going to do is chase more jobs to China and India, where they’ve been putting up coal-fired plants every 10 minutes.”
In sum, after a slew of hearings and three unsuccessful votes on the Senate floor, the Democrats taught us that cap-and-trade is a great big tax that will raise electricity prices on consumers, enrich Wall Street traders, and send jobs to China and India-all without any impact on global temperature.
So off we go into the August recess, secure in the knowledge that cap-and-trade is riddled with flaws, and that Democrats are seriously divided over one of President Obama’s top domestic policy priorities.
And we also know that, according to recent polling, the American public is increasingly unwilling to pay anything to fight global warming.
But all of this does not mean cap-and-trade is dead and gone. It is very much alive, as Democratic leaders, as they did in the House, are eager to distribute pork on unprecedented scales to secure the necessary votes to pass cap-and-trade into law.
So be assured of this: We will markup legislation in this committee, pass it, and then it will be combined with other bills from other committees. And we will have a debate on the Senate floor.
Throughout the debate on cap-and-trade, we will be there to say that:
According to the American Farm Bureau, the vast majority of agriculture groups oppose it;
According to GAO, it will send our jobs to China and India;
According to the National Black Chamber of Commerce, it will destroy over 2 million jobs;
According to EPA and EIA, it will not reduce our dependence on foreign oil;
According to EPA, it will do nothing to reduce global temperature;
And when all is said and done, the American people will reject it and we will defeat it.
Thank you, Madame Chairman.
# # #


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93530e16',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly articles in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



 _The Current Wisdom_ only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



Previous editions of _Wisdom_ , which began in 2010, are available at our blog Cato@Liberty (www​.cato​-at​-lib​er​ty​.org/).



A major pillar supporting the catastrophic vision of climate change is a large and rapid rise in the level of the global oceans. There’s an awful lot of infrastructure and property by the shore – and people who live there. Total insured property values along the Atlantic coast of the U.S. (which includes the Gulf of Mexico) are roughly equal to the nation’s annual GDP.



So should you sell your beach house because of the impending doom? I say yes. You need to beat the rush, put it on the market at a bargain‐​basement price, and sell it to me. And then I will keep it until the cows come home.



In its Fourth Assessment Report (AR4) published in 2007, the United Nations’ Intergovernmental Panel on Climate Change (IPCC) projected that sea levels would rise somewhere between 7 and 23 inches with a central value of about 15 inches by century’s end. Alarmists were quick to argue that these numbers were far too low because the IPCC did not include dynamic changes that may occur to the vast ice sheets located in Greenland and Antarctica. Since the publication of the AR4, a host of papers (e.g., Vermeer and Rahmstorf, 2009; Grinsted et al., 2009) have been published that suggest that a sea level rise of 3 to 6 feet by the year 2100 is a much more likely expectation than is 15 inches.



But, as we have noted previously in this _Wisdom_ , many of the proposed mechanisms for such a rapid rise — which is caused by a sudden and massive loss of ice from atop Greenland and/​or Antarctica — don’t seem to operate in such a way as to produce a rapid and sustained ice release.



But the rapid sea level rise beat goes on. In global warming science, we note, the number of scientific papers with the conclusion “it’s worse than we thought” vastly outnumbers those saying “new research indicates things aren’t so dire as previous projections.” In a world of unbiased models and data, they should roughly be in balance. 



This just in: it’s worse than we thought! In research published last month, Eric Rignot and colleagues have combined observations and models to derive a history of the surface mass balance (SMB) of ice (gains from snowfall minus losses from surface melting and glacial discharge) over Greenland and Antarctica. What they find is that over the past two decades, the SMB of ice in both locations has gone from positive (i.e., net gains) to negative (i.e., net loss). In other words, the amount of ice was increasing annually on both landmasses back in the early 1990s, but now ice is currently being lost. And if that were not bad enough, the negative trend in SMB means that the rate of ice loss is increasing (i.e., ice loss is accelerating). The acceleration is about 50% greater in Greenland than it is in Antarctica.



Extrapolating the acceleration forward, Rignot et al. find that the combined ice loss from Greenland and Antarctica becomes the largest contributor to sea level rise by the mid‐​to‐​late 21st century, exceeding that from thermal expansion and ice loss from mountain glaciers in other parts of the world. This is a significant finding because in the AR4 the IPCC actually projects that, in combination, Greenland and Antarctica gain ice over the 21st century and thus act to retard the rise in sea level from other contributors. The implication from Rignot et al. is that the IPCC AR4 sea level rise projections for the end of the century (which average about 15 inches) are too low — by about 2 feet. Thus, Rignot et al. lends support for the recent projection of sea level rise over the 21st century of 3 feet or more (e.g., Vermeer and Rahmstorf, 2009; Grinsted et al., 2009).



So please stop here. Read no more, and, in a panic, sell me the house.



The SMB history for Antarctica and Greenland history is very noisy. The magnitude of recent trends (which are derived from less than 20 years worth of data) may not be terribly representative of the value of the long‐​term trend. Nor is the glacial behavior in Greenland and Antarctica well‐​understood; recent papers have suggested that a larger speed‐​up in the rates of glacial flow is likely not sustainable (see previous _Current Wisdom_ articles for more details).



Now there is more.



We (my research team) just published a paper in the _Journal of Geophysical Research_ in which we estimate the history of the extent of surface ice melt (a major component of SMB) in Greenland for the last two and a quarter centuries. Our goal was to provide some longer context it which to place the relatively short period of direct ice melt observations (such as those included in Rignot’s analysis). What we found is that, while the current rate of surface ice melt is high (and increasing), there have been times in the past (primarily from the late 1920s through the early 1960s) during which the ice melt across Greenland was just as high (Figure 1). _And_ , _this is important_ , the period of the lowest ice melt extent across Greenland for more than a century occurred from the early 1970s through the late 1980s – or very near the beginning the time period analyzed by Rignot et al.1







We describe this in our paper:



It is worth noting that the satellite observations of Greenland’s total ice melt, which begin in the late 1970s, start during a time that is characterized by the lowest sustained extent of melt during the past century. Thus, the positive melt extent trend [during the past 2–3 decades] includes nearly equal contributions from the relatively high melt extents in recent years but also from the relatively low ice melt extent in the early years of the available satellite record. The large values of ice [melt] extent observed in recent years are much less unusual when compared against conditions typical of the early to mid 20th century, than when compared against conditions at the beginning the of the satellite record.



In other words, the recent increase in melt across Greenland (contributing to a negative trend in SMB) may in part a result of rising temperatures from sources other than dreaded greenhouse warming, and therefore extrapolating the observed trends in SMB forward may not be such a great idea.



We put the impact of recent melt in historical perspective:



However, there is no indication that the increased contribution from the Greenland melt in the early to mid 20th century, a roughly 40 year interval when average annual melt was more or less equivalent to the average of the most recent 10 years (2000–2009), resulted in a rate of total global sea level rise that exceeded ~3 mm/​yr. **This suggests that Greenland’s contribution to global sea level rise, even during multidecadal conditions as warm as during the past several years, is relatively modest.** [emphasis added]



Or, to put it another way, the IPCC forecasts of sea level rise don’t appear to be in jeopardy from ice loss from Greenland — a conclusion in contrast to that of Rignot et al.



Further evidence of this can be found in a just‐​published paper in the _Journal of Coastal Research_ by James Houston (Director Emeritus of the Engineer Research and Development Center of the Army Corps of Engineers) and Robert Dean (Professor Emeritus in the Department of Civil and Coastal Engineering at the University of Florida)2. Houston and Dean analyzed long‐​term observations (more than 60 years in length) from tide gauges installed along the U.S. coast looking for signs of an accelerated rise. They couldn’t find any — in fact, they found a _deceleration_. Expanding their analysis to included tide gauges from around the world produced the same thing — a recent _slowing of the rate of sea level rise_.



According to Houston and Dean:



Our analyses do not indicate acceleration in sea level in U.S. tide gauge records during the 20th century. Instead, for each time period we consider, the records show small decelerations that are consistent with a number of earlier studies of worldwide‐​gauge records. The decelerations that we obtain are opposite in sign and one to two orders of magnitude less than the +0.07 to +0.28 mm/​y2 accelerations that are required to reach sea levels predicted for 2100 by Vermeer and Rahmsdorf (2009), Jevrejeva, Moore, and Grinsted (2010), and Grinsted, Moore, and Jevrejeva (2010). Bindoff et al. (2007) note an increase in worldwide temperature from 1906 to 2005 of 0.74Ã�Â°C. It is essential that investigations continue to address why this worldwide‐​temperature increase has not produced acceleration of global sea level over the past 100 years, and indeed why global sea level has possibly decelerated for at least the last 80 years.



 _Please_ disregard these findings, get with the “consensus,” and sell me your beach house. Now.





**Notes:**



 **1.** As you might guess, this problem afflicts other satellite‐​derived measurements of polar ice, too. The sensors were launched in 1978, which just happens to be at the end of the coldest period of summers in the Arctic since the early 1920s. As a result, the satellite began to measure Arctic ice when it was unusually expanded. It is worth noting at the time that one of the justifications for the platforms was the spectre of advancing ice from global cooling.  
  
 **2.** It has been noted others that many scientists wait until they are retired before attempting to publish papers that go against the disastrous vision of global warming. This is not a sign of a healthy science.



 **References:**



Frauenfeld, O.W., P.C. Knappenberger, and P.J. Michaels, 2011. A reconstruction of annual Greenland ice melt extent, 1784–2009. _Journal of Geophysical Research_ , doi: 10.1029/2010JD014918, in press.  
Grinsted, A., J.C. Moore, and S. Jevrejeva. 2009. Reconstructing sea level from paleo and projected temperatures 200 to 2100AD. _Climate Dynamics_ , doi: 10.1007/s00382-008‑0507-2.  
Houston, J.R., and R.G. Dean, 2011. Sea‐​level acceleration based on U.S. tide gauges and extension of previous global‐​gauge analyses. _Journal of Coastal Research_ , in press.  
Rignot, E., I. Velicogna, M.R. van den Broeke, A. Monaghan, and J.T.M. Lenaerts, 2011. Acceleration of the contribution of the Greenland and Antarctic ice sheets to sea level rise. _Geophysical Research Letters_ , **38** , L05503, doi: 10.1029/2011GL046583.  
Vermeer, M., and S. Rahmstorf, 2009. Global sea level linked to global temperature. _Proceedings of the National Academy of Sciences_ , **106** , 51, doi: 10.1073/pnas.0907765106, 21527–21532.
"
nan
"
New predictions for sea level rise
Sea level graph from the University of Colorado is shown below:

University of Bristol Press release issued 26 July 2009
 Fossil coral data and temperature records derived from ice-core measurements have been used to place better constraints on future sea level rise, and to test sea level projections.
The results are published today in Nature Geoscience and predict that the amount of sea level rise by the end of this century will be between 7- 82 cm (0.22 to 2.69 feet)

– depending on the amount of warming that occurs – a figure similar to that projected by the IPCC report of 2007.
Placing limits on the amount of sea level rise over the next century is one of the most pressing challenges for climate scientists. The uncertainties around different methods to achieve accurate predictions are highly contentious because the response of the Greenland and Antarctic ice sheets to warming is not well understood.
Dr Mark Siddall from the Earth Sciences Department at the University of Bristol, together with colleagues from Switzerland and the US, used fossil coral data and temperature records derived from ice-core measurements to reconstruct sea level fluctuations in response to changing climate for the past 22,000 years, a period that covers the transition from glacial maximum to the warm Holocene interglacial period.
By considering how sea level has responded to temperature since the end of the last glacial period, Siddall and colleagues predict that the amount of sea level rise by the end of this century will be similar to that projected by the Fourth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC).
Dr Siddall said: “Given that the two approaches are entirely independent of each other, this result strengthens the confidence with which one may interpret the IPCC results. It is of vital importance that this semi-empirical result, based on a wealth of data from fossil corals, converges so closely with the IPCC estimates.
“Furthermore, as the time constant of the sea level response is 2,900 years, our model indicates that the impact of twentieth-century warming on sea level will continue for many centuries into the future. It will therefore constitute an important component of climate change in the future.”
The IPCC used sophisticated climate models to carry out their analysis, whereas Siddall and colleagues used a simple, conceptual model which is trained to match the sea level changes that have occurred since the end of the last ice age.
The new model explains much of the variability observed over the past 22,000 years and, in response to the minimum (1.1 oC) and maximum (6.4 oC) warming projected for AD 2100 by the IPCC model, this new model predicts, respectively, 7 and 82 cm of sea-level rise by the end of this century. The IPCC model predicted a slightly narrower range of sea level rise – between 18 and 76 cm.
The researchers emphasise that because we will be at least 200 years into a perturbed climate state by the end of this century, the lessons of long-term change in the past may be key to understanding future change.
Please contact                        Cherry Lewis for further information.

Further information:
The paper: Constraints on future sea-level rise from past sea-level reconstructions. Mark Siddall, Thomas F. Stocker and Peter U. Clark. Nature Geoscience .



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94186d9a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"JP Morgan Chase is to end fossil fuel loans for Arctic oil drilling and phase out loans for coal mining under new climate initiatives. The world’s largest financier of fossil fuels set out its plans at an investor event on Tuesday, days after the bank’s economists warned that the climate crisis threatened the survival of humanity. JP Morgan will aim to offer $200bn (£153bn) in environmental and economic development deals to help support clean energy and other sustainable projects instead.  JP Morgan’s green pledges put the bank on a par with Goldman Sachs which became the first large US bank to rule out future financing of oil drilling or exploration in the Arctic and in new mines for thermal coal. But environmental groups said the bank’s green pledges were dwarfed by its huge financial support for fossil fuels. JP Morgan has provided $75bn in financial support to expand shale fracking, and Arctic oil and gas exploration since the Paris climate agreement. Eli Kasargod-Staub, of ethical shareholder group Majority Action, said: “These steps pale in comparison to JP Morgan Chase’s responsibility to confront the climate crisis and the systemic risks it poses to investors and global financial stability.” The group called on JP Morgan to disclose its climate impact and realign its lending with the global goal of limiting heating to 1.5C.  JP Morgan’s climate strategy is expected to fall short of the green pledges announced by BlackRock, the world’s largest hedge fund, which will cut companies that rely on thermal coal for more than a quarter of their revenues from its actively managed portfolios. JP Morgan’s coal finance restrictions will apply to companies whose primary business is coal mining, but could allow a loophole to continue financing conglomerates that earn less than half of their revenue from coal. Jeanne Martin, a campaigner at the investment charity ShareAction, said JP Morgan’s climate pledges were an “anticlimax” but proved that “even the world’s largest fossil fuel financier has no choice but to listen to its shareholders and civil society on climate change”. The leaked document from JP Morgan’s economists, dated 14 January, warns that the bank “cannot rule out catastrophic outcomes where human life as we know it is threatened” by rising global temperatures. “Although precise predictions are not possible, it is clear that the Earth is on an unsustainable trajectory. Something will have to change at some point if the human race is going to survive,” the report says. ShareAction, which promotes responsible investing, said JP Morgan’s new policy “is at best an anticlimax and at worst dangerously omissive of a huge part of the coal market”. Martin said: “If the world was waiting for JP Morgan to move meaningfully on its funding of the climate crisis after warning that human life ‘as we know it’ could be threatened by climate change, it will be sorely disappointed.”"
"More investment in flood defences and improved planning for future disasters are urgently needed, scientists have warned. They predict that the number of extreme wet days – which have already increased this century – will continue to rise in the coming decades and will bring even greater devastation than that experienced this month after Storm Ciara and Storm Dennis swept across the country.  Ciara brought rain and wind gusts of up to 97mph, triggering more than 190 flood alerts. More than 500 properties were flooded and about 25,000 homes left without power. A week later Storm Dennis followed, which in some areas caused more than a month’s rain to fall within 24 hours. Thousands of people had to be moved from their flooded homes, rivers – including the Wye in Hereford – rose to record levels, while the Environment Agency issued a record number of flood warnings and alerts, including more than 600 last weekend. Four people were killed during Storm Dennis. “We are simply not prepared for the flooding coming our way in future,” said Prof Hannah Cloke, of Reading University. “We need to carry out a complete overhaul of our defences and be prepared to spend a lot more on them over a longer period of time.” So far the government has committed to spending £4bn over the next five years on improving flood defences. But both the amount and timescale were criticised for being insufficient last week. “Extremely wet days during UK winters are currently up by around 15% compared with previous decades,” said Dann Mitchell of Bristol University’s Cabot Institute for the Environment. “Wetter future winters is a consistent projection with some predicting a 30% to 35% increase in rain by 2070. Our government and town planners need to invest significantly in UK flood defences.” Last week George Eustice, the environment secretary, said he wanted to see more nature-based solutions, such as the construction of dams made of natural materials and the planting of trees in upper catchment areas. These would hold on to water and prevent it from pouring too quickly into rivers and estuaries. But this approach was dismissed as inadequate by Roger Falconer, professor of water management at Cardiff University. “It is like putting a small sticking plaster on a major open wound to control profuse bleeding. It would certainly be insufficient when dealing with the 30% increase in winter rainfall which the Met Office has predicted for some areas.” Instead, Falconer called for the construction of a large number of flow-through or perforated dams above towns at high risk of flooding. “Such a dam fills during flooding in the upper parts of the river basin and is then emptied, under controlled conditions, after the flood,” Falconer said. “We need many more of these.” Engineers and hydrologists also pointed out to modifications made to river channels, flood plains, land cover and drainage, and these often have serious impacts on water flow and bedevil attempts to predict how rivers will react to downpours. For example, one recent study of the River Afan in Wales found 259 barriers had been erected along its course over the past 200 years, although only 33 had been officially recorded. Work on pinpointing barriers like these urgently needs to be carried out, scientists have said. “Blame [for flooding] under these circumstances is misguided and unhelpful, and politicians should be very careful to ensure they understand the facts of flooding before seeking to champion any particular action,” said Prof David Sear, of Southampton University. Bringing a halt to the construction of houses on flood plains has also emerged as a key issue. One in 10 new homes built in England since 2013 has been built on ground at high risk of flooding, official figures show. Prof Robert Wilby, of the University of Loughborough, told the Guardian that the government should review its housebuilding targets in view of the increased risks from floods. This was backed by Mohammad Heidarzadeh, head of coastal engineering at Brunel University. “The UK’s flood defence systems were developed decades ago and are not fit to address the current climate situation,” he said. “While the interval for major floods was 15 to 20 years in the past century, it has shortened to two to five years in the past decade. “The country needs further investment in its flood systems, but such investment should be within a holistic and integrated framework.”"
"The school climate strikes show that young people want to fight climate change, but their enthusiasm  for collective action is largely untapped. A volunteer conservation army could mobilise their talent and passion by channelling it into work to restore ecosystems. The Green New Deal – endorsed by US Congresswoman Alexandria Ocasio-Cortez and numerous presidential candidates – is a plan to eliminate carbon emissions in ten years, provide full employment in building clean energy infrastructure and redistribute wealth to tackle inequality. The Green New Deal has encouraged people to embrace radical solutions to climate change by sharing its name and ethos with the New Deal of the 1930s. President Franklin D. Roosevelt’s New Deal was a transformation of America’s economy which put thousands to work in manufacturing and redistributed wealth to help the country recover from the Depression.   One of the first and most popular programmes of the New Deal was the Civilian Conservation Corps (CCC) – a public work relief programme that enlisted millions of young men in conservation work throughout the natural environment of the US. Reviving the scheme could prove a popular and effective way for countries to mobilise the climate strike generation in environmentally beneficial work. During the 1930s dust storms devastated the ecology of the Southern Plains in the US. Severe drought and a failure to apply shallow plowing to prevent wind erosion created the Dust Bowl, which forced tens of thousands of poverty-stricken families to abandon their farms, unable to pay mortgages or grow crops.  Doing “the kind of public work that is self-sustaining” in the president’s words, CCC members planted more than 2 billion trees on more than 40 million farm acres between 1933 and 1942. These trees acted as wind breaks and helped bind moisture in the soil – halting the erosion that caused the Dust Bowl. Members also built flood barriers, fought forest fires and maintained forest roads and trails. By enlisting three million men aged between 18 and 25, the CCC helped restore and repair ecosystems throughout the US with hundreds of projects in forestry and conservation. The CCC made many Americans mindful of the sustainability of timber, soil and water for the first time and introduced them to the efforts needed to ensure their preservation. Today, most people are aware of climate change, pollution and biodiversity loss. Through the internet, promoting awareness is certainly easier than in Roosevelt’s era. But the environmental problems themselves are more serious and will require radical changes in society and the economy to overcome. Leaving behind the programme’s legacy of racial segregation, a modern CCC could mobilise any young person who wants to get their hands dirty fighting climate change. A modern volunteer army of conservationists could get to work in every country, adjusting their efforts according to the environmental needs of each setting. The first task set could be in environmental monitoring – collecting data on pollution and wildlife abundance. These surveys would provide invaluable information about the health of ecosystems and how they are changing. Ecosystems could then benefit from projects which reintroduce species and restore habitats. Mass tree planting could absorb atmospheric carbon and provide new habitat for returning wildlife. Wetlands – coastal ecosystems which protect against sea level rise – could be expanded with vegetation which would also create sanctuaries for migratory birds. Reintroduced beavers and other ecosystem engineers could act as animal recruits who create new habitats, such as dams and lakes, which allow even more species to thrive.  Planting trees around river banks in particular provides a food source for aquatic organisms and provides nutrient input to the system. Volunteers could build fencing around freshwater environments to prevent livestock entering the water and  transferring organic material and fertiliser from the surrounding fields into the water. This can cause eutrophication which strips oxygen from the water, eventually causing mass dead zones in coastal waters where nutrient-loaded water is discharged. Legions of litter pickers in parks and on beaches could significantly reduce the amount of plastic pollution which reaches the ocean too. Volunteers could be trained to test water quality and take an active role in monitoring pollution and local sea life. 


      Read more:
      Plastic pollution: seaside communities coming together will save us – not technology


 In rural areas, building drystone walls without mortar encourages mosses and lichens to grow and provides nooks and crannies for birds, toads, newts and insects to set up home. For every tonne of cement manufactured and used in a traditional wall, approximately a tonne of carbon dioxide is released into the atmosphere. Building new infrastructure which uses as little as possible or entirely different materials could be another task for volunteers. Planting hedgerows could create corridors of vegetation which link wildlife to wooded habitat and provide food and shelter. Volunteers could also construct habitat highways – corridors of vegetation which provide safe passage for wildlife under or over major road networks, allowing reproduction between populations to continue. As well as being rewarding and educational, young people taking part in the scheme would develop transferable skills. Working in nature has a positive impact on wellbeing and participants would also benefit from a healthy dose of exercise.  The New Deal of the 1930s sought to tackle an environmental crisis while reorienting the American economy to delivering social justice. Today’s Green New Deal could harness the same ethos, but to more ambitious ends, with a socially inclusive CCC that restores ecosystems and fights climate change. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
I found this press release on the UC Davis website interesting, because it discusses something new to me, “winter chill”. I found it interesting. But immediately, I thought of this study on irrigation by Dr. John Christy of the University of Alabama, Huntsville.
Irrigation most likely to blame for Central California warming
Given that the UC Davis researchers seem to have only looked at temperature records to establish trends, it looks like they may have missed a significant contributor to the trends – increased humidity due to irrigation. – Anthony
From UC Davis News: Warming Climate Threatens California Fruit and Nut Production
July 21, 2009
 






No more cherry picking?







  Winter chill, a vital climatic trigger for many tree crops, is likely to decrease by more than 50 percent during this century as global climate warms, making California no longer suitable for growing many fruit and nut crops, according to a team of researchers from the University of California, Davis, and the University of Washington.
In some parts of California’s agriculturally rich Central Valley, winter chill has already declined by nearly 30 percent, the researchers found.
“Depending on the pace of winter chill decline, the consequences for California’s fruit and nut industries could be devastating,” said Minghua Zhang, a professor of environmental and resource science at UC Davis.
Also collaborating on the study were Eike Luedeling, a postdoctoral fellow in UC Davis’ Department of Plant Sciences and UC Davis graduate Evan H. Girvetz, who is now a postdoctoral research associate at the University of Washington, Seattle. Their study  appears July 22 in the online journal PLoS ONE.
The study is the first to map winter chill projections for all of California, which is home to nearly 3 million acres of fruit and nut trees that require chilling. The combined production value of these crops was $7.8 billion in 2007, according to the California Department of Food and Agriculture.
“Our findings suggest that California’s fruit and nut industry will need to develop new tree cultivars with reduced chilling requirements and new management strategies for breaking dormancy in years of insufficient winter chill,” Luedeling said.
About winter chill
Most fruit and nut trees from nontropical locations avoid cold injury in the winter by losing their leaves in the fall and entering a dormant state that lasts through late fall and winter.
In order to break dormancy and resume growth, the trees must receive a certain amount of winter chill, traditionally expressed as the number of winter chilling hours between 32 and 45 degrees Fahrenheit. Each species or cultivar is assumed to have a specific chilling requirement, which needs to be fulfilled every winter.
Insufficient winter chill plays havoc with flowering time, which is particularly critical for trees such as walnuts and pistachios that depend on male and female flowering occurring at the same time to ensure pollination and a normal yield.
Planning for a warmer future
Fruit and nut growers commonly use established mathematical models to select tree varieties whose winter chill requirements match conditions of their local area. However, those mathematical models were calibrated based on past temperature conditions, and establishing chilling requirements may not remain valid in the future, the researchers say. Growers will need to include likely future changes in winter chill in their management decisions.
“Since orchards often remain in production for decades, it is important that growers now consider whether there will be sufficient winter chill in the future to support the same tree varieties throughout their producing lifetime,” Zhang said.
To provide accurate projections of winter chill, the researchers used hourly and daily temperature records from 1950 and 2000, as well as 18 climate scenarios projected for later in the 21st century.
They introduced the concept of “safe winter chill,” the amount of chilling that can be safely expected in 90 percent of all years. They calculated the amount of safe winter chill for each scenario and also quantified the change in area of a safe winter chill for certain crop species.
New findings
The researchers found that in all projected scenarios, the winter chill in California declined substantially over time. Their analysis in the Central Valley, where most of the state’s fruit and nut production is located, found that between 1950 and 2000, winter chill had already declined by up to 30 percent in some regions.
Using data from climate models developed for the Intergovernmental Panel on Climate Change Fourth Assessment Report (2007), the researchers projected that winter chill will have declined from the 1950 baseline by as much as 60 percent by the middle of this century and by up to 80 percent by the end of the century.
Their findings indicate that by the year 2000, winter chill had already declined to the point that only 4 percent of the Central Valley was still suitable for growing apples, cherries and pears — all of which have high demand for winter chill.
The researchers project that by the end of the 21st century, the Central Valley might no longer be suitable for growing walnuts, pistachios, peaches, apricots, plums and cherries.
“The effects will be felt by growers of many crops, especially those who specialize in producing high-chill species and varieties,” Luedeling said. “We expect almost all tree crops to be affected by these changes, with almonds and pomegranates likely to be impacted the least because they have low winter chill requirements.”
Developing alternatives
The research team noted that growers may be able change some orchard management practices involving planting density, pruning and irrigation to alleviate the decline in winter chill. Another option would be transitioning to different tree species or varieties that do not demand as much winter chill.
There are also agricultural chemicals that can be used to partially make up for the lack of sufficient chilling in many crops, such as cherries. A better understanding of the physiological and genetic basis of plant dormancy, which is still relatively poorly understood, might point to additional strategies to manage tree dormancy, which will help growers cope with the agro-climatic challenges that lie ahead, the researchers suggested.
Funding for this study was provided by the California Department of Food and Agriculture and The Nature Conservancy.
About UC Davis
For 100 years, UC Davis has engaged in teaching, research and public service that matter to California and transform the world. Located close to the state capital, UC Davis has 31,000 students, an annual research budget that exceeds $500 million, a comprehensive health system and 13 specialized research centers. The university offers interdisciplinary graduate study and more than 100 undergraduate majors in four colleges — Agricultural and Environmental Sciences, Biological Sciences, Engineering, and Letters and Science — and advanced degrees from six professional schools — Education, Law, Management, Medicine, Veterinary Medicine and the Betty Irene Moore School of Nursing.
Media contact(s):
• Minghua Zhang, Land, Air and Water Resources, (530) 752-4953, mhzhang@ucdavis.edu
• Eike Luedeling, Plant Sciences, (530) 574-3794, eluedeling@ucdavis.edu
• Pat Bailey, UC Davis News Service, (530) 752-9843, pjbailey@ucdavis.edu


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e946da7f7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The owners of a card and gift shop say they are defying lockdown laws ""on principle"" and to pay their bills.**
Alasdair Walker-Cox, of Grace Cards and Books in Droitwich, said despite police visits and a council prohibition order they would stay open during lockdown.
He said: ""If we shut we won't be able to pay suppliers, the rent, let alone support the family. If we open we can.""
Wychavon District Council said it was a type of business that must ""close by law"" and issued a Â£1,000 fine.
Footage of West Mercia Police officers telling the owners they may lose their licence if they ignored the council's order has been circulated on social media.
Non-essential shops were among places in England told to close for four weeks on 5 November to curb the spread of coronavirus.
Businesses can be fined by local authorities or the police if they fail to comply, with penalties ranging from Â£1,000 for a first offence to Â£10,000 for the fourth and all subsequent offences.
The prohibition notice was issued on 19 November. The fine was issued on Monday but the shop remained open on Tuesday.
Mr Walker-Cox, who runs the shop with wife Lydia, said he believed lockdowns did not ""work"" against the virus and ""on principle"" wanted to open and support their family and suppliers instead.
Mrs Walker-Cox added it was an ""essential shop"" because it sold icing and edible decorations to cake makers working at home.
The district council said the shop could trade by delivery or ""click and collect"" and said its food offering is not substantial enough to detract from its core activity which is a card and gift shop.
It added it was supporting businesses through the pandemic \- paying out more than Â£600,000 during the current lockdown.
Ch Supt Paul Moxley said West Mercia Police remained ""hugely sympathetic to the difficult times"" faced by business owners.
""We understand the restrictions can be challenging, and we know this business is well-loved in Droitwich, but the government legislation is in place to minimise the spread of Covid-19 and to keep us all safeâ¦. we all have a critical part to play in that,"" he said.
_Follow BBC West Midlands on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to:_newsonline.westmidlands@bbc.co.uk"
"Mining giant Rio Tinto says it wants its globe-spanning operations to reach net zero greenhouse gas emissions by 2050 and will spend US$1bn over the next five years to reduce its carbon footprint. The second biggest miner in the world has also committed to reducing its emissions by 15% by 2030.  Rio Tinto’s decision, which follows pressure from investors, puts it in line with Australian business groups and unions and at odds with an Australian government that has devoted itself to attacking the Labor opposition over its commitment to the same net zero target. Its commitment to cut emissions is easier for Rio Tinto than BHP and other big global miners because unlike BHP, it does not mine coal or oil. However, unlike BHP, Rio Tinto has refused to set a target to reduce so-called “scope 3” emissions produced by its customers. Announcing Rio Tinto’s full-year results on Wednesday, the company’s chief executive Jean-Sébastien Jacques also warned that the coronavirus outbreak could hurt its operations. Chinese steel mills are major Rio Tinto customers, however Jacques said the company’s iron order books were full.  “But we are likely to see some short term impacts such as supply chains and possibly even provision of services from Chinese suppliers,” Jacques said. “We acknowledge that there will be some short-term volatility and uncertainty, but we are very well positioned.” He said Rio Tinto’s new commitments on climate added to a 46% cut in the company’s emissions since 2008, although much of that reduction was due to it selling operations that produce a lot of pollution. To reduce its total emissions by 15% by 2030, every new business opened by Rio Tinto in the next decade will need to be carbon neutral. Jacques said the company was open to buying carbon offsets if necessary but this was a “last port of call”. He said the emissions cut to 2030 would be achieved using existing technology but Rio Tinto would also be investing part of the $1bn on developing new ways of eliminating carbon production. “We have approved around US$100m last week for the Pilbara, I think it would be a good example of things we may look at going forward,” he said. “It’s a 34MW solar photovoltaic plant and a battery system of 12MW per hour storage facility at one of our operations. “By doing this investment … we’ll be able to take out about 90,000 tonnes compared to commercial gas power generation.” This was equivalent to taking 28,000 cars off the road or about 3% of the company’s emissions from the Pilbara, he said. “At the same time we are investing serious money in order to find the technology for the future. We have only a pathway for the next 10 years. If we don’t work today on options beyond the next 10 years, we’ll never get there.” He defended Rio Tinto’s decision not to set scope 3 targets – something rival BHP did in July as part of a $400m program to cut its emissions. “We will not set targets for our customers,” Jaques said. “Having said that, we are looking to partner with our customers and the customers of our customers to look for ways to work together in order to improve emissions across the value chain.” He said examples included a deal the company struck with giant Chinese steel mill and key customer Baosteel in September to reduce emissions from steelmaking and another deal struck with Apple and the government of Quebec two years ago relating to aluminium. “Remember, we are the only large diversified mining and metal company that is not selling either coal, and the carbon associated with coal, or drilling oil and gas and the carbon associated with oil and gas. “So if you step back, if you believe in climate change – and we do believe in climate change – we know we need to have more high quality copper, high quality aluminium, in order to be part of the solution.” Australian prime minister Scott Morrison’s conservative Coalition government campaigned hard against Labor’s net zero target as part of its re-election strategy at the federal election last May, and in recent weeks has revived its attacks on the opposition over the policy by demanding costings and raising the spectre of soaring energy prices. Asked if Rio Tinto would like the government to commit to net zero by 2050, Jaques said: “I don’t think that would make a massive difference.” “What is absolutely clear is that for us to be able to meet our net carbon target by 2050, there will be a need for new technology. “Any government – so I’d make a broad point – that can provide us with some smart, clever policy to create a pathway, a framework for us to accelerate our development in that space, would be more than welcome.” Andrew Gray, the head of environmental, social and governance issues at Australia’s biggest superannuation fund, the $175bn AustralianSuper, said Rio Tinto’s move followed pressure from investor group Climate Action 100+. “These are issues that AustralianSuper as a lead investor has been engaging with Rio on for more than two years as part of the Climate Action 100+ initiative,” he said. “We look forward to continuing to engage with Rio to fulfil these commitments.” The company declared a profit after tax for 2019 of US$7bn, down from US$13.9bn the previous year."
"**The north of England faces a return to 1980s-style prolonged economic decline, the Labour mayor of Greater Manchester Andy Burnham has warned.**
This is in spite of a government promise to ""double down on levelling up"" at the Spending Review this week.
Regional inequalities have increased due to the Covid-19 crisis, Mr Burnham said.
But the government pointed to billions spent supporting the UK economy during the pandemic.
Mr Burnham told the BBC: ""We could be looking at another period like the 1980s in the north of England, coming out of the Covid crisis, the 2020s could even be worse than the 1980s.
""So that is the test facing this government. And it's still unclear whether or not they're going to pass it,"" Mr Burnham said.
He argued the Covid crisis had increased the UK's regional inequalities and that the country had been ""levelled down"", and said the billions expected to be spent on big new infrastructure projects will not be enough.
""It won't be good enough for the chancellor to come to the Commons on Wednesday, and promise railway lines in 20 or 30 years time when people's lives are basically on hold now.
""If the chancellor ignores all of those things, I don't think people will take seriously his claims to spend all of this money in decades to come.
""He has to support people. He has to support businesses now. Otherwise, there will be no economy to rebuild in 2021 or 2022.""
The mayor, a former chief secretary to the Treasury, said that changes to the way the government evaluates where to make investments in big projects were ""moves in the right direction"" to address ""a bias against the north which goes back decades to governments of all colours"".
The changes to the Treasury's Green Book process will be detailed in the Spending Review on Wednesday.
A landmark government rail investment - the fast trans-Pennine link known as ""Northern Powerhouse Rail"" - may not pass the current Treasury tests, he said.
""There is a real risk that the Treasury will say Northern powerhouse rail doesn't pass the test. So this is why the chancellor's commitment to rewrite the Green Book is very welcome,"" he said.
But Mr Burnham, who clashed with the government over support to businesses in lockdown earlier in the autumn, stressed that funding and reforms that enabled lower bus fares in Manchester than London should be the priority.
""If the government wants people to buy into levelling up, they've got to do something to bring down the cost of transport, make it more reliable, and do that within a timeframe. That's a matter of a couple of years, not 20 or 30 years,"" he said.
The government pointed to the tens of billions in support given to wages and businesses during the Covid crisis.
New economic forecasts will be released by the independent Office for Budget Responsibility tomorrow, alongside the Spending Review."
"The UK is widely seen as a climate leader. Its Climate Change Act, which passed into law ten years ago, is the envy of the world. It has targets for carbon reduction enshrined in law, and recently, the government hinted that it would adopt a target of zero greenhouse gas emissions by 2050 (the current target is an 80% reduction). Four years ago, the government, with cross-party support, announced it would phase out coal-fired power generation by 2025. And yet, at a planning committee meeting in the northern English county of Cumbria, where I live, local councillors have voted unanimously to approve a new deep coal mine, Britain’s first in three decades. The mine would extract nearly 3m tonnes of coal a year, primarily for the steel industry rather than power generation. According to Scientists for Global Responsibility, this would result in more than 9m tonnes of carbon dioxide being released into the atmosphere, every year for 50 years – that’s equal to the emissions from a million households. How can a country with such strong ambitions to reduce carbon emissions, approve a plan to increase them so significantly? My research, which is based on interviews with MPs and looks at how politicians understand and respond to climate change, suggests why such a contradictory situation could have arisen. For the past decade, there has been a cross-party consensus in support of long-term carbon targets. Just five out of 650 MPs voted against the Climate Change Act. Yet a side-effect of this consensus has been that politicians have not talked about climate change very much. Political debate is conspicuous by its absence, the climate is rarely discussed in parliament, and few MPs champion the issue. One climate-conscious MP said that he was “known as a freak” for speaking out, while another told me “it’s important not to be a climate change zealot”. They might have voted for carbon targets, but they are reluctant to discuss what this means for our society or economy. Up until now, you could argue that it hasn’t mattered much. The UK has reduced its carbon emissions through structural changes to the economy, and switching away from coal in electricity generation, towards gas and renewables. But there are now fewer and fewer savings like this to be had. As the government’s official advisers, the Committee on Climate Change notes, the UK is not on track to meet future targets.  Now, we need carbon savings right across the economy, through changes to transport, housing, land use and industry. This in turn means changes in the jobs we do, what our houses look like, and how we travel. These changes can bring many benefits – not just reduced emissions, but healthier cities, warmer homes and stronger communities. But it’s the job of politicians to articulate these benefits, alongside the scientific case for climate action, and to build a mandate for the changes needed. In short, there is a need for proper political debate on climate change. Which takes us back to those local politicians in Cumbria who decided to approve the coal mine. Look at it from their point of view. Local authorities have no clear responsibilities or targets to reduce carbon (though it is a factor in planning law). Local politics, in an economically deprived area, is dominated by the need for good employment. Dangle 500 jobs, even high carbon jobs, in front of a local planning committee; make a claim that this is in line with climate commitments; add in the cultural norms that I described above, which make it difficult for politicians to make a political case for climate action; and it’s not surprising that the answer comes out as a yes. Of course, the local councillors should take responsibility for the decision they have made. But responsibility lies elsewhere as well. National climate policy failed Cumbria in two ways. First, a decade of ambiguity and inconsistency – the price paid for lack of proper debate – means that there is no direct line of sight between carbon targets set at a national level, and individual decisions taken by local councils. And second, climate policy has been top-down and expert-led, with no attempt made to engage citizens or local areas in the need for, and benefits of, the transition to a zero-carbon society. This is a serious failing. And yet it points to the way forward. What the UK now needs is an open, positive political debate about how to meet its carbon targets, in place of the quiet ambiguity that has characterised the past decade. Such a debate would encourage honesty about some fundamentals: that high-carbon infrastructure like coal mines and new runways are simply incompatible with the climate challenge. This honesty would, in turn, allow discussion of what the zero-carbon transition looks like in areas like Cumbria – and what it would mean for jobs and communities. Scotland’s Just Transition Commission, and the Green New Deal in the US, could be models for this. Local politicians, and local communities, will need to design and own these strategies – and national government must give them the powers and resources to do so. Approving the coal mine was the wrong decision. But it may just force a new, more honest politics onto the table – and one which benefits Cumbria as well as the wider world. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"A German teenager dubbed the “anti-Greta” – climate sceptics’ answer to the schoolgirl activist Greta Thunberg – is set to address the biggest annual gathering of US grassroots conservatives. Naomi Seibt, 19, who styles herself as a “climate sceptic” or “climate realist”, will this week address the Conservative Political Action Conference (CPAC) near Washington, joining speakers including Donald Trump and Vice-President Mike Pence. Seibt is in the pay of the Heartland Institute, a thinktank closely allied with the White House that denies established science showing humans are heating the planet with dangerous consequences. CPAC will be the biggest stage yet for Seibt, a so-called “YouTube influencer” who tells her followers Thunberg and other activists are whipping up unnecessary hysteria by exaggerating the climate crisis. “Climate change alarmism at its very core is a despicably anti-human ideology,” she has said. The teenager, from Münster in western Germany, claims she is “without an agenda, without an ideology”. But she was pushed into the limelight by leading figures on the German far right and her mother, a lawyer, has represented politicians from the Alternative für Deutschland (AfD) party in court. Seibt had her first essay published by the “anti-Islamisation” blog Philosophia Perennis and was championed by Martin Sellner, leader of the Austrian Identitarian Movement, who has been denied entry to the UK and US because of his political activism. A Facebook post by the AfD youth wing names Seibt as a member and she spoke at a recent AfD event, though she has denied membership of the party. In May 2019 she posted her first video on YouTube, reading out verses submitted for a poetry slam competition organised by the AfD. The impact of the clip and its follow-ups put her on the radar of the Heartland Institute, which is based in Chicago. It has lobbied on behalf of the tobacco and coal industries but recently concentrated its efforts on challenging the scientific consensus on climate change. Last December, as Thunberg addressed the United Nations’ Cop25 global warming summit in Madrid, Seibt gave the keynote speech at a rival conference organised by the Heartland Institute a few miles away. In a sting operation carried out for German broadcaster ZDF and investigative outlet Correctiv, the Heartland Institute strategist James Taylor told journalists posing as potential donors his thinktank had signed up Seibt to record climate change sceptic videos for young people. Seibt has admitted that she receives “an average monthly wage” from the institute. According to official figures, the average net monthly income in Germany is just under €1,900 (£1,590, $2,066). The Heartland website features a low-budget video introducing Seibt, who speaks to the camera from what appears to be a home. “I’ve got very good news for you,” she says. “The world is not ending because of climate change. In fact, 12 years from now we will still be around, casually taking photos on our iPhone 18s “We are currently being force-fed a very dystopian agenda of climate alarmism that tells us that we as humans are destroying the planet. And that the young people, especially, have no future – that the animals are dying, that we are ruining nature.” In another film, Naomi Seibt vs Greta Thunberg: Whom Should We Trust?, Seibt says: “Science is entirely based on intellectual humility and it is important that we keep questioning the narrative that is out there instead of promoting it, and these days climate change science really isn’t science at all.” Seibt has also uploaded a video with the title Message to the Media – HOW DARE YOU – an obvious reference to a speech by Thunberg at the UN in which she rebuked world leaders: “We are in the beginning of a mass extinction, and all you can talk about is money, and fairytales of eternal economic growth. How dare you!” Thunberg began her activism at 15 by missing school and camping outside the Swedish parliament. She has since met the pope, addressed members of Congress in Washington and heads of state at the UN and helped inspire 4 million people to join a global climate strike. Last year she became the youngest Time magazine Person of the Year, much to Trump’s chagrin. The Washington Post observed: “If imitation is the highest form of flattery, Heartland’s tactics amount to an acknowledgment that Greta has touched a nerve, especially among teens and young adults.” Since Trump’s election, CPAC has paraded hard-right figures such as the former White House officials Steve Bannon and Sebastian Gorka as well as numerous climate sceptics. In his speech there last year, the president mocked the Green New Deal, proposals championed by Democrats including Alexandria Ocasio-Cortez. “No planes,” the president said. “No energy. When the wind stops blowing, that’s the end of your electric. ‘Let’s hurry up. Darling, darling, is the wind blowing today? I’d like to watch television, darling.’” Connor Gibson, a researcher for Greenpeace USA, said: “Climate science is understood by a majority of Americans, liberal and conservative alike. Unfortunately, you won’t meet any of those people, or any climate scientists, at an event like CPAC. “The Heartland Institute is funnelling anonymous money from the US to climate denial in other countries. It relies on the media to advance false equivalence strategies to attempt to normalise fringe beliefs. Climate denial is not a victimless crime, and it’s time for the perpetrators to be held accountable.”"
nan
"Air pollution is recognised as a major threat to human health worldwide. Nine out of ten people breathe polluted air, resulting in 7m premature deaths a year.  While air pollution respects no boundaries, and affects almost all of us, it impacts some populations more than others. Deaths attributed to air pollution are ten times more likely in low and middle income countries compared to high income countries. Sources of outdoor air pollution include industry, traffic and agriculture. Sources of indoor air pollution are mostly cooking and heating using solid fuels (including wood and charcoal). Many people living in urban informal settlements (or slums) are exposed to high levels of indoor and outdoor air pollution. Despite efforts to tackle exposure levels, reductions in air pollution have not been observed. Life in an informal settlement is not easy and there are many daily challenges, of which air pollution is just one. If the choice is between using dirty fuel or not feeding your kids, then is there a choice? Current approaches to reducing exposure to air pollution in informal settlements include  awareness raising and campaigns on how to reduce exposure. But these methods have very little input from the people they target. As a result, they may have a low rate of acceptance. Campaigns also generally focus on one source of air pollution, but effective solutions and improvements to health need to take into account all sources of exposure. And so community-centred approaches are needed to ensure an understanding of the local context and to explore concerns and challenges faced by residents. This will ensure that solutions are culturally relevant, inclusive and therefore more likely to be effective. This is what we have been doing in Mukuru, which is an informal settlement in Nairobi, Kenya. More than 100,000 families live in crowded conditions with limited access to basic services. Exposure to air pollution can lead to respiratory infection, chronic lung disease, heart disease stroke and lung cancer. In Mukuru, exposure is continuous due to burning of rubbish and industrial emissions. The immediate effects reported by residents include burning eyes, sore nasal passages, coughing and asthma attacks.  Along with a series of interdisciplinary colleagues, we set up the AIR Network so that residents of Mukuru could work together with African and European researchers to explore how best to raise awareness and begin to develop solutions to tackle local air pollution issues. Our creative methods and the involvement of the community allowed us to recognise a series of sources of pollution that we might not have otherwise. To minimise “Western” and “academic” preconceptions, which can result in a blinkered view, and to maximise engagement, trust and participation, our network used a variety of creative methods. These included theatre, storytelling, photography and drawing. We were determined from the start to create a democratic and participatory research project so that we could begin to understand the challenges that informal settlement dwellers encounter day to day, with the community deeply involved from the start. We began with a week-long workshop in Mukuru. For many of us, the creative approaches used were novel and we became a collective, learning together – as well as laughing, eating, sharing and building trust. Barriers were broken down not just between community and researcher, but also between researchers from different disciplines.  This is a community that is marginalised, with very few rights or regulations in place to protect them and limited access to basic resources. It is also a youthful community that is hugely self-motivated, bursting with talent, energy and activism. It is key that the voices of communities such as this are heard. The community educated us on which of the creative methods would work well in Mukuru, and for the next six months, we worked on putting our plans into action.  Our team included talented film makers, and we used digital storytelling to document personal experiences of air pollution. Here, for example, Dennis Waweru  talks about the impact of air pollution on the health of his community. Artists from the Mukuru-based Wajuuku Arts Centre painted maps on canvas and took these out into the community so that local residents could use them to identify pollution hotspots and pollution sources. Music was also highlighted as an effective and important communication tool. Local musicians and rappers composed songs to raise awareness about air pollution and the AIR Network itself.  We also used forum theatre (also known as theatre of the oppressed) to develop short plays about key air pollution problems in Mukuru, and then invite local people to become actors and explore potential solutions to the problems presented on stage.  These forum theatre plays were subsequently developed into legislative theatre pieces, which were performed to people in positions of influence or power. Audience members were then also invited to take part in playing out solutions to key air pollution issues, allowing a dialogue to develop between the “ordinary person” and the policy maker, shifting the usual direction of flow and breaking down existing hierarchies. Industry, burning of waste and bad drainage were identified as key sources of air pollution in Mukuru. It turns out that dangerous unregulated working conditions and lack of protective clothing are a major cause of exposure. As is a lack of infrastructure for firefighting, waste disposal (the smoke and smell of burning plastic is constant) and sanitation (sewage was identified by residents as a major source of air pollution). If we had gone into the community with aims and ambitions that had already been decided according to the commonly acknowledged causes of air pollution (traffic, industry, cooking methods) we may not have had space to reveal or acknowledge these other sources. Instead, we identified issues that the community recognises as indirect causes of air pollution, such as workers rights, alleyways between dwellings that are too narrow for fire-fighting equipment, and poor waste management. In September 2018, these activities culminated in an arts festival, Hood2Hood, at the local football ground. A stage and a sound system appeared out of nowhere. Forum theatre and storytelling pieces were performed. Rappers, MCs and dance groups played live. A mural was created. Visual and interactive games were used to collect data. Around 1,500 local people attended the festival during the course of the day, to find out what we had been doing and to make their own contributions to discussions around air pollution.  Wickedly complex global problems such as air pollution, climate change and antimicrobial resistance can only be properly addressed by using multidisciplinary approaches, real world actionable strategies and buy in from the public. Using creativity is key: it allows non-experts to participate more fully in this process so that initiatives and interventions will be culturally relevant and more effective."
"**A pub which ""flouted"" Covid-19 rules by allowing people in to drink during lockdown and serving alcohol without food has been closed by a council.**
Burnley's The Angel Inn had been the subject of the ""highest number"" of coronavirus-related reports in the local area, Lancashire Police told Burnley Council's licensing committee.
The force said in one raid, officers found three drinkers hiding upstairs.
The committee suspended the pub's licence ahead of a 4 December hearing.
In a statement to the committee, PC Michael Jones said a colleague had spotted people drinking illegally at the pub on Accrington Road after investigating lights and music coming from it at about 21:15 GMT on 13 November.
He said those inside had tried to escape, but officers sealed the exits and, after being let in by a woman who said she was drinking alone, ""found three people hiding in an upstairs room"".
All four had been fined for breaching Covid-19 regulations, he said.
PC Jones also stated that during two previous inspections before the second lockdown, the pub was found to be ignoring coronavirus rules about the serving of alcohol.
He said on 17 October, the day Burnley moved from tier two to tier three restrictions, a PC found men sitting near the bar with ""four or five pints of beer lined up on a table [and] little or no evidence of food consumption"".
A week later, a second officer found about 30 people drinking in the pub and a single tray holding servings of pie and peas on a table, he added.
Tier three restrictions at the time banned pubs from serving alcohol without substantial meals.
Following the hearing, the pub's owners said they would be meeting with solicitors on Friday to discuss the situation.
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"

The editors of the _New Republic_ say we have a “moral responsibility” to invade Burma in order to distribute disaster relief. The editors observe that no one taken seriously is seriously advocating doing this and lament: 



This is, put simply, an unacceptable abdication of our moral responsibilities. Even though our standing in the world has been severely diminished by Iraq, we should at least be debating intervention in Burma. There are, no doubt, many logistical complications and unintended consequences that would follow from such a policy. But there are also reasons why it should be a live option. The goal of such an intervention need not be regime change; it should simply be to make sure that a vulnerable population receives the supplies it desperately needs. Of course, if violating the sovereignty of a murderous regime happens to undermine that regime’s legitimacy, then that would not be such a terrible result. But this does not necessarily have to be our goal.



One should not, I suppose, be too surprised that this sort of slipshod advocacy still emanates from the epicenter of liberal imperialism, a publication that was as influential as any in urging the Iraq war on the American people. (Neither should the fact that its leadership attempted to make their non‐​apology apology for Iraq look magnanimous.) The piece’s curtsy at post‐​Iraq reality is even sort of endearing, in a child‐​like way.   
  
  
Note also the focus not on the particular policy of invading and taking responsibility for disaster relief in Burma, but rather on the importance of “debating” such a policy. After all, the _New Republic’s_ writers aren’t going to be the ones to invade the country and deliver the aid. Rather, the _important_ question is whether the political climate will allow for TNR’s writers to churn out tough‐​minded and uncompromising articles that allow them to stretch their rhetorical legs yet still keep them within the beloved Broderian mainstream of American politics.   
  
  
But maybe the most disappointing point of that paragraph is that instead of the rote “to be sure” formulation, the editors chose to dodge completely the substance of the policy they’re advocating for by using the more indirect “there are, no doubt, many logistical complications…” phrasing. Write what you know, guys.
"
"

The United Nations will throw its biggest environmental party in 10 years later this month in Johannesburg. In preparation, the U.N. has rushed to publication a preliminary report about a new environmental pestilence, the so‐​called Asian Brown Cloud (ABC). The U.N. says the Brown Cloud will kill millions and wreck the Asian Monsoon, which is responsible for feeding about 2 billion people in one way or another. But like many U.N. environmental reports this one fails to mention some crucial points.



Nightmarish reports like the ABC have a way of appearing right before big U.N. environmental conferences — and being proven wrong not long thereafter. In 1995, a Geneva meeting, which gave rise to the infamous Kyoto Protocol on global warming, was prefaced with a breathless pronouncement that we now had climate models that matched the real atmosphere, lending credibility to gloom‐​and‐​doom forecasts of climate change. Months later, Nature magazine was compelled to publish a paper showing that the data which the U.N. cited was incomplete, and when all the numbers were put in, the correspondence vanished.



The U.N.‘s most recent world environment confab occurred last fall in Marrakech. Days before that one, we learned that the poor islanders of Tuvalu were being drowned by sea level rises caused by global warming. Within days, an article appeared in Science magazine showing that sea level around Tuvalu has been falling, not rising, for most of the last 50 years.



Lest anyone think the U.N. has learned anything about its environmental misrepresentations, let’s examine the Brown Cloud story.



Summarizing the U.N.‘s report, CNN said that the ABC is so awful that it has “scientists warning that it could kill millions of people in the area, and pose a global threat.” Further, the cloud “could cut rainfall over northwest Pakistan, Afghanistan, western China and west Central Asia by up to 40%.” 



Sleazy air exiting Asia is nothing new to climatologists. Reid Bryson, the eminent scientist who many believe is the progenitor of the modern notion of human‐​induced climate change, wrote about it in the 1950s. Since then, climate scientists have searched and searched through Indian monsoon data to try to find any systematic changes, and there have been none. 



Don’t take my word for it. Look at page 144 of the 2001 compendium on climate change published by the selfsame United Nations, and you won’t find any systematic changes in South Asian rainfall.



The U.N.‘s pre‐​Johannesburg hype prompted CNN to write that the ABC “has led to some erratic weather, including flooding in Bangladesh, Nepal, and northeastern India, [and] drought in Pakistan, and northwestern India.” The fact is that there isn’t a single shred of scientific evidence to back up those claims. In fact, in its 2001 report, the U.N. noted that there’s no evidence for any systematic changes in extreme weather around the planet. 



What’s really killing people in Bangladesh and causing the ABC is poverty. The place is so low‐​lying and poor that a tropical storm, which would harm no one in America, kills 10,000 in the Ganges Delta.



Speaking of tropical storms, they feed on the heat of the surface of the ocean. The more it warms, the more energy can be directed to spin up their fearsome winds. But the ABC blocks out sunlight, reducing the amount of warming at the ocean surface. Everything else being equal, it would reduce the frequency or magnitude of tropical storms in Bangladesh.



When we get near these worldwide gatherings, there isn’t a piece of U.N. science that isn’t political. That’s because what these meetings are about is blaming the West (read: the United States) for environmental degradation, and holding us up for money. Poverty — not America — is the cause of the ABC. Poverty requires the use of cheap fuels, such as dung, and lousy, inefficient ways of combustion, such as cooking fires. And, more than any of my green friends do, poverty recycles: Families grow, which leads to more and more dung fires, and lousier and lousier air.



Rather than shaking down the United States, the U.N. would be better advised to encourage free market development — -which everyone knows is highly correlated with cleanliness — -and discourage its favorite form of political economy, socialism. The history of the Soviet Union, Eastern Europe, and present‐​day China show a clear correlation between Big Government socialism, pollution and poverty. In freer societies, there is less government, less poverty and less pollution.



It’s time for the U.N. to stop hyping pseudo‐​science in support of inefficient, dirty governments, and to get on with the future — -where free markets breed efficiency and environmental protection.
"
"Spending time in nature is good for you. A person’s access to parks and green, open spaces is important for their health, as research from the NHS and the OPENSpace research group at Edinburgh and Heriot Watt universities shows. Spending time in parks lowers the risk of cardiovascular disease and asthma, helps address obesity and mitigates mental health issues. National parks are often considered the best places to escape to and enjoy the benefits of immersion in nature. Occupying whole landscapes in picturesque rural areas, they provide space for hiking, bird watching and mountain biking. Due to their size, they also perform critical environmental functions by providing a home to biodiversity and storing atmospheric carbon in vegetation.   But approximately 50% of the UK’s poorest people live over 15 miles from a national park and most people require transport to get to them. For the most disadvantaged people in Britain, who predominantly live in urban areas, these places can seem largely inaccessible.  Within low income communities, opportunities to explore national parks are hindered by inadequate transport options compared to communities of higher socioeconomic standing. The most affluent 20% of wards in the UK also have five times the amount of green space than the most deprived 10%. Promoting the value of these green spaces for health and well-being is therefore disingenuous without acknowledging that access isn’t equal. Local parks, meanwhile, are embedded within neighbourhoods and could ensure that immersion in nature isn’t just a luxury for the rich to enjoy. Typically starting at about two hectares in size and located within a ten-minute walk of residential areas, local parks provide everyday spaces where people can connect with nature. These are the places where kids play football and ride their bikes and where there is the opportunity for daily contact between people, nature and their communities, all of which is essential for social cohesion.  If people can’t use national parks, perhaps local parks can provide the health and social well-being benefits within the community. As a result of austerity, however, local governments and the environment sector, which are responsible for managing local parks, are underfunded. This affects the quality of these parks and means they are less attractive to their communities. Some local authorities are even considering the sale of their green spaces to limit the annual costs of maintaining them. National parks generate significant income from tourism. In many cases, this is their main economic support. Without the opportunity to exploit similar revenue streams or draw from alternative funding sources, issues of access and quality continue to hit local parks more heavily. “Destination parks”, such as Hyde Park in London or Heaton Park in Manchester, are exceptions – situated in large, urban areas, they enjoy similar opportunities to generate additional revenue as national parks, using events and tourism to generate income. Local parks within poorer communities are unable to attract commercial events with the same frequency or magnitude and are therefore more vulnerable to funding cuts.    All people, regardless of their wealth, should have access to attractive and functional green spaces. However, the UK government’s announced funding boost of £13m for local parks in February 2019 shows an ignorance of the scale of austerity felt by some parks managers. Many have experienced budget cuts of up to 90% since 2010. This means lower quality parks with staff redundancies, reduced maintenance and a falling number of council-run activities.   The falling quality of local parks will hit lower income residents hardest – restricting their interaction with nature and their opportunities to socialise and relax. These are also the people in society who already have the least access to personal transport, disposable income or affiliations with organisations such as the National Trust.   If the slashing of funding for local parks continues, there may be a corresponding fall in attachment to nature among poorer communities, as described by psychologist Richard Louv. Public desire to spend time in nature and concern for its welfare could be lost.  


      Read more:
      Parklife: Britain's beloved urban parks need a funding boost to save them


 Properly funding the environmental and health services provided by parks is essential to ensure liveable places for a majority of the British public. Local parks in particular deserve more attention as they are used more frequently by disadvantaged communities and have ongoing benefits for community cohesion.  While national parks offer ecosystem services and the opportunity to escape for a day among natural beauty, if we don’t acknowledge that access to all green spaces is as important as the benefits they can provide, we risk overlooking the inequality that holds many back from enjoying them."
"It takes an hour from the surface of the Indian Ocean, descending 3,000 metres in a submersible research pod, to reach the bizarre creatures that cluster around hydrothermal vents on the seabed. “You’re in a titanium sphere that is about two metres in diameter,” says evolutionary biologist Julia Sigwart, describing her voyage to Kairei hydrothermal vent field, east of Madagascar.  The vessel is equipped with robotic arms, probes and cameras – like a manned, underwater version of the Mars rover. In lieu of seats, there’s a padded floor. “So you’re hunched up together with the two pilots who are driving it and manipulating it,” she says. With not even a loo on board, it’s definitely on the bijou side for an eight-hour working day, but for Sigwart, director of the marine laboratory at Queen’s University, Belfast, the experience is worth it. “As you go down the light fades out rapidly. When you turn off the lights of the submersible you can see all of the bioluminescence of everything that’s alive in the water all around you – big and small. It’s like a beautiful starscape.” While much of the ocean floor looks like a ghost town to the naked eye, the concentrated patches of life around hydrothermal vents are as densely, if not as diversely, populated as coral reefs. The vents are where mineral-rich hot water, between 300C and 400C (572-752F), bursts out from below the Earth’s crust, swirling into the cold seawater like black smoke. “These smoking chimneys loom up at you, out of the blackness. They’re just incredible,” says Sigwart. One current evolutionary hypothesis is that the special conditions around deep-sea thermal vents sparked the beginnings of life on Earth.  But these rare and vital ecosystems are under serious threat from deep-sea mining for minerals such as zinc, used for car batteries and mobile phone circuit boards, say campaigners. You might expect that in open water, which does not belong to anyone, the seabed would be safe from commodification, but in 2019 Greenpeace reported that 30 floor-exploration licences had been granted worldwide by the International Seabed Authority (ISA), a UN body. Deep-sea mining is the process of retrieving mineral deposits from the deep sea – the area of the ocean below 200 metres, which is the largest and least explored environment on Earth, occupying 65% of the planet’s surface. Metals found there such as copper, nickel, aluminium, lithium, cobalt and mangen are increasingly needed to make batteries, smartphones and solar panels.   When will it happen?So far, 30 exploration licences have been granted by the International Seabed Authority (ISA), a UN body. In total, 1.5m km2 has been set aside for mineral exploration (equivalent to an area the size of Mongolia) in the Pacific and Indian Oceans as well as along the mid-Atlantic ridge. No exploitation contracts have yet been allocated but they are expected to be given out as early as this year when ISA’s Mining Code is expected to be approved. This will be a set of rules “to regulate prospecting, exploration and exploitation of marine minerals in the international seabed area”. Why is it a problem?Critics are concerned mining could do huge damage to the deep sea and the creatures and ecosystems that exist there. Underwater ecosystems like volcanic mountains, hydrothermal vents and deep-sea trenches are still poorly understood. Many endemic deep-sea species could be wiped out by the creation of a single large mine, while more mobile creatures will be indirectly affected by noise and light pollution.  What can be done? Comprehensive studies need to be carried out to assess the potential damage to biodiversity before deep-sea mining goes ahead, according to the International Union for Conservation of Nature (IUCN). Fundamentally, the IUCN also says people need to recycle and reuse products so there is less demand for extraction of natural resources. Researchers have created a list of priorities for deep-sea conservation. The survey, published in Nature Ecology & Evolution, included the responses of 112 scientists.  Phoebe Weston Mining companies in Germany, China, Korea, India and the UK are among the recipients. “They’re not supposed to be used for commercial-scale mining, but several of the licences have been renewed and they’re into a second 10-year term, says Sigwart, adding that the ISA is currently developing a regulatory framework for commercial mining in the high seas. The race is now on for Sigwart and other biologists to identify and learn more about the vent-dwelling creatures and lobby for their protection. Many are only found in these unique and isolated places. The vivid mottled orange snail, Gigantopelta aegis, has only been located in one area estimating 8km squared. Elin Thomas, Sigwart’s PhD student, has set to work assessing the vent species discovered so far against the criteria for the International Union for the Conservation of Nature’s red list. Because of its small, singular habitat and the threat of mining in the Indian Ocean, the Gigantopelta aegis is now classified as “critically endangered”. Another colourful character on the vent scene is Alviniconcha strummeri, named after the Clash’s Joe Strummer on account of its spiky shell resembling punk rockers. Its red list status is “vulnerable”. In total, 15 hydrothermal vent species – described fondly as “super weird” by Sigwart – have been added to the red rist. The mythical-looking sea pangolin, AKA the scaly-foot snail, was the first to be identified as at risk (status: “vulnerable”). Resplendent in armoured skirts that would be the envy of any Roman centurion, the layers of black flaps around its foot, along with its helmet-like black shell, are a result of the very mineral riches that are attracting the mining industry. “The iron that precipitates out of the vent fluid,” says Sigwart, “is incorporated into the shell and the scales of the scaly foot. It hasn’t grown an iron shell, but the available environmental iron on the surface has integrated into it.” The scaly-foot snail and Gigantopelta aegis are the most fascinating to Sigwart, because each has independently evolved a cunning way to bypass the whole kerfuffle of having to eat. All life around the vents depends on bacteria for energy. There are no plants, so the creatures have to either graze on slimy microbial mats, or eat each other. Rather than bother with any of that, however, these two evolutionary geniuses have an internal organ inside which microbes live, providing all their energy needs. It’s not all snails and germs down there, however. There are giant ghostly white crabs scuttling about, stalked barnacles, tube worms, shrimps and mussels. Different vent systems around the world have their own assemblages of animals. The first vents, discovered in the late 1970s, were in the east Pacific and are known for their metre-long lipstick worms. In Sigwart’s experience, people often assume these ecosystems are “out of harm’s way, nobody can reach them. It’s all fine. But it’s no longer fine, because now we’re on a path to developing commercial-scale deep-sea mining and vents are a target. We can no longer naively hope that the depths of the oceans are still pristine and untouched.” “More and more,” says Sigwart, “it’s clear that they are already impacted by human activities. We find plastic in deep-sea sediments, the ocean circulation patterns are being altered by global climate change.” Crucially, she says, the tide is turning when it comes to scientists becoming more vocal about the animals that would otherwise stay out of sight and out of mind. “Deep-sea biology is fascinating and exciting, and it inspires a sense of wonder in everybody,” says Sigwart. “There are very few of us that have the privilege of actually working on these animals and habitats. We have a burden of responsibility to try to explain them to other people before the damage is done.” Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features"
"When we look at the path to zero net emissions by 2050 two things stand out – firstly it is exactly in line with Labor’s old policy of a 45% cut by 2030, and secondly the government’s target of a 26% cut is woefully below what is needed. This week, Anthony Albanese announced that Labor will commit to achieving zero net emissions by 2050. And let’s state clearly – this is a damn good thing. You might want to argue that we need to get to zero net earlier, and that’s a worthy argument, but let us not get silly and suggest this target shows the ALP is somehow hostage to fossil fuel companies. This is a massive step for our economy. Last year Australia expelled 532m tonnes of greenhouse gasses. Aiming to get to zero net in 30 years is not something you can dismiss as a sign of a party doing nothing. The government is currently refusing to set any target. Instead, the prime minister is talking up his “technology over taxation” approach. There is a very easy way for the government to demonstrate it agrees with the science on climate change and is committed to reducing emissions – set a target to reduce emissions that agrees with the science. The IPCC made this pretty clear for governments in its “summary for policymakers” report released in October 2018. It stated that to limit global warming to 1.5C above pre-industrial levels, “human-caused emissions of carbon dioxide would need to fall by about 45% from 2010 levels by 2030, reaching ‘net zero’ around 2050”. We should pause here and note that within six months of this very clear statement, the Coalition went to the election with a target of a 26% cut from 2005 levels by 2030, while the ALP went with a target of a 45% cut. And of course the focus of the debate from most of the media was on the cost of the 45% cut rather than why the Coalition was ignoring the science. A damning moment for our media. Boris Johnson of all people has committed to zero net emissions by 2050, but our government will not, cowed as it is by people such as senator Matt Canavan who tweeted this week: “There is a simple reason that Labor won’t cost its net zero emissions policy because net zero emissions = net zero jobs!” As the Australian Institute’s Matt Grudnoff noted, “net zero jobs” actually means any jobs lost will be balanced by job gains, which is perhaps not what Canavan is trying to argue. This brings me to the second point – this is not an astonishing target. It is in effect just what the ALP was already planning. Getting to zero net emissions by 2050 goes along the same path as aiming for a 45% cut by 2030, as we can see in the graph. I say this not to discount the policy, but to highlight that given it is in line with what the ALP was already targeting and also is what a conservative government in the UK is planning, the big issue is not that the ALP has this target, but that the government does not. It is based on the science and it also will greatly benefit our economy. I have written in the past that getting to zero net zero emissions is going to be tough. If it was easy it would have been done already, so let us not lie and say it can be done with no pain. It will require a huge change in our economy. But the good news is it will hugely benefit our economy and nation. The CSIRO has estimated that if we continue on our current path, our GDP out to 2060 will grow by just 2.1% annually; but if we target zero net emissions by 2050, our economy will be so transformed and improved that our GDP will grow on average each year by 2.75% and real wages will be greatly improved. Doing nothing is not the answer – it is economic degradation. We must get to zero net emissions by at least 2050, and the ALP target – which will need strong interim targets as well – must be the baseline from which all climate-change policy debate occurs. • Greg Jericho writes on economics for Guardian Australia"
"The past may be a surprisingly useful guide for predicting responses to future climate change. This is especially important for places where extreme weather has been the norm for a long time, such as the Indian subcontinent. Being able to reliably predict summer monsoon rainfall is critical to plan for the devastating impact it can have on the 1.7 billion people who live in the region.  The onset of India’s summer monsoon is linked to heat differences between the warmer land and cooler ocean, which causes a shift in prevailing wind direction. Winds blow over the Indian Ocean, picking up moisture, which falls as rain over the subcontinent from June to September. The monsoon season can bring drought and food shortages or severe flooding, depending on how much rain falls and in what duration. Understanding how the monsoon responded to an abrupt climate transition in the past can therefore help scientists better understand its behaviour in the future. When we researched this weather system’s ancient past, we found it was highly sensitive to climate warming 130,000 years ago. Our new study published in Nature Geoscience showed that the Indian summer monsoon pulled heat and moisture into the northern hemisphere when Earth was entering a warmer climate around 130,000 years ago. This caused tropical wetlands to expand northwards – habitats that act as sources of methane, a greenhouse gas. This amplified global warming further and helped end the ice age. The rate at which today’s climate is changing is unprecedented in the geological record, but our study shows how sensitive the Indian summer monsoon was during a global transition into warming in the past and may still be. Over the last one million years, the climate fluctuated between a cold glacial – known as an ice age – and a warm interglacial as the Earth’s position relative to the sun wobbled in its orbit. The last transition from an ice age into the warm climate of the present interglacial – known as the Holocene – occurred around 18,000 years ago. This period of Earth’s history is relatively well understood, but how Earth system processes responded to these climate changes deeper in time is still something of a mystery. A recent expedition to drill deep into the ocean floor of the Bay of Bengal gave an opportunity to reconstruct past Indian monsoon behaviour over hundreds of years before the last ice age. Our study used these deep sea sediments from the northern Bay of Bengal to capture a direct signal of the Indian summer monsoon from 140,000 to 128,000 years ago, hidden in the fossilised shells of tiny microscopic creatures called foraminifera. These plankton species once lived in the upper ocean water column and captured the environmental conditions of the surrounding seawater in the chemical make up of their shells. We detected the ocean surface water freshening from river discharge induced by the rains of the Indian summer monsoon from 140,000 to 128,000 years ago – a sign of the strengthening monsoon system. This occurred when the Earth was coming out of a glacial state and into the interglacial which occurred before the one we live in, separated by a single ice age. During this period – which we’ll refer to as the penultimate deglaciation – sea levels rose from six to nine metres worldwide. Ice-core records show that Antarctica began to warm first during the penultimate deglaciation. Southern Hemisphere warming provided a source of heat and moisture which fuelled the strengthening of the Indian summer monsoon, as seen in our records of surface freshening and river runoff from the northern Bay of Bengal. During this warming period around 130,000 years ago, the Indian summer monsoon responded to southern hemisphere warming while the northern hemisphere and other monsoon systems, such as the East Asian summer monsoon – which affects modern day China, Japan and the Far East – remained in a glacial state. The Indian summer monsoon pulled heat and moisture northwards, driving glacial melting in the northern hemisphere and helping tropical wetlands expand their range. These expanding tropical wetlands resulted in more methane release into the atmosphere which caused even more warming, setting changes in motion which ended the global ice age. The Indian summer monsoon is an incredibly dynamic system. Though confined to the tropics, the system is sensitive to climatic conditions in both hemispheres. Due to its role in contributing to methane emissions, the Indian summer monsoon also has an outsize impact on the global climate. Monsoons should not be viewed in isolation, just as the polar ice sheets shouldn’t. Earth’s internal climate system is intrinsically linked and abrupt changes at one place can have significant consequences over time elsewhere. Climate change is inevitable. Our response to it isn’t. Click here to subscribe to our climate action newsletter."
"A group of forestry and climate scientists are calling for an immediate and permanent end to the logging of all native forests across Australia as part of a response to climate change and the country’s bushfire crisis. In an open letter, the group said forestry workers involved in logging in native forests should be redeployed to support the management of national parks.  A briefing document to back the letter, coordinated by The Australia Institute thinktank, argues logging in wet eucalypt forests promotes more flammable regrowth. Dr Jennifer Sanger, a forest ecologist who is in Canberra today to deliver the letter told Guardian Australia: “As we face this climate crisis, we see our forests are worth far more standing. “We have to start taking this climate emergency more seriously and protective native forests is a simple step we could take and in my mind, a logical call.” Some experts told Guardian Australia they disagreed, saying it could effectively rule out one potential response to managing forests in the face of climate change. Among the signatories to the letter are University of Tasmania’s distinguished conservation ecologist Prof Jamie Kirkpatrick, James Cook University ecologist Prof Bill Laurance, and Prof Tim Flannery, of the University of Melbourne. The letter says: “We write to ask you to respond to the climate, fire, drought and biodiversity loss crises with an immediate nationwide cessation of all native forest logging.” Large old-growth trees are important for capturing and storing carbon, the letter said, adding that native forest logging “is heavily subsidised by our taxes, which can be better spent on fire mitigation”. Government data shows that 5m hectares of native forests are open to logging and that annually, 73,000 hectares are harvested. According to the briefing document, 12% of logs harvested in Australia come from native forests, and an end to native forest logging would directly impact 3,250 workers.  “The best economic use for native forests would be to leave the forests intact and push for inclusion in a carbon trading scheme,” the document said. When wet eucalypt forests are cleared the regrowth and understorey is drier and more flammable, according to the document. Species that live in forests make up 80% of all Australia’s threatened species. Sanger added: “Native forest logging just isn’t beneficial. It is not profitable, and there are not a lot of jobs that rely on it. “Ecologically [forests] are under a lot of stress from other impacts, including climate change and habitat destruction, and it does not make sense to be logging these forests.” The call to ban native forest logging comes after the government announced a Royal Commission into the bushfire crisis that focuses on adapting to climate change, with measures including the use of hazard reduction to be investigated. Prof Rod Keenan, the University of Melbourne’s chair of forest and ecosystem science, told Guardian Australia he did not agree that all native forest logging should cease. “The letter proposes a simplistic solution to a complex problem. Current timber harvesting is not the problem,” he said. Native forest logging had declined over the last 20 years and was heavily regulated to protect habitats, he said, “so the environmental benefits of such a ban are unclear”. He argued a ban would have “significant social and economic impacts for local communities” that had already been hit hard by fires. He said: “The suggestion we can supply all our wood requirements from plantations is also incorrect. We have a large trade deficit in wood products, there is no immediate replacement for native timbers to industry and the plantation estate has taken a significant hit from recent fires. “The industry will need to adjust to recent fire impacts and adapt to a changing climate. New types of silviculture, including timber harvesting, can be part of the solution in reducing the impacts of future fires. “Rather than knee-jerk decisions, we need to keep all options on the table as we work through the best responses to these catastrophic fires.” Prof Peter Kanowski, an international forest governance expert at the Australian National University’s Fenner School of Environment and Society, said he also could not support a ban on native forest logging. “We need to protect populations of plants and animals post-fire, and we need to organise any timber harvesting cognisant of that,” he said. “But beyond that, we have to think differently about a much more adaptive and integrated approach to how we manage forested landscapes in the future under climate change.” He said that banning native forest logging would “be precluding options that we should not be precluding”. The Australian Forest Products Association declined to comment on the open letter."
"One reason why people find it difficult to think about climate change and the future may be their understanding of human history. The present day is believed to be the product of centuries of development. These developments have led to a globalised world of complex states, in which daily life for most people is highly urbanised, consumerist and competitive. By this account, humanity has triumphed over the dangers and uncertainties of the natural world, and this triumph will continue to unfold in the future. Anything else would seem to be going “backwards”, in a world where “backwardness” is pitied or despised. But it is now clear that we haven’t triumphed. The future has become very uncertain and our way of thinking needs to change. Could new historical narratives help? How might they look? The current view of the past, present and future as a trajectory of progress is constantly reiterated by politicians and taught to children in schools. It does not offer many alternatives to the ideas and practices driving climate change and ecological breakdown.  There is a reassuring promise in this narrative that things naturally improve with time, requiring no commitment from ordinary people. Progress is delivered through steady work by governments and scientists, with moments of transformation by activists or visionaries. The direction of history itself is towards the general good.  It is very hard, then, for anyone thinking in this framework to imagine a future in which societies adapt to the challenges of climate change. This is especially the case where adaptions might have to take the form of significantly reduced consumption, unfamiliar forms of social organisation, and harder work to produce food or manage local environments.  These ideas about the future look very different from the technologically advanced and globalised tomorrow that the progress narrative seemed to promise. At present, ideas in popular culture about the impact of climate change are often apocalyptic and dystopian. Ideas about mitigating climate change seem limited to fantasies of last-minute salvation by scientific genius or alien intervention. In this respect, climate change stands in contrast to other issues that are more rooted in a cultural understanding of history. Arguments around Britain’s departure from the European Union, for example, matter to people across the political spectrum because they’re integrated with ideas about the nation’s past trajectory, as well as the immediate concerns of people and communities.  Responding to climate change, meanwhile, demands a collective rupture from several centuries of development within a timescale of decades. This poses both a challenge and an opportunity to the study of history. 


      Read more:
      European colonisation of the Americas killed 10% of world population and caused global cooling


 Fields such as climate, environmental or global history help to think about the past in planetary rather than national terms. Some of that questions the western interpretation of history and the exploitation of people and nature which punctuates it. Recovering the stories of people marginalised from these narratives helps people think about life in a different light. Many indigenous peoples, for example, have ideas about the past that situate humans within complex ecosystems. Environmental historians also ask how past societies interacted with their surroundings and consider how and why more ecologically stable ways of living were destroyed through colonisation by powerful, expanding empires. Bruce Pascoe’s Dark Emu looks at the sustainable land management techniques of Australia’s First Peoples, which were ignored by British settlers. He suggests a way forward for Australian agriculture based on those practices. Their subject also explores how climatic and environmental change affected earlier civilisations. The fall of Rome, for example, fits into a global shift in climate conditions around 500 C.E. that also resulted in the “fall” of complex states in China, India, Mesoamerica, Peru, and Mexico.  Population health and biodiversity improved significantly in the following period, popularly known as the “Dark Ages”. So were powerful states always a good thing? The destruction of indigenous populations by Europeans from 1500 onwards may have caused huge environmental changes on the American continent. As 56 million lives were extinguished, the regrowth of forests on abandoned farms may have absorbed enough atmospheric carbon to cool the global climate in the Little Ice Age. Societies across the world suffered during this period. In Europe, it was a time of savage persecution of “witches”, partly due to the belief that they were deliberately causing the “unnatural” weather conditions.  The Dutch Republic did show resilience in the harsher climate conditions of “the frigid golden age”. Its innovations for harnessing the energy of changing weather and wind patterns in shipping fuelled an aggressive trading empire. While such strategies are not templates for future action, they do underline the fact that humans have and can adapt with radically altered lifestyles, expectations, aspirations and standards of living. They needn’t always aspire to more of the same that they have at present. This idea begs questions about the nature of history itself. Must history continue to be a story of humans alone? Could it become the study of humans in complex ecosystems, exploring the entangled pasts of people, animals, insects, microbes, plants, trees, forests, soils, oceans, glaciers, stones, volcanic eruptions, solar cycles and orbital variations? Narrating a richer past would lessen the shock of discovering that we are, after all, earthbound inhabitants of the only planet where life is known to exist. It could show us that our survival is dependent on countless complicated and delicate relationships. Relationships that “progress” narratives have required us to ignore, despise and even fear.  In recognising that the established view of human history can and must change, people can think radically about society, rather than following the present course out of a failure of imagination."
"

The American pika ( _Ochotona princeps_ ) is an insanely cute critter often found in above-timberline rock fields in the western U.S.   






Because they often live near mountain peaks, there’s been concern that global warming could push them over the top, to extinction. Writing in the _Journal of Mammalogy_ , Smith and Nagy (2015) state that American pikas ( _Ochotona princeps_ ) “have been characterized as an indicator species for the effects of global warming on animal populations,” citing the works of Smith _et al_. (2004), Beever and Wilkening (2011) and Ray _et al_. (2012). Indeed, as they continue, “a consideration of the effects of climate, primarily recent warming trends due to climate change, has dominated much of the recent literature on American pikas and their persistence.” Hoping to provide some additional insight on the subject, the two Arizona State University researchers set out to investigate the resilience of a pika metapopulation residing near Bodie, California, USA, that was exposed to several decades of natural warming.   




The investigation, which Smith and Nagy characterized as “the longest study of any pika species,” focused on the Bodie metapopulation for two primary reasons. First, it is “situated at the warmest locality of any longitudinal study of the American pika.” As such, its area of habitat is comparatively warm and fully capable of inducing warm temperature stress. Second, the population has been well-studied, having been censused (for patch occupancy data) several times since the early 1970s. Given these two characteristics of the Bodie metapopulation (location and well-studied) the two researchers were able to test for a relationship between pika extinctions/recolonizations and chronic/acute temperature warming. So what did their analysis reveal?   
  
With respect to _chronic_ temperature warming, Smith and Nagy report that despite a relatively high rate of patch (islands of pika-suitable habitat) turnover across the study location, there was “a near balance” of pika patch extinctions and recolonizations during the past four decades of intense data collection (see figure below). Furthermore, a series of statistical analyses that were performed on the patch turnover and historic temperature data revealed there was “no evidence that warming temperatures have directly and negatively affected pika persistence at Bodie.” In fact, the only significant correlation they found among these two parameters occurred between mean maximum August temperature and the number of pika recolonizations the following year, which correlation was _positive_ , indicating that _higher_ August temperatures lead to a _greater_ rate of pika recolonization the next year, a result which the authors describe as “in the opposite direction of the expectation that climate stress inhibits recolonizations.”   






_Two decades of patch extinctions and recolonizations in a Bodie, California, American pika (Ochotona princeps) metapopulation. Source: Smith and Nagy (2015)._



With respect to _acute_ temperature warming, defined as the number of hot summer days exceeding a temperature threshold of 25°C or 28°C, Smith and Nagy write that “neither warm chronic nor acute temperatures increased the frequency of extinctions of populations on patches, and relatively cooler chronic or acute temperatures did not lead to an increase in the frequency of recolonization events.”   
  
Taken together, the above findings demonstrate that the Bodie metapopulation of American pikas is “resilient at the individual (Smith, 1974) and population scales” to both chronic and acute temperature warming, and has “been so for at least 60 years.” And, as an “indicator species” for the effects of global warming on animal populations, the future for American pikas and other animal species looks bright!   
  
**References**   
  
Beever, E.A. and Wilkening, J.L. 2011. Playing by new rules: altered climates are affecting some pikas dramatically -- and rapidly. _The Wildlife Professional_ **5** : 38-41.   
  
Ray, C., Beever, E. and Loarie, S. 2012. Retreat of the American pika: up the mountain or into the void? Pp. 245-270 in _Wildlife conservation in a changing climate_ (J.F. Brodie, E. Post, and D.F. Doak, eds.). University of Chicago Press, Chicago, Illinois.   
  
Smith, A.T. 1974. The distribution and dispersal of pikas: influences of behavior and climate. _Ecology_ **55** : 1368-1376.   
  
Smith, A.T., Li, W. and Hik, D. 2004. Pikas as harbingers of global warming. _Species_ **41** : 4-5.   
  
Smith, A.T. and Nagy, J.D. 2015. Population resilience in an American pika ( _Ochotona princeps_ ) metapopulation. _Journal of Mammalogy_ **96** : 394-404.


"
"**The aviation sector is ""on the brink"" according to the boss of a regional airport which has shut down temporarily due to a lack of demand.**
Cornwall Airport Newquay hopes to reopen in December ""for a rally of demand ahead for the Christmas period"".
Managing director Pete Downes said airlines had made it ""clear there is not sufficient demand to make it viable"".
Cornwall Chamber of Commerce said it was ""absolutely devastated"".
""If we are serious about Cornwall being a 21st century business destination then we have really got have an airport just as we have to have roads and digital infrastructure like 5G and superfast broadband,"" its chief executive Kim Conchie said.
The airfield at Cornwall Airport Newquay will remain available for emergency flights and staff have been furloughed.
The airport is subsidised by Cornwall Council, who agreed in September that Â£5.6m could be diverted from its Â£12m funding for the Cornwall spaceport, should no other support be secured from the government.
Mr Downes said it was a ""turning point for the industry"" and called on the government to offer greater support to regional airports.
""The government has to decide how much of this industry it wants to have left at the end of this period,"" he said.
""Everybody has been surviving with drastically reduced revenues and continuing to run airports for the benefit of regional connectivity around the UK, with no sector specific support from the government.
""People have been holding on but unless some support is forthcoming over the winter we will start to lose parts of this industry. And once they are gone we will not be able to get them back.""
The Department for Transport said it recognised the impact of the second lockdown on the travel industry, including regional airports, and said work was being done to ""develop measures to support aviation's long-term future"".
""The furlough scheme has been extended, and we're providing wider support through action on airport slots, loans and tax deferrals,"" a spokesman said."
"
Share this...FacebookTwitterUpdated 4/25/2011, 16:53 CETMSNBC, surprisingly, reports here on the very dark side of Earth Day co-founder Ira Einhorn. Let us recall that the first Earth Day took place on April 22, 1970 (April 22 happens to be the birthday of Vladimir Lenin).
MSNBC starts with (emphasis added):
Ira Einhorn was on stage hosting the first Earth Day event at the Fairmount Park in Philadelphia on April 22, 1970. Seven years later, police raided his closet and found the ‘composted’ body of his ex-girlfriend inside a trunk.”
According to the report, he was a hippie guru who advocated peace, love and flower power, but then murdered his girlfriend Helen “Holly” Maddux after she had broken up with him. After she went to his apartment to gather her things, she was never seen again. Einhorn told the police:
…that she had gone out to the neighborhood co-op to buy some tofu and sprouts and never returned.”
Tofu and sprouts at the neighborhood co-op? It gets even funnier. Einhorn was arrested but he jumped bail and fled to Europe. Eventually, after having been on the run for 23 years, he was extradited to the States by the French. Here’s how he explained the murder to police:
Taking the stand in his own defense, Einhorn claimed that his ex-girlfriend had been killed by CIA agents who framed him for the crime because he knew too much about the agency’s paranormal military research.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Maybe the real reason she was composted was probably because she had voted Republican, or criticised Paul Ehrlich. who knows? Indeed there are a number of theories Einhorn used to explain the death of his former girlfriend, including claiming that it was a set-up by the CIA and that he was surprised when he learned Holly Maddux’s body was found in his steamer chest in his closet in his apartment – months later and after foul liquids had been oozing through the floor and into the apartment below.
Wikipedia writes he was a friend and contemporary of Jerry Rubin and Abbie Hoffman. Einhorn’s past is naturally an embarrassment for the Green movement, and like the Medieval Warm Period, they would prefer for us forgetting about it. MSNBC writes:
Understandably, Earth Day’s organizers have distanced themselves from his name, citing Gaylord Nelson, an environmental activist and former Wisconsin governor and U.S. senator who died in 2005, as Earth Day’s official founder and organizer.”
So let’s add Einhorn to the list of green psychopaths along with Bin Laden and Charles Manson – with Bill “bummer outer” McKibben, Michael “f-ing” Tobis, Richard “1010” Curtis and Ben “beat-the-crap” Santer on the potential list.
Finally, claiming that one invented the Internet seems to be a sort of mental bug greens are prone to having. According to http://cmm.lefora.com, Einhorn, like Al Gore, also claimed he created the Internet:
He further claims he ‘left the money economy in 1963, creating a lifestyle based on information transfer.’ Einhorn credits himself with ‘creating an international information network under the auspices of Bell of Pa. and AT&T, called the Internet before the Internet that reached into 27 countries.’  Then he provided a (long) list of accomplishments.”
Weird.
And here’s much more on the violent background of the man who was part of the founding of Earth Day: http://cmm.lefora.com/2009/01/20/ira-einhorn-bully-conman-mooch-murderer/.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe’ve all heard how climate change causes everything that’s bad for humans and the planet to increase and to intensify. Well, ironically, that also now includes boredom on the subject of climate change itself. Many are simply just getting bored to death by it.
Indeed a spate of recent studies have shown that people are growing tired and unconcerned about climate change in a number of countries. Much of this is due to the lack of credibility the science is suffering in the wake of all the scandals, scams and exaggerations that have been exposed over the last couple of years.
Climate boredom has also gripped Sweden too, as the English-language The Local writes here:
Swedes’ environmental interest plummets: report

Public interest in climate issues has dropped to its lowest point in five years, according to the upcoming annual SOM report from the University of Gothenburg. Only 14 percent of Swedes currently consider the environment to be among our most important problems, reports daily newspaper Svenska Dagbladet.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This is the lowest figure since 2006, and far behind the record-high figures of the late eighties, when Swedes’ environmental involvement truly boomed.”
When one considers how often people have been exposed to the same, worn-out climate disaster reruns about surging sea levels, storms, weather extremes, droughts, floods, dying oceans, etc., then no one ought wonder why boredom on the subject is so deeply entrenched.
According to The Local, greens blame the lack of media interest in climate change. The figures show a plummet in Swedish public interest:
This trend has now been broken, and last year this number had decreased heavily, from 21 to 14 percent.”
To be honest, it has nothing to do with media slacking off on the subject. It has everything to do with them making false projections time and again. You can only cry wolf so many times before you lose all credibility and respect. Once shown to be a liar and a charlatan, it later becomes virtually impossible to convince others of anything else.
Today climate change is probably not boring only for climate skeptics. Indeed for us it is now just beginning to get interesting.
Share this...FacebookTwitter "
"

Scientific American has sicced the big dogs on Danish statistician Bjorn Lomborg for having the audacity to publish a highly referenced book, “The Skeptical Environmentalist,” which argues that global warming and many other environmental “threats” are overblown. What gives?



Scientific American now joins the magazines Science and Nature in blasting Lomborg. They all editorialize that his “book is a failure” and call out four well‐​traveled attack dogs from the Washington big government/​greenie/​lefty establishment in support. They include:



*John Holdren, a defense expert from Harvard. In 1995, he published a paper for the United Nations University advocating “a condition in which no nation ‘s military forces were strong enough to threaten the existence of other states.” Good thing we didn’t listen. *Tom Lovejoy, former director of the World Wildlife Fund, the biggest green lobbying organization in the history of the planet. *John Bongaarts, vice president of the Population Council, the most influential lobby in the Down With People crowd. And, *Steve Schneider from Stanford. Compared to the rest, Schneider is a real atmospheric scientist, and (naturally) he wrote the nastiest of the four Fatwas on Lomborg.



Why draw so much attention to a book you don’t want to sell? Clearly, the editorial boards of Nature and Scientific American, as well as the leadership of the American Association for the Advancement of Science (AAAS, the publisher of Science) perceive a big threat if Lomborg goes unanswered.



This writer has hung around D.C. enough to smell the danger: “The Skeptical Environmentalist” threatens billions of taxpayer dollars that go into the global change kitty every year. The AAAS isn’t located on H Street in Washington — known locally as “gravy train lane” for its packs of lobbyists — for nothing.



Do the arguments against Lomborg have merit? Let’s examine two of the many assertions made by Schneider against Lomborg.



Emissions Scenarios. Schneider complains that “Lomborg …dismisses all but the lowest” scenarios for future carbon dioxide emissions and consequent global warming.



Lomborg does so with good reason. An analysis of atmospheric carbon dioxide concentrations over the last quarter‐​century reveals that the standard assumption of strong exponential growth is wrong. You could read about that it in NASA scientist James Hansen’s recent writings in the “Proceedings of the National Academy of Sciences.”



Future Warming. Schneider takes great exception with Lomborg’s statement that “temperatures will increase much less than the maximum estimates from the IPCC” with the likely change less than 2ºC (3.6ºF) by 2100.



The truth is that Lomborg is behaving like a scientist here, and Schneider and Scientific American don’t like the result.



As is shown graphically in the latest report by the UN Intergovernmental Panel on Climate Change (IPCC), the ensemble of future climate models predicts a warming that, once started, continues at a virtually constant rate for the next century. However, they differ in the rates of their projected warming.



It is also the consensus of the IPCC, first stated in 1996 and repeated in 2001, that “the balance of evidence suggests a discernible human influence on global climate.” In other words, alterations of the atmosphere resulting from human emissions are producing a detectable signal in global and regional temperatures. By using the combination of those two realities, Lomborg is forced to conclude that warming will be relatively modest. That is all a scientist can do: reconcile disparate models with observed data.



The reason Scientific American is apoplectic about that argument is because it is scientific and convincing. If it convinced the Bush administration to walk away from Kyoto, how long will it be before it convinces Congress to derail the multibillion‐​dollar gravy train feeding the global warming claque?



Then there’s the jealousy component. Each of Scientific American’s four writers also has their own books. While Lomborg’s is immensely popular, ranking #1 in sales under “Environmental Science” (and 354th overall), the others aren’t so hot. Comparative sales from ama​zon​.com for Jan. 4 show the following: John Bongaarts’ “Beyond Six Million” ranks 463,784; Tom Lovejoy’ s “Blueprint for a Green School” is at 583,463; Schneider’s “Are we Entering the Greenhouse Century” comes in at 574,469; and trailing this field of glueboxes is John Holdren’s “Global Ecology,” the 1,340,727th best selling book at Ama​zon​.com. If these were horses, you’d have to clock them with a calendar.



The bottom line is that all of the Lomborg‐​bashing makes Scientific American look like a bunch of attack dogs, manufacturing arguments that won’t hunt and are in fact canine themselves.
"
"Where is Boris Johnson? As flooding devastates large parts of the north, the Midlands and Wales, the prime minister’s strategy is to hide from public scrutiny and hope the whole thing blows over. It’s now 10 days and counting since his last public appearance. This is a huge miscalculation. Because for all of Dominic Cummings’ talk of “superforecasting”, the government is ignoring the issue that will define politics for decades to come: our response to the climate crisis. From wildfires in Australia and the Amazon, to extreme heat in India, to drought in east Africa, climate-induced extreme weather is already reshaping our planet in disastrous ways.  To win, Labour needs to make sure 2024 is a climate election. Too often we have fought on terms defined by the Tories. David Cameron and George Osborne dominated politics in the aftermath of the financial crisis, winning two elections by pinning the blame on Labour’s fabled “overspending”. Last December, the narrative was defined by “Get Brexit done”, and we failed to provide a convincing answer. Labour can only win the next election by confidently setting the agenda. Our shift to the politics of anti-austerity under Jeremy Corbyn put the Tories on the back foot and enabled the gains made in 2017. But as our devastating loss to Johnson showed, standing still is not enough. That’s why as leader, I’ll centre Labour’s plans to tackle the climate crisis in everything we do, and make sure it defines the next election. That starts with being clear about where the blame for climate breakdown lies: with the fossil fuel executives who are knowingly burning our planet, and with the Tories who refuse to take the action necessary to stop them. On this terrain, Labour can win. I know that we can build a winning electoral coalition around our green industrial revolution, uniting working people everywhere with a just, aspirational response to the climate crisis. Long before the next election, people in every community up and down the country need to know concretely what this will mean for them. Well-paid, unionised green jobs in every town, city and village; economic renewal and pride for communities left behind by deindustrialisation; and a leading role for our country in combating the climate crisis across the world. This isn’t just a set of policies that need to be sold better, it has to be the core of our vision for a socialist future. The groups of voters Labour must unite in order to win are often pitted against each other. We’re told they are polarised and hold opposing interests: city against town, young against old, homeowner against renter. But they all want a livable planet, and all aspire to a future that is healthier, wealthier and more free. Labour’s green industrial revolution can unite diverse communities across the country on this basis, forming the coalition we need to win. But we cannot wait until 2024. Labour needs a leader who will relentlessly hold the Conservative government to account over its failings on climate change here and now. These failings couldn’t be starker than in Johnson’s abandonment of communities devastated by flooding. The government must take urgent steps to seriously address the crisis. First, immediate action must be taken to tackle the root cause, by rapidly phasing out the UK’s greenhouse gas emissions, working to a 2030 decarbonisation target. We cannot afford to delay action any longer, with vague or distant targets. Second, the government must follow the recommendations of the Environment Agency and National Infrastructure Commission and increase its capital spend on flood defences to £1bn per year. Alongside restoring the UK’s natural landscapes, and ensuring our firefighters are properly funded, equipped and resourced, these are simple steps that could be taken immediately to protect at-risk communities from further flood damage. Lastly, with the damage from flooding projected to increase substantially, I would question why costs should be borne by homeowners rather than those most responsible for causing the climate crisis. That is why I have called for a Climate Justice Fund, which would make fossil fuel companies pay damage costs incurred by flooding and wildfires, as well as contributing to losses suffered by countries in the global south. Soon, all politics will be climate politics. Labour needs a leader who recognises this, and has a strategy to build a winning coalition around a plan for rapid decarbonisation and economic renewal, working with trade unions, community groups and the climate movement. This is the basis on which we will win in 2024, transform Britain – and help save the planet.  • Rebecca Long-Bailey is the Labour MP for Salford and Eccles and a candidate for the Labour party leadership"
"
The Amplification Invitation 
 
Guest Post by Willis Eschenbach
Tropical: the ITCZ from space. Source: NASA Earth Observatory. Click for larger image
A  while ago I started studying the question of the amplification of the tropical  tropospheric temperature with respect to the surface. After months of research  bore fruit, I started writing a paper. My intention was to have it published in  a peer-reviewed journal. I finished the writing about a week ago.
During  that time, I also wrote and published The Temperature Hypothesis here on WUWT. This got me to thinking about science, and about how we establish  scientific facts. In climate science, the peer review process is badly broken.  Among other problems, it is often an “old boy” system where very poor work is  waved through. In common with other sciences, turnaround of ideas in journals  takes weeks. Under pressure to publish, journals often do only the most cursory  examination of the papers.
Upon reflection, I have decided to try a  different way to examine the truth content of my paper. This is to invite all of  the authors whose work I discuss, and other interested scientists of all  stripes, to comment on the paper and on whether they can find any flaws in it.  To facilitate the process I have provided all of the code and data that I used  to do the analysis.
To make this process work will require cooperation.  First, I ask for science and science only. No discussions of motives. No ad  homs. No generalizations to larger spheres. No asides. No disrespect, we can be  gentlemen and gentlewomen. No comments on politics, CO2, or AGW, no snowball  earth. This thread has one purpose only, to establish whether my ideas stand:  to either attack and destroy the ideas I put forth in the paper below, or to  provide evidence and data to support the ideas I put forth below.

People think science is a cooperative endeavor. It is not. It is a  war of ideas. An idea is put out, and scientists gather round to attack it and  disprove it. Sometimes, other scientists may support and defend it. The more  fierce the attack, the better … because if it can withstand the strongest  attack, it is more likely true. When your worst scientific enemies and greatest  disbelievers can’t show that you are wrong, your ideas are taken as scientific  fact. (Until your ideas in turn are perhaps overthrown). Science is a blood  sport, but all of the attack and parry is historically done in private. I  propose to bring it out in public, and I offer my contribution below as the  first victim.
Second, I will insist on a friendly, appreciative attitude  towards the contributions of others. We are interested in working together to  advance our primitive knowledge of how the climate works. We are doing that by  trying to tear my ideas down, to disprove them, to find errors in them. To make  this work we must do this with respect for the people involved and the ideas  they put forwards. You don’t have to smash the guy to smash the idea, in fact it  reduces your odds.
Third, no anonymous posting, please. If you are think  your ideas are scientifically valid, please put your name on them.
With  that in mind, I’d like to invite any and all of the following authors whose work  I discuss below, to comment on and/or tear to shreds this study.
J. S.  Boyle,  J. R. Christy,  W. D. Collins,  K. W. Dixon,  D. H. Douglass,  C.  Doutriaux,  M. Free, Q. Fu,  P. J. Gleckler,  L. Haimberger,  J. E. Hansen,  G.  S. Jones,   P. D. Jones,  T. R. Karl,  S. A. Klein,  J. R. Lanzante,  C. Mears,   G. A. Meehl,  D. Nychka,  B. D. Pearson,  V Ramaswamy,  R. Ruedy, G. Russell,  B. D. Santer,  G. A. Schmidt,  D. J. Seidel,  S. C. Sherwood, S. F. Singer,  S.  Solomon,  K. E. Taylor,  P. W. Thorne,  M. F. Wehner,  F. J. Wentz,  and T. M.  L. Wigley
(Man, all those 34 scientists on that side … and on this side  … me. I’d better attack quick, while I have them outnumbered … )
I  also invite anyone who has evidence, logic, theory, or data to disprove or to  support my analysis to please contribute to the thread. Because at the end of  this process, where I have exposed my ideas and the data and code to the attacks  of anyone and everyone who can find flaws with them, I will have my own answer.  If no one is able to disprove or find flaws in my analysis, I will consider it  to be established science until someone can do so. Whether you consider it  established science is up to you. However, it is certainly a more rigorous  process than peer-review, and anyone who disagrees has had an opportunity to do  so.
I see something in this nature, a web-based process, as the future of  science. We need a place where scientific ideas can be brought up, discussed and  debated by experts from all over the world, and either shot down or  provisionally accepted in real time. Consider this an early experiment in that  regard.
Three months to comment on a Journal paper is so 20th century.  I’m amazed that the journals haven’t done something akin to this on the web,  with various restrictions on reviewers and participants. Nothing sells journals  like blood, and scientific blood is no different from any other.
So  without further ado, here is my paper. Tear it apart or back it up, enjoy, ask  questions, that’s science.
My best to everyone.
w.



A New Amplification Metric
 
 
ABSTRACT: A new method is proposed for exploring  the amplification of the tropical tropospheric temperature with respect to the  surface. The method looks at the change in amplification with the length of the  record. The method is used to reveal the similarities and differences between  the HadAT2 balloon, UAH MSU satellite, RSS MSU satellite, RATPAC balloon, AMSU  satellite, NCEP Reanalysis, and five climate model datasets In general, the  observational datasets (HadAT2, RATPAC, and satellite datasets) agree with each  other. The climate model and reanalysis datasets disagree with the observations.  They also disagree with each other, with no two being  alike.

Background

“Amplification” is the term  used for the general observation that in the tropics, the tropospheric  temperatures at altitude tend to vary more than the surface temperature. If the  surface temperature goes up by a degree, the tropical temperature aloft often  goes up by more than a degree. If surface and tropospheric temperatures were to  vary by exactly the same amount, the amplification would be 1.0. If the  troposphere varies more than the surface, the amplification will be greater than  one, and vice versa.
There are only a limited number of observational  datasets of tropospheric temperatures. These include the HadAT2 and RATPAC  weather balloon datasets, and two versions of the Microwave  Sounding Unit (MSU) satellite data. At present the satellite record is about  thirty years long, and the two balloon datasets are about 50 years in length.

Recently there has been much discussion of the the Santer et al.  2005, Douglass et al. 2007, and the Santer et al. 2008 papers on tropical  tropospheric amplification. The issue involved is posed by Santer et al. 2005 in  their abstract:
The month-to-month variability of tropical  temperatures
is larger in the troposphere than at the Earth’s  surface.
This amplification behavior is similar in a range of
observations  and climate model simulations, and is
consistent with basic theory. On  multi-decadal timescales,
tropospheric amplification of surface warming is a  robust
feature of model simulations, but occurs in only one
observational  dataset [the RSS dataset]. Other observations show weak or
even negative  amplification. These results suggest that
either different physical  mechanisms control
amplification processes on monthly and  decadal
timescales, and models fail to capture such behavior, or
(more  plausibly) that residual errors in several
observational datasets used here  affect their
representation of long-term  trends.


Metrics of  Amplification

Santer 2005 utilizes two different metrics of  amplification, viz:
We examine two different amplification metrics:  RS(z), the ratio between the temporal standard deviations of monthly-mean  tropospheric and TS anomalies, and Rβ(z), the ratio between the multi-decadal  trends in these quantities.


Neither of  these metrics, however, measures the amount of the amplification at altitude  which is related to the surface variations. Ratios of standard deviations  merely measure the size of the swings. They cannot indicate whether one is an  amplified version of the other. The same is true of trend ratios. All they can  show is the size of the difference, not whether the surface and atmosphere are  actually correlated.
In order to measure whether one dataset is an  amplified version of another, the simplest measure is the slope of the ordinary  least squares regression line. This measures how much one temperature varies in  relation to another.
Despite a variety of searches, I was unable to find  published studies showing that the “amplification behavior is similar in a range  of observations and climate model simulations” as Santer et al. 2005 states.  To investigate whether the tropical amplification is  “robust” at various timescales, I decided to calculate the tropical and global  amplification (average slope of the regression line) at all time scales between  three months and thirty years or more for a variety of temperature datasets.  I  started with the UAH and the RSS versions of the satellite record. Next I looked  at the HadAT2 balloon (radiosonde) dataset. The results are shown in Figure 1.
To measure the amplification, for every time interval (e.g. 5 months) I  calculated the amplification of all contiguous 5-month periods in the entire  dataset. In other words, I exhaustively sub-sampled the full record for all  possible contiguous 5-month periods. I took the average of the results for each  time period. Details of the method are given in the Supplementary Online  Material (SOM) Sections 2 & 3 below.
I plotted the results as a curve  which shows the average amplification for each of the various time periods from  three months to thirty years (the length of the MSU datasets). This shows the  “temporal evolution” of amplification, how it changes as we look at longer and  longer timescales. I show the results at a variety of pressure levels in Figure  1. In general, at all pressure levels, short term (3 – 48 month) amplifications  are much smaller than longer term amplifications.

Figure 1. Change of amplification with  length of observation. Left column is amplification in the tropics (20N/S), and  the right column is global amplification. T2 and TMT are middle troposphere  measurements. T2LT and TLT are lower troposphere. Starting date is January 1979.  Shortest period shown is amplification over three months. Effective weighted  altitudes (from the RSS weighting curves) are about 450 hPa for the lower  altitude TLT (~ 6.5 km) and 350 hPa (~ 8 km) for the higher altitude  TMT.

Tropical Observations  1979-2008
 Figure 1(a). UAH and RSS  satellite data. This was the first analysis done.  It confirmed the sensitivity of this temporal evolution method, as it shows a  clear difference between the two versions (RSS and MSU) of the MSU satellite  data. Both of the datasets (UAH and RSS) are quite  close in the first half of the record. They diverge in the second  half.

The higher and lower altitude amplifications are very similar in both  the RSS and the UAH versions. The oddity in Fig 1(a)  is that I had expected the amplification at higher altitude (T2 and TMT) to be larger than at the  lower altitude (T2LT and TLT) amplification. Instead, the higher altitude record  had lower amplification. This suggests a strong stratospheric influence on the  T2 and TMT datasets. Because of this, I have used only the lower altitude T2LT  (UAH) and TLT (RSS) datasets in the remainder of this analysis.
Figure  1(b). HadAT2 radiosonde data. (Note the difference in vertical scale.)  Despite the widely discussed data problems with the radiosonde data, this shows  a clear picture of amplification increasing with altitude to 200 hPa, and  decreasing above that. It confirms the existence of the tropical tropospheric  “hot spot”, where the amplification is large. It conforms with the result  expected from theoretical consideration of the effect of lapse rate. It also  shows remarkable internal consistency. The amplification increases steadily with  altitude up to 200 hPa, and decreases steadily with altitude above that. Note  that the 1998 El Nino is visible as a “bump” in the records at about month 240.  It is also visible in the satellite record, in Fig. 1(a).
Figure 1(c).  HadAT2, overlaid with MSU satellite data. Same vertical scale as (a). The  satellite and balloon data all agree in the first half of the record. In the  latter half, the fit is much better for the UAH satellite data analysis than the  corresponding RSS analysis. Note that the amplification of the UAH version is a  good fit with the 500 hPa level of the HadAT2 data. This agrees with the  theoretical effective weighted altitude of the T2LT measurement.

Global Observations 1979-2008

Figure 1(d). Global UAH and RSS satellite data. Note  difference in vertical scale. There is little  amplification at the global level.
Again, the UAH  and RSS records are similar in the short term but not the long  term.
Figure 1(e). Global HadAT2 radiosonde data. Note difference  in vertical scale. Here we see that the amplification is clearly a tropical  phenomenon. We do not see significant amplification at any  level.
Figure 1(f). Global HadAT2, overlaid with MSU satellite data. Same vertical scale as (d). Once again, the satellite and balloon data all  agree in the first half of the record. In the latter half, again the UAH  analysis generally agrees with the observations. And again, the RSS version is a  clear outlier.
Both in the tropics and globally, amplification of the  levels above 850 hPa start low. After that they show a slow increase. The  greatest amplification occurs at 8-10 years. After that, they all (except RSS)  show a gradual decrease up to the 30 years end of the record.

Having seen the agreement between the UAH T2LT and the HadAT2  datasets, I next compared the tropical RATPAC data with the HadAT2 data. The  RATPAC data is annual and quarterly. I averaged the HadAT2 data annually and  quarterly  to match. They are shown in Fig. 2. Note that these are fifty-year  datasets, a much longer timespan than Fig. 1.

Figure 2. RATPAC and HadAT2 Tropical Amplification, 3 years to  50 years. Left column is annual data. Right column is quarterly  data.

There is very close agreement between the HadAT2 and the RATPAC  datasets. The annual version shows a number of levels of pressure altitude. The  quarterly version averages the troposphere down into two levels, one from  850-300 hPh, and one from 300-100 hPa. Both annual and quarterly RATPAC versions  agree well with HadAT2.
Before going further, let me draw some  conclusions about tropical amplification from Figs. 1 & 2.
1. Three  of the four observational datasets (HadAT2, RATPAC, and UAH MSU) are in  surprisingly close agreement. The fourth, the RSS MSU dataset, is a clear  outlier. The very close correspondence between HadAT2 and RATPAC at all levels  gives increased confidence that the observations are dependable. This is  reinforced by the good agreement in Figs. 1(c) and 1(f) between the UAH MSU and  the HadAT2 500 hPa level amplifications, both in the tropics and  globally.
2. Figure 1(c) clearly shows the theoretically predicted  “tropical hot spot”. It is most pronounced at 200 hPa at about 8-10 years. At  its peak the 200 hPa level has an amplification of about 2. However, this  gradually decays over time to a long-term (fifty year) amplification of about  1.5.
3. The lowest level, 850 hPa, has a short-term amplification of just  under 1. It gradually increases over time to an amplification of about 1. It  varies very little with the length of observations. RATPAC and HadAT2 are in  excellent agreement regarding the amplification at the 850 hPa level.
4.  The amplification of the next two levels (700 and 500 hPa) are quite similar.  The higher level (500 hPa) has slightly greater amplification than the lower.  Again, both datasets (RATPAC and HadAT2) agree very closely. This is supported  by the UAH MSU satellite data, which agrees with the 500 hPa level of both the  other datasets.
5. The amplification of the 300 and 200 hPa levels are  also quite similar. The amplification of the higher level (200 hPa) exceeds that  of the next lower level in the early part of the record. However, the 200 hPa  amplification decreases over time more than that of the lower level (300 hPa).  This accelerated long-term decay in amplification is also seen in the 150 and  100 hPa levels.
6. Between 700 and 200 hPa, amplification rises to a peak  at around 8-10 years, and declines after that.
Observations and  Models

Because it is the most detailed of the observational records,  I will take the HadAT2 as a comparison standard. Fig. 3(a) shows the full length  (50 year) HadAT2 record. Figures 3(b) to (e) show the outputs from five climate  models. These models were selected at random. They are simply the first five  models I found to investigate.
In Fig. 3(a) the decline in the observed  amplification at the 200 hPa level seen in the shorter 30 year record in Fig.  1(c) continues unabated to the end of the 50 year record. The 200 hPa  amplification crosses over the 300 hPa level and keeps declining. The models in  Fig. 3(b-f), however, show something completely different.


 Figure 3. Three month to fifty year  amplification for HadAT2 and for the output of various computer  models.
The model results shown in Figs  3(b) to (e) were quite unexpected. It was not a surprise that the models  disagreed with observations. It was a surprise that the model results varied so  widely among themselves. The atmospheric amplification at the various pressure  levels are very different in each of the models
In the observations, the  greatest amplification is at 200 hPa at around eight to ten years. Only one of  the models, the GISSE-R, Fig. 3(d), reproduces that slow buildup of  amplification. Unfortunately, the model amplification continues to increase from  there on to the end of the record, the opposite of the observations.
The  observed amplification at all levels except 850 hPa peaks at 8-10 years and then  decreases over time. This again is opposite to the models. Amplification in all  levels above 850 hPa of all of the models either stay level, or they increase  over time.
In the observations, amplification increases steadily with  altitude from 850 hPa to 200 hPa. Not a single model showed that result. All of  the models examined showed one or more inversions, instead of the expected  steady increase in amplification with altitude shown in the  observations.
The 850 hPa observations start slightly below 1, and  gradually increase to 1. Only one model, BCCR (c), correctly reproduced this  lowest level.
Variability of Observations and  Models

To investigate the natural variability in  the amplification of both observations and models, I looked at thirty-year  subsets of the various 50-year datasets. Figure 4 shows the amplification of  thirty-year subsets of the datasets and model output. This shows how much  variability there is in thirty year records. I show subsets taken at 32 month  intervals, with the earliest ones at the back of the stack.
Once again,  there are some conclusions that can be drawn from first looking at the  observations, which are shown in Fig 4 (a).
1. The relationship between  the various layers is maintained in all of the subsets. The lowest level (850  hPa) is always at the bottom. It is invariably below (smaller amplification  than) the rest of the levels up to 200 hPa. The 700/500 hPa pair are always very  close, with the higher almost always having the greater amplification. The 200  hPa level is above the 300 hPa level for all of the early part of the record.  This order, with amplification steadily increasing with altitude up to 200 hPa,  holds true for every one of the thirty-year subsets of the  observations.
2. The 700 hPa amplification is never less than the 850 hPa  amplification. As one goes lower, so does the other. They cross only at the  short end of the time scale.
3. At all but the 850 hPa level,  amplification peaks at somewhere around 8-10 years, and subsequently generally  declines from that peak.
4. The amplification at 200 hPa is much larger  than at 300 hPa at short (decade) timescales, but decreases faster than the 300  hPa amplification.
5. There is a surprising amount of variation in these  thirty-year overlapping subsets. This implies that the satellite record is still  too short to provide more than a snapshot of the variation in  amplification.

Figure 4. Evolution of amplification in thirty year subsets of  fifty-year datasets. The interval between subsets is 32 months. Fig. 4(a) is  observations (HadAT2). The rest are model hindcasts.
The most obvious  difference between the models and the observations is that most of the models  have much less variability than the observations.
The next apparent  difference is that the amplification in the models trend level or upwards with  time, while the observed amplifications generally trend downwards.
Next,  the pairings of levels are different. The only model which has the same pairings  as the observations (700/500 and 300/200 hPa) is the HadCM3 model. And even in  that model, both pairs are reversed from the observations. The other models have  700 hPa paired with (and often below) the 850 hPa level.
There is a  final oddity in the model results. The short term (say four year) amplification  at 200 hPa is very different in the various models. But at thirty years the  models converge to a range of around 1.5 to 1.9 or so. This has led to a  mistaken idea that the models reveal a “robust” long term amplification.

DISCUSSION
Having  examined the changes in amplification over time for both observations and  models, let us return to re-examine, one by one, the issues involved as stated  at the beginning:
The month-to-month variability of tropical  temperatures
is larger in the troposphere than at the Earth’s  surface.
 
This is clearly  true. There is an obvious tropical “hot spot” of high amplification in the upper  troposphere. It peaks at a pressure altitude of 200 hPa at about 8-10  years.


This amplification behavior is similar in a  range of
observations and climate model simulations, and is
consistent  with basic theory.


This  amplification behavior is in fact similar in a range of observations. However,  it is very dissimilar in a range of climate model simulations. While the  observations are consistent with basic theory (amplification increasing with  altitude up to 200 hPa), the climate models are inconsistent with that basic  theory (higher levels often have lower amplitude than lower  levels).
On multi-decadal timescales,
tropospheric  amplification of surface warming is a robust
feature of model simulations,  but occurs in only one
observational dataset [the RSS dataset].


There are no “robust” features of  amplification in the model simulations. They have very little in common. They  are all very different from each other.
Multi-decadal amplification  decreases gradually over the 50-year observational record. Three of the  observation datasets (UAH, RATPAC, and HadAT2) all agree with each other in this  regard. The RSS dataset is the outlier among the observations, staying level  over time. This RSS behavior is similar to several of the models, which also  stay level over time. One possible explanation of the RSS difference is that I  understand it uses computer climate modeling to inform a portion of its analysis  of the underlying MSU data.
Other observations show weak or
even  negative amplification.


None of the  tropical observational datasets above 850 hPa show “negative amplification”  (amplification less than 1) at timescales longer than a few years. On the other  hand, all of the observational datasets show negative amplification at short  timescales, as do all of the models.
These results suggest that
either  different physical mechanisms control
amplification processes on monthly and  decadal
timescales, and models fail to capture such behavior, or
(more  plausibly) that residual errors in several
observational datasets used here  affect their
representation of long-term  trends.


The problem is not that the observations  fail to capture the long term trends. It is that every model disagrees with  every other model, as well as disagreeing with the observations.
It  appears that different physical mechanisms do indeed control the amplification  at different timescales. The models match the observations in part of this, in  that amplification starts out low and then rises to a peak over a period of  years. In most of the models examined to date, however, this happens on a much  shorter time scale (months to a few years) compared with observations (8-10  years).
However, the models seem to be missing a longer-term mechanism  which leads to long-term decrease in amplification. I suspect that the problem  is related to the steady racheting up of the model temperature by CO2, which  increases the longer term amplification and leads to upward trends. Whatever the  cause may be, however, that behaviour is not seen in the observations, which  decrease over time.

Conclusions

1. Temporal evolution of  amplification appears to be a sensitive metric of the state of the atmosphere.  It shows similar variations in the various balloon and satellite datasets with  the exception of the RSS MSU satellite temperature dataset.
2. The wide  difference between the individual models was unexpected. It was also surprising  that none of them show steadily increasing amplification with altitude up to 200  hPa, as basic theory suggests and observations confirm Instead, levels are often  flipped, with higher levels (below 200 hPa) having less amplification than lower  levels.
3. From this, it appears that we have posed the wrong question  regarding the comparison of models and data. The question is not why the  observations do not show amplification at long timescales. The real question is  why model amplification is different, both from observations and from other  models, at almost all timescales.
4. Even in scientific disciplines which  are well understood, taking the position when models disagree with observations  that “more plausibly” the observations are incorrect is adventurous. In climate  science, on the other hand, it is downright risky. We do not know enough about  either the climate or the models to make that claim.
5. Observationally,  amplification varies even at “climate length” time scales. The thirty year  subsets of the observations showed large changes over time. Climate is ponderous  and never at rest. Clearly there are amplification cycles and/or variations at  play with long timescales.
6. While at first blush this analysis only  applies to temperatures in the tropical troposphere, it would not be surprising  to find this same kind of amplification behavior (different amplification at  different timescales) in other natural phenomena. The concept of amplification,  for example, is used in “adjusting” temperature records based on nearby  stations. However, if the relationship (amplification) between the stations  varies based on the time span of the observations, this method could likely be  improved upon.

Additional Information
 The Supplementary  Online Material contains an analysis of amplification in the AMSU satellite  dataset and the NCEP Reanalysis dataset. It also contains the data, the data  sources, notes on the mathematical methods used, and the R function and R  program used to do the analyses and to create the graphics used in this  paper.

References

Douglass DH, Christy JR, Pearson BD,  Singer SF. 2007. A comparison of tropical temperature trends with model  predictions. International Journal of Climatology 27:  Doi:10.1002/joc.1651.
Santer BD, et al. 2005. Amplification of surface  temperature trends and variability in the tropical atmosphere. Science 309:  1551–1556.
Santer BD et al. 2008. Consistency of modelled and observed  temperature trends in the tropical troposphere. Int. J. Climatol. 28:  1703–1722
SUPPLEMENTARY ONLINE MATERIAL
 
SOM Section 1. Further  investigations.


AMSU  dataset
 A separate dataset from a single AMSU (Advanced Microwave  Sounding Unit) on the NOAA-15 satellite is maintained at  <http://discover.itsc.uah.edu/amsutemps/>. Although the dataset is short,  it has the advantage of not having any splices in the record. The amplification  of that dataset is shown in Fig. S2.

Figure S-1  Short-term Global Amplification of AMSU satellite data, MSU data, and HadAT2  data . The dataset is only ten years long.
Figure S-1(a). AMSU satellite data. This is a curious  dataset. The 400 hPa level is a clear and dubious outlier. The distinctive  “duck’s head facing right” shape of the second half of the 900 and 600 hPa  levels is similar, while that of the 400 hPa level is quite different. It is  very doubtful that one level would be that different from the levels above and  below it. This was a strong indication of some unknown error with the 400 hPa  data that is affecting the longer term amplification.

Figure S-1(b). Adjusted and unadjusted AMSU satellite data. To attempt to correct this error, I added a simple linear trend to the 400  hPa level. I did not adjust it until the amplification was right. Instead, I  adjusted it until its long-term trend ended up proportionally between the  long-term trends of the layers above and below. This gave the adjusted version  (light green) of the amplification.

Figure  S-1(c). Adjusted AMSU satellite data. After the adjustment of the 400 hPa  trend, the amplifications fit together well. Curiously, despite being adjusted  by a linear trend, the shape of the latter half of the 400 hPa level has  changed. Now it has the “duck’s head facing right” shape of the 600 and 900 hPa  levels. This unexpected change supports the idea that there is indeed an error  in the trend of the 400 hPa data.

Figure  S-1(d). Interpolated AMSU satellite data. Unfortunately, the referenced  levels in the two datasets are at different pressure altitudes than HadAT2. To  compare the AMSU to HadAT2, we need to interpolate. Fortunately, the HadAT2  dataset range (850 to 200) fits neatly inside the AMSU range (900 to 150). This,  along with the similar and close nature of the signals at various levels, allows  for linear interpolation to give the equivalent AMSU amplification at the HadAT2  levels. The interpolated version is shown.
Figure S-1(e). HadAT2 and  RSS/UAH satellite data. The global observational data over this short period  (ten years) is scattered. Also, the HadAT2 data has a more jagged and variable  shape. We may be seeing the effects of the paucity of the observations. In the  short-term (this is ten years and less) the RSS and UAH amplification records  are quite similar. As before, both are close to the 500 hPa HadAT2  amplification.
Figure S-1(f). AMSU and HadAT2 data. Close, but not  a good match. The 200, 700, and 850 hPa levels match, but 300 and 500 hPa are  quite different. Overall, the satellite data seems more reliable. The AMSU data  shows a gradual change in amplification with altitude. The HadAT2 data is  bunched and sometimes inverted.
My conclusions from the AMSU dataset  are:
1. It contains an error, which appears to be a linear trend error,  in the 400 hPa level.
2. Other than that, it is the most internally  coherent of the observational datasets.
3. It points up the weakness of  using short (one decade) subsets of the HadAT2  dataset.
NCEP REANALYSIS

One of the attempts to  provide a spatially and temporally complete global dataset despite having  limited observational data is the NCEP reanalysis dataset. Figure S-2 compares  the temporal evolution of amplification of HadAT2 and NCEP Reanalysis  output

Figure S-2 Evolution of amplification of HadAt2 and NCAR. Left column  is amplification from 3 months to 50 years, right column is amplification of 30  year subsets of the 50 year datasets. The interval between the individual  realizations in the right column is 32 months.
The NCEP reanalysis  data in Fig. S-2 (b) shows a fascinating pattern. The three middle levels  (700,500, and 300 hPa) are close to the HadAT2 observations. The 300 hPa levels  agree extremely well. And while the 700 and 500 hPa levels are flipped in NCEP,  still they are in the right location and are very close to the observed  values.
But at the same time, the amplification of the lowest and highest  levels are way off the rails. The 850 hPa amplification starts at 1, and just  keeps rising. And the 200 hPa amplification starts out reasonably, but then  takes a big jump with a peak around thirty years. That seems  doubtful.
The observation that there are problems at the lowest and  highest levels is reinforced by the analysis of the variation of thirty year  subsets in the right column of Fig. S-2. These show the 200 hPa amplification  varying wildly over all of the different 30 year datasets. In one of the thirty  year subsets the 200 hPa amplification dips down to almost touch the highest 850  hPa line. There is clearly something wrong with the NCEP output at the 200 hPa  level.
In addition, in the full NCEP record shown in Fig. S2(b) and all  of the 30 year subsets shown in Fig. S2(d), the lowest level (850 hPa) increases  steadily over time. After about 20 years it has more amplification than either  of the 700 and 500 hPa levels. This behavior is not seen in either the  observations or any of the models.

Conclusions  from the NCEP reanalysis

1. The 700, 500, and 300 hPa level of the  NCEP reanalysis are accurate. The 850 and 200 hPa levels suffer from large  problems of unknown origin.
2. Use of the NCEP reanalysis in other work  seems inadvisable until the 850 and 200 hPa amplification problems are  resolved.

SOM Section 2. Data Sources

KNMI was the source for much of  the data. It has a wide range of monthly data and model results that you can  subset in various ways. Start at http://climexpknmi.nl/selectfield_co2.cgi?someone@somewhere . It contains both Hadley and UAH data, as well as a few model atmospheric  results. My thanks to Geert for his excellent site.
 
Surface data for all  observational datasets is from the CruTEM dataset at http://climexp.knmi.nl/data/icrutem3_hadsst2_0-360E_-20-20N_n.dat
UAH  data is at http://www.nsstc.uah.edu/data/msu/t2lt/uahncdc.lt
RSS  data is at http://www.remss.com/data/msu/monthly_time_series/
HadAT2  balloon data is at http://hadobs.metoffice.com/hadat/hadat2/hadat2_monthly_tropical.txt
CGCM3.1  model atmospheric data is at http://sparc.seos.uvic.ca/data/cgcm3/cgcm3.shtml in the form of a large (250Mb) NC file.
Data for all other models is from  the “ta” and “tas” datasets from the WCRP CMIP3 multi-model database at  <https://esg.llnl.gov:8443/home/publicHomePage.do>
In particular,  the datafiles used were:
GISSE-R: ta_A1.GISS1.20C3M.run1.nc, and  tas_A1.GISS1.20C3M.run1.nc
HadCME:  ta_A1_1950_Jan_to_1999_Dec.HadCM3.20c3m.run1.nc, and  tas_A1.HadCM3.20c3m.run1.nc
BCCR: ta_A1_2.bccr_bcm2.0.nc, and  tas_A1_2.bccr_bcm2.0.nc
INCM3: ta_A1.inmcm3.nc, and  tas_A1.inmcm3.nc

As all of  these are very large (1/4 to 1/2 a gigabyte) files, I have not included them in  the online data. Instead, I have extracted the data of interest and saved this  much smaller file with the rest of the online data.

SOM Section 3. Notes on the Function and Code.

The main function  that does the calculations and created the graphics is called “amp”.
 
The  input variables to the function, along with their default values are as follows:
datablock=NA : the input data for the function. The function requires  the data to be in matrix form. By default the date is in the first column, the  surface data in the second column, and the atmospheric data in any remaining  columns. If the data is arranged in this way, no other variables are required  The function can be called as amp(somedata), as all other variables have  defaults.
sourcecols=2 : if the surface data is in some column other  than #2, specify the column here
datacols=c(3:ncol(datablock)) : this is  the default position for the atmospheric data, from column three  onwards.
startrow=1 : if you wish to use some start row other than 1,  specify it here.
endrow=nrow(datablock) : if you wish to use some end row  other than the last datablock row, specify it here.
newplot=TRUE :  boolean, “TRUE” indicates that the data will be plotted on a new blank  chart
colors=NA : by default, the function gives a rainbow of colors.  Specify other colors here as necessary.
plotb=-2 : the value at the  bottom of the plot
plott=2 : the value at the top of the  plot
periods_per_year=12 : twelve for monthly data, four for quarterly  data, one for annual data
plottitle=”Temporal Evolution of Amplification”  : the value of the plot title
plotsub=”Various Data” : the value of the  plot subtitle
plotxlab=”Time Interval (months)” : label for the x  values
plotylab=”Amplification” : label for the y  values
linewidth=1 : width of the plot lines
linetype=”solid” :  type of plot lines
drawlegend=TRUE : boolean, whether to draw a legend  for the plot
SOM Section 4. Notes on the Method.

An example  will serve to demonstrate the method used in the “app” function. The function  calculates the amplification column by column. Suppose we want to calculation  the amplification for the following dataset, where “x” is surface temperature,  “y” is say T200, and each row is one month:
x   y
1   4
4   7
3    9
Taking the “x” value, I create the following 3X3 square matrix, with  each succeeding column offset vertically by one row. This probably has some kind  of special matrix name I don’t know, and an easy way to calculate it. I do it by  brute force in the function:
1     4     3
4     3     NA
3    NA     NA
I do the same for the “y” value:
4     7     9
7     9      NA
9    NA    NA
I also create same kind of 3X3 matrix for x times  y, and for x squared.
Then I take the cumulative sums of the columns of  the four matrices. These are used in the standard least squares trend formula to  give a fifth square matrix:
slope of regression line = (N*sum(xy) –  sum(x)*sum(y)) / (N*sum(x^2) – sum(x)^2)


I then  average the rows to give me the average amplification at each  timescale.
This method exhaustively samples to find all contiguous  sub-samples of each given length. This means that there will be extensive  overlap (samples will not be independent). However, despite the lack of  independence, using all available samples improves the accuracy of the method.  This can be appreciated by considering a fifty year dataset. There are a number  of thirty year contiguous subsets of a fifty year dataset, but if you restrict  your analysis to non-overlapping subsets, you only can pick one of them …  which way will give the best estimate of the true 30-year  amplification?
SOM Section 5. Of Averages and Medians.

The  distribution of the short-term amplifications is far from normal. In fact, it is  not particularly normal at any scale. This is because the amplification is  calculated as the slope of a line, and any slope is the result of a division.  When the divisor approaches zero, very large numbers can result. This makes  averages (means) inaccurate, particularly at the shorter time scales.
One  alternative is the median. The problem with the median is that it is not a  continuous function. This limits its accuracy, particularly in small samples. It  also makes for a very ugly stair-step kind of graph.
Frustrated by this,  I devised a continuous Gaussian mean function which outperforms the mean for  some varieties of datasets, and outperforms the median on other datasets. It is  usually in between the mean and median in value. In all datasets I tested, it  equals or outperforms either the mean or the median.
To create this  Gaussian mean I reasoned as follows. Suppose I have three numbers picked at  random from some unknown stable distribution, let’s say they are 1,2, and 17.  What is my best guess as to the actual underlying true center of the  distribution?
Since we don’t know the true distribution, the best guess  as to its shape has to be a normal distribution. With such a distribution, if we  know the standard deviation, we can iteratively calculate the mean.
To  do so, we start by calculating the standard deviation, and picking a number for  the estimated mean, say 5. If that is the mean, the numbers (1,2,17) when  measured in standard deviation units is (-0.6, -0.5,  1.2). Each of those  standard deviations has an associated probability. I figured that the sum of  these individual probabilities is proportional to the probability that the mean  actually is 5.
I then iteratively adjust the estimated mean to maximize  the total probability (the sum of the three individual probabilities. It turns  out that the gaussian mean of (1, 2, 17) calculated by my method is 3.8. This  compares with an average for the three numbers of 6.7, and a median of  2.
In general there is very little difference between my gaussian mean,  the arithmetic mean, and the median. However, it is much better behaved than the  mean in non-normal datasets. And unlike the median, it is a continuous function,  which gives greater accuracy.
All three options are included in the  amp() function, with two of them remarked out, so you can see the effects of  each one. The only noticeable difference is that the mean is not very accurate  at short time scales, and the gaussian mean and the mean are not discontinuous  like the median.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94dcb0fd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A clear and growing majority of Coalition voters support the Morrison government adopting a net zero target for 2050, with support for that proposition climbing 12 points in a month, according to the latest Guardian Essential poll. The latest fortnightly survey shows a majority of Australian voters support net zero either strongly or somewhat (75%, up four points in a month), and 68% of Coalition voters in the sample hold that positive view. Last month, the proportion of supportive Coalition voters was 56%. The decisive shift in positive sentiment from Coalition voters follows calls from within the government to consider the 2050 target, and Labor’s decision late last week to sign on to net zero – confirmation that has reignited the climate wars in Canberra. While the Morrison government is leaving its options open on a 2050 target, its current messaging is suggestive of substituting a technology roadmap for a target. The debate about net zero dominated parliament on Monday. The Coalition is blasting Labor for adopting the target in the absence of a fleshed-out plan to get there, while Labor and the Sydney independent MP Zali Steggall are pressuring the government to detail the impact of failing to act to prevent dangerous global heating. More than 70 countries and 398 cities say they have adopted a net zero position. Every Australian state has signed up to net zero emissions by 2050, and these commitments are expressed either as targets or aspirational goals. Net zero is also supported by large companies and by their lobby group, the Business Council of Australia. Given the resumption of partisan brawling about climate policy and tensions within both of the major parties about the future of coal, voters in the Guardian Essential survey were asked a number of questions about their attitudes to a transition to low emissions. A majority of voters in the survey (75%) believe improvements in renewable energy means it will become less necessary to burn coal for electricity, and 65% say both advances in technology and global agreements on emissions reduction will result in coal becoming uneconomical to extract in the future. A majority (64%) says if Australia is serious about climate action, we will need to get out of coal as soon as possible. All of those propositions attract majority support amongst Coalition voters (70%, 60%, 54%). But a majority in the sample (61%, and 72% of Coalition voters) also say Australia should continue to export coal for steel production, even if we stop exporting coal for use in power plants. The survey also points to divided sentiment between the city and regional areas. For example, city dwellers are more likely to agree with a statement that if Australia is serious about the climate emergency we need to exit coal (67%) than people in the regions (56%). Voters were also asked about the future of coal-fired power plants, given the Morrison government has allocated $4m for a feasibility study examining a new facility at Collinsville. Also in prospect is taxpayer underwriting of coal – a development championed by Nationals but criticised by some moderate Liberals. Just under half the sample (47%) say coal plants should continue to operate as long as they are profitable, but the industry should not be subsidised or expanded. Coalition voters are more comfortable than other voting cohorts with subsidies. Greens voters are the most supportive of moves to shut down the coal industry (62%) with that position endorsed by 36% of Labor voters and 21% of Coalition voters in the sample. As well as the climate questions, voters were asked about the coronavirus, with experts saying the world is fast approaching a tipping point in the spread of the illness. Just under one-third of survey respondents have changed their behaviour in some way to try to avoid contracting the virus – either avoiding restaurants and shopping centres, or cancelling an overseas trip. While 70% of people in the survey say their personal behaviour has been business as usual, four out of five respondents agree that because of global movements in people, humanity is more vulnerable to the spread of viruses (81%). There is also strong support (80%) for the travel ban that prevents Chinese visitors entering Australia, and only 20% say the border with China should remain open to protect revenue from tourism and overseas students. Both the Morrison government and the media get the thumbs up from the sample for managing the risks and reporting the latest developments. There has been controversy post-election about the reliability of opinion polling, as none of the major surveys – Newspoll, Ipsos, Galaxy or Essential – correctly predicted a Coalition victory last May. The polls instead projected Labor in front on a two-party-preferred vote of 51-49 and 52-48. The lack of precision in the polling has prompted public reflection at Essential, as has been flagged by its executive director, Peter Lewis. Guardian Australia is not now publishing measurements of primary votes or a two-party-preferred calculation, but is continuing to publish survey results of responses to questions about the leaders and a range of policy issues. The poll’s margin of error is plus or minus 3%. The sample size this fortnight is 1,090 respondents."
"With veganism on the rise and entire supermarket aisles now dedicated to veggie and vegan food ranges, it’s a good time to consider what motivates people to go vegan.  There are many reasons why people decide to cut animal products from their diet, but the negative health effects of excessive meat and dairy consumption and the enormous environmental impacts of industrial agriculture are popular ones. However, the suffering of billions of animals each year in factory farming, referred to in a 2015 Guardian article as one of the “worst crimes in history”, is the most powerful motivation for many, including myself.  Refraining from something that causes so much harm and suffering is laudable, but there’s one argument occasionally used in vegan and animal rights campaigns that warrants closer attention – the idea that consuming other creatures is morally wrong in its own right.  Such views are often bolstered by powerful moral arguments framing animals as subjects of a life, able to experience pain, and as leaders of complex emotional lives. Opposing meat eating on ontological grounds – meaning, simply because animals are sentient beings, we shouldn’t eat them – separates humans from nature and prevents truly ethical relationships between humans, animals and the natural world. The late environmental philosopher Val Plumwood coined “ontological veganism” to describe this absolute opposition. Ontological veganism asserts that beings that count as ethical subjects should not be eaten, in the same way that there’s a widespread taboo about eating humans. While this thinking erects another unhelpful boundary between animals and other life forms, it’s also ironic that the rationale underlying taboos against eating humans is the desire to radically separate humans from other animals. By framing the consumption of other living beings as an inherent moral wrong, ontological veganism also risks demonising predation. In order to avoid this, a common approach is to “excuse” animal predation by arguing that the latter is part of “nature” while humans, as cultural beings, should be exempt. Some of us – especially those living in wealthy countries – can indeed choose to opt for vegan products, but this argument reproduces another false dichotomy: nature vs. culture. Life is entanglement, with no clear boundaries between “humans” and other species, or between “nature” and “society”.  Come among the deer on the hill, the fish in the river, the quail in the meadows. You can take them, you can eat them, like you they are food. They are with you, not for you. This quote is from the late utopian author Ursula Le Guin, in her novel Always Coming Home. Her idea is akin to Plumwood’s theory of ecological animalism, which seeks to replace human supremacy over nature with mutual and respectful use between humans and other species. Ontological veganism would frame using or consuming animals itself as inherently exploitative. But consider forms of mutual use seen in symbiotic relationships, such as those between pollinating insects and plants. In such scenarios, use isn’t oppressive or exploitative. It’s the form of use seen within industrial capitalism, where humans and non-humans alike are treated only as a means to an end, that prevents ethical relationships. Ecosystems and all living beings depend upon mutual use and consumption. Orcas consume fish and other marine mammals, we must consume living vegetable matter at least, and when we die, we become food for a host of microorganisms, nourishing them in turn.  If humans are indeed animals who differ from other species only by degrees rather than kind, then like them, we are food. To deny this is to deny that humans are embedded within the ecosystems they originate from and are sustained by. The horrific cruelty involved in industrial factory farming reduces living beings to mere profitable commodities. This is why I am a vegan, and it is here where calls for eradicating or at least reforming animal agriculture find firmer ground. The ways in which animals are currently treated in agriculture represent the exact opposite of respect and mutuality. No wonder Aldous Huxley observed in his poignant ecotopian work, Island, that For animals… Satan, quite obviously, is Homo sapiens. Ecological animalism offers a powerful basis for truly ethical and egalitarian ways of relating to other species. We are all food, and crucially, so much more. We are with and not for one another, and we are all worthy of respect. Go vegan whenever and wherever possible, but be mindful of the underlying rationales involved, lest we reproduce the same harmful dualisms we want to dismantle. More on evidence-based articles about veganism and diets: Vegan diet: how your body changes from day one Why aren’t more people vegetarian? Five ways to encourage people to reduce their meat intake – without them even realising"
"As temperatures at Kew Gardens soared past 21℃, February 26 2019 became the UK’s warmest winter day on record. That same day, a number of wildfires broke out in several different parts of the UK and Ireland – there was a substantial blaze on Marsden Moor, Yorkshire, gorse fires on Arthur’s Seat overlooking Edinburgh, and in the Dublin Mountains, and two separate fires in Ashdown Forest, East Sussex. Given the unusual weather, and the unusual winter fires, an obvious question is: has human-caused climate change played a role? I research the impact of climate change and look at questions exactly like this, so I’m well aware these things are complicated and need proper study – it’s not possible to give a simple yes or no answer immediately. But we can make a first assessment based on general understanding. A first thing to clarify is exactly what we are talking about – increased risk of fire due to the environmental conditions, or the actual occurrence of fires. The fires themselves were almost certainly started by people in some way or other, either deliberately or, more likely, accidentally (lightning fires are rare and there were no storms). But the fact that so many fires took hold and spread at the same time is a clear indication that the environmental conditions were conducive to fire, and that’s what I’ll focus on here. By “environmental conditions”, I mean both the weather at the time of the fire, and the state of the dead leaves, twigs and branches, and in some locations, peat, that  fuel the fire. The amount of fuel and its dryness is crucial, and depends not just on the weather in the preceding days, but also the weeks and months before.  Land management methods including peatland drainage can also play a role. The warm conditions in the days up to Feb 26 were certainly very unusual for this time of year. There was high pressure centred over central Europe and the British Isles, which not only brought settled, dry conditions beneath it but also featured winds moving clockwise around it. This meant the UK and Ireland, on the western edge of the high pressure, experienced warmer air from the south. Whether that kind of weather system is becoming more likely due to climate change is a difficult question to answer. However, at this point we can say that when such conditions occur, they are likely to be hotter than they would have been without climate change, as a result of the general warming trend. Research is already planned to establish how much of the hotter, drier conditions were due to climate change. A further question is whether the dry conditions preceding those few days were due to climate change. This includes both last summer and the winter in between.  Climate change is expected to mean hotter, drier summers in the British Isles, and it made the 2018 heatwave 30 times more likely. So if vegetation was made more susceptible to fire by last summer’s heatwave, this would be a link to climate change. Again, this also needs further research. However, while winters are becoming generally milder, scientists also expect more rainfall. So on the face of it, this year’s dry January was different to what we expect from climate change. However, in a complex, variable system such as the weather, it’s important not to just look at averages but also the ups and downs, and also look at combinations of different weather factors and not just individual factors like rain or temperature in isolation. Although we expect winters to become wetter on average, not every winter will be wetter – we’ll still get dry Januaries sometimes. And given the long-term warming trend, we can expect that when we do get winters with lower rainfall, the fuel will dry quicker due to increased evaporation. So while on average the UK and Ireland might expect reduced fire danger in winter, in years when winters are dry and mild, it could be higher. Of course all this needs working through properly, and no doubt will be. Colleagues and I did do a very preliminary study on this for the 1st UK Climate Change Risk Assessment years ago which suggested that annual average fire danger would increase with climate change. This was since backed up by a more detailed study which suggested that wetter conditions would generally reduce the risk of fire in winter, partly offsetting the increased risk in summer in the annual average. However, this is not inconsistent with my argument here – although overall we might expect climate change to reduce the chances of winter fires in most years, in years when winter rainfall bucks the trend and is drier than usual, we may actually see increased fire risk. So on the question of whether these specific fires are linked to human-caused climate change, I’d say “maybe – we need to look into it more”. But on a more general question of whether we should expect more fires in the British Isles as the world continues to heat up, the answer is clear: yes."
"
This is an official NCAR News Release (National Center for Atmospheric Research) Apparently, they have solar forecasting techniques down to a “science”, as boldly demonstrated in this press release. – Anthony
Scientists Issue Unprecedented Forecast of Next Sunspot Cycle
BOULDER—The next sunspot cycle will be 30-50% stronger than the last one and begin as much as a year late, according to a breakthrough forecast using a computer model of solar dynamics developed by scientists at the National Center for Atmospheric Research (NCAR). Predicting the Sun’s cycles accurately, years in advance, will help societies plan for active bouts of solar storms, which can slow satellite orbits, disrupt communications, and bring down power systems.
The scientists have confidence in the forecast because, in a series of test runs, the newly developed model simulated the strength of the past eight solar cycles with more than 98% accuracy. The forecasts are generated, in part, by tracking the subsurface movements of the sunspot remnants of the previous two solar cycles. The team is publishing its forecast in the current issue of Geophysical Research Letters.
“Our model has demonstrated the necessary skill to be used as a forecasting tool,” says NCAR scientist Mausumi Dikpati, the leader of the forecast team at NCAR’s High Altitude Observatory that also includes Peter Gilman and Giuliana de Toma.
Understanding the cycles
The Sun goes through approximately 11-year cycles, from peak storm activity to quiet and back again. Solar scientists have tracked them for some time without being able to predict their relative intensity or timing.




NCAR scientists Mausumi Dikpati (left), Peter Gilman, and Giuliana de Toma examine results from a new computer model of solar dynamics. (Photo by Carlye Calvin, UCAR)



Forecasting the cycle may help society anticipate solar storms, which can disrupt communications and power systems and affect the orbits of satellites. The storms are linked to twisted magnetic fields in the Sun that suddenly snap and release tremendous amounts of energy. They tend to occur near dark regions of concentrated magnetic fields, known as sunspots.
The NCAR team’s computer model, known as the Predictive Flux-transport Dynamo Model, draws on research by NCAR scientists indicating that the evolution of sunspots is caused by a current of plasma, or electrified gas, that circulates between the Sun’s equator and its poles over a period of 17 to 22 years. This current acts like a conveyor belt of sunspots.
The sunspot process begins with tightly concentrated magnetic field lines in the solar convection zone (the outermost layer of the Sun’s interior). The field lines rise to the surface at low latitudes and form bipolar sunspots, which are regions of concentrated magnetic fields. When these sunspots decay, they imprint the moving plasma with a type of magnetic signature. As the plasma nears the poles, it sinks about 200,000 kilometers (124,000 miles) back into the convection zone and starts returning toward the equator at a speed of about one meter (three feet) per second or slower. The increasingly concentrated fields become stretched and twisted by the internal rotation of the Sun as they near the equator, gradually becoming less stable than the surrounding plasma. This eventually causes coiled-up magnetic field lines to rise up, tear through the Sun’s surface, and create new sunspots.
The subsurface plasma flow used in the model has been verified with the relatively new technique of helioseismology, based on observations from both NSF– and NASA–supported instruments. This technique tracks sound waves reverberating inside the Sun to reveal details about the interior, much as a doctor might use an ultrasound to see inside a patient.




NCAR scientists have succeeded in simulating the intensity of the sunspot cycle by developing a new computer model of solar processes. This figure compares observations of the past 12 cycles (above) with model results that closely match the sunspot peaks (below). The intensity level is based on the amount of the Sun’s visible hemisphere with sunspot activity. The NCAR team predicts the next cycle will be 30-50% more intense than the current cycle. (Figure by Mausumi Dikpati, Peter Gilman, and Giuliana de Toma, NCAR.)



Predicting Cycles 24 and 25
The Predictive Flux-transport Dynamo Model is enabling NCAR scientists to predict that the next solar cycle, known as Cycle 24, will produce sunspots across an area slightly larger than 2.5% of the visible surface of the Sun. The scientists expect the cycle to begin in late 2007 or early 2008, which is about 6 to 12 months later than a cycle would normally start. Cycle 24 is likely to reach its peak about 2012.
By analyzing recent solar cycles, the scientists also hope to forecast sunspot activity two solar cycles, or 22 years, into the future. The NCAR team is planning in the next year to issue a forecast of Cycle 25, which will peak in the early 2020s.
“This is a significant breakthrough with important applications, especially for satellite-dependent sectors of society,” explains NCAR scientist Peter Gilman.
The NCAR team received funding from the National Science Foundation and NASA’s Living with a Star program.
IMPORTANT NOTE:
The date of this NCAR News Release is March 6, 2006
Source: http://www.ucar.edu/news/releases/2006/sunspot.shtml
(hat tip to WUWT reader Paul Bleicher)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95a3cab5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHere’s a scary clip produced by some green Marxists at the German Federal Office of the Environment, who seem to have taken it upon themselves to tell the rest of us how to live. If this represents the government’s view and the target it has in mind for its citizens, then it’s awfully spooky. (Clip is in German – main points are described below).

It’s back to central planning, housing project living, and being told exactly how to live – all the failed experiments of the past, all combined and packaged as a religion that promises salvation.
Remember how the communists promised paradise for workers? This is the same pipe dream. The problem with all of it is that it is not based on the fundamental laws of economics, and so it will definitely fail. Many of the diagnoses offered in the first half of the clip are not even true, and the remedies proposed thereafter contradict each other.
The makers of this clip are sure that the world’s problems are because we are all behaving badly, and that we have to be made to behave differently – dictated – in a way that suits their world view. The world is threatened, says the film, by population growth and our consumption, and demands a system of redistribution.
Here are some of the main points:
 1. We are all spoiled. We cannot, or simply do not want to, go without the things we use daily. We are using more than what nature is capable of giving. We are using much too much. We are living way beyond our means. 
2. We are addicted to a huge array of products that we believe make our lives easier. We are leading lives of profligacy, which requires gigantic amounts of resources like, water, raw material and energy. The planet will soon be depleted. 
The disgruntled among us have been saying this for at least 2 centuries. Yet, everyday we keep finding more than we need. Things are better today than ever.
3. The guilty parties are North America and Europe. The industrialised countries use more than their fair share. Each German uses 60 tons of material annually, Americans 130 tons. This can no lonager be tolerated. 
This is called prosperity. Obviously a concept that the clip’s makers have an aversion to. Note that most material is not consumed – but used. It doesn’t disappear.
4. By 2030, 20% of the most important raw materials will reach their production limits. 
5. All this consumption is leading to many things, particularly climate change and species extinsction. 
6. Expanding deserts, multiplication od disaeases, wars and resource shortages are threatening future generations.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The clip doesn’t say which materials. And actually deserts are shrinking. They tend to grow when the planet cools.
7. People in industrialised nations are only able to be so wasteful because people in other poor countries have to go without raw materials.
8. Our level of prosperity is not possible for everyone. 
That’s false. It’s people in developing countries that need to learn how to use raw materials. This comes with free markets and education. Any normal person with a 4th grade education or more knows that the planet gets destroyed in undeveloped countries, and protected in developed countries.  The solution is for the rest of the world to live like we do. The clip claims:
9. We have to change our thinking radically. Only through a drastic reduction in our consumption of resources through dematerialisation will we be able to sustainably secure our future. We have to consume less.
10. For example, many of the things that we seldom use can be borrowed among friends. Things like the electric drill, lawnmower. We have to redesign our tools so that they have longer lifetimes, or can be repaired, easily dismantled and to recycle.
This is so stupid – economic ignorance now in full display. Imagine if we designed a long life copy machine to last 20 years. It would be obselete in three years, and so it would wind up consuming too much energy and operating too slowly during the remaining 17 years. This film was made by technical and economic illiterates.  The clip then suggests we need to:
11. Use public laundermats, buy second hand clothes, ride bicycles and have fun at “sustanaible parties”. 
WTF? Yeah right, When I was young I worked years with the target of getting away from having to do that. This is back to Soviet living. And the clip says we ought to think about using kooky ideas like:
12. Wind-powered ships, wind-powered buildings.
13. Powering everything with renewables.
Get ready to pay a fortune for an unreliable and primitive source of energy. Indeed get ready to fork it all over. The film then has the temerity to say:
14. Today’s prices do not reflect the ecology. That has to change.
15. Taxes on resources have to go up!
Yes, they want to take everything away from us. Everybody has to get along with much less – except for the state, of course. For them it’s more! more! more! Now we know what all this is about – state power. Power and wealth to the state, and not the individual. This clip is an example of the propaganda, one that despises humanity, that we are getting in Germany for our tax euros.
Folks, you’ve got to start talking to your local and regional politicians and business leaders about this. The higher-ups are all drugged up with their “let’s-take-over-and-save-the-planet” fantasies. It has to start from the botrtom up, like the tea party candidates.  
It’s not the people that “need to drastically change their thinking”. It’s the intoxicated politicians that do.
 
 
Share this...FacebookTwitter "
"

While the social cost of carbon (SCC) is still being mulled over by the Office of Management and Budget, other federal agencies continue to push ahead with using the SCC to help justify their many regulations.   
  
  
The way this works is that for every ton of carbon dioxide (CO2) that any new regulation is supposed to keep from being emitted into the atmosphere, the proposing agency gets about $32 credit to use to offset the costs that the new regulation will generate. This way, new regulations seem less costly—an attractive quality when trying to gain acceptance.   
  
  
The idea is that the damage resulting from future climate changes will be decreased by $32 for every ton of carbon dioxide that is not emitted.   
  
  
There is so much wrong with the way the government arrives at this number that we have argued that the SCC should be tossed out and barred from use in all federal rulemaking. It is far better not to include any value for the SCC cost/​benefit analyses, than to include one which is knowingly improper, inaccurate and misleading.   
  
  
Further, that the federal regulations limiting carbon dioxide emission will have any detectable impact on future climate change is highly debatable. To see for yourself, try out our global warming calculator that lets you select the magnitude of future carbon dioxide emissions reductions as well as which countries participate in your plan. The best that the U.S. can do—even if it were to halt all CO2 emissions now and forever—is to knock off about 0.1°C from the total climate model‐​projected global temperature rise by the year 2100. In other words, U.S. actions are not very effective in limiting future climate change.   
  
  
Apparently, the feds, too, agree that their plethora of proposed regulations will have little impact on carbon dioxide emissions and future climate change. But that doesn’t stop them from issuing them.   
  
  
The passage below is from the proposed rulemaking from the Department of Energy to alter the Energy Conservation Standards for Commercial and Industrial Electric Motors (this is only one of many proposed regulations making this claim):   




The purpose of the SCC estimates presented here is to allow agencies to incorporate the monetized social benefits of reducing CO2 emissions into cost‐​benefit analyses of regulatory actions that have small, or “marginal,” impacts on cumulative global emissions.



In other words, DoE’s regulations won’t have any real impact on global CO2 emissions (and, in that manner, climate change), but nevertheless they’ll take a monetary credit for reduced damages that supposedly will result from the non‐​effective regulations.   
  
  
(I wonder if can try that on my taxes)   
  
  
It seems a bit, uh, cheeky, to take credit for something that you admit won’t happen.   
  
  
But that’s the logic of the federal government for you!
"
"
If you are just joining us, the story is this. After 10 years of data being withheld that would allow true scientific replication, and after dozens of requests for that data, Steve McIntyre of Climate Audit finally was given access to the data from Yamal Peninsula, Russia. He discovered that only 12 trees had been used out of a much larger dataset of tree ring data. When the larger data set was plotted, there is no “hockey stick” of temperature, in fact it goes in the opposite direction. Get your primer here.
Red = 12 hand picked Yamal trees Black = the rest of the Yamal dataset
Now there’s independent confirmation from a study presented at the American Geophysical Union Conference in 2008 that there is no “hockey stick of warming” at Yamal.
The presentation is” Cumulative effects of rapid climate and land-use changes on the
Yamal Peninsula, Russia by D.A. Walker, M.O. Leibman, B.C. Forbes, H.E. Epstein. (click link for PDF)
In the hallway poster for their AGU presentation, they have this graph, with the caption saying a “nearly flat temperature trend” for Yamal, especially for the late 20th century period where the “hockey stick” from those 12 trees emerges:


See the AGU poster here (warning, big 18 MB PDF file)
Here is how they summarize the graph above in the AGU presentation:

Sea ice: -25%
Summer surface temperature: +4%
Maximum NDVI: +3%
None of the trends are significant at p =0.05 because of high interannual variability.

NDVI is the vegetation index.
There’s also an interesting polar sea ice, temperature, and vegetation index trend map that is similar to what Lucy Skywalker recently plotted.
Click for larger image
I’m sure we’ll see an explosion from “Tamino” any minute now to refute this, oh wait, he’s gone on record as saying:
As for Steve McIntyre’s latest: I’m really not that interested. He just doesn’t have the credibility to merit attention. I have way better things to do.
OK then, one less angry, sciency, rant by an anonymous coward who won’t put his name to his own work to worry about. Talk about credibility. Sheesh.
Here is the conclusion Walker et al makes in their AGU presentation

Satellite data suggest that there has been only modest summer land-surface warming and
only slight greening changes across the Yamal during the past 24 years. (Trend is much
stronger in other parts of the Arctic, e.g. Beaufort Sea.)
Kara-Yamal: negative sea ice, positive summer warmth and positive NDVI are correlated
with positive phases of the North Atlantic Oscillation and Arctic Oscillation.

So it seems sea ice extent, the NAO, and the AO are the bigger factors for temperature in Yamal. It also appears that the Arctic is getting slightly more green.
If anyone has access links to the full paper, feel free to post it here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e929ad868',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Many of you have probably heard by now of  the UN. Report saying that “global warming is killing 300,000 people a year”. There’s a Times Online Story (h/t to Gary Boden) about it today that has some startling admissions. Here are some excerpts:
Climate change is already killing 300,000 people a year in a “silent crisis”  that is seriously affecting hundreds of millions more, an influential  humanitarian group warned today.
A report by the Global Humanitarian Forum, led by Kofi Annan, the former UN  Secretary-General, says that the effects of climate change are growing in  such a way that it will have a serious impact on 600 million people, almost  ten per cent of the world’s population, within 20 years. Almost all of these  will be in developing countries.
“Climate change is the greatest emerging humanitarian challenge of our time,  causing suffering to hundreds of millions of people worldwide,” Mr Annan  said.
“As this report shows, the first hit and worst affected are the world’s  poorest groups, and yet they have done least to cause the problem.”
//
  
    
The report claims that 90 per cent of the deaths are related to gradual  environmental degradation caused by a warming climate, which exacerbates  existing threats — mainly malnutrition, diarrhoea and malaria. The rest are  said to be the result of weather disasters.
     But here is the kicker (emphasis mine):
Mr Annan said the report could never be as rigorous as a scientific study, but  said: “We feel it is the most plausible account of the current impact of  climate change today.”
Translation: “close enough for government work” (click for definition)
Worse, the U.N. didn’t even do the report themselves. The farmed it out:
The research was carried out by Dalberg Global Advisers, a consultancy firm,  who collated all existing statistics on the human impacts of climate change.  The report acknowledges a “significant margin of error” in its estimates.
But it is good enough for the MSM to use to scare the crap out of everybody and guilt the gullible into “action”.
‘Bogus’, doesn’t even begin to describe this political ploy.
For a real report, using real data, reflecting the real world situation, please read these reports by WUWT contributor Indur Goklany:
Going Down: Death Rates Due to Extreme Weather Events
How the IPCC Portrayed a Net Positive Impact of Climate Change as a Negative
Wrong: World Health Organization claims that health goes down as carbon goes up
Dealing with climate change in the context of other, more urgent threats to human and environmental well-being


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95b6658f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"I recently found myself in the surreal world of the Consumer Electronics Show in Las Vegas discussing the next generation of pollution sensors that one day you might find inside your phone. The exhibits I saw suggested the next big thing in home technology could be anything from intelligent cat litters to internet-enabled teapots, with everything powered by mysterious machine learning and the unfathomable blockchain. But there was no escaping that air quality and air purification is now a seriously big thing in the consumer products world. Most major white goods manufacturers have a range of products. There are also plenty of start-ups offering new variants – including purifying robots that wander forlornly around your home and bizarre bio-inspired devices that blow air over the leaves of poor unsuspecting houseplants.  If you live in Europe it could be easy to dismiss these as tech gadgets that may never catch on, but that would be badly misjudging the ever-expanding user base for home air filtration that already exists in Asia and beyond. These devices are for sale because people want them, and the market could be worth in excess of US$30 billion per year by 2023. In some regards, indoor air purification is an individually empowering technology. In a well-sealed home, filtration-based purifiers clearly make a difference and can noticeably reduce concentrations of tiny harmful particles, particularly if the home is somewhere with lots of pollution outdoors, such as central Beijing or Delhi.  The evidence for the removal of harmful gases indoors, including volatile organic compounds from paints and glues, is sketchier. Some systems get the gases to stick to a charcoal-based filter, but there is little independent data that shows these actually work. In other types of purifiers UV radiation is used to accelerate a chemical reaction that turns those gases into carbon dioxide and water. However manufacturers have not yet published data to show that this process doesn’t actually end up converting relatively benign compounds into something more harmful. Outdoor air filtration demonstrators have so far proved ineffective, simply because the atmosphere is so huge relative to the size of the filtering system. However, indoors, the balance shifts. Homes have internal volumes measured in the hundreds to maybe several thousands of cubic metres and, simply due to natural drafts and leaks, the indoor air is swapped with outdoor air perhaps once per hour. That is still a lot of cubic metres of air to clean, but the maths begins to stack up. Yet the costs of filtration are possibly larger than they first appear. Most air purifiers use cellulose or polymer membranes that are replaced every month or so, often as part of a regular service contract. The air is pushed through the filters with fans and pumps which use energy, perhaps anywhere between 100 watts (equivalent to a bright lightbulb) and 1000 watts (a microwave), depending on the size of the air cleaner and home.  Poor air quality in this sense then impacts on climate by increasing energy demand in the home and the city, and of course it adds directly to the user’s electricity bill. The power demands of air filtration are not as great as air cooling, but would potentially run 365 days a year, not just in the summer months. If you add 500 watts of continuous demand to millions of homes, this becomes a big deal. Then there is the elephant in the room. What happens to all those millions of microfiber particle filters or traps full of activated carbon? I asked that question more than 20 times in Las Vegas and the answer was always the same – you put them in the bin.  Should we care? Possibly, yes. Filters in the home that collect particles end up concentrating some rather unpleasant toxic chemicals gathered from air outside – heavy metals from brake wear, polycyclic aromatic compounds from wood and coal fires, nitrosamines from cigarette smoke, the list goes on.  A filter may end up holding milligrams (and maybe more) of individual chemicals that were initially found in air at very diluted concentrations, and whose previous fate was probably to deposit as a very thin layer over huge areas of land. If hundreds of millions of filters from millions of homes are then all dumped in the same few city landfills we double down on the concentration process. Are we simply shifting a problem from the air into a problem of those same chemicals now leaching out into the soil and water? It’s unclear how much thinking has gone into this, or the energy demand consequences should hundreds of millions of people start purifying their own air at home. (Thinking more positively for a moment: perhaps those millions of waste filters would offer someone an opportunity to “mine” the trace metals collected?) There are some obvious conclusions to be drawn, the most striking being that there is a financial opportunity for someone in every crisis. But this particular solution comes with costs that we haven’t yet well quantified. Air filtration adds electricity demand for sure, it needs raw materials and resources to build, maintain and support and it is possibly creating chemical disposal problems we’ haven’t yet evaluated. It does however reinforce the well-trodden scientific principle that it’s always more efficient to stop pollution at source than try to clean up afterwards."
"**England enters a tougher version of its three tier system of restrictions on Wednesday, as a four-week lockdown ends.**
Northern Ireland has a two-week circuit-breaker lockdown, while Wales is banning the sale of alcohol in pubs, cafes and restaurants from Friday. Scotland has its own five-tier system.
Across the UK, some restrictions will be relaxed over Christmas, to allow three households to form a ""Christmas bubble"".
From just after midnight on Wednesday 2 December, areas will be placed in one of three tiers: medium, high and very high.
About 99% of England has been placed into the high and very high coronavirus risk category - tiers two and three.
The placing of areas in each tier will be reviewed every 14 days, with the first review on 16 December.
**Areas in tier two**
**Tier two (high) rules**
**Areas in tier three**
**Tier three (very high) rules**
Additional restrictions apply:
**Areas in tier one**
Only three areas have been placed in the lowest tier:
**Tier one (medium) rules**
Areas in the lowest tier will have some restrictions relaxed:
There are exceptions in all tiers for childcare and support bubbles. More details of the plan are here.
The new coronavirus tier restrictions will mean 55 million people will be banned from mixing with other households indoors. The decision about which tier to place an area in is based on:
Lockdown restrictions in Wales were eased on 9 November.
**The current rules say:**
People who you don't live with still cannot come into your home socially, unless you are in an extended household (bubble) with them. Tradespeople can enter your home to carry out work.
However, from **Friday 4 December:**
Read Wales' official guidance.
Northern Ireland started a two-week circuit-breaker lockdown from 00:01 GMT on Friday 27 November.
Read Northern Ireland's official guidance.
Each area of Scotland has been placed in one of five tiers.
Eleven local authority areas in west and central Scotland have recently moved from level three to level four, affecting two million people.
First Minister Nicola Sturgeon told MSPs the level four measures would be lifted at 18:00 GMT on Friday 11 December.
**Areas in level zero**
No areas have been placed in the lowest tier.
**Level zero (nearly normal) rules**
**Areas in level one**
**Level one (medium) rules**
Additional restrictions apply:
**Areas in level two**
**Level two (high) rules**
Additional restrictions apply:
**Areas in level three**
**Level three (very high) rules**
Additional restrictions apply:
**Areas in level four**
**Level four (lockdown) rules**
Additional restrictions apply:
Schools stay open in all levels, and here must also be no non-essential travel between Scotland the rest of the UK.
**Do you meet other people for exercise? Have you been out walking during the November lockdown? You can share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:"
"

On July 23, 178 nations agreed to a new draft of the Kyoto Protocol on global warming. The United States did not. The United States, alone in the world, did the right thing–whether or not you care about global warming. If you don’t care, the bottom line is that our economy will prosper. And if you do care, the bottom line is that our economy will prosper and produce technologies that must reduce the relative production of greenhouse gases, which we’ll gladly sell to everyone else who ratifies the “New Kyoto.” 



The “New Kyoto” is a revision of a 1997 instrument that must be ratified by nations that produce 55 percent or more of the world’s carbon dioxide emissions in order for it to enter into force. The United States isn’t going along (our Senate may currently sport about 12 of the 67 votes required for ratification), and the only way this magic number can be reached is if the Japanese come on board. They had signaled that they would not, until the Old Kyoto was modified in their favor. 



In a nutshell, while the Old Kyoto required that the major industrialized nations reduce their emissions of carbon dioxide to an impossibly low 5.2 percent below 1990 levels, beginning 6.4 years from now, the New Kyoto requires an impossibly low 1.8 percent below 1990 levels at the same time. This is impossible because most of the world is already running about 12 percent above 1990, as emissions grow with population and prosperity. Most of the 178 signatory nations, including China and India, have no commitments to reduce emissions under this Protocol. The lion’s share falls onto Europe, Canada and Japan. Almost all of the industrial emissions of carbon dioxide come from the combustion of fossil fuels. 



The “New Kyoto” replaces the Old Kyoto as the most ineffectual environmental treaty ever proposed. Here are the numbers: 



Assume that the earth’s temperature is destined to rise 4.5ºF for a doubling of atmospheric carbon dioxide. This is a standard U.N. assumption, subject to considerable debate but it is a common reference point. (Note that temperatures rose 1.0ºF in the last 100 years and most people prospered.) Also assume that the nations–mainly European–that have to do something under the New Kyoto live up to their commitments (they won’t), and compare the projected temperature changes to what would happen if no one made any special attempt to reduce emissions–the so‐​called “business‐​as‐​usual” approach. 



The New Kyoto produces a world, in 2050, whose surface temperature is 0.04ºF lower than it would be if no one did anything. That’s four hundredths of a degree. This is about 30 percent of the warming “saved” by the Old Kyoto, which itself was small beer. By 2100, the saved warming is 0.11ºF. These numbers come from the U.N.‘s own computer models. 



The warming rate in the U.N. models is fairly constant, once you chose your “storyline” (that’s what they call their future social projections these days). The mean “storyline” in vogue these days warms the surface about 4.5ºF in 100 years, or 0.045ºF per year. Thus does the New Kyoto signify nothing. Do the math. If everyone does what they say they will, the mean global surface temperature that would have normally been expected on January 1, 2050, will appear on September 18, 2050. The New Kyoto delays this warming by 288 days. 



This, of course, assumes that the United States does nothing, while the other nations raise taxes enough to drive emissions to 1.8 percent below 1990 levels. That’s the only way we know to reduce the energy use that produces these emissions. No one knows what the total cost will be. But it certainly means that European governments are going to gobble up more of their people’s income and corporate profits than they do now. 



This will have the salutary effect of forcing multinational business over to our side of the ocean, where people will have more money to invest. Like stockholders everywhere, they are going to demand more production with increased efficiency. Thus the New Kyoto will in fact force investment in technologies that are more likely to produce things that cost less energy to operate. 



The irony of all of this is that our European friends have sentenced themselves to economic stagnation while doing nothing about global climate change. At the same time, they have insured a vibrant United States that will, with the investment dollars that the New Kyoto diverts in our direction, produce a cleaner future. 



All of this is inevitable if only President Bush stays the course and stays away from the New Kyoto.
"
"
Share this...FacebookTwitterThe online Financial Times Deutschland reports that a British team of astronomers, led by Jane Greaves of the University of St. Andrews in Scotland, have found strong evidence of global warming of Pluto’s atmosphere using the 15-meter James Clerk Maxwell Telescope in Hawaii. They also detected carbon monoxide in its atmosphere.
The researchers also say that new findings show that Pluto’s atmosphere extends to more than 1860 miles (3000 km) above the surface –  or a quarter of the distance out to its largest moon, Charon. Before it was thought to be only 100 km thick. Greaves will present the new discovery today at the Royal Astronomical Society’s National Astronomy Meeting in Wales.
Pluto’s atmosphere appears to have expanded due to warming. Greaves says:
The change in brightness over the last decade is startling. We think the atmosphere may have grown in size, or the carbon monoxide abundance may have been boosted.”
The Financial Times writes;.
Pluto’s extremely low density atmosphere has a fragile balance made up of the coolant carbon monoxide and the greenhouse gas methane. It is probably the most sensitive in the solar system, Greaves said.”
The far away dwarf planet is probably currently experiencing climate change, said Greaves. ‘We believe that the expansion of the atmosphere has grown. in 1989 Pluto passed its closest point to the sun in its orbit. Probably the stronger solar radiation vapourised additional ice and the atmosphere expanded.”
In the new study, scientists found that the carbon monoxide gas on Pluto is extremely cold, at about minus 364°F (-220°C ).
‘This simple, very cold atmosphere, which is greatly influenced by the sun’s warmth, could give us important information on the fundamental physical interactions and thus a better understanding of the earth’s atmosphere,’ Greaves said.”
Yeah – like the sun plays the major role on atmospheric behaviour and climate, even when it is 3 billion miles away (the earth is only 93 million miles away) and that everything else, like oceans and atmospheres, reacts to its changes and orbital changes.
Share this...FacebookTwitter "
"Scott Morrison has acknowledged there are “costs associated with climate change” but has declined to spell out what 3C heating would do to job creation and economic growth in Australia. Ahead of the release of its technology roadmap, the federal government is attempting to ramp up political pressure on Labor over its commitment to a net zero target by 2050, blasting the opposition for adopting a target without a fleshed-out strategy to meet it, and pointing out that CSIRO research cited positively by Labor assumes a carbon price of more than $200 to drive the transition.  But the government is also having to fend off sustained questions about basic contradictions in its own messaging. In question time on Monday, Labor asked why the government was criticising the opposition’s 2050 target when the Paris agreement, which the current government signed and ratified, required carbon neutrality by mid-century. The emissions reduction minister, Angus Taylor, declared that was incorrect, because Paris involved “the world achieving net zero in the second half of the next century”. The independent MP Zali Steggall – who is championing a bill to lock Australia in on a net zero target – also asked Morrison to detail the costs of inaction, given the government has been blasting Labor for days for failing to outline the cost of action. In response to Steggall’s question, Morrison stepped around the specifics, but said “we do understand there are costs associated with climate change that we are indeed taking action on to reduce emissions”. The prime minister said it was important to build resilience, and develop adaptation measures “to ensure that Australians can thrive in the climate we live in while taking the necessary action when it comes to emission reduction”. During the run of questions on Monday, Morrison told parliament the government had “a clear plan” to achieve a 2030 emissions reduction target of 26%, without mentioning that almost 100 megatonnes of the Coalition’s proposed reductions are booked to unspecified “technology improvements” and additional pollution cuts are attributed to an electric vehicles strategy that the government has not yet announced. The government is also counting, in its 2030 plan, a 367-megatonne contribution from carryover credits – an accounting system that allows countries to count carbon credits from exceeding their targets under the soon-to-be-obsolete Kyoto protocol periods against their Paris commitment for 2030. The latest emissions data confirms pollution has dipped slightly on the back of new clean energy and a sharp fall from agriculture due to the drought, but the decline was almost entirely wiped out by surging industrial pollution. While keeping all its options open, the government has been signalling for some days it is unlikely to adopt a 2050 target. On Monday, the government attempted to ramp up a parliamentary attack against Labor’s proposal, contending such a transition would be devastating for sectors like agriculture. At one point, the deputy prime minister, Michael McCormack, asked Labor to “look a steer in the eye, and say how are you going to stop your methane!” Asked repeatedly on Monday whether the government would nominate a long-term target, Taylor said the government would be “focusing on technology”. After question time, the Labor leader, Anthony Albanese, said he was confident that championing a net zero position would not cost the ALP seats in Queensland at the next federal election. He said if it was reasonable for Morrison to demand Labor to detail the costs of action, it was also reasonable to ask the government about the costs of inaction. “We saw a bit of the cost of inaction over the bushfire season, when we lost 33 lives, when we lost over 3,000 homes, when we lost 12 million hectares of land,” Albanese told Sky News. “What we know is that the cost of that season certainly won’t be $2bn that the government talks about. We know the consequences of dangerous climate change are catastrophic for our economy. “Global economists predict that the cost of a greater than 2C increase in global temperatures will be between 15 and 25% lower economic growth – a greater cost than we saw during the great depression.”"
"
Share this...FacebookTwitter Germany’s leading political daily the Frankfurter Allgemeine Zeitung (FAZ) published an open letter written by geology Professor Dr. Friedrich-Karl Ewert rejecting the claims made by the Potsdam Institute for Climate Impact Research (PIK) that sea level rise is accelerating. The open letter is now available at the European Institute for Climate and Energy (EIKE), read here (part after Schellnhuber). 
According to the PIK:
“Due to global warming, sea level is rising today faster than at any time in the last 2000 years. Since the start of the Industrial Revolution, the curve is going ‘up steeply’.”
These claims are based on cherry-picked data from a single location at the North Carolina coast. In response Ewert’s wrote the open letter, excerpts of which follow. Ewert writes (emphasis added):
This claim must be refuted because the declaration made by the PIK is factually false. Correct is that scientists have determined just the opposite. The Journal of Coastal Research reported in journal 27/3 (May 2011).”
Here according to the journal, the global temperature increase during the last 100 years did not accelerate sea level rise, Indeed sea level rise has recently slowed down.
Ewert writes that there is a simple proof for the falsehood of PIK’s claim:
At the coral reefs in the Great Barrier Reef and also at the coasts of the Caribbean Islands one finds dry coral reefs which were about 3.5 meters over today’s sea level, When these reefs formed, the sea level was accordingly higher. No one knows how fast the sea level rose during the time before that.”
Ewert also rejects the PIK’s claim that the Western Antarctic Ice Shelf is about to tip and that global warming is racing ahead. Ewert writes in his letter:
Detailed evaluation of temperature data shows a completely other picture: In many parts of the globe a rewarming took place after 1700, after the Little Ice Age, and has reached its peak in the mid 1990s. During this long period a number of warming and cooling phases took place, and were earlier often much stronger; the last pronounced warming phase took place in the first half of the 20th century, before man made carbon emissions; and since 1998 a new cooling phase has started in many regions of the earth, and is still taking place and is at times quite stark. The results of the evaluation will be published after the conclusion of a comprehensive analysis. ‘Our CO2’ has not had any real impact on this development.”
Share this...FacebookTwitter "
"
The Monthly Energy Review for August 2009 has been published by the US Energy Information Administration and it has some interesting CO2 production data which you can see here in tabular form.
I’ve graphed the data of interest in two separate graphs. First we have the annual plot of “Carbon Dioxide Emissions From Fossil Fuel Consumption by Source” with data to the end of 2008 for the USA:
Click for larger image
Note that in 2008 a significant drop was seen in total CO2 produced. Corresponding to the drop is a drop in CO2 produced by petroleum, which seems to indicate that high gasoline prices last year which contributed to less miles driven, may have been the dominant factor.
The Department of Transportation notes in U.S. Traffic Volume Trends:
Cumulative Travel for 2008 changed by -3.6% (-107.9 billion vehicle miles). The Cumulative estimate for the year is 2,921.9 billion vehicle miles of travel.
Gas prices receded though in late 2008 and into 2009. But our economy continued its slide with layoffs, store closings, and less demand for durable goods during that same period.
Click for larger image
The graph above compares USA CO2 output by source for the first 5 months of 2008 and 2009. As you’d expect, there is a seasonal drop in coal and natural gas related to less heating requirements, but there remains significant offsets compared to the same months in 2009 for petroleum and coal use. With the severe winter and cool spring seen in much of the US eastern areas with the heaviest population, one might expect increased demand for heating. In fact, this EIA report shows that average heating degree days from 2008 to 2009 tripled, with significant jumps in the east, Midwest, Great Lakes, and New England.
With heating demand actually went up in the first 5 months of 2009, one explanation for this 2008 to 2009 drop in CO2 production could be our sagging economy. With less demands for durable goods, manufacturing and transport are reduced. This affects coal due to lowered electricity demands and petroleum is less for for lowered goods transport. Unemployment may also figure in lowering petroeum usage due to less daily commuting.
I found it interesting that despite all the eco-pronouncements of reducing fossil fuel use, the one thing that appears to have made a significant difference is our sagging economy.
The EIA web page with additional reports is available here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93adf856',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The ten hottest years on record were all during the past two decades and the hottest global ocean temperatures ever were recorded in 2018 – a heat increase from 2017 equivalent to 100 million times that of the Hiroshima bomb. Climate change is here and it’s already wreaking havoc. The polar bear – something of a poster child for climate change – is just one of countless victims in this warming world. It’s thought that if global temperatures continue to rise by an average of 4.5°C since pre-industrial times, which is likely to happen if we do nothing to reduce our carbon emissions, half of the world’s wildlife could be lost from Earth’s most biodiverse places. As ocean temperatures melt ice sheets – the hunting grounds of polar bears – these large carnivores have to search new areas for food, which is why 52 polar bears “invaded” a Russian town in February 2019, looking for their next meal. Locals were frightened to go outside – with good reason: polar bears can, and do, hunt people.  Unfortunately, climate change is only going to make these negative interactions between humans and wildlife more common. Already, while Australia heats up, wildlife is seeking refuge in towns. Kangaroos have swarmed human settlements in search of food and flying foxes have had to be hosed down by locals to stop them from overheating. In southern Africa, more frequent droughts have meant thirsty elephants have raided villages to eat crops and pilfer water from storage tanks. Most wild animals are naturally averse to being so close to humans, so their incursions into our lives shows how desperate they are getting. As climate change begins to take its toll on humans, by reducing crop productivity for example, we are likely to become less tolerant of these sorts of human-wildlife conflicts. Poor African villagers who have had their entire yearly crop destroyed by a herd of hungry elephants can hardly be blamed for wanting to get rid of the problem by killing the animals. Sadly, elephants – like most other species – are already experiencing precipitous declines in their populations and this is almost exclusively due to human activities. Climate change will exacerbate conflicts over natural resources between and within species – ourselves included. For example, some observers have suggested climate change was partly responsible for the Arab Spring uprisings, as droughts forced people from rural areas into overcrowded cities and inflamed tensions. If conflicts within our own species can’t be overcome, there is little hope for mitigating conflicts with other species – especially as resources become scarcer. But there is a small glimmer of hope – there are effective methods to reduce damage caused by wildlife. Polar bears can be scared away from human settlements by flares and water tanks can be made elephant-proof. These technical fixes can help limit immediate conflict between wildlife and humans in the short term, providing much-needed relief in poor communities from the damaging effects of intruding wildlife. Realistically however, technical fixes to human-wildlife conflict are only a temporary stopgap. To truly address the issue, we must focus on the root cause. Carbon emissions must be reduced – not only for the sake of wildlife but for the survival of humans too.  Wildlife habitat must be protected to ensure that species have space and food without needing to enter human settlements. Equally, societies must address their insatiable demand for natural resources, reduce overconsumption and excessive waste. Much of this is easier said than done, of course. Without political will and sufficient funding all of this falls short. Global leaders must step up to the task – and it is partly up to ordinary people to pressure them to act. Movements such as the Extinction Rebellion and the school students organising global strikes against climate change are an encouraging start and must be built upon. We need to cause an uproar like our lives depend on it – because they do. We have no planet B, as the refrain goes – and neither do the planet’s 8.7m other species."
"**Labour activists in the US say big retailers like Amazon and Walmart must do more to protect workers as surging coronavirus cases coincide with the holiday shopping rush.**
They are calling for hazard pay, paid sick leave and better communication about outbreaks, among other things.
The campaign comes as workers across the US have spoken out about condition and concerns over their health.
""Associates like me are scared,"" said Walmart worker Melissa Love.
The workers rights campaign launched on Monday was organised by United for Respect, a workers rights non-profit that says it represents more than 16 million people across the US.
Separately, the labour union UFCW, whose members include grocery and meatpacking plant workers, also called on employers to do more to protect staff.
""Simply put, frontline workers are terrified because their employers and our elected leaders are not doing enough to protect them and stop the spread of this virus,"" UFCW International President Marc Perrone said.
""As holiday shopping begins this Thanksgiving, we are already seeing a huge surge of customer traffic. Unless we take immediate actions beginning this holiday week, many more essential workers will become sick and more, tragically, will die.""
Ms Love, a member of United for Respect who has worked at Walmart for five years, said on a call organised for reporters that she feared a rush of holiday shoppers could turn Walmart into a ""super-spreader"" hub.
""Working Black Friday this year comes with an obvious danger,"" said Ms Love, who is based in California. ""I do not believe Walmart should be trying to entice crowds into our stores on Friday and risk a super-spreader event.""
Courtenay Brown, another member of the group, who picks groceries for Amazon, said the company has had to send out repeated notifications in recent days about infected staff. Amazon had boosted pay for frontline staff by $2 (Â£1.50) an hour, but that policy ended in June.
""Right now it's what we call the turkey apocalypse, where we are forced to just push out as much as we possibly can,"" said Ms Brown, who works in New Jersey.
She said she's happy to have a job, but to Amazon ""my life doesn't really mean much - it's just a means for Bezos to continue making billions off of us"".
Workers said the companies had the means to spend more to protect and compensate workers, noting the way profits have soared during the pandemic, which has shifted purchases to essentials and kept many smaller competitors closed.
Amazon and Walmart did not respond to requests for comment on Monday, but they have defended their practices in the past.
Walmart has changed how it is handling Black Friday discounts, offering the deals online and over several days to try to avoid crowding at its stores.
Amazon last month said nearly 20,000 people, or 1.4% of its staff, at Amazon and Whole Foods, the grocery store it owns, had tested positive for Covid-19 in the US since the start of the pandemic.
It said that was lower than would be expected given wider infection rates.
""All in, we've introduced or changed over 150 processes to ensure the health and safety of our teams, including distributing over 100 million face masks, implementing temperature checks at sites around the world, mandating enhanced cleaning procedures at all of our sites, and introducing extensive social distancing measures to reduce the risk for our employees,"" the company said.
More than 250,000 people in the US have died from coronavirus since the start of the pandemic. In recent weeks, case numbers, death rates and hospitalisations have soared, straining the health system and prompting many places to re-impose restrictions.
Some major retailers, including Amazon and Walmart, have posted strong results this year as many customers opted to buy online instead of venturing out to the local store.
A recent analysis by the Brookings Institute, a Washington think tank, found that company profits at 13 of America's biggest retailers increased by an average of 39% this year, while pay for frontline workers had increased by just $1.11 per hour - ""a 10% increase on top of wages that are often too low to meet a family's basic needs"".
Workers said they are well aware of the disparity.
""They closed the corporate office until July 2021 because of the virus meanwhile we're expected to keep risking our lives to pay for their big salaries,"" Ms Love said."
"**People arriving in England from abroad will be soon able to reduce their quarantine by more than half if they pay for a Covid test after five days, the transport secretary has announced.**
The rules will come into force from 15 December and the tests from private firms will cost between Â£65 and Â£120.
Grant Shapps said the scheme would ""bolster international travel while keeping the public safe"".
The travel industry welcomed the policy but described it as ""long overdue"".
It follows Boris Johnson's announcement that England will come under ""toughened"" three-tiered regional restrictions when the lockdown ends on 2 December.
Under the new travel rules, passengers who arrive from a foreign destination not on the government's travel corridors list will still need to enter self-isolation.
However, if they pay for a test after five days and it comes back negative, they will no longer need to self-isolate.
Results will normally be issued in 24 to 48 hours. This means people could be released from quarantine six days after arrival.
Mr Shapps said: ""Our new testing strategy will allow us to travel more freely, see loved ones and drive international business. By giving people the choice to test on day five, we are also supporting the travel industry as it continues to rebuild out of the pandemic.""
Scotland and Northern Ireland are considering a similar scheme while Wales said it was ""supportive in principle of the proposals for a Test and Release scheme"".
A Welsh government spokesperson said: ""We will need to consider the data and evidence underpinning this scheme before making any decisions on changing international travel restrictions in Wales.""
Scotland said it was working with the main commercial airports in Scotland and clinical advisers to understand the risks and benefits of the scheme as well as the capacity of private sector labs to conduct testing to a minimum standard.
A spokesperson added: ""It is important that any travellers arriving in the UK understand, and respect, the different restrictions in place in the different nations.""
That means that if someone is flying into England but their final destination is an address in Scotland, it would not be possible for passengers to take the test in England.
Northern Ireland's Department of Health said: ""Consideration is currently being given to the implementation of testing scheme which could potentially allow those who have arrived in Northern Ireland from a non-exempt country to end their self-isolation following receipt of a negative test, which would be privately provided.""
A step in the right direction - that's how airlines are describing the government's decision to ease the quarantine regime.
The industry is in survival mode, desperate for money so anything that could open up routes and bring in much-needed cash is being welcomed. And this does raise the prospect of Christmas holiday ticket sales.
But they say the new plan is still far from perfect.
Passengers will still have to go into quarantine, and realistically, a test on day five is still likely to leave them in isolation for the best part of a week in total, as they wait for the results to come through.
There's also the cost. The test has to be done privately, and typical prices range from Â£100-150. For a family of four, for example, that's a sizeable extra chunk on the cost of a winter holiday.
What airlines are calling for is something more radical. They want a pre-departure testing regime, or a system of quick, regular and cheap tests - which would allow quarantine to be avoided altogether, until a vaccine is ready.
But they say this announcement means there is, at least, some light at the end of the tunnel.
Tim Alderslade, chief executive of Airlines UK, the industry association representing UK-registered carriers, said the announcement provided ""light at the end of the tunnel"" for the aviation industry and people wanting to go on holiday.
He predicted demand for air travel will ""tentatively return"" following the decision but said a pre-departure testing regime that can completely remove the need to self-isolate is ""the only way we're going to comprehensively reopen the market"".
Michael O'Leary, Ryanair's chief executive, told BBC Radio 4's Today programme quarantine for arrivals was a ""fig leaf that doesn't work"" and that testing for travellers coming to England should happen before departure, rather than after arrival.
He also ruled out requiring proof of vaccination for passengers on short-haul routes after Australian carrier Qantas said it would require it for travellers.
When Mike Hansford, 27, travels to the Canary Islands for Christmas next month, he and his family will already have paid a private company Â£120 each for a Covid test. The Spanish island insists that visitors produce a negative Covid test done within the previous 72 hours when they reach its shores.
""We have looked into the NHS's service,"" said Mr Hansford. ""However, what we were explicitly told by the Foreign Office was: 'Do not go to the NHS for your Covid test. You must have one done privately'.""
Mr Hansford, who is travelling with his wife Lucy, her parents and two friends, said he assumed they were told to go private to keep tests free for people who are not well, as well as guaranteeing the results arrive in time.
On the way back, Mr Hansford said it was highly unlikely he and his wife would pay a further Â£120 for another private test as both work from home and are happy to quarantine for 14 days.
""But I'm not sure how my friends who I'm travelling with will feel,"" he said. ""One of them is a builder and a carpenter by trade so I think it is safe to say he'd rather probably avoid a two-week quarantine.""
The PM said the latest news regarding the University of Oxford's vaccine was""incredibly exciting"".
However, he also warned that the virus would not grant a ""Christmas truce"" and urged families to make a ""careful judgement about the risks of visiting elderly relatives"" ahead of a UK-wide approach to Christmas being announced later this week."
"Not everyone cheered for the school children striking against climate change. In the US, democratic senator Dianne Feinstein accused them of “my way or the highway” thinking. German Liberal Democrats leader Christian Lindner said that the protesters don’t yet understand “what’s technically and economically possible”, and should leave that to experts instead. The UK’s prime minister, Theresa May, criticised the strikers for “wasting lesson time”. These criticisms share a common accusation – that the striking children, while well-intentioned, are behaving counter-productively. Instead of having a rational response towards climate change, they let emotions like fear and anger cloud their judgement. In short, emotional responses to climate change are irrational and need to be tamed with reason. The view that emotions are intrusive and obscure rational thinking dates back to Aristotle and the Stoics – ancient Greek philosophers who believed that emotions stand in the way of finding happiness through virtue. Immanuel Kant – an 18th-century German philosopher – saw acting from emotions as not really agency at all. Today, much of political debate is moderated with the understanding that emotions must be tamed for the sake of rational discourse. While this view stands in a long tradition of Western philosophy, it invites Jordan Peterson and Ben Shapiro to insist that “facts, reason and logic” can dismiss an emotional response to anything in debates. However, the view that emotions aren’t part of rationality is false. There’s no clear way of separating emotions from rationality, and emotions can be rationally assessed just like beliefs and motivations. Imagine you’re walking in the woods, and a huge bear approaches you. Would it be rational for you to feel fear? Emotions can be rational in the sense of being an appropriate response to a situation. It can be the correct kind of response to your environment to feel an emotion, an emotion might just fit a situation. Fear from a bear coming towards you is a rational response in this sense: you recognise the bear and the potential danger it represents to you, and you react with an appropriate emotional response. It could be said to be irrational not to feel fear as the bear walks towards you, as this wouldn’t be a correct emotional response to a dangerous situation. Imagine you find out that a meteor will kill millions of people across the world, displace hundreds of millions more, and make life for the remainder of humanity much worse. The world’s governments neither put a defence system in place, nor do they evacuate the people threatened. Fear from the meteor, and anger at the inaction of governments, would be a rational response as they are an appropriate reaction to danger. And if you don’t feel fear and anger, you’re not appropriately responding to a dangerous situation. As you’ve probably guessed, the meteor is climate change. The world’s governments aren’t addressing the causes of climate change or preparing to mitigate its impact. For the people of Mozambique, who are reeling from the devastation of Cyclone Idai, anger is entirely appropriate. Climate change is largely a product of economic development in richer countries, while the world’s poorest are bearing the brunt of its effects. Regardless of how fitting an emotional response is, it may sometimes be unhelpful for what a person wants to achieve. Theresa May makes this point about the school strike: understandable, but young people missing valuable lessons makes it harder for them to solve climate change. As others have already pointed out, climate change demands rapid action – waiting until some vague point in the future when the children are old enough to do something is relinquishing responsibility instead of meaningful action. It is, however, hard to deny that fear and anger sometimes lead people to choices they regret. However, dismissing emotional responses on this basis is too quick. There are many examples where fear and anger have triggered the correct response and created a motivational push for change. As Amia Srinivasan, an Oxford philosopher working on the role of anger in politics, puts it,  Anger can be a motivating force for organisation and resistance; the fear of collective wrath, in both democratic and authoritarian societies, can also motivate those in power to change their ways. A lot of social change has happened because of anger against injustice, empowering the weak and oppressed, while causing those in power to fear they may be ousted leads to reforms and change. We do need scientific understanding of the climate crisis to solve it, but banning emotions from the debate and dismissing rational fear and anger about climate change may encourage people to do nothing.  So, not only are children, who are angry and scared about climate change, rational, they might be more so than the adults criticising them. Emotions play a bigger part in life beyond rationality – they mark values and indicate what people care about. Fear of the future and anger at inaction are ways young people can express their values. Their emotions are, in the words of feminist writer Audra Lorde, an invitation to the rest of society to speak. Dismissing the emotions of school children not only invalidates their rational responses to a grave situation – it implicitly states that their values aren’t taken seriously, and that adults don’t want to reach out to them. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitter German leaders seem to have a habit of driving their country into a wall.
This time it’s their desire to show environmental supremacy. Many really do believe they are powerful enough to control the climate and at the same time defy the laws of economics.
The European Institute For Climate And Energy (EIKE) has a piece written by Dr. Dietmar Ufer about an interview with economics professor Joachim Weimann on MDR public television.
Video of interview here at MDR:

Here’s the text of the interview:
MDR (0:08): With respect to the current price spiral for energy, is it the right approach?
Weimann: We’ve decided on an energy transition, and on a type of energy transition that is very very expensive. That means energy is going to be very expensive. It’s going to hit the poor very hard. That was to be expected- It was completel clear. That was easily predictable. It is indeed only the start of the price spiral. We have only begun to switch off the nuclear power plants and to start using renewable energy. It’s going to be very expensive.
MDR (0:57): You believe it’s going to impact a large spectrum of citizens.
Weimann: It has to be clear that the supply of energy that we want, one that is without nuclear energy, preferably without fossil fuel and mostly from renewable energy, is an extremely expensive way of producing energy and to save CO2
MDR (1:32): Are there alternatives available that could have avoided these extreme prices?
Weimann: Of course there are alternatives. Economists have been warning for years that subsidizing renewable energies just as we are doing is a bottomless pit that will have minimal effect. Just look at the fact that just for solar energy we have invested 100 billion euros without this technology having made any notable contribution to climate protection. This is going to cost everyone, especially gthe poor and also the working class – many are going to suffer immensely.
At the 2:24 mark, the Youtube clip then looks at veteran journalist Günter Ederer who 4 months ago in April warned of the exploding cost consequences of Germany’s shock energy transition. The pain is just beginning.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman climate blog Readers Edition here has been keeping an eye on the winter in South America. While much of the news has been buzzing about the “record heat wave” hitting the US last week (a whopping 0.4% of the stations reported record highs! /sarc), Europe and South America for example are being left out in the cold.  

No warming in sight. Chart source: http://wxmaps.org/pix/temp8.html
In South America, dozens of people have died from the bitter cold in 7 countries so far, just when cold snaps were supposed to be getting rarer and the heat waves more frequent.  The cold is repeat of last year’s brutal South American winter.
Readers Edition writes (paraphrased):
In southern Peru, temperatures in the higher elevations of the Andes fell to -23°C. Since the beginning of last week 112 people have died of hypothermia and flu.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Coldest winter in 10 years
In Argentina the lowest temperatures  in 10 years were measured – the temperature dropped to -14°C. At least 33 people died, some froze to death and some from poisonous gases emitted from faulty heaters.
Thousands of cattle freeze to death in fields.
It was unusually cold in neighbouring countries. In the tropical regions of Bolivia where temperatures rarely fall below 20°C (68°F), the temperature hovered near 0°C.  At least four people died because of the cold. Two homeless persons died in Uruguay. Thousands of cattle froze in the fields in Paraguay and Brazil.

Natural gas shortages
In some areas of Bolivia and Peru, school was cancelled for some kids at the end of the week. Emergency shelters were opened for the homeless in larger cities. In Argentina some provinces faced natural gas shortages.
Heavy snow in Chile
Unusually heavy snows have fallen in parts of Chile. States of emergency have been declared in 8 communities with some buried in  up to 3 meters of snow. In the south of the country about 170 people have become isolated from supply lines.
Share this...FacebookTwitter "
"Pine martens are returning to areas of the UK after an absence of nearly a century. Following releases in mid-Wales during 2015, reintroductions are proposed in north Wales and southern England for 2019. The pine marten is a small native carnivore that inhabits a range of woodland habitats. It’s an excellent climber and often nests within tree cavities. This opportunistic predator has a varied diet including fruit, eggs, songbirds and small mammals. By the 1920s, pine martens were virtually extinct in the UK after centuries of persecution to protect game birds and poultry. Only a population in north-west Scotland and small numbers in northern Wales and England survived. With UK legal protection, their range has expanded since the 1980s, increasing their encounters with the grey squirrel. Since George Monbiot penned “how to eradicate grey squirrels without firing a shot” in 2015, the media has courted the charismatic mammal as the saviour for the UK’s embattled red squirrels. The media message is simple: the return of pine martens will herald the decline or even eradication of grey squirrels, which, since their arrival from North America in 1876, have caused regional extinctions of the native red squirrel. That’s because pine martens supposedly prefer eating greys, while leaving reds alone. The optimism around pine martens in the UK originated from research in Ireland and Scotland. In Scotland, scientists studied forests containing pine martens, red squirrels and grey squirrels. The more pine martens they recorded using a woodland area, the more likely they were to find red squirrels and the less likely grey squirrels were to be there. Like earlier Irish studies, this suggested that pine martens suppress grey squirrel populations to the overall benefit of red squirrels.  However, that’s not quite the whole story. There’s a desire in the media to find heroes and villains in nature which simplifies the situation and obscures the potential impact of a returning predator on British wildlife and livestock. Sadly, ecology and conservation are rarely simple and the restoration of pine martens will not always follow a script. The Scottish pine marten researchers make clear that pine martens sometimes eat red squirrels. In a small number of other studies conducted elsewhere in Europe, reds were in fact a significant seasonal component of pine marten diet – up to 53% in one case. It’s therefore incorrect to suggest, as some conservation groups have, that dietary studies show pine martens very rarely eat red squirrels. The reality is that predation rates reflect the relative abundance of red squirrels to other prey, encounter rates and local habitat characteristics. Why grey squirrels have declined in the presence of pine martens remains uncertain. The impact of martens on greys may vary geographically and it’s unwise to simply extrapolate the findings from Scotland and Ireland to the rest of the British Isles without a note of caution. Suggesting the pine marten is the best long-term solution for grey squirrel control in England is premature and requires more research to confirm. Pine martens have been absent from much of England for around 100 years, a period of significant agricultural and urban change. Landscapes have altered dramatically and many potential prey species have regionally declined. Pine marten predation upon these could therefore prove to be locally significant. This should not be a barrier to reintroducing pine martens. Instead, it reinforces the need for informed discussions with all interest groups likely to be affected. We must acknowledge that as a last resort, lethal control of predators may be necessary to conserve rare species such as some ground nesting birds. As the pine marten becomes more common in the UK and Ireland, inevitably there will be scenarios where lethal intervention is unavoidable. A pine marten predating a seabird colony was shot in 2018 under licence to protect an internationally important breeding population. Measures to prevent predation of poultry or game birds are frequently recommended where pine marten restoration is occurring. These include the installation of electric fencing, cutting back branches overhanging pens and ensuring that wire netting has no holes martens could get through. While these management recommendations are useful, many people may find it difficult to implement them. As a result, any negative impacts of a returning arboreal predator will fall heavily upon a handful of poultry owners. The return of the pine marten may also complicate the conservation or reintroduction of other species. Although the location and other details are confidential, there were concerns that a pine marten was adversely affecting a red squirrel conservation programme after an individual was found to be regularly visiting release enclosures. As pine martens naturally spread from Scotland into northern England, adaptive and measured responses will be needed to responsibly manage their return. An approach to conservation that’s media-friendly but built on limited evidence rarely works, and certainly won’t in pine marten restoration."
"
No we aren’t talking pianos, but Grand Solar Minimums. Today a new milestone was reached. As you can see below, we’ve been leading up to it for a few years.
Above: plot of Cycle 23 to 24 sunspot numbers in an 11 year window 
(Update: based on comments, I’ve updated the graph above to show the 2004 solar max by sliding the view window to the left a bit compared to the previous graph. – Anthony)
A typical solar minimum lasts 485 days, based on an average of the last 10 solar minima. As of today we are at 638 spotless days in the current minimum. Also as of today, May 27th, 2009, there were no sunspots on 120 of this year’s (2009) 147 days to date (82%).
Paul Stanko writes:
Our spotless day count just reached 638.
What is so special about 638?  We  just overtook the original solar cycle, #1, so now the only cycles above  this are: cycles of the Maunder minimum, cycles 5 to 7 (Dalton  minimum), and cycles 10 + 12 to 15 (unnamed minimum).
Since the last  one is unnamed, I’ve nicknamed it the “Baby Grand Minimum”, in much  the same way that you can have a baby grand piano. We would now seem to  have reached the same stature for this minimum.  It will be interesting  to see just how much longer deep minimum goes on.
Of course it depends on what data you look at. Solar Influences Data Center and NOAA differ by a few days. As WUWT readers may recall, last year in August, the SIDC reversed an initial count that would have led to the first spotless month since 1913:
Sunspeck counts after all, debate rages…Sun DOES NOT have first spotless calendar month since June 1913
NOAA did not count the sunspot, so at the end of the month, one agency said “spotless month” and the other did not.
From Spaceweather.com in an April 1st 2009 article:
The mother of all spotless runs was of course the Maunder Minimum. This was a period from October 15, 1661 to August 2, 1671.
It totaled 3579 consecutive spotless days. That puts our current run at 17.5% of that of the Maunder Minimum.
By the standard of spotless days, the ongoing solar minimum is the deepest in a century: NASA report. In 2008, no sunspots were observed on 266 of the year’s 366 days (73%). To find a year with more blank suns, you have to go all the way back to 1913, which had 311 spotless days (85%):

The lack of sunspots in 2008, made it a century-level year in terms of solar quiet. Remarkably, sunspot counts for 2009 have dropped even lower.
We do indeed live in interesting times.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96016f4d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The last two weeks of July are normally the hottest of the year, so it’s no surprise that we’re being deluged with public‐​service announcements about the horrors of global warming. Radio and television stations are compelled to transmit these announcements at no charge because of a long‐​standing policy that they must provide “public good.” “Don’t Litter” and “Fasten Seat Belts” come to mind. Now the notion has been expanded to “Fight Global Warming.”



By defining it as something we all should fight, these announcements tell us warming must be bad — something no comprehensive treatise on the science and economics of climate change has ever demonstrated. 



Ogilvy and Mather, a prestigious public‐​relations firm whose for‐​profit clients include IBM and Motorola, produced the global‐​warming ads for free on behalf of Environmental Defense, a major environmental nonprofit that clearly advocates certain types of global‐​warming legislation.



Like their ads for Motorola, Ogilvy and Mather’s global‐​warming announcements are clearly targeted towards sullen youth — a brilliant idea, considering the appallingly low level of scientific knowledge our children have in comparison to their counterparts around the world. But scientific exploration requires critical skepticism, and these ads are full of unquestioned certainties. 



Perhaps the most egregious is a radio ad, called “The Gift.” It mentions dying coral reefs, rising sea levels, melting ice caps, devastating floods, and hurricanes, and accuses us of leaving them all to our children. 



The ads ignore facts that are widely accepted in the scientific community. Take hurricanes. The frequency of category 4 and 5 storms — the really destructive ones — has increased as the planet warmed. Good sound bite, with only one problem: It’s back to where it was in the 1940s and 1950s, long before human beings started warming things up.



In fact, as late as the 1970s, scientists were more concerned with planetary cooling, as revealed in the 1974 CIA report, “Potential implications of trends in world population, food production, and climate,” that presented cooling‐​related food shortages as a major strategic threat. The report first appeared in public in the _New York Times_ on May Day, 1976. Soon, global cooling abruptly reversed into global warming. Crop yields rose. 



The public‐​service announcements are all similarly big on melting polar ice caps and consequent rises in sea level. The Arctic cap loses ice in the summer, but no one bothers to mention that we only began collecting data on it in 1979, at the end of the second‐​coldest period in the Arctic in a century. The ice had to be abnormally expanded then. 



It’s also floating ice, and melting it and doesn’t change sea level at all. And, for all the headlines about loss of ice in Greenland, which does contribute to rising sea levels, the mean temperature there was much higher from 1910 through 1940. Between then and the late 1990s, temperatures in southern Greenland — the region where ice is melting — declined sharply. One has to presume that Environmental Defense knows this.



Around the world, in Antarctica, for the last few decades, average temperatures across the continent have been going _down_. Snowfall has increased, resulting in more continental ice. In fact, every modern computer simulation of 21st century climate has Antarctica continuing to accrete ice.



Ogilvy and Mather marketed their public‐​service announcements through the Ad Council, whose website says that “reversing the global warming trend is possible.”



This suggests that humans have the power to turn planetary warming into cooling — a scientific absurdity. We have neither the technology, the means, the money, nor the political will to do this. 



Consider the Kyoto Protocol, a “baby step” in the fight against global warming. It “requires” the U.S. to reduce its emissions of carbon dioxide to seven percent below 1990 levels by 2008–2012. Requirements vary by a percent or so for most other signatories such as Canada and the EU nations. Yet if every nation of the world met its Kyoto targets, the amount of warming that would be prevented is .07 degrees Celsius per half‐​century — an amount too small to even measure, as average surface temperatures fluctuate by about twice that much from year to year.



Neither the U.S. nor the EU nor virtually anyone else will be able to fulfill the Kyoto targets. EU emissions rose last year, while U.S. emissions remained unchanged. “Reversing” warming would require reducing carbon‐​dioxide emissions by 60–80 percent, which is simply impossible. The world economy would implode.



Ogilvy and Mather’s corporate website feature a quote from founder David Ogilvy: “We pursue knowledge the way a pig pursues truffles.” But what about knowledge on hurricanes, ice caps, and the real possibilities with respect to global warming?



The best course is one in which we continue to use our economic wherewithal to invest in successful companies, which are generally those that produce things efficiently or produce efficient things. Stating _that_ would be a public service. The ads you’re seeing and hearing are not.
"
"
Share this...FacebookTwitterWith the spate of bogus horror “climate papers” coming out recently, e.g. rapid sea level rise, oceans dying and spreading drought, one has to wonder if they weren’t timed to scare politicians into taking rash action at climate-rescue conferences. If they were, they have failed miserably so far.

EU Environment Council in Luxembourg on Tuesday. (Photo: President of the Council)
Firstly, the international climate talks in Bonn, which were aimed at forging a successor agreement to the Kyoto Protocol, which expires next year, broke down, failed and ended with no result. There won’t be a Kyoto-2 anytime in the foreseeable future. See Kyoto obituaries everywhere and Yvo de Boer: Kyoto is dead.
Yet, Europe still insists on being gung-ho about going it alone in rescuing the climate with its roughly 10% modest share of global emissions – to show the world the way. But perhaps reality is finally beginning to sink in in Europe too. The leftist and warmist Klimaretter here reports that talks yesterday in Luxembourg between the EU’s 27 environment ministers also collapsed and ended in failure. Klimaretter writes:
Uproar among the EU Minister Council in Luxemburg: The 27 ministers have postponed negotiations.
Without energetic efforts, the EU will fall way short of its long-term climate targets. Artur Runge-Metzger, responsible director at the General Climate direction of the EU-Commission, drew up a handout for the environment ministers that hardly could have better illustrated political failure:  instead of an 80% reduction, the production of climate gases will be reduced by only 40%.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Not that this difference would make any climatic difference globally to begin with. To put it in perspective, the EU is responsible for just over 10% of the world’s CO2 emissions. That means the difference between 40% and 80% reduction in Europe would make little climatic difference globally, yet would entail huge costs and sacrifice for European citizens.
Initially a press conference had been scheduled for the early afternoon to present details on the new plan. But by 6 pm there was still no press conference. It was becoming clear that no agreement could be reached. Klimaretter writes:
So there was little wonder that by 6 pm there still had not been a press conference: Hopelessly without agreement, the ministers psotponed the matter. ‘A black day for the leadership of the EU,’ said the British Minister for Energy and Climate Change, Chris Huhne, who put the blame on Poland: Poland refused to grant its approval for a step-by-step plan, and thus caused Runge-Metzger’s preparations to wind up in the dustbin.”
Also read here, http://www.reuters.com/article/2011/06/21/eu-climate-britain-idUSB5E7HK01M20110621 – hat-tip GWPF.
So how does one say “Thank You!” in Polish?
Dzieki Poland!
Share this...FacebookTwitter "
"As cities get bigger and cover more land, the need to make space for wildlife – including insects – in urban areas has become more pressing. Research has shown that cites may not be such a bad place for pollinating insects such as bumble bees, solitary bees and hoverflies. In fact, one UK study of ten cities and two large towns found a greater variety of species in urban areas than in rural areas, while another study showed some UK urban areas hosted stronger bumble bee colonies than those in rural areas.  But other research has found that cities only support the most common pollinators, such as the buff-tailed bumble bee (which is a flexible, generalist forager), and that many species decline in number as urbanisation increases.  One way to help pollinators in urban areas is to provide flowers for them to feed on, in an environment which is otherwise empty of flowering plant life. Flowers have been planted on roadsides around the UK for this very purpose.  Planting more flowers is a great idea – but it is difficult to predict which flowers different insects will use the most, and whether enough flowers are being provided for them. This is why, for our recent study, we teamed up with the National Botanic Garden of Wales to find out what some different bee species think of the wildflower strips planted by Bournemouth Borough Council. Gardeners and councils who want to plant the right flowers to attract bees usually choose them based on how easy they are to plant, and by watching which ones the insects already visit. Instead of doing this, we collected the pollen from bees who were visiting flower patches. Bees were caught and temporally held in a tube before release. The pollen that had fallen or rubbed off of the bee was used for DNA analysis to find out which flowers they had visited.  The technique we used is called DNA meta-barcoding. This allows us to look at a specific part of the plant genome and compare it to a database containing DNA barcodes for numerous British plants, created by the National Botanic Garden of Wales. This technique is relatively new and has previously been used to identify pollen in honey and pollen from the bodies of hoverflies to see which plants they had visited. By collecting the pollen from the bee’s body, we can find out the bee’s foraging history and get samples from places where you cannot follow the bee – like up in the trees or into people’s gardens. And because it is not destructive, there is the potential to collect from an individual more than once. But why use DNA techniques rather than simply looking at the pollen under a microscope? Well, it takes a long time to process and identify pollen grains with a microscope and DNA meta-barcoding can be done in a few days. In addition, accurately identifying pollen is very difficult even for those with a high level of expertise. The identification results from DNA meta-barcoding are also now comparable to or better than traditional pollen identification under a microscope. There are some limitations, however. In particular, DNA meta-barcoding cannot provide a count of each pollen type in a sample, only a relative proportion. Our results show that the bees are indeed using the floral patches put out for them in cities – but these areas alone are not enough. Some of the bees’ favourite flowers in the Bournemouth sample area were purple tansy (Phacelia), chrysanthemums (chrysanthemum), poppies (Papava), cornflowers (Centaurea) and viper’s bugloss (Echium). We also commonly found that they visit garden plants, for example lupins (Lupinus), hydrangeas (Hydrangea), buddleja (Buddleja) and privet (Ligustrum), and wild plants like brambles (Rubus), sow thistles (Sonchus) and wild lettuce (Lactuca). This shows that bees travel around the urban environment to find what they need, and don’t just rely on the small floral strips planted for them. After all, bees need high quality food and variety in their diet to stay healthy, just as humans do.  Our results also showed that different bees like different things depending on their size. For example, small solitary bees are restricted to using more open flowers like daisies, while bumble bees are less restricted because they have long tongues that can reach into deep flowers. So planters need to cater for all tastes if we hope to support bee diversity.  This study only covered a tiny percentage of the UK’s pollinator diversity and there are many other insects such as hoverflies, beetles and butterflies who rely on urban flowers, too. So while the research improves our knowledge on a small number bee species’ flower preferences, there is still lots of work to do in order to make cities friendly for a wide range of pollinators."
"
Share this...FacebookTwitterDr. Hermann Ott, German global warming mobber who not long ago called for a science pogrom aimed at skeptics of dubious global warming science, brings our attention here to a planned conference at the Bundestag in Berlin on June 10, 2011.The conference is designed to attack skepticism in science, and is hosted by the German Green Party faction.
It is dubbed: Strategies of the so-called climate sceptics and who is behind it. Here’s an excerpt of the conference description:
In the USA skepticism has had a long tradition, and with the recent congressional elections it has reached new dimensions. The ‘arguments’ of the climate change skeptics have found fertile ground there, and also in Germany. […] At our conference we wish to shine light on the background of the climate skeptic activities. What are the strategies of the so-called climate sceptics, who is behind them and who finances them?”
Yes, the Greens have to go back to the old worn out pages of its propaganda playbook and rehash all the old drivel about “climate change deniers”, evil industry funding, Fred Singer and Big Oil conspiracies as being behind the skeptic movement.
They’re stuck on stupid. They don’t have the science, so they keep using the old stories.
Unfortunately, Ott, Rahmstorf and the rest of the green supremacists have forgotten that all these questions concerning skepticism in Germany have been already answered, see see here in German and here in English.
Yet in response, many of us have questions of our own for the Greens, and we sent them a list. For example, the Free Democrats of the Friedrich Naumann Institute and other freedom and science-friendly organizations prepared a list of questions for the Greens to answer and sent it to the Green Parliamentarians – way back in November – more than 6 months ago.
Questions to the Greens still unanswered – 6 months and still waiting
The list of questions was even signed by thousands of taxpaying citizens with a copy posted in the Internet. After 6 months – still no sign of a reply. In case the Greens have forgotten, here is the list of questions once again, in short form.
1. Are the Greens aware of the 800 900 peer-reviewed papers that question AGW?
2. Are the Greens aware that climate science is a relatively new science that still entails lots of uncertainty, and that there is no consensus?
3. Will the Greens even send a representative to the 3rd 4th International Conference on Climate and Energy?
4. If the Greens think the question of climate change is already settled, then why spend billions more for financing of climate research work?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




5. Are the Greens aware of any other institution, except for the Pope and their own party, that claims to be infallible?
6. Are the Greens aware that scientific papers questioning man-made climate change were suppressed, and are they aware of Climate-gate, Himalaya-gate, Glacier-gate, Amazon-gate?
7. Are the Greens aware of the scientific achievemnts of Prof. Fred Singer’s distinguished scientific career?
8. Are the Greens aware that Prof. Dr. Judith Curry said: “Man made climate change is a theory and is highly uncertain”, and that distinguished professor Willaim Happer called it: “The dubious science of the climate crusaders”?
9. Is it the Greens’ view that serious, scientific work is only so if it supports the political aspirations of your party?
10. Why is it that no journalist has ever gotten the idea to check up on the scientific reputation of experts that are paid by the Greens?
11. What is the Greens’ position on the fact that your politics accompanies the profligate subsidies to solar and wind energy, bought by donations from, among others, IBC Solar AG, SMA Solar Technology AG, Ostwind, Umweltkontor Renewable Energy, EWO Energietechnologie GmbH, Conergy AG, Pro Vento, Nordex AG, Windpark G. W. Meerhof GmbH & Co. KG, Ersol AGder Windpark GmbH & Co. KG, Wind Project Development GmbH, Solarworld AG, SMA Technologie AG, Solon AG fir Solar Technology, AGU Energy and Electrotechnology GmbH?
12. Which renewable energy industries and to what extent have Parliamentarians of the Green Faction invested?
13. Do the Greens intend to continue their constant use of the expression “climate denier”?
14. Will the Greens continue to use public money to encourage and incite others to commit criminal acts such as vandalizing railways?
15. Do the Greens call protests at nuclear waste storage facilities only when they are in the opposition, and do the opposite when they are not?
16. Are you Greens aware of the point you’ve reached today, when you are asked such questions?
How about answers to our questions, Mr Ott? With us as taxpayers and you as a Parliamentarian, it is only fitting that you provide a reply. Why are you and your faction, along with Drs Rahmstorf and Schellnhuber, always hiding from open debate? Why do you and the scientists at PIK insist in remaining mired in the narrow-mindedness of dogmatism?
What are you afraid of?
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI got the following as a reader comment. It’s a statement that appears to be direct from Janez Potočnik’s office. He is a European Commissioner for Environment and he refutes the story that they plan to mandate water saving measures in Europe.  That’s a relief – saved from another stupid idea, at least for now (Don’t worry though – they’ve got plenty of others, for sure).
 The statement was sent by a fellow named Joe Hennon.
——————————————————–
Statement by Janez Potočnik, European Commissioner for Environment
I am aware of allegations that appeared in the German press concerning future plans to restrict water use in the Member States and to impose regulations on landlords and households. These allegations are unfounded.
I wish to make it very clear that the Commission has no plans at present to make water-saving taps mandatory in any Member State, no plans to oblige Member States to reduce household water consumption, and no targets have been set in these areas.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The main aim of EU water policy has always been to ensure that good quality water is available throughout the EU in sufficient quantities. The Commission is committed to addressing water scarcity as part of a Blueprint for Water in 2012 and, as always, a number of studies are ongoing in this area.
Several policy options are being examined regarding the technical, environmental and economic feasibility of regulatory and non-regulatory options for water performance requirements for buildings, but no new decisions have been taken in this area, nor are they indeed pending.
The European law-making process is a highly consultative one. When legislation is proposed, proposals are accompanied by studies assessing the potential environmental, social and economic effects. Any targets or legislation in the area of water use would be accompanied by the appropriate impact assessment as a matter of course, taking account of the extent to which the situation in Europe is diverse, with a view to avoiding a “one size fits all” approach.
I regret that articles on sensitive questions such as Commission proposals for water use in Member States appear in the media before any details have been checked with the relevant staff at the European Commission
Joe Hennon
ec.europa.eu/commission_2010-2014/potocnik/index_…
joseph.hennon@ec.europa.eu
————————————————————-
(No mention if Joe Hennon is an aide to Potočnik, or what his function is.)
Share this...FacebookTwitter "
"

Today it is not unusual to hear it suggested that the undeveloped world’s best hope lies in private property, the market economy, and the rule of law. But a short time ago, that suggestion would have scandalized many audiences. Peter Bauer is a major reason for that shift.



Lord Bauer, the son of a Budapest bookmaker, came to Britain in 1934 to study economics at Gonville and Caius College, Cambridge, where he later became a fellow. His pioneering work in development economics, which began with his study of the Southeast Asian rubber industry in the 1940s and his classic 1954 book, West African Trade, led him to question, and later overturn, many of the beliefs held by mainstream development experts. This work was carried out primarily from the London School of Economics and Political Science, where he taught from 1960 to 1983 and where he is currently emeritus professor of economics. In 1982, he was made a life peer and is a fellow of the British Academy.



Bauer’s work is characterized by careful observation of how countries move from subsistence to exchange economies, an application of simple economic principles, and a sound understanding of the role of non‐​economic variables in promoting material advance. As he noted in his book Dissent on Development, “Economic achievement depends primarily on people’s abilities and attitudes and also on their social and political institutions. Differences in these determinants or factors largely explain differences in levels of economic achievement and rates of material progress.”



What Bauer observed was that people in poor countries respond to price incentives just like people in rich countries. He also observed that when people have the freedom to own property and to trade, and when government is limited to the protection of those rights, they have a better chance of achieving prosperity.



The intellectual climate in the late 1950s was not hospitable to Bauer’s critique of state‐​led development policy. In 1956, Swedish economist Gunnar Myrdal, later a Nobel laureate, wrote, “The special advisers to underdeveloped countries who have taken the time and trouble to acquaint themselves with the problem … all recommend central planning as the first condition of progress.” That view persisted well into the 1960s and has only recently been supplanted by a more market‐​friendly view. It was not until after the collapse of communism in Eastern Europe and the Soviet Union that the World Bank admitted, in its 1997 development report, “State‐​led intervention emphasized market failures and accorded the state a central role in correcting them. But the institutional assumptions implicit in this world view were, as we all realize today, too simplistic.”



Bauer recognized, as noted in his book Reality and Rhetoric, that “the critics who propose replacing the market system by political decisions rarely address themselves to such crucial matters as the concentration of economic power in political hands, the implications of restriction of choice, the objectives of politicians and administrators, and the quality and extent of knowledge in a society and its methods of transmission.”



In observing economic reality and adhering to the logic of the price system, Bauer refuted key propositions of orthodox development economics, the most basic one being the idea of a “vicious circle of poverty.” Poor countries were said to be poor because people had low incomes and could not generate sufficient savings to allow for capital accumulation, one of the prerequisites for economic growth, as spelled out in mainstream growth models. Bauer observed that many people and many countries had moved from poverty to prosperity and that large‐​scale capital investment is neither necessary nor sufficient for material advance. His study of small holdings in the Malaya (now Malaysia) rubber industry and his observation of the importance of small‐​scale traders in West Africa convinced him that the reality of development was different from the rhetoric of development experts.



A corollary of the vicious circle is that poor countries cannot become rich without external aid from developed countries. However, the nations that have become rich had no access to foreign aid, while those that have received substantial external aid are for the most part still poor, as in Africa. So Bauer argued that foreign aid is more likely to perpetuate poverty than to alleviate it. And history has borne him out.



Bauer also strongly disagreed with the widely held view that population growth is a drag on development. In his essay “Population Growth: Disaster or Blessing?” he wrote, “Economic achievement and progress depend on people’s conduct not on their numbers.” Unlike many of the development experts who wanted to use government to “help the poor,” Bauer thought that poor people could lift themselves out of poverty through their own efforts, if only governments would safeguard both economic and personal freedom. When people are free to choose and bear the responsibility for their choices, as they do under a system of private property and free markets, they will be more able to improve themselves and provide for their families‐​as well as have stronger incentives to do so‐​than when they are dependent primarily on the state.



Bauer was one of the first economists to clearly see that state‐​led development policies and the quest for “social justice” would politicize economic life, impair individual freedom, and fail to achieve long‐​run prosperity for the majority of people. He also noted that those countries that had the fewest commercial contacts with the West were the least developed. Thus, he recognized the dynamic gains from free trade. In his most recent book, From Subsistence to Exchange and Other Essays, he wrote, “Contacts through traders and trade are prime agents in the spread of new ideas, modes of behavior, and methods of production. External commercial contacts often first suggest the very possibility of change, including economic improvement.” Certainly the experience of people in Japan, South Korea, Taiwan, China, and Hong Kong support that observation.



Bauer’s emphasis on individual merit, character, culture, property rights, and markets, and his distrust of big government, foreign aid, and the welfare state place him squarely in the classical‐​liberal tradition. His life’s work has been in the broad context of political economy, not in the narrow technical confines of modern development economics or the even narrower space of formal economic modeling. 



Bauer’s keen understanding of how individuals and nations grow rich comes from practical experience combined with plain economic theory and a deep knowledge of history. His work has stood the test of time. That is why he is now widely recognized as a hero of the revolution in development economics.
"
"
Smearing around data or paint - the results are similar
Jeff Id of The Air Vent emailed me today inviting me to repost Ryan O’s latest work on statistical evaluation of the Steig et al “Antarctica is warming” paper ( Nature, Jan 22, 2009) I thought long and hard about the title, especially after reviewing the previous work from Ryan O we posted on WUWT where the paper was dealt a serious blow to “robustness”. After reading this latest statistical analysis, I think it is fair to conclude that the paper’s premise has been falsified.
Ryan O, in his conclusion, is a bit more gracious:
I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
Not only that, Ryan O did a more complete job of the reconstruction than Steig et al did, he mentions this in comments at The Air Vent:
Steig only used 42 stations to perform his reconstruction.  I used 98, since I included AWS stations.
The AWS stations have their problems, such as periods of warmer temperatures due to being buried in snow, but even when using this data, Ryan O’s analysis still comes out with less warming than the original Steig et al paper
Antarctica as a whole is not warming, the Antarctic peninsula is, which is signficantly removed climatically from the main continent.
Click for a larger image
It is my view that all Steig and Michael Mann have done with their application of RegEm to the station data is to smear the temperature around much like an artist would smear red and white paint on a pallete board to get a new color “pink” and then paint the entire continent with it.
It is a lot like “spin art” you see at the county fair. For example, look (at left) at the different tiles of colored temperature results for Antarctica you can get using Steig’s and Mann’s methodology. The only thing that changes are the starting parameters, the data remains the same, while the RegEm program smears it around based on those starting parameters. In the Steig et al case, PC and regpar were chosen by the authors to be a value of 3. Chosing any different numbers yields an entirely different result.
So the premise of the Steig et al paper paper boils down to an arbitrary choice of values that “looked good”.
I hope that Ryan O will write a rebuttal letter to Nature, and/or publish a paper. It is the only way the Team will back down on this. – Anthony
UPDATE: To further clarify, Ryan O writes in comments:
“Overall, Antarctica has warmed from 1957-2006. There is no debating that point. (However, other than the Peninsula, the warming is not statistically significant. ) 
The important difference is the location of the warming and the magnitude of the warming. Steig’s paper has the warming concentrated on the Ross Ice Shelf – which would lead you to entirely different conclusions than having a minimum on the ice shelf. As far as magnitude goes, the warming for the continent is half of what was reported by Steig (0.12 vs. 0.06 Deg C/Decade).
Additionally, Steig shows whole-continent warming from 1967-2006; this analysis shows that most of the continent has cooled from 1967-2006. Given that the 1940’s were significantly warmer in the Antarctic than 1957 (the 1957-1960 period was unusually cold in the Antarctic), focusing on 1957 can give a somewhat slanted picture of the temperature trends in the continent.”
Ryan O  adds later:  “I should have said that all reconstructions yield a positive trend, though in most cases the trend for the continent is not statistically significant.” 

Verification of the Improved High PC Reconstruction
Posted by Jeff Id on May 28, 2009
There is always something going on around here.
Up until now all the work which has been done on the antarctic reconstruction has been done without statistical verification. We believed that they are better from correlation vs distance plots, the visual comparison to station trends and of course the better approximation of simple area weighted reconstructions using surface station data.
The authors of Steig et al. have not been queried by myself or anyone else that I’m aware of regarding the quality of the higher PC reconstructions. And the team has largely ignored what has been going on over on the Air Vent. This post however demonstrates strongly improved verification statistics which should send chills down their collective backs. 
Ryan was generous in giving credit to others with his wording, he has put together this amazing piece of work himself using bits of code and knowledge gained from the numerous other posts by himself and others on the subject. He’s done a top notch job again, through a Herculean effort in code and debugging.
If you didn’t read Ryan’s other post which led to this work the link is:
Antarctic Coup de Grace
——————————————————————————–

Fig. 1: 1957-2006 trends; our reconstruction (left); Steig reconstruction (right)


HOW DO WE CHOOSE?


In order to choose which version of Antarctica is more likely to represent the real 50-year history, we need to calculate statistics with which to compare the reconstructions. For this post, we will examine r, r^2, R^2, RE, and CE for various conditions, including an analysis of the accuracy of the RegEM imputation. While Steig’s paper did provide verification statistics against the satellite data, the only verification statistics that related to ground data were provided by the restricted 15-predictor reconstruction, where the withheld ground stations were the verification target. We will perform a more comprehensive analysis of performance with respect to both RegEM and the ground data. Additionally, we will compare how our reconstruction performs against Steig’s reconstruction using the same methods used by Steig in his paper, along with a few more comprehensive tests.
To calculate what I would consider a healthy battery of verification statistics, we need to perform several reconstructions. The reason for this is to evaluate how well the method reproduces known data. Unless we know how well we can reproduce things we know, we cannot determine how likely the method is to estimate things we do not know. This requires that we perform a set of reconstructions by withholding certain information. The reconstructions we will perform are:
1. A 13-PC reconstruction using all manned and AWS stations, with ocean stations and Adelaide excluded. This is the main reconstruction.
2. An early calibration reconstruction using AVHRR data from 1982-1994.5. This will allow us to assess how well the method reproduces the withheld AVHRR data.
3. A late calibration reconstruction using AVHRR data from 1994.5-2006. Coupled with the early calibration, this provides comprehensive coverage of the entire satellite period.
4. A 13-PC reconstruction with the AWS stations withheld. The purpose of this reconstruction is to use the AWS stations as a verification target (i.e., see how well the reconstruction estimates the AWS data, and then compare the estimation against the real AWS data).
5. The same set of four reconstructions as above, but using 21 PCs in order to assess the stability of the reconstruction to included PCs.
6. A 3-PC reconstruction using Steig’s station complement to demonstrate replication of his process.
7. A 3-PC reconstruction using the 13-PC reconstruction model frame as input to demonstrate the inability of Steig’s process to properly resolve the geographical locations of the trends and trend magnitudes.
–
Using the above set of reconstructions, we will then calculate the following sets of verification statistics:
–
1. Performance vs. the AVHRR data (early and late calibration reconstructions)
2. Performance vs. the AVHRR data (full reconstruction model frame)
3. Comparison of the spliced and model reconstruction vs. the actual ground station data.
4. Comparison of the restricted (AWS data withheld) reconstruction vs. the actual AWS data.
5. Comparison of the RegEM imputation model frame for the ground stations vs. the actual ground station data.
–
The provided script performs all of the required reconstructions and makes all of the required verification calculations. I will not present them all here (because there are a lot of them). I will present the ones that I feel are the most telling and important. In fact, I have not yet plotted all the different results myself. So for those of you with R, there are plenty of things to plot.
Without further ado, let’s take a look at a few of those things.
Fig. 2: Split reconstruction verification for Steig reconstruction
You may remember the figure above; it represents the split reconstruction verification statistics for Steig’s reconstruction. Note the significant regions of negative CE values (which indicate that a simple average of observed temperatures explains more variance than the reconstruction). Of particular note, the region where Steig reports the highest trend – West Antarctica and the Ross Ice Shelf – shows the worst performance.
Let’s compare to our reconstruction:
Fig. 3: Split reconstruction verification for 13-PC reconstruction

There still are a few areas of negative RE (too small to see in this panel) and some areas of negative CE. However, unlike the Steig reconstruction, ours performs well in most of West Antarctica, the Peninsula, and the Ross Ice Shelf. All values are significantly higher than the Steig reconstruction, and we show much smaller regions with negative values.
As an aside, the r^2 plots are not corrected by the Monte Carlo analysis yet. However, as shown in the previous post concerning Steig’s verification statistics, the maximum r^2 values using AR(8) noise were only 0.019, which produces an indistinguishable change from Fig. 3.
Now that we know that our method provides a more faithful reproduction of the satellite data, it is time to see how faithfully our method reproduces the ground data. A simple way to compare ours against Steig’s is to look at scatterplots of reconstructed anomalies vs. ground station anomalies:
Your browser may not support display of this image.
Fig. 4: 13-PC scatterplot (left); Steig reconstruction (right)

The 13-PC reconstruction shows significantly improved performance in predicting ground temperatures as compared to the Steig reconstruction. This improved performance is also reflected in plots of correlation coefficient:
Fig. 5: Correlation coefficient by geographical location
As noted earlier, the performance in the Peninsula , West Antarctica, and the Ross Ice Shelf are noticeably better for our reconstruction. Examining the plots this way provides a good indication of the geographical performance of the two reconstructions. Another way to look at this – one that allows a bit more precision – is to plot the results as bar plots, sorted by location:
Fig. 6: Correlation coefficients for the 13-PC reconstruction

Fig. 7: Correlation coefficients for the Steig reconstruction

The difference is quite striking.
While a good performance with respect to correlation is nice, this alone does not mean we have a “good” reconstruction. One common problem is over-fitting during the calibration period (where the calibration period is defined as the periods over which actual data is present). This leads to fantastic verification statistics during calibration, but results in poor performance outside of that period.
This is the purpose of the restricted reconstruction, where we withhold all AWS data. We then compare the reconstruction values against the actual AWS data. If our method resulted in overfitting (or is simply a poor method), our verification performance will be correspondingly poor.
Since Steig did not use AWS stations for performing his TIR reconstruction, this allows us to do an apples-to-apples comparison between the two methods. We can use the AWS stations as a verification target for both reconstructions. We can then compare which reconstruction results in better performance from the standpoint of being able to predict the actual AWS data. This is nice because it prevents us from later being accused of holding the reconstructions to different standards.
Note that since all of the AWS data was withheld, RE is undefined. RE uses the calibration period mean, and there is no calibration period for the AWS stations because we did the reconstruction without including any AWS data. We could run a split test like we did with the satellite data, but that would require additional calculations and is an easier test to pass regardless. Besides, the reason we have to run a split test with the satellite data is that we cannot withhold all of the satellite data and still be able to do the reconstruction. With the AWS stations, however, we are not subject to the same restriction.
Fig. 8: Correlation coefficient, verification period, AWS stations withheld

With that, I think we can safely put to bed the possibility that our calibration performance was due to overfitting. The verification performance is quite good, with the exception of one station in West Antarctica (Siple). Some of you may be curious about Siple, so I decided to plot both the original data and the reconstructed data. The problem with Siple is clearly the short record length and strange temperature swings (in excess of 10 degrees), which may indicate problems with the measurements:
Fig. 9: Siple station data

While we should still be curious about Siple, we also would not be unjustified in considering it an outlier given the performance of our reconstruction at the remainder of the station locations.
Leaving Siple for the moment, let’s take a look at how Steig’s reconstruction performs.
Fig. 10: Correlation coefficient, verification period, AWS stations withheld, Steig reconstruction
Not too bad – but not as good as ours. Curiously, Siple does not look like an outlier in Steig’s reconstruction. In its place, however, seems to be the entire Peninsula. Overall, the correlation coefficients for the Steig reconstruction are poorer than ours. This allows us to conclude that our reconstruction more accurately calculated the temperature in the locations where we withheld real data.
Along with correlation coefficient, the other statistic we need to look at is CE. Of the three statistics used by Steig – r, RE, and CE – CE is the most difficult statistic to pass. This is another reason why we are not concerned about lack of RE in this case: RE is an easier test to pass.
Fig. 11: CE, verification period, AWS stations withheld
Your browser may not support display of this image.
Fig. 12: CE, verification period, AWS stations withheld, Steig reconstruction
The difference in performance between the two reconstructions is more apparent in the CE statistic. Steig’s reconstruction demonstrates negligible skill in the Peninsula, while our skill in the Peninsula is much higher. With the exception of Siple, our West Antarctic stations perform comparably. For the rest of the continent, our CE statistics are significantly higher than Steig’s – and we have no negative CE values.
So in a test of which method best reproduces withheld ground station data, our reconstruction shows significantly more skill than Steig’s.
The final set of statistics we will look at is the performance of RegEM. This is important because it will show us how faithful RegEM was to the original data. Steig did not perform any verification similar to this because PTTLS does not return the model frame. Unlike PTTLS, however, our version of RegEM (IPCA) does return the model frame. Since the model frame is accessible, it is incumbent upon us to look at it.
Note:    In order to have a comparison, we will run a Steig-type reconstruction using RegEM IPCA.
There are two key statistics for this: r and R^2. R^2 is called “average explained variance”. It is a similar statistic to RE and CE with the difference being that the original data comes from the calibration period instead of the verification period. In the case of RegEM, all of the original data is technically “calibration period”, which is why we do not calculate RE and CE. Those are verification period statistics.
Let’s look at how RegEM IPCA performed for our reconstruction vs. Steig’s.
Fig. 13: Correlation coefficient between RegEM model frame and actual ground data

As you can see, RegEM performed quite faithfully with respect to the original data. This is a double-edged sword; if RegEM performs too faithfully, you end up with overfitting problems. However, we already checked for overfitting using our restricted reconstruction (with the AWS stations as the verification target).
While we had used regpar settings of 9 (main reconstruction) and 6 (restricted reconstruction), Steig only used a regpar setting of 3. This leads us to question whether that setting was sufficient for RegEM to be able to faithfully represent the original data. The only way to tell is to look, and the next frame shows us that Steig’s performance was significantly less than ours.
Fig. 14: Correlation coefficient between RegEM model frame and actual ground data, Steig reconstruction
The performance using a regpar setting of 3 is noticeably worse, especially in East Antarctica. This would indicate that a setting of 3 does not provide enough degrees of freedom for the imputation to accurately represent the existing data. And if the imputation cannot accurately represent the existing data, then its representation of missing data is correspondingly suspect.
Another point I would like to note is the heavy weighting of Peninsula and open-ocean stations. Steig’s reconstruction relied on a total of 5 stations in West Antarctica, 4 of which are located on the eastern and southern edges of the continent at the Ross Ice Shelf. The resolution of West Antarctic trends based on the ground stations alone is rather poor.
Now that we’ve looked at correlation coefficients, let’s look at a more stringent statistic: average explained variance, or R^2.
Fig. 15: R^2 between RegEM model frame and actual ground data
Using a regpar setting of 9 also provides good R^2 statistics. The Peninsula is still a bit wanting. I checked the R^2 for the 21-PC reconstruction and the numbers were nearly identical. Without increasing the regpar setting and running the risk of overfitting, this seems to be about the limit of the imputation accuracy.
Fig. 16: R^2 between RegEM model frame and actual ground data, Steig reconstruction

Steig’s reconstruction, on the other hand, shows some fairly low values for R^2. The Peninsula is an odd mix of high and low values, West Antarctica and Ross are middling, while East Antarctica is poor overall. This fits with the qualitative observation that the Steig method seemed to spread the Peninsula warming all over the continent, including into East Antarctica – which by most other accounts is cooling slightly, not warming.
CONCLUSION
With the exception of the RegEM verification, all of the verification statistics listed above were performed exactly (split reconstruction) or analogously (restricted 15 predictor reconstruction) by Steig in the Nature paper. In all cases, our reconstruction shows significantly more skill than the Steig reconstruction. So if these are the metrics by which we are to judge this type of reconstruction, ours is objectively superior.
As before, I would qualify this by saying that not all of the errors and uncertainties have been quantified yet, so I’m not comfortable putting a ton of stock into any of these reconstructions. However, I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
NOTE ON THE SCRIPT
If you want to duplicate all of the figures above, I would recommend letting the entire script run. Be patient; it takes about 20 minutes. While this may seem long, remember that it is performing 11 different reconstructions and calculating a metric butt-ton of verification statistics.
There is a plotting section at the end that has examples of all of the above plots (to make it easier for you to understand how the custom plotting functions work) and it also contains indices and explanations for the reconstructions, variables, and statistics. As always, though, if you have any questions or find a feature that doesn’t work, let me know and I’ll do my best to help.
Lastly, once you get comfortable with the script, you can probably avoid running all the reconstructions. They take up a lot of memory, and if you let all of them run, you’ll have enough room for maybe 2 or 3 more before R refuses to comply. So if you want to play around with the different RegEM variants, numbers of included PCs, and regpar settings, I would recommend getting comfortable with the script and then loading up just the functions. That will give you plenty of memory for 15 or so reconstructions.
As a bonus, I included the reconstruction that takes the output of our reconstruction, uses it for input to the Steig method, and spits out this result:
Fig. 17: Steig reconstruction using the 13-PC reconstruction as input.

The name for the list containing all the information and trends is “r.3.test”.
—————————————————————-
Code is here Recon.R


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95dca824',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Vice President Gore continues to assault Gov. George W. Bush’s $1.5 trillion tax cut plan on the grounds that it would “spend all the budget surplus.” But Gore’s own federal spending promises are more costly than Bush’s tax cut, by a long shot.



Gore’s campaign proposals — for universal federal preschool funding, drug benefits to seniors, the Kyoto global warming treaty, anti‐​smoking programs, expanded Medicaid health coverage, and on ad infinitum — would add $1.6 trillion to the federal budget over the next 10 years, and that price tag could double in the following decade. Bush is right: No Democratic presidential candidate in the last 30 years — neither Dukakis nor Mondale nor McGovern — put forward such a high‐​priced menu of new federal initiatives. In fact, the cost of Gore’s spending schemes exceeds the proposals of Ralph Nader’s Green Party.



I have scoured through all of the spending proposals presented on the Gore 2000 web site and in the latest Clinton‐​Gore budget proposal presented to Congress. I have added to that the taxpayers’ tab for all the special‐​interest campaign promises Gore has made over the past several months. The biggest‐​ticket items are new entitlement programs. For example, Gore’s gold‐​plated prescription drug benefit program for seniors would cost $432 billion. His “Retirement Savings Plus” plan would dole out another $200 billion in tax dollars to low‐​income workers — many of whom cannot afford to save on their own because of the 12 percent Social Security tax.



Expanding government health coverage to uninsured families would, conservatively estimated, cost $146 billion. His plan to provide free or subsidized preschool for three and 4 year‐​olds carries a $115 billion price.



Gore’s blueprint also envisions beefing up the budgets of most of the federal regulatory agencies, including OSHA, the EPA, and the civil rights and antitrust snoops at the Justice Department. He wants $16 billion for teacher pay raises, a $200 million anti‐​smoking initiative, $45 million for curtailing violence at abortion clinics, $2 billion to combat suburban sprawl, several hundred million to develop solar energy and other alternatives to fossil fuels, $2 billion for a “livable cities” plan, at least $1 billion more for researching global climate change, and the ultimate in political correctness: a new Labor Department program to “train women for high‐​tech jobs” (no price tag listed).



The precise total comes to $1.64 trillion in new spending through 2010 — or almost $15,000 for every household in America. What is even more astonishing is that Gore has suggested virtually no offsetting budget cuts. All this new spending would be paid for by squandering the expected tax surpluses. Out of the several thousand federal programs in the 1,600-page federal budget, Al Gore, the man who invented reinventing government, hasn’t yet identified a single one in his presidential campaign that should be terminated. Regrettably, neither has Bush.



In the presidential debates and on the campaign trail, Gore has cultivated a fiscally moderate image. But the truth is that from the moment he first entered Congress more than 20 years ago, Gore has been a relentless advocate of nanny‐​state government expansionism. In 1989 and 1990, Gore won the National Taxpayers Union award for the biggest spender on Capitol Hill, on both occasions nudging out Ted Kennedy for this dubious honor. In 11 of 13 years, Gore received the lowest possible NTU grade on taxpayer issues.



If enacted, Gore’s new generation of federal welfare state entitlement programs would be ticking fiscal time bombs that will explode over the next decade, just when Baby Boomers are set to retire. Gore’s audacious $1.5 trillion agenda to nationalize day care, health care, education, crime fighting, transportation policy, health care, zoning and traffic patterns are brilliantly softened with conservative rhetoric about advancing “fiscal responsibility.”



Gore is not so much a man who wants to reinvent government as he is a man who wants to relegitimize it and expand it as much as possible. What is truly unseemly is how he pursues that objective by mendaciously employing the language of fiscal restraint. His proposed blitz of new spending is more expensive than any other presidential candidate has sought since Lyndon Johnson unveiled the Great Society. And just when we thought the era of big government was over.
"
"
Leif Svalgaard writes in with a collection of points on the 10.7 cm solar radio flux. Being busy tonight, I’m happy to oblige posting them. – Anthony
Leif writes:
People often call out that F10.7 flux has now reached a new low, and that a  Grand minimum is imminent.
Perhaps this graph would calm nerves a bit:

The  blue curve is the current F10.7 flux [adjusted to 1 AU, of course] and the  red curve is F10.7 back at the 1954 minimum. The D spike (in 1954) was due to an old cycle [18] region.
There is always the problem of  how to align two such curves.. These two were aligned by eye to convey the  general nature of the flux over a minimum. The peaks labeled B and C and the  low part A were arbitrarily aligned, because peaks often influence the flux  for several weeks so would form natural points of correspondence. The  detailed similarity is, of course, of no significance. Note, however that  because of the 27-day recurrence one some peaks are aligned others will be  too. again, this has no further [deeper] significance. The next solar cycle  is predicted to be quite low and the cycle following the 1954 minimum  was one of the largest recorded. We will, of course, with excitement  watch how the blue curve will fare over the next year or so, to see how  the ‘ramp up’ will compare to the steep ramp up in 1955-1956.
Of course, as there was more activity before and after the minimum and even  during [as cycles overlap]. For the very year of the minimum apart from the  spike at D there is very little difference. The important issue [for me] is  the absolute level, because that is a measure of the density and temperature  of the lower corona, generated by the ‘network’ or background magnetic field,  which seems very constant from minimum to minimum, and certainly does not  portend an imminent Grand Minimum, which is not to say that such could not  come, just that a low F10.7 is not an indicator for it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94ef3f1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSince the “dramatic monster killer” tornadoes left a trail “of death and destruction” in the USA, the German media have once again began to blame abnormal human behaviour and SUV-witchcraft for the “man-made” disasters.
One example is here at the German leftist Rote Fahne News site. News is what they call it – others might term it rants and ravings of lunatics. Hat-tip DirkH. Note that this kind of reporting is not exclusive to kook sites like Rote Fahne, but is everywhere in the mainstream media. It really is so (Yesterday I heard the dubious claim on NDR public radio).
Here’s how the Rote Fahne explains the tornadoes and the US reaction to them.
That such conditions are becoming increasingly favourable because of global warming due to the burning of fossil fuels like oil, gas, and coal is being kept a secret. Huge amounts of carbon dioxide are being released. That acts ‘like a greenhouse glass roof over the planet. That leads to the greenhouse effect and takes the climate out of whack,’ says the climate program of the MLPD. US President Obama now expresses his deep sadness because of the victims and vast damage for citizens – but at the same time the destruction is being blamed on the natural environment!”
The nerve! Someone claiming the tornadoes are natural!
This of course reminds us of the 17th century witch hunts and trials held in both Europe and early colonial America. Superstitious people, claiming the authority of the latest science, claimed natural disasters and their ensuing misery, along with their own neurotic behavior, were caused by witches. Witch hunts then quickly became a method of eliminating opponents. Today, the climate science madness expressed in the text of Rote Fahne is precisely the same. Compare today’s global warming movement to the old witch trial times in the following Youtube video:
http://www.youtube.com/watch?v=NuSwx3d0R7w&feature=related


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Aren’t the parallels absolutely striking? There’s nary a difference.
Today’s version of what is driving the new witch hunts is only slightly modified (see the 0:45 mark of the above video). Today it could read as follows: “What is written in the Climate Bible is the Word of Gore; it is infallible and we have to live by it on a daily basis. And when you read the book of CO2, it is written: ‘Thou shalt not suffer a skeptic to live’.”
The paranoia and madness of 1692 Salem, thanks to modern media, has re-emerged and gone global today. It’s the same stuff. Today people like Hansen, Gore, and Schellnhuber are acting as the new Pope Gregory the 9ths or Innocent the 8ths.
Today they claim people are behaving “abnormally” and so terrible things are happening because of it. The natural disasters are all the proof they need. Today it is capitalist witches at work. How do we know? Rote Fahne delivers the proof, and writes:
The environmental crisis has developed concurrently as a by-product of capitalism by law.”
For them, that’s enough proof. Prepare the gallows! The author of this leftist excrement obviously has forgotten the environmental cesspool that communism and socialism produced in the former regimes of East Germany and the Soviet Union. Thanks to shutting down these inefficient systems, united Germany’s CO2 emissions dropped 15% overnight.
Make no mistake about it – eventually history will look back and view these “infallible scientists” as Pope Gregory the 9ths or Innocent the 8th-type crackpots and zealots who just lost their way in the darkness of their own ignorance and arrogance.
Share this...FacebookTwitter "
"**Spain's King Felipe VI has begun ten days of quarantine after coming into contact with a person who tested positive for coronavirus.**
Palace sources say the king, 52, was in ""close contact"" with the individual on Sunday, but gave no further details.
The monarch's wife and the couple's two daughters will continue their activities as normal.
Spain has recorded nearly 1.6 million cases and 43,131 deaths since the pandemic began.
Last week, the World Health Organization warned that Europe, which is once again at the centre of the pandemic, faced a ""tough"" six months ahead.
However, recent results from a number of vaccine trials have given hope and on Tuesday the Spanish government is due to meet to discuss plans to vaccinate the population.
Last week, Prime Minister Pedro SÃ¡nchez said the country hoped to offer the vaccine to ""a very substantial part"" of its population within the first half of 2021."
"
Share this...FacebookTwitterIt’s over. All that is left to die is that last flicker of European hope.Another European politician has long since seen the bold-letter writing on the wall. The following was written by European Parliamentarian Holger Krahmer and appears in German at the Achse des Guten here. What follows is an English translation (emphasis added).
=============================
Only Europe Still Believes In Rescuing The Climate
by Holger Krahmer (MdEP)
The climate policy of the European Union is now stuck for good in a dead end. Europe wanted to be the leader – showing the world the way. They especially wanted to export the “market-economic” instrument of emissions trading as a new standard of regulation. The climate summits in Copenhagen and in Cancun were supposed to herald in a successor treaty for the 1997 Kyoto Protocol, which expires in 2012. Both summits yielded zero results. Today it is clear: There is going to be no successor agreement. Also the option of simply extending the existing Kyoto Protocol was thrown overboard by the main countries at the last G8 summit.
The situation in global climate politics can be summarized in short: There isn’t any.
Especially the emerging economies of Asia are refusing to allow their possibilities for growth to be curbed by obligatory CO2 reductions. Everywhere globally, climate laws are being buried for good or put on ice. Especially the once ballyhooed instrument of emissions trading is obsolete. China, India and Australia are waving goodbye. In the USA the Chicago Climate Exchange was closed just after the last midterm Congressional elections. Just before that, the self-anointed climate pope Al Gore cashed in by selling his shares. “Climate politics is a dead project“ is the word in Washington today. Yet, the EU is still clinging to all measures and is even discussing making them even stricter. As a result, we are now left alone with the political costs of CO2 reduction. We are ignoring international reality with an amazing level of tenacity.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As always, we continue to stick to the naive, worn-out argument: “Someone has to start the process.“ That start is a go-it-alone! And if we do not wake up to that, then we will ruin our market economy with one-sided massive costs.
In the end, that will do nothing for the climate. No matter what CO2 may do in the atmosphere, every reduction in European emissions will be offset very quickly by other emissions someplace else in the world. In addition to the political realities, questioning the scientific basis of climate change put out by the IPCC is being increasingly equated as a sort of blasphemy.
Now that the IPCC is completely discredited by scandals and political influence, the current CO2 hypotheses need to be re-evaluated. That climate change is complex was always known. That it is mainly driven by a trace gas in the atmosphere is unlikely when one soberly examines it closely with an open scientific mind and when one examines the long-term history of the climate.
Whoever has the power over the religion, also has the power over the people. That has always been the case for religions. Today the belief in manmade global warming appears to have become a sort of substitute religion.
=====================================================
Readers, please write a few words of support to Mr Krahmer!
holger.krahmer@europarl.europa.eu
http://www.holger-krahmer.de/
Share this...FacebookTwitter "
"Africa’s elephant population has plummeted from roughly a million in 1970 to around 400,000 today – a decline which is largely blamed on poaching for their ivory tusks. At its peak in 2011, poaching claimed 36,000 elephants a year, or one every 15 minutes.  Many of us are familiar with these statistics thanks to campaigns to end the ivory trade. But with our attention focused on poaching, an arguably greater threat to Africa’s elephants has emerged. In the time that Africa’s elephant population has crashed, its human population has boomed. The number of people living in Africa has doubled since 1982, reaching a billion in 2009, and is expected to double again by 2050.  To feed and house this growing population, natural habitats have been fragmented by roads and railways and entire swathes have been converted to farmland and settlements. As a result, Africa’s elephants have been squeezed into smaller and increasingly isolated pockets of land. It’s very possible that the future for all of Africa’s elephants will resemble what is currently seen in South Africa. Here, elephants are largely confined to small, fenced reserves separated by vast human-dominated landscapes. Elephants can’t disperse from these reserves, but their relative protection within them has seen their densities increase. So much so that in stark contrast to the “elephant extinction” narrative we’re used to, some consider South Africa’s reserves to have “too many elephants”. Elephants play a crucial role in Africa’s savanna ecosystems as seed dispersers. Their dung recycles valuable nutrients and by feeding on trees they maintain the savanna’s matrix of woodland and grassland and the biodiversity it supports.  However, over prolonged periods, high elephant densities can reduce tree cover, which shrinks woodland and expands grassland habitats. This may threaten browsing species, such as black rhinoceros and bushbuck, for which trees provide food and shelter. Managing elephants to prevent habitat change and preserve biodiversity has a long history. Culling programmes continued into the late 20th century and only ended in Kruger National Park in 1994. Culling remains a “last resort” for managing elephants in South Africa and there have been recent calls to resume culling operations in Botswana. Culls have now largely been replaced by non-lethal approaches, including translocating elephants to other areas and contraceptives to reduce birth rates.  However, all management interventions cause some degree of stress for elephants. There’s always a small risk with using anaesthetics and hormonal contraceptives can alter an elephant’s behaviour.  The fundamental question over the future of Africa’s elephants is whether we are happy to allow them to exist only where they are heavily managed. If so, then we’ll need more research to understand the most effective and ethical ways of managing elephants. If not, then securing more space for elephants alongside human communities could be the answer. This boils down to an old debate – to spare land or share it. Land sparing means separating pristine wildlife habitats from areas of human activity while land sharing involves maintaining biodiversity within landscapes shared by humans. But which is best for conservation? South Africa shows us what land sparing means for elephants – expensive, ongoing management in densely populated reserves. The alternative land sharing approach gives elephants greater access to Africa’s landscapes, but relies on coexistence between people and elephants. At present, land sharing systems outside of Africa’s national parks and reserves are fragile. Human-elephant interactions can threaten the lives of both parties but strategies exist to help coexistence. At the heart of all of them is an understanding that there must be clear benefits to humans in sharing space with elephants. The revenue from tourists paying to see elephants can provide direct employment but education programmes are also necessary to help people understand how elephants benefit the wider ecosystem. Livelihoods outside agriculture must be encouraged to reduce pressures on habitats and wildlife while providing stable incomes in the face of changing environments. Thoughtful land management and planning should ensure vital elephant habitats are protected. Groups across Africa are already working on solutions which can deliver this. Alongside tourism, projects have emerged which generate revenue from elephants without harming them or the environment, such as producing paper and gifts made from elephant dung. The charity Save the Elephants teaches local children about the benefits of living in harmony with elephants and organisations such as the Amboseli Ecosystem Trust have started working with conservationists, politicians and local communities to plan how coexistence can be achieved. Land sharing between humans and elephants will depend on this kind of collaboration between governments, conservation groups and local communities. If people want more for Africa’s elephants than confinement to heavily managed reserves, then everyone needs to be consulted. Only then can there be hope for peaceful coexistence between people and elephants."
"
Everyone see things in the clouds. People, animals, Christ on the cross,  UFO’s, angels, and even schizophrenically imagined chemical attacks by contrails. You name it, somebody has seen it. So when I was prodded with a news item that said “new cloud type defined” I was thinking “uh oh, here we go again”. It is a lot like cyclomania, as humans tend to assign patterns to randomly ordered observations of nature. Looking for meanings in the clouds isn’t much different than looking for meanings in the alignments of the stars and planets.
From ChattahBox and The UK Telegraph:
Click for a larger image
(ChattahBox)—Meteorologists around the world have taken notice of a new storm cloud on the horizon, literally. And if they have their way the dark and choppy cloud will take its rightful place among its more famous cousins, cumulus, cumulus, cirrus and nimbus.
Cloud gazing Meteorologists first noticed the stormy and billowy formation floating over the Scottish Highlands and above Snowdonia, Wales. The unique gray storm cloud was also spotted over Australia, the cornfields of Iowa and high above the Arctic Sea off the coast of Greenland.
A group in England dedicated to cloud watching, the Cloud Appreciation Society, became quite excited when viewing numerous photos of the new storm cloud floating in the atmosphere.
The Cloud appreciators describe the cloud as “…a bit like looking at the surface of a choppy sea from below,” said Gavin Pretor-Pinney, founder of the Cloud Appreciation Society, and the first man to identify the new cloud.
The Royal Meteorological Society has named the new cloud, “Asperatus,” the Latin word for rough, since the cloud has the appearance of a rough, choppy ocean.
The Royal meteorologists are now attempting to have Asperatus officially recognized by the UN’s World Meteorological Organization in Geneva to have it included in the International Cloud Atlas.
If the meteorologists are successful, this would mark the first time a new cloud  was officially recognized since 1953.
Source

I have seen clouds like this, but did not see them as being a new classification. Thus a little trouble with the idea of making an entirely new classification for this cloud, a sub classification perhaps would be more appropriate, especially since this cloud does not appear to inhabit the middle and higher levels of the atmosphere.
Here are the existing classifications:



Latin Root

Translation

Example


cumulus
stratus
cirrus
nimbus

heap
layer
curl of hair
rain

fair weather cumulus
altostratus
cirrus
cumulonimbus



Classifications 
 High-Level Clouds
Cloud types include:  cirrus and cirrostratus.
Mid-Level Clouds
Cloud types include: altocumulus, altostratus.
Low-Level Clouds
Cloud types include: nimbostratus and  stratocumulus.
Clouds with Vertical Development
Cloud types include: fair weather cumulus and  cumulonimbus.
Other Cloud Types
Cloud types include: contrails,  billow clouds,  mammatus, orographic and pileus clouds.
Source: http://ww2010.atmos.uiuc.edu/(Gh)/guides/mtr/cld/cldtyp/home.rxml

So for “asperatus” I could see maybe “stratoasperatus” but not “altoasperatus” since there is no evidence of them at the high altitudes, and clouds at that level tend not to be rough edged.
I actually hope WMO doesn’t accept this ploy for attention by the Cloud Appreciation Society, if they do, it could open an avalanche of new cloud classification applications, we may see pitches of the most absurd kind.
For example, here’s another one from the Cloud Appreciation Society:
This contrail formation has been sent in by several different cloudspotters, and has become known as the Dorset Doughnut. Over Dorset, U.K.
 
“Altostratus Obamus” perhaps?
People see all sorts of things in the sky, if this new one is accepted, the petitioning for WMO recognition of new cloud types would never end.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95824fcb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Waking up to the shock the UK economy's had this year, you'd be forgiven for wanting to pull the duvet back right back over your head.**
Rishi Sunak probably feels the same.
But on Wednesday in his Spending Review he'll have to talk us through just how bad things are. Then he'll spell out how he plans to tackle the next year.
It's about more than just tweaking numbers. It'll give us some idea of who might get a pay cut, what areas the government wants to invest in, such as schools, hospitals and roads.
It may also tell us where jobs will be created, and if - or maybe how soon - we could be facing higher taxes.
Here's what to look out for:
Melissa Aitchison, a student at Nottingham University, wants to know when the economy will get stronger.
She's financing her Master's year by waitressing in a restaurant, but even when it re-opens after lockdown she'll be on shorter hours and taking home less money.
""People aren't giving tips as much,"" she says, and she's worried it'll take a while for confidence to return.
""Even when we do have a vaccine and get busier I think it would take a long time because people are still wary of the [economic] consequences of corona and will be saving money,"" she says.
So the more Mr Sunak can do to boost general economic growth the better as far as she's concerned, even more so once she starts seriously job hunting. A lot of her friends who graduated last year are struggling to find jobs to fit their qualifications.
We're going to hear how much the economy has shrunk this year and it could be more than 10%. In normal years it grows at least a bit.
Anyone looking for work and relying on state benefits should listen out for what Rishi Sunak says about the extra Â£20 a week that was added to Universal Credit at the start of the pandemic. It is currently set to go next April.
Financial blogger Kara Gammell says if the chancellor wants to offer some good news ""this is the time he might announce an extension to that"".
But the chancellor's also under pressure to rein back the astronomical levels of spending triggered by the pandemic, so that may not happen.
There are other ways he may look to cut back too.
Sophie Wilkes, a 25-year-old primary school teacher, wants to know if Mr Sunak will go ahead with a plan to freeze pay for public sector workers like her.
Teachers and other public sector workers had their pay frozen for nearly a decade after the financial crisis. Once inflation is taken into account, holding pay flat amounts to a pay cut in terms of what it will buy you.
Given how much extra work school staff at her inner-city primary in Cardiff have put in his year, Sophie thinks the idea is ""a bit shocking"".
Police, fire and prison officers, local government workers, and members of the armed forces could be in the same boat. But NHS staff are expected to still get a pay rise.
While the coronavirus pandemic has upended the chancellor's plans for spending, 24-year-old beef farmer and cattle breeder Matt Rollason hopes Mr Sunak will have more to say on post-Brexit plans.
""The government needs to make sure that there is seamless trade of food and agricultural goods, not just to the European Union (EU), but elsewhere too. To do that, they need to commit money to things like customs checks, border checks and admin.""
Without this extra funding, there could be a ""major risk"" for the ""intricate and complicated"" system of supplying food in the UK, Matt says.
In the long-term, the Lancashire farmer hopes that the chancellor will also back up the switch from EU farming grants to a new English scheme with a ""serious amount of money"".
""If the government is serious about agriculture moving to net zero emissions and the climate challenge we're all facing, that has to be in [the Spending Review],"" he says.
Rishi Sunak could also hint at future tax rises to help pay for the pandemic spending - though specific changes are more likely to come in the Budget next spring.
Economists say cutting spending or raising taxes too soon risks pulling the rug from under the economy before it has had time to get back on its feet and getting the economy growing is a much better way to increase tax take.
If we are paying more tax or our pay is low we're less likely to go out and spend, points out Iona Bain who writes the Young Money blog. ""And that's what's going to be needed to kickstart the economy.""
Iona's expecting the chancellor to talk us through some infrastructure projects, especially where they could help ""level up"" the parts of the UK that have the deepest economic problems.
But she'd also like to see some more ambitious plans for improving the UK's digital networks and for housebuilding that could in the long run bring down house prices. That could give a boost to jobs in engineering, technology, transport, sustainable energy and construction.
""If they make good on infrastructure spending promises that could create opportunities for younger people. If for instance you are in a sector that has been really damaged by lockdown, there's a chance you can pivot, retrain.""
Entrepreneur and engineer, Roni Savage is hoping for a bit more clarity beyond the one-year scope of the Spending review.
What her firm, environmental engineering consultancy, Jomas Associates, needs is a sign from the chancellor that small and medium-sized businesses (SMEs) will benefit as well as the industry giants.
""He could acknowledge at this stage the importance of involving SMEs,"" she says. ""That would be a really big win for us.""
Any pledges to green the economy should benefit her firm too.
""Boris talked about building back better. We're looking for that not to just be words but action""."
"By now, most of us have heard that the use of plastics is a big issue for the environment. Partly fuelled by the success of the BBC’s Blue Planet II series, people are more aware than ever before about the dangers to wildlife caused by plastic pollution – as well as the impact it can have on human health – with industries promising money to tackle the issue. Single use plastics are now high on the agenda – with many people trying to do their bit to reduce usage. But what if all of this just provides a convenient distraction from some of the more serious environmental issues? In our new article in the journal Marine Policy we argue plastic pollution – or more accurately the response of governments and industry to addressing plastic pollution – provides a “convenient truth” that distracts from addressing the real environmental threats such as climate change. Yes, we know plastic can entangle birds, fish and marine mammals – which can starve after filling their stomachs with plastics, and yet there are no conclusive studies on population level effects of plastic pollution. Studies on the toxicity effects, especially to humans are often overplayed. Research shows for example, that plastic is not as great a threat to oceans as climate change or over-fishing. 


      Read more:
      Plastics in oceans are mounting, but evidence on harm is surprisingly weak


 Taking a stand against plastic – by carrying reusable coffee cups, or eating in restaurant chains where only paper straws are provided – is the classic neoliberal response. Consumers drive markets, and consumer choices will therefore create change in the industry.  Alternative products can often have different, but equally severe environmental problems. And the benefits of these small-scale consumer driven changes are often minor. Take, for example, energy-efficient light bulbs – in practice, using these has been shown to have very little effect on a person’s overall carbon footprint.  But by making these small changes, plastic still appears to be an issue we can address. The Ocean Cleanup of plastic pollution – which aims to sieve plastic out of the sea – is a classic example. Despite many scientists’ misgivings about the project and its recent failed attempts to collect plastic the project is still attractive to many as it allows us to tackle the issue without having to make any major lifestyle changes.  That’s not to say plastic pollution isn’t a problem, rather there are much bigger problems facing the world we live in – specifically climate change.  In October last year the Intergovernmental Panel on Climate Change (IPCC) produced a report detailing drastic action needed to limit global warming to 1.5˚C. Much of the news focused on what individuals could do to reduce their carbon footprint – although some articles did also indicate the need for collective action.  Despite the importance of this message, environmental news has been dominated by the issues of plastic pollution. So it’s not surprising that so many people think ocean plastics are the most serious environmental threat to the planet. But this is not the case. In 2009 the concept of planetary boundaries was introduced to indicate safe operating limits for the Earth from a number of environmental threats.  Three boundaries were shown to be exceeded: biodiversity loss, nitrogen flows and climate change. Climate change and biodiversity loss are also considered core planetary boundaries meaning if they are exceeded for a prolonged time, they can shift the planet into new, less hospitable, stable states.  These “clear and present dangers” of climate change and biodiversity loss could undermine the capacity of our planet to support over seven billion people – with the loss of homes, food sources and livelihoods. It could lead to major disruptions of our ways of life – by making many areas uninhabitable due increased temperatures and rising sea levels. These changes could start to happen within the current century. This is not to distract from the fact that some significant steps have been taken to help the planet environmentally by reducing plastic waste. But it is important not to forget the need for large-scale systemic changes needed internationally to tackle all environmental concerns. This includes longer-term and more effective solutions to the plastic problem – but also extending to more radical large-scale initiatives to reduce consumption, decarbonise economies and move beyond materialism as the basis for our well-being.  The focus needs to be on making the way we live more sustainable by questioning our overly consumerist lifestyles that are at the root of major challenges such as climate change, rather than a narrower focus on sustainable consumer choices – such as buying our takeaway coffee in a reusable cup. We must reform the way we live rather than tweak the choices we make.  There is a narrow window of opportunity to address the critical challenge of, in particular, climate change. And failure to do so could lead to massive systemic impacts to the Earth’s capacity to support life – particularly the human race. Now is not the time to be distracted by the convenient truth of plastic pollution, as the relatively minor threats this poses are eclipsed by the global systemic threats of climate change."
"Climate change is arguably the single most important issue facing the planet over the next century. Today’s school students will be the ones who must reverse it, cope with it as best they can, or experience its consequences. We know students feel engaged and passionate about global issues like this, now adults have to work much harder to make school courses relevant and real. Challenging students to tackle “live” data and watch climate change in action is one way to approach this. To take one example, let’s look at the natural “cycles” of two key substances: water and carbon. Both are absolutely crucial to how the natural world works, and both have been badly disrupted by human activity. Understanding climate change is impossible without grasping what has happened to the water and carbon cycles, but unfortunately, as you may remember from school yourself, these are not the most thrilling topics. Globally, the two collide at the cellular level in tiny pores in the leaves of plants through which CO₂ enters before processing by photosynthesis. Importantly, while CO₂ enters the plant, water leaves in transpiration. As CO₂ levels rise, plants can choose whether to take advantage of this artificial, human-driven fertilisation and fix more carbon (grow faster), or reduce water loss and deal with drought more efficiently. In contrast, these naturally intermeshed carbon and water cycles do not collide in the English and Welsh school system. There, pupils are taught that carbon dioxide is taken into plants and feeds the carbon cycle while, entirely separately, in the water cycle, transpiration injects water into the atmosphere, affecting cloud cover and so the balance between cooling and warming. Climate change cannot be understood without an appreciation of how human actions are affecting all sorts of interlocking natural cycles, and colleagues and I hope that, as researchers, we can help school teachers to join these dots in the classroom. It’s true that talking about carbon and water cycles can seem abstract and disconnected from the exciting and forward-looking parts of the curriculum such as medical advances or molecular biology. But colleagues and I want to change that perception by engaging 15 and 16-year-old students in climate change research, enabling them to use real data to understand these cycles and how they relate to climate change. The materials are being rolled out this term in a pilot programme at a school linked to the University of Birmingham. These approaches are particularly timely. The new high school science courses in England and Wales (known as GCSEs) have a much greater emphasis on applying knowledge and ideas to unfamiliar scenarios as well as analysing and evaluating information than previous courses. One of the aims of the biology course, for instance, is to develop curiosity in the natural world and to encourage pupils to appreciate the relevance of biology to their everyday lives. These skills can be difficult to achieve in a classroom – especially as teachers are under pressure to just “get through” a seemingly ever-growing body of knowledge that pupils must understand and remember. Using materials we have developed, pupils have been able – in just one lesson – to make links between topics such as environmental change and the cycling of materials, or plant cell organisation and photosynthesis and transpiration. At the same time they are also getting to grips with large data sets and experimental design on a grand and unfamiliar scale. This data comes from Birmingham’s FACE project, which is fertilising a forest of mature oak trees with extra carbon dioxide on a huge scale. For at least the next decade, rings of 35m-high pylons will feed the trees extra CO₂ to see how such forests might respond to a carbon-rich atmosphere.  At this point, we have some information from lab studies but real experiments out in the forests are rare and have never looked at UK oak forests. What is clear is that the global consequences of climate change will be mediated through trees, as they balance the uptake of CO₂ with water loss and in turn directly affect any further global warming.  This is not an ivory tower experiment – the forest is wired and connected to the digital world, and pupils will be able to explore the experiment themselves online and monitor its progress in real time. Data sets can be downloaded and used in the classroom to directly address the water and carbon cycle aspect of the syllabus. Direct participation with this high tech research enriches both the school experience and the science itself in a two-way process. Trees get bigger as photosynthesis takes carbon from the atmosphere, but they shrink as water is lost through the open pores. So over months and years tree trunks get thicker, but they shrink and expand over a daily cycle. Here in a single organism the consequences of elevated CO₂ for carbon and water cycles are played out and captured in a single lesson plan. We hope that this sort of interactive, citizen science approach will revolutionise the study of ecosystems and climate change. This is an opportunity for students not just to learn about how science is progressing but to directly participate – ultimately, we aim not just to transmit knowledge, but to inspire action."
"The Green New Deal has broadened imaginations worldwide on the subject of climate change, encouraging people to consider what action to tackle it could do for society. US congresswoman Alexandria Ocasio-Cortez announced the Green New Deal resolution in February 2019, calling for a rapid transition to net zero greenhouse gas emissions, a massive investment in infrastructure and financial redistribution.  While the project would attempt to halt further warming, it would also counter inequality and compensate losers from the energy transition, such as workers in carbon-intensive industries such as coal mining.  It’s already helped wrest the political agenda in the US from the regressive policies and scandals of the Trump administration, and has gained bipartisan support among US voters, despite right-wing pundits denouncing it as a communist plot. The Green New Deal borrows its name and ethos from the New Deal – introduced in the 1930s by then US president Franklin D. Roosevelt to kickstart an economy crippled by the Great Depression. But are strategies which echo the needs of the 1930s and 1940s – ending the Depression and defeating Nazism – suitable for the rapid transition from fossil fuels that defines our needs in the early 21st century?  Can any strategy which relies on historical analogies be adapted to the current climate emergency? The Green New Deal’s proposed investment in public infrastructure and focus on inequality mirrors the original aims of the New Deal, but economic transformation will look very different under a Green New Deal. Whereas Roosevelt’s New Deal aimed to grow the economy, its modern equivalent entails shrinking many economic activities currently central to the economy’s operations. Another way of looking at this is that the original New Deal spurred a massive increase in greenhouse gas emissions. By generating huge public investment in roads and power stations, as well as redistributing wealth through the emerging welfare state, it set the stage for what some call the “great acceleration” in greenhouse gas emissions during and after World War II.  In the US, military build-up was central to this early on, but then it was sustained by the expansion of consumption after the war – most directly by the shift to mass car ownership and urban sprawl that “locked in” high fossil energy use, not only in transport but in housing. The Green New Deal therefore contains a basic contradiction that anyone pursuing it will have to wrestle with as it develops. Many of the measures proposed – such as investing in infrastructure and spreading wealth more evenly – will intrinsically work in tension with efforts to decarbonise the economy.  They create dynamics that increase energy use at the same time as other parts of the Green New Deal are trying to reduce it. For example, building infrastructure such as new road networks will both create demand for carbon-intensive cement manufacture and opportunities for more people to travel by car.  To reach net zero emissions by sometime early in the second half of the 21st century, as the Paris Agreement and the IPCC Special Report on 1.5°C state we must, the global economy has to decarbonise by at least 3% per year. In rich countries such as the US, this needs to happen more rapidly so that poorer countries, which have contributed less overall to global warming, have more time to adapt.  


      Read more:
      Carbon emissions: our research shows a decade of steady decline across Europe and the US


 The targets in the Green New Deal are consistent with this sort of time-frame for decarbonising the global economy. But, even if wealthy countries like the US “only” have to achieve 3% cuts per year, as the economy grows by – say – 2%, then in effect the country has to cut emissions by around 5% per year relative to the growing size of the economy. To illustrate the scale of this challenge, historically, emissions have declined relative to GDP by only about 1% per year, in the aftermath of the 2008 recession. So the challenge is enormous. But of course, the effect of much of the Green New Deal – to invest in infrastructure, to redistribute income – will be to generate significant economic growth. Indeed, this is the point – to get the US economy out of its present stagnation. But it’s hard to see how this will be done without generating new sources of carbon emissions – more housing, more cars and more consumption generally. Herein lies the tension that will recur through the life of the Green New Deal, even if it gets through the immediate quagmire of US politics. Its supporters will have to manage this tension, even though the vast majority of the US left and environmental movement are behind it.  If one imperative is to build new infrastructure to get the US economy going, how much of this will really do more than pay lip service to the energy system transformation in practice? The “Green” in Green New Deal demands that all new infrastructure built is effectively carbon neutral. Even new transit infrastructure, for example, would have to be entirely electric, at the same time as that electricity system is supposed to rapidly abandon coal and then natural gas. It’s easy to imagine which will win when that tension works its way through the political process. It’s not that the Green New Deal isn’t worth pursuing – it’s an extremely promising development. It’s just important to remember Naomi Klein’s invocation that “this changes everything” – dealing with climate change is unlikely to lend itself to off-the-shelf solutions from an earlier age."
"

Predictably, our European friends spent July 16 berating the United States for its refusal to go along with the infamous Kyoto Protocol on global warming. As most people know, Kyoto is an international agreement to reduce the emissions of greenhouse gases that, in reality, has no detectable influence on climate and costs a fortune. This reality notwithstanding, another semi‐​annual “Conference of the Parties” to Kyoto is taking place this week and next in Bonn, Germany. The last one was at The Hague, in the Netherlands, last November.



Chief among Monday’s berators was Juergen Trittin, Germany’s Environment Minister, who thundered that, “We cannot allow the country with the biggest emissions of greenhouse gases to escape responsibility.” That’s us, because we have the world’s biggest economy (which just happens to also be one of the most energy‐​efficient).



So who killed Kyoto? If any one person will be fingered by history, it will be Trittin himself. If any group of nations is to be singled out, it will be the EU, which has been out of step with the rest of the world on Kyoto since day one.



Kyoto’s last best chance at adoption was last November, when the same people who are now berating us in Bonn met at The Hague, two weeks after Election Day. The Clinton‐​Gore team, struggling to find some economically defensible way of meeting Kyoto’s totally unrealistic target–which would require a 33 percent reduction in total U.S. emissions (read: energy use)–proposed that we meet half of that target by planting trees, building up the organic content of our soils, and selling/​giving clean power production technology to polluting, poor (the two are highly correlated) nations.



Jurgen Trittin and the French Environment Minister, Dominique Voynet, said no. To them, speaking for the EU, the United States had to meet Kyoto by directly reducing energy use. Here they proved to even many radical American greens that Kyoto has nothing to do with climate and everything to do with hatred for the United States, very chic these days in Berlin, Paris and London.



So, the United States then proposed that it would only salt away 40 percent of its emissions in trees. No, said Trittin, Voynet and the EU. 30 percent? 20 percent? No. No. President Clinton gained the intercession of his friend, British PM Tony Blair. Voynet then turned on him, saying that he “had conceded too much to America.”



In disgust, the U.S. negotiation team packed its bags and left. As it later admitted to USA Today, the final proposals would have caused grave economic damage. On the way out, EU security guards sat on their hands, as green demonstrators assaulted U.S. negotiator Frank Loy with a pie in the face on world television.



Surely the EU knew that, despite the November turmoil, there was a pretty good chance George Bush was going to be the next president. And not long after this happened, National Security Advisor Condoleeza Rice announced, “Kyoto is dead.”



For that, we have been subject to incessant rants about the United States being a “pariah” and a “rogue state.” So who’s the pariah here? Kyoto doesn’t apply to China, the world’s most populous nation. Nor India, the second largest. Are people in Russia clamoring for its adoption? What about Indonesia, Pakistan, the Middle East? Africa has real fish to fry, like AIDS.



It is clear that the vast majority of the world’s citizens either aren’t bound by Kyoto or don’t care anyway. The United States is merely siding with the majority against a vocal and radical European minority that supports an ineffectual and expensive treaty, which they say can only be implemented in a fashion that will cause us (and, ultimately, the rest of the world) grave harm. There is no way the U.S. Senate will ratify it, anyway.



Kyoto always was sickly. At its inception, in December 1997, the Europeans pressed for impossibly large emission reductions, agreeing to a cut to 8 percent below 1990 levels for a five‐​year period centered around 2010. At Kyoto in 1997, as in The Hague in 2000, the EU proved incapable of standing up to its most radical green elements. Nor has the EU learned from these mistakes. On July 16 in Bonn the 15 EU leaders issued a joint declaration promising to fulfill their treaty commitments, adding one final farce to this tragic comedy. Why anyone would engage in a failed effort to do something that everyone knows wouldn’t even have a measurable effect on global climate remains a mystery.



So, who killed Kyoto? Not us. Bush was merely the coroner. Jurgen Trittin, now railing about holding the United States “responsible” for his own irresponsibility, was the perpetrator, and the EU, wildly out of step with the rest of the world, was the accomplice. But they’re Not Guilty, by reason of insanity.
"
"**A hot tub party host who told officers he ""didn't believe"" in Covid-19 laws has been ordered to appear in court, police said.**
Nottinghamshire Police were called to reports of a party in Poplar Grove, Forest Town, on Saturday.
When they arrived, they found people believed to be from up to five different households mixing.
The host was reported for summons to court and six people at the party were dispersed.
Police added the 32-year-old had refused to give his details and when they tried to explain the law, he told them he ""didn't believe in the Covid-19 legislation and continued to be obstructive"".
The force also broke up a ""large party"" involving 17 people at a house on Layton Avenue, Mansfield, on Monday.
The homeowner was fined and the guests were asked to leave.
England is currently in a national lockdown with strict rules on household mixing.
Latest NHS data showed Sherwood Forest Hospitals NHS Trust, which runs King's Mill hospital in the north of the county, had 92 Covid-positive patients in its beds last week, 12 of whom were on ventilators.
Assistant chief constable Steve Cooper said: ""I find it pretty astonishing that anyone would think it is OK to behave in this sort of way when there has been clear guidance given around not mixing households.
""We are still very much in a lockdown as a nation and we need to keep abiding by these laws.""
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"
Steve McIntyre published an update tonight showing the last 200 years of the Yamal tree ring data versus the archived CRU tree ring data used to make the famous hockey stick. For those just joining us, see the story here.
First here’s the before an after at millennial scale.
Steve McIntyre writes:
The next graphic compares the RCS chronologies from the two slightly different data sets: red – the RCS chronology calculated from the CRU archive (with the 12 picked cores); black – the RCS chronology calculated using the Schweingruber Yamal sample of living trees instead of the 12 picked trees used in the CRU archive. The difference is breathtaking.

Figure 2. A comparison of Yamal RCS chronologies. red – as archived with 12 picked cores; black – including Schweingruber’s Khadyta River, Yamal (russ035w) archive and excluding 12 picked cores. Both smoothed with 21-year gaussian smooth. y-axis is in dimensionless chronology units centered on 1 (as are subsequent graphs (but represent age-adjusted ring width).
Now lets have a look at the data for the last 200 years where that hockey stick lives (and dies):

Steve writes:
Here is a comparison of the Briffa chronology of the spaghetti graphs (red) versus the “SChweingruber” variation i.e. using russ035w instead of 12 recent of 252 CRU cores, leaving 240 unchanged. (The red curve here is the archived CRU chronology, which varies slightly from my emulation of the RCS chronology.)

Viva la difference!
Still broken.
h/t to Mosh


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e933c9833',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterLubos Motl, I am pleased and honoured to say, has also written a piece of his own about Herr Masterplan Hans Schellnhuber. Boy, I thought I was being tough on the guy, Lubos takes it right to him. I found his post so enjoyable that I had to feature it here. He adds a lot more to what I’ve said, so do read it.
=======================================
Herr Schellnhuber has a master plan 
By Lubos Motl
Pierre Goselin is discussing a remarkable interview with the top German climate ideologue in Spiegel:
We Are Looting the Past and Future to Feed the Present (English)
Joachim Schellnhuber, a doomsday crackpot who calls himself a physicist (the inflation in using this term has been significant), starts with the assertion that nuclear power plants should be ready for infinitely strong earthquakes and economics and economy shouldn’t play any role because they’re “crazy logics”…
Continue reading…
Share this...FacebookTwitter "
"

Just days before the Trans‐​Pacific Partnership is scheduled to be signed by its 12 member governments, an official expert from the UN Human Rights Council released a statement criticizing the agreement for being incompatible with the goals of the UN human rights regime. The criticism isn’t about the TPP in particular so much as the modern model of trade agreements as an inadequate vehicle for furthering wealth redistribution and massive regulatory intervention to pursue progressive goals. That is, it’s a complaint about what the TPP _doesn’t_ do.   
  
  
There are, of course, lots of things the TPP doesn’t do. Critics have complained that the TPP doesn’t prevent climate change, doesn’t eliminate human trafficking, and doesn’t reform repressive regimes in Vietnam and Brunei. But these are not things the TPP was ever supposed to do. It’s like complaining that Obamacare doesn’t end the drug war.   
  
  
There are legitimate criticisms to be leveled against the TPP—things it does but shouldn’t and things it doesn’t do as well as it should. There’s also a lot to like. But debates over trade agreements often get bogged down with unrelated controversies that are easier to argue about. Not one of the complaints the UN expert makes is explicitly about trade liberalization.   
  
  
The statement includes two specific criticisms of the TPP. One is the secrecy of the negotiations, and the other is investor‐​state dispute settlement. These are well‐​worn, standard complaints opponents of the TPP have been making for years. The persuasiveness of both arguments relies on reflexive fear of the unknown—opponents can hint at what horrible things might happen from the TPP rather than looking at specific, measurable impacts.   
  
  
These issues have become so controversial, in fact, that eliminating ISDS from future trade agreements and increasing transparency in negotiations would probably result in more free trade.   
  
  
The proliferation and prominence of non‐​trade arguments against trade agreements show that agreements like the TPP have strayed too far away from their core mission. Using “human rights” as an argument against trade agreements will be harder to do if they focus more on simply eliminating tariffs, quotas, and subsidies. A debate over the value of protectionism in promoting national and global welfare sounds very appealing and would surely lead to better policy.
"
nan
"
On June 25th the Competitive Enterprise Institute (CEI) released a draft copy of the suppressed EPA report by EPA employee Alan Carlin critical of the EPA’s position on Carbon Dioxide saying:
The released report is a draft version, prepared under EPA’s unusually short internal review schedule, and thus may contain inaccuracies which were corrected in the final report.
While we hoped that EPA would release the final report, we’re tired of waiting for this agency to become transparent, even though its Administrator has been talking transparency since she took office. So we are releasing a draft version of the report ourselves, today,” said CEI General Counsel Sam Kazman.
CEI notes that: Internal EPA email messages, released by CEI earlier in the week, indicate that the report was kept under wraps and its author silenced because of pressure to support the Administration’s agenda of regulating carbon dioxide.
I’m pleased to say that we have the final report exclusively available here, courtesy of our verified contact at the EPA, who shall remain anonymous. For some background on this contact, developed with the help of Tom Fuller at the San Francisco Environmental Policy Examiner, please read the WUWT story below. The download link is also below.
Source inside EPA confirms claims of science being ignored, suppressed, by top EPA management
The title page of the final report from Alan Carlin of the EPA reads:
Comments on Draft Technical Support Document for Endangerment Analysis for Greenhouse Gas Emissions under the Clean Air Act
By Alan Carlin
NCEE/OPEI
Based on TSD Draft of March 9, 2009
March 16, 2009
Alan prepared an update to this document which is on page 3, I’m reproducing it here for our readers:

Important Note on the Origins of These Comments
These comments were prepared during the week of March 9-16, 2009 and are based on the March 9 version of the draft EPA Technical Support document for the endangerment analysis for Greenhouse Gases under the Clean Air Act. On March 17, the Director of the National Center for Environmental Economics (NCEE) in the EPA Office of Policy, Economics, and Innovation communicated his decision not to forward these comments along the chain-of-command that would have resulted in their transmission to the Office of Air and Radiation, the authors of the draft TSD.
These comments (dated March 16) represent the last version prepared prior to the close of the internal EPA comment period as modified on June 27 to correct some of the non-substantive problems that could not be corrected at the time. No substantive change has been made from the version actually submitted on March 16. The following example illustrates the type of changes made on June 27. Prior to March 16 the draft comments were prepared as draft comments by NCEE with Alan Carlin and John Davidson listed as authors. In response to internal NCEE comments this was changed on March 16 to single author comments with assistance acknowledged by John Davidson. There was insufficient time, however, because of deadlines imposed by the Office of Air and Radiation, to make the corresponding change in the use of the word “we” to “I” implicit in the change in listed authorship. This change has been made in this version.
It is very important that readers of these comments understand that these comments were prepared under severe time constraints. The actual time available was approximately 4-5 working days. It was therefore impossible to observe normal scholarly standards or even to carefully proofread the comments. As a result there are undoubtedly numerous unresolved inconsistencies and other problems that would normally have been resolved with more normal deadlines. No effort has been made to resolve any possible substantive issues; only a few of the more evident non-substantive ones have been resolved in this version.
It should be noted, of course, that these comments represent the views of the author and not those of the US Environmental Protection Agency or the NCEE.
Alan Carlin
June 27, 2009

UPDATE: Before downloading, please read the paragraph above from Alan Carlin to get some perspective. Certainly, this document is not perfect. How could it be? The EPA gave an internal comment period of 1 week on the most far reaching “finding” the agency has ever dealt with. This short window was unprecedented. So ask yourself, could you produce a paper like this, covering many disciplines outside of your own, that is “perfect” on 5 working days notice?
The EPA’s procedure here is the culprit.
Download the final report from Alan Carlin here, link:  Endangerment comments v7b1 (PDF 4MB)

Sponsored IT training links:
Get guaranteed success in 1Y0-A11 exam using best quality 000-200 prep tools including 642-611 dumps and other study resources.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9546c601',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"People learn by making mistakes. The same is true for firms and society – success depends on being able to internalise lessons and behave differently in future, to avoid repeating the same errors. Firms tend to review their organisational structures and routine practises to flag problems before they occur, or respond quickly to unexpected problems to minimise their impact. This is apparently not the case with Vale, however – the fifth largest mining company in the world. Vale is the world’s biggest producer of iron ore and nickel and is also responsible for what may be the largest environmental disaster in Brazil’s history, after one of its tailings dams – an embankment which is supposed to hold back a vast reservoir of toxic mining byproducts – collapsed on January 25, 2019 at the Corrego do Feijao mine in south-eastern Brazil.  Following the collapse, 186 people were confirmed dead and 122 are still missing. Official data from the Brazilian Environmental Agency says that the mudflow destroyed 270 hectares, of which more than half was native vegetation or protected forest. The swathe of natural habitat destroyed is equivalent to 300 football pitches. Tragically, this happened only three years after a similar accident on another of Vale’s dams in the southern Brazilian state of Minas Gerais, near the city of Mariana, which killed 19 people. By any measure of business sustainability, Brumadinho has been a heavy blow to Vale’s performance and reputation. Since then, the company has had its credit rating downgraded, not to mention suffered crippling damage to its public image. Vale appointed Fabio Schvartsman as CEO after the Mariana disaster in November 2015. Schvartsman took the job and announced to shareholders, employees and the Brazilian people a strong slogan: “Mariana, never again”. He failed miserably.  Considering Vale’s recent history and the magnitude of these disasters, the corporate response to Brumadinho’s tragedy seems ludicrous. Schvartsman said to the Brazilian parliament, in a session which assessed the condition of other mining dams in Brazil after the collapse at Brumadinho: Vale is a Brazilian jewel that cannot be condemned for an accident that took place in one of their dams as much, as it was [considered] a tragedy. Despite his confidence, questions remain unanswered. Why didn’t the company move a canteen that was below the dam level and in a high risk area, according to the assessment of reports dated October 3, 2018? The precautionary principle states that if an operation has a risk that might cause severe or irreversible harm to the public or to the environment, that operation must be stopped – even if the likelihood of it happening is low. So why wasn’t this applied? Since standards clearly hadn’t improved over the last three years, how can Brazilians now trust the safety of other dams? Considering the normal course of an accident, why did sirens allegedly fail to work when the dam collapsed to alert employees and the local community to evacuate?  Vale must account for its operational failure with the same gravity as is the standard in developed countries. The Canadian mining company Imperial Metals is still suffering from the environmental liabilities of the Mount Polley mine disaster in Canada.  There is precedent for CEOs and executives to be forced to resign after serious errors, such as the former CEO of BP, Tony Hayward, who resigned after the Deepwater Horizon disaster that killed 11 people in 2010. Schvartsman’s temporary resignation sends an unclear message about the company’s commitments to the lives of its employees and the communities it operates in.    Vale’s next CEO will need to go beyond Schvartsman’s rhetoric and consider what stricter operating procedures might be necessary. Otherwise, it could only be a matter of time before the next “accident”. There are more than 50 similar dams still functioning under Vale’s operations in Minas Gerais state alone that could be another tragedy waiting to happen. Vale cannot operate at the expense of lives and environmental destruction. Brumadinho can still be a turning point in the history of the company and show it is able to learn to better avoid accidents and tragedies. It may still transform itself into a more responsible company. For that, however, it will need to urgently embrace change as never before."
"

Junk science from Harvard, purveyed by a senior official from the United Nations and printed on the front page of The New York Times. Who would have thought such a thing possible? 



James J. McCarthy, a Harvard oceanographer, recently took a summer tourist cruise to the North Pole. When his ship, a Russian icebreaker, followed open water ever northward (as icebreakers do), it eventually wound up in a few‐​square‐​mile patch of water at 90 degrees North.



McCarthy didn’t publish this in the peer‐​reviewed literature. It would not have been accepted, because it is not at all unusual. Instead, he called The New York Times, which dutifully ran a front‐​page story on Aug. 19 by John Noble Wilford, whose first words were, “The North Pole is Melting.” The Times went on to note that “the last time scientists can be certain that the pole was awash in water was more than 50 million years ago.”



This is all nonsense, and especially troubling because McCarthy is co‐​chair of the United Nations’ Intergovernmental Panel on Climate Change (IPCC) section on “adaptation and impacts” of climate change. The Times report is not science; we scientists don’t take a single observation of anything and then draw large systematic conclusions. The famous Supreme Court decision on junk science, Daubert v. Merrell Dow, identified this type of statistical irrelevancy as trash. 



Why didn’t McCarthy get online and check IPCC’s own temperature histories for the region? They are probably no more than four mouse clicks away. Had he done so, he would have found that summer North Pole temperatures are no different than they were for several decades in the early 20th century, long before dreaded economic growth could have warmed the region. Further, based on fossil evidence, most climatologists think the period from 4,000 to 7,000 years ago averaged at least 2 C warmer than the current era at high latitude. That’s three millennia. It seems pretty obvious that the summer polar ice back then would have been considerably more abraded than it is today, and maybe even gone completely in certain years. The climatic consequences? No one can find them, except that the period was noteworthy for the flowering of agriculture and the rise of civilization. 



McCarthy could have also read the latest issue of the scholarly journal Climatic Change, in which University of Colorado climatologist Mark Serreze writes that not only has there been no net change in summer North Pole temperatures over the last 70 years, there has been no change in annual North Pole temperatures.



Times reporter John Noble Williams could have done this too. Instead, after receiving a firestorm of criticism for the irresponsibility of his first report, he tried to back down in a modified, limited hang‐​out sort way. His August 29 Times article notes that McCarthy now “would not argue with critics who said that open water at the pole was not unprecedented.”



Instead, he went to the Serreze article and disgorged another distortion: “The data scientists are now studying reveal evidence that on average Arctic temperatures in the winter have risen 11 degrees over the past 30 years.”



This is more junk. Both Serreze and the IPCC winter data show that a) 30 winters ago, in 1969, temperatures were around their lowest for the entire 20th century, and b) the net rise since then is 1.5 C. In terms of the departure from normal, they are currently running around 0.7 C above the standard reference mean, an inconsequential number. Wilford picked a small region of the Arctic where temperatures rose a great deal and said that this area applied “on average” to the entire Arctic. But that warm spot is balanced by many areas in which temperatures have fallen, which is how we achieve the unremarkable average change that has been observed.



These things are not difficult to check. But it’s easier to unquestioningly print the pronouncements of a Harvard professor bearing U.N. authority, even if he’s speaking from a cruise ship with a sample size of one. If I were McCarthy’s dean at Harvard, I’d be livid. And if I were Robert Watson, head of the United Nations’ climate panel, I’d find a new co‐​chair for the section on “adaptation and impacts” before the current one completely destroys the panel’s credibility.
"
"Australia’s greenhouse gas emissions have dipped slightly on the back of new clean energy and a sharp fall from agriculture due to the drought, but the decline was almost entirely wiped out by surging industrial pollution. Official data released on Monday revealed national emissions were down 0.3% in the year to September.  Emissions from electricity generation fell 2% (3.6m tonnes of carbon dioxide) as the now-filled renewable energy target boosted invested in solar and wind power. Pollution from agriculture fell even further – 4.1m tonnes, or 5.8% – due to the impact on livestock of the historic eastern seaboard drought and north Queensland floods. But fugitive emissions, released during coal and gas extraction, were estimated to have jumped 6% (3.3m tonnes). Pollution from stationary energy – which includes the manufacturing, construction and commercial sectors – was up 2.6% (2.6m tonnes). The biggest leap was in the surging liquefied natural gas export industry, now worth $50bn after a wave of investment, particularly in Western Australia. LNG emissions were up 16.9% (6.3m tonnes). As Guardian Australia has reported, the rise in industrial pollution has occurred as the government has allowed big polluters to increase emissions limits under a scheme known as the “safeguard mechanism”, which was promised in 2015 to prevent rises above business-as-usual levels. National emissions across the year were estimated to be 530.8m tonnes, nearly 1% lower than they were in 2000. The government’s 2020 target over that timeframe is a 5% cut. Emissions are down about 1.3% since the Coalition was elected in 2013, but just 0.4% since 2014, the year the national carbon price was repealed and replaced by then prime minister Tony Abbott with the emissions reduction fund. Climate pollution fell about 14% when Labor was in power under Kevin Rudd and Julia Gillard. The data was released in the wake of the Labor leader, Anthony Albanese, recommitting the party to a target of Australia having net zero emissions by 2050. It has not yet explained how it would get there. Every Australian state, the Business Council of Australia and more than 70 countries have supported a goal of net zero emissions by 2050. Asked on Monday if the federal government would make the same commitment, the minister for emissions reduction, Angus Taylor, said the world had agreed “to get to net zero emissions in the second half the century” under the Paris climate agreement but the Coalition would not adopt a target that was unfunded or not backed by a plan. The government is expected to soon release a “technology investment roadmap”, but has confirmed few details. It has also received a report from a panel led by the businessman Grant King, which was last year asked to advise on new ways to cut emissions. “We’ve got to do our bit, and that’s why we’ll be focusing on technology,” Taylor said on Monday. In a statement on the new emissions data, Taylor emphasised emissions were down 13% since 2005, the year against which Australia’s Paris target for 2030 (a 26-28% cut) is measured. Labor’s climate spokesman, Mark Butler, said the fall in emissions was “little more than a rounding error”, and the government’s policies were “hopelessly inadequate”. “There was no reduction in emissions in the quarter to September 2019 and annual emissions only reduced by a pitiful 0.3%,” he said. “Scott Morrison is failing to protect Australians from the dangerous realities of climate change.” Taylor said the Australian economy was “steadily decarbonising” but growth in exports was putting upward pressure on emissions. In particular, he said, LNG exports were reducing global emissions “by displacing more emissions-intensive fuels overseas”. Guardian Australia last year asked for the evidence that Australian LNG was displacing coal-fired power in Asia to the extent the government has claimed. It was not made available. A government report at the time suggested gas would be increasingly competing with zero-emissions nuclear and renewable energy in Japan, Australia’s biggest LNG market. Taylor said emissions from export industries had increased 54% since 2005 levels, but per capita pollution and the emissions intensity of the economy – the amount released relative to production – were at the lowest levels in nearly three decades. “The value of our exports has increased by $83bn since September 2013, reflecting the Coalition government’s good economic management,” Taylor said."
"On a global scale the science is settled: human emissions of greenhouse gases have already led to a rise in global temperature of more than 1°C, and the consequences are visible around the world. Already, in 2019, Australia has sweltered in record-breaking heat, while the US Midwest was hit by freezing conditions colder than Antarctica.  In the UK meanwhile, winter temperatures soared past 20°C for the first time ever. Britain is not particularly known for intense heatwaves, vicious hurricanes or snowstorms. But, in a world that is getting warmer, climate change will mean the country experiences more and more extreme weather.  To be identified as “extreme”, a weather event must significantly differ from normal patterns, be associated with severe impacts and be historically infrequent. In the UK such events include floods, heatwaves and droughts. The UK’s national weather service, the Met Office, has examined historical trends in extreme weather events and has projected how the climate may change over the 21st century. If emissions continue to increase, there is an increased chance of milder, wetter winters and hotter, drier summers. But at the same time, the country may see an increase in the frequency and intensity of hot days and heavy rainfall events.  This does not mean that every season will be warmer than the previous one. Daily and annual changes in weather patterns will continue to produce some unusually cold summers and winters, even as the climate warms.  As global temperatures continue to climb, by 2050 heatwaves similar to that witnessed in 2018 could occur every other year. By 2100, heatwaves will likely become more intense and longer lasting, with the possibility of temperatures exceeding 40°C and heatwaves lasting 50 days. The laws of physics tell us that a warmer atmosphere can hold more moisture, increasing the frequency and strength of extreme rainfall events. From year to year, rainfall patterns across the UK will still vary – but when it does rain it may fall in more intense bursts. Recent research has found that there is a one-in-three chance of record-breaking rainfall hitting parts of England and Wales each winter. That means in the future, wet winters like 2015/16 could become more common. While summers are expected to become drier overall, climate change means downpours could become heavier. Researchers at the Met Office, the UK’s national weather service, modelled the future of UK precipitation in very fine detail (down to squares of just 1.5km) and found that intense rainfall associated with severe flash flooding could become almost five times more frequent by the end of this century. The emerging field of “event attribution” is enabling scientists to better understand the drivers of extreme weather. New and stronger evidence confirms that abnormally high temperatures and associated extreme weather are indeed related to human activities. For example, the very latest attribution study found that human-caused climate change had at least doubled the likelihood of the remarkable northern Europe heatwave in 2018. Rainfall is harder to predict, as it depends more on relatively local factors such as atmospheric circulation and the availability of moisture. Many of these processes are not adequately represented in observational data or climate models. So, while a warmer world is associated with an increase in atmospheric moisture, what that means for extreme rainfall events in the UK will vary substantially from region to region. Having said that, we can use past observations and future climate model experiments to infer whether the intensity and likelihood of such events are a result of climate change. In December 2015 Storm Desmond travelled across the North Atlantic, leaving in its wave an “atmospheric river” of super-moist air. Record-breaking rainfall caused major flooding across Ireland and northern England. Researchers at Oxford University and the Royal Netherlands Meteorological Institute have shown climate change increased the frequency of a rainfall event like Storm Desmond by 59%. This isn’t the first time that extreme UK rainfall has been linked to a changing climate. By analysing the extremely wet winter in 2013/14, scientists found that climate change had made such a season 25% more likely. While there are many uncertainties in attribution studies, researchers have shown that human-caused climate change is nudging the temperatures up and increasing the odds of new extremes in heat and rainfall."
"As with Brexit in Britain, the outcome of a referendum in Hamburg came as a blow to the establishment. By the slimmest of majorities, voters defied the status quo, and the party that did the best job convincing the public it would honour their will collected the political rewards. The dynamic unleashed by the 2013 plebiscite on buying the local utility grids back from private providers helps to explain why the city state is expected to buck national trends at this Sunday’s elections.  Polls predict the ruling centre-left Social Democrats (SPD), who are in decline across Germany, will hang on to their Hamburg stronghold with relative ease. Their junior coalition partner, the Green party, is expected to increase its share of the vote, leading to a strengthened mandate for the current power-sharing deal. The CDU of Angela Merkel is likely to drop into third place while the far-right Alternative für Deutschland may even struggle to make it above the 5% threshold for entry into the Hamburg parliament. Yet a centre-left success in the city state is more likely to be the result of local factors than national trends. In September 2013, 51% of the electorate voted to “re-municipalise” the local electricity and district heating networks, as proposed by grassroots initiative “Our Hamburg Our Grid”. And while the energy debate has only played a minor role in this year’s election campaign, it has increased the profile of the two governing parties. For one, Hamburg has managed to get de-privatisation done. In Berlin, by contrast, the senate announced its intention to buy back the district heating grid but has so far failed to drive the project through the courts. Hamburg sealed a €650m (£545m) deal last September. For Hamburg’s SPD – which campaigned against re-municipalisation, and is widely seen as more pro-business than elsewhere in the country – the buy-back has helped show it can also apply its famed pragmatism to more traditionally leftwing projects. The local party has been quicker to discover its green heart than other SPD branches in the country. And on Thursday, Hamburg’s SPD mayor, Peter Tschentscher, surprised his Green coalition partners by proposing to shut one coal unit at the city’s Moorburg power plant and convert the second to gas, in addition building a 100MW electrolyser to produce green hydrogen at the site. “If we hadn’t listened to voters, we would have been punished,” said Matthias Ederhof, an SPD candidate on the regional party list. “We are on the up because voters see us as competent.” The re-municipalised electricity grid, for which Hamburg paid the Swedish energy group Vattenfall €610m, has been making the city senate a healthy profit since 2017. The Greens are finding that having utility grids in public hands allows them to propose initiatives that would otherwise have sounded like pipe dreams. Hamburg, a city whose wealth has historically grown around its inland port, has in recent years turned into a busy laboratory for renewable energy schemes. Hamburg is now the first state in Germany to write a legally binding date into its constitution, the year 2030, not just for halving its carbon emissions from 1990 levels, but also for phasing out coal power for district heating altogether. The 250,000 households in the city which use district heating are still powered by coal stations in Wedel and Tiefstack. In the future, the Greens propose, Hamburg homes will be heated entirely from renewable sources such as waste incineration, biomass or solar power. There is a proposal to build a large heat storage system underneath the harbour, capturing excess heat produced in the summer for use during the winter by pumping hot salt water hundreds of meters deep into the soil. Even an old air raid bunker in the Wilhelmsburg district has been transformed into a so-called “energy bunker”, with a large heat reservoir. The city has the highest number of charging points for electric cars in Germany, more than Berlin or Munich. If Hamburg’s utility grid was still owned by Vattenfall, such experiments would have been unthinkable, said Christian Maass, a former state secretary for the environment and urban planning. “Private monopolies become a problem especially when the state doesn’t have the power and capacity to regulate them. In Hamburg, it therefore made more sense for the senate to hold the monopoly, because politicians can be held to account much more effectively than private companies,” said Maass, who now works as consultant on renewable energy. “Expectations on state-owned energy or heating grids are much more long-term and less orientated around maximising profit. If you want to shape the future of a city, that’s an immeasurable advantage.”"
"Policies matter. Good policies lead to good outcomes, while bad policies can lead to disaster. But what about where there is no policy, or a policy that is incohesive and incomplete? We only need to look at the state of science research policy in Australia to find out. Scientific research in Australia has always suffered from political influence, because research in Australia is heavily dependent on federal government funding. But political interference in scientific research has been weaponised during the past decade of Coalition governments.  The most obvious and destructive manifestation of this political interference on the nation’s scientific research effort is the lack of a comprehensive science policy. It can be argued that some consequences of this meddling in the research effort have been this summer’s bushfire emergency and the widespread environmental destruction, mostly initiated via climate change. The scientific community and the whole of rational Australia were stunned by the decision of the Abbott government not to appoint a minister for science in 2013. This was coincident with the defunding of research into climate change and the environment. There was no science policy put forward by the Abbott government. The Turnbull government did go some way to redress this lack of policy through the national innovation and science agenda in 2015, which promised to set “a focus on science, research and innovation as long-term drivers of economic prosperity, jobs and growth”. What they came up with was Australia’s national science statement in 2017 which, in turn, gave Australia a grab bag of science policies, programs and projects. This is the closest the Coalition government has come to providing a science policy. But this is not a science policy in either scope or execution. It’s a political agenda favouring a few scientific research programs that resonate with the government’s economic agenda. There is a strong emphasis on projects that may provide an economic return in the near future and there is no scope for funding research into areas that the government does not favour. This political agenda has been carefully crafted to make it look like the government is supporting scientific research, while in reality it neither understands what science has to offer or has the desire to fund any more research than it has to. The centrepiece of the current government’s science agenda is its statement on science, technology, engineering and mathematics (Stem). It is quite clear that this agenda is closely linked to economic outcomes and returns. This reduces the Australian scientific research effort to a cookie jar of favoured projects that can make a financial return for the canny investor. It turns Australia’s scientists into the lapdogs of industry. Perhaps the saddest part of this political game is that the favoured issues and projects do genuinely deserve support and funding but they have been turned into sock-puppets aimed at distracting public attention from areas of research that have been excluded from such support. Issues such as advancing women in Stem and research into emerging technologies such as nanotechnology, robotics, quantum computing and space industries are all worthy of public money, but so too are issues of climate and environmental research. Not that Coalition governments are all that interested in research anyway. In 2018 research funding for Australian universities was cut to the lowest levels in 40 years. In that year our research effort plunged well below the OECD average. A minor bounce in the 2019 budget for research funding has not redressed this loss. If we are to judge this government by its actions, it is not interested in research and never has been. It is not hard to see why the current and former Coalition governments want to play favourites with scientific research. There has been a consistent theme of climate denial and environmental inaction from the federal government since 2013. All the while there has been a clarion call for action on climate and environmental issues led by scientists working in those areas. Seemingly immune to facts and reason (one Coalition MP recently vowed not to let his opinions be informed by evidence), why would you want to fund the nagging voice of science? No need to shoot the messenger when you can simply starve them to death. So what have been the consequences of this lack of a proper science policy for Australia? The weakened voice of our climate and environmental research scientists has been easier to ignore. With minimal monitoring and reporting on the environment and next to no effective research into the effects of climate change in Australia, the early troubling signs of the unfolding catastrophes were largely unobserved and unreported. We drifted into the unprecedented drought, the drying of the Murray-Darling, the catastrophic bushfires and the ferocious flooding events, untroubled by any warnings from a research community that had been effectively silenced. A wise government would have a broad-based science policy backed up by funding support that is at least on par with the world average. Since we have neither of these, we can only question the wisdom of the current federal government. • Associate Professor Paul Willis is adjunct in palaeontology at Flinders University. Former director of the Royal Institution of Australia and long-time science presenter with the ABC"
"

Global dimming and brightening: A review
Martin Wild
Institute for  Atmospheric and Climate Science, ETH Zurich, Zurich, Switzerland
There is  increasing evidence that the amount of solar radiation incident at the  Earth’s surface is not stable over the years but undergoes significant  decadal variations. Here I review the evidence for these changes, their  magnitude, their possible causes, their representation in climate models, and  their potential implications for climate change. The various studies  analyzing long-term records of surface radiation measurements suggest a  widespread decrease in surface solar radiation between the 1950s and 1980s  (“global dimming”), with a partial recovery more recently at many  locations (“brightening”). There are also some indications for an  “early brightening” in the first part of the 20th century. These  variations are in line with independent long-term observations of  sunshine duration, diurnal temperature range, pan evaporation, and,  more recently, satellite-derived estimates, which add credibility to  the existence of these changes and their larger-scale  significance.
Current climate models, in general, tend to simulate these  decadal variations to a much lesser degree. The origins of these  variations are internal to the Earth’s atmosphere and not externally forced  by the Sun. Variations are not only found under cloudy but also  under cloud-free atmospheres, indicative of an anthropogenic  contribution through changes in aerosol emissions governed by economic  developments and air pollution regulations. The relative importance of  aerosols, clouds, and aerosol-cloud interactions may differ depending on  region and pollution level. Highlighted are further potential implications  of dimming and brightening for climate change, which may affect  global warming, the components and intensity of the hydrological cycle,  the carbon cycle, and the cryosphere among other climate  elements.
Received 14 November 2008; accepted 10 March 2009; published 27  June 2009.
Citation: Wild, M. (2009), Global dimming and brightening: A  review,
J. Geophys. Res., 114, D00D16, doi:10.1029/2008JD011470.
I found this passage that parallels a lot of what I’ve been saying about data quality:
The assessment of the magnitude of these SSR (surface solar radiation) variations faces a number of challenges. One is related to data quality. Surface radiation networks with well-calibrated instrumentation and quality standards as those defined in BSRN [Ohmura et al., 1998] need to be maintained on a long-term basis and if possible expanded into underrepresented regions (see Figure 1b).
However in this figure, citing CRU surface temperature, he likely doesn’t understand what data quality issue might have contributed to the trend from 1960-2000

One of the effects of urbanization is the compression of the diurnal temperature variation. I recently was able to demonstrate this between two stations in Honolulu. One is in the middle of the Airport and had a sensor problem, the other was in a more “rural” setting about 4 miles away. Note how the ASOS station at the airport has an elevated temperature overall, but that the biggest difference occurs in the overnight lows, even when the ASOS sensor giving new record highs was “fixed”:

Urbanization affects Tmin more than Tmax. For example, here’s the nighttime UHI signature of Reno, NV that I drove as a measurement transect using a temperature datalogger:

Click for larger image
Even several hours after sunset, at 11:15PM, the UHI signature remained. The net result of  urbanization is that it increases Tmin more than Tmax, and thus minimizes the diurnal range, which we see in Wild’s diurnal range graph.
Even the IPCC misses it:

Wild probably has no idea of this type of issue in the CRU data, but again it speaks to data quality which he seems to be keen on. He’s looking for a global solar signature in temperature data, something Basil Copeland and I have done, to the tune of much criticism. The signature is there, but small. But, when diurnal temperature variation is looked at, any solar signature is likely swamped by the urbanization signal. I’m not saying there is no solar component to what Wild is looking at, but it seems fairly clear that UHI/urbanization/land use change plays a significant role also.
Even rural stations can be affected by our modern society, as Dr. John Christy demonstrated in California’s central valley:
A two-year study of San Joaquin Valley nights found that summer nighttime low temperatures in six counties of California’s Central Valley climbed about 5.5 degrees Fahrenheit (approximately 3.0 C) between 1910 and 2003. The study’s results will be published in the “Journal of Climate.”
The study area included six California counties: Kings, Tulare, Fresno, Madera, Merced and Mariposa.
While nighttime temperatures have risen, there has been no change in summer nighttime temperatures in the adjacent Sierra Nevada mountains. Summer daytime temperatures in the six county area have actually cooled slightly since 1910. Those discrepancies, says Christy, might best be explained by looking at the effects of widespread irrigation.
Wild’s study is a very interesting  and informative paper, I highly recommend reading the entire paper here (PDF 1.4 mb)
h/t and sincere thanks to Leif Svalgaard for bringing this paper to my attention.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e955ba37c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The Government of Jersey has been given the power by the States Assembly to make wearing masks in shops mandatory.**
Members also approved laws to limit the size of gatherings, as part of updated Covid-19 regulations.
Currently the wearing of masks is not legally enforceable, with the government continuing to promote them in guidance.
The regulations have not created new rules, rather they have set the terms of possible restrictions.
The maximum penalty for individuals breaching any mask or gathering law would be set at Â£1,000.
Children under 12 years old would not have to wear a mask, along with people exempt for health or disability reasons, according to the regulations.
If a law requiring wearing of face coverings is introduced, it would apply to specific workplaces where a member of the public is present as a customer.
The regulations also allow orders to oblige businesses to collect personal data to aid contact tracing and to refuse service to those not wearing masks.
Any restrictions on the size of gatherings will only apply to groups of 10 or more people.
Visiting people's homes in Jersey was banned during the first wave of the pandemic before the ban was lifted in May, although public health guidance has discouraged meeting indoors since."
"
Share this...FacebookTwitterThe eruption of Grímsvötn on May 20, 2011 has produced a cloud of volcanic ash that shot up over 50,000 ft and has drifted over parts of Europe closing a number of major airports and creating air traffic havoc.
Now the online Der Spiegel reports today that another volcano, Hekla, is on the verge of exploding as well. Satellite altimetry measurements show that the mountain has swollen – more than it did right before it exploded the last time in 2000. Der Spiegel writes what scientists have found:
On the Hekla volcano they have discovered a 20 km wide swelling. Magma has risen up under the ground and is pushing the ground up, reports a group around Benedikt Ofeigsson of the University of Iceland in Reykjavik in the magazine ‘Journal of Geophysical Research‘. An eruption soon is ‘very likely,’ confirms vulcanologist Birger-Gottfried Lühr of the PotsdamGeosciences Research Centre.”
Hekla is right now under extreme pressure.
Is Hekla next? ‘If it keeps its rhythm of the last decades, then it is now due,’ says Lühr.
Instruments on the mountain show that Hekla has swollen up more than it’s last eruptions in 2000 and 1991.”
According to Wikipedia, during the Middle Ages, Icelanders called the volcano the “Gateway to Hell.” In January 2010 there were reports of patches near to the summit not covered with snow. Hekla had massive eruptions in 5050 BC, 3900 BC, 2310 BC and 950 BC, which threw about 7.3 km of volcanic rock into the atmosphere, placing its Volcanic Explosivity Index (VEI) at 5. This would have cooled temperatures in the northern parts of the globe for a few years afterwards.
After being dormant for 250 years, Hekla erupted again in 1104 AD with os VEI of 5. Hekla has also erupted every 10 years since 1970. Some eruptions had a VEI of 3, which sent ash 15 km into the atmosphere. If the scientists today are right, it could be a disruptive year for European air travellers.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOur friend Rudolf Kipp at the German Science Skeptical site here has a shocking report on a callous Der Spiegel piece that appeared yesterday. I’ve translated Rudolf’s report in English with his permission.
===================================================
Organic Foods Are Killing – This Time in Africa
by Rudolf Kipp
Children are dying in Africa – just so that German organic food shops can keep their store shelves well-stocked. This is what author Laura Koch writes in Der Spiegel Online in a story titled “How the Malaria Wonder-Weapon Drives Farmers Into Poverty”.
In the Spiegel report’s introduction, the author writes:
Malaria transmitted by mosquitoes kills hundreds of people in Uganda daily – that’s why the government there uses the insecticide DDT. But the use of the pesticide has grave consequences for people living out in the countryside: Suppliers of organic foods are no longer able to sell their products, and now they are threatened by abject poverty.”
These introductory words alone bring up 2 fundamental questions. Firstly: Is the planting of organic foods the only possibility that Ugandan farmers have in providing for their livelihoods? Secondly: Since hundreds of people can be saved from death by DDT daily, how many Ugandans are we willing to sacrifice in order to allow a few farmers to produce crops that meet the directives of some European and US-American organic food associations? Just one note on the side: Half of the malaria-caused deaths are small children.
The eco-movement’s downfall
Within enlightened circles, the ban of DDT pushed by environmental groups and government bodies since the early 1960s has become known as the eco-movement’s downfall. Already in the early 1970s it was clear that the horror stories connected to the use of DDT were scientifically unfounded. Nonetheless, efforts were made to ban the substance globally. Eventually bans were enacted through various instruments involving political and economic pressure.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One can rightly criticize the massive agricultural use of DDT that took place in the 1960s. From this time it was possible to detect traces in the fat tissue of animals in the Arctic and Antarctic. But these times are long gone. Substitute substances have been found for use in agriculture and are much more effective, and they break down and dissipate much more quickly.
Wonder weapon DDT
When combating the anopheles mosquito, the main transmitter of malaria, the case is different though. Here DDT remains by far the most effective and the most economical weapon against the disease. And only very small amounts are needed compared to the amounts used for crop protection. Here it is already enough to spray the walls of homes located in risk areas with a trace amount of DDT only twice a year. Mosquitoes that remain on the wall die promptly.
Of course there also exist alternatives to DDT when combating malaria. But none are as effective, and, what is particularly crucial in the impoverished countries of Africa, none is as cost-effective. Mosquito nets, which are always propagated by aid organisations and environmental groups, function poorly and only when one sleeps under one. Anyone who goes outside during twilight hours still gets exposed to the lethal infection. Carbamates are also as effective as DDT, but are 4 to 6 times more expensive and must be sprayed many times more often. Organo-phosphates cannot be sprayed inside homes and apartments because of their hazard. And the often-mentioned wonder weapon of synthetic pyrethroide against malaria has been shown to be considerably less effective.
Eco-imperialism 
Let’s emphasize yet one more time: When using DDT for combating malaria, the substance is no longer sprayed over large land areas. Rather, it is used in small amounts in a targeted manner. In all countries that have used DDT, the number of of people falling ill or dying from malaria has decreased significantly. Many countries that have bent to the will of aid organisations and governments of western countries have once again experienced explosions in the number of those who have fallen ill or died.
Taking all this into account, it is especially reprehensible to call for a ban of DDT just so that western countries can eat organic food that does not contain DDT also in the ultra-trace amounts. Sadly in our prosperity some among us are obviously prepared to accept the otherwise avoidable death of millions of people – all in the name of protecting ourselves from an extremely hypothetical risk. That is eco-imperialism in its purest and worst form.
Rudolph Kipp
==========================================
Share this...FacebookTwitter "
"**Councils in Lancashire have written to the prime minister's interim chief of staff to criticise ""unfair"" levels of financial support received during lockdown.**
In the letter to Sir Edward Lister, seen by BBC News, leaders accused the government of ""breaching"" an agreement reached when the area entered the top level of restrictions in October.
They also said northern councils had received less than the south, despite a longer period of restrictions.
BBC News has approached the government for comment.
Lancashire Councils say an agreement was reached that Â£30m of additional support for businesses would be made available by government, when the area entered the highest level of coronavirus-related restrictions under the previous tiered system in mid-October.
In the letter to Sir Edward, the 15 council leaders said the agreed funding was intended to cover a ""28 day surge period"", during which the area would be under heightened restrictions.
But on 31 October, Boris Johnson announced the whole of England would be entering a second period of wider lockdown measures from 5 November to 2 December - with the same level of financial support extended to all English councils as a one-off grant.
Lancashire's leaders said it was ""unfair"" and a ""breach"" of the agreement, as areas facing four weeks of restrictions under the lockdown received the same amount as they did for seven weeks.
And they are now calling for a further Â£20m from central government to cover the additional period.
The leaders wrote: ""We have been advised that Lancashire will receive no additional funds, despite the fact that we are now essentially subject to seven weeks of restrictions.
""This is inherently unfair and divisive - it is likely that London and other areas, predominantly in the south, will receive the same compensation for 4 weeks of restrictions.""
They added: ""This is an unacceptable position for us and the people of Lancashire and a breach of the agreement we made with the government through the negotiations.""
The government's been clear that this time there'll be no regional negotiations over which restrictions apply where.
No showdowns with local leaders of the type we saw play out in Greater Manchester.
But that doesn't mean there won't be regional pushback.
Some Tory MPs are already warning about higher tiers being applied to large areas, regardless of some local spots being low risk.
And then there's the financial fight too.
Local authorities across the board have long warned of budget shortfalls, despite several billions of pounds of extra funding the government's put in during the pandemic.
In particular councils in the areas hardest-hit by this virus, many in the midlands and north of England, say funding packages aren't enough to help them support communities which have been living under restrictions for some time.
Ministers have pointed to a plethora of support schemes for individuals and businesses, and there will be more cash for councils in the highest tiers.
But this could still be a flashpoint when the regional rules are announced, and a government that's promised to address regional inequality can't afford to leave any area feeling hard done by.
Some parts of Lancashire, such as Blackburn with Darwen, have been under extra restrictions since August.
Geoff Driver, the Conservative leader of Lancashire County Council, said extra funding was ""vital"" to businesses in his area.
""Lancashire's 15 leaders have set aside political differences to make the case to government to provide us with this support,"" he said.
""It is really disappointing that we haven't had this, not least because Lancashire has been ready, willing and able to work with central government to support our people and businesses throughout this pandemic - and we remain so.
""Looking forward, we don't know yet what tier or tiers Lancashire will be in, but it is clear that some if not all of the county will be facing significant restrictions and our businesses will continue to require support.""
Mohammed Iqbal, the Labour leader of Pendle Borough Council in Lancashire, said: ""While I recognise we entered national lockdown before the end of the 28 day period, Lancashire was under additional restrictions without additional support for more than two weeks.
""To date, we haven't received a penny.
""The government has rowed back on its promise to fund tier three areas and it beggars belief that ministers think it fair that councils in Lancashire under seven weeks of restrictions are not entitled to more support than those areas that are under four weeks of restrictions.""
Ministers have put aside an additional Â£900 million to support councils in tier three of the new tiered restrictions system to come into effect on 2 December.
Previous business support grants for councils in tier three or under national lockdown restrictions amounted to Â£1.1 billion.
An announcement is expected Thursday on which areas will be under the highest level of restrictions once the current lockdown ends."
"**Animals have been ""a lifesaver"" for people struggling during Covid lockdown, according to retailer Pets at Home which has seen sales rise sharply.**
Chief executive Peter Pritchard said pets had played ""an incredibly important role"" through a period of ""social loneliness"".
He added that during the early days of lockdown one of the few reasons people could go out ""was to walk your dog"".
In the six months to 8 October, Pets at Home saw revenues rise by 5.1%.
Mr Pritchard told the BBC's Today programme: ""The pet care market has been incredibly strong throughout and I think that tells you an awful lot about people's relationships with their pets and the roles that pets play in people's lives.
""It has been a lifesaver for many through this incredibly challenging period for everybody in the country.""
He added that the change in more people working from home had allowed them to get a dog or a cat. ""More people have considered having a pet because their lives have changed and they are at home more often,"" he said.
The company sells some small animals and fish but does not sell cats or dogs. It said, however, that membership of its Puppy & Kitten Club for new owners had risen by 25% during the six-month period.
Pets at Home said the first half of its financial year, which runs between April and 8 October, reflected the entire period since the week after national lockdown was implemented. Restrictions on households weighed on trade in the first quarter before a 12.7% jump in like-for-like sales in the second three months.
Pets at Home is classed as an essential retailer and has been allowed to stay open during lockdown. Total sales over the six months rose to Â£574.4m while pre-tax profit grew by more than 14% to Â£38.9m.
The company did not place any employees on furlough and said that it has actually been recruiting more staff.
However, Pets at Home's share price dropped by 7.3% to 388p in early trading.
The company warned of uncertainty because of the pandemic and said: ""At this stage, absent any escalation of restrictions, or other significant disruption to our operations, we now anticipate full-year underlying pre-tax profit to be in line with the prior year.""
Julie Palmer, partner at professional services firm Begbies Traynor, said: ""Looking ahead, with the value of the resilient UK pet market set to hit Â£7bn next year and the nation's love affair with pet ownership showing little sign of abating, chief executive Peter Prichard's optimism for the future appears well justified.""
However, she added that the retailer would be ""mindful of the dampening effect of social distancing measures in store, which may impact margins over the all-important Christmas period and into the first quarter of 2021""."
"Going on safari in Africa offers tourists the opportunity to see some of the most spectacular wildlife on Earth – including African elephants (Loxodonta africana). Known for their complex social systems, long memory and high intelligence, this species is also threatened by poaching and shrinking habitats, so further disturbance to their precarious existence could have serious consequences. Wildlife tourism can help protect these animals and their habitat by generating income for conservation and providing stable work in local economies. Countries such as South Africa and Kenya receive two to five million visitors to protected areas each year, generating receipts of up to USD$90m. But as it becomes more popular worldwide, it’s worth remembering that we often don’t know how tourism affects the animals we observe. In Madikwe Game Reserve in South Africa, tourists stay in lodges within the park and go on safari twice a day in large, open vehicles driven by professional field guides. Over 15 months in Madikwe, we recorded how often elephants performed stress-related, vigilant or aggressive behaviours to find out whether they increased during months when there were more tourists. Vigilant behaviour could be an elephant extending its trunk into the air to smell. Stress-related behaviour included elephants bunching together or fleetingly touching their faces with their trunks – a response akin to a nervous tic in humans. Aggression was noted, for example, if an elephant charged at another, spread its ears to appear larger or hit another elephant with its tusk. We also watched the movements of elephant herds to see if they stuck around or moved away from tourist vehicles. We found that elephants were more likely to be aggressive towards other elephants in months when tourist numbers in the park were high. Elephant herds were also more likely to move away from tourist vehicles when there were more vehicles present. So, it appears that tourism does have some impact on elephant welfare – but this may not be entirely bad news. We didn’t observe an increase in stressed or vigilant behaviour in response to higher numbers of tourists, and the effect of increased aggression was small. Hunting can have much greater effects on elephants, even among those who aren’t attacked by humans. Studies which measured levels of stress hormones in elephants after they witnessed hunts or were nearby have found they increase significantly. Humans riding on the backs of elephants is also much worse for elephant welfare than observation tours. Wildlife watching, without physical contact, seems to be the better mode of tourism for elephant welfare, but it’s not without its concerns. Although these results were interesting, they are only from a single population in South Africa where driving regulations were enforced. We don’t know how elephants are affected in areas where tourists drive their private vehicles on safari unaccompanied by professional guides. We also don’t know what exactly was causing the changes in behaviour. More tourists per month meant there were more vehicles on the roads, but also more air traffic, more diverse smells and sounds and who knows what else. Parks could create refuge areas where safari tours are restricted and contact with wildlife minimised, perhaps in areas where there are fewer roads already. Tour companies could strictly enforce a no off-roading rule here and prohibit guided walks by tourists. Such refuge areas have previously been shown to have great potential in reducing pressure on elephants during times of increased stress, such as following large wildfires. Tourism can be a great conservation tool as long as it is monitored closely, and measures are taken to alleviate the potential pressures it can put on animals. If you’re ever lucky enough to find yourself on a safari, think twice about getting up close and personal with that iconic species. Instead, keep your distance and the welfare of the animals in mind."
"
Bob Tisdale writes in with:
What Do You Suppose They’ve Been Doing At The National Hurricane Center This Summer?
 http://i27.tinypic.com/im1m2r.gif
Source: http://www.nhc.noaa.gov/
Bocce maybe?  Horseshoes?
UPDATE – Ryan Maue of Florida State University writes in comments:
Global (Northern Hemisphere) tropical cyclone ACE for the months May – June – July is the lowest in at least the past 30-years or more.
I, for one, am not surprised.  Continued inactivity should persist for the next few weeks until the atmosphere catches up with the radiative warming of the tropical oceans due to the season called summer.
2007 was a dud.   2008 was saved from being a record year by 2007.  2009 is behind the pace of both years.  Amazing how natural variability affects tropical cyclone formation, tracks, and intensity.  Who would have thought?
Ryan’s Tropical web page at Florida State University has this graph that shows accumulated cyclone energy (ACE) :
click for larger image
Sorted monthly data: Text File 
Note where 2009 is in the scheme of things.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e945cd97a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Language, music, and art often vary between adjacent groups of people, and help us identify not only ourselves but also others. And in recent years rich debates have emerged and spawned research into culture in non-human animals. Scientists first observed chimpanzees using tools more than half a century ago. As this complex behaviour appeared to differ across different populations, researchers concluded that tool use in apes was socially learned and therefore a cultural behaviour. This was the beginning of exploring what behaviours in other species might be considered cultural as well. Killer whale pods and dolphins exhibit different dialects and use tools differently, for instance. Scientists have mostly focused on primates, however. Capuchin monkeys of Central and South America exhibit 13 variants of social customs, to take one example, while different orangutan populations vary their callsand the use of tools, nests or other objects. But no species has garnered more discussion on the presence, importance, and evolution of culture than chimpanzees. Examples of chimpanzee culture range from social customs, such as the way they grasp their hands during grooming, to how males sexually display, to the type of tools used for cracking nuts or ant-dipping. An early study argued that there are as many as 39 different behaviours that are candidates for cultural variation. This set off an eager debate about whether animals have culture or not and how we would be able to detect it.  As in humans, cultural behaviours in chimpanzees are likely critical for individuals to demonstrate community membership. If a young chimpanzee in the Tai forest in the Ivory Coast wants to signal to a peer that they would like to play around, then they build a small, rudimentary ground nest and sit in it. In most other chimpanzee groups, ground nests are mainly used for resting. But chimpanzees now face the daunting task of surviving in a habitat increasingly infested and assaulted by humans. And as their populations decline, so does their behavioural variation. In short, humans are causing chimpanzee cultural collapse.  Two of us (Alexander and Fiona) were involved in a new study which integrated data from 144 chimpanzee communities across Africa, and found the more that humans had disturbed an area, the less behavioural variants are exhibited by nearby chimpanzees. The results are published in the journal Science. The actual mechanism behind this is not entirely known. The most obvious explanation is that increased human disturbance means there are fewer chimpanzees overall. Even those that remain have to be more inconspicuous in order to survive in areas where their food and nesting sites are threatened by logging operations, their water sources are polluted by miners, and they risk being hunted for bushmeat by poachers brought into their forests by newly-built roads.  All this forces the chimpanzees to forage in smaller groups and use less long distance communication like pan hoots and drumming on tree trunks. This likely leads to a decrease in the spread of cultural behaviours, as associating in smaller group sizes lowers the chance of learning socially from one another.  Chimpanzees have also been observed to adapt to human disturbance by inventing new coping mechanisms such as eating human crops. But despite these rare adaptations, overall human activity is vastly erasing the rich behavioural diversity that now characterises chimpanzees. But, if the species is gradually merging into a single cultural entity that stretches all the way from Senegal to Tanzania – why does this matter? After all, monocultural species are not inherently problematic. There is no direct relationship between cultural diversity and species distribution, for example. Flies, rats and crocodiles are all disseminated across a vast area, and yet have not yet been described as cultural. Losing chimpanzee behavioural diversity doesn’t itself threaten the species survival.  Losing diversity could be representative of larger issues, however, not least that the species is on the decline, which is the worst scenario. For example, we don’t yet know how adaptive these behaviours are. A loss of behavioural diversity could represent compromises in how animals respond to selection pressures like changes in food availability and how they adapt to climate change. The risk is that we humans are irreversibly endangering a unique chance to discover the full extent of cultural diversity in our closest living relatives. When scientists discover a new group of wild chimpanzees it often exhibits unique behaviours that have never been observed previously, and it is hard to know what would be eradicated before we know about it.  If things continue as they are, the opportunity to study common evolutionary roots with our own species might soon be forever lost. Making protection of cultural diversity a conservation priority, which extend to numerous other species, would help to ensure the survival of our extraordinary primate heritage."
"

Back in December 2000, President Clinton and Vice President Gore were busy fellows — what with dishes to pack, furniture to ship and an election to contest. So busy were they that they neglected to read some of the fine print in a cascade of administration‐​ending paperwork. One of these was an obscure item called the “Federal Data Quality Act” (FDQA), which was dutifully signed by the president.



Put simply, the FDQA prohibits the use of junky science in the promulgation of federal regulations and laws. And, now that the new hats are in town, it shouldn’t be much of a surprise that the FDQA is being turned against the “science” of the Clinton‐​Gore team, particularly concerning the global environment.



Specifically, it has been turned against the “U.S. National Assessment of the Potential Consequences of Climate Variability and Change” (USNA), a document that breaks the cardinal rule of science: If a hypothesis doesn’t work, throw it out. The Assessment can’t pass the simplest of scientific tests.



The Assessment began with a 1997 letter from Gore to all the federal agencies, and was published 10 days before the 2000 election. If the Office of Management and Budget chooses to apply FDQA, the Assessment will be redacted down the Memory Hole soon.



And none too soon. The power of the USNA’s bad science can be seen in recent drafts of Sen. Tom Daschle’s (D-S. Dak.) energy bill, where the USNA provides the findings necessary to induce new fuel economy measures and prohibit drilling for domestic oil — all in the name of global warming and its pernicious effects on America.



In fact, that it serves as the basis for legislation is the reason that the USNA has run afoul of the law. The FDQA requires scientific objectivity and normal reproducibility of positive results in any simulation or scientific experiment that underpins prospective regulations. The Assessment has neither.



The Assessment purports to project the consequences of United States warming, produced by two computer models. One is from Canada and the other from the United Kingdom. Both models are extreme outliers. Unlike the consensus of the dozens of available models, the Canadian model produces an exponentially increasing heating. The result is a ridiculous rise of 8.1ºF in projected U.S. temperatures this century. The UK model predicts greater precipitation changes than any other model the USNA looked at.



A horde of peer reviewers–some from federal laboratories that have a track record of global warming doomsaying–told the USNA that the use of these two models was wrong. Even the greens at the United Nations agree that these models can’t be used to make local and regional climate projections with any reliability.



How does even the rankest climate amateur know the Canadian model is a joke when applied to the United States? Because it “predicts” that U.S. temperatures should have changed 300 percent more than they did in the last 100 years. In fact, neither the Canadian model nor the British can beat a table or random numbers when it comes to predicting U.S. temperature for the last century.



A climate model is nothing but a statement of scientific hypothesis: What we “think” should happen based upon currently fashionable theory. When a hypothesis doesn’t work (i.e., performs worse than a bunch of darts thrown at the Dow Jones), the ethic of science requires that it be thrown out. In this case, it means that the USNA should have used better models, or, absent a defensible model, it should have used none. If a computer simulation of climate can’t beat a table of random numbers over the United States, it borders on scientific malpractice to continue to apply it.



It wasn’t that the politically chosen leaders of the USNA didn’t know there was a problem. In fact, the USNA’s politically handpicked steering committee was so disturbed about the finding of the peer‐​reviewers that it commissioned its own study. Guess what? The USNA’s own scientists verified that the temperature models didn’t work over the United States. And yet the report went forward, now serving as the basis for the most sweeping energy legislation ever introduced in this nation’s history.



Well, anyway, all of these shenanigans are precisely what the Federal Data Quality Act was designed to prevent. The irony is that the obscure piece of legislation that slipped through when Clinton and Gore weren’t minding the store is about to throw the USNA and its global warming hysteria into its well‐​deserved dustbin.
"
"

On July 25, at a hearing of the House Oversight and Investigations Subcommittee (the same folks who grilled WorldCom last month), the nation found out how little real science there is about global warming. The hearing was prompted by the discovery that federal scientists were using computer models that they knew could not replicate U.S. temperatures. They appeared in two landmark documents that have served as the basis for very expensive and intrusive energy legislation. 



What came out of the hearing has people asking if the same problems affecting Enron, WorldCom, Global Crossing, etc…are now troubling environmental science.



Puffy federal documents create consequences. In this case, look no further than a recent California law requiring the first statewide reductions on global warming emissions from cars. It cites many findings from the “U.S. National Assessment” of global warming, one of the documents that provoked the House Oversight hearing.



Science operates by a hard and fast ethic. Theory must conform to reality and be tested by reality. In the case of climate science, our “theories” are huge computer models that project various amounts of warming for the next 100 years. 



In my review of the Assessment, I discovered two very disturbing facts. The Assessment team considered several computer models, but chose the two that predicted the most extreme changes in temperature and rainfall over the United States. That’s prima facie evidence for some type of bias, which isn’t surprising. The team was vetted through four Clinton administration committees, one of which was headed by Al Gore. Worse, these models couldn’t beat a table of random numbers, or two dice on a crap table, when it came to predicting U.S. temperatures.



The Assessment team was required to publicly comment on the science reviews, and swept this criticism under the rug. Behind the scenes they were much more fearful, and replicated my experiment. They found out that the computer models they were using couldn’t even simulate 25‐​year temperature averages over the United States as the greenhouse effect changed in the last 100 years. 



How, then, could these models be used to assess climate change in the next 100 years?



It gets worse. The temperature and precipitation data these models spit out are then input to other sectors of the U.S. economy, such as agriculture, forestry, and water supplies.



In science, random numbers are garbage, and that’s what went in. Refuse in science is what we call “transitive.” Start with it and you end with it. Garbage out.



Tom Karl, who heads the National Climatic Data Center in Asheville, North Carolina, and co‐​chaired the production of the Assessment, was the lucky person who had to come to Washington to defend these shenanigans. His dancing was about as painful as you would expect when a noted scientist has to defend something so wrong.



The core defense of the federal establishment was that they were not making “predictions” of future U.S. climate with these models. Instead they were “plausible projections.” Are the most extreme estimates of U.S. temperature and rainfall changes “plausible”? 



Can someone explain to a reporter, seeing these dire results spit out by extremist computer models, that there’s a lick of difference between a “prediction” and a “projection”? Does anyone believe that the political impact of either, if based upon bad models, isn’t the same, i.e., bad legislation? 



This rings the same bell that Bill Clinton did when he said, “That depends on what the definition of the word ‘is’ is.” Predictions, projections, forecasts…they’re all the same to any reporter, or, for that matter, to any professor. And when they’re based upon computer models that can’t beat dice dancing in Atlantic City, they’re junk.



This story isn’t going away. It surfaced on MSNBC’s ” Hardball” last week, and it will to bubble up every time someone comes up with a new law on climate change in the United States.



It’s no accident that all of this wound up in front of the Oversight and Investigations Subcommittee. Recently it targeted corporations that cook books and hide things from shareholders. 



Forecasting something as important as future U.S. climate based upon models that do not work is as deceptive as stating assets that a company does not have. In this case it is the federal science establishment and we, citizens of the United States, who are the investors. 



It’s not just CEO’s and CFO’s who inflate results to jack up their currency. The corruption has now spread to science. 
"
"The National Trust has ditched plastic for the annual membership card it sends out to 5 million members, it has announced. The new card will be made from a type of strong and durable paper featuring a tough water-based coating, with the paper certified by the Forest Stewardship Council. They will be produced in a mill powered by its own biomass.  The trust said the new cards would avoid the use of 12.5 tonnes of plastic a year. The new cards will be entirely recyclable and compostable, as well as coming in at a fraction of the cost of the old cards, which were made of plastic and chalk, a byproduct of the mining industry. The National Trust said the move was part of a range of measures it was bringing in to protect the environment and tackle the climate emergency, after a survey showed it was backed by the majority of its members. Mel Nursaw, from the trust’s membership team, said: “Replacing our membership cards is a great step towards helping to reduce our impact on the environment, which we know is an important issue for so many of our supporters.” In total, the National Trust now has almost 6 million members, including children and life members who do not receive an annual card. The trust is also exploring how to transfer its physical cards to digital ones. Elsewhere, the charity is looking at removing plastic from most of its greeting cards and wrapping paper, looking at alternatives to plastic tree guards and trialling drink dispensers to reduce sales of bottled drinks."
"

Since the June House vote on the Waxman‐​Markey “cap‐​and‐​trade” bill, lawmakers from both chambers have backed significantly away from the legislation. The first raucous “town hall” meetings occurred during the July 4 recess, before health care. Voters in swing districts were mad as heck then, and they’re even more angry now. Had the energy bill not all but disappeared from the Democrats’ fall agenda, imagine the decibel level if members were called to defend it and Obamacare.   
  
  
But none of this has dissuaded the editorial boards of the _The New York Times_ and _Washington Post_. Both newspapers featured uncharacteristically shrill editorials today demanding climate change legislation at any cost.   
  
  
The _Post_ , at least, notes the political realities facing cap‐​and‐​trade and resignedly confesses its favored approach to the warming menace: “Yes, we’re talking about a carbon tax.” The paper—motto: “If you don’t get it, you don’t get it”—argues that in contrast to the Boolean ball of twine that is cap‐​and‐​trade, a straight carbon tax will be less complicated to enforce, and that the cost to individuals and businesses “could be rebated…in a number of ways.”   
  
  
Get it? While ostensibly tackling the all‐​encompassing peril of global warming, bureaucrats could rig the tax code in other ways to achieve a zero net loss in economic productivity or jobs. Right. Anyone who makes more than 50K, or any family at 100K who thinks they will get all their money back, please raise you hands.   
  
  
The prescription offered by the _Times_, meanwhile, is chilling in its cynicism and extremity. It embraces the fringe—and heavily discredited—idea of “warning that global warming poses a serious threat to national security.” It bullies lawmakers with the threat that warming could induce resource shortages that would “unleash regional conflicts and draw in America’s armed forces.”   
  
  
(Note to the Gray Lady: This is why we have markets. Not everyone produces everything, especially agriculturally. For example, it’s too cold in Canada to produce corn, so they buy it from us. They export their wheat to other places with different climates. Prices, supply, and demand change with weather, and will change with climate, too. Markets are always more efficient than Marines, and will doubtless work with or without climate change.)   
  
  
Appallingly, the piece admits that “[t]his line of argument could also be pretty good politics — especially on Capitol Hill, where many politicians will do anything for the Pentagon. … One can only hope that these arguments turn the tide in the Senate.” In other words: the set of circumstances posited by the national‐​security strategy are not an object reality, but merely a winning political gambit.   
  
  
There’s no way that people who see through cap‐​and‐​trade are going to buy the military card, but one must admire the _Times_ ’ stratagem for durability. Militarization of domestic issues is often the last refuge of the desperate. How many lives has this cost throughout history?   
  
  
Nevertheless, one must wonder at the sudden and inexplicable urgency that underpins the positions of both these esteemed newspapers. Global surface temperatures haven’t budged significantly for 12 years, and it’s becoming obvious that the vaunted gloom‐​and‐​doom climate models are simply predicting too much warming.   
  
  
Still, one must admire the _Post_ and _Times_ for their altruism. The economic distress caused by a carbon tax, militarization, or any other radical climatic policy certainly won’t be good for their already shaky finances, unless, of course, the price of their support is a bailout by the Obama Administration.   
  
  
Now that’s cynical.
"
"
One of the common themes seen with the surfacestations.org project has been the proximity of BBQ grills to official NOAA thermometers used in the United States Historical Climate Network (USHCN). Despite now having surveyed over 77% of the 1221 station network, some truths continue to be self evident.
USHCN climate station of record, Hartington, NE
This station was photographed by our prolific volunteer, Eric Gamberg. The proximity to the concrete patio earns this station a CRN4 rating, it may be a CRN5 when they wheel out the BBQ away from the house. But who knows? The grilling schedule is not part of the metadata.
But fear not, NASA GISS adjusts for such problems of concrete and BBQ grills. Consider the following blink comparator:
Notice how the past is adjusted cooler, increasing the trend
Source: NASA GISS
USHCN RAW:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=1&num_neighbors=1
GISS Homogenized:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=2&num_neighbors=1
I’m not sure why the hinge point is 1978, perhaps that’s when the homeowner acquired the BBQ? Sure, that is an absurd claim, but certainly no more absurd than the GISS homogenization adjustment itself. Adjusting the past increases the overall positive slope of the temperature trend.
For those new to the whole concept of USHCN stations, the NOAA thermometer is the white slatted object on the post in the center of the photo. It is known as an MMTS thermometer and a cable goes from it into the home where the volunteer observer will write down the high and low into the B91 logbook and send in the report once a month to the National Climatic Data Center (NCDC).There are more photos of this station which you can see in my online station database.
The Gallery of photos can be seen here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95f1fddf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Contiguous U.S. GISTEMP Linear Trends:  Before and After
Guest post by Bob Tisdale
Many of us have seen gif animations and blink comparators of the older version of Contiguous U.S. GISTEMP data versus the newer version, and here’s yet another one. The presentation is clearer than most.
 http://i44.tinypic.com/29dwsj7.gif
It is based on the John Daly archived data:
http://www.john-daly.com/usatemps.006
and the current Contiguous U.S. surface temperature anomaly data from GISS:
http://data.giss.nasa.gov/gistemp/graphs/Fig.D.txt
In their presentations, most people have been concerned with which decade had the highest U.S. surface temperature anomaly: the 1940s or the 1990s. But I couldn’t recall having ever seen a trend comparison, so I snipped off the last 9 years from current data and let EXCEL plot the trends:

 http://i44.tinypic.com/295sp37.gif
Before the post-1999 GISS adjustments to the Contiguous U.S. GISTEMP data, the linear trend for the period of 1880 to 1999 was 0.035 deg C/decade. After the adjustments, the linear trend rose to 0.044 deg C/decade.
Thanks to Anthony Watts who provided the link to the older GISTEMP data archived at John Daly’s website in his post here:
http://wattsupwiththat.com/2009/06/28/an-australian-look-at-ushcn-20th-century-trend-is-largely-if-not-entirely-an-artefact-arising-from-the-%e2%80%9ccorrections%e2%80%9d/
NOTE: Bob, The credit really should go to Michael Hammer, who wrote that post, but I’m happy to have a role as facilitator. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9515c18e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"Whülü Thurr is a staunch believer in ancient farming traditions. “There is an old adage,” she says, “which goes ‘even a single stalk of millet can revive a dying man’.” The 65-year-old farmer, from New Phor village in Nagaland state, north-east India, is a devotee of the ancient grain millet, and is well versed in its nutritional benefit. She is one of the few farmers here who has stayed with the traditional crop over the decades. Many other farmers in Nagaland, the majority of whom are women, have stopped growing it due to a lack of demand.  White rice, easily available both in markets and via the government public distribution system – which sells it at a subsidised rate – is preferred by consumers across the state. But growing rice requires large quantities of water, which makes it vulnerable to the effects of the climate crisis. Faced with a changing climate, Thurr and other farmers are now working to bring millet back to the region and encourage more people to eat it. Thurr joined the Millet Farmers Group, created by the Nagaland chapter of North East Network (NEN), a women’s rights non-profit organisation. Through the network, 200 millet farmers from 11 villages in the district of Phek are sharing knowledge, increasing their output, and accessing new markets. Thurr says she has seen her yields increase since she joined the group. NEN Nagaland also provides interest-free loans of between 1,500 and 2,000 rupees (£16 and £21) to millet farmers. And it is trying to build demand for the grain, including producing a cookbook featuring baked goods, fried snacks, sweets and pancakes – all made from millet flour – aimed at a younger urban audience. “Before joining NEN and the Millet Farmers Group, the millet I grew was only for my family. With the increased production, I now have a surplus which I’m able to sell,” says Thurr. “This money is very helpful towards meeting our household expenses as well as for our medical needs.” Wekoweu Tsuhah, the Nagaland state director of the network, says: “Rice farmers here were already affected by climate change. We’ve had erratic rainfall, rising temperatures, new pests and new crop diseases. So, in 2010–11, we began focusing on millet farming as a way to ensure food security and build climate resilience. “Millet needs very little water, grows well on poor soil, is fast growing and suffers from very few diseases. Once harvested, it stores well for years. We see growing millet as a tool to empower local women farmers in the face of climate change.” Foxtail is the most commonly cultivated type of millet in the region, but other varieties such as sorghum, proso and pearl are also being grown to ensure diversity. Many farmers grow crops to feed their families but some of the bigger producers also sell their output. As well as trying to improve access to markets, NEN Nagaland has helped some of the women to buy dehusking machines. Without access to these machines farmers had to dehusk the grain manually, which put many off millet as a crop. “For years after I resumed millet farming in 2013, I did not have a dehusking machine. I had to manually pound the grain to dehusk it. It took up a lot of my time and energy as I had no help – these activities are considered to be a woman’s job. But I persisted. It was important to me to cultivate this traditional crop,” says Povelu Shijo, a farmer from Phuhgi village. In 2018, with the support of NEN Nagaland, Shijo was able to buy a dehusking machine at a subsidised rate from the government. Inspired by her enthusiasm, Povelu’s entire village is now growing millet. In Akhegwo village, Chutso and her Millet Farmers Group are allowed to cultivate on village land, rent-free. “We already knew of the nutritional benefits of millet,” she says. “But now that NEN has made us aware of its ecological value, our farmer’s group is focusing on seed production.” Last year, the group harvested between 130 to 150kg of seeds, which were distributed to the community. “This year, millet farming will happen on a larger scale in our village. We will start sowing soon,” adds Chutso. In Sumi, the village council rewards the farmer with the highest millet production with a cash prize of 2,000 rupees. Thurr says she is happy that young farmers in her village have started to grow millet again. Not only is it generating income, it is reclaiming its cultural significance. “I received millet as a wedding present from my parents and I’ve gifted each of my four daughters 10 baskets of millet at their weddings.” Wekoweu adds: “By reviving millet farming, we are not only preserving our traditional knowledge, we are also preparing for an uncertain future.”"
nan
"Clothing in Britain is increasingly characterised by a high volume/low value approach to business. Judging by past precedent, consumers will discard some 680m items of clothing when they spring clean their wardrobes this year. Replacements are cheap: dresses can be bought for as little as £5 from online retailers (indeed, a mere £3.75, reduced 25%, in Boohoo’s current sale). Cheap prices are praised for providing wider access to consumers. Fashion retailers argue that they are a sign of efficiency. But there is a dark side. A new report from the Environmental Audit Committee enquiry into sustainable fashion reveals how consumers are only benefiting from cheap clothes at considerable cost to the environment and through exploitation of poor and vulnerable garment workers. The environmental impact of fashion is well known. Cotton production uses large amounts of pesticides and water, while synthetic fibres such as polyester are derived from finite oil supplies. Bamboo, increasingly used as a cotton replacement, sounds pleasingly natural but is a semi-synthetic fibre, the production process of which involves the use of chemicals such as caustic soda. And while British consumers with an environmental conscience may feel less guilty as they take their unwanted garments to a charity store or vintage shop, many of these end up in landfill or incinerated because they cannot attract buyers – domestically or overseas. The social impact of fashion similarly raises concern. Evidence suggests that fashion companies do not yet monitor supply chains with such diligence that consumers can be sure that their purchases have not involved exploitation of the workforce. In Britain, many garment workers in Leicester are apparently being paid less than the minimum wage. Abroad, slave labour, child labour and poor working conditions persist, more than five years after the collapse of the Rana Plaza complex in Bangladesh left more than 1,100 garment workers dead. In recent years, the Waste and Resources Action Programme (known as WRAP), which is supported largely by public funds and works closely with the fashion industry, has done an excellent job in promoting clothing sustainability. Its report Valuing our Clothes provided a strong evidence base for action. It has warned government and industry of the environmental impact of a throwaway culture, producing guidelines for designers and a protocol for companies wanting to produce longer-lasting clothing. But designing garments for longevity is pointless if they are discarded prematurely and merely add to the vast tonnage of clothing waste generated each year. Every garment that is produced contributes to the industry’s environmental impact. In a sustainable fashion culture, far fewer garments would be produced and, when no longer wearable, the materials would either be reused – for example, through upcycling, where unwanted clothes are redesigned into new items – or recycled.  It is a vision that still appears a long way off. The Environmental Audit Committee’s report, however, offers hope. Vast amounts of clothing is discarded and, currently, barely 1% is recycled. A recent study highlighted the many obstacles to recycling that need to be overcome. The UK government’s Resources and Waste Strategy promised to review and consult on textile waste – but only by 2025, giving a strong indication that it does not regard the issue as a priority. By contrast, the Environmental Audit Committee’s report proposes a “producer responsibility” scheme in which producers would pay a 1p charge per garment to improve clothing collection and recycling in order to address textile waste. Such a strategy is long overdue and, like all tax threats, inevitably attracted the most media headlines. But the report proposes several other initiatives that could prove even more significant. For example, noting that Sweden has reduced VAT on clothing repair services, the committee recommends a more general reform of taxes, suggesting that “the chancellor should use the tax system to shift the balance of incentives in favour of reuse, repair and recycling to support responsible companies”. And it concludes that the voluntary approach represented by WRAP’s Sustainable Clothing Action Plan, which requires supporting companies to reduce their water, waste and carbon footprints, has had its day. Instead, it argues, such targets should be mandatory for all retailers with a turnover exceeding £36m. A revival in lessons on designing, creating and repairing clothes in the school curriculum is also proposed: a much-needed return to what was once taught as home economics. Whether the government will have the courage to accept this challenge at the expense of STEM subjects will be interesting to see. Economic and educational measures are needed, because recycling does not address the fundamental problem of unsustainable levels of production and consumption in the clothing sector. WRAP’s evidence to the committee revealed that improved production efficiency has reduced environmental impacts – but that these gains were more than offset by increased consumption. The situation is especially bad in Britain. The Textile Recycling Association reported in 2018 that British consumers purchase far more clothes than consumers in other European countries: more than 26kg each year, compared with  17.6kg in Germany, 14.5kg in Italy and 12.6kg in Sweden.  In short, companies produce too much and consumers buy too much. The sector is based on an outmoded and unsustainable business model and relies on insatiable consumer demand.  Thus the fashion industry produces more garments than retailers are able to sell, while the secondhand market is unbalanced – with supply far exceeding demand. Meanwhile, wardrobes in Britain hold a vast surplus stock of garments, much of it unworn because we lack repair and alteration skills. The recent report contains many welcome proposals, but we need to go further. In Britain we consumed more than 1.13m tonnes of clothing in 2016, a significant increase compared with 2012. A target to halve this by 2030 would be an appropriate goal to focus people’s minds."
"
It appears that all is not well with the idea of “climate forecasting” as the Ministry of Defence pulls the financial rug out from under the Met Office climate program. Now how will they pay for the electricity to run “deep black”, the 1.2 megawatt supercomputer they just purchased?
Phil Jones may have drain the moat and sell access to his climate data lists and code to pay the bills.

From the Register, UK
Weather soothsayers lose £4.3m
By Austin Modine
Story excerpts:
The Met Office, home of UK weather soothsaying, is getting its climate research budget chopped by a quarter after the Ministry of Defence ended financial support to focus on “current operations.”
A loss of £4.3m ($7m) funding will hit the Met Office Hadley Centre for Climate Change, according to the science journal Nature. The research institute provides the government with bleeding-edge computer models indicating which parts of the UK should stockpile sunscreen and floaties for the coming Thermageddon.
Please support net journalism. Read the entire story at the Register -> here
Read a couple of interesting articles about the shenanigans of the Hadley Climate Reasearch Unit at Climate Audit:
The UK Met Office Deepens The Moat
Phil Jones: the Secret Agent in Hawaii




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9528ac31',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"As part of one of the largest environmental protests ever seen, over a million young people went on strike on Friday March 15 2019, calling for more ambitious action on climate change. Inspired by Greta Thunberg, a Swedish school girl who protested outside the Swedish parliament every Friday throughout 2018, young people in over 100 countries left their classrooms and took to the streets. The previous #YouthStrike4Climate on February 15 2019 mobilised over 10,000 young people in over 40 locations in the UK alone. Their marches, chants and signs captured attention and prompted debates regarding the motivations and methods of young strikers. Many were criticised by those in the government and the media for simply wanting an opportunity to miss school. My PhD research explores youth participation in climate change governance, focusing on the UN climate negotiations. Between 2015 and 2018 I closely studied the Youth Climate Coalition (UKYCC) – a UK based, voluntary, youth-led group of 18 to 29 year olds – which attends the international negotiations and coordinates local and national climate change campaigns. My research shows that young people are mobilised by concern for people and wildlife, fears for the future and anger that climate action is neither sufficiently rapid nor ambitious. Young people need to feel as though they are “doing something” about climate change while politicians dither and scientists release increasingly alarming projections of future climate conditions.  The strikes have helped young activists find like-minded peers and new opportunities to engage. They articulate a collective youth voice, wielding the moral power of young people – a group which society agrees it is supposed to protect. All the same, there are threats to sustaining the movement’s momentum which need to be recognised now. The paternalism that gives youth a moral platform is a double-edged sword. Patronising responses from adults in positions of authority, from head teachers to the prime minister, dismiss their scientifically informed concerns and attack the messenger, rather than dealing with the message itself. You’re too young to understand the complexity of this.  You’ll grow out of these beliefs. You just want to skip school. Stay in school and wait your turn to make a difference. Striking may hurt your future job prospects. The list goes on … This frightens some children and young people into silence, but doesn’t address the factors which mobilised them in the first place. These threats are also largely unfounded.  


      Read more:
      Climate change: a climate scientist answers questions from teenagers


 To any young person reading this, I want to reassure you, as a university educator, that critical thinking, proactivity and an interest in current affairs are qualities that universities encourage. Over 200 academics signed this open letter – myself included – showing our support for the school strikes. Growing up is inevitable, but it can cause problems for youth movements. As young people gain experience of climate action and expand their professional networks, they “grow out of” being able to represent youth, often getting jobs to advocate for other groups or causes. While this can be positive for individuals, institutional memory is lost when experienced advocates move on to do other things. This puts youth at a disadvantage in relation to other groups who are better resourced and don’t have a “time limit” in how long they can represent their cause.  Well-established youth organisations, such as Guides and Scouts, whom I have worked with in the past, can use their large networks and professional experience to sustain youth advocacy on climate change, though they lack the resources to do so alone. It would also help for other campaigners to show solidarity with the young strikers, and to recognise youth as an important group in climate change debates. This will give people more opportunity to keep supporting the youth climate movement as they get older. Researching the same group of young people for three years, I have identified a shift in their attitudes over time. As young participants become more involved in the movement, they encounter different types of injustices voiced by other groups. They hear activists sharing stories of the devastating climate impacts already experienced by communities, in places where sea level rise is inundating homes and droughts are killing livestock and causing starvation.  The climate justice movement emphasises how climate change exacerbates racial and economic inequality but frequently overlooks the ways these inequalities intersect with age-based disadvantages.
Forgetting that frontline communities contain young people, youth movements in developed countries like the UK begin to question the validity of their intergenerational injustice claims.  Many feel ashamed for having claimed vulnerability, given their relatively privileged position. Over time, they lose faith in their right to be heard. It would strengthen the entire climate movement if other climate justice campaigners more vocally acknowledged young people as a vulnerable group and shared their platform so that these important voices could better amplify one another.   With my own platform, I would like to say this to the thousands who went on strike. You matter. You have a right to be heard and you shouldn’t be embarrassed to speak out. Have confidence in your message, engage with others but stay true to your principles. Stick together and remember that even when you leave school and enter work – you’re never too old to be a youth advocate. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterInstead of utopia and paradise, Germany’s green dreams are delivering an ecological nightmare, turning the country into a toxic cesspool of lethal man-made biocides: botulism and now E-coli bacteria. Millions of consumers are now at risk.
A few days ago I wrote about how biogas plants are producing and distributing deadly botulism throughout Germany and how a thousand farms and thousands of heads of cattle have been infected, along with wildlife. Even humans have been infected.
Worse, the media and authorities are choosing to ignore the problem completely – and thus are putting thousands of lives at risk. Veterinary officials even refuse to ban the slaughter of botulism-infected cattle (to avoid the compensation of farmers by the state), and so the meat of the botulism-infected cattle is being marketed to consumers!
Now E-coli bacteria – likely from organic farms
Now there’s a new deadly bacteria released that is rapidly spreading rampantly all over Germany – E-coli bacteria. So far it has infected and sickened hundreds of Germans and has claimed the lives of 2 people. The online Bild daily here reports that on Saturday an 83-year old woman died of the “Killer-Bacteria E-coli” and that a 25-year old woman died yesterday in Bremen – showing the symptoms (tests must still be done to confirm the E-coli). Bild writes:
The E-coli bacteria are spreading at a rapid speed in Germany. About 300 people have been infected thus far, over 40 patients are suffering from the life-threatening hemolytic-uremic syndrome (HUS), which is caused by the intestinal bacteria. It leads to kidney failure, damage to blood vessels and anemia.
and
A spokesman at the Hamburg Health Dept.: ‘The situation is serious!’ “
The news of the E-coli epidemic is making headlines everywhere; authorities are in a state of alarm.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The bacteria cause bloody diarrhea and intestinal bleeding, and can lead to death if not treated in time.
Origin of the deadly bacteria
Officials are searching feverishly for the origin. Media reports and experts say the source is very likely from raw, unwashed vegetables. I immediately wondered if all that healthy organic food grown in nature without industrial processing could be the source. My suspicions seem to be well-founded. Bild writes:
The horrible suspicion: Contaminated manure, often used as fertilizer for organic vegetables could be the source!”
The Federal Institute for Hazard Assessment warned already back in January: ‘Through contact with feces, for example manure fertilizers, food plants could be burdened with E-coli.’ Martin Hofstätter, Greenpeace agriculture expert: ‘The manure is spread by the wind and lands on also on fruit and vegetables of neighbouring fields.’
It is quite possible that the contaminated manure is still finding its way onto fields.”
Someone ought to tell that Greenpeace “expert” that industrial food at least gets washed and cleanly processed according to strict quality standards, and not picked and handled by soiled hands and sold directly to consumers as “fresh and healthy”.
So much for the green paradise. Add this contamination to the list, which now includes mercury from  energy saving lights, botulism from biogas plants, and cadmium in solar panels. And let’s not forget the bird-shredding windmills that litter the landscape.
Also read here: http://www.thelocal.de/national/20110524-35217.html
Share this...FacebookTwitter "
"The equivalent of 1,145 truckloads of oil is stolen in Mexico per day from PEMEX – the state-owned petroleum company. That’s 146 billion Mexican Pesos (USD$7.4 billion) in lost revenue since 2016 – a significant hit for a country where 3.8% of GDP comes from oil exports. President Andrés Manuel López Obrador has introduced reforms to tackle the problem, including defining oil theft as a serious felony and releasing a new national strategy for oil production. The new strategy includes shutting off several major pipelines and working at reduced capacity until appropriate measures can be taken to protect them. While flow through the pipelines is halted, oil remains inside at a constant pressure. Theft causes pressure differences that actually help pipeline operators to detect where thieves are taking oil, but the stationary fuel has tempted more and more people to take risks.    A pipeline exploded recently after it was tapped by people trying to fill containers with oil. A spark ignited the fuel and killed more than 70 people. A few days later, on January 28, another pipeline exploded although, thankfully, without casualties. The government has since enlisted the military to patrol pipelines. Fuel shortages have gripped several areas of the country, but illegal tapping by ordinary people can only be blamed for about 20% of total theft. Corruption within the oil industry and organised crime make up the majority of the problem. PEMEX has already fired 100 workers for complicity in fuel theft schemes and many more are under investigation for their ties to organised crime and for sharing information about the location of pipelines.  A study from EnergeA, a consultancy hired by the Mexican Energy Regulation Commission, looked at the hydrocarbon sector’s security and found that drug cartels, militias, petrol stations, PEMEX staff and police have all been implicated as profiteers from bribes or the direct illegal sale of oil.  Many sell the oil to petrol stations at lower prices than official distributors. Their activities go beyond tapping and include the theft of barrels at PEMEX facilities, before distributing it for sale to the black market. People in Mexico have been generally supportive of the president’s measures to prevent theft, despite widespread shortages and explosions at pipelines where oil has stopped and tapping has occurred. However, political adversaries have tried to take advantage of the situation, such as former presidents Felipe Calderon and Vicente Fox, by demanding López Obrador change course. The president’s enduring public approval ratings of 57% suggest many citizens support a hard line on halting the supply of oil and making oil theft a felony. But to tackle thieves throughout the oil supply chain, Mexican authorities can learn from India, where theft has been substantially reduced.  Pipelines in India are routinely patrolled to monitor unusual activities and new technologies have been developed which allow engineers to detect even small signs of theft. New pipelines are also routed near highways or railway tracks to keep pipelines under close vigilance and ensure emergencies can be quickly addressed.    One gadget used in India is a handheld system which identifies leaks and illicit tapping to pinpoint the location of a theft, allowing pipeline operators to coordinate with local police to catch thieves in the act. Portable data loggers can be deployed and installed on pipelines where leaks or thefts are suspected.  Thieves learn to counter traps quickly, so innovative approaches which combine technology and an understanding of why and how people are stealing oil could bring permanent solutions in Mexico. Given the scale of the problem and how ingrained the “shadow supply” of oil is within Mexican society, focusing on social change is necessary too. The president has appealed to businesses, oil distributors and citizens to boycott the black market in stolen fuel, but transforming social practices will be challenging.  Young people could be educated about the negative effects of oil theft for society and the economy. These include fluctuations in the price of fuel, the operational costs for a public utility and the significant personal risk to thieves.  All of this will need support from strong, credible and transparent institutions, as corruption currently limits the authority of state organisations and the police and excuses the participation of private businesses and citizens, such as petrol station owners. There are many fixes for plugging leaks in the supply chain but stopping oil theft won’t happen without reliable governmental bodies. This includes regulators responsible for monitoring fuel production and organisations enforcing the legal supply of fuels to service stations. Without transparency and accountability in organisations such as PEMEX, their role as enablers of a shadow supply chain will continue. This demands rethinking corruption as a systemic problem within Mexico, one that is costing lives and goes far beyond oil. Introducing mechanisms which can help identify and report corruption could be a step towards broader social change."
"BP is to sever links with three US-based trade associations, including the country’s main refining lobby, because of disagreements over their climate-related policies and activities. The decision comes after the UK oil corporation’s new chief executive, Bernard Looney, set an ambitious target to shrink its carbon footprint to net zero by 2050. To achieve this it will have to cut more greenhouse gas emissions every year than the amount produced by the whole of the UK. BP said it would pull out of the American Fuel and Petrochemical Manufacturers (AFPM), following in the footsteps of Shell and France’s Total, which left the lobby group last April. It will also quit the Western States Petroleum Association (WSPA) and the Western Energy Alliance (WEA). Over the past six months, BP has conducted a review of how its climate crisis-related policies and activities compare with those of 30 trade associations. BP cited material differences in its views on carbon pricing in relation to the positions adopted by AFPM and WSPA. It will not renew its WEA membership because of significant differences around the federal regulation of methane. Looney said: “BP will pursue opportunities to work with organisations who share our ambitious and progressive approach to the energy transition. And when differences arise we will be transparent. But if our views cannot be reconciled, we will be prepared to part company. “My hope is that in the coming years we can add climate to the long list of areas where, as an industry, we work together for a greater good.” The AFPM president and chief executive, Chet Thompson, said: “We are certainly disappointed with BP’s decision. We do not believe that BP’s trade association report accurately reflects AFPM’s position and commitment to finding solutions that address climate change. “As an active member of our executive committee, BP knows full well that AFPM recognises that climate change is real and that we are committed to engaging on and developing policies that enable our members to provide the fuels and petrochemicals that humanity needs to thrive in a sustainable way.” BP has identified a further five organisations with which it is only partially aligned on the climate crisis, and has told them about these differences. They are the American Petroleum Institute, Australian Institute of Petroleum, Canadian Association of Petroleum Producers, National Association of Manufacturers and the US Chamber of Commerce. The American Petroleum Institute has lobbied on behalf of oil companies to weaken the landmark US environmental laws enshrined in the the 50-year-old National Environmental Policy Act, which green groups fear will increase greenhouse gas emissions and accelerate the climate crisis. BP also successfully lobbied US policymakers directly for the regulation to be weakened. A Guardian investigation revealed last year that five of the world’s largest oil and gas companies – ExxonMobil, Chevron, Shell, BP and Total – spend nearly $200m (£153m) every year on lobbying governments to block or water down climate policies. BP spent $53m, including a $13m campaign to successfully derail a proposed carbon tax in Washington state.  Mel Evans, a climate campaigner for Greenpeace UK, was not impressed. “Judge a company by the company they keep. BP are sticking with the American Petroleum Institute, the lobby group who wrecked Obama’s methane restrictions,” she said. “When BP remains part of a group whose position they claim to oppose, it’s easier to get clarity on what they really think by looking at where their money is going – more oil, more gas, more climate change. “This report appears to be tokenistic, inadequate and hypocritical – like all of BP’s climate plans that we’ve seen so far.”"
"
In case you are just joining us, here is some background on the story below. I know the identity of the mole. The ball is now in CRU’s court. Steve McIntyre reports below and throws down the gauntlet.

Met Office/CRU Finds the Mole
 
by Steve McIntyre on July 28th, 2009
More news on the Met Office/CRU molehunt.
Late yesterday (Eastern time), I learned that the Met Office/CRU had identified the mole. They are now aware that there has in fact been a breach of security. They have confirmed that I am in fact in possession of CRU temperature data, data so sensitive that, according to the UK Met Office, my being in possession of this data would, “damage the trust that scientists have in those scientists who happen to be employed in the public sector”, interfere with the “effective conduct of international relations”, “hamper the ability to protect and promote United Kingdom interests through international relations” and “seriously affect the relationship between the United Kingdom and other Countries and Institutions.”
Although they have confirmed the breach of security, neither the Met Office nor CRU have issued a statement warning the public of the newCRU_tar leak. Nor, it seems, have they notified the various parties to the alleged confidentiality agreements that there has been a breach in those confidentiality agreements, so that the opposite parties can take appropriate counter-measures to cope with the breach of security by UK institutions. Thus far, the only actions by either the Met Office or CRU appear to have been a concerted and prompt effort to cover up the breach of security by attempting to eradicate all traces of the mole’s activities. My guess is that they will not make the slightest effort to discipline the mole.
Nor have either the Met Office or CRU contacted me asking me not to further disseminate the sensitive data nor to destroy the data that I have in my possession.
By not doing so, they are surely opening themselves up to further charges of negligence for the following reasons. Their stated position is that, as a “non-academic”, my possession of the data would be wrongful (a position with which I do not agree, by the way). Now that they are aware that I am in possession of the data (and they are aware, don’t kid yourselves), any prudent lawyer would advise them to immediately to notify me that I am not entitled to be in possession of the data and to ask/instruct me to destroy the data that I have in my possession and not to further disseminate the sensitive data. You send out that sort of letter even if you think that the letter is going to fall on deaf ears.
Since I am always eager to help climate scientists with these conundrums, I’ll help them out a little here. If, prior to midnight Eastern time on Thursday, a senior executive of the Met Office or the University of East Anglia notifies me that I am in wrongful possession of the data and directly requests me to destroy my copies of the CRU station data in question and thereby do my part in the avoidance of newCRU_tar proliferation, I will do so.
I will, of course, continue my FOI requests since I do not believe, for a minute, that their excuses have any validity nor am I convinced that the alleged confidentiality agreements actually exist nor, if they exist, am I convinced that they prohibit the provision of the data to me.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94ab520d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

We now have a large tax cut, but little tax reform. The primary objective of the $1.35 trillion cut passed Memorial Day weekend seems to have been to maximize revenue loss rather than to minimize tax distortions and disincentives. Not only did we end up with a package containing little bang for a lot of buck, but one that disappears into the sunset after 2010. Congress will have no choice but to revisit the issue, and it had better not wait until 2009 when anxiety about future tax rules will become unsettling.



The trouble with the tax law of 2001 — like those of 1990 and 1993 — is that Congress had no thought of its ultimate destination. It rushed into the debate with far too many political objectives and far too few economic principles. Specifically, it avoided discussion of the two most serious questions in tax reform — what should be taxed and how. That is, what should be the tax base and the tax rate?



When it comes to the tax base, a large and growing majority of public‐​finance economists agree that tax policy should move toward taxing income devoted to consumption. That means ending, or at least minimizing, today’s multiple taxation of personal and corporate savings. This would include taxes on profits, dividends, interest, capital gains and estates.



Economists of all stripes also agree that high marginal tax rates are the main source of tax distortions that suffocate economic growth. On tax rates, Congress at least got the direction right, if not the magnitude and pace.



Yet most of the plan fell short. For example, suppose we counted the liberalization of tax‐​deferred pensions and lightened taxation of estates as steps toward alleviating overtaxation of savings. Even then, only $187.6 billion of the $1.35 trillion tax bill, or 13.9 percent, could be counted as progress toward a consumption tax base. What about marginal rates? Reduction in the highest four tax rates meant a static revenue loss of $420.6 billion over 10 years. With only 14 percent of the tax cut devoted to treating savers more fairly, and only 31 percent devoted to reducing significant marginal rates, that leaves an amazing 45 percent of the new tax cut in some miscellaneous category — disconnected from the two fundamental goals of tax reform.



If George W. Bush truly cares about reform, he must ensure that the next phase of the tax debate is focused on enacting those few changes that would make a big improvement in the tax climate for entrepreneurs and savers while costing the Treasury little or nothing:



Estate and gift taxes: The new law on estate and gift taxation inspires no long‐​term confidence. Instead of reducing the tax rate to 20 percent by 2008, Congress opted to keep the tax at 45 percent through 2009, while more than tripling the exemption. The result is an inefficient tax, one that does the most damage for the least loot. Congress also opted to keep the gift tax forever, a superfluous move if it is serious about killing the estate tax. The estate tax is repealed in 2010, but only for one year. In that same year, assets would begin to be inherited at their purchase price rather than market value (carryover basis), so heirs would inherit old capital‐​gains tax liabilities. The bookkeeping burden alone would be horrendous. If carryover basis were maintained after 2010, when the estate tax is automatically reinstated, then heirs could end up brutally taxed on both the value of inherited assets and old gains on those assets.



Corporate income: There has been no attention paid to the corporate income tax since 1986. Treasury Secretary Paul O’Neill says he’d like to abolish the corporate tax — heresy among politicians but not among economists. But taxing corporations as we tax partnerships would be a tough sell. In the spirit of moving close to a consumption tax base, the administration should instead push for an end to the complicated and capricious depreciation of plants and equipment. Congress has wisely allowed small businesses to write off increasing amounts for capital investments as soon as the expense is incurred. That would be equally appropriate for capital outlays by all businesses, big or small. Expenses are expenses — not income. Expensing would lose little revenue over the longer‐ run, because writing off more today means less in the future, and because improved investment means a larger economy and tax base.



Capital gains: Needless complexities and distortions arise from imposing a penalty rate of 35 percent on short‐ term gains. There is no justification for abusing tax policy to distort the timing of asset sales. There would be no revenue loss from taxing short‐ and long‐​term gains equally. Smart investors realize short‐​term gains only when they have offsetting losses. But the special penalty on realizing short‐ term gains seriously depresses turnover and liquidity, and therefore potential tax collection on trades that don’t happen.



In this year’s initial tax skirmish, the Bush White House willingly closed its eyes to some of the worst abuses of good tax policy. The Senate was almost invited to dump the brunt of its alleged revenue savings on “rich” families in the 31 percent tax bracket, whose marginal rate will now dip to 28 percent rather than to 25 percent. Since the White House voiced no strong concern about reducing estate tax rates, it became vulnerable to the symbolism of a one‐​year repeal in 2010 at the expense of high tax rates, retention of the gift tax, and a new tax on inherited capital gains.



Serious and lasting improvement in tax policy must first begin with a clear discussion and explanation of the ideal destination. Serious and lasting political leadership must refuse changes that do not move the country toward that destination, and that have no durability.
"
"
Share this...FacebookTwitterSometimes timing is everything. Yesterday we looked at how a climate scientist is busy making “masterplans” for transforming our society instead of studying the climate.Some people were not happy about my reminder of where this sort of dubious activity can lead. After all, planning societies and claiming absolute truth is not the job of  scientists, especially those as dogmatised as Schellnhuber and those at his Potsdam Institute For Climate Impact Research.
Organising society is best left to democracy, where everyone’s vote is equal, and must be so – no matter how ignorant Schellnhuber thinks the population is. It’s not perfect, but it’s the best system we’ve got. “Masterplanners” in history have invariably led to disasters. And, as much as people do not want to be reminded of it, a not so little concentration camp in Poland is an example in the worst extreme.
Today, the worst part is that all these “masterplans” are based on flaky, often times fraudulent and alarmist science – all designed to promote panic rather than reason. This has already led to disastrous results (biofuel induced global hunger or landscape desecration by windmills to name two) and will surely lead to even greater disasters.
Another example of a so-called masterplan that has just come to our attention is the latest European proposal to ban all fossil-fuel-powered cars from cities by 2050 – again all this with little or no thought about the potential conseqeunces this junk-science-based regulatory hyper-zealousness could have. The UK Telegraph here writes:
Cars will be banned from London and all other cities across Europe under a draconian EU masterplan to cut CO2 emissions by 60 per cent over the next 40 years.
These masterplans are designed to regulate and control our lives, and have nothing to do with saving the planet – all confirmed by Siim Kallas, the EU transport commission, who said of the masterplan:
Action will follow, legislation, real action to change behaviour.”
The Association of British Drivers reacted harshly to the proposed restriction on mobility, and rightly so. Hugh Bladon, a spokesman for the BDA said:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I suggest that he goes and finds himself a space in the local mental asylum. If he wants to bring everywhere to a grinding halt and to plunge us into a new dark age, he is on the right track. We have to keep things moving. The man is off his rocker.”
This can be said about all the social engineering master planners out there who have taken it upon themselves to tell the rest of society how to behave.
But it doesn’t stop there. The transportation masterplan also includes air transport. The plan provides for the end of cheap flights and has the target of forcing more than 50% of all journeys above 300 km to be done by rail.
Air travel is also a big target of the enviro-zealots. So it should not come as a surprise that a new study is just out claiming that air travel is “even worse for the climate than they previously thought” , this reports the Austrian Der Standard, which leads with:
Scientists calculate the impacts of air traffic in the sky and come to unexpected results. Vapour trails from aircraft and their impacts apparently have a far greater impact on climate than previously thought.”
The scientists who claim this are Ulrike Burkhardt, of the German Centre For Aviation And Aeronautics, and Bernd Kärcher – in a paper published yesterday in Nature titled: Global radiative forcing from contrail cirrus. In the abstract the authors claim:
We show that the radiative forcing associated with contrail cirrus as a whole is about nine times larger than that from line-shaped contrails alone. We also find that contrail cirrus cause a significant decrease in natural cloudiness, which partly offsets their warming effect. Nevertheless, net radiative forcing due to contrail cirrus remains the largest single radiative-forcing component associated with aviation.”
The timing of this contrail paper and the EU masterplan for transportation just couldn’t be more convenient.
Share this...FacebookTwitter "
"**With over a million new Covid-19 cases nationwide in the past week, according to Johns Hopkins University, public health officials are cautioning people not to gather in large groups this holiday season.**
Next week's Thanksgiving weekend poses a particular concern. Americans traditionally travel home to be with loved ones, taking part in meals, parades and shopping sprees.
The Centers for Disease Control and Prevention (CDC) strongly recommended that this year, Americans stay home and celebrate only with those they live with.
The nation's top infectious disease expert Dr Anthony Fauci urged Americans to ""think twice"" about holiday travel plans, adding that even ""innocent home gatherings"" with family and friends could result in several outbreaks.
As hard-hit regions have reimposed pandemic restrictions, even the Bidens and Trumps have made changes this year. President-elect Joe Biden revealed he and his wife, Dr Jill Biden, will have just one guest at their Thanksgiving dinner, while outgoing President Donald Trump and First Lady Melania Trump will remain at the White House for the weekend.
We asked people from around the country what changes they had made to their Thanksgiving plans and how they felt about 2020's pared-down holiday season.
_Daniela is a college student in Virginia, but is flying to meet family in two different states._
**How are you spending Thanksgiving?**
I'm visiting my dad's brothers who live in Atlanta and I'm trying to stay as safe as I can. We'll be here for a week, then next week I go back to Louisiana to be with my immediate family. I was a little worried because the last time I flew on a plane was June or July and there was not much separation between people.
I felt a lot more comfortable this time because there were seats between people and that helped out a lot. I've been wearing my mask, staying socially distanced from people I don't typically see very often and, on the plane, I made sure to wear a bit of a higher grade mask. I was debating whether to come home for Thanksgiving or for Christmas, and we decided it would be best to do Thanksgiving.
**What's the most difficult thing about Thanksgiving this year?**
Not being able to see my family as much as I would like. Keeping distance and making sure that I don't spread the virus is going to be a little hard. I was the one in the airport, so if we get Covid, it would probably be my fault. That added level of stress is going to be very difficult for me, but I'll just have to push through it.
**How are you feeling about the holidays this year?**
It's been rough and has put a damper on my life. On top of that, we had to deal with the election too, so it's been a lot. But it's getting better, and I'm feeling very happy now that I'm here with my family and get to see everyone.
_Rab is a Bangladeshi-born palliative care specialist raised in New Jersey._
**How are you spending Thanksgiving?**
We initially planned to see both sets of parents in New Jersey. Given that they are in their mid-to-late 70s, we have opted to stay in the Ohio area. I have two brothers and their families are there too.
We are masking up and limiting the number of people we are in contact with. If we do meet people, it's always socially distanced. If I walk with a friend outside, I do it masked up. And I expect lots of Zoom and FaceTime calls to friends and family.
**What's the most difficult thing about Thanksgiving this year?**
We are used to family gatherings. Not being able to spend holidays with family - especially not visiting parents - is hard, but it's the right and responsible thing to do. We want them around.
**How are you feeling about the holidays this year?**
I'm sure many people will be lonely without the physical contact of loved ones. It's sad, frustrating, lonely and exhausting at times. This should have been under better control sooner.
_Eliana is a professional dancer._
**How are you spending Thanksgiving?**
Nothing has changed too much. My family and I are meeting at one of my brother's homes and all of us are going to stay overnight because he's got enough room for us. If we do holidays at someone's house that's far away, we generally get a hotel, but because of Covid, we would rather stay together.
**What's the most difficult thing about Thanksgiving this year?**
There's nothing difficult in particular. Things just get a little more sad over the years, meeting together and seeing all the kids grow up.
**How are you feeling about the holidays this year?**
Generally, I look back at where I was this time last year, hoping I'm in a more progressive place in my life. This year, I feel pretty neutral because I'd love to have moved further in my career but that was taken away by the pandemic - an uncontrollable variable - but I'm just grateful to be here. So I'd say there's more positive feelings than negative, because at least we're here and we get to celebrate the holidays with people we care about.
_Meredith is an administrative assistant at a university._
**How are you spending Thanksgiving?**
We normally travel to my parents' house in Raleigh, North Carolina or to an extended family event with my husband's family on Long Island in New York. Our families have been understanding about our desire not to travel this year. My husband and I will have just one friend over, who has been part of our 'bubble' all year.
**What's the most difficult thing about Thanksgiving this year?**
Watching all of the people who are still planning big gatherings despite everything that's going on, and justifying it as ""but my [grandparent/parent] is old, maybe this is the last holiday we'll have with them!""
It very well could be, if you bring Covid to the table to share.
**How are you feeling about the holidays this year?**
Exhausted. Friends are putting up Christmas trees and lights already, using the excuse that it's been a slog of a year and everyone could use a little cheering up.
Seeing decorations in stores and aggressive holiday shopping campaigns earlier than normal definitely is not helping with my already-distorted sense of time's passage.
_Ryan is a graduate student at the University of Wyoming._
**How are you spending Thanksgiving?**
I am staying home and avoiding any unnecessary travel or contact with other people. Normally we travel to spend the Thanksgiving holiday with our family. We had planned to make the 12-hour road trip after self quarantining for two weeks. But even with making plans to never interact with other people on the drive, it just isn't fair to create the additional risk of a car accident which would put unnecessary pressure on the doctors and nurses at our hospitals that are already overwhelmed.
At this point the only respectful and responsible thing to do is to self isolate. It sucks, but in the end it is a small sacrifice.
**What's the most difficult thing about Thanksgiving this year?**
Hearing the disappointment in our parents' voices when we said we would not be coming home. I'm sad I will not be able to see my grandma. I miss being in their presence.
**How are you feeling about the holidays this year?**
Not well. This is a sombre time and I am feeling like the holiday season this year needs to be one of remembrance and reflection. In a sane world we would be laser focused on reducing transmission of this plague, making sacrifices now for the long term collective good.
We would have compassion and empathy for our fellow humans, honouring the heroes fighting this disease in the hospitals around the country."
"
Share this...FacebookTwitterBy Ed Caryl
There have been multiple studies of solar influence on global and regional temperature changes. Just of few of them are here, here, here, and here. This author made a contribution to the question here. Many of these studies show a some correlation of the sun’s output versus temperature, but most researchers think that this relationship is a weak one, contributing from one third to one half of the observed warming. So, from where is the remainder coming?
Figure 1: Heat map of Buffalo, New York, USA, NASA image.
Pierre and this author have written about urban warming here and here. It is clear from the infrared satellite photos by NASA that urban heat islands are both more wide spread and warmer that previously known. Particularly in the eastern half of the U. S., the heat islands are blending together, raising the temperature of the whole region. How much?
No one (to my knowledge) has researched what part of the global temperature rise is due to energy use. All energy use ultimately goes to heat. This is what causes the heat islands. Much of energy usage is immediately wasted as heat: cooling towers at power plants, automobile radiators, heat loss through home insulation, heat loss up the chimney, electric motor heat loss, heat from electric lights, are just a few examples of heat losses. Even energy used to transport things is ultimately lost as heat. Just moving something through the air, heats the air. For this reason, we can convert all the energy used into watts and calculate the temperature rise. In these calculations, the energy used will be considered over particular land areas.
The first area considered is the U.S.A. There are figures for the energy consumption in the U. S. in 2005, 29 Pwh (Petawatt hours, a PetaWatt is 1015 watts. That is 1 with 15 zeros. The area of the contiguous U. S. (the lower 48 states) is 8,080,464 km2. If we divide the energy used by the area, we get 3,589 Wh/m2. Divide that by 8766 hours in a year we get 0.409 W/m2, or 9.826 Wh/m2/day, as the average energy dumped into the environment in the U. S. in 2005.
The sun provides about 4.5 kW/m2/day on a horizontal, flat, black, surface at the average latitude of the U.S. The average albedo of the earth’s surface is 0.3, which means that on average, the surface will absorb 70% of the insolation (solar energy) that strikes it. This means that the effective heating will be 70% of 4.5 kW/m2/day, or 3,150 W/m2/day. The energy dumped into the environment by every American’s energy use is 0.312% of the sun’s energy. This will raise the temperature by 0.312%. The average temperature in the U. S is 11.6°C or 284.75°K. The temperature rise will be about 0.89°C.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As you can see on the temperature chart below from NOAA, this will neatly take care of the temperature rise seen in the last 25 years.
Figure 2. Source: http://www.noaanews.noaa.gov/stories2009/images/1208natltemp.png
What about the global picture? Figures are available for global energy consumption for 1988 through 2006. As most of this consumption is in the northern hemisphere, and that is where we see the most warming, the calculation uses the northern hemisphere land area, 100,228,500 km2. The same average insolation value will be used as in the U. S. example above, 3150 W/m2/day. Figure 3 is a chart of global and hemispheric temperature trends and the calculated temperature rise in the land area of the northern hemisphere due to energy usage. The temperature anomaly data comes from GISS/NASA here. The chart may underemphasize the temperature rise due to energy use because energy use is localized to a limited local areas in few countries: the US, Europe, including Russia, and China. The temperature data may also be overemphasized because the surface temperature measuring sites are at airports and other urban settings that are even warmer than the average location.
Figure 3. NH, SH, and Global temperature rise, and the rise caused by energy use (dark blue line).
Dr. Richard C. Wilson states that 50% of the temperature anomaly is due to total solar irradiance changes (TSI). Drs. Judith Lean and David Rind make predictions based on TSI along with ocean cycles. Neither mentions any influence from energy usage.
Here are two maps. The first is the lower troposphere temperature rise over the period 1978 to 2006, much the same period as the chart above.
Figure 4. Source: University of Alabama Huntsville.
The second map is of energy usage by country.
Figure 5. Source: Energy Information Administration, U.S. Department of Energy, 2006.
Though the energy usage map is very coarse, one can see that the maximum energy usage area roughly coincides with the same areas as the northern hemisphere temperature rise.
The planet has been heating in the last two hundred years. Some of that change in temperature comes from ocean cycles, some from the sun and its various influences, and some from man. Much of the anthropogenic (man caused) portion is simply energy use that has dramatically increased in the last fifty years. A thorough, honest, investigation needs to be done before we blame it all on CO2.
Share this...FacebookTwitter "
"The most recent 12% of time on Earth is a striking anomaly when compared with the great bulk of our planet’s evolution. After 3 billion years or more of Earth as a microbial world, cells found ways to grow and assemble in their millions to build eyes, guts, muscles, nerve systems with brains, skeletons and the rest of the complex structures that form the mobile, sentient, marvellously various animals that fill our now familiar world.  The speed of this transition, seen by geologists in the sudden appearance of complex fossils such as trilobites in layers of rock, seemed so shockingly abrupt that it amazed and worried even Charles Darwin. As this flowering of complex multicellular life is used to mark the beginning of the Cambrian Period of Earth time, 541m years ago, it has long been called the “Cambrian explosion”. We now know it was not quite so abrupt. The evolution actually took place in distinct stages over more than 30m years – the pioneering geologist Preston Cloud called it “the Cambrian eruption”. The period has, somewhat mysteriously, left behind more than its fair share of amazing fossil localities called “Lagerstätten”, where not only hard skeletons but soft and delicate tissues are preserved, to give a much clearer window on the totality of life than the usual fragments of shell and bone. The classic in this respect was long the Burgess Shale, high up on Mount Stephen in British Columbia, Canada, and mined for its wondrous fossils for more than a century. In the past three decades it has been joined by China’s Chengjiang deposits.  Together these have given a detailed picture from near the dawn of animal life in the marine realm (the land, then, was still largely barren): a wealth of arthropods, worms, lamp shells, sponges, chordates, and even some animals that still defy biological assignment. Now another Cambrian Lagerstätte has turned up – and the first results, just published in Science, suggest that it may rival those of the Burgess Shale and Chengjiang. It is also in China, in Qingjiang in Hubei province, and is about the same age as the Chengjiang, some 518m years old, and so in the early part of the Cambrian Period. Indeed, it formed on the same general stretch of continental shelf sea as the Chengjiang fossils, about 1,000km away, and seemingly in somewhat deeper waters. What is surprising is that not only are the fossils just as exquisite, but they represent quite different animal communities. The Cambrian seas were clearly more diverse than thought, even in those early days. Although the Qingjiang fossils do include many worms, arthropods and sponges, they also are prolific in animals possessing the most delicate and translucent of tissues, such as jellyfish, here preserved with mouths and tentacles, and their distant relatives “comb jellies”. These are rare in the Burgess Shale and Chengjiang deposits, but among the Qingjiang fossils there are many astounding examples, flattened on the shale surfaces and preserving fine details of these softest of anatomies.  The comb jellies may be the earliest form of animal – a title that they currently contest with the sponges. Both these kinds of animal are preserved here, in numbers and fidelity that may help resolve the dispute. Other strange and wonderful animals appear in the Qingjiang strata, and represent curious and perhaps profound trends in evolution. There are kinorhynchs, for example – these are the “mud dragons”, animals that are today obscure because they are part of the “meiofauna”, those tiny creatures that live between grains of sediment on the sea floor.  Here, three new fossil forms have been found, some up to 4cm long, rather than the contemporary sub-millimetre size. These Cambrian giants suggest that some of today’s meiofauna started off “normally” sized, and then became miniaturised – for good. It is a true cornucopia. How did it form? The same kind of quickfire preservation process is mooted as for the Burgess Shale and the Chengjiang fossils: the animals were caught up in mud slurries, and carried down to deep, oxygen-starved parts of the sea floor to be rapidly buried in the stifling mud. It makes sense – but then such conditions and processes persisted long after the Cambrian, and yet were rarely associated with such bonanza fossil finds. There is much that remains mysterious about the dawn of life as we know it – but the Qingjiang fossils, as we study them more, will slowly shed light on these enigmas."
"
Share this...FacebookTwitterWe’ve seen that kind of thinking before. Authoritarianism.
Forget democracy, human freedom and free markets. These concepts, which have made today’s human prosperity and long lifespans a reality for humans, wherever and whenever they have been given the chance to work, are upsetting a small but very elitist group of individuals who view these concepts as a threat to their world view.
German Hans Joachim Schellnhuber has a “Master Plan” for society. Source: http://nachhaltigkeit2009.commerzbank.de/reports/commerzbank/annual/2009/nb/German/3010/_innovations-chancen-nutzen_.html
 
The English edition of Der Spiegel has a recent interview with Prof Hans Joachim Schellnhuber, Director of the Potsdam Institute for Climate Impact Research. In this interview Schellnhuber announced that he would unveil his “Master Plan” for transforming society – one no doubt that suits his world view. In Schellhuber’s view, human society needs to be scaled back and managed by an elite group of “wise men” who know what is best for the rest.
Schellnhuber’s contempt for today’s organisation of western soctety is illustrated by his statements. For example Der Spiegel stumps Schellnhuber with a simple question:  “Why is it that your messages haven’t been all that well received until now?” Schellnhuber responds:
I’m neither a psychologist nor a sociologist. But my life experiences have shown that the love of convenience and ignorance are man’s biggest character flaws. It’s a potentially deadly mixture.”
Oh the contempt. So we are all too comfortable and ignorant. It’s time for “wiser men” to think for the rest of us.  Your message, Mr Schellnhuber, has not been well received because it is anti-democratic and authoritarian. Your kind of thinking is a threat to the principles of Germany’s Constitution and so belongs under government observation.
Schellnhuber indeed has a very poor understanding of history. History teaches us a different lesson – that it is not the combination of love for convenience and ignorance that is man’s greatest flaw, rather it is the combination of towering arrogance and ignorance that is man’s greatest flaw. That is the real threat to society. Schellnhuber also denies climate history and concocts his own fraudulent version to warrant an authoritative intervention.
The belief that the planet needs to be dictated by a higher authority is what makes people like Schellnhuber so dangerous to democracy. Their impatience and frustration with democracy, and their claim to have superior knowledge, remind us of others who have led us down very dark paths back in the 20th century.
These people are convinced they are all-knowing. Der Spiegel asks: “Do you feel that the government’s abrupt change of course in relation to its energy policy is adequate?” Schellnhuber replies:
No. It can only be the beginning of a deep-seated shift. The German Advisory Council on Global Change, which I chair, will soon unveil a master plan for a transformation of society. Precisely because of Fukushima, we believe that a new basis of our coexistence is needed.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A master plan for transforming society drafted by a climate scientist, a person who admits having no background in sociology. This plan no doubt will be asserted by authority, and not democracy. Schellnhuber and others have indicated many times that democracy is flawed and is a nuisance. It’s getting in the way of solving the “world’s crises” (like zero temperature growth over the last 15 years).
Well, here is a little history lesson for the all-knowing Herr Schellnhuber and his bunch, who are frustrated with democratic principles and individual freedom, on where this type of contempt can take us if left unchecked:

This is where people were turned into numbers. Into this pond were flushed the ashes of 4 million people. And that was not done by gas. It was done by arrogance. It was done by dogma. It was done by ignorance. When people believe they have absolute knowledge with no test in reality, this is how they behave. This is what men do when they aspire to the knowledge of gods…Every judgement in science stands on the edge of error, and is personal.”
=================================================
I’ve brought up this Ascent of Man clip before, and am bringing it up again, and will continue to bring it up in the future. Watch the entire following series Ascent of Man, Knowledge or Certainty:
Ascent of Man – Knowledge or Certainty
Other links:
EU master plan to ban cars from cities by 2050.
ALERT-German Climate Advisor proposes creation of a CO2 budget
Share this...FacebookTwitter "
"**Mink at all fur farms should be routinely screened for coronavirus, according to a leading scientist.**
A mandatory surveillance programme is urgently needed, with Denmark, which is culling all mink, acting as a warning, said Prof Marion Koopmans.
She pointed to a ""major concern"" that the virus could spread to wildlife via escaped mink.
And there were questions over whether mink played a role in the origins of Sars-CoV-2, she said.
Writing in The Lancet journal, Prof Koopmans, who has been leading investigations into cases in mink in the Netherlands, highlighted the risk of escaped mink transmitting the virus to other wildlife.
Speaking to BBC News, the head of the Erasmus MC Department of Viroscience said mandatory early warning screening for mink was already in place in The Netherlands, which should be made mandatory worldwide.
While human cases seen in mink farmers ""are not a major public health risk"", it is crucial to learn lessons from the pandemic, Prof Koopmans added.
""Animals and animal farms are an important source of food and income for many, but there are risks associated with large scale animal production and the increasing demand does require reflection,"" she said.
""This is not to point fingers to the animal sector, this is a joint responsibility for public health and citizens. There is no large-scale farming without large scale consumer demand.
""This is part of a much larger sustainability agenda. I really hope that is what we will retain from this pandemic: the need to seriously look at more sustainable production systems for the future. ""
According to the European Centre for Disease Prevention and Control in Solna, Sweden, Europe has an estimated 2,750 mink farms and produces more than 27 million pelts per year.
Denmark is culling an estimated 17 million mink, over fears the virus is mutating.
Infections have also been detected in mink in France, Spain, Sweden, Italy, the US and Greece, as well as the Netherlands, which will now ban fur farming by March 2021.
On Tuesday, the first coronavirus cases at mink farms in Poland were detected in the north of the country, according to the Medical University of Gdansk.
Last week, Ireland said it would cull mink at its three remaining mink farms, citing concerns over the mutated form of the virus detected in mink on a farm in Denmark. Tests found no cases in the mink, but culling was recommended as a precaution.
Mink appear particularly susceptible to Sars-CoV-2, which can spread rapidly in farms.
Sars-CoV-2 has the potential to infect a range of farmed and wild mammals, with opportunities for the virus to mutate, said Prof Christine Kreuder Johnson of the school of veterinary medicine at the University of California.
She told BBC News: ""A great deal (of) vigilance and monitoring of animal populations will be needed to understand genetic mutations and implications this could have for human vaccines.
""This is just another indication that we have lots of work to do to keep Covid-19 at bay for the long term.""
Follow Helen on Twitter."
"

Here’s a new list of bloggers who are citing, discussing and writing about Cato commentary and analysis: 



If you’re blogging about Cato, let us know on Twitter (@catoinstitute) or email cmoody@​cato.​org.
"
"
Share this...FacebookTwitter30 years ago Waldsterben (forest dieback) was probably Germany’s first post-war environmental hysteria to grip the country. Today we see that all the prophecies of doom were completely wrong.
The excellent Michael Miersch brings our attention to this oustanding arte Franco-German documentary called “The Forests are Dying Again“ (in German, and here in French), which takes a look back at one of the greatest environmental hysterias ever to grip a population: Waldsterben (forest dieback), a.k.a. acid rain.
The documentary also exposes the dirty tricks the media used to keep the hysteria alive (see 24-min. mark). There are so many parallels to today’s modern climate hysteria.
Again, back then there was “consensus”, all the scientists agreed, there was no denying the catastrophe, and politicians called it a grave threat that required immediate action. Fear gripped Germany. Environmentalists, union leaders, church leaders, citizens, politicians, etc. marched on the streets and demanded the government take action. The culprit was clear: emissions from industry and man were producing acid-rain that was chemically searing forests. At the 1:29 mark of the documentary:
The early 1980s, thousands of people took to the streets, an entire country is in panic, the German forest is dying. That’s for sure. But we alone are at fault due to our unbridled efforts to attain prosperity and progress. We treated nature like crap, and now there is nothing left to do but take it to the grave.”
Der Spiegel triggers the hysteria
The scare was first set into motion by Der Spiegel’s November, 1981 front page story called: “The Forests are Dying. Acid Rain Over Germany“. Soon all other media outlets fell over themselves to see who could produce the most sensational stories.
Der Spiegel wrote that the forest had only 5 years left. Stern, not to be outdone,followed with: “Acid Death” and claimed that the forest had only 3 years left.  Waldsterben remained the hottest story for years in the German press. The scare even served as one of the major springboards that launched Germany’s Green Party.
At the 3:08 mark, the documentary cuts back to present-day 2011 Allgäu, 30 years later, where we see the forests look completely healthy. “How can that be?” the documentary asks.
Rudy Holzbergercollected 150 media clippings about the tree-dieback hysteria, and has gone back and analyzed them. While some media outlets like Stern claimed the forest would die in as little as 3 years, all agreed on one thing, Holzberger says:
All of them said the forest would be dead at the latest by the year 2000.”
Holzberger then goes on to explain that the science behind the scare was flaky and thin. Sound familiar? Most of the forest dieback junk-science is traced back to University of Göttingen professor Bernhard Ulrich, who says at the 6:36 mark:
“There’s no doubt for those who are involved in the science the cause is air pollution, acid rain, and everything that comes with it.
We have to expect that after a warm and dry year it will lead to widespread forest damage and death.”
According to Professor Ulrich, German forests would soon appear as dead as those shown at the 7.50 mark of the documentary.
Later in the documentary, tree rings reveal that an even more widespread tree die-off occurred in 1947, and that the tree die-off in the early 1980s was nothing unusual and part of the natural cycle. The 1980s episode, however, showed how the media for the first time could drive an entire nation into mass panic.
Happening faster than anyone expected
The panic eventually spread into France (but to a lesser degree) thanks to assertions made by Professor Josef Reichelt, who claimed that French trees were dying off as well. But the French press ignored the story as a whole. Yet, there were still some kooks like Richard Kletty who claimed:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“It’s happening unbelievably fast. We know the resistance that trees have, and so it really surprised us how fast the damage is taking place and the trees are dying.”
Joschka Fischer
Today we hear the same about sea ice melt. Yet, the scare never took off in France as it did in Germany, where the topic was emotionalized rather than being based on science and reason. At the 14:20 mark the documentary tells us how the German Greens made the jump into the German Parliament, with a young Joschka Fischer (looks like him, anyway) marching in carrying a dead tree.
The all-knowing, bearded Greens protested the inauguration of Helmut Kohl, claiming he was dealing with the problem irresponsibly. The political payoff for the greens was handsome. As the forests appeared to be dying, Germany embarked on the path of turning “green”.
The forests then recovered, but the media ignored it
At the 23-minute mark, the documentary tells us that eventually by 1993 the trees, which go through natural cycles of losing needles and greening again, depending on rainfall, were back in a state of ruddy health and that there was no longer any danger of the once feared massive forest die-off. How did the media react to this news? At the 23:55 mark, Helmut Schulz says:
We made an analysis of the press to see how they reported on this. Of 54 daily newspapers, only 4 reported on the positive news. All the others, 50 newspapers, reported negatively.”
The media had no interest in an improving forest health – they wanted to remain stuck on Armageddon. Instead they rolled out more apocalyptic headlines. At the 23:42 mark, Holzberger shows some of the headlines: Stern in 1994: The Death Struggle of the Trees which included words like: “If Trees Could Scream, which described the death of trees in human terms.“It was complete nonsense”, says Holzberger.
Tree-dieback deniers got smeared
It was also a difficult period for scientists who did not share the apocalyptic views of mass forest die-off. In 1996 Professor Heinrich Spieker published a scientific assessment of European forests commissioned by a Finnish forestry institute. The report was called: Growth Trends of European Forests, which reached the conclusion: “The forest in Europe is growing faster and they are healthier”. This is not what the media wanted to hear. It contradicted prevailing dogma. The reaction from the media was harsh.
Here were some of the claims made by the media (see 26.46 mark), the Süddeutsche Zeintung:
EFI study is superficial and fundamentally flawed.”
and called the deniers:
Witch doctors and charlatans”
and one German activist group wrote:
Half of the financing came from the Finnish government, and that Spieker was married to a Finnish woman.”
Today, it’s clear that Heinrich Spieker was right, and that it is the slimy media who have egg on their faces. Indeed German forests are expanding 170 sq km annually. And again today in climate science, the very same newspapers and groups are at it again.
Ironically, today’s forest die-off is due to the green biofuels craze
At the 27-minute mark, the documentary focuses on today’s claims that climate change is threatening yet another forest die off. But as the documentary shows, forests are adapting as they always have, and that the Sahara is getting greener. The science shows that the warming temps over the last 30 years (likely due to ocean cycles) is making the planet greener, and not browner.
In the last segment of the documentary we see the real threat to forests – especially tropical forests. It is deforestation to make way for green bio-crop plantations – to grow crops that allegedly will save forests from climate change.
Share this...FacebookTwitter "
"**A man who asked police officers about coronavirus and what would happen if coughed on them, then sneezed and spluttered in the back of their car.**
Police said Thomas Coates, 21, had deliberately sneezed at the officers after he was arrested on 13 June.
Coates, from Wellesbourne, Warwickshire, has been jailed for five months after admitting two counts of assaulting an emergency worker.
Police said he put officers and their families' health at risk.
There was ""no indication the defendant had Covid"", Warwickshire Police said.
The incident happened after Coates, of Hotchkiss Close, was arrested over a separate offence. He asked the officers their view on coronavirus and what would happen if he coughed on them.
He then, in the car, ""deliberately sneezed and coughed"", Warwickshire Police said.
Coates had not been displaying symptoms and neither was anyone at his home, so in line with government guidance, officers were advised to look out for symptoms in themselves and self-isolate if they materialised, the force said.
At Warwick Crown Court Coates was given five months each for the two counts to run concurrently.
Head of local policing Ch Supt Ben Smith said: ""At a time when police officers are at the front line of keeping the public safe during the pandemic it is shocking that people think it acceptable to act in this way.
""Coates's actions not only put the officers' health at risk but the health of their colleagues and families.""
_Follow BBC West Midlands on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to:_newsonline.westmidlands@bbc.co.uk"
"
Share this...FacebookTwitterScott Portman
No, I’m not becoming a warmist or a tree-hugger. But I am pleased to give environmentally concerned citizens the microphone here. 
The following is from Scott Portman of Atlanta, who politely asked to have his essay published here. It’s about the EPA and regulating mercury. I’m in favour of reducing mercury emissions, as long as the benefits outweigh the costs. I think Scott here will very much appreciate your comments.
========================================
The Debate On Mercury Emission Standards
by Scott Portmann
The United States’ Environmental Protection Agency has recently proposed the first-ever national standards for mercury and other air pollutants. The agency’s proposed standards are meant to regulate coal fired power plants in the US. They currently believe that with the new regulations, public health will be dramatically improved on a global level; in the US alone, they project that 17,000 premature deaths from lung diseases, such as mesothelioma, will be prevented. The standards, in addition, will prevent a whopping 11,000 heart attacks and 120,000 cases of childhood asthma.
The administrator of the US EPA, Lisa Jackson, confirmed her belief on the subject, claiming in a statement:
With the help of existing technologies, we will be able to take reasonable steps that will provide dramatic protections to our children and loved ones, preventing premature deaths, heart attacks, and asthma attacks”
It is possible that these new standards might be a result of the recent pressure the EPA has come under, due to a lot of pushback from the Republican Party. They hold the belief that the EPA is hurting the global economy with their rigid regulations. In an attempt to try to reign in the EPA, Republican lawmakers have targeted the agency’s climate rules. In their attempt, these new mercury standards have come under fire, too.
Currently, the technology exists to make this environmental goal a reality. Just by installing the regulating systems, power plants could effectively lower a slew of harmful emissions. US President Barack Obama has even issued an executive order that mandates the EPA to make sure their regulations are cost effective and not overly burdensome to industry. In response to that, the EPA has claimed that their standards are so cost-effective that for every $1 spent, the public will see $13 in benefits.
As to be expected, there are opponents to the EPA’s mercury regulations. Some believe that the standards would impose major economic burdens to manufacturing companies, costing many people their jobs. In the current economic climate, they raise a legitimate concern. They believe that the expenses will be passed on to consumers, who will face higher electricity bills.
On the flipside, if the standards pass the public will most definitely see increased health benefits. Thousands will live longer, and even more will breathe easier. Perhaps most importantly, the environment will be safeguarded and it will be a step towards preventing climate change. Toxic mercury will be reduced from bodies of water, and as a result, fish will be safer to eat. With fewer illnesses, there will be fewer expenses due to hospital and doctor visits as well. The money saved from collateral costs will most likely outweigh any additional electricity costs. It just seems that the positive aspects of the mercury standards far outweigh the negatives.
==================================================
A health, safety, and political advocate with a passion for economics, Scott Portman is an aspiring journalist who currently resides in the South East United States
Share this...FacebookTwitter "
"

_Politico_ asks, “Was he convincing?”   
  
  
My response:   
  
  
In Copenhagen this morning, President Obama convinced only those who want to believe — of which, regrettably, there is no shortage. Notice how he began, utterly without doubt: “You would not be here unless you, like me, were convinced that this danger is real. This is not fiction, this is science.” The implicit certitude is no part of real science, of course. But then the president, like the environmental zealots cheering him in Copenhagen, is not really interested in real science. Theirs, ultimately, is a political agenda. How else to explain the corruption of science that the East Anglia Climate Research email scandal has brought to light, and the efforts, presently, to dismiss the scandal as having no bearing on the evidence of climate change? If that were so, then why these efforts, or the earlier suppression of contrary or mitigating evidence that is the heart of the scandal?   
  
  
We find such an effort in this morning’s _Washington Post_, by one of those at the center of the scandal, Penn State’s Professor Michael E. Mann. Set aside his opening gambit — “I cannot condone some things that colleagues of mine wrote or requested” — this author of the famous, now infamous, “hockey stick” article seems not to recognize himself in Climategate. That he then goes after Sarah Palin as his critic suggests only that on a witness stand, confronted by his real critics, he’d be reduced to tears by even a mediocre lawyer. One such real critic is my colleague, climatologist Patrick J. Michaels, who documents the scandal and its implications for science in exquisite detail in this morning’s _Wall Street Journal_.   
  
  
But to return to the president and his speech, having uncritically subscribed to the science of global warming, Mr. Obama then lays out an ambitious policy agenda for the nation. We will meet our responsibility, he says, by phasing out fossil fuel subsidies (which pale in comparison to the renewable energy subsidies that alone make them economically feasible), we will put our people to work increasing efficiency in our homes and buildings, and we will pursue “comprehensive legislation to transform to a clean energy economy.”   
  
  
Mark that word “legislation,” because at the end of his speech the president said: “America has made our choice. We have charted our course, we have made our commitments, and we will do what we say.” But we haven’t made “our choice” — cap and trade, to take just one example, has gone nowhere in the Senate — even if Obama has made “our commitments.” And that brings us to a fundamental question: Can the president, with no input from a recalcitrant Congress, commit the nation to the radical economic conversion he promises?   
  
  
Environmental zealots say he can. Look at the report released last week by the Climate Law Institute’s Center for Biological Diversity, “ _Yes He Can_ : President Obama’s Power to Make an International Climate Commitment Without Waiting for Congress,” which argues that in Copenhagen Obama has all the power he needs under current law, quite apart from the will of Congress or the American people, to make a legally binding international commitment. Unfortunately, under current law, the report is right. I discuss that report and the larger constitutional implications of the modern “executive state” in this morning’s _National Review Online_.   
  
  
There is enough ambiguity in the president’s remarks this morning to suggest that he may not be prepared to exercise the full measure of his powers. But there is also enough in play to suggest that it is not only the corruption of science but the corruption of our Constitution that is at stake.
"
"

 **S** o the great energy fight of ’01 is on. Conservatives are doggedly rallying around the newly released Bush energy plan while liberals are attacking it with relish. Unfortunately, the fight is nine parts political theater to one part policy — and that “one part” of policy is so pregnant with economic mischief and counterproductive rhetoric that it’s beyond me why conservatives are so determined to play the role the administration is casting for them.



First of all, why are free‐​market types cheering the introduction of a “comprehensive national energy strategy”? After all, conservatives didn’t cheer a “comprehensive national Internet strategy” when one was proposed by Al Gore, a “comprehensive national industrial strategy” when one was proposed by Clinton Labor Secretary Robert Reich, or a “comprehensive national health‐​care strategy” when one was proposed by Hillary Clinton. Conservatives as a general matter believe that the government ought to leave markets alone and that “comprehensive national economic strategies” are things that old Soviet commissars and young French socialists are in the business of promoting, not free‐​market American presidents.



“Energy’s different,” they say. Excessive regulation and environmental opposition have shut down energy production and delivered us into a mess that only a “comprehensive national energy strategy” can sort out. Really?



Without the guidance of a “comprehensive national energy strategy,” investors are currently pouring billions into the energy sector. For instance, we’re currently in the midst of a power‐​plant construction boom, with some 90,000 megawatts of new electricity capacity scheduled to come on line by 2002 and a staggering 150,000–200,000 megawatts by 2004. This will not only burst the electricity‐​price bubble but will probably produce an electricity glut in the near future. Similarly, so many billions are flooding into the natural‐​gas market today that futures contracts are being made at half the price of today’s wholesale spot price. And high gasoline profit margins are inducing foreign refineries to enter the American market for the first time in decades and bringing new investment in domestic refining capacity as well. Barring some unforeseen supply disruption in the refining sector, gasoline prices will actually begin to decline slowly but steadily as the summer wears on.



What about all those “Not‐​In‐​My‐​Back‐​Yard” activists supposedly blocking the new wires and pipelines necessary to get energy from producers to consumers? You can certainly make an argument that the real problems in the energy sector are delivery problems, not production problems, but it’s unclear whether NIMBY is at the root of it.



Incumbent utilities have little incentive to build new transmission lines that would make it easier for ratepayers to buy cheaper power from competitors in neighboring service territories. Nor do utilities have an incentive to invest in new power lines when the profits allowed them by the Federal Energy Regulatory Commission are too low to make those investments particularly worthwhile. And with transmission rules still up in the air and unsettled at both the federal and state level, regulatory uncertainty is likewise dampening investment.



Similarly, there is little evidence that investors have been inhibited from increasing pipeline capacity when profit opportunities present themselves. The Energy Information Administration notes that pipeline capacity “has grown with end‐​use demand, and as new supplies have developed, new pipelines have been built to bring this gas to markets.” The Gas Research Institute likewise concludes that “growth in pipeline capacity is not a constraint on growth in gas supply. If supply is available, history has demonstrated that the pipelines will be built as needed. It is simply an investment and engineering issue.”



If government’s not in the way, why then did energy prices shoot up in the first place? Well, energy markets, like most commodity markets, are subject to boom and bust cycles. Energy prices after adjusting for inflation have been plummeting more or less for 15 years. Investors took money out of production and exploration budgets because profits were hard to come by. The bust suddenly ended last year, catching almost everyone by surprise, and the boom is now on. Investors are scrambling to expand supply, but capital investments take time. Let me make this simple: High prices = high profits = increased investment = price declines. It might take some time to get from here to there, but government’s record in speeding up the process is abysmal.



Regardless of President Bush’s gloomy rhetoric, this isn’t the beginning of America’s descent into the long dark economic night unless the feds can somehow come to the rescue. There’s plenty of energy around for suppliers to get their hands on and plenty of reason for people to conserve in the face of high prices. Rather, we’re experiencing the economic equivalent of a low‐​pressure front that will soon pass (indeed, already is passing) as long as government doesn’t do anything foolish in the meantime.



So how does the administration’s plan rank on that front? While there are literally about a hundred proposals that call for the government to “consider” this, “examine” that, and “investigate the possibility” of the other thing, the concrete proposals within the plan’s 170 pages are few and far between. Let’s look at the highlights:



Drilling on Federal Lands. Even if you’re happy digging up the tundra, there’s little reason to think that drilling in ANWR will do much to bring down energy prices. Industry’s best estimate is that ANWR could produce about 1 million barrels of oil per day at its peak. That’s a 1.25 percent increase in global production that, all things being equal, would reduce world oil prices by about 10 percent, from $25 per barrel to $22.50. While that’s nice, it wouldn’t do much of anything to deliver America from the power of the OPEC cartel, particularly since OPEC’s likely reaction would be to cut its own production to maintain world crude prices at today’s levels.



The administration would also like to increase industry access to various fields on federal lands in the lower‐​48 and in the Gulf of Mexico, primarily to get at natural gas deposits. That’s fine, but again, it’s not as if, without those new fields, existing gas wells would run dry.



None of this is to say that the federal government shouldn’t be a more reasonable economic steward of public lands. But it is to say that the administration is overselling the benefits that those policies will deliver to energy consumers.



Federal Eminent Domain Power For Electricity Transmission. As noted above, there are lots of reasons why utilities aren’t investing much in new transmission. NIMBY is only one of those reasons — and perhaps not even the most important. Having the feds step in and force private property owners to cut deals they don’t want to make with power companies seems antithetical to an administration that likes to talk about its commitment to private property rights.



The administration’s energy plan calls for the feds to adopt incentive‐​based rate making for transmission investments in lieu of the present rate‐​of‐​return regulatory regime. This ought to help some, but a better idea is to remove rate caps on transmission charges entirely. Still, let’s wait to pull out the federal guns on private landowners at least until we know they’re absolutely needed.



Tax Incentives for New Energy Production. If you’ve got an energy lobbyist in Washington, has Dick Cheney got a sack of money for you! Everyone’s a winner: oil; gas; hydrogen; hybrid and fuel‐​cell vehicles; superconductors; landfill methane; coal (make that “clean” coal, the adjective that is de rigueur whenever the word “coal” is used by this administration); ethanol; nuclear fission; nuclear fusion; solar; wind; bus, truck and automobile engine manufacturing; fuel cells; biomass; industrial cogeneration plants; and producers of energy efficient this and that. With this blizzard of new federal research and development initiatives, accelerated depreciation allowances, production tax credits, consumption tax credits, and subsidies for energy businesses competing in foreign markets, don’t expect the tax code to get any more comprehensible or your tax burden to get any lighter anytime soon.



It’s unclear why we need to bribe investors with tax money to take advantage of profit opportunities, and government’s track record at turning dubious ideas that don’t attract private investment into wonderful new economic toys is pretty bad (remember synfuels?). But hey, corporate welfare is what makes the political world go around.



Energy Welfare. If you’re poor, might soon be poor, or live in the Northeast, the Bush administration feels you pain. More tax money to the notoriously wasteful Low Income Energy Assistance Program; more tax money for the Weatherization Assistance Program; and more money for the Northeast Heating Oil Reserve. For this we elect Republicans?



Regulatory Fine‐​Tuning. This is a story of the good, the bad and the ugly. The good: repeal of the antiquated Public Utilities Holding Company Act (PUHCA), which dictates both the organizational structure and permissible service territories of electric power companies; and expedited renewal of permits for construction of the Trans‐​Alaskan natural gas pipeline. The bad: rumblings about increasing Corporate Average Fuel Efficiency (CAFÉ) standards for automobiles, standards which are simply back‐​door taxes on big cars, SUVs and mini‐​vans; further federal prohibitions on energy “inefficient” (read “cheap”) appliances; and a directive to federal agencies to pursue international agreements to address global climate change (haven’t we had enough of that for a while?). The ugly: tighter regulation of power plant emissions of sulfur dioxide, nitrogen oxide and mercury. Is more environmental regulation of power plants really necessary or really helpful at the moment?



So what exactly have we got here? The political equivalent of a sugar pill. The Bush energy plan won’t do much good, but it won’t do too much harm either. And that’s pretty good for government work.



On balance, we’d be better off if the administration had not opened this can of political worms. Had the administration simply focused on tinkering with regulations where necessary and revising federal land‐​use rules where politically possible, we would not need to put up with so much chaff for so little wheat. Instead, we’re now in the midst of an empty but still white‐​hot argument about whether we should more heavily subsidize this rather than that. We’re also subjected to a bizarre debate about whether “the nation” (as if we have some command‐​and‐​control, Soviet‐​style economy) should invest more heavily in supply or whether “the nation” should invest more heavily in conservation.



And to make its case, the administration is finding it useful to dredge up ludicrous arguments, such as the horrific implications of importing oil or the apocalyptic consequences of having investors go about their business without some detailed, comprehensive federal energy planning document to guide them.



Free‐​market types have no business in this intellectual ghetto. Nor do they have any business promoting most of this interventionist, corporate‐​welfare agenda.
"
"
Share this...FacebookTwitterGermany will completely abandon nuclear energy and technology by 2022, this decided by Angela Merkel’s government. Merkel and German activist leaders think they can competitively power the country with windmills, biogas, solar, and blind faith. The last nuclear reactors so will go offline by the year 2022. Read here.China, seeing a golden opportunity, aims to capitalise on Merkel’s hasty, panicked decision and now hopes to lure German engineers and nuclear scientists to China in order to accelerate its own use of nuclear energy (hat-tip: The Liberale Institute here), so writes Der Spiegel here:
The People’s Republic wants to profit from Merkel’s nuclear power stop. Peking wants to attract researchers and employees from German power plants. The country has embarked without any hesitation on a path to nuclear energy – the Chinese all but exclude a disaster like that in Fukushima.”
China is not worried about nuclear accidents and safety issues. Gee I wonder why? I wonder if an unemotional look at the following graphic might have something to do with that:

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
China’s leaders are bewildered by Germany’s hysterical move. Der Spiegel quotes a CNEA deputy:
It is false that a country with so few resources of its own would abandon nuclear power, Deputy General Secretary of the Chinese Atomic Agency CNEA, Xu Yuming, told the Frankfurter Allgemeine Zeitung. His criticism also included an offer to the German nuclear specialists: ‘We invite the German specialists to research for us in China and to work. The German nuclear plants are mong the best in the world, the engineers and scientists have a great reputation’.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




China has ambitious nuclear energy plans for the future as it gets set to put the country on a rapid development course. According to the World Nuclear Association, 62 reactors are currently under construction worldwide, 27 in China alone. Here’s what the rest of the planet, outside of the Berlin Enviro-Wall, is doing.
Plants under construction (a few selected countries)
China: 27 reactors
Russia: 10
India: 5
Germany: 0
German nuclear engineers and scientists won’t have have any trouble finding work, that’s for sure. Many more reactors are being planned – 158 in total.
Reactors planned:
China: 50 reactors
India: 18
Japan: 12
Russia 14
USA: 9
Germany: 0
And many more are being proposed (324 reactors in all):
China: 110 reactors
India: 40
Italy 10
Russia 30
Ukraine: 20
Japan: 12
Russia 14
USA: 23
UAE: 10
Vietnam: 12
Turkey: 4
Poland: 6
Germany: 0
Not everyone is happy about Germany cutting and running on nuclear energy. Especially industry that relies on cheap and reliable energy are not pleased about it. For example, Daimler CEO Dieter Zetsche is not happy and is quoted in Die Welt here, calling it a “risky and emotional decision” that makes Germany a less attractive location for industry.
China has got to be laughing.
Share this...FacebookTwitter "
"

Argentina’s miseries now cry out in the headlines: riots and violence, a farcical procession of presidents‐​for‐​a‐​day, and the gathering doom of default and devaluation. But behind the headlines lurk deeper ills that gnaw away at the foundations of the country’s political and economic life. Those ills helped to bring about the current crisis, and they will persist long after the media spotlight now on Argentina fades away.



Argentina’s woes are many, but underlying them all is the dilapidated state of its political and legal institutions. According to an annual index of corruption levels published by Transparency International and based on surveys of business people, academics and risk analysts around the world, in 2001 Argentina ranked a dismal 57th out of 91 countries. Worse, in other words, than Botswana, Namibia, Peru, Brazil, Bulgaria, and Colombia, and on par with notoriously corrupt China. 



The same results came through in the 2000 Global Competitiveness Report, coproduced by Harvard University and the World Economic Forum, which surveyed business leaders from 4,022 firms in 59 countries on their perceptions of business conditions. Again, Argentina languished near the bottom: 40th for the frequency of irregular payments to government officials; 54th in the independence of the judiciary; 55th in litigation costs; 45th for corruption in the legal system; and 54th in the reliability of police protection. 



It wasn’t always this way. The disrepair of Argentina’s institutional infrastructure is a legacy of its Perónist past. Look, for example, at the crucial question of judicial independence. Prior to the descent into statism, justices of Argentina’s Supreme Court enjoyed long tenures undisturbed by political interference. At the beginning of Juan Perón’s first administration in 1946, Supreme Court justices averaged 12 years on the bench. 



It’s been downhill since then. Since 1960, the average tenure has dropped below four years. After Perón (he left the presidency for the second time in 1974), five of 17 presidents named every member of the court during their term, a distinction that had previously been limited to Bartolomé Mitre, the country’s first constitutional president (1862–1868). And so, while before Perón, it was typical for a majority of the court to have been appointed by presidents from the political opposition, that was no longer the case. The Supreme Court, the supposed bulwark of the rule of law, was reduced to a puppet of executive power. 



The pro‐​market reforms of the early 1990s brought little improvement. President Carlos Menem, who deserves credit for stabilizing the currency and privatizing industries, nonetheless persisted in traducing the integrity of the country’s institutions. Faced with a politically hostile Supreme Court, Mr. Menem responded with a court‐​packing scheme — he expanded the court from five to nine members and filled the new slots with political supporters. 



His transgressions did not stop there: Allegations of corruption swirled throughout his two terms in office. Those charges finally caught up with him in June of last year, when the former president was arrested for his alleged role in an illegal arms‐​shipments deal. But after five months of house arrest, Mr. Menem was set free by his hand‐​picked Supreme Court. 



Corruption in Argentina extends far beyond Buenos Aires. To get a first‐​hand look at the problem, I visited the northwestern province of Tucumán earlier this year. During the “dirty war” of the 1970s, Tucumán served as a refuge for pro‐​Castro guerillas and was roiled by bloody fighting. Today it is better known as home to the world’s largest producer of lemons, as well as a now‐​declining sugar industry, and its problems are more prosaic: bloated and corrupt bureaucracy, and a backward and unreliable legal system. 



The public sector in Tucumán, for example, serves primarily to enrich politicians and fund patronage jobs. Out of a formal work force of some 400,000, there are nearly 80,000 provincial and municipal government employees and another 10,000 federal government workers. Elected officials siphon off small fortunes for themselves: The annual salary for provincial legislators is roughly $300,000. 



Tucumán is by no means noteworthy for such abuses. In the impoverished province of Formosa on the country’s northern border, about half of all formally employed workers are on the government payroll, and many show up only once a month — to collect their paychecks. 



Such profligacy lies at the root of Argentina’s present financial crisis. Government spending as a percentage of gross domestic product climbed to 21% in 2000 from 9.4% in 1989 despite the fact that sweeping privatizations were alleviating significant fiscal burdens. 



And while the country’s mess may begin in the capital, free‐​spending provincial officials bear much of the blame as well. Operating expenses at the provincial level rose 25% from 1995 to 2000 even though inflation was nonexistent. The spending binge was financed by an unsustainable runup of external debt — the reckoning for which has now arrived. 



Meanwhile, as the public sector ballooned uncontrollably, vital government responsibilities went unfulfilled, among them the provision of a legal system that promptly and reliably vindicates the rights of the citizenry. As a result, the acute financial traumas that now beset Argentina are compounded by a business environment that is profoundly hostile to investment, dynamism, and growth. 



In San Miguel de Tucumán, the capital of Tucumán province, I spoke with Ignacio Colombres Garmendia, the head of a major law firm in town. “The legal system is absolutely vital for our region’s economic development,” he noted, “but the politicians are blind to it. It’s hard to see what doesn’t happen because of a bad legal climate, and so nobody knows about it. But every day I see deals collapse — I see potential investors who decide not to come to Tucumán — because of the legal risks. They call and ask me about this or that legal issue, and I have to tell them, and they say ‘Thank you very much’ and that’s the end of it. ‘The world is a big place,’ a client told me once, ‘and we don’t need Tucumán.’ ” 



It takes an average of five years to foreclose on a commercial mortgage in Tucumán. And given the punishingly high interest rates that prevail now in Argentina, delays like that can render even excellent collateral insufficient to cover the amount ultimately due. In a vicious circle, the risks caused by delay and uncertainty serve to drive interest rates up even higher. And, lo and behold, the net effect of a system that leaves investors and creditors so badly exposed is simple: less investment, less financing, and less growth and opportunity. 



It is fashionable now to blame Argentina’s problems on the free market. The country’s latest president, old‐​school Perónist and unabashed protectionist Eduardo Duhalde, has joined the anti‐​market chorus by vowing to break with the “failed economic model” of the past decade. But Argentina’s tragic crack‐​up occurred not because pro‐​market reforms went too far, but because they did not go nearly far enough.



A healthy market economy requires not just the absence of statist controls; it requires the presence of sound institutions. And although the reforms of the Menem era made strides toward meeting the former requirement, they ignored the latter altogether. Today Argentina is suffering grievously from that oversight. Until it is corrected and the country’s ramshackle political and legal systems are overhauled, there is little hope that a stable and prosperous Argentina can emerge from the wreckage. 
"
"
Share this...FacebookTwitterPortuguese website Ecotretas here informs us that the European carbon market, the EU Emissions Allowances, has crashed over 20% last week alone, see the following chart.
Carbon price per ton in euros over the last one year. Graphic source: http://www.eex.com
Read more about what is behind the crash at Ecotretas
In the scheme, carbon emission allowances, called EU Allowances (EUAs), are allocated to specific industrial sectors and cap the total level of emissions at levels which reduce over time. There are just over two billion allowances on issue, which are traded between emitters and other market participants on exchanges and via brokers.
The European Union started the market in carbon dioxide emissions in 2005.
The EU trading acheme applies to 7300 companies and 11,500 installations in sectors with high carbon dioxide emissions across the 27 nations of the EU. These include: energy utilities, oil refineries, iron and steel producers, the pulp and paper industry as well as producers of cement, glass, lime, brick and ceramics. Aviation. The scheme is regulated by the European Commission (EC).
First the science crumbles, then the costs explode, the illusions melt away, and finally the markets crash. But don’t expect Europe to change course. In socialist Europe it’s: “Let no economic suicide go unfinished!”
No wonder China doesn’t want any part of this scam, and so refuses to buy any Airbus planes from bossy Europe.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
Parts of southern Europe are experiencing drought conditions. So what does the European Union want to do about it? They want the wet European countries to use less water too.

Okay, it’s not a solution, but I guess it’s a way of showing solidarity with dry countries, or something. Call it Europe’s next folly.
The online Die Welt has a report here called “Billions For New Faucets“. Now that energy saving lights have been decreed by the EU Burgermeister-Meisterburger, now comes water-saving faucets. Die Welt writes:
In the coming years additional tens of billions of euros in extra costs may be levied on homeowners and tenants. The EU Commission wants to increase the efficiency of buildings with respect to water consumption by reducing it 30% using a new directive in member states. It is being considered to obligate homeowners and landlords to replace shower heads, faucets and toilets with new ones that have have considerably less consumption.”
This is for real. Got to hand it to the EU Commission – they really know how to come up with ways to harass and infuriate its citizens, and to interfere with their lives. For Americans and non-EU citizens, it will soon be coming to you too.
All of this is designed to benefit a few select companies, primarily manufacturers of high-end household fixtures who are having difficulty selling their high-priced wares due to the economic crisis in Europe and USA. This is going to be expensive for normal citizens. Die Welt writes:
Using a conservative figure of 400 euros per living unit, owners of the more than 25 million homes and apartments in Germany will have to fork out over 10 billion euros.”
If you do the math for all of Europe, you can estimate about €50 billion! And for what? For the luxury of having less water of course. Well, didn’t you know? Everyone dreams of having less water. Die Welt adds:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




With the planned regulation, Slovenian EU-Environment Kommissar Janez Potocnik wants to mainly fight the water shortage in southern Europe.”
Can someone tell me how using less water in rainy Britain is going to make things wetter in Romania? Die Welt takes a look at some water statistics in Germany and what a 30% reduction in consumption would mean.
‘Of the annually available 188 billion cubic meters of water in Germany, only 2.7 percent gets used by public water works,’ says Martin Weyland, head managing director of the Federal Association of Energy and Water Management (BdEW).”
Eventually, that 2.7% ends up going right back into the water cycle. And a 30% reduction would mean that Germany would use only 1.9% instead of 2.7% of the water it has available. The result: that little, meaningless dip in the statistics would cost €10 billion. Yes folks, the EU masterminds are indeed again at work.
Die Welt also writes that already many public wastewater utilities say their sewage systems are having problems because NOT ENOUGH water is being fed into them by households, and so the utilities themselves have to flush their sewers with fresh water from time to time. Weyland says:
This is the only way to prevent foul odors and damage to sewage lines from deposits.”
Die Welt also brings up that it is highly questionable whether replacing faucets, shower heads and toilets would lead to any savings at all. People will simply take longer showers, or flush several times. Moreover, wastewater utility companies would have to flush their lines out with fresh water more often.
Any other brilliant ideas Herr Kommissar?
Although the Die Welt piece has lots more interesting points, I don’t need to write on more about this; you all get the picture. Enough lunacy for today.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSteffen Hentrich at the  “Thinking For Freedom”, blog of the (Classic) Liberal Institute Friedrich Naumann Foundation For Freedom brings our attention to this Youtube Video by Ronald Bailey (Reason Magazine) und Julian Morris (International Policy Network) which features the Top 5 Environmental Disasters That Never Happened.

Enviro-leftists have a talent for concocting scare stories about mass environmental disaster, and thus stampeding the public into destrcutive policy-making that always ends up killing millions of poor people. Eventually, the scare story gets exposed as a hoax, and the media look like a bunch of thoughtless dupes.
You’d think these enviro-madmen would eventually learn something from their terrible mistakes. Well, you’d be dead wrong. In fact, they seem to get a kick out it, a ghoulish delight, and so they simply go on and concoct new schemes to stampede people into self-destruction.
The latest of course is catastrophic man-made climate change precipitated by the burning of fossil fuels. And now as man attempts to scale back use of fossil fuels, the poor once again are getting hammered the hardest.
I really believe too many of these enviro-crackpots are evil and diabolical, especially when you listen to how they talk about human population – referring to people as parasites, scourges – a disease to the planet. One could arguably call them “green genocidists”. Yes, I’m getting myself worked up. Here are Reason Magazine’s The top 5 diasters that never happened:
No. 5: Frankenfoods
Nonsense upon stilts.”
Think about golden rice, and all the children who have to hungry, or blind, or die prematurely because agricultural progress is irrationally impeded.
No. 4:  The end of biodiversity
70 to 80% of all animal species would be extinct by the year 1995.”
“Not based at all on evidence.”
Today, there are no signs of biodiversity shrinking.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




No. 3. The energy crisis
By the year 2000, if present trends continue, we will be using crude oil at such a rate that there won’t be any more crude oil left.”
“And of course what happened is that our government did all kinds of idiotic things under President Richard Nixon.”
Sound familiar? And who gets hit the hardest when energy prices go up? The poor of course. It ends up causing more hunger, disease, and ultimately premature death.
No. 2: “A Silent Spring” The Banning of DDT
There was a really apacolyptic vision that was frankly based on a flimsy and inaacurate representation of arginal science.”
“And those bans have led to some unpleasant and unintended consequences.”
“The result, well, now a million people a year are dying of malaria.”
No. 1: Malthusian famine
The battle to feed all of humanity is over. In the 1970s hundreds of millions of people are going to starve to death in spite of any crash programs embarked upon now.”
“Ehrlich was spectacularly wrong.”
“If we had followed his policies, we would have created the famines that he claimed were inevitable.”
“More people are being better fed today than at any time in human history.”
Wvery one of these problems was an iminent threat to life on the planet, promoted by “leading experts” who were certain, and insisted action had to be taken quickly. Many world leaders are now behaving like Nixon, and implementing idiotic policies.
The global warming scare is nothing new and will also end up in the graveyard of past scare stories as well. But in the meantime it indeed has the potential of “creating the famines that are claimed to be inevitable.” These professors like Hansen, Ehrlich, Schellnhuber, Mann, Jones, etc. are indeed dangerous and need to be reviewed by rational thinkers.
================================================
Here’s another example of a crackpot prediction: http://thegwpf.org/science-news/2861-un-predictions-a-the-eco-refugees-scare.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe German online Die Zeit here takes a look at the series of tornadoes that have ravaged the USA and conducted an interview with US meteorologist and Mississippi State University professor Grady Dixon.
Meteorology professor Grady Dixon: ""Terrible mistake"" to relate tornado up-tick to climate change. (Photo source: Mississippi State University)
Die Zeit asks the question: “Herr Dixon, is the number of such lethal storms rising in the USA?” Dixon replies:
No, to the contrary. Over the long term the number of deadly tornadoes has even dropped dramatically. […] However, we have to expect that more people will be hit by tornadoes in the future. Not because there are more storms, but because the population is growing and suburbs and cities are expanding. In any case, 2011 is an unusually violent tornado year and it is just a fluke.”
Dixon is also asked if climate change favors the creation of more tornadoes. Dixon answers:
Research results are mixed on this. […] But all indications show that it does not necessarily mean that tornadoes will be increasing in frequency.”
On the frequency of tornadoes, Dixon is also quoted by the English-language France 24 here:
‘It’s having to do with better (weather tracking) technology, more population, the fact that the population is better educated and more aware. So we’re seeing them more often,’ Dixon said.
But he said it would be ‘a terrible mistake’ to relate the up-tick to climate change.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




France 24 also quotes a FEMA official:
Craig Fugate, administrator of the Federal Emergency Management Agency (FEMA), also dismissed Thursday climate change as a factor in the deadly tornadoes: ‘Actually what we’re seeing is springtime,’ he said.
‘Many people think of Oklahoma as ‘Tornado Alley and forget that the southeast United States actually has a history of longer and more powerful tornadoes that stay on the ground longer’.”
Many weeks back I recall Joe Bastardi predicting a humdinger of a tornado season, and of course we now see that his warnings were spot on. This spike in tornadoes is not due to warmth, but to cooling brought on by La Nina, with cold northern air smashing into warm, moist southern air.
David Imy from the NOAA Storm Prediction Center in Norman, Oklahoma adds:
We knew it was going to be a big tornado year. But the key to that tip-off was unrelated to climate change: It is related to the natural fluctuations of the planet.”
A rare moment of sanity coming from the NOAA? Sorry Romm, but on this one you’re a lone fool (again) out in the desert.
========================================================
Unrelated: Benny Peiser brings or attention to news that a Global Climate Treaty Is DOA. Looks like Europe will be joining Romm out in the desert.
Share this...FacebookTwitter "
"**Here are five things you need to know about the coronavirus pandemic this Tuesday morning. We'll have another update for you at 18:00 GMT.**
Travellers who arrive in England from high-risk countries will soon be able to reduce their quarantine period by more than half if they pay for a Covid test after five days - and of course, if that test is negative. The new system will begin on 15 December and will cost between Â£65 and Â£120 per person. The travel industry welcomed the policy but described it as long overdue. The transport secretary said it would allow people to see loved ones and give a boost to business. Read more on the current quarantine rules.
We're expecting restrictions to be eased over Christmas, but discussions between ministers in the four nations are continuing over very practical concerns - the length of any relaxation, the impact on public transport, and what exactly constitutes a ""household"". Overall, the tone has been cautious - the PM called it the season to be ""jolly careful"". On Monday, he announced the new ""toughened"" tier system for England \- the BBC's Laura Kuenssberg says the real political test will come on Thursday when it becomes clear how many regions are in the highest level.
Every household in Northern Ireland is to be given a voucher worth about Â£200 to spend on the High Street, as part of the devolved government's economic support package. The country re-enters a strict lockdown for two weeks from Friday. Other measures include money to help older and disabled people with their heating bills, and cash for drink-only pubs. Read more on the new restrictions coming for Northern Ireland. Meanwhile, concerns have been raised about rising rates in certain parts of England, especially Kent, despite the ongoing lockdown.
Tomorrow, Chancellor Rishi Sunak begins setting out plans for the economy beyond Covid-19 - although such is the uncertainty that his lookahead, or Spending Review, has been limited to the next 12 months, rather than the usual three or four years. The economic shock of the pandemic has left the UK a poorer country, so what might Mr Sunak do? Our BBC Business colleagues take a look. The overseas aid budget certainly seems on course for a cut - find out more.
One thing you can start planning for Christmas is your TV viewing, and this morning the BBC has revealed its festive schedule. Staples such Doctor Who, Call the Midwife and Mrs Brown's Boys are all on the list, along with some new entries, including dramas Black Narcissus and The Serpent. BBC chief content officer Charlotte Moore said it had been ""a real struggle"" to make the usual range of festive shows during the pandemic, but the stars and crews ""pulled out all the stops"".
Get a longer news briefing from the BBC in your inbox, each weekday morning, by signing up here.
Find more information, advice and guides on our coronavirus page.
Plus, we now know four Covid vaccines have shown very promising results in final-stage trials, so how and when might a programme of jabs begin? Our health reporter Philippa Roxby explains.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"

In last week’s budget standoff, the headlines had President Obama repeatedly “summoning” the speaker and the Senate majority leader to the White House. “Why,” my colleague David Boaz asked, “doesn’t the speaker ‘summon’ the president to what I think is the real seat of government?”



It’s a good point: Whether or not you score the budget deal as a win for the GOP, the casual way the media describes the president’s role shows how dangerously far we’ve drifted toward one‐​branch rule.



Congressmen of old “would have received as a personal affront” any message from the president calling on them to change their position, Massachusetts Sen. George Hoar wrote in his 1903 memoirs: “If they visited the White House, it was to give, not to receive advice.” “In a republican government,” the Federalist explains, “the legislative authority necessarily predominates,” and is therefore most to be feared.





In the shell game of modern American governance, we’ve let ourselves become easy marks.



Today, not so much. Consider the controversial “policy riders” that almost sank the budget deal. The measure defunding Planned Parenthood got most of the coverage, overshadowing important provisions aimed at restricting the Environmental Protection Agency’s power to regulate greenhouse gases.



Seizing on the Supreme Court’s 2007 ruling that the Clean Air Act’s definition of “pollutant” was broad enough to encompass CO2 — a gas essential to life on Earth — the Obama administration has begun to “legislate” global warming policy in an end‐​run around Congress.



Whatever your views on climate change, you ought to find it unsettling that, here and elsewhere, most of the actual “law” in this country is crafted by unelected executive‐​branch bureaucrats.



But that’s where we are. A few months back, the _New York Times_ reported that some 230 health regulators had descended on Bethesda — paying double rent for office space so they could immediately begin drafting more than 300 rules implementing Obamacare. Thanks to “mega‐​bills passed by Congress,” the _Times_ explained, regulators are issuing “hundreds of sweeping financial and health care regulations that will ultimately affect most Americans.”



As at home, so too abroad: having ceded its constitutional power, Congress sits on the sidelines and carps while the president wages war.



Two weeks ago — nine days after we started bombing Libya — President Obama got around to explaining why. His televised address ran more than 3,000 words, but “Constitution” never appears — and “Congress” occurs only once: a passing reference to “consulting the bipartisan leadership of Congress.”



The president’s supposed to do more than “consult”; the real “decider” is Congress. Our Constitution grants the legislature sufficient power to make talk of “co‐​equal branches” a misnomer.



The constitutional scholar Charles Black once commented, “My classes think I am trying to be funny when I say that, by simple majorities,” Congress could shrink the White House staff to one secretary, and that, with a two‐​thirds vote, “Congress could put the White House up at auction.” (I sometimes find myself wishing they would.)



But Professor Black wasn’t trying to be funny: it’s in Congress’ power to do that. And if Congress can sell the White House, surely it can defund an illegal war and rein in a runaway bureaucracy.



If they don’t, it’s because they like the current system. And why wouldn’t they? It lets them take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the bureaucracy when it imposes costs in the course of deciding what those statutes mean.



But it’s our fault as well. In the shell game of modern American governance, we’ve let ourselves become easy marks. Unless and until voters wise up and demand accountability, Congress will continue to take our money and shirk its duty.
"
"
Share this...FacebookTwitterDie Welt, one of Germany’s flagship dailies, wrote up a comprehensive assessment of solar energy as a supply of energy to meet demand titled: The Great Solar Swindle.
The authors of the piece, Daniel Wetzel and Reto Klar, as suggested by the title, conclude that solar energy is a well-executed swindle that was and is promoted by slick industry lobbyists. Today solar panels and systems, massively subsidized by the government, are enjoying a boom in Germany. Solar energy is the number one desired form of renewable energy by Germans, according to a recent survey. The industry, needless to say, is making money hand over fist.
And because the solar energy boom is riding a tsunami of political populism, leaders are afraid of a political backlash should they try to put the brakes on the runaway gravy train.
Indeed behind all the solar energy brightness, the very dark clouds of economic and technical reality are gathering quickly. Despite all the billions in investment, solar energy in Germany today still only contributes a measly 3% of the country’s energy needs. It is many times more expensive than the conventional coal or nuclear power, and thus is causing rates for consumers to rise rapidly. And as Die Welt writes, it is not creating thousands of green jobs, except in Asia that is, where 70% of the solar modules now installed in Germany are made.
Expert council advises the German government to scale back solar installations
One big problem is that solar energy does not work well in often gray and overcast Germany, and so it makes little economic sense to add more solar capacity. Die Welt writes that an…
Expert council for environmental issues, a high level advisory commission for the federal government, recommends no longer forcing the expansion of photovoltaic, but rather to restrict them to very tight limits. Flensburg environment scientist Olav Hohmeyer, a member of the expert council, requests that the current rate of solar expansion be scaled back by at least 85% to only 500 to 1000 megawatts annually.”
Clearly the government is beginning to see that the power supply is becoming vulnerable and is at risk. When the power supply is at risk, then so are the consumers and industry who need a steady supply that can be relied on. Costs and feasibility are now under hefty criticism. Die Welt writes:
The Rhine Westphalia Institute for Economic Research (RWI) says that because of the ‘hype surrounding photovoltaic’, a growing cost tsunami will hit Germany.
and later quotes Thomas Bareiß, energy policy coordinator of Merkel’s CDU/CSU faction:
What is taking place here makes no economic sense and is socio-politically irresponsible.”
The authors list 10 reasons why solar energy needs to be re-evaluated:
1. It is not cheap.
Just for the modules built by the end of 2010, the German consumer will be saddled with pure subsidy costs, or so-called ‘solar debt’, to the tune of € 81.5 billion, which will have to be paid over a period of 20 years.
According to calculations by the RWI, German consumers will incur another €42 billion in costs by 2020.”
Add necessary grid expansion to the cost calculation and costs explode conservatively beyond $200 billion!
2. Nuclear power is not expensive
Cheap energies like coal and nuclear are being  forced out of the market and replaced by expensive alternative energies. The cost of the additional infrastructure needed by wind and solar will make nuclear and coal power look like a real bargain in the long run. And then consider plans by the EU to go ahead with Desertec, which is estimated today at €400 billion, and whose price tag will surely skyrocket over time. Worse, it will be located in the Sahara, i.e. in countries that are hardly stable.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




3. There’s no demand for it.
“The solar lobby wants to install 70 gigawatts by 2020.” The problem is that there will be little power generation in the wintertime, and all the surplus generation in the summer time will have to be given away to foreign countries.
4. Energy independence is a fairy tale
At Germany’s latitudes, peak production capacity of solar modules are only possible 875 hours of the 8760 hours in a year. The rest of the time is night, bad weather or winter. That means solar panel owners are forced to draw power from the rest of the grid 9 of every 10 hours, like everyone else has to.”
5. Local communities are being weakened
Energy independence and local production is supposed to strengthen the community, but the opposite is true. For example communities with hydro-power do not need solar or wind power, and so cannot accept it.”
But now public utilities have to invest millions of euros in additional equipment just so that a few dozen people can earn a few thousand euros with solar power.
6. The problem of storing power
Solar panels often produce energy when you don’t need it, or produce nothing when you need it the most. If only there was a cheap way to store the energy. Boston Consulting Group (BCG) says Germany’s current storage capacity using pump reservoirs is 7 gigawatts only.
To be able to supply enough energy for one week without wind or solar and without conventional power plants, 1260 pump-reservoir power plants of the type used in Goldisthal would be needed. ‘Ignoring that Germany has no suitable locations for such facilities’, say Bode and Groscurth and write ‘the costs per kilowatt hour would be astronomical because most of these systems are operated only for a few hours each year’.”
7. Hightech industry – but not in Germany
Global market share of German-made solar cells dropped last year alone from 15.4 to 9.7%. During the same short time period, China’s global market share climbed 25% to 48% overall. Promoted by cheap credits from the Chinese state banks, Chinese manufacturers such as Yingli, Suntech or JA Solar are snapping up complete production lines that use the latest western technology.”
8. Job engine solar industry is a myth
Even using the rosy figures from the German solar industry, the number of jobs by 2020 will not only not grow, but will even shrink.”
9. The solar boom has been a big success – for a few lobbyists.
The rest will have to pay through the nose.
10. Contribution to climate protection is insignificant, and costly.
Photovoltaic is the most expensive way of climate protection. Scientists of the International Energy Agency (IEA) or the RWI have calculated how much it costs to prevent one ton of greenhouse gas CO2 from be emitted from a fossil fuel plant by other energy sources. The result: The CO2 prevention cost for photovoltaic was a record high of €648 per ton.”
Share this...FacebookTwitter "
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. How do creatures living in the deep sea stay alive with the pressure? – Torben, age eight, Sussex, UK. Hi Torben,  This is a great question – thank you so much for asking it. The deep sea is a very difficult place to live. There is no light, it’s cold, there’s not much oxygen and little food – and, as you rightly point out, the creatures that live there have to deal with the enormous pressure of the water above.  In the deepest part of the Atlantic, the pressure can be 840 bars – that’s about 840 times the pressure we experience at sea level. At Challenger Deep in the Mariana Trench – the very deepest part of all the world’s oceans – the pressure may be 1,000 bars or more.  But the creatures that live in the deepest parts of the ocean have special features, which help them deal with these tough conditions – including the crushing pressure.  When you dive to the bottom of a deep swimming pool, you might start getting a painful or unpleasant feeling in your ears and sinuses. This is because they contain air: that feeling comes from the air sacs in your body being squashed by the pressure of the water.  Fish living closer to the surface of the ocean may have a swim bladder – that’s a large organ with air in it, which helps them float up or sink down in the water. Deep sea fish don’t have these air sacs in their bodies, which means they don’t get crushed.  The deepest dwelling species of fish, called the hadal snailfish, can be found at depths of about 8,200m.  But having a body with no air cavities will only get you so far, since high pressure can also destroy the very structure of molecules – the tiny building blocks that make up all matter.  


      Read more:
      Curious Kids: is everything really made of molecules?


 To help with this, deep sea creatures have “piezolytes” – small, organic molecules which have only recently been discovered. These piezolytes stop the other molecules in the creatures’ bodies, such as membranes and proteins, from being crushed by the pressure (though we’re not exactly sure how, yet).  Another interesting thing about piezolytes is that they give fish their “fishy” smell. Shallow water species have piezolytes too, but deep sea species have many more – so deep water species would smell much more fishy.  This molecule is only effective up to certain depths though; as the water gets deeper, the pressure becomes too much, even for snailfish. Tiny organisms called microbes have been recovered from the very bottom of the Mariana Trench, and they have peizolytes to help protect them, too.   While some animals live full time in the deep sea, others just visit. Species such as Cuvier’s beaked whale commute between the surface of the water, to breathe, and depths of over 2,000m, to feed.  These whales breathe air, but their lungs are collapsible, so they don’t get crushed when the whales dive into the deep sea for almost two hours at a time.  When diving, these whales store the oxygen from the air they breathe in their blood and muscles. They can do this because they have higher levels of haemoglobin and myoglobin molecules – which are used to store oxygen – than other whale species. Cuvier’s beaked whales can also reduce their heart rate and temporarily stop the blood flowing to certain parts of the body, which helps the oxygen to last longer.  So, there are a few different ways creatures can survive in the deep sea, depending on whether they are just visiting, or live there all the time.  There’s one last thing to think about: it’s very difficult for scientists to study deep sea animals, as they tend to die when they are brought to the surface – so there might be many other remarkable features we don’t yet know about.   More Curious Kids articles, written by academic experts: How does heat travel through space if space is a vacuum? – Katerina, age ten, Norwich, UK. What makes a shooting star fall? - Katelyn, age seven, Adelaide, Australia. What causes the northern lights? – Ffion, age 6.75, Pembrokeshire, UK."
"**The number of patients treated within 18 weeks of referral has fallen by more than half compared with the same time last year, figures show.**
Public Health Scotland (PHS) data from September shows fewer than 30,000 patients were seen within 18 weeks, down from about 73,000 a year ago.
However, the figure has been growing since July.
The Scottish government said NHS services were now being resumed with a focus on those who needed urgent care.
Health boards were asked to suspend all non-urgent elective treatment in mid-March as the NHS was put on an ""emergency footing"" to cope with the pandemic.
It has led to a backlog of patients awaiting treatment in Scotland.
The latest PHS figures cover the three months until the end of September 2020. The statistics cover all health boards apart from NHS Borders, NHS Grampian and NHS Lothian, which have been unable to provide figures.
The figures show that the number of people waiting more than six weeks for key tests to diagnose conditions including cancer is three times higher than the same time last year, rising from 15,509 to 47,968.
However, this figure has also been falling from a high of 66,726 at the end of May this year.
Scottish Labour said the waiting times for conditions such as cancer were ""unacceptable"" and risked a ""tidal wave of future health problems"".
The party's health and social care spokeswoman, Monica Lennon, said: ""SNP ministers have been warned for months to bring in routine testing for all healthcare workers and put measures in place to maintain Covid-safe spaces in our hospitals.
""Scotland is facing a tidal wave of health problems and the SNP government must urgently start giving our NHS the support it needs in order to prevent unnecessary deaths and chronic illness.""
The Scottish government said its ""guiding principle"" during the pandemic had been to keep as many people safe as possible.
A spokesperson said: ""Services are now being resumed, with a focus on patients who need urgent care, including cancer treatment, to ensure they are treated as quickly and safely as possible.
""To support this, the health secretary published a Clinical Prioritisation Framework last week to help boards prioritise care for those patients with the greatest need.""
More than 4,500 outpatients have now been seen at NHS Louisa Jordan, with a wider range of non-Covid healthcare being considered at the temporary hospital in Glasgow, the spokesperson added."
"
Michael Ronayne writes:
To the right of the burned out pixel, a second Sunspot group, with two spots, is forming which can be seen in this image:

The burned out pixel between the two groups is a fairly common issue with SOHO, and they routinely “bake” the sensor to get rid of them. Sometimes people mistakenly interpret them as sunspots in this new age of counting sunspecks.
The way to determine if it is a burned out pixel or not is to look for other off-colr pixels immediately arround it. If the pixel stands by itself, it is a burned out pixel.
So far these have not been assigned a number. They are just barely what one would call sunspots and my bet is that much as we’ve seen before from SC24 specks, they will be short lived, probably 48 hours or less.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95926819',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Yesterday, 215 scientists released a petition in Bali — site of a global confab to talk about whether and how we should talk about a future treaty to reduce greenhouse gas emissions — that “begs” the world to cut greenhouse gas emissions in half by 2050. I was quoted in an AP story on the matter to the effect that scientists are in no position to intelligently dictate such a policy. And as expected, some in the blogosphere howled.   
  
  
I do not believe in leaving public policy to “guys in white coats” — in any discipline. And that’s not necessarily a proposition that vitiates against environmentally‐​friendly public policy. Climate scientists do not have the training to tell us whether the costs associated with reducing greenhouse gas emissions are less than, equal to, or greater than the costs of business as usual. And that’s something you would want to know before signing off on greenhouse gas emission reductions. When climate economists have explored that matter, they find little to support such emission reductions even if we accept the prognostications about the future coming out of the IPCC.   
  
  
Likewise, economic calculations about the same are heavily predicated on how you feel about future costs and benefits. If you believe in valuing dollars and lives in, say, 2150 as much as you value dollars and lives today, then it’s hard to accept IPCC reports and not conclude that GHG emission cuts pass a cost‐​benefit test. If you apply a discount rate of, say, 3, 5, or 7 percent, then it’s hard to accept IPCC reports and not conclude that GHG emission cuts don’t pass a cost‐​benefit test. But how you value the future is subjective, and economists have no objectively “better” preference regarding that matter than you or I.   
  
  
Many have argued that we should value our great grandchildren’s lives and money as much as we value our own. Fine — there is nothing objectively wrong with that belief. But if you do, hand in your Rawlsian membership card. That’s because you’re endorsing a policy that will transfer wealth and well‐​being from the relatively poor (us) to the very rich (them). That is, even if the Stern Review is correct about the economic costs of climate change, real per capita income in developing countries will be higher than that of the developed world today by 2100. Moreover, if you value the future every bit as much as you value the present — and thus embrace, say, a 0.1% discount rate — then simple math suggests you ought to be saving just about everything you earn.   
  
  
I do not believe that “the experts” in any field should be dictating climate policy because there are plenty of important value judgments built in to those policies and experts however defined have no objectively better values than you or I. I do believe, however, that any serious reflection on the ethics of reducing greenhouse gas emission will find that the case for such a policy is harder to make than you might think, even if you accept what the IPCC is telling us.
"
"

This week President Barack Obama will open the doors to Hu Jintao, President of China. The visit will include an arrival ceremony, a joint press conference, and a glitzy state dinner. In addition to highlighting the importance of the relationship between the world’s two largest economic powers, which have become so central to growth of the global economy and the stability of the international system, President Hu’s meetings with President Obama will provide an opportunity to press the reset button on Sino‐​American ties.



The relationship has been strained since President Obama’s visit to China in November 2009: the tensions during the UN Climate Change Conference in Copenhagen, and in the aftermath of Obama’s meeting with the Dalai Lama, his decision to authorize $6.4 billion in military sales to Taiwan. Then there have been continuing American complaints over allegations that China was manipulating its currency, Chinese concerns over U.S. naval and air military exercises with South Korean forces in the Yellow Sea, and American opposition to China’s newly assertive claims to disputed waters in the East and South China Seas.



The meeting also comes as the two nations are recovering from the global economic crisis and reassessing their geo‐​strategic interests in East Asia and elsewhere. In Washington, Beijing, and other world capitals, there is a recognition that China, with its huge surpluses, and the U.S., with its gigantic deficits, need to cooperate in order to rebalance the global financial system and avoid another meltdown. At the same time, traditional U.S. allies in East Asia have been expressing support for continuing American military presence as a way of counter‐​balancing China’s growing assertiveness.



So much has been said and written about the evolving strategic and economic ties between China and the U.S., with pundits drawing a variety of historical analogies to apply to what has been described the “world’s most important relationship.” It could determine whether the first part of the twenty‐​first century will see the continuing expansion of the economic globalization and international peace, or whether Beijing and Washington are doomed to relive the Cold War that had existed between the U.S. and the former Soviet Union – or worse, that the economic and military tensions between two global powers will resemble those between Germany and Britain on the eve of World War I: a rising power (Germany then; China now) that is perceived to be challenging the existing pro‐​status quo power (Britain then; the U.S. now).



No one expects one visit to help provide clear answers to these and similar questions, and it is quite possible that – contrary to the conventional wisdom, preoccupied with the notion that an insecure “declining” America and an overconfident “emerging” China are bound to come to blows in the near future – neither Washington nor Beijing are expecting dramatic changes in their relationship anytime soon. In fact, Americans are trying to recover from the Great Recession and to end military quagmires in the broader Middle East, and the Chinese are managing the consequences of their dramatic and yet risky economic growth; both sides may be interested in steadying their relationship instead of rocking the boat.



The public statements made by Presidents Obama and Hu suggest that the two are hoping to stabilize the relationship and take steps to avert potential crises over issues like China’s overvalued currency or Taiwan. But on both sides there are political and bureaucratic forces that do want to pick a fight with the other side. Chinese nationalists believe that the U.S. is trying to contain their country’s rise to economic and military power by pressing Beijing to re‐​value the yuan and to resolve their territorial conflicts with their neighbors.



Mirror imaging these fears are the suspicions of those Americans who believe that China is using a mercantilist strategy aimed destroying the American economy while building up their military as part of an effort to get U.S. forces out of Asia. And without a clear sign that the leaders in Beijing and Washington are in control in managing the bilateral relationship, these mutual fears could force both sides into a widening insecurity trap and ignite more confrontation in the future.



This danger was on display this week on the day when – just as U.S. Defense Secretary Robert Gates was meeting with Chinese President Hu in Beijing – China’s new J-20 stealth fighter took what was believed to be its maiden test flight. Reports suggested that that was an attempt by the Chinese military leaders to demonstrate Chinese might to Gates, displeasing President Hu and his political leaders, who were interested in patching things up with the U.S. and avoiding any tensions before Hu’s State visit to Washington.



Similarly, while President Obama and his top aides have resisted pressure from protectionists and China bashers on Capitol Hill to punish the Chinese for refusing to revalue their currency, there is little doubt that if the U.S. unemployment rate remains high and the American economic recovery slows down, Congress will end up embracing a more confrontational approach towards China and demand that the administration impose economic sanctions on it. And that could result in an all‐​out trade war between the two countries. But the last thing that the Obama Administration officials need now as the U.S. economic recovery is gaining some momentum is new economic tensions with Beijing.



It is unlikely that the U.S. and Chinese leaders will be able to come up with detailed plans to rebalance the financial flows between the two economies. That will require politicians on both sides to make difficult decisions; they must change current patterns of spending and savings, making it possible to slash U.S. debt and increase Chinese domestic consumption. More likely, the visit will focus on Chinese willingness to remove existing barriers to American exports and provide stronger protection for intellectual property rights.



The U.S. and China will likely provide more specific details on how they plan to implement the 2009 Joint Statement issued by the two sides during Obama’s visit to China, which committed the two governments to expand bilaterally in areas such as science and technology, clean energy, civil aviation, agriculture, public health, space science, and cultural and educational exchanges. Some progress in these areas was made in the Joint Commission on Commerce and Trade last year.



Each leader needs to come out of the summit able to demonstrate to an anxious public that the relationship between the two countries is advancing each nation’s economic interests – helping to accelerate U.S. economic recovery and create more American jobs while providing more momentum for China’s economic growth – and showing that the relationship does not amount to a zero‐​sum‐​game. That will most effectively diminish the influence of both the China‐​bashing crowd in Washington and the anti‐​American nationalists in Beijing.
"
"Of all the world’s great natural predators, hyenas are surely among the most maligned. They are often seen as good for nothing scavengers, the bullies of the African plains, laughing as they gang up to steal hard won meals from their more majestic competitors.  Footage of spotted hyenas (the largest and most familiar of the four hyena species) seen on nature documentaries often strengthens this notion, as does their portrayal in stories such as Disney’s Lion King. But where does such a negative view come from? And is it justified?  The origins of the hyena’s reputation may lie in the role it has played in African folklore. In Tanzania, witch doctors often kept spotted hyenas in cages and were said to ride on their backs at night. There is also an old superstition in that country that if a child is born at night while a hyena is crying, he or she will grow up to become a thief.  Then there is the hyena’s association with scavenging, and a widely held traditional belief that hyenas are there to clean up rotting carcasses – including human ones. Indeed, for the Masai of Kenya something was believed to be wrong with a person if their corpse was not consumed by a hyena. As a result they used to cover human corpses in blood and fat to encourage consumption – and avoid social disgrace.  Today, the label of “scavanger” is portrayed as being a negative trait. But the idea that hyenas are purely scavengers that profit from the hard work of other (more popular) carnivores such as lions or cheetahs is incorrect. It is a myth often perpetuated by nature documentaries which show large groups of hyenas mobbing lions after a kill.  The truth is that spotted hyena are actually excellent hunters in their own right. Indeed, the majority of all the prey they consume comes from their own hunting efforts. Given the opportunity to scavenge or steal prey from another carnivore they will take it – but so would any other carnivore on the African plains.  Such kleptoparasitism (parasitism by theft) makes perfect sense from an energy conserving point of view. The nutritional gain provided by a carcass can be obtained without any risk of being injured during a hunt or the energy expenditure involved in a chase. Of course, the act of stealing a carcass from a hungry lion is not without its risks, and individuals can be killed in the attempt. But more often the species with the larger numbers in their ranks will prevail.  For some reason though, we tend to ignore the hunting skills of hyenas, while admiring the efforts of their rivals. We marvel at the power of a leopard dragging its prey up a tree, at the speed of a cheetah coursing a gazelle, and the team work of lions as they pursue large and dangerous animals. Yet we fail to notice that hyenas are just as impressive and efficient hunters.  They typically hunt alone or in groups of up five individuals – the size of potential prey increases with the hunting group size. One adult has been observed taking down a fully grown wildebeest, a testimony to the hyena’s awesome strength.  They can also run up to 40-50km per hour over several kilometres, with one pursuit being observed over an astonishing distance of 24km. This combination of strength and speed make them formidable hunters and one of the top predators in the African savannas.  Living in female dominated clans, hyenas are also one of the most social of all carnivores. Cubs are reared communally (although females only suckle their own offspring) with the group providing safety in numbers, improved vigilance of adults, and an effective defence of territory and food. As is common in many social carnivores, the spotted hyena has a wide repertoire of vocalisations to aid communication. The high pitched cackling heard at kills or when competing for carcasses is widely referred to as a “laugh”. But the laugh is actually a submissive call, showing that the individual making the sound is not a threat.  The other call spotted hyenas are well known for is a “whoop”, a long-distance call used to communicate to other clan members, and one of the iconic sounds of the African bush.  So perhaps it is time to cast aside Disney stereotypes and “baddy” bit parts in nature documentaries. The hyena is a wonderful predator in its own right. It is just as important and impressive as any of Africa’s great carnivores, and an animal with attributes that humans admire in other species, as well as in ourselves.  Yes, they steal prey from other carnivores, but so do lions when they get a chance. They may appear like an angry mob at times, but what they are actually displaying is superb team work."
"
How to properly place your outdoor thermometer
04:52 PM PDT on Wednesday, July 29, 2009
  By TRAVIS PITTMAN / KING5.com 

excerpts:
SEATTLE – With temperatures in the Puget Sound region breaking records this week, many people are playing a watching and waiting game – waiting to see when the thermometer outside their home will reach triple digits.
…
Below is a photo of a thermometer sent to KING 5 News Tuesday afternoon by a viewer in Oso, east of Arlington. It clearly shows the temperature reading 116 degrees. You can also clearly tell the sun is reflecting off it and it’s mounted right next to a building.

source: KING 5 Viewer…
The National Weather Service says this is where you need to place your thermometer to get an accurate reading:
– It must be in a shaded, well-ventilated and open area, 5 feet above        ground, give or take a foot.
– Away from sprinkler systems
– No closer than four times the height of any obstruction. For example, if a building is 10 feet tall, it needs to be no closer than 40 feet from that building.
– Located over natural ground such as grass, dirt or sod.
– At least 100 feet from road or concrete.
…
The picture they provide is what the surfacestations project is all about. Note the 100 foot distance from asphalt.
 
source: National Weather Service
Here is a diagram of how to properly place your thermometer to get an accurate reading.
Full article here h/t to WUWT reader “Ed”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94296fff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"“Ugly” or “wonky” veg were blamed for up to 40% of wasted fruit and vegetables in 2013, as produce was discarded for failing to meet retailer appearance standards. About 1.3 billion tonnes of food is wasted worldwide every year and, of this, fruit and vegetables have the highest wastage rates of any food type. But just how much of that is due to “ugly veg” being tossed by farms and supermarkets? The biggest culprit for food waste may be closer to home than we’d like to admit. “Ugliness” is just one reason among many for why food is wasted at some point from farm to fork – there’s also overproduction, improper storage and disease. But the problem of “wonky veg” caught the public’s attention. A report published in 2017 suggested that sales of “wonky veg” have risen in recent years as retailers have acknowledged the problem with wasting edible food, but it’s estimated that up to 25% of apples, 20% of onions and 13% of potatoes grown in the UK are still wasted on cosmetic grounds. Morrisons reported that consumers had begun to buy more misshapen food, whereas Sainsbury’s and Tesco both report including “wonky veg” in their recipe boxes, juices, smoothies and soups. Not all ugly veg is wasted at the retail point of the supply chain however. WRAP, a charity who have been working with governments on food waste since 2000, have investigated food waste on farms and their initial findings suggest a major cause of fruit waste is due to produce failing aesthetic standards. For example, strawberries are often discarded if they’re the wrong size for supermarkets. The National Farmers Union also reported in 2014 that around 20% of Gala apples were being wasted prior to leaving the farm gate as they weren’t at least 50% red in colour. Attitudes seem to be changing on “ugly veg” at least. Morissons ran a campaign to promote its “ugly veg” produce aisle, and other supermarkets are stocking similar items. Despite this, household waste remains the biggest culprit for food waste in the UK. Just under 5m tonnes of food wasted in the UK occurs in households – a staggering 70% of all post-farm gate food waste. A further million tonnes is wasted in the hospitality sector, with the latest government report blaming overly generous portion sizes. This suggests that perhaps – despite the best effort of campaigns such as Love Food Hate Waste – farms and retailers have been unfairly targeted by the “wonky veg” campaigns at the expense of focusing on where food waste really hits home. The 2013 Global Food Security Report put the figure for household and hospitality waste at 50% of total UK food waste. There are some signs we’re getting better at least. WRAP’s 2015 research showed that, at the household level, people now waste 1m tonnes of food per year less than they did in 2007. This is a staggering £3.4 billion per year saved simply by throwing less edible produce away. As climate change and its influence on extreme weather intensifies, reducing waste from precious food harvests will only become more important. Knowing exactly where the majority of waste occurs, rather than focusing too much on “wonky veg” in farms and supermarkets, is an important step towards making sure everyone has enough affordable and nutritious food to live on. During the UK’s “Dig for Victory” campaign in World War II, a large proportion of the population had to grow their own fruit and vegetables. Now the majority of people live in cities and towns – typically detached from primary food production. In the UK, the MYHarvest project has started to uncover how much “own-growing” contributes to the national diet and it seems demand for land to grow-your-own is increasing.  Research in Italy and Germany found that people who grow their own food waste the least. One way to fight food waste at home then – whether for “wonky” fruit and vegetables or otherwise – may be to replace the farm-to-fork supply chain with a garden-to-plate approach."
"
Share this...FacebookTwitter Just call it bazaar science.
The projection of sea levels has become quite the political football. So much hinges on the projections. Der Spiegel in an article titled IPCC haggles over data for sea level rise writes that 146 million people live in areas 1 meter or less above sea level. Tens of billions of dollars would be needed to expand dikes to keep the waters back, or to relocate citizens should seas rise too much. So the numbers are hotly contested.
To make things complex, there are hundreds of studies that offer a huge range of projections, up to 5 meters sea level rise by 2100. The job of deciding which sea level rise the IPCC should bank on in its next IPCC report rests on 18 scientists from 10 countries.
In the past each successive IPCC report lowered the sea level rise that is expected to occur by 2100. Critics pounced on the IPCC’s downward corrections, and so fears of rising seas diminished along with the IPCC’s credibility. Now the IPCC faces a dilemma (and irrelevance): Will it go back to alarmism? That may be real tough to do. Der Spiegel writes (emphasis added):
Now for the next IPCC report [due in 2013] the UN experts have to examine hundreds of reports – but indeed the selection is tougher than ever. The haggling over the results is like dealing at a bazaar: On one hand scientists have published alarming sea level prognoses, which surpass those given by the last IPCC Report. And on the other hand the actual sea level measurements indicate no detectable extreme increase.
4000 experts recently met at the IUGG Conference recently in Melbourne and Der Spiegel writes that the motto was: “Who bids the most!” NASA alarmist junkie James Hansen appears to have been the highest bidder at 5 meters. Currently sea levels are rising about 3 mm per year, which is just 1/17 of what Hansen projects.
Jim Houston and Bob Dean have a recent paper saying  there has been no detectable acceleration, while Stefan Rahmstorf says there is (though measurements don’t show it). Der Spiegel then cites other experts:
Simon Holgate, sea level researcher at the National Oceanogrphy Centre in Liverpool: Likely the irregularities in data arising from the changeover in measurement instruments are responsible for the differences [in the recent results].



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




 I believe that it is improbable that the sea level increase accelerated in the same year that satellites were put into service.”
Guy Wöppelmann of La Rochelle in France, Der Spiegel writes:
The increase sea level rise since 1993 is nothing unusual, as the sea level during the 20th century accelerated before, only then to decelerate.”
Eduardo Zorita:
The sea level rise rate has slowed down during the last 8 years. What happens in the future is unknown.”
Of course there are also a number of alarmist scientists who insist that sea level is accelerating and that Greenland and Antarctica pose a serious risk. But so far data measurements don’t show it.
Obviously the risk is all in the modeling (and not the actual measurements).
===================================
UPDATE 1: Schellnhuber now offering 70 m! Does anyone offer 80m? http://stevengoddard
h/t: DirkH
Share this...FacebookTwitter "
nan
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"Pupils from around the world are going on strike from school to demand urgent action from the world’s leaders on climate change. Here, a scientist answers  teenagers’ questions about climate change, gathered by the Priestley International Centre for Climate at a previous strike in February. You can find more Q&As like this on the centre’s website. If you’re a teen, you can have your questions about the world answered by academic experts – find out how at the end of this article. The “12 years” date you’ve heard comes from a special report requested by the United Nations, which looks at the impacts of global warming at 1.5°C above pre-industrial levels. At the moment, the world is 1°C warmer than in the late 19th century: the earliest period for which we have reliable temperature measurements and just before the Industrial Revolution got into full swing.  To avoid global warming above 1.5°C, humanity needs to cut its carbon dioxide (CO₂) emissions to about half of today’s levels by 2030, and to zero by 2050. The 2030 date – 12 years from when the report was released in October, 2018 – got a lot of media attention. Missing the 2030 deadline would make it very difficult to keep global warming under 1.5°C. That temperature is not necessarily safe, but the damage caused by climate change will quickly get worse with higher levels of warming.  At today’s 1°C of warming, there have already been increases in extreme weather events (such as heat waves and flooding), as well as food shortages and effects on food production. Entire species are already going extinct, for reasons related to climate change. At 2°C of warming or above, rising sea levels, more frequent extreme weather and damaging effects on food and water supplies will make some parts of the world very hard to live in. As a result, it’s predicted that many people will need to leave their homes and become climate refugees, while many millions more people worldwide will be exposed to poverty. What’s more, many species will be lost and virtually all corals will die.  Unfortunately, we are not on track to keep warming below 1.5°C, or even 2°C. If countries hit their existing targets, temperatures will rise by around 3°C – or more than that, if emissions continue to grow. The planet itself will survive man-made climate change. In fact, it has been warmer, millions of years ago, although the world looked very different back then. Humans are not expected to go extinct – but we will have to learn to cope with a warmer world, and all its challenges. This means cooperating and providing support and resources to vulnerable people. No single policy will end climate change, but a very effective strategy would be to quickly phase out fossil fuels such as coal and petrol, which are used to create electricity and power transport. There are many different ways to achieve this goal, and it’s important that leaders choose policies that create good jobs and strengthen communities.  For example, governments need to put money towards safe, reliable, efficient and affordable trains and buses, so people can get around without using cars. Towns and cities should be designed to be more friendly to walking, cycling and public transport. Homes should have good transport links, and be built or modified to be more energy efficient, so that they’re easier to keep cool in the summer and warm in the winter.  International air travel is also responsible for a growing share of global emissions and governments around the world need to work together to come up with a response.  Farming – especially meat and dairy production – also creates a surprisingly large amount of emissions. So, governments should encourage farmers to use sustainable approaches. Agriculture can also lead to deforestation. Since trees remove carbon dioxide from the atmosphere, forests must be protected, and new trees planted. First, you can find out what your environmental footprint is using this questionnaire from the World Wide Fund for Nature (WWF). The survey gives advice to help you and your family reduce your impact. Research has also highlighted the biggest changes a person can make to help the climate. They are:  Smaller actions in your daily life can also help. Turning down the heating or air conditioning at home and only heating or cooling the rooms you’re using will save money and reduce carbon emissions. Try to buy less clothing, plastics and gadgets, since it takes resources and energy to make these items.  Make, borrow, swap, buy secondhand or find things for free, and recycle as much as possible that can’t be reused. When you’re old enough, you can also choose to put your money in an ethical bank account, and get electricity from 100% renewables. Individual changes will only go so far, but remember that your actions can inspire others. Use your voice! Talking about climate change with your friends, family and classmates really helps to raise awareness and drive further action."
"
Guest Post by David Archibald
NASA’s David Hathaway has adjusted his expectations of Solar Cycle 24 downwards.  He is quoted in the New York Times here Specifically, he said:
” Still, something like the Dalton Minimum — two solar cycles in the early 1800s that peaked at about an average of 50 sunspots — lies in the realm of the possible.”
NASA has caught up with my prediction in early 2006 of a Dalton Minimum repeat, so for a brief, shining moment of three years, I have had a better track record in predicting solar activity than NASA.

The graphic above is modified from a paper I published in March, 2006.  Even based on our understanding of solar – climate relationship at the time, it was evident the range of Solar Cycle 24 amplitude predictions would result in a 2°C range in temperature.  The climate science community was oblivious to this, despite billions being spent.  To borrow a term from the leftist lexicon, the predictions above Badalyan are now discredited elements.
Let’s now examine another successful prediction of mine.  In March, 2008 at the first Heartland climate conference in New York, I predicted that Solar Cycle 24 would mean that it would not be a good time to be a Canadian wheat farmer.  Lo and behold, the Canadian wheat crop is down 20% this year due to a cold spring and dry fields. Story here.
The oceans are losing heat, so the Canadian wheat belt will just get colder and drier as Solar Cycle 24 progresses.  As Mark Steyn recently said, anyone under the age of 29 has not experienced global warming.  A Dalton Minimum repeat will mean that they will have to wait to the age of 54 odd to experience a warming trend.
Where to now?  The F 10.7 flux continues to flatline.  All the volatility has gone out of it.  In terms of picking the month of minimum for the Solar Cycle 23/24 transition, I think the solar community will put it in the middle of the F 10.7 quiet period due to the lack of sunspots.  We won’t know how long that quiet period is until solar activity ramps up again.  So picking the month of minimum at the moment may just be guessing.
Dr Hathaway says that we are not in for a Maunder Minimum, and I agree with him.  I have been contacted by a gentleman from the lower 48 who has a very good solar activity model.  It hindcasts the 20th century almost perfectly, so I have a lot of faith in what it is predicting for the 21st century, which is a couple of very weak cycles and then back to normal as we have known it.  I consider his model to be a major advance in solar science.
What I am now examining is the possibility that there will not be a solar magnetic reversal at the Solar Cycle 24 maximum.

Sponsored IT training links:
Achieve guaranteed success using up to date 646-230 dumps and 642-426 study guide prepared by 642-661 certified experts.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e947c662e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In the “Steig et al – falsified” thread, since we have been discussing geothermal activity along the Antarctic peninsula, I thought I’d pass along these images that show other parts of the planet where geothermal heat seems capable of melting ice and making it all the way to the surface. Lake Baikal is quite deep, over 5000′ feet in places, so this demonstrates that even in deep water, the melting of ice from that geothermal heat is a real possibility. Hat tip to WUWT commenter “Mark”  – Anthony
By Betsy Mason, Wired News 
Click for a larger image - photo from NASA - ISS
Astronauts aboard the International Space Station noticed two mysterious dark circles in the ice of Russia’s Lake Baikal in April. Though the cause is more likely aqueous than alien, some aspects of the odd blemishes defy explanation.
The two circles are the focal points for ice break-up and may be caused by upwelling of warmer water in the lake. The dark color of the circles is due to thinning of the ice, which usually hangs around into June.
Upwelling wouldn’t be strange in some relatively shallow areas of the lake where hydrothermal activity has been detected, such as where the circle near the center of the lake (pictured below) is located.
Circles have been seen in that area before in 1985 and 1994, though they weren’t nearly as pronounced. But the location of the circle near the southern tip of the lake (pictured above) where water is relatively deep and cold is puzzling.
The lake itself is an oddity. It is the largest by volume and the deepest (5370 feet at its deepest point), as well as one of the oldest at around 25 million years. The photo above was taken by an astronaut from the ISS.
The photo below was taken by NASA’s MODIS satellite imager.
Click for a larger image - photo from NASA - MODIS
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95c9c39f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Churches in NI are to be allowed to stay open for individual prayer during the two-week coronavirus lockdown.**
Stormont ministers met on Tuesday morning and agreed the clarification to the regulations.
It followed calls by church leaders across NI for the change.
The executive had initially agreed that places of worship should close for all but weddings, civil partnerships and funerals from Friday until 11 December.
They will also be allowed to carry out drive-in services.
Outdoor visitor attractions will have to close as well, while the executive has confirmed that self-catering accommodation will only be permitted to operate on a ""restricted basis"", in line with arrangements already in place for hotels and B&Bs.
On Tuesday, the Department of Health also confirmed that 11 more people have died after contracting coronavirus, taking the total to 947 in Northern Ireland.
As of Tuesday, there were 445 people in hospital, of those 37 people are in intensive care, 29 are on ventilation.
NI's chief medical officer Dr Michael McBride also confirmed the R number had not yet dropped to below one.
Dr McBride said they hoped to get the R number below one with the next two weeks of interventions.
Speaking during a media briefing Dr McBride said there had not been a significant reduction in the number of hospital admissions or the number of patients in ICU.
He said there was an opportunity to ""redouble efforts"" to make difference and make the restrictions over the next two weeks ""count"".
Non-essential retail shops obliged to close later this week for the so-called circuit-breaker will be permitted to operate click-and-collect services.
First Minister Arlene Foster welcomed the executive's decision to allow places of worship to remain open.
""This weekend sees the beginning of Advent and we have agreed that it is important that all places of worship can remain open in a limited way,"" she said.
She added that the small adjustments would help to maintain well-being and reduce pressures.
Deputy First Minister Michelle O'Neill said the executive had listened to views on a range of issues.
""These two weeks offer us the best chance of pushing community transmission as low as possible to allow a safer Christmas for everyone,"" she said.
In a tweet, Justice Minister Naomi Long said that it was ""important"" that smaller retailers could operate on this basis.
AodhÃ¡n Connolly, director of the NI Retail Consortium, said the decision would be a ""lifeline"" for retailers coming up to Christmas.
""We need public support to make this work. Please abide by the time that you are given to collect your goods,"" he added.
""Leave extra time for your shopping and be kind to staff and fellow shoppers. This is in no way a normal Christmas but we can make it better we work together.""
The two-week lockdown was agreed by Stormont ministers last Thursday, in a bid to reduce the transmission of Covid-19 in Northern Ireland.
It is much tougher than previous periods of restrictions imposed by the executive, but schools will be allowed to stay open.
Last week, the Moderator of the Presbyterian Church had described the closure of places of worship as a cause of ""significant regret and concern"".
In a statement, Rt Rev Dr David Bruce said the Presbyterian Church continued to make representations regarding public worship to the Northern Ireland Executive in Belfast and to the Irish Government in Dublin.
The Roman Catholic Archbishop of Armagh and Primate of All Ireland Eamon Martin had questioned why off-licences could remain open but churches had to close.
He had urged the executive to accept that for many people a ""meaningful Christmas"" is about more than shopping, eating and drinking and that spiritual preparation is essential.
Earlier this year, the leaders of NI's four main Christian Churches asked parishioners to wear face coverings during services."
"

From the abstract of the lead paper by Martin Wild: Recent evidence suggests that solar radiation reaching the Earth’s surface has not been constant over time but has undergone substantial variations on decadal timescales. The available observations suggest a widespread decrease in surface solar radiation between the 1950s and 1980s (popularly referred to as “global dimming”), with some more recent evidence for a partial recovery (“brightening”).
From ETH Zurich News
“Global dimming and brightening” –  The role of solar radiation in climate change
A special volume of the “Journal of Geophysical Research” reviews the growing research field of “global dimming” and “global brightening” in over 20 articles. These phenomena, supposedly human-induced, control solar radiation incident at the Earth’s surface and thus influence climate.

Clouds and aerosols influence the solar radiation on the earth’s surface and therefore the climate. (Photo: flickr/Schrottie)<!– (mehr Bilder) –>


Special instruments have been recording the solar radiation that reaches the Earth’s surface since 1923. However, it wasn’t until the International Geophysical Year in 1957/58 that a global measurement network began to take shape. The data thus obtained reveal that the energy provided by the sun at the Earth’s surface has undergone considerable variations over the past decades, with associated impacts on climate.
Research focus at ETH Zurich
Investigating which factors reduce or intensify solar radiation and thus cause “global dimming” or “global brightening” is still very much a nascent field of research in which especially scientists from ETH Zurich became renowned. The American Geophysical Union (AGU) has now published a special volume on the subject which presents the current state of knowledge in detail and makes a considerable contribution to climate science. “Only now, especially with the help of this volume, is research in this field really taking off”, stresses Martin Wild, senior scientist at the Institute for Atmospheric and Climate Science of ETH Zurich, who is a specialist on the subject.
Decrease in solar radiation discovered
The initial findings, which revealed that solar radiation at the Earth’s surface is not constant over time but rather varies considerably over decades, were published in the late 1980s and early 1990s for specific regions of the Earth. Atsumu Ohmura, emeritus professor at ETH Zurich, for example, discovered at the time that the amount of solar radiation over Europe decreased considerably between the 1950s and the 1980s. It wasn’t until 1998 that the first global study was conducted for larger areas, like the continents Africa, Asia, North America and Europe for instance. The results showed that on average the surface solar radiation decreased by two percent per decade between the 1950s and 1990.
In analyzing more recently compiled data, however, Wild and his team discovered that solar radiation has gradually been increasing again since 1985. In a paper published in “Science” in 2005, they coined the phrase “global brightening” to describe this new trend and to oppose to the term “global dimming” used since 2001 for the previously established decrease in solar radiation.
Only recently, an article in the journal “Nature”, which Wild was also involved in, brought additional attention to the topic of global dimming/brightening.
Air pollution favors photosynthesis
In this study, for the first time, the scientists examined the connection between global dimming/brightening and the carbon cycle. They demonstrated that more scattered light is present during periods of global dimming due to the increased aerosol- and cloud-amounts, enabling plants to absorb CO2 more efficiently than when the air is cleaner and thus clearer. According to the scientists, this is because scattered light penetrates deeper into the vegetation canopy than direct sunlight, which means the plants can use the light more effectively for photosynthesis. Consequently, there was around 10 percent more carbon stored in the terrestrial biosphere between 1960 and 1999.
The special volume, which appears in the AGU’s renowned “Journal of Geophysical Research”, provides an overview of the current state of knowledge. Almost half of the publications in the volume were either completely or partially written by ETH Zurich scientists. Wild is the guest editor, and author or co-author of ten of these articles.
The articles provide the first indication of the magnitude of these effects, how they vary in terms of time and space and what the possible consequences might be for climate change. They also discuss in detail the underlying causes and mechanisms, which are still under debate.
Many questions left open
It is particularly unclear as to whether it is the clouds or the aerosols that trigger global dimming/brightening, or even interactions between clouds and aerosols, as aerosols can influence the “brightness” and lifetime of the clouds. The investigation of these relations is complicated by the fact that insufficient – if any – observational data are available on how clouds and aerosol loadings have been changing over the past decades. The recently launched satellite measurement programs should help to close this gap for the future from space, however.
“There is still an enormous amount of research to be done as many questions are still open”, explains Wild. This includes the magnitude of the dimming and brightening effects on a global level and how greatly the effects differ between urban and rural areas, where fewer aerosols are released into the atmosphere. Another unresolved question is what happens over the oceans, as barely any measurement data are available from these areas.
A further challenge for the researchers is to incorporate the effects of global dimming/brightening more effectively in climate models, to understand their impact on climate change better. After all, studies indicate that global dimming masked the actual temperature rise – and therefore climate change – until well into the 1980s. Moreover, the studies published also show that the models used in the Intergovernmental Panel on Climate Change’s (IPCC) fourth Assessment Report do not reproduce global dimming/brightening adequately: neither the dimming nor the subsequent brightening is simulated realistically by the models. According to the scientists, this is probably due to the fact that the processes causing global dimming/brightening were not taken into account adequately and that the historical anthropogenic emissions used as model input are afflicted with considerable uncertainties.
“This is why at ETH Zurich we are working with a research version of a global climate model, which contains much more detailed aerosol and cloud microphysics and can reproduce global dimming/brightening more effectively”, says Wild. For him, the studies so far constitute “initial” estimates that need to be followed up with further research.
Link to these papers in JGR here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93ed862e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Even as Secretary of Defense Donald Rumsfeld and Secretary of State Colin Powell visited Canberra for the annual AUSMIN (Australia-U.S. Ministerial) consultations, talking of expanded security relations, mutterings of disappointment were heard. Peter Hartcher of the Australian Financial Review wrote of “a climate of deflated expectations.” But it is time for not just Australia, but U.S. allies around the world, to expect less of the United States and do more themselves.



American security obligations are breathtaking. The United States maintains 100,000 troops in Western Europe to defend against phantom security threats, and is talking about further expanding NATO. Washington helps garrison the Balkans, an area never of strategic interest to the United States. Washington routinely attempts to dictate affairs in Central America and occasionally blunders into Africa. Washington’s support for Israel and demand for oil have led to multiple interventions in Lebanon, war against Iraq, and permanent garrisons in Kuwait and Saudi Arabia. An additional 100,000 troops are stationed in East Asia. Treaties, informal security guarantees and bases litter the region. U.S. and Australian officials suggested creating an informal JANZUS security group of America, Australia, Japan and maybe South Korea. It was one thing to carry such a global burden during the Cold War, when the Soviet Union and its many surrogates threatened friendly nations that had been wrecked or weakened by World War II. But the threats have dissipated and the Western states are far stronger than their potential antagonists. Which means that Washington need not do as much.



Even changing its policy is considered by others to be arrogant and “unilateral” — the term of opprobrium, that seems to have replaced isolationist. In fact, it is amazing how seldom the United States acts unilaterally. Washington subsidizes every international aid agency, participates in almost every international organization, ratifies most international treaties and dominates all of the leading military alliances. Other countries routinely act unilaterally when they believe it to be in their interest. So, too, should the United States.



Ultimately, Washington owes loyalty to the citizens of the United States, not those of Japan, Germany, Uzbekistan, Kosovo or Australia. Although the United States cannot pursue its own interest without restraint, it is under no obligation to put the interest of other states first — especially when they are able to help themselves. So it is with AUSMIN.



Washington should act unilaterally and transform the alliance into a much looser cooperative relationship. For what purpose does AUSMIN exist? The former Red Navy is rusting in port. Japan is a most unlikely repeat aggressor. China is years away from possessing a serious offensive capability, let alone one able to threaten Australia. Vietnam and Malaysia make unlikely invaders. There is only Indonesia, which threatens a flotilla of refugees, not soldiers. What Australia most needs, then, is not a superpower alliance, but a more robust military and stronger regional ties.



ASEAN is better situated than the United States to handle a messy breakdown in Indonesia. Cooperative relationships with India and Japan could substitute for reliance on the United States in forging a naval force to deter a future aggressive China from interfering with international navigation. No doubt, Canberra might prefer to rely on the United States than to spend the money and take the effort to develop regional forces and relationships.



But a preference to be subsidized by Uncle Sam is no reason for Uncle Sam to do so. After all, having defended most of the known world for the last half century, Americans might suggest that it was time for, say, the British to man Alaskan radar stations, Japanese to patrol United States sea lanes, and Australians to maintain U.S. bases in Hawaii, Wake Island and Guam. Such requests likely would not meet with much favor anywhere, including in Canberra.



This latest AUSMIN consultation was filled with the usual platitudes and terms of endearment. But the United States and Australia have divergent interests. Canberra’s concerns are largely local. Instability in Indonesia, Fiji and Papua‐​New Guinea could spill over. Hostility among ASEAN states could greet Australian activism. None of these issues matter much to Washington, which is more concerned about relations with past and potential future superpowers: Russia, China and India.



The United States cannot accept a rival hegemonic domination of Eurasia; little else poses significant worry. The relationship between the United States and Australia should be based on their many shared cultural, economic and political interests. Harmonizing trade relations is perhaps the most important issue now before them. Military ties should be left to informal cooperation, such as intelligence sharing. Then the United States won’t be promising to defend yet another distant dependent. And Australia will be focusing on building the kind of regional relationships that will provide it with the greatest long‐​term security.
"
"
Share this...FacebookTwitterEven with all the global warming we are supposedly having, summers are still too short. So I’m taking a few days off to enjoy some time outside.
Back early next week!
Here’s something to watch in the meantime (h/t Andrew Bolt):

Share this...FacebookTwitter "
"During a period of ill health a few years back, as I struggled with depression and addiction, three of the four elements that helped me to recover were straightforward: psychiatry; psychotherapy; and the support of others. The fourth was more mysterious, and I set out on a journey to try to understand it. In my teenage years, I had developed an aptitude for drinking, as many adolescents do. But it was while working as a music journalist in my 20s that my addictions accelerated. By April 2012, at the age of 27, I was on my knees. I knew that the drink and drugs had to go: the highs were getting harder to achieve; the lows were becoming more dangerous, more self-destructive. I knew my mental and physical health were deteriorating, but I couldn’t seem to stop or cut down. After months of trying to do this alone, I found a rehabilitation programme and set out on a path to sobriety.  I had time on my hands, weekends loomed large and I needed to keep busy. So I started walking, wandering daily on Walthamstow Marshes in north-east London to watch the kestrels, caterpillars and the shaggy old heron. It made me feel safe and secure. Gradually, I realised that my mind needed these walks and I grew to rely on them. The natural world had become a kind of rehab: it soothed my rawness and patched me back together. In those very early sober months, when I was learning to live without the crutches I had used for years, I felt as if I was walking around without skin. The critical inner voices that had contributed to my periods of mental illness had folded me into myself. But on the marshes, I started to look up and outwards. Even though I was usually alone when I walked, I never felt lonely. I was realising that I belonged to a wider family of species, the matrix of life, from the spiders to the lichen and the cormorants to the coots. Nature picked me up by the scruff of my neck, and I rested in her care for a while. With the kind of urgent desire I had once had for mind-altering substances and music, I was now drawn to trees, birds, flowers and plants . I understood that time in nature softened the voices in my head and stabilised my mood, but I didn’t, at the beginning, understand what was happening to my body, brain and mind. I hadn’t realised that the essence of nature – the geometry, the scents, the sounds, the colours, the textures, the chemical makeup – could have such a life-changing power but, quite quickly, this became apparent. I began to plant things to watch them grow. One of the first things I noticed was that after gardening, digging my hands deep into the soil, I felt happy, upbeat, less stressed and generally more positive. Reading up on this, I saw that there may be a biological reason for it. One of the species of bacteria found in soil, M vaccae, has been found to affect the brain and increase stress resilience. In 2004, lung cancer patients at the Royal Marsden hospital in Surrey who were given an immunisation containing the bacteria reported feeling happier. As Dr Christopher Lowry, one of the leading neuroscientists in the field, said at the time: “These studies leave us wondering if we should all spend more time playing in the dirt.” Our microbiota are healthiest when they are diverse – and a diverse microbiota is influenced positively by an environment filled with organisms, which are found more abundantly outside. We are woven into the land, and wider ecosystems, more than we realise. Crucially, these old friends that we evolved with are able to treat or block chronic inflammation, which can also affect the brain and have a direct impact on mood. Studies have shown that spending just two hours in a forest can significantly lower levels of cytokines – an inflammation biomarker – in the blood, which could be caused by exposure to important organisms. It is all very well thinking about how more connection with the natural world would make people happier and healthier. But it is all the more pressing given the situation we find ourselves in. How can forest bathing be prescribed when forests are threatened and diminishing across the world? How can people spend more time in green spaces when many of our parks are in decline?  As I fell in love with the trees and the soil, I began to see how endangered much of nature had become, and how these opportunities to commune with other species were slipping through our fingers. Alongside our disconnection from nature is, of course, the fact that the natural world is rapidly vanishing; our time on this Earth is haunted by habitat destruction, species loss and climate breakdown. While revelling in the glory of the rest of nature, I also fell into a state of ecological grief, the name for the psychological response to the nature and climate emergency; a mourning for the communities, wildlife and landscapes that are disappearing and a fear of what is to come. Of course, people in many countries – including Britain’s flooded areas – are already suffering directly from the impact of the climate emergency. Others are experiencing anxiety, worry and dread about an anticipated future of ecological loss. The more I learned about the benefits a connection to nature can have on our minds, the more it seemed appalling that access to nature is so threatened in some places. The destruction of ancient woodlands across the country; the felling of much-loved street trees in Sheffield and other urban areas; children barely given opportunities to play in woods, fields and parks; the legislation that is failing to protect our rivers, streams and wild places; and the fact that the UK will miss almost all its biodiversity targets that were set a decade ago. Spending time in restorative natural environments is dependent, partly, on weather, which is in flux. A study of antidepressants by the environmental psychologist Terry Hartig has suggested that colder summers may constrain restorative activities that reduce stress and depressive symptoms. Rates of SSRI prescriptions increased for men and women in an unseasonably cooler July in Sweden. At the other extreme, emerging evidence links potentially dangerous high temperatures to increased mental health problems, illness and suicide. For those on the frontline of the climate crisis, the mental health impacts of ecological loss are already severe. In Kulusuk, Greenland, where the ice has melted, rates of depression, suicide and alcoholism have risen. In the Nunatsiavut region of Labrador, Canada, residents report feeling stressed, depressed and anxious because of the melted ice and changing weather patterns. It strikes at the very heart of identity. Farmers in the Australian wheat belt, whose farms have blown away in dust storms, have compared losing their farms to a death. As Glenn Albrecht, the Australian academic who coined the term “solastalgia”, to describe the distress caused by environmental change, points out, what is disordered isn’t ecological grief, eco-anxiety or global dread but “the world that is causing you to feel that way”. It is a natural response to loss – and it is likely to become more common. Our relationship with nature, even if it could be restored, isn’t quite as simple as a soothing, serene ramble in the wild. The wild barely exists. So what is the effect of biodiversity loss on our minds, our inner selves, and the collective psyche? When we walk in the woods, or by a lake, or spend time in a garden or park, evidence suggests that our parasympathetic nervous system is more likely to be activated. This is responsible for the “rest and digest” processes at work inside your body, associated with feelings of contentment, sleep and safety. The sympathetic nervous system’s main function is to stimulate the body’s reaction to stress, and ignore any non-essential business, such as immune function. Ideally, we want a balanced nervous system. After exposure to nature, our stress-recovery response is faster and more complete when compared with exposure to built environments. This has important consequences for our health at a time when stress-related diseases are on the increase. It also suggests that if we, as a society, are allowing trees to be cut down, or natural spaces in urban areas to be paved over, we are acting in a way that is damaging to public health. We need nature in order to recover from the stresses of life. There are many studies that link natural sounds – particularly diversity and richness of bird song – to decreased stress and a quicker recovery of a balanced nervous system. Even people under anaesthetic have been found to produce fewer chemical biomarkers associated with stress – such as amylase in saliva – when played a recording of soft wind or birdsong. A few years into my research, I had a baby and moved to a town in Hampshire. Near my new home, I found a beautiful, wild cemetery containing ancient ruins, where a majestic beech tree shines batter-yellow in the autumn sun. Rabbits flicker in the long grass, occasionally stopping long enough so I can see the black inkwells of their eyes; goblets of ancient epiphytes decorate the brick walls. When I look closer at the yellow blotches of lichen with my pocket microscope, I see tiny cities made of gold, with depth and dimension and cherry-red microscopic bugs invisible to the naked eye. The science of awe was first considered by a psychologist called Dacher Keltner at the University of California at Berkeley in a landmark paper published in 2003. Many experiences of awe in the modern world still come from an encounter with nature, despite our disconnection from it. Keltner found that awe increases happiness and lowers stress, perhaps unsurprisingly, but he also discovered just how powerful an experience of awe can be to the body and mind. The lab found that people were more ethical, kind and generous after feeling awe. Why? Perhaps from simply being in a good mood. But using functional MRI to look at the brains of participants, scientists saw that awe reduced activity in the default mode network, the area of the brain associated with a sense of self. It reminded me of a phenomenon I had heard about from recovering addicts: “addiction.fm” or the “washing-machine head”. In other words, the self-centred, negative ruminations that a substance can hush temporarily but, in the long run, will only feed. I visited the cemetery with renewed vim, searching for moments of awe and finding them everywhere. Around that time, I got postnatal depression and there was a frightening period when nature didn’t touch the sides. I felt nothing in the wild spaces that had previously brought me wonder and succour. But after medical help, I found that taking a walk through the cemetery instead of down a busy, polluted road, made me feel noticeably calmer and lighter. I was drawn to the effulgent green moss, the old, sprawling yews, the buzz of spotting a nuthatch or goldcrest or sparrowhawk. It turns out that these walks may have been affecting my brain in immediate and significant ways. Researchers in Edinburgh asked a group of people to walk from a busy urban space to a public park, or vice versa. Both groups started with a high stress response. What was interesting was how green space seemed to have a buffering effect on the stresses of the urban area. Those who started in green space and walked into a busy built-up space experienced an increase in alpha brainwaves – the electrical activity of the brain associated with relaxation. Nature seemed to undo the stress of the city, in the moment. When I started on this journey, I might have said that a relationship or connection with the rest of nature isn’t for everyone; that some people just don’t like the outdoors. But, in fact, research shows that background nature is essential across the population for good mental health. Without access to natural landscapes, rich in biodiversity, our potential for restoration, peace and psychological nourishment is sorely degraded. If we feel it, we can be galvanised by our ecological grief. We need a new relationship with the Earth, one that positions us not as conquerors, but co-tenants with wildlife and rivers and mountains and trees, respecting and caring for natural spaces because it is the right thing to do – and because we need the rest of nature for our lives and for our sanity. Losing Eden: Why Our Minds Need The Wild by Lucy Jones is published by Allen Lane on 27 February"
"

Bangkok is a crowded city, teeming with vehicles that observe the rules of the road more in the breach than in the practice. Despite this maelstrom, surprisingly, everyone seems to know how to handle the confusion, and even the ubiquitous tuk‐​tuks (three wheeled motorized open‐​air taxis) thrive in this atmosphere. Indeed, trying to impose order on this chaos would only cause more of a mess than it would solve.



Thailand now faces an economic situation similar to the haphazard streets of Bangkok, only the vehicles and all the drivers are making a dash for the road out of town. The epicenter of the Asian crisis of 1997, Thailand is now being confronted with a similar problem, as capital is starting to desert the country in droves and the baht has lost 15 percent of its value since last year.



The mood turned even gloomier last week as Chatu Mongol, the Bank of Thailand’s governor and the man who helped to pull Thailand out of the Asian crisis, was ousted after an argument with the government over interest rate policy. Investors have become unsettled by this change of management, and the question on everyone’s lips is, will Thailand follow Malaysia’s lead in imposing capital controls? And will these restrictions be able to control the flow of capital, or will they merely force a traffic jam and disrupt a functioning system?



The economic arguments against capital controls rest on both efficiency and equity considerations. Capital controls are highly ineffective for myriad reasons, not least of which is that they don’t work. The longer they are in place, the craftier people become in evading them, through methods such as falsification of invoices in trading, leads and lags in paperwork, substitution of exempted flows with restricted flows, and illegal methods (such as bribery and smuggling). Capital controls are the proverbial immovable object that the irresistible force always overwhelms.



More important than the theoretical arguments against capital controls is the instructive experience of Thailand’s neighbor, Malaysia, which imposed them at the end of the Asian crisis in October 1998 after much of the worst damage from the Asian crisis had passed. Ironically enough, Malaysia was well poised to weather the storm, having undergone a major banking crisis in 1986 and having embarked on a fairly successful restructuring immediately afterwards. Moreover, the short‐​term flows that were blamed for the volatility made up a relatively small part of Malaysia’s capital stock–the most portfolio equity ever accounted for was 5.7 percent of gross domestic product in 1993, and this had fallen to 4.38 percent in 1996.



Indeed, Malaysia’s imposition of capital controls seemed to be a nakedly political move by despot Mahathir Mohammed, who sought to consolidate his own grip on power and gave him a pretense to remove (and jail) his main political rival, Finance Minister Anwar Ibrahim. While they may have saved Mahathir’s political position, the arcane array of capital controls didn’t do their highly touted job. The international markets, in particular, were just waiting out the controls: in the first quarter this year, after Malaysia dismantled most of its restrictions, Malaysia’s central bank lost nearly $1 billion a month in foreign reserves, and speculation that the overvalued ringgit may be devalued is prompting more capital flight.



With Mahathir now in charge as the Finance Minister as well as Prime Minister (following the resignation of Daim Zainuddin at the beginning of May), the capital markets are not sanguine about the necessary reforms in Malaysia being pushed through. Thus, the effect of the controls was to poison the investment climate and merely postpone the inevitable. Somehow, this doesn’t appear to be a policy that should be replicated.



But perhaps Thailand has learned some of the lessons of the Asian crisis, as Finance Minister Somkid Jatusriptiak explicitly ruled out controls in an interview with the Financial Times last week. Since the Asian crisis, Thailand has faltered, but has generally remained open to the world and has undergone an extraordinary political shift. Malaysia, on the other hand, remains mired in its authoritarian ways and further estranged from the liberalization that the region needs.



Thailand’s eschewing of capital controls may prove to be the decisive lesson in a region that was once hailed as the new model for economic policy; as Malaysia has demonstrated, any attempts to rollback liberalization can leave a country stranded in an endless roundabout.
"
"Greta’s father, Svante, and I are what is known in Sweden as “cultural workers” – trained in opera, music and theatre with half a career of work in those fields behind us. When I was pregnant with Greta, and working in Germany, Svante was acting at three different theatres in Sweden simultaneously. I had several years of binding contracts ahead of me at various opera houses all over Europe. With 1,000km between us, we talked over the phone about how we could get our new reality to work. “You’re one of the best in the world at what you do,” Svante said. “And as for me, I am more like a bass player in the Swedish theatre and can very easily be replaced. Not to mention you earn so damned much more than I do.” I protested a little half-heartedly but the choice was made. A few weeks later we were at the premiere for Don Giovanni at the Staatsoper in Berlin and Svante explained his current professional status to Daniel Barenboim and Cecilia Bartoli. “So now I’m a housewife.” We carried on like that for 12 years. It was arduous but great fun. We spent two months in each city and then moved on. Berlin, Paris, Vienna, Amsterdam, Barcelona. Round and round. We spent the summers in Glyndebourne, Salzburg or Aix-en-Provence. As you do when you’re good at singing opera and other classical music. I rehearsed 20 to 30 hours a week and the rest of the time we spent together. Beata was born three years after Greta and we bought a Volvo V70 so we’d have room for doll’s houses, teddy bears and tricycles. Those were fantastic years. Our life was marvellous. One evening in the autumn of 2014, Svante and I sat slumped on our bathroom floor in Stockholm. It was late, the children were asleep. Everything was starting to fall apart around us. Greta was 11, had just started fifth grade, and was not doing well. She cried at night when she should be sleeping. She cried on her way to school. She cried in her classes and during her breaks, and the teachers called home almost every day. Svante had to run off and bring her home to Moses, our golden retriever. She sat with him for hours, petting him and stroking his fur. She was slowly disappearing into some kind of darkness and little by little, bit by bit, she seemed to stop functioning. She stopped playing the piano. She stopped laughing. She stopped talking. And she stopped eating. We sat there on the hard mosaic floor, knowing exactly what we would do. We would change everything. We would find the way back to Greta, no matter the cost. The situation called for more than words and feelings. A closing of accounts. A clean break. “How are you feeling?” Svante asked. “Do you want to keep going?” “No.” “OK. Fuck this. No more,” he said. “We’ll cancel everything. Every last contract,” Svante went on. “Madrid, Zurich, Vienna, Brussels. Everything.” One Saturday soon afterwards, we decide we’re going to bake buns, all four of us, the whole family, and we’re determined to make this work. It has to. If we can bake our buns as usual, in peace and quiet, Greta will be able to eat them as usual, and then everything will be resolved, fixed. It’s going to be easy as pie. Baking buns is after all our favourite activity. So we bake, dancing around in the kitchen so as to create the most positive, happiest bun-baking party in human history. But once the buns are out of the oven the party stops in its tracks. Greta picks up a bun and sniffs it. She sits there holding it, tries to open her mouth, but… can’t. We see that this isn’t going to work. “Please eat,” Svante and I say in chorus. Calmly, at first. And then more firmly. Then with every ounce of pent-up frustration and powerlessness. Until finally we scream, letting out all our fear and hopelessness. “Eat! You have to eat, don’t you understand? You have to eat now, otherwise you’ll die!” Then Greta has her first panic attack. She makes a sound we’ve never heard before, ever. She lets out an abysmal howl that lasts for over 40 minutes. We haven’t heard her scream since she was an infant. I cradle her in my arms, and Moses lies alongside her, his moist nose pressed to her head. Greta asks, “Am I going to get well again?” “Of course you are,” I reply. “When am I going to get well?” “I don’t know. Soon.” On a white sheet of paper fixed to the wall we note down everything Greta eats and how long it takes for her to eat it. The amounts are small. And it takes a long time. But the emergency unit at the Stockholm Centre for Eating Disorders says that this method has a good long-term success rate. You write down what you eat meal by meal, then you list everything you can eat, things you wish you could eat and things you want to be able to eat further down the line. It’s a short list. Rice, avocados and gnocchi. School starts in five minutes. But there isn’t going to be any school today. There isn’t going to be any school at all this week. Yesterday Svante and I got another email from the school expressing their “concern” about Greta’s lack of attendance, despite the fact that they were in possession of several letters from both doctors and psychologists explaining her situation. Again, I inform the school office of our situation and they reply with an email saying that they hope Greta will come to school as usual on Monday so “this problem” can be dealt with. But Greta won’t be in school on Monday. Because unless a sudden dramatic change occurs she’s going to be admitted to Sachsska children’s hospital next week. Svante is boiling gnocchi. It is extremely important that the consistency is perfect, otherwise it won’t be eaten. We set a specific number of gnocchi on her plate. It’s a delicate balancing act; if we offer too many our daughter won’t eat anything and if we offer too few she won’t get enough. Whatever she ingests is obviously too little, but every little bite counts and we can’t afford to waste a single one. Then Greta sits there sorting the gnocchi. She turns each one over, presses on them and then does it again. And again. After 20 minutes she starts eating. She licks and sucks and chews: tiny, tiny bites. It takes for ever. “I’m full,” she says suddenly. “I can’t eat any more.” Svante and I avoid looking at each other. We have to hold back our frustration, because we’ve started to realise that this is the only thing that works. We’ve explored all other tactics. Every other conceivable way. We’ve ordered her sternly. We’ve screamed, laughed, threatened, begged, pleaded, cried and offered every imaginable bribe. But this seems to be what works the best. She devotes the whole Christmas break to telling us about unspeakably awful incidents Svante goes up to the sheet of paper on the wall and writes: Lunch: 5 gnocchi. Time: 2 hours and 10 minutes. Not eating can mean many things. The question is what. The question is why. Svante and I look for answers. I spend the evenings reading everything I can find on the internet about anorexia and eating disorders. We’re sure it’s not anorexia. But, we keep hearing that anorexia is a very cunning disorder and will do anything to evade discovery. So we keep that door wide open. I speak endlessly to the children’s psychiatry service (BUP), the healthcare information service, doctors, psychologists and every conceivable acquaintance who may be able to offer the least bit of knowledge or guidance. At Greta’s school there’s a psychologist who is experienced with autism. She talks with both of us on the phone and says that a careful investigation must still be conducted, but in her eyes – and off the record – Greta shows clear signs of being on the autism spectrum. “High-functioning Asperger’s,” she says. Meeting after meeting follows where we repeat our story and explore our options. We talk away while Greta sits silently. She has stopped speaking with anyone except me, Svante and Beata. Everyone really wants to offer all the help they can but it’s as if there’s no help to be had. Not yet, at least. We’re fumbling in the dark. After two months of not eating Greta has lost almost 10kg, which is a lot when you are rather small to begin with. Her body temperature is low and her pulse and blood pressure clearly indicate signs of starvation. She no longer has the energy to take the stairs and her scores on the depression tests she takes are sky high. We explain to our daughter that we have to start preparing ourselves for a stay at the hospital, where it’s possible to get nutrition and food without eating, with tubes and drips. In mid-November there’s a big meeting at BUP. Greta sits silently. As usual. I’m crying. As usual. “If there are no developments after the weekend then we’ll have to admit you to the hospital,” the doctor says. On the stairs down to the lobby Greta turns round. “I want to start eating again.” All three of us burst into tears and we go home and Greta eats a whole green apple. But nothing more will go down. As it turns out, it’s a little harder than you think to just start eating again. We take a few careful, trial steps and it works. We inch forward. She eats tiny amounts of rice, avocado and bananas. We take our time. And we start on sertraline, an antidepressant. Svante and Greta have been at the end-of-term ceremony at school where they tried to make themselves invisible in the corridors and stairwells. When students openly point and laugh at you – even though you’re walking alongside your parent – then things have gone too far. Way too far. At home in the kitchen, Svante explains to me what they’ve just experienced while Greta eats her rice and avocado. I get so angry at what I hear that I could tear down half the street we live on with my bare hands, but our daughter has a different reaction. She’s happy it’s in the open. She devotes the whole Christmas break to telling us about unspeakably awful incidents. It’s like a movie montage featuring every imaginable bullying scenario. Stories about being pushed over in the playground, wrestled to the ground, or lured into strange places, the systematic shunning and the safe space in the girls’ toilets where she sometimes manages to hide and cry before the break monitors force her out into the playground again. For a full year, the stories keep coming. Svante and I inform the school, but the school isn’t sympathetic. Their understanding of the situation is different. It’s Greta’s own fault, the school thinks; several children have said repeatedly that Greta has behaved strangely and spoken too softly and never says hello. The latter they write in an email. They write worse things than that, which is lucky for us, because when we report the school to the Swedish schools inspectorate we’re on a firm footing and there’s no doubt that the inspectorate will rule in our favour. I explain to Greta that she’ll have friends again, later. But her response is always the same. “I don’t want to have a friend. Friends are children and all children are mean.” Greta’s pulse rate gets stronger and finally the weight curve turns upwards strongly enough for a neuropsychiatric investigation to begin. Our daughter has Asperger’s, high-functioning autism and OCD, obsessive-compulsive disorder. “We could formally diagnose her with selective mutism, too, but that often goes away on its own with time,” the doctor tells us. We aren’t surprised. Basically, this was the conclusion we drew several months ago. On the way out, Beata calls to tell us she’s having dinner with a friend, and I feel a sting of guilt. Soon we’ll take care of you too, darling, I promise her in my mind, but first Greta has to get well. Summer is coming, and we walk the whole way home. We almost don’t even need to ration the burning of calories any more. Six months after Greta received her diagnosis, life has levelled out into something that resembles an everyday routine. She has started at a new school. I’ve cleared my calendar and put work on the back burner. But while we’re full up with taking care of Greta, Beata’s having more and more of a tough time. In school everything is ticking along. But at home she falls apart, crashes. She can’t stand being with us at all any more. Everything Svante and I do upsets her and in our company she can lose control. She is clearly is not feeling well. One day near her 11th birthday I find her standing in the living room, hurling DVDs from the bookshelf down the spiral staircase to the kitchen. “You only care about Greta. Never about me. I hate you, Mum. You are the worst bloody mother in the whole world, you bloody fucking bitch,” she screams as Jasper the Penguin hits me on the forehead. It’s autumn 2015 when Beata undergoes an evaluation for various neurodevelopmental disorders. She is diagnosed with ADHD, with elements of Asperger’s, OCD and ODD [oppositional defiant disorder]. Now that she has the diagnosis it feels like a fresh start for her, an explanation, a redress, a remedy. At school she has marvellous teachers who make everything work. She doesn’t have to do homework. We drop all activities. We avoid anything that may be stressful. And it works. Whatever happens we must never meet anger with anger, because that, pretty much always, does more harm than good. We adapt and we plan, with rigorous routines and rituals. Hour by hour. We try to find habits that work. The fact that our children finally got help was due to a great many factors. In part it was about existing care, proven methods, advice and medication. It was also thanks to our own toil, patience, time and luck that Greta and Beata found their way back on their feet. However, what happened to Greta in particular can’t be explained simply by a psychiatric label. In the end, she simply couldn’t reconcile the contradictions of modern life. Things simply didn’t add up. We, who live in an age of historic abundance, who have access to huge shared resources, can’t afford to help vulnerable people in flight from war and terror – people like you and me, but who have lost everything. In school one day, Greta’s class watches a film about how much rubbish there is in the oceans. An island of plastic, larger than Mexico, is floating around in the South Pacific. Greta cries throughout the film. Her classmates are also clearly moved. Before the lesson is over the teacher announces that on Monday there will be a substitute teaching the class, because she’s going to a wedding over the weekend, in Connecticut, right outside of New York. “Wow, lucky you,” the pupils say. Out in the corridor the trash island off the coast of Chile is already forgotten. New iPhones are taken out of fur-trimmed down jackets, and everyone who has been to New York talks about how great it is, with all those shops, and Barcelona has amazing shopping too, and in Thailand everything is so cheap, and someone is going with her mother to Vietnam over the Easter break, and Greta can’t reconcile any of this with any of what she has just seen. She saw what the rest of us did not want to see. It was as if she could see our CO2 emissions with her naked eye. The invisible, colourless, scentless, soundless abyss that our generation has chosen to ignore. She saw all of it – not literally, of course, but nonetheless she saw the greenhouse gases streaming out of our chimneys, wafting upwards with the winds and transforming the atmosphere into a gigantic, invisible garbage dump. She was the child, we were the emperor. And we were all naked. ‘You celebrities are basically to the environment what anti immigrant politicians are to multicultural society,” Greta says at the breakfast table early in 2016. I guess it’s true. Not just of celebrities, but of the vast majority of people. Everyone wants to be successful, and nothing conveys success and prosperity better than luxury, abundance and travel, travel, travel. Greta scrolls through my Instagram feed. She’s angry. “Name a single celebrity who’s standing up for the climate! Name a single celebrity who is prepared to sacrifice the luxury of flying around the world!” I was a part of the problem myself. Only recently I had been posting sun-drenched selfies from Japan. One “Good morning from Tokyo” and tens of thousands of “likes” rolled in to my brand-new iPhone. Something started to ache inside of me. Something I’d previously called travel anxiety or fear of flying but which was now taking on another, clearer form. On 6 March 2016 I flew home from a concert in Vienna, and not long after that I decided to stay on the ground for good. A few months later we walked home from the airport shuttle having met Svante and Beata off a flight from Rome.“You just released 2.7 tonnes of CO2,” Greta says to Svante. “And that corresponds to the annual emissions of five people in Senegal.” “I hear what you’re saying,” Svante says, nodding. “I’ll try to stay on the ground from now on, too.” Greta started planning her school strike over the summer of 2018. Svante has promised to take her to a building supplier’s to buy a scrap piece of wood that she can paint white and make a sign out of. “School Strike for the Climate”, it will say. And although more than anything we want her to drop the whole idea of going on strike from school – we support her. Because we see that she feels good as she draws up her plans – better than she has felt in many years. Better than ever before, in fact. On the morning of 20 August 2018, Greta gets up an hour earlier than on a regular school day. She has her breakfast. Fills a backpack with schoolbooks, a lunchbox, utensils, a water bottle, a cushion and an extra sweater. She has printed out 100 flyers with facts and source references about the climate and sustainability crisis.  She walks her white bicycle out of the garage and rolls off to parliament. Svante cycles a few metres behind her, with her home-made sign under his right arm The weather is rather lovely. The sun is rising behind the old town and there is little chance of rain. The cycle paths and pavements are filled with people on their way to work and school. Outside the prime minister’s office, Greta stops and gets off her bicycle. Svante helps her take a picture before they lock the bicycles. Then she nods an almost invisible goodbye to Dad and, with the sign in her arms, staggers around the corner towards the government block where she stops and leans the sign against the greyish-red granite wall. Sets out her flyers. Settles down. She asks a passerby to take another picture with her phone and posts both pictures on social media. After a few minutes the first sharing on Twitter starts. The political scientist Staffan Lindberg retweets her post. Then come another two retweets. And a few more. The meteorologist Pär Holmgren. The singer-songwriter Stefan Sundström. After that, it accelerates. She has fewer than 20 followers on Instagram and not many more on Twitter. But that’s already changing. Now there is no way back. A documentary film crew shows up. Svante calls and tells her that the newspaper Dagens ETC has been in touch with him and are on their way. Right after that [another daily newspaper] Aftonbladet shows up and Greta is surprised that everything is moving so fast. Happy and surprised. She wasn’t expecting this. Ivan and Fanny from Greenpeace show up and ask Greta if everything is OK. “Can we help with anything?” they ask. “Do you have a police permit?” Ivan asks. She doesn’t. She didn’t think a permit would be needed. But evidently it is. “I can help you,” Ivan says. Greenpeace is far from alone in offering its support. Everyone wants to do their utmost to help out. But Greta doesn’t need any help. She manages all by herself. She is interviewed by one newspaper after the next. The simple fact that she is talking to strangers without feeling unwell is an unexpected joy for us parents. Everything else is a bonus. The first haters start to attack, and Greta is openly mocked on social media. She is mocked by anonymous troll accounts, by rightwing extremists. And she is mocked by members of parliament. But that’s no surprise. Svante stops by to make sure that everything is OK. He does this a couple of times every day. Greta stands by the wall and there are a dozen people around her. She looks stressed. The journalist from [newspaper] Dagens Nyheter asks whether it’s OK if they film an interview, and Svante sees out of the corner of his eye that something is wrong. “Wait, let me check,” he says, and takes Greta behind a pillar under the arch. Her whole body is tense. She is breathing heavily, and Svante says that there’s nothing to worry about. “Let’s go home now,” he says. “OK?” Greta shakes her head. She’s crying. “You don’t need to do any of this. Let’s forget about this and get out of here.” But Greta doesn’t want to go home. She stands perfectly still for a few seconds. Breathes. Then she walks around in a little circle and somehow pushes away all that panic and fear that she has been carrying inside her for as long as she can remember. After that she stops, and stares straight ahead. Her breathing is still agitated and tears are running down her cheeks. “No,” she says. “I’m doing this.” We monitor how Greta is feeling as closely as we can. But we can’t see any signs that she’s feeling anything but good. She sets the alarm clock for 6.15am and she’s happy when she gets out of bed. She’s happy as she cycles off to parliament, and she’s happy when she comes home in the afternoon. During the afternoons she catches up on schoolwork and checks social media. She goes to bed on time, falls asleep right away and sleeps peacefully the whole night long. Eating, on the other hand, is not going well. “There are too many people and I don’t have time. Everyone wants to talk all the time.” “You have to eat,” Svante says. Greta doesn’t say anything. Food is a sensitive topic. The most difficult one. But on the third day something else happens. Ivan from Greenpeace stops by again. He’s holding a white plastic bag. “Are you hungry, Greta? It’s noodles. Thai,” he says. “Vegan. Would you like some?” He holds out the bag and Greta leans forward and reaches for the food container. She opens the lid and smells it a few times. Then she takes a little bite. And another. No one reacts to what’s happening. Why would they? Why would it be remarkable for a child to be sitting with a bunch of people eating vegan pad thai? Greta keeps eating. Not just a few bites but almost the whole serving. Greta’s energy is exploding. There doesn’t seem to be any outer limit, and even if we try to hold her back she just keeps going. By herself.  Beata sits with Greta one day in front of parliament. But this is Greta’s thing. Not hers. The sudden fuss over her big sister is not easy to handle. Beata sees that Greta suddenly has 10,000 followers on Instagram, and we all think that’s crazy. But Beata handles it well. Even when her own feed is filled with comments about Greta, and can you tell her this and that. All everyone suddenly cares about is Greta, Greta and Greta. “It’s nuts,” Beata says one afternoon after school. “It’s exactly like Beyoncé and Jay-Z,” she states, with an acerbic emphasis. “Greta is Beyoncé. And I’m Jay-Z.” We get death threats on social media, excrement through the letter box, and social services report that they have received a great number of complaints against us as Greta’s parents. But at the same time they state in the letter that they “do NOT intend to take any action”. We think of the capital letters as a little love note from an anonymous official. And it warms us. More and more people are keeping Greta company in front of parliament. Children, adults, teachers, retirees. One day an entire class of elementary-school pupils stops and wants to talk, and Greta has to walk away for a bit. Feels mild panic. She steps aside and starts crying. She can’t help it. But after a while she calms herself down and goes back and greets the children. Afterwards she explains that she has a hard time associating with children sometimes because she has had such bad experiences. “I’ve never met a group of children that hasn’t been mean to me. And wherever I’ve been I’ve been bullied because I’m different.” Several times a day people come up and say that they have stopped flying, parked the car or become vegans thanks to her. To be able to influence so many people in such a short time is bewildering in a good way. The phenomenon keeps growing. Faster and faster by the hour. In the run-up to the end of the strike, Greta is being followed by TV crews from the BBC, German ARD and Danish TV2. Altogether 1,000 children and adults sit with Greta on the last day of the school strike. And media from several different countries report live from Mynttorget Square. She has succeeded. Some say that she alone has done more for the climate than politicians and the mass media have in years. But Greta doesn’t agree. “Nothing has changed,” she says. “The emissions continue to increase and there is no change in sight.” At three o’clock Svante comes and picks her up and they walk together over to the bicycles outside Rosenbad. “Are you satisfied?” Svante asks. “No,” she says, with her gaze fixed on the bridge back towards the old town. “I’m going to continue.” The next day is Saturday 8 September. It’s the day before the Swedish parliamentary elections and Greta is going to speak at the People’s Climate March in Stockholm. She has only given one speech before at a small event. Prior to that she’d never spoken in front of more people than fit in a classroom, and on those few occasions she had not exactly seemed at ease. There are a lot of people in the park for the march and the rally. Almost 2,000 have crowded together at the stage and more are on their way. Somehow there’s a different feeling about this protest. It doesn’t feel the same as usual. It feels as if something might happen. Soon. It’s no longer just the familiar faces. The regulars. The activists. The Greenpeace volunteers in polar-bear suits. Here, suddenly, are all conceivable kinds of people and characters. People who might have all sorts of jobs. “This is my first demonstration,” states a well-dressed man in his 40s. “Mine too,” a woman next to him says, with a laugh. The host introduces Greta and she walks slowly but steadily into the middle of the stage. The audience cheers. Svante, on the other hand, is scared out of his wits. What will happen now? Will she start crying? Is she going to run away? He feels like an awful parent for not putting his foot down and saying “No” from the start. All this is starting to feel too big and unreal. But Greta is as calm as can be. She takes the speech out of her pocket and looks out over the sea of people. Then she grasps the microphone and starts speaking. “Hi, my name is Greta,” she says in Swedish. “I am going to speak in English now. And I want you to take out your phones and film what I’m saying. Then you can post it on your social media.” “My name is Greta Thunberg and I am 15 years old. And I have schoolstriked for the climate for the last three weeks. Yesterday was the last day. But…” She pauses. “We will go on with the school strike. Every Friday, as from now, we will sit outside the Swedish parliament until Sweden is in line with the Paris agreement.” The crowd cheers. Greta continues. “I urge all of you to do the same. Sit outside your parliament or local government, wherever you are, until your country is on a safe pathway to a below-two-degree warming target. Time is much shorter than we think. Failure means disaster.” Her voice is steady and there are no signs of nervousness. She appears to be at ease up there. She even smiles sometimes. “The changes required are enormous and we must all contribute in every part of our everyday life. Especially us in the rich countries, where no nation is doing nearly enough.”  The audience stands up. Shouting, applauding. The ovation doesn’t stop. And Greta is smiling the most beautiful smile I have ever seen her smile. I’m watching everything from a live stream on my phone in the hallway outside the dressing rooms at the Oscarsteatern. The tears keep coming. • This is an edited extract from Our House Is on Fire: Scenes of a Family and a Planet in Crisis by Malena and Beata Ernman, Greta and Svante Thunberg, published by Penguin on 5 March (£16.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15"
"
Share this...FacebookTwitterI get mail, too.
Though not very friendly at times. But here’s one from reader Jimbo who delivers quite a neat collection of contradictory reports. No matter if it’s hot-cold, wet-dry, green-brown, windy-calm, bad-nice – its all due to manmade climate change.
=====================================================
Hi Pierre Gosselin,
The last time you posted the following list on Notrickszone a Warmist [Hunneycut?] attacked it as being from news stories. I have now changed all the links to point to PEER REVIEWED materials and made it even longer.
Partly referenced and inspired by Numberwatch.
What I want to know from ‘Warmists’ is what would falsify the theory of Anthropogenic Global Warming?
Jimbo
LIST
Amazon dry season greener
Amazon dry season browner
Avalanches may increase
Avalanches may decrease – wet snow more though [?]
Bird migrations longer
Bird migrations shorter
Bird migrations out of fashion
Boreal forest fires may increase
Boreal forest fires may continue decreasing
Chinese locusts swarm when warmer
Chinese locusts swarm when cooler
Columbia spotted frogs decline
Columbia spotted frogs thrive in warming world
Coral island atolls to sink [?]
Coral island atolls to rise [? – ?]
Earth’s rotation to slow down
Earth’s rotation to speed up
East Africa to get less rain
East Africa to get more rain – pdf
Great Lakes less snow
Great Lakes more snow
Gulf stream slows down
Gulf stream speeds up a little
Indian monsoons to be drier
Indian monsoons to be wetter
Indian rice yields to decrease – full paper
Indian rice yields to increase
Latin American forests may decline
Latin American forests have thrived in warmer world with more co2!
Leaf area index reduced [1990s]
Leaf area index increased [1981-2006]


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Malaria may increase
Malaria may continue decreasing
Malaria in Burundi to increase
Malaria in Burundi to decrease [?]
North Atlantic cod to decline
North Atlantic cod to thrive
North Atlantic cyclone frequency to increase
North Atlantic cyclone frequency to decrease – full pdf
North Atlantic Ocean less salty
North Atlantic Ocean more salty
Northern Hemisphere ice sheets to decline [? – ? – ?]
Northern Hemisphere ice sheets to grow [?]
Plant methane emissions significant
Plant methane emissions insignificant
Plants move uphill
Plants move downhill [?]
Sahel to get less rain
Sahel to get more rain
Sahel may get more or less rain
San Francisco less foggy
San Francisco more foggy
Sea level rise accelerated
Sea level rise decelerated – full pdf
Soil moisture less
Soil moisture more
Squids get smaller
Squids get larger
Stone age hunters may have triggered past warming [?]
Stone age hunters may have triggered past cooling
Swiss mountain debris flow may increase
Swiss mountain debris flow may decrease
Swiss mountain debris flow may decrease then increase in volume
UK may get more droughts
UK may get more rain
Wind speed to go up [?]
Wind speed slows down [?]
Wind speed to speed up then slow down
Winters maybe warmer [? – ?]
Winters maybe colder ;O)
—END—
=================
Update: Also see Joe d’Aleo’s link here:
http://icecap.us/index.php/go/joes-blog/ten_major_failures_of_so_called_consensus_science/
Share this...FacebookTwitter "
"**Northern Ireland businesses have called for urgent financial aid after Stormont ministers agreed a multi-million pound support package to help people hit by Covid-19 lockdown restrictions.**
From Friday, non-essential shops and businesses will close for two weeks, as part of tougher measures across NI.
The executive had pledged to provide additional financial support to businesses forced to close.
B&B owner, Scott Borthwick, said ""there is a real sense of fear"".
The Stormont Executive's immediate package will be worth about Â£338m, while Â£150m is being set aside for longer-term rates relief.
Mr Borthwick, who is sole director of a bed and breakfast in Portrush, County Antrim, said he had yet to receive any financial assistance.
He told BBC NI's Good Morning Ulster programme: ""Where is the money? I have not seen any of the money.
""I know there are hundreds of people who messaged me last night haven't seen the money.
""What are they doing, why has the money not gone out? They have had months to do this.""
Mr Borthwick said he had taken a factory job for 12 weeks during the initial lockdown in order to survive.
""It was a sink or swim situation for myself, we had no money coming in - it couldn't have hit us at a worse time, the hospitality sector, this has come in the middle of March when you are coming out of a winter period,"" he added.
His business was able to reopen for a short time but has now closed again.
""There's no money left, it's got to the stage where there is nothing,"" he said.
Ciaran Smyth, who is the owner of licensed premises in Belfast city centre welcomed Monday's announcement for ""wet bars"", and said he had been ""devastated by four months of closure"".
""We are glad to receive something, the government aid doesn't really cover the expenses it is slightly token,"" he said.
""I maintain they don't have the money to cover us - you have rent, insurance, security, furlough shortfalls.
""They tend to forget the huge loans that we took out which all have to be paid now.""
Mr Smyth said at this stage they had ""no idea"" what they were going to get in terms of a financial package.
""My issue is we can't live just guessing what is going to happen the next day and that is the situation we are put in,"" he added.
""The executive should be considering these issues a long time ahead of what has happened.""
Finance Minister Conor Murphy set out full details of the plan in the assembly on Monday afternoon.
Mr Murphy said uncertainty with the virus and not knowing how much Stormont would receive from the Treasury had made planning difficult.
He said the financial support package he was announcing was as a result of an additional Â£400m provided from Westminster two weeks ago to support the executive's response.
The executive had faced criticism for not having new financial support in place before it announced the lockdown measures last Thursday evening.
But Mr Murphy said his officials were working as quickly as possible to process payments to those who need them."
"

Last week’s presidential debate revealed to 40 million viewers that global warming is an issue on which the candidates have clear differences, both on policy and in the veracity of their responses. Gov. Bush argued that “some of the scientists…have been changing their opinion a bit on global warming” and that we need a “full accounting” of the issue before creating policy. 



Bush is clearly correct here, and Vice President Gore is not countenancing the whole truth when he cites the supposedly unified opinion of scientists. Many scientists have reappraised global warming, most notably NASA’s James Hansen, who now argues that the rate of warming is much lower than initially forecast because plants are taking up carbon dioxide at an increasing rate. 



In other words, the planet is becoming greener. This is precisely the same position that has been maintained by the coal and oil industries for years. There has been no “full accounting” of global warming because no one has yet been able to devise a system whereby scientists who assess the problem do not also profit from defining it as a problem. 



Gore tried this with his much‐​awaited “National Assessment” of global warming, which is now held up by a lawsuit. The Assessment team, much like Mrs. Clinton’s health care consortium, apparently did an awful lot behind locked doors. 



Gore also intoned that “many people see the strange weather conditions that the old‐​timers say they’ve never seen before in their lifetimes” and that “storms are getting more violent an unpredictable.” 



Those claims are totally false, as anyone who studies weather knows. There are dozens of different weather parameters measured every day: high and low temperatures, rainfall, snowfall, and wind speeds, for example. The chance that an individual will see one of those parameters at an extreme value in their lifetime is exactly 100 percent. Some day must be the hottest day in your life.



Furthermore, the chance that an extreme value will appear in a given year is also high. Let’s work an example with monthly temperature and rainfall. There are 12 months in a year, each of which is ranked according to temperature and precipitation. That’s 48 chances in a year that a given month will be record warm, cold, wet or dry. Most climate information started being recorded in 1948, giving 52 years of data. Rounding the numbers off, if there are the same number of chances to set a record as there are years of observations, the chance that a record will be set this year is 50–50. 



Gore is fond of pointing to increased flood frequency in the United States, based on a study by federal climatologist Tom Karl. But other, equally esteemed climatologists at the U.S. Geological Survey just wrote to Gore’s assessment team admonishing that Karl’s result could be duplicated with random numbers. 



According to the United Nations, hurricane severity is decreasing for the storms that strike the United States. Tornado deaths are also declining. New research shows that, along with global warming, the extreme U.S. coldest temperatures have risen sharply while extreme high temperatures have declined. 



In the policy sphere, Bush’s most intriguing response during the debate was in code. Although the science promoting global warming is shaky at best, Bush is a big supporter of “clean coal technologies” and has proposed spending about $2 billion on their advancement.



This used to be a buzzword for getting pollutants such as sulfur and nitrogen oxides out of the combustion stream. But now it could mean more, such as getting carbon dioxide–the biggest contributor to the greenhouse effect–out, too. That’s a difficult operation, but engineers can do it today. 



How much will this raise the cost of energy? It might not be as expensive as initially suspected. If this technology is imposed on all fossil fuels, coal still comes out as comparatively cheaper, because there are about a jillion tons of it under our feet. Under this scenario, if you believe global warming is serious, locking up federal land and prohibiting mining is about the dumbest thing you can do for the environment, as President Clinton recently did in Utah. It also isn’t very savvy. Coal miners in Democratic West Virginia just announced they support Bush, in large part because of their fear that Gore, in his Jihad against global warming, will dial coal out of the nation’s energy stream.



Who says Dubya is slow? Gore reiterated in the debate that global warming is “the central organizing principal for civilization,” whereas Bush proposed a program that fights climate change and would have the enthusiastic support of both the fossil fuel industry and the United Mine Workers.
"
"

Like many indulgent boomers, I live in a big old house — thousands of square feet of Victorian, to be specific, overlooking the bucolic Shenandoah Valley of Virginia. Years ago, I did as I was told, installing a gas furnace, stove and, recently, a water heater. The very leakiness of houses like this frees them from zillions of trapped allergens and the buildup of dreaded radon, but the price is that heated air escapes, too. So last month I paid $600 to heat air, food and water.



Why? Two related reasons: The price of natural gas is at historic highs, even after allowing for inflation, and the November‐​December average temperature was the lowest ever measured in our 106‐​year temperature record.



This, in turn, has created a short supply, resulting in some compensatory behavior. Because prices are high, I now turn the heat way down at night. And the cats, once thrown out, are now invited in as bed‐​warmers, showing once again why throughout civilization, the number of Homo sapiens and Felis domesticus has been roughly equal.



But there’s another way to manage energy shortages that I discovered by searching the newspapers for what happened in previous cold winters. The last ones that looked like this one occurred consecutively, in 1976–77 and 77–78. Rather than allow prices to rise, President Carter announced we were in an “energy crisis” that was “the moral equivalent of war.” The headline in the Jan. 30, 1977 _Washington Post_ , 10 days into his presidency, screamed “Carter Urges Four‐​Day Work Week.”



He actually proposed legislation outlawing work. The Democratic Governor of New Jersey, Brendan Byrne, went one better. According to page A5 of the same _Post,_ “Byrne told homeowners to lower thermostats to 65 degrees in the day and 60 at night or face stiff fines and even prison sentences.”



So which do you prefer: markets or jail?



It’s also worth noting that similarities abound whenever the weather is extreme. That same 1977 edition of the _Post_ also carried this headline, “Changes in the Earth’s Weather Are Expected to Bring Trouble,” a story that blamed cold temperatures on global warming. Nevermind that this vetoes the First Law of Thermodynamics, which states that heating causes warmth and vice‐​versa. And, just as the story played ad nauseam a quarter‐​century later, climate fluctuation was painted as an abject disaster. The _Post_ cited a then‐​recent (1974) CIA report that said that “climate is now a critical factor. The politics of food will become the _central issue of every government._ ”



Nothing could have been more wrong, but little has changed. Two years ago I attended a hush‐​hush Defense Department briefing where the CIA again stated that climate would be the central issue for global security. The fact that the agency has botched this for 25 years speaks volumes about the continuity of culture in our nation’s capital. In that quarter‐​century world crop production increased at a demonstrably greater rate than food consumption.



What about the future? Dreaded global warming will tend to ameliorate the coldest temperatures of winter more than anything else, “tend” being the key word. In some years, like this one, it won’t be sufficient to relieve our usual misery. In others it will be a downright windfall. Consider 1997–98, when warming, in concert with El Nino, reduced the winter demand for energy by a whopping 15 percent, ultimately sending gasoline prices to their lowest level in decades and helping to kite dozens of stocks, which built hundreds of wealth‐​effect beach houses. Unless my profession is completely wrong (a distinct possibility), winters like that one will become more common.



Nonetheless, sometime in the coming decades this current winter, or something like it, will probably repeat. If we don’t have the energy supply, so will this year’s outrageous costs. While the last quarter‐​century has taught us that markets beat the slammer as rational energy use, it’s also time to acknowledge that abundant energy and economic wealth go hand‐​in‐​hand. Can we drill for energy in the Arctic National Wildlife Refuge and keep the Caribou alive? Of course, as the Trans‐​Alaska Pipeline, built in response to the 1973 Arab Oil Embargo, demonstrates.



But imagine, if you will, the state of the nation this chilly January if the U.S. Senate had ratified the Kyoto Protocol on global warming, which would have required us to reduce energy consumption by around one‐​third. Many calculations show that this would require a doubling in the price of fuel. In that case, only the rich could heat their old houses, cats wouldn’t do enough to ward off the cold and, as with Jimmy Carter, we’d be inaugurating a new president in four years.
"
"Christopher de Bellaigue makes quite a jump from the 1947 Agriculture Act and its guaranteed payments system to the EU arable area payments regime (Is this the future of farming?, 25 February). Once again under the “oven ready” new environmental land management proposals, farmers – as they mostly always have done – will be asked to farm under some sort of intrusive government regulation. In return for what? There is no disagreement that farming is a risky enterprise, needing good forward planning and security, and with climate change this will get worse. Having farmed under the transitional arrangements, with arable area and hill subsidies still in place, some sort of transition will still be necessary and a fallback, either through tax regulations or a form of “agrilite” subsidy support mechanism, will help farmers in the transition.  Farming without direct support may produce a generation of growers prepared to farm intensively without subsidy and with their own nods to biodiversity, causing a denudation of hills and marginal land with the subsequent loss of rural communities. All parts of the UK can benefit from more environmentally friendly farming practices, but to succeed the regulations must be less intrusive and not dilute much of the good work, done by the UK and EU in partnership, on reducing the input of agricultural chemicals, and on water and air qualityChris Jones(Nature reserve manager), Biddenham, Bedford • Christopher de Bellaigue covers in depth many aspects of the problems we face in our management of the land. He mentions very briefly “mixed” farming which is using a rotational system whereby fertility removed by arable crops is returned by other crops such as turnips and grass leys used by grazing animals. Thomas Coke invented the Norfolk four-course rotation and put it into practice on his Holkham estate in Norfolk. In 2015, over 140 years later, a similar updated six-course rotation was introduced on the same estate and the results in terms of increased yields etc have been remarkable. As somebody who farmed in the era of chemical farming, I think it fair to say that many farmers, with much encouragement from the fertiliser and chemical giants, found there was no need to rotate and basically forgot how to farm. This has ultimately resulted in deterioration of soil structure, wildlife habitats, and water quality, and obviously cannot continue. A return to a sustainable rotational system is not impossible, and the old farming chestnut “farm the land as though you are going to live forever and run the farm business as if you were going to die tomorrow” should be remembered.Kevin CaveneyWick, Glastonbury, Somerset   • Christopher de Bellaigue accurately sets out the facts which will ensure that British farming is about to enter a process of major change. In doing so he points out that 71% of the land area of Britain is devoted to farming, yet this industry only produces 10% of greenhouse gas emissions. In a previous Guardian article (Lab-grown food will soon destroy farming – and save the planet, George Monbiot, 8 January) it was suggested that agriculture is a major source of GG emission. Would it not be better if we were to concentrate on reducing the 90% of greenhouse gas emissions that emanate from only 30% of the land area?Richard HarveyOakham, Rutland  • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition  "
nan
"Having reached the self-evident conclusion that one can’t win government without the support of at least a handful of regional communities, Labor is now coming to terms with its use of the C-word. With their opponents to their left and their right taking up the crude chant of “coal, coal, coal” – as shorthand for economic betrayal on the one side and environmental betrayal on the other – watching Labor field questions on climate is like watching a tightrope walker attempt to cross Niagara Falls. For a party that was designing and implementing detailed economy-wide carbon-reduction policies in government just a decade ago, their journey to the trenches of the coal culture war that passes as climate debate these days is like a descent into hell. In some heartland communities coal has become Labor’s kryptonite: the lack of vocal support for the industry a sure sign of neglect and betrayal, “just transition” code for human redundancy, green jobs a fairytale. When Labor talks of climate action, these communities hear that they are being asked to sacrifice their livelihoods for a greater good that doesn’t include them – a message the Nationals and those further to the right are only too happy to reinforce. But for Labor’s progressive city base, the lack of a vocal rejection of coal is portrayed as a lack of genuine commitment to climate action. For them coal is more like a herpes sore: Labor’s got it, they are not going to get rid of it, they don’t want to talk about it, but they will be duty bound to ’fess up to their condition before they can expect anyone to kiss them. Ergh. In this context, securing an alliance to win government becomes a challenging two-step. First, Labor must convince heartland communities that action on climate change does not render them collateral damage. Then they will need to convince progressives that maintaining the coal industry does not render them incapable of meaningful climate action. Opposition leader Anthony Albanese has begun to confront the issue head on in recent weeks, bringing the party into line with global and state governments with a 2050 target for net zero carbon emissions while asserting an ongoing role for coal exports in the economy. As this week’s Essential report shows, the first part of this formulation is not controversial, with the long-term zero target is accepted among the vast majority of voters. Indeed, Coalition support for net zero has shifted net 24 points in just a month: https://essentialvision.com.au/climate-change-policy-proposals. Despite the broad public sentiment, the Coalition can sniff division, seizing on these targets as a blunt instrument, with the “cost of action” mantra re-emerging amid threats of rural Armageddon. But it’s not just Labor supporting the zero by 2050 mechanism. Independent Zali Steggall has introduced a private members bill to this effect. In the unlikely event that the bill ever sees the light of day, there will be a cohort of Coalition MPs who will be forced to make a call between backing in the global benchmark or staying on the side of denial. Fun times for them, to be avoided at all cost. But given the 2050 target is endorsed by everyone from the NSW premier to Tory PM Boris Johnson, landing on the long-term target may be the easy part of Labor’s coal conundrum. Coming up with a way of keeping faith with progressives, particularly younger people demanding urgent action that includes the rapid decarbonisation of the energy sector, will be the real challenge. New Greens leader Adam Bandt’s response to Albanese’s weekend media where he accepted a long-term future for coal exports is instructive: “Scott Morrison loves coal, but it seems Anthony Albanese does too,” he tweeted. “If Labor thinks we’ll be mining and selling thermal coal in 2050, they’re not serious about climate change or ‘zero emissions’.” But even in this attack, Bandt is using nuance, distinguishing “thermal coal” used to produce electricity with “metallurgical” or “coking” coal, which still is a necessary ingredient for producing steel. Shining a light on this nuance may be the way through. It starts with being open about what decarbonisation really means: a long-term transition in energy that will see coal-fired power replaced by renewable technologies at both a local and global level. Over time it may also see new technology, such as hydrogen, replacing coal in steel production, although that technology is still nascent. For now, the transition means thinking through the next phase of government investment in energy, where the debate over new coal-fired power is critical and the points of difference between the government and opposition most stark. What should give the opposition leader some succour in embracing the C-word is that voters are ready to accept this sort of nuance. When given the choice, nearly half support a staged phase out of coal-fired power plants, rather than a rapid shutdown or taxpayer-funded life support. But the debate on coal can’t stop there. Labor needs to be upfront that coal exports are central to the Australian economy and prosperity, with steel critical to the development of our neighbours. In this context, differentiating types of coal and recognising the importance of this trade, when resources account for 60%  of our export income, is crucial. Finally, Labor can make the case to those who demand an immediate end to coal exports that this is outside the global frameworks that they champion, that the path to global abatement will come from domestic targets, not a unilateral closure of international trade. Playing a meaningful role in the evolution of the global framework, rather than sabotaging the process as the current government does, is the best way to ensure that exports taper off. None of these debates are easy to mount, or popular for those who see the moral imperative to act in black and white terms. But engaging progressives by talking about coal as a resource to be managed and not as poison to be banned seems the only way through the morass. Of course, by taking a middle track, Labor risks being wedged. When the political debate on climate action is reduced to a binary proposition on coal – for or against – the tightrope is impossible for Labor to walk. It’s not only politically fraught but nonsensical in practice. But by confronting the complexity of coal, its importance to the economy, the difference between domestic and export use, Labor can bring the discussion back down to earth, and in doing so, offer its credentials to lead Australia through this complex transition. After all, the 2050 net zero target will require transformation across many industries beyond energy, and what could be worse than a culture war every time? Trucks: for or against? Beef: for or against? And so on. It was a tantalising glimpse of climate wars 2.0 when agriculture minister David Littleproud posited that Labor’s net zero target meant “most of the national herd would likely have to go”. The problem with so much of the climate debate is that it is defined by slogans rather than nuance. “Kill coal”, while an eventual consequence of energy transition, is not a theory of change. Climate change is too big a challenge for slogans on either side of the debate. Finding a pathway to decarbonise the economy starts with changing government, and that will be even harder if Labor can’t use the C-word. Despite the sometimes deafening noise, our numbers suggest there are plenty of Australians open to a discussion about coal that’s not just black and white. • Peter Lewis is an executive director of Essential, a progressive strategic communications and research company"
"
Share this...FacebookTwitterDer Spiegel today has a report here on why the Vikings left Greenland, hat-tip reader DirkH. Also read Brown University press release here in English.
Proxies from all over the world have shown that global climate was as warm or even warmer during the so-called Medieval Warm Period back around a thousand years. It’s not called a “warm period” for nothing!
William D'Andrea, right, and Yonsong Husang, left, with sediment cores. Photo credit: William D'Andrea/Brown University. 
Der Spiegel writes about how scientists from USA and Great Britain examined sediment cores from two lakes near Viking settlements in Greenland and how the Little Ice Age hit and was one of the big factors that drove the Vikings off Greenland beginning in the middle of the 14th century. The scientists were able to produce a temperature reconstruction going back 5600 years. This is now reported in the Proceedings of the National Academy of Science.
Der Spiegel quotes William D’Andrea of Brown University in Providence, Rhode Island.
‘There really was a drop in temperature shortly before the Vikings disappeared.’ As a consequence the times for crop growing was shortened and little feed was provided to cattle.”
Yes, there really was a drop in temperature. How many more proxies is it going to take before the warmists abandon the fantasy that temperatures were more or less stable over the last 1000 years and shot up only when the Industrial Revolution began? D’Andrea adds:
“It is interesting to consider how rapid climate change may have impacted past societies, particularly in light of the rapid changes taking place today.”
Of course it’s plain to see where D’Andrea is headed. But there are big differences today. Firstly, society is much better equipped technically to adapt to climate change, and 2) climate gets nasty when it cools, and not when it warms. History shows that warm has benefited human and natural development, and cold always set it back. Indeed we are the first generation where we have a few whiners who constantly complain about warmer conditions.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There are also some non-differences. For example, the change we’ve seen over the last 100 years is no more remarkable than changes we had in the past. There is no more evidence of a significant human imprint today than there was 1000 or 500 years ago.
As another example of the misery brought on by cold, the Brown University press release writes:
The researchers examined how climate affected the Saqqaq and Dorset peoples. The Saqqaq arrived in Greenland around 2500 B.C. While there were warm and cold swings in temperature for centuries after their arrival, the climate took a turn for the bitter beginning roughly 850 B.C., the scientists found.
Yes – warm swings, cold swings, and turn for ther bitter, were all natural. The press release continues.
The Saqqaq exit coincides with the arrival of the Dorset people, who were more accustomed to hunting from the sea ice that would have accumulated with the colder climate at the time. Yet by around 50 B.C., the Dorset culture was waning in western Greenland, despite its affinity for cold weather.
Just how abrupt was climate change back then? The very beginning of the Brown University press release tells us (emphasis added):
Greenland’s early Viking settlers were subjected to rapidly changing climate. Temperatures plunged several degrees in a span of decades, according to research from Brown University.”
And today’s global temperature rise of 0.7°C over the last 100 years is supposed to be “unprecedented”? Yeah, right. Today’s climate change is no different than what we’ve seen in the past. If anything, some change-episodes in the past were considerably worse.
====================================================
Note: Scanning the German headlines, centrist and conservative publications are reporting this Greenland story while leftist publications are ignoring it.
Share this...FacebookTwitter "
"There are an awful lot of insects. It’s hard to say exactly how many because 80% haven’t yet been described by taxonomists, but there are probably about 5.5m species. Put that number together with other kinds of animals with exoskeletons and jointed legs, known collectively as arthropods – this includes mites, spiders and woodlice – and there are probably about 7m species in all. Despite their ubiquity in the animal kingdom, a recent report warned of a “bugpocalypse”, as surveys indicated that insects everywhere are declining at an alarming rate. This could mean the extinction of 40% of the world’s insect species over the next few decades.  What is particularly worrying is that we don’t know exactly why populations are declining. Agricultural intensification and pesticides are likely a big part of the problem, but it’s certainly more complicated than that, and habitat loss and climate change could also play a part. Although some newspaper reports have suggested that insects could “vanish within a century” total loss is unlikely – it’s probable that if some species die out, others will move in and take their place. Nevertheless, this loss of diversity could have catastrophic consequences of its own. Insects are ecologically important and if they were to disappear, the consequences for agriculture and wildlife would be dire.  It’s difficult to overstate how many species there are. Indeed, the 7m estimate above is likely a major underestimate. Lots of insects that look alike – so-called “cryptic species” – are distinguishable only by their DNA. There are an average of six cryptic species for every easily recognisable kind, so if we apply this to the original figure, the potential total number of arthropods balloons to 41m. Even then, each species has multiple kinds of parasites which are mostly specific to just one host species. Many of these parasites are mites which are themselves arthropods. Conservatively allowing just one kind of parasitic mite per host species brings us to a potential total of 82m arthropods. Compared with only around 600,000 vertebrates – animals with backbones – that’s 137 species of arthropod for every vertebrate species. Astronomical numbers like these caused the physicist-turned-biologist Sir Robert May to observe that “To a good approximation, all [animal] species are insects.” May was good at guessing big numbers – he became the UK Government’s chief scientist – and his quip in 1986 now seems pretty close to the mark. 


      Read more:
      Insect 'Armageddon': five crucial questions answered


 That’s just diversity though. How many individual insects would be lost in a mass extinction? And how much might they weigh? Their ecological importance will likely depend on both measures. It turns out that insects are so numerous that even though they are small, collectively their weight far outstrips that of the vertebrates. Perhaps the most celebrated ecologist of his generation, the Harvard ant enthusiast E.O. Wilson estimated that each hectare (2.5 acres) of Amazonian rainforest is inhabited by only a few dozen birds and mammals but well over one billion invertebrates, almost all of which are arthropods. That hectare would contain about 200kg dry weight of animal tissue, 93% of which would be made up of invertebrate bodies, and a third of that being just ants and termites. This is uncomfortable news for our vertebrate-centric view of the natural world. The role allotted to all these tiny creatures in the grand scheme of nature is to eat and be eaten. Insects are the key components of essentially every terrestrial food web. Herbivorous insects, which make up the majority, eat plants, using the chemical energy plants derive from sunlight to synthesise animal tissues and organs. The job is a big one, and is split into many different callings.  Caterpillars and grasshoppers chew plant leaves, aphids and plant hoppers suck their juices, bees steal their pollen and drink their nectar, while beetles and flies eat their fruits and devastate their roots. Even the wood of huge trees is eaten by wood-boring insect larvae.  In turn, these plant-eating insects are themselves eaten, being captured, killed or parasitised by yet more insects. All of these are, in their turn, consumed by still larger creatures. Even when plants die and are turned to mush by fungi and bacteria, there are insects that specialise in eating them. Going up the food chain, each animal is less and less fussy about what kind of food it will eat. While a typical herbivorous insect might consume only one species of plant, insectivorous animals (mostly arthropods, but also many birds and mammals) don’t much care about what kind of insect they catch. This is why there are so many more kinds of insect than birds or mammals. Because only a small fraction of the material of one kind of organism is transformed into that of its predators, each successive stage in the food chain contains less and less living matter. Even though efficiency in this process is known to be greater higher up the food chain, the animals “at the top” represent only a few percent of the total biomass. This is why big, fierce animals are rare. And so it’s obvious that when insect numbers decrease everything higher up in the food web will suffer. This is already happening – falling insect abundance in Central American tropical forest has been accompanied by parallel declines in the numbers of insect-eating frogs, lizards and birds. We humans ought to be more careful about our relationship with the little creatures that run the world. As Wilson commented: The truth is that we need invertebrates, but they don’t need us. Knowing about insects and their ways is not a luxury. Wilson’s friend and sometime colleague Thomas Eisner said: Bugs are not going to inherit the earth. They own it now. If we dispossess them, can we manage the planet without them?"
"The tropical cyclone rampaging south-eastern Africa has been described as one of the worst disasters ever to strike the southern hemisphere, with up to 2.6m people potentially affected in Mozambique, Malawi and Zimbabwe. The death toll may not be known for months, but it is already likely to have run to hundreds and possibly thousands of people. The brunt of the disaster has been borne by the coastal city of Beira in central Mozambique, 90% of which has been reportedly destroyed.  It is inevitable that people will connect Idai and climate change. It is always tricky to establish a direct causal link, but thanks to the evidence provided by a number of reports from the Intergovernmental Panel on Climate Change (IPCC), including this most recent one from October 2018, we know that climate change is bound to increase the intensity and frequency of storms like Idai. At the very least, this crisis is a harbinger of what is coming. Knowing this is a luxury that we must not squander. The IPCC estimates we have 12 years to prevent the Earth’s climate from crossing the 1.5℃ warming threshold, beyond which the effects are likely to become significantly worse. We should be spending this time both trying to minimise the increase in global temperatures and making people more prepared for similar events in future. The West has a duty to shoulder most of the burden here, for reasons we’ll explain in a moment. To do so, it needs to rethink its entire approach to international development.  Over the next few weeks, we may hear that Beira’s geography makes it particularly prone to natural disasters. We may hear that the region lacks an efficient early-warning system to alert its population. We may even hear some victim-blaming rhetoric that local people refused to leave despite being warned. Arguments like these all obscure a much more important explanation for what has happened.  It is not only the intensity of environmental disasters that makes them devastating – poverty also has a huge bearing on how things play out. Houses in poorer areas will often be less stable, storm barriers may be weaker, sanitation is often a problem, emergency services will be poorly resourced – and preventing disease outbreaks may be hindered by the poor state of public health services. The list of disadvantages goes on and on. Good example are hurricanes Katrina and Sandy in the US. Katrina struck New Orleans and the surrounding region in 2005 while Sandy hit New York and New Jersey in 2012. Sandy hit a far more densely populated area, but the death toll was at least five times lower than Katrina and it only caused about half the damage.  While Sandy was a category three storm to Katrina’s five, this was certainly not the only reason for the disparity. New Orleans, one of the poorest cities in the US, had poorly constructed levees which were easily overcome by the flood. Many people did not have cars, so couldn’t evacuate easily when the authorities told them to.  The earthquakes that struck Haiti and Japan in 2010 and 2016 respectively are another example. Both were of similar magnitude, but between 100,000 and 316,000 died in Haiti while in Japan it was just 42. One reason the Haitian disaster was so much worse  was the many thousands of unstable shanty houses in Port-au-Prince.  Inequalities within countries matter as well. The most vulnerable people are usually women, children, the poor, the elderly, ethnic minorities or indigenous people. Hurricane Katrina hit the elderly poor of New Orleans disproportionately hard, for instance, since they found it hardest to escape.  In all this inequality, the world’s wealthiest countries are heavily culpable. It stems from a complex economic system that disadvantages the Global South – not to mention the centuries-long experience of colonialism, the effects of which have hampered human development until this day.  In a world where 26 billionaires own as much wealth as the bottom half of humanity, the prospect of more frequent and intense climate disasters is only bound to exacerbate those inequalities. At the same time, Mozambique, Malawi and Zimbabwe contribute only a small fraction of the emissions that are causing such disasters. The West’s responsibility – along with other 
big emitters such as China – is therefore also a matter of climate justice. Part of that responsibility lies in changing the current approach to disaster aid. In major donor countries such as the US and UK, the guiding modus operandi of disaster relief has been reactive as opposed to proactive measures. The UK spent £1.2 billion in 2018/19 on emergency responses such as humanitarian interventions, while disaster prevention and preparedness has received a mere £76m. In the case of Cyclone Idai, the Department for International Development has now earmarked £18m to assist humanitarian relief efforts in Mozambique and Malawi – tripling the original pledge from a couple of days earlier.  To be clear, humanitarian responses are absolutely key, but insufficient on their own. They bandage wounds rather than fix what caused them. Instead, donor countries need to prioritise identifying the most vulnerable people both before and after a disaster, and ensure they receive the required support and are granted the agency to be actively involved in the process.   Besides the high-profile attempts to reduce global emissions, countries such as the UK should be offering support to poorer countries with everything from building flood defences to supporting social services to transferring technology. They should be forgiving national debt, redistributing wealth or at least giving them preferential trade deals to help them adapt to climate change themselves. This requires a rethinking not just of humanitarian aid but of development assistance in general.  Fortunately this is not just blue skies thinking on our parts. The House of Commons International Development Committee is currently reviewing the aid budget and considering an approach built around climate justice. This emerging discipline is gaining traction and credibility around the world and will be the subject of a World Forum taking place in Glasgow in June. Ahead of that, Tahseen Jafry – one of the co-authors of this article – will be making a keynote presentation in New York in April.  In short, it feels like momentum is steadily building. The UK caused a disproportionately large part of climate change. Now it ought to show leadership by pioneering a new approach to development that has inequality at the top of the agenda.  Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

Despite the overwhelming evidence that markets perform best when left alone by the government, it is still virtually taken for granted that one consumer product should be completely controlled by every government in the world. One product, so ubiquitous, that it’s used by almost everyone in the world on a daily basis: money.



Money is vitally important; the lifeblood of our financial system, but it is a product nonetheless. Consumers use this product not just as a medium of exchange but also as a liquid store of value and as a basis for accounting. Money producers, i.e., central banks, profit through seignorage, the ability to earn interest on their assets while issuing notes, e.g., dollar bills that pay no interest. The Fed creates money from thin air when it buys an interest bearing Treasury security and credits the seller with dollars. These dollars are not backed by anything in the sense that the Fed is not obligated to buy or convert dollars into anything.



Now there would be nothing wrong with this if the Fed were just some private institution trying to earn a living in an unregulated market. But of course, the Fed is a government‐​protected monopoly. Even prior to the Fed’s creation in 1913 there was always some level of government regulation of money. Prior to the Civil War, private banks used to issue dollar notes that were convertible into gold. Scholars debate whether the banking crises and panics during this period were a product of government regulation of these currencies and banks. What is clear is that there was never a completely free market in currency issuance. Well, the time has come.



To believe that the Fed is necessary is to believe that money is such a special product that it is optimal to give power to a group of expert economists either to use their best collective judgment in setting policy or to remove their discretion and create certain rules for them to administer. The only alternative is to eliminate government control of money.



Let’s examine these beliefs. Currently, Fed policy is essentially targeting low inflation. There are discussions about whether there is sufficient productivity growth to allow a high rate of GDP growth but the bottom line is protecting against an increase in inflation. While the target is not explicit, I would argue that markets are free enough and evolved enough to force the Fed towards this implicit target. 



The Fed controls the spot interest rate market (Fed funds) by creating or destroying spot dollars. All the other points along the money market yield curve are more or less determined without government interference. If the market feels the Fed is being lax on inflation, all the non‐​spot money market rates will rise and spot dollars will be sold for inflation hedges like commodities and foreign currencies. These trends will continue until the market catches the Fed’s attention and the Fed funds rate target is increased.



A more efficient policy would be to announce an explicit inflation target. With the continued development of the inflation protected bond market, which trades on real rates, this policy could be accomplished by explicitly targeting the market’s expected inflation rate (a first order approximation is calculated by taking the geometric difference of real rates from the nominal rates of comparable bonds). The Fed could choose the means by which to affect the markets. But this still leaves the problem of what inflation rate to target and how to define an inflation index.



It is arguable whether stock and real estate prices should be included in an inflation index because price bubbles are destabilizing and an indication that monetary policy is too loose. There certainly are non‐​linear effects in markets that can cause self‐​reinforcing trends that push markets away from their fundamentals. But bubbles, at best, can only be defined after the fact. Like an Oliver Stone movie where lack of evidence is an indication of the strength of the conspiracy, prices continuing to go up are evidence of the increasing size of the bubble! The term is meaningless to describe current conditions because it can’t be disproved; prices go up because there is a bubble; prices go down because there was a bubble, either outcome is proof of the existence of a bubble. So let’s stick with CPI.



Changes in the CPI can occur for non‐​monetary reasons (i.e., events independent of too much or little money creation). If the Fed is going to target CPI and minimize inflation due to monetary policy it has to be able to adjust for these non‐​monetary events. Central bankers are happy (and correct) to overlook price rises from supply shocks, such as droughts. The flipside is that price drops due to supply shocks coming from productivity growth also need to be adjusted for. If some innovation allows the cost of producing widgets (of equal quality) to drop the credit should not go to monetary policy. 



So the ability to measure productivity is critical regardless of whether an inflation target is explicit or implicit. Even with the way current Fed policy is formulated, productivity is at the core of the debate over the appropriateness of policy. Higher productivity growth means the economy is running more efficiently and can therefore run faster without increased inflation. But the technological and financial advances that we have seen and will continue to see (if the government can stay somewhat out of the way) are quickly transforming our ability to define let alone measure productivity. 



In the economy of the 1950s productivity was easy to measure as it was largely a function of widgets produced per man‐​hour. Today more and more firms are valued not for how many widgets they produce but for the quality of the ideas they produce — their intellectual property. These ideas may not generate any revenue today but in the future, and can therefore only be calculated today by looking at a firm’s market capitalization, a highly volatile measure. (This is not to argue that a firm’s market value is a perfect measure but that a priori it is the best unbiased measure available.)



Consider a firm whose sole function is to develop patents for licensing. The only quantitative measure of their output will be the number of patent applications generated per man‐​hour. However, what is important is the economic value of these patents, which may not correlate with the number of patents generated. This economic value can only be ascertained by seeing what value the market gives to the company.



While there may not be a lot of pure research companies like this, these types of companies effectively reside within virtually every tech firm. The research aspect of a tech firm is perhaps its most vital part because the trade secrets and patents produced are the engine of its future growth.



Many would be happy to sweep these issues under the table because, in their view, the economy is doing great and the Fed deserves much of the credit. In other words, if it ain’t broke don’t fix it. After all, the stock market has had an incredible run (despite being down some this year), the dollar is strong, inflation is seen to be under control, and the last time unemployment was this low Bill Clinton was busy not inhaling. Going back to the start of 1995, the S&P 500 has more than tripled and GDP growth has been 4.1 percent (go back further and this rate drops).



But as good as things have been, they fall short in comparison with the average GDP growth of over 5 percent in the hundred years prior to 1971, when Nixon closed the gold window and floated the dollar. This may not sound like much of a difference but it means that national income would have been more than 25 percent higher during these past five years had the economy been growing at a historically average rate.



Additionally, while inflation is low, it isn’t zero (even considering any and all measurement biases). So our money is still being debased, albeit at a slow pace. Given the tremendous technological advances and the subsequent increases in productivity, we should really see falling prices if the dollar were truly maintaining its usefulness as a store of value. Not a 1930s‐​style severe deflation that was caused by a variety of bone‐​headed fiscal and monetary policies that squashed both demand and supply. Just a steady, modest price decline reflecting productivity growth adjusted for quality improvement in products and services, where even products whose quality stays the same become cheaper to produce, raising the standard of living for all.



This recent period has also had many advantages over the prior period. With the Cold War over and defense spending being cut, valuable capital and human resources have been deployed in far more productive activities. Financial markets have evolved to be far more efficient, sophisticated, and global than a generation ago, allowing capital to be allocated more efficiently.



U.S. corporate performance has reaped the benefits of the restructurings during the 1980s. Granting employee options is now prevalent. This gives both better incentives to workers and provides a tax‐​advantaged way of compensation resulting in a far more efficient way to pay staff.



Yet despite these advantages we can’t seem to match the growth rates of the past. Of course this foregone income cannot all be chalked up to the failings of monetary policy. Certainly the enormous increases in taxes, regulation and government spending bear much of the blame.



But a truly market‐​based monetary policy might not have been such a willing accomplice to the increasing encroachment of government into the private sector. Without the implicit inflation tax and monopoly profits from the Fed, government would be forced explicitly to raises taxes (politically difficult) or to lower spending and deregulate. If the latter path is accompanied by tax cuts, experience shows that the economy benefits. In other words, the dynamic of tighter monetary policy (to eliminate current inflation) hurting short‐​term growth can be completely offset by fiscal policy. (Shockingly, many still hold to the Keynesian notion that increasing spending is the fiscal antidote to slowing growth when, in fact, it will exacerbate poor economic conditions by displacing the market’s more efficient allocation of resources. Japan in the 1990s is the latest example of this although they suffer under many other bad policies too.)



There’s certainly reason to have confidence that competent Fed officials will be able to avoid a sustained resurgence of inflation. It is also clear, as shown above, that there are many reasons to believe that we should be doing better and that therefore monetary policy is not optimal. The simple fact is that it is not just hard but impossible to tell what exactly the right policy should be at any given point in time. Economies are just too dynamic and are composed of too many players. No matter how smart members of the Fed may be, it makes no more sense to have them set monetary policy that it did for elite economists in centrally planned economies to set the price of food, cars or any other product.



Even if it were possible for Fed members to have completely mastered the art of central banking, is it healthy to vest so much power in one person or group of people? Imagine how the markets would react if Greenspan should have a very sudden demise.



So, how do we solve this problem of imperfect people using imperfect data creating a one size fits all policy? The first step is to remove discretion over policy from the government. The explicit targeting of inflation, previously mentioned, accomplishes this except that setting the goal would still be discretionary and likely to be sub‐​optimal given productivity measurement problems. An alternative is a return to a gold standard as way to effectively automate monetary policy. Despite its popularity in some quarters there are serious problems with any type of gold standard.



Go ahead and ignore the fact that no country has ever been able to maintain a gold standard (perhaps with Hong Kong and Argentina both having kept their currencies fixed to the dollar even during periods of severe economic distress, it is an indication that times have changed). The main flaw in fixing the dollar price of gold is that it assumes that the value of gold doesn’t change, like some physical constant, as immutable as the maximum speed of light. Gold’s value comes in two parts. First as a commercial product with limited use as an industrial metal and as jewelry or ornamentation. Clearly, the price of any commercial product will vary for a variety of reasons having nothing to do with an economy’s general price level.



The other component of gold’s value derives from its long history as a store of value and is wrongly assumed to be intrinsic. The supply of gold varies as new mines are found and mining techniques improved. Demand for gold as a store of value is based on the ability of gold to compete against other liquid stores of value. So, the only advantage of gold over a fiat currency is that the production of gold is not controlled by the government (ignoring taxes and regulation).



But if we fix the dollar to something that floats are we accomplishing anything? If the supply of gold increases due to a new way to mine, the dollar price of gold needs to fall. (As an example if the supply of gold doubled as a result of productivity boosts and there was no change in the productivity of producing other goods, the dollar price of gold should be halved as to protect the purchasing power of the dollar.) If the dollar price of gold is held constant, as in the gold standard, inflation will ensue (which is historically what happened under these conditions). 



Irving Fisher attempted to solve this problem with his compensated dollar plan. The plan allows for the dollar price of gold to be adjusted by a CPI priced in gold. The problem is that this plan only works if productivity in the mining sector is the same as in all other sectors of the economy. Not a bad assumption, perhaps, in Fisher’s day but unlikely to be true now and even less likely in the future. Two other possible solutions are to let the price of gold vary based on the relative productivity of gold production to productivity in the rest of the economy or instead of fixing the price of gold, fix the dollar price of gold plus other commodities (i.e., define the dollar as some fixed basket of commodities).



Well, the first solution is pretty messy, as it would require the government to calculate productivity rates that as argued above are inherently impossible to measure precisely. Furthermore, even if productivity could be measured precisely, it could in practice only be measured with a time lag. Not very helpful for knowing today’s price of something.



The second solution sounds more workable but determining the composition of the basket that would define the dollar is tricky. Leave something out of the basket and you have the same problem of the relative production productivities of what’s in and out of the basket. So just about everything the economy produces must go in the basket (obviously there are diminishing returns to accuracy for each additional product added, so you start with the most important products and work towards things that are used less). Now with many products in your basket you need to determine relative weights to reflect relative usage and importance of these products. But the resultant basket is just the same as that used to calculate inflation (with the identical problems as mentioned earlier)! Linking the dollar to this basket is exactly the same as having the Fed explicitly target zero inflation. 



The key to getting better monetary policy is not to merely limit the discretion of the government but to get the government out of the money business altogether by privatizing the Fed. Sell the whole thing to the highest bidder. The government would also have to deregulate enough to allow competitors to arise in the currency issuance game. 



By eliminating this government protected monopoly, more of the informational value of consumers ever changing preferences and behaviors could be used by producers to create a product that can best balance everyone’s needs. The free market can be looked at as a computer, calculating within all the constraints and using the near infinite amount of interrelationships and feedback between economic actors, on a global scale, to solve the problem of what are the best forms of money. This is an impossible calculation to do any other way.



Consumers would choose between U.S. dollars, euros, Citi dollars, GE dollars, etc. This choice would be based on confidence in the issuer and how well the product serves the consumer’s needs. Companies would issue money solely as a means to profit. Produce too much money and it becomes worthless, too little and not enough people will be able to use your money for you to profit. The notion of a government having a monetary policy goes the way of governmental industrial policy (or choosing which firms receive government help — see Asia). 



If private firms were allowed to compete on equal legal footing as a private Fed, currency competition would lead to better money just as market forces improve the quality of any product. In an unfettered environment, using precious metals as a backing for a currency is just a starting point. Nobody can predict the improvements and ingenious ideas that would emerge. Already consumers have reaped benefits from quasi‐​currencies like airline miles and credit card rebates in the form of products (clearly, these things don’t currently have the liquidity to make for a good form of money). 



There are usually several objections to private money. It is argued that it is necessary to have a lender of last resort in times of crisis and that only the Fed can fill this role. Of course, there is no reason why a private central bank couldn’t provide sufficient liquidity during a crisis and no reason why private regulators couldn’t get big financial institutions together to provide liquidity (J.P. Morgan did this in the panic of 1907, prior to the Fed’s creation). Also prior to the Fed’s creation, private clearinghouses would lend to members who were solvent but needed liquidity. Furthermore, using the Fed as a lender of last resort creates moral hazard. If the perception is that the Fed will be compelled to act in a crisis, the effect will be to allow participants to take more risk, as they believe they are receiving some downside protection from the Fed.



In 1998, a financial crisis that started in Asia threatened world markets. “Contagion” was used to describe the effect of countries with healthy economies seeing their markets roiled. The contagion spread through trade and capital flows between countries and from the market realizing countries with a certain economic profile (high current account deficits, low foreign exchange reserves, and a pegged currency) were vulnerable. Markets in developed countries became infected as credit spreads widened and volatility increased (a mathematical implication being that correlations between markets increase). To extend the biology analogy, the best defense for a population against mutant viruses is genetic diversity. The economic equivalent is the diversity of products and regulation that spring forth from a free market. This diversity would reduce the frequency of crises that might need a lender of last resort.



Another concern about a world with only private money is that things would be too complicated with all the exchange rates that would exist for these new currencies. Technology can easily solve this problem. Only want to see prices in one particular currency? Your handheld device or browser could automatically convert all prices and even facilitate conversion of your currency to one that would be acceptable to a merchant. Currencies that were too volatile would quickly fall into disuse, as part of their utility would always derive from them being a stable store of value.



As with any free market reform, there is no expectation that private money would lead to a perfect world where there are no crises or problems, just a better world. Better not just in a strictly utilitarian sense but also in a moral sense as people could store the fruits of their labor however they see fit and not be forced to submit to a tax (inflation) that is not explicitly levied and voted on.



Many involved in the information revolution are confident that with the help of new technologies, markets will continue to evolve and reach a point that government’s ability to tax and regulate will be stifled. What these optimists miss is that government too has the ability to evolve and counter these trends. Government’s monopoly on the use of force can trump any market innovation. Ultimately, it is necessary for the political climate to change before government will acquiesce and not try to fight the liberalizing effects of technology.



These changes are certainly not around the corner, but they should be our goals. 



Deregulate. Privatize. To improve our standard of living, to increase the level of freedom, and to set a powerful example for the world to follow. 
"
"
Foreword: I give thanks to Steve McIntyre for this analysis. Steve came to a conclusion similar to what I alluded to in my initial rebuttal where I said:
“For all I know, they could be comparing homogenized data from CRN1 and 2 (best stations) to homogenized data from CRN 345 (the worst stations), which of course would show nearly no difference.“
Steve does a superb job of deconstructing the memo’s undocumented results. Perhaps someday Dr. Thomas Peterson of NCDC will tell us how he did his analysis and show supporting data and methods. – Anthony
The Talking Points Memo
by Steve McIntyre reposted from Climate Audit
The NOAA Talking Points memo falls well short of a “full, true and plain disclosure” standard – aside from the failure to appropriately credit Watts (2009).
They presented the following graphic that purported to show that NOAA’s negligent administration of the USHCN station network did not “matter”, describing the stations as follows:
Two national time series were made using the same gridding and area averaging technique. One analysis was for the full data set. The other used only the 70 stations that surfacestations.org classified as good or best… the two time series, shown below as both annual data and smooth data, are remarkably similar. Clearly there is no indication for this analysis that poor current siting is imparting a bias in the U.S. temperature trends.

Figure 1. From Talking Points Memo.
Beyond the above sentence, there was no further information on the provenance of the two data sets. NOAA did not archive either data set nor provide source code for reconciliation.
The red graphic for the “full data set” had, using the preferred terminology of climate science, a “remarkable similarity” to the NOAA 48 data set that I’d previously compared to the corresponding GISS data set here  (which showed a strong trend of NOAA relative to GISS). Here’s a replot of that data – there are some key telltales evidencing that this has a common provenance to the red series in the Talking Points graphic.

Figure 2.  Plot of US data from www1.ncdc.noaa.gov/pub/data/cirs/drd964x.tmpst.txt
An obvious question is whether the Talking Points starting point of 1950 is relevant. Here’s the corresponding graphic with the 1895 starting point used in USHCN v2. Has the truncation of the graphic start at 1950 “enhanced” the visual impression of an increasing trend? I think so.

Figure 3. As Figure 2, but to USHCN v2 start
The Talking Points’ main point is its purported demonstration that UHI-type impacts don’t “matter”. To show one flaw in their arm-waving, here is a comparison of the NOAA U.S. temperature data set and the NASA GISS US temperature data set over the same period – a comparison that I’ve made on several occasions, including most recently here. NASA GISS adjusts US temperatures for UHI using nightlights information, coercing the low-frequency data to the higher-quality stations. The trend difference between NOAA and NASA GISS is approximately 0.7 deg F/century in the 1950-2008 period in question: obviously not a small proportion of the total reported increase.

Figure 4. Difference between NOAA and NASA in the 1950-2008 period.  In def F following NOAA (rather than deg C)
As has been discussed at considerable length, the NASA GISS adjusted version runs fairly close to “good” CRN1-2 stations – a point which Team superfans have used in a bait-and-switch to supposedly vindicate entirely different NASA GISS adjustments in the ROW, (adjustments which appear to me to be no more than random permutations of the data, a point discussed at considerable length on other occasions.)
For present purposes, we need only focus on the observation that there is a substantial trend difference between NOAA and GISS trends.
Given that, when NOAA’s Talking Points claim that there is a supposedly negligible difference between the average of their “good” stations and the NOAA average (which we know to run hot relative to GISS), then arguably this raises issues about the new USHCN procedures.
Y’see, while NOAA doesn’t actually bother saying how it did the calculations, here’s my guess as to what they did. The new USHCN data sets (as I’ll discuss in a future post) ONLY show adjusted data.  No more inconvenient data trails with unadjusted and TOBS versions.
When I looked at SHAP and FILNET adjustments a couple of years ago, one of my principal objections to these methods was that they adjusted “good” stations. After FILNET adjustment, stations looked a lot more similar than they did before. I’ll bet that the new USHCN adjustments have a similar effect and that the Talking Points memo compares adjusted versions of “good” stations to the overall average.
So what they are probably saying is this: after the new USHCN “adjustments” (about which little is known as the ink is barely dry on the journal article describing the new method and code for which is unavailable), there isn’t much difference between the average of good stations and the average of all stations.
If the NASA GISS adjustment procedure in the US is justified (and most Team advocates have supported the NASA GISS adjustment in the US), then the Talking Points memo merely demonstrates that there is something wrong with the new USHCN adjustments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95059674',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Mike Bloomberg has donated hundreds of millions of dollars to environmental advocacy causes, but his campaign is coming under fire for a climate plan that lags far behind the other Democratic candidates for president. In the latest televised Democratic presidential debate on Wednesday night, Bloomberg said he wouldn’t “go to war with China” over its highest-in-the-world carbon emissions. He called fracked gas a “transition fuel”, and said that “we want to go to all renewables, but that’s still many years from now”. But, he added: “The world is coming apart faster than any scientific study had predicted. We’ve just got to do something now.”  The former New York City mayor has committed to rejoining the international climate agreement that Donald Trump plans to exit. He says he would cut carbon pollution in half by 2030 and pursue a 100% clean energy standard, both on par with what scientists say is needed. But he has offered few details on how he would achieve those goals compared with his competitors. He has broadly pledged to financially support new energy technology, nix fossil fuel tax subsidies and issue a new federal rule for power plant emissions. One organization, the Center for Biological Diversity’s Action Fund, ranks Bloomberg’s plan last out of six contenders – tied with the Minnesota senator Amy Klobuchar. By comparison, the group gives the Vermont senator Bernie Sanders a perfect score. Sanders would declare a national climate emergency, end fossil fuel production and exports, ban fracking for fossil gas, prosecute oil companies for their role in the crisis and give the public – rather than private companies – control of the power system. Brett Hartl, chief political strategist for the Center for Biological Diversity’s Action Fund, said Bloomberg, who came to the race late and is massively outspending his peers on advertising, is trying to mask his inadequate plan with “snazzy climate ads”. “There’s no discussion about how he would actually accomplish any of this and most of it is frankly sort of vague things that actually Congress would have to do,” Hartl said. “It’s just not very ambitious, there aren’t a lot of details and when you look at all of the other candidates it’s probably the least fleshed-out plan of any of them.” Bloomberg spokeswoman Daphne Wang said: “When it comes to comparing the candidates’ plans, most have similarly aggressive targets for decarbonizing the US economy, but we think that Mike has a more detailed and concrete, practical plan for how to actually meet those goals.” She added: “His plans look at every possible lever for climate action – using executive authority, budget and tax bills, and appropriations bills, not just legislation – so that he can maximize climate progress regardless of whether the Senate is in Democratic control.”  Wang said some rankings are focused on shutting down fossil fuels, ceasing production and exports, but “the difference with these groups’ priorities is a difference of strategy, not goals”. “These rankings assume that the most effective way of reducing fossil fuel usage is to limit the supply; Mike thinks the way for the world to move beyond coal, oil and gas is to replace them with something cleaner,” she said. She added that “unlike most of his primary rivals, Mike has actually meaningfully reduced emissions”, including by helping to retire more than half the US coal fleet with contributions to the Sierra Club. While the climate crisis is increasingly important to voters, it falls behind other concerns, particularly about the state of healthcare in the US, according to a new poll from Politico and Harvard’s TH Chan School of Public Health. A little more than half of respondents, and 68% of Democrats, said that “making major increases in federal spending and regulation to reduce climate change” is an “extremely” or “very important” priority. But some advocates worry that without further review voters will assume Bloomberg’s climate plan is sufficient because of his background. Bloomberg is the only candidate who hasn’t committed to an overall climate spending level, which his campaign said is because his various policy plans are related and can only be assessed once they are all rolled out. Sanders says his blueprint would cost $16tn. At the lower end of the range candidates have pledged, Klobuchar would spend $1tn on energy infrastructure. Bloomberg also doesn’t specify a date by which he would phase out fossil fuels, although he says his first priority would be to issues a series of executive actions to reach 80% clean electricity within eight years. A high-profile philanthropist, Bloomberg is hugely influential in environmental advocacy, having spent big on many groups, including the Sierra Club’s Beyond Coal campaign which funded lawyers to bring legal battles against coal plants at the state level. Last summer Bloomberg pledged $500m to a new program called Beyond Carbon, aimed at closing every coal plant in the country and halting the growth of gas-powered electricity. As one of the richest people in the United States, Bloomberg has also built a large political and support network for local leaders, and he has won endorsements from high-profile mayors, including London Breed in San Francisco. By entering the race late, Bloomberg escaped scrutiny of his major policy proposals. He also dodged pressure from activists such as the youth-run Sunrise Movement, which argues that despite Bloomberg’s record of climate advocacy he’s not aligned with the “vision and values” of a Green New Deal – a broader reimagining of American society that would address both inequity and the climate crisis – which Sanders outlined and co-sponsored with the New York congresswoman Alexandria Ocasio-Cortez. The Sunrise Movement – which has endorsed Sanders – has criticized Bloomberg, a former Republican, for fighting labor unions and backing the re-election of Michigan’s former Republican governor Rick Snyder following the water crisis in Flint. The group says Bloomberg’s history of supporting racial profiling through New York’s stop-and-frisk programs, which he has since renounced, is at odds with a Green New Deal. And the organization has highlighted how he has backed gas as a replacement to coal. Coal has a higher carbon output than gas, but gas is increasingly a major threat to climate progress. Bloomberg would not ban fracking, a method of extraction that has made a huge amount of cheap gas available to drillers. Bloomberg’s campaign pushed back on the criticism, arguing he supports the right to form unions and has a “progressive agenda for criminal justice reform”. “While gas played a role in the early stages of retiring coal plants because it was less polluting and cheaper for consumers, once renewable energy became even cheaper than gas, Mike announced his investment in Beyond Carbon – and his goal to stop the expansion of gas plants,” his campaign said. Bloomberg also supported the Keystone XL pipeline which would move petroleum from oil sands in Canada. Julian Brave NoiseCat, director of Green New Deal strategy for the thinktank Data for Progress, said Bloomberg’s late entry has allowed “for a very easy narrative for him, which is that he’s a big philanthropist that has put his money where his mouth is and done a lot for funding of climate [work]. “We need a more substantive conversation about what the difference is between what looks like the two frontrunners,” NoiseCat said. He said he “wouldn’t be surprised if more and more activists show up to his rallies and take him to task”. On the other hand, he added, “having someone who could spend basically unlimited money on paid media and could leverage his massive philanthropy and political network to just mobilize the gears on climate and make something happen is worth sitting with for a minute”."
"Like something out of a zombie movie, species that were once thought extinct seem to be rising from the dead. Between February 21 and March 4 2019, three notable rediscoveries were announced – the Fernandina Island Galápagos tortoise (Chelonoidis phantasticus), which was last seen in 1906; Wallace’s giant bee (Megachile pluto), which had supposedly disappeared in 1980; and the Formosan clouded leopard (Neofelis nebulosa brachyura), which disappeared after the last sighting in 1983 and was officially declared extinct in 2013.  These rediscoveries suggest we may know very little about some of the world’s rarest species, but they also raise the question of how species are declared extinct in the first place. The IUCN Red List collates a global register of threatened species and measures their relative risks of extinction. The Red List has a set of criteria to determine the threat status of a species, which are only listed as “Extinct” when… … there is no reasonable doubt that the last individual has died. According to the Red List, this requires… … exhaustive surveys in known and/or expected habitat, at appropriate times… throughout its historic range [which] have failed to record an individual. Surveys should be over a time frame appropriate to the taxon’s life cycle and life form. Given all the evidence – or rather, lack of evidence – that’s needed, it’s surprising that any species is ever declared extinct. The criteria show that to understand whether a species is extinct, we need to know what it was doing in the past.  Sightings at a certain time and in a certain place make up our knowledge of a species’ survival, but when a species becomes rare, sightings are increasingly infrequent so that people start to wonder whether the species still exists. People often use the time since the last sighting as a measure of likelihood when deciding if a species has died out, but the last sighting is rarely the last individual of the species or the actual date of extinction. Instead, the species may persist for years without being seen, but the length of time since the last sighting strongly influences assumptions as to whether a species has gone extinct or not. But what is a sighting? It can come in a variety of forms, from direct observation of a live individual in the flesh or in photographs, indirect evidence such as foot prints, scratches and faeces, and oral accounts from interviews with eyewitnesses.  But these different lines of evidence aren’t all worth the same – a bird in the hand is worth more than a roomful of recollections from people who saw it in the past. Trying to determine what are true sightings and what are false complicates the declaration of extinction. The idea of a species being “rediscovered” can confuse things further. Rediscovery implies that something was lost or forgotten but the term often gives the impression that a species has returned from the dead – hence the term “lazarus species”. This misinterpretation of lost or forgotten species means the default assumption is extinction for any species that hasn’t been seen for a number of years. So, what does this mean for the three recently “rediscovered” species? While a living specimen of the Fernandina Island Galápagos tortoise had not been seen since 1906, indirect observations of tortoise faeces, footprints and tortoise-like bite marks out of prickly pear cacti had been made as recently as 2013. The uncertainty around the quality of these later observations and the long time since the last living sighting probably contributed to it being declared “Critically Endangered (Possibly Extinct)” in 2015. In the natural world, a species is presumed extinct until proven living. Wallace’s giant bee may not have been recorded in the last 38 years but it was never actually declared extinct according to the IUCN Red List. In fact, for many years it languished under the criteria of Data Deficient and was only recently assessed as Vulnerable. So, while this is an exciting find for something that hadn’t been seen for so long, its rediscovery shows how little is known about many rare species in the wild, rather than how scarce they are. The Formosan clouded leopard, meanwhile, was actually listed as Extinct. The last sighting of the species was in 1983, based on interviews with 70 hunters, and extensive camera trapping during the 2000s failed to detect its presence. It was officially declared extinct in 2013.  While the giant tortoise and bee were proclaimed alive after living specimens were found, the clouded leopard’s rediscovery is more uncertain. Based on sightings on two separate occasions by two sets of wildlife rangers, the evidence is compelling. But whether the Formosan Clouded Leopard has really risen from the dead will require considerably more effort to prove."
"**On Wednesday, Chancellor Rishi Sunak begins setting out plans for what he hopes will be an economy beyond Covid-19.**
This Spending Review - detailing the money government departments will get for things like the NHS, education, roads, and police - only covers the financial year 2021-22. It will also set out money for the devolved administrations Scotland, Wales and Northern Ireland.
In normal times, reviews cover three or four years. But such is the economic uncertainty that this look-ahead has been limited to the next 12 months.
Even so, Mr Sunak will point to the direction of travel for spending (and possibly tax rises) for future years. Few reviews can have been so anticipated. Here's what to watch out for.
The economic shock has left the UK poorer. By the end of this year the economy is expected to be at least 10% smaller than pre-pandemic.
Alongside the Spending Review, Mr Sunak will disclose latest forecasts for the economy and public finances from the independent Office for Budget Responsibility (OBR).
Earlier this year, the OBR forecast a 13% contraction. While it is not expected to be that bad, the shrinkage will still likely be in the double-digits, and with public borrowing topping Â£350bn - something not seen in peacetime.
A difficulty for the chancellor is that big tranches of public service spending have already been made. Despite that, some areas will reportedly get more: NHS England, schools and defence. According to the Institute for Fiscal Studies (IFS), some two-thirds of public service spending has been pre-determined.
The key question is whether the remaining third is enough to go round. The answer is almost certainly not. The IFS thinks ""unprotected"" services such as the courts, prisons or local government are vulnerable to cuts. The overseas aid budget is also in the line of fire.
Saving, not spending, will dominate Wednesday's agenda. And one of the biggest savings could be a public sector pay freeze. It would be hugely controversial. Media leaks last week claimed Mr Sunak wants a freeze for everyone except frontline NHS staff.
That won't go down well with the police, teachers, civil servants or anyone who thinks they've done their bit to ensure the public sector keeps going in tough times. Even a return to a 1% cap is likely to be fiercely resisted.
Some commentators think the media reports were Treasury kite-flying. Even so, in the summer, Mr Sunak suggested that as private sector pay had taken a huge hit, in the ""interest of fairness"" the public sector's 5.4 million workers should share some pain.
Trouble is, relative to pay in the private sector, public sector pay has fallen to its lowest level in decades, according to the IFS.
Only during the pandemic has public sector pay performed more strongly than in the private sector. Union leaders have already warned of industrial action to ensure members' pay does not fall further behind.
Many promises have been thrown off-course because of the pandemic, and the government will be keen to get its north-south levelling up agenda back on track as soon as possible. Infrastructure spending is key to this.
The north has long complained that the Treasury methodology used to calculate the cost-benefit of spending money on big projects is inherently biased towards London and the rest of the south east. So, expect some changes to these calculations. And watch out for whether any spending promises are new money, or simply projects brought forward.
To underline his commitment to spend on big long-term projects, there is talk that Mr Sunak could publish details of a National Infrastructure Strategy and a Research and Development Strategy.
And in a symbolic move that levelling up is more than a question of infrastructure, the Financial Times has reported that the chancellor could also announce that parts of government could relocate from the capital - with the Treasury leading the way.
While Wednesday will be about spending and borrowing, at some point the chancellor will have to decide how it will be paid for. He will start to address this in next March's Budget, although most economic commentators feel the economy will still be too fragile for major tax rises.
It is possible that, with the success of a Covid vaccine, the economy could bounce back, limiting the need for big rises. However, Paul Johnson, director of the IFS, told the BBC that four or five years down the road he still expects the economy to be about 4%-5% smaller than before the pandemic.
Rein in spending and raise taxes too early, and recovery will be choked off. Leave it too late, and the public finances will spin out of control.
""It's a fine judgement,"" said Mr Johnson. Both the chancellor and Prime Minister Boris Johnson have, however, said they don't want a return to austerity.
There have been reports the Treasury could raise money from changes to Capital Gains Tax, pensions relief or self-employment taxes. But this is tinkering.
Mr Johnson believes Â£40bn of tax rises are necessary over the short-term, and that sort of money cannot be raised without touching the Big Three: income tax, VAT or national insurance. These bring in almost two-thirds of government revenue."
"**Taxi drivers say they are struggling to pay bills and put food on the table because of fewer passengers during the coronavirus pandemic.**
Unite Wales, which represents cabbies, claims ministers have ""forgotten"" the industry, while concentrating support on buses and trains.
One private hire driver in Cardiff said he had been lucky to earn Â£25 a day during the restrictions.
The Welsh Government said it was continuously reviewing support.
Taxi drivers are preparing to hold a demonstration outside Cardiff's City Hall on Tuesday.
During lockdown, pubs, bars and restaurants were closed and events were cancelled, and while many venues have now reopened, early closing times and social distancing measures are in place with people being urged to work from home.
Unite branch secretary Yusef Jama said the restrictions meant many cabbies were spending hours waiting for a fare and many were earning less than Â£25 a day.
Mr Jama said anxiety and depression among drivers was now at a crisis point, and many were struggling financially and worried for their families.
""I've had really, really concerning conversations with drivers, where they feel left on their own, nobody to turn to and to speak about the problems they go through,"" he said.
""Some of these conversations, it's got to the point where I don't know if these drivers are going to be alive tomorrow.
""They feel like they're going to lose their house, they're having problems with their family because they're not providing for their families.""
At the rank at Cardiff Central Station, Abdel Kadir, a driver in the city for 18 years, said he had waited three hours before his first fare.
Before the pandemic, he said, the rank would be busy taking people from the station, but now there are fewer passengers to transport across the city.
""All the taxis here depend on the London train. Before there would be 10 to 15 cars moving, now maybe one or two or three if you are lucky because the trains come empty,"" he said.
""With this pandemic people aren't moving and all the staff are working from home or by video.""
Father-of-three Rofikul Islam said his family was terrified of him going to work and, potentially, being exposed to the virus, but he worried about falling into debt.
""It's very bad at the moment,"" he said. ""We have to wait three to four hours for a fare and after the three hours, four hours, we get a Â£6 fare. All day we work for Â£25, Â£27.""
He added: ""Nobody's helping us, so we have to try ourselves.
""We're taking a risk, we can't do nothing, we have to come out to do our best.""
The majority of taxi drivers are self-employed and are entitled to the UK government's Self Employed Income Support Scheme grant extension.
The grant, which is taxable, covers 80% of profits for November, December and January, up to a limit of Â£7,500.
But drivers claim that the industry was struggling before the pandemic, with many making little profit once vehicle costs and licences were taken from the fares.
Eva Dukes' income disappeared overnight when schools and offices closed in March.
The private hire driver, who lives in Cardiff with her three children, said her contracts stopped and, with her husband also being a taxi driver, the family were struggling to make ends meet.
While Eva and husband Philip received grants under the scheme, it worked out at about Â£1,000 a month, and she said they had gone through all their savings.
""It's dire to say the least. Just putting food on the table is a weekly struggle,"" she said.
""We need help now. They've given money to the buses and the trains and all the other transport, and now we need help for taxi transport, private hires, contractors, they need help.""
In Scotland, the government announced a Â£30m means-tested fund for drivers, and in Northern Ireland a Â£14m scheme has been set up.
Union representatives say they want the Welsh Government to provide the same kind of funding.
Alan McCarthy, from the Unite union, said with people not going out and being told to stay home, many drivers had been back at work but unable to make a living for months.
The BBC requested an interview with Transport Minister Ken Skates but it was declined.
In a statement the Welsh Government said support was available for drivers, but it recognised the extent of the challenge coronavirus had caused.
""We are in regular discussion with the sector and will continue to review what support can be made available,"" said a spokesman."
"**The process of applying for a Welsh Government coronavirus grant has been described as ""unfair"" and ""shambolic"" by businesses who tried to access it.**
The Â£100m available from the third phase of the Economic Resilience Fund (ERF) opened on 28 October but closed the next day.
While many firms were still chasing quotes and bank statements at the time, others could apply without them.
The Welsh Government said it had worked hard to support businesses.
But the Welsh Conservative economy spokesman, Russell George, said businesses had been ""let down by lack of communication; let down by a lack of clarity; let down by the Welsh Labour Government"".
While Plaid Cymru's spokeswoman Helen Mary Jones said targeted support was needed for businesses unable to resume trading until the coronavirus pandemic was over.
Applicants were allowed 20 minutes to complete the questions on each page and asked to provide supporting documents for their application, including three months of bank statements, proof that they could match-fund 10% of the grant and quotes for work to be carried out.
The guidance notes for the application stated the documents were ""required as part of the application process"" and that ""if application forms are sent without the required additional documents, your application will be declined"".
Despite this, some businesses were allowed to provide these documents after submitting their application form with an extended cut-off date of 2 November, according to evidence seen by BBC Wales.
On 30 October, less than 48 hours after the fund opened, the Welsh Government stated that ""more than 5,500 businesses"" had applied for the grant and it was now ""fully subscribed"".
Jamie Williams, who runs North Wales Active in Betws y Coed, missed the grant ""by a few hours"" and said the process was ""shambolic"".
After taking a couple of days to gather supporting evidence, Mr Williams thought they would ""fly through"" the process.
""But as we tried to apply for it the next day, it had obviously gone, which is just ridiculous,"" he said.
The Â£10,000 grant was ""hugely important"", Mr William added. ""Nobody's booking our activities because either they are in lockdown or we are in lockdown.""
Natalie Isaac and her family run five restaurants including two in Cardiff - Assador 44 and Bar 44.
Applications were made for both. Bar 44 was turned down for not attaching bank statements, which Ms Issac said they did.
But for the other, the business was given an opportunity to submit information to a case officer.
""So that's two establishments with two completely different approaches,"" said Ms Issac, who is contesting the claim regarding Bar 44.
""It should be a fair and reasonable process. We don't believe that everyone's being treated equally.""
Ms Isaac said their business was currently ""Â£3.7m down on sales"", with staff numbers across their five restaurants down from 120 to 52 since March.
She said some of the staff that were let go could have been brought back had she got the ERF.
Kerry DeCaux, who runs Rolfe's florist in Cardiff, was also rejected for not uploading enough supporting documents.
""The application was timed, I wasn't expecting that, I read the guide notes as best as I could as I completed each step, then sent it.""
She said she felt ""quite pressurised"", adding: ""I did it the best I could.""
Ms DeCaux said when she received the rejection email she asked for ""an opportunity to fix the elements I failed on, but was told there was no appeal process"".
""I honestly felt broken. I sat on the floor of my shop and sobbed my heart out,"" she added.
North Wales Tourism surveyed 111 members on their experiences of applying for the Â£100m fund - 44 had managed to make an application and were awaiting an outcome.
Of the 67 who had not made an application, 29 referred to a ""very complex process"" and ""time constraints"".
The Chief Executive of North Wales Tourism, Jim Jones, said the grant ""did not provide the help that is so desperately needed.""
He added firms that did not get the support ""can't just be left in complete limbo"" without help into 2021.
A Welsh Government spokesman said: ""We've worked hard to support businesses through this incredibly difficult year and we've already provided businesses with Â£100m from the third phase of the Economic Resilience Fund since it was launched last month.
""We also ring-fenced funding in third phase for tourism and hospitality businesses.
""This is in addition to the hundreds of millions of pounds provided to firms through the first two phases of the ERF and our Covid-19 business rates based grants.""
He said to date, 80% of recipient firms of the ERF, including phases one and two, had been micro businesses accessing support through the digital application process.
The Welsh Government said it had launched its eligibility checker three weeks before ERF3 ""to give businesses plenty of time to prepare their applications"" and asked them to provide necessary supporting documentation ""to ensure necessary due diligence"".
""We recognise that some businesses missed out on this round of funding and we are exploring options for how we might allocate any future support,"" the spokesman added.
He said the finance minister had set aside funding for a fourth phase of the ERF and was currently developing proposals."
"
Not only does the Met Office/Hadley Climate Center have trouble with pesky “moles” this week, they are now finding a staunch ally, the BBC, is questioning their forecasting ability. One wonders if they will improve using “deep black”, the 1.2 megawatt supercomputer they just purchased.

Met Office cools summer forecast
 
 By Roger Harrabin 
 BBC environment analyst 
excerpts:
You will need a brolly on holiday in the UK in August – the Met Office is issuing a revised forecast for more unsettled weather well into the month.
It is a far cry from the “barbecue summer” it predicted back in April.
The news will raise questions about the Met Office’s ability to make reliable seasonal forecasts.
…
It did indeed stress at the time of the summer forecast in April that the odds of a scorching summer were 65%. It explains that it coined the phrase “barbecue summer” to help journalists’ headlines.
But this has come back to bite the organisation because many people do not feel like they have been enjoying a “good” summer, especially compared with previous searing years.
Jet stream
Some now ask if the Met Office risks its reputation by attempting to popularise its work this way.
…
The real problem for the Met Office is that this is the third summer in a row where its forecast has failed. In 2007, the Met Office chirped: “The summer is yet again likely to be warmer than normal. There are no indications of a particularly wet summer.”
We got downpours and floods in the wettest summer for England and Wales since 1912. Temperatures were below average.
In April 2008, the Met Office forecast: “Summer temperatures are likely to be warmer than average and rainfall near or above average.”
That did not prepare people for one of the wettest summers on record with high winds and low sunshine.
In both instances, the Met Office failed to predict the movements of the jet stream – the high-level wind that races round the world 10km above the surface.
…
read the entire article at the BBC here
h/t to WUWT reader Kristinn


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e949b1838',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Real Climate’s Misinformation
From Climate Science — Roger Pielke Sr. @ 7:00 am
Real Climate posted a weblog on June 21 2009 titled “A warning from Copenhagen”.  They report on a Synthesis Report of the Copenhagen Congress which was handed over to the Danish Prime Minister Rasmussen in Brussels the previous week.
Real Climate writes
“So what does it say? Our regular readers will hardly be surprised by the key findings from physical climate science, most of which we have already discussed here. Some aspects of climate change are progressing faster than was expected a few years ago – such as rising sea levels, the increase of heat stored in the ocean and the shrinking Arctic sea ice. “The updated estimates of the future global mean sea level rise are about double the IPCC projections from 2007″, says the new report. And it points out that any warming caused will be virtually irreversible for at least a thousand years – because of the long residence time of CO2 in the atmosphere.”
First, what is “physical climate science”? How is this different from “climate science”. In the past, this terminology has been used when authors ignore the biological components of the climate system.
More importantly, however, the author of the weblog makes the  statement that the following climate metrics “are progressing faster than was expected a few years ago” ;
1. “rising sea levels”
NOT TRUE;  e.g. see the University of Colorado at Boulder Sea Level Change analysis.
Sea level has actually flattened since 2006.
2.  “the increase of heat stored in the ocean”
NOT TRUE; see
Update On A Comparison Of Upper Ocean Heat Content Changes With The GISS Model Predictions.
Their has been no statistically significant warming of the upper ocean since 2003.
3. “shrinking Arctic sea ice”
NOT TRUE; see the Northern Hemisphere Sea Ice Anomaly from the University of Illinois Cyrosphere Today website. Since 2008, the anomalies have actually decreased.
These climate metrics might again start following the predictions of the models. However, until and unless they do, the authors of the Copenhagen Congress Synthesis Report and the author of the Real Climate weblog are erroneously communicating the reality of the how the climate system is actually behaving. 
Media and policymakers who blindly accept these claims are either naive or are deliberately slanting the science to promote their particular advocacy position. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94bd62bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

For over a decade, many industry groups have maintained that putting carbon dioxide in the air would produce a general “greening” of the planet. In fact, that’s the thesis of a famous 1992 video, “The Greening of Planet Earth,” which riled the environmental community more than just about anything else that business has done in its own defense on this issue.



“Greening” was put out by energy‐​industry activists (you can get your own copy by contacting http://​www​.greeningearth​so​ci​ety​.org), who discovered that several big‐​name scientists were willing to appear and argue that carbon dioxide will enhance global plant growth by directly stimulating plants and by warming the coldest air of winter. These scientists were confident because the growth stimulation had been observed in literally thousands of controlled lab experiments and reported in the scientific literature. At the same time, the climate data were pouring in showing that “global” warming was much more “winter” warming, and in the coldest air, which was sure to lengthen the growing season for plants.



On September 16, industry will be vindicated with a vengeance in the Journal of Geophysical Research, when Liming Zhou and five co‐​authors publish their paper demonstrating a profound greening of the planet poleward of latitude 40º, or north of a circle passing through New York City.



Using a satellite designed to measure changes in vegetation, they found that the time of active growth has advanced as much as 18 days per year in Eurasia. (The freeze‐​free period averages around 170 days at latitude 40º.) In North America the results are more spotty, with a few areas increasing by up to 12 days. On our continent, the results are confounded by the well‐​known (to scientists, not to newspaper‐​readers) cooling trend in northeastern North America that has been going on for about 70 years.



Zhou et al. attribute this “greening” (their word) to “global warming,” because it matches areas that show maximum warming since 1980 in land‐​based climate records. In fact, the winter warming in the dead of Siberia has been quite striking for decades, and, everything else being equal, this will lengthen the growing season. (Last year’s record cold merely proves that climate is a very variable thing.) Summer warming in Siberia has been less profound, a fact not generally disseminated because it doesn’t support the popular global‐​warming‐​as‐​disaster paradigm.



What isn’t noted in the paper or the brief flurry of news reports — there would probably be a bit more coverage if Zhou had written “global warming is killing the North Woods” — is that the beginning of their satellite record, in 1981, corresponds in the Northern Hemisphere to the end of the coolest era of the last 70 years. The fact is that all analyses show a cooling of our hemisphere from roughly the mid‐​1940s to the late 1970s. During this climate spasm “global cooling” became popular. So this paper starts at an unusual point, but, unfortunately, that is when the satellite went up. If the satellite went up in, say, 1950, the changes it would have found in the growing season would have been much smaller.



The contra is also true: According to Zhou’s findings, if we had continued down that cooling spiral, the world north of 40º would now be much more barren than it was 20 years ago. It’s quite reasonable to ask if human‐​induced global warming has saved the world from a food crisis.



What really ticked off the greens about “The Greening of Planet Earth” were the many sound bites from prominent agricultural scientists about how the future atmosphere would be much more conducive for food production. But look at what NASA, which funded this study, now says about Zhou’s work: “The pattern of high growth is especially noteworthy in boreal [northern] Eurasia…This includes the grasslands and croplands of south central Russia.…[emphases added].”



In other words, dreaded global warming will produce more food for Russia.



Russians rightfully fear the cold. In 1972, near the bottom of the mid‐​century cooling (and around the height of global cooling fear) they were so short of food that they purchased just about every kernel of American grain. This sent grocery store prices here to alarming levels. By the end of the crop year 1972, world grain reserves stood at a stunningly low 19 days. Since we warmed up, those fears have become a thing of the past. Now food shortages are largely local and political, and commodity prices have been in the tank for years, reflecting vast supply compared to demand.



So is this what global warming has wrought? It appears to have created a more comfortable planet with more food. The video was right. The greens were wrong. The world is greener.
"
"The world’s accountants must put the climate crisis at the forefront of their work to spur global companies to adopt green policies and help prepare them for the risks, according to the Institute of Chartered Accountants in England and Wales (ICAEW) and other industry groups. They have called on a global alliance of accountants, representing more than 2.5 million professionals worldwide, to put their skills to use by helping companies prepare for a climate emergency. The groups want all members of the Accounting for Sustainability Project (A4S) network to integrate climate risk into their company audits to drive companies to set more sustainable business strategies. Michael Izza, the chief executive of the ICAEW, said it is crucial that chartered accountants use their “unique position” at the core of almost every business, government and non-governmental organisation to make the case for sustainability. “Chartered accountants bring practical skills like measurement and management to the table, and can work with business to build green policies into their working practices. We need to make this a decade of transition for business; failure to make this move will make the inevitable adjustments required much more difficult,” he said. Accountants should demonstrate the risks to business posed by the climate crisis, such as the impact of flooding or the effect of drought on the price of crops needed in the supply chain, the industry groups said. The call for climate action comes after the accounting watchdog, the Financial Reporting Council, launched a review into whether companies and their auditors are adequately reflecting the financial risks of the climate crisis in their annual financial accounts. Sir Jon Thompson, the FRC’s chief executive, said auditors have a responsibility to challenge management to face up to the financial risks of the climate crisis and provide clear information to investors. Bruce Cartwright, the chief executive of Icas, the global professional body, said there are “dramatic implications for failing to tackle climate change – environmental, social and economic”. He said: “It is our future that is at risk and the urgency at which we are required to act must remain front of mind. Icas is working with fellow accountancy bodies to act now to limit the negative effects of climate change. Our individual actions, collectively, have the ability to make a difference.” "
"

Here’s a disaster script that might interest the burgeoning Virginia film industry: Three hundred thousand acres, including much of Shenandoah National Park, go up in smoke; downtown D.C. hits 110 degrees day after day; mega‐​drought costs Virginia $1 billion, sending state finances into a death spiral; the president declares a national emergency; and hordes of thirst‐​crazed snakes attack a turkey farm.



This isn’t the storyboard for the next global warming flick. It’s a scene from a much more fearful genre: reality. Welcome to the summer of 1930.



Judging droughts is a little like judging an ugly contest, a somewhat subjective affair. With that caveat, I’ll opine that the current one is very reminiscent of the prolonged moisture shortage that plagued our region and much of the Northeast from 1964 to 1967. And even though Virginia Gov. Mark R. Warner declared a state of emergency on Friday, this drought is nothing compared with what happened in the summer of 1930. That was the peak of a two‐​year drought that remains the benchmark for this area in the era of modern rainfall measurement, which began in 1895. On a graph charting the Palmer Drought Severity Index, a measure of drought for central Maryland, the 1930 outlier is obvious.



Our current misery is one of the low points in a prolonged rainfall deficit that began during the spring of 1998, right after the end of that generator of misplaced fears, El Niño. Within a year, disaster struck Virginia’s Shenandoah Valley, as crop yields took their biggest hit in postwar history. Ironically in view of all this suffering, Maryland got into a misplaced water war with Virginia over the Potomac (the river’s supply was and will remain fine, because there’s been adequate flow into the huge upstream reservoirs that water Washington and its vicinity). Since 1999, things have broiled right along in fits and starts, with the latest dry spell giving everyone a lot more of the former than the latter. The situation is particularly terrible in central Maryland, the Virginia Piedmont (with the exception of Northern Virginia, which has been lucky with thunderstorms compared with its surroundings), and the central Shenandoah Valley, where an unknown number of small farmers still struggling in the wake of the 1999 disaster now face bankruptcy.



Most of the time, one person’s drought can be a mere inconvenience for someone else. A short‐​term deficit — merely six summer weeks with little rain — can devastate crops. Droughts lasting a year, which occur in the Mid‐​Atlantic when we get about 60 percent of our normal 40 inches of rain, begin to draw down shallow‐​water wells and dry up livestock ponds. But the most severe droughts, like 1930’s, touch everyone in ways that are not immediately obvious.



For example, dry ground produces spectacular heat. Normally, our high temperatures (away from the urban cores) cap at around 95 to 98 degrees on a sunny day. That’s because it’s usually wet around here (the official climate designation for the Mid‐​Atlantic is “humid subtropical”), and a lot of the sun’s energy goes toward the evaporation of water, contributed by the jungle of vegetation that covers our region.



In really dry years, like 2002, 1999 or 1930, the soil becomes so arid that trees stop transpiring and the sun finds no water to evaporate. This began in earnest in early August this year, as trees started to shed leaves normally retained until October. When things get this dry, all the sun’s energy goes toward heating the surface and rural temperatures bump from the high 90s into the low 100s, usually hot enough to beat daily records lasting a century or so.



In fact, most Mid‐​Atlantic weather stations average only about one 100‐​degree day per year. Not in 1930. Rural Woodstock, hard in the middle of the Shenandoah Valley, had 21 of them. The high on July 20 was 109, which remains Virginia’s official record. Rainfall was virtually nonexistent that summer. Except for a few places that received a lucky thunderstorm, much of the Mid‐​Atlantic saw less than an inch between June 20 and the end of August, or about 10 percent of normal rainfall. For comparative purposes, the driest regions currently are running around 40 percent of normal for the past three months (not counting last week’s beneficial rain).



The 1930 horror show reached its peak between July 19 and Aug. 10. Here’s the run of that summer’s high temperatures recorded at Charlottesville’s Leander McCormick Observatory, a rural station on a hilltop covered with thick vegetation. If anything, these figures should have been lower than those in the surrounding areas. Starting on July 19, they were: 103, 107, 106, 105, 97, 92, 102, 104, 98, 103, 106, 90, 95, 91, 97, 101, 106, 102, 90, 100, 99, 105 and 98.



 _The Washington Post_ maintained an accurate thermometer at a downtown newspaper kiosk that didn’t have the luxury of being a well‐​vented “official” site. Shade temperatures reached 107 on July 19, 1930, 109 on July 27 and Aug. 3, and 110 on July 20 and Aug. 9. Thousands of Washingtonians slept in the District’s parks. Former Minnesota congressman Charles Davis died of heatstroke. On Aug. 7, The Post headlined a story, “30 Thirsty Snakes Die for Attacking Turkeys.” (Farmers reported that the reptiles were after blood.)



What would happen if 1930 repeated itself in 2003? Given a drought of similar magnitude and geographical distribution, D.C. would be much hotter. The city is much larger than it was, and airflow is even more impeded. Conservatively estimating, urban core temperatures would probably reach around 113.



It is impossible to predict exactly how many deaths would result. If the power stayed on, research by my University of Virginia colleague Robert Davis and myself suggests maybe 50 to 100. But it’s not at all clear that the air conditioners would continue to run. A similar, but short‐​lived, hot blast occurred on July 4 and 5, 1999, again enhanced by very dry conditions. Rural temperatures peaked at 105 degrees. Regional power people have told me they fear that only the Fourth of July holiday prevented a major electrical failure. If it had been a workday, the system would likely have exceeded its previous peak, the frigid morning of Jan. 11, 1994, which was marked by rolling brownouts as people turned up their heat.



Soon after the power goes out, deaths begin. We know this from the Chicago heat wave of July 1995, when, a thundershower caused a major outage. About 500 died.



Air quality? Some 300,000 acres of Virginia forest firecrackered skyward in 1930, about 30 times the current annual average. Atmospheric chemistry doesn’t care whether polluting organic compounds or obnoxious oxides are produced by a burning tree or an aging Belchfire 8. The result is the same: unhealthy air.



Low‐​level ozone, a pulmonary irritant, increases exponentially with temperature. There have been 10 Code Red (unhealthy) and two Code Purple (very unhealthy) air quality days in Virginia in 2002. Prior to this year, we’d had only one Code Purple, in 1999. Mix in 1930’s temperatures, add an increasing amount of urban warming and a bit more forest‐​fire smoke, plus the shimmering conga lines of SUVs panting on Interstates 66 and 395, and we would probably have to invent a new category: Code Black (deadly).



Droughts are expensive. After adjusting for inflation, the 1930 drought remains the most costly natural disaster in the modern Virginia record, totaling close to a cool $1 billion. It is noteworthy that the governor has called attention to the current drought as an additional load upon already strapped Virginia finances. The cost of 1930 puts some legendary local events to shame, namely Hurricanes Camille (1969), Agnes (1972) and Floyd (1999). Given our infrastructural changes, including dependence on automobile commuting, which would likely be severely curtailed because of the exponential increase in pollution caused by extreme heat, a repeat of 1930 could easily hit the multibillion‐​dollar threshold. Which costs more, closing National Airport because of terrorists or closing I-395 because of ozone?



In 1930, the James River was a sea of rocks. Water was so hard to come by that thirsty state highway road crews were refused drinking water by nearby residents. I’d like to say that’s as bad as the disaster documentary can get here — fires, incivility, deaths, thirsty snakes and agricultural bankruptcy — but then there’s the Fortress Monroe, Va., precipitation record. Virtually continuous back to 1837 when combined with figures from nearby Norfolk, it is one of only a handful of precipitation records of such quality and length over all of North America. And it provides a perspective that puts even 1930 in the shade.



Droughts are defined by a combination of intensity and duration. On the one‐​year time frame, droughts are considered large when the 12‐​month rainfall averages about 60 percent of normal. That’s been true of the driest periods in our current long‐​term drought. The driest 12‐​month periods in the drought that peaked in the summer of 1930 ran approximately 50 percent of normal. That additional 10 percent deficit accrued mainly in the summer and is the difference between this year’s misery and 1930’s tragedy.



On the multi‐​year time scale, 75 percent of normal rainfall indicates a serious problem, which is what makes the Fortress Monroe record so ominous. The entire period from 1851 to 1855 — half a decade — shows the area receiving only 60 percent of its average rainfall of 44 inches per year. For comparative purposes, over the past three drought‐​prone years, Maryland and Virginia have averaged between 80 percent and 90 percent of normal rainfall.



I first stumbled upon the Fortress Monroe record, as well as the better‐​known 1930 history, soon after I became state climatologist, more than 22 years ago. Since then, through several small‐​fry droughts and a couple of whoppers, I have made every effort to draw the remarkable Fortress Monroe anomaly to public attention. It is a worst case with unimaginable consequences, a movie I do not want to see, and a contingency for which there has been no preparation.



All we can do in the current situation is conserve water where it is needed. Rather than invoke statewide measures, a clear political mistake made in Maryland in 1999 (the drought was not statewide), we now rely more on local authorities and rational geography. On Friday, Warner ordered conservation methods for specific river basins rather than artificially derived political boundaries. These actions are in concert with local restrictions, which tend to be applied conservatively. Local authorities respond to local politics and they surely are loath to impose discomfort when it is not necessary. Whether conservation is mandated or set by market pricing (an increasingly attractive option in many local communities), people soon realize that the SUV with the “Save the Earth” bumper sticker — that rolling non sequitur common at regional beaches — doesn’t have to be washed every week. Nor do our children require half‐​hour showers.



These measures will help. But I find little comfort in history. If people think conditions are bad now, they need to look first backward, to 1930 and the mid‐​19th century, and then forward. How will we adapt to the inevitable repetition?
"
"**Plans to raise tolls on the Tamar Bridge and Torpoint Ferry have been halted after a Â£1.6m government grant, managers have said.**
Crossings owners Cornwall Council and Plymouth City Council were considering increases to cover financial shortfalls caused by Covid-19 reducing traffic.
The cash meant plans to increase tolls at the start of 2021 ""will not need to go ahead at this stage"", managers said.
A charge for a car would stay at Â£2 or Â£1 for pre-paid journeys, they added.
The routes are free to cross from Plymouth, but tolls apply when coming from Cornwall.
The Tamar Bridge and Torpoint Ferry JointÂ Committee said each council was receiving Â£821,553 from a compensation scheme set up by the government to help local authorities deal with the impact of the coronavirus and it would cover the period up to July 2020.
The money meant ""proposals to increase toll prices at the beginning of next year will not need to go ahead at this stage"", it said.
Bosses were planning to apply for more money for the period after July, it added.Â
The bridge and ferry carry about 18 million vehicles a year.
They receive no government subsides and had an operational budget of about Â£10m in the 2018/19 financial year."
"Greta Thunberg will visit the UK next week to take part in a youth protest in Bristol. The 17-year-old climate activist, who launched a global youth-based movement when she began a “climate strike” outside Sweden’s parliament in 2018, plans to join protesters on College Green on Friday.  It will be second time Thunberg has joined protesters in the UK in the past year. Last April, she addressed Extinction Rebellion activists in central London. She also made a speech in parliament, telling MPs the UK’s support for fossil fuels and airport expansion was “beyond absurd”. One of the organisers of the Bristol Youth Strike 4 Climate, seventeen-year-old Milly Sibson, told PA Media: “We are all just so excited – everyone is so excited about the thought of hearing her talk. “I would love the chance to meet her because she is the founder of this movement and she is so important to it – she is an idol even though she is younger than me. We really hope loads of people join us to welcome her to Bristol.” Milly said Greta had originally planned to visit London, but as the area planned for the protest in the capital was too small the organisers had recommended Bristol instead. Heading for the UK! This Friday, the 28th, I’m looking forward to joining the school strike in Bristol! We meet up at College Green 11am! See you there! @bristolYS4C pic.twitter.com/n1GOJqMUVQ Bristol was awarded the title of European Green Capital in 2015. In 2018, the city became the first UK authority to declare a “climate emergency”, with the council unanimously backing a commitment to be carbon neutral by 2030. By contrast, the government has committed to net zero carbon emissions in the UK by 2050. Last month, Bristol declared an ecological emergency because of the local decline of many birds, insects and some mammals."
"**Scotland's largest teaching union has said there is ""clear support"" among its members for industrial action over Covid safety concerns in some schools.**
The EIS has been calling for schools in level four areas to either close or move to a blended learning model.
But the Scottish government insists that the evidence clearly supports schools remaining open.
The union has been asking members whether they would potentially take industrial action over the issues.
It has now published the results of the online survey, with half of the 18,733 respondents backing schools in level four areas closing to pupils, while about a third supported a move to a part-time blended model.
About two thirds (65.7%) of the respondents said they would be willing to take industrial action, including potentially strike action, if those options were rejected by councils or Public Health Scotland.
The EIS said many teachers who responded to the survey had indicated they do not currently feel safe at work despite the social distancing and additional hygiene measures that have been put in place.
And it argued that keeping schools fully open ""cannot come at the expense of teacher and pupil wellbeing"".
The union's general secretary, Larry Flanagan, said the survey showed that teachers held a range of opinions on the best means of keeping people safe in schools.
But he said there was ""clear support for moving to industrial action in higher risk areas to protest where teachers feel that the measures required to keep schools safe have not been delivered.""
Mr Flanagan added: ""For level four restrictions to be as effective as we would wish them to be, short term closure or part closure of schools need to be considered.""
The survey results showed that only 4.6% of respondents felt very safe in schools, while 25.9% felt safe.
A further 26.3% said they felt neither safe or unsafe, with 32.5% saying they felt unsafe and 9.5% very unsafe.
Mr Flanagan said the feeling of being at risk was heightened for teachers in secondary schools, for teachers in higher risk areas under level three or level four restrictions, and for teachers in vulnerable groups or who live with or provide care for vulnerable family members.
The Scottish government says the evidence shows that schools are ""not a significant area of transmission"".
But Education Secretary John Swinney said he was ""concerned"" that some teachers who responded to the survey had said they do not feel safe.
He added that extensive guidance was in place to reduce the risk of Covid transmission in schools, and that enhanced risk mitigations were in place in level three and level four areas to protect clinically vulnerable staff and pupils.
Mr Swinney said school staff could already to get a coronavirus test even if they do not have symptoms, with plans being made to potentially pilot and roll out rapid testing in schools.
But Mr Swinney acknowledged: ""We need to do more to ensure everybody feels safe"".
The education secretary told BBC Scotland last week that the number of positive cases among pupils represented only 0.1% of all pupils.
He said the level for teachers varied between about 4% and 7%, which he said was ""no different to any other workforce in that category"".
Parents group Us For Them Scotland has called for the government to keep schools open - even in level four areas.
""We know there are influential groups who've wanted schools shut right from the start, and now strike action is being used as another tactic to force this through,"" organiser Jo Bisset said last week.
""All of this serves to damage the wellbeing and prospects of children.""
""I really don't know many teachers who want to close schools, but increasingly we're asking each other: 'Do you think it's safe to be here?',"" one teacher - who asked to remain anonymous - told BBC Scotland.
He said there had been ""a lot of tears"" from colleagues who were anxious to be at work.
In his school, which is in a level four area, several teachers have lost loved ones to Covid.
The teacher said now was the time to move to blended learning - ""we worked so hard planning for it, now is the opportunity. Teachers are still in every day, it's not that we want to close the schools. We just want to be safe,"" he said.
He added that some children were now into their third period of isolating since the summer, yet teachers were hardly being told to isolate at all.
He said teachers were angered by claims that transmission rates were not increased in schools.
These claims were ""patronising"" given how ""blatant"" it was that transmission was occurring, he added."
"**Chancellor Rishi Sunak is promising a Â£4.3bn package to help hundreds of thousands of jobless back to work as he prepares to unveil his Spending Review.**
He said in an announcement ahead of Wednesday's review that it would include Â£2.9bn for a new Restart jobs scheme and Â£1.4bn to expand the Jobcentre Plus agency.
Mr Sunak said his ""number one priority is to protect jobs and livelihoods"".
The review will outline spending for such things as roads, police and NHS.
But it comes against a backdrop of an economy hit by the coronavirus pandemic and huge job losses.
Earlier this month, official figures showed the UK's unemployment rate rose to 4.8% in the three months to September, up from 4.5%. There was a big rise in the number of 16 to 24-year-olds out of work.
Earlier this month, the Bank of England forecast that the jobless rate could rise to nearly 8% by the middle of next year
Under the Restart scheme, people who have been out of work for more than 12 months will be provided with regular intensive support tailored to their circumstances.
Mr Sunak will also confirm in his Commons statement on Wednesday more funding for the next stage of his Plan for Jobs - including Â£1.6bn for the Kickstart work placement programme, which the Treasury says will create up to 250,000 state-subsidised jobs for young people.
The scheme, first launched in August, offering employers Â£2,000 for every new worker they take on, is to be extended to the end of March.
There will also be a Â£375m skills package, including Â£138m of new funding to deliver Prime Minister Boris Johnson's Lifetime Skills Guarantee.
Mr Sunak said on Tuesday: ""This Spending Review will ensure hundreds of thousands of jobs are supported and protected in the acute phase of this crisis and beyond with a multibillion package of investment to ensure that no-one is left without hope or opportunity.""
The package has won the support of business and industry. Matthew Fell, policy director at the CBI employers' group, said the chancellor was right to focus on job creation.
""Covid-19 has swept away many job opportunities, for young people in particular,"" he said. ""The scarring effects of long-term unemployment are all too real, so the sooner more people can get back into work the better.""
Claire Walker, co-executive director, at the British Chambers of Commerce, said retraining and reskilling was vital to getting people back to work.
""Investment in Kickstart, in which Chambers are playing a leading role, and the launch of the Restart scheme, will be critical in helping support the recovery,"" she said.
However, economist Nye Cominetti, from the Resolution Foundation, said the government must learn lessons from previous schemes which failed to meet expectations.
""The chancellor is right to put in place help for those out of work for long periods as they often struggle most in periods of high unemployment.
""The Restart Scheme is a big step up in terms of job support. The Â£2.9bn allocated for the coming three years exceeds that spent on the Work Programme over five years after the financial crisis.
""But for the new approach to be effective, ministers must learn lessons from the patchy record of that scheme, particularly the need for more intensive support for harder-to-help groups, who were too often side-lined."""
"**A woman who took part in a Covid-19 vaccine trial has said she volunteered due to the ""hope"" it offered.**
Faye Wilson was part of the programme led by Newcastle Hospitals, which has helped the Oxford coronavirus study.
The vaccine has been shown to be highly effective at stopping people developing symptoms, according to a large trial.
Ms Wilson, 72, and from Morpeth in Northumberland, said it was an ""incredible privilege"" to have played a ""tiny part"" in delivering it.
In total the North East recruited the third-highest number of participants in the UK for the study.
Ms Wilson said she was ""ecstatic"" about being part of it.
""I think through all of this, what people have been able to hang on to is some hope and some light at the end of the tunnel,"" she said.
""For me, it was around hope and around knowing there was a way out, and to be part of contributing to that hope and be part, a tiny part of delivering it, has been an incredible privilege.""
Ms Wilson, who has lost friends to coronavirus, encouraged people to think to the future but remember the present threat of the pandemic.
""At the moment it's a really difficult time because it's not there yet and people are frightened and tired and want to change things,"" she added.
""If we don't hold on and hold on to what we are doing, every death will be wasted because we can get there and it will make a difference, so these next few months are really, really challenging times for us all.
""But we have got hope and it's there and we need to hang on.""
Dr Christopher Duncan, honorary consultant in infectious diseases at Newcastle Hospitals, based at the RVI, said it was great that the region played ""such a leading role"" in delivering the vaccine.
He added: ""It's just fantastic news for all of the team and particularly for everyone that has been working so hard to deliver this trial, and every single day at the moment in the hospital there are lots and lots of patients seriously ill with this devastating disease, and it comes at a time where we really, really need it.""
_Follow BBC North East & Cumbria on _Twitter _,_Facebook _and_Instagram _. Send your story ideas to_northeastandcumbria@bbc.co.uk _._"
"
Jeff Id at the Air Vent has been doing some interesting work lately. Before the NSIDC Arctic Sea Ice anomaly plot went kaput due to failure of the satellite sensor channel they have been using, they had created a vast archive of single day gridded data packages for Arctic sea ice extent. Jeff plotted images from the data as viewed from directly over the North Pole. It took him over 15 hours of computational time. An example image is below.

Jeff gathered up all the resultant plotted images and turned them into a movie, but placed them on the website “tinypic” where the movie won’t get much airplay.
I offered Jeff the opportunity to have it hosted on YouTube and posted here, where it would get far greater exposure and I completed the conversion this afternoon.
What I find most interesting is watch the “respiration” of Arctic Sea Ice, plus the buffeting of the sea ice escaping the Arctic and heading down the east coast of Greenland where it melts in warmer waters.
Jeff writes:
I find the Arctic sea ice to be amazingly dynamic. Honestly, I used to think of it as something static and stationary, the same region meltinig and re-freezing for dozens or even hundreds of years – not that I put much thought into it either way. Shows you what I know.
This post is another set of Arctic ice plots and an amazing high speed video. The NSIDC NasaTeam data is presented in gridded binary matrices in downloadable form HERE. 
The data is about 1.3Gb in size so it takes hours to download, I put it directly on my harddrive and worked from there. The code for extraction took a while to work out but was pretty simple in the end. This code ignores leap years. Formatting removed courtesy of WordPress.
filenames=list.files(path=”C:/agw/sea ice/north sea ice/nasateam daily/”, pattern = NULL, all.files = TRUE, full.names = FALSE, recursive = TRUE)
trend=array(0,dim=length(filenames)-1)
date=array(0,dim=length(filenames)-1)
masktrend=array(0,dim=length(filenames)-1)
for(i in 1:(length(filenames)-1))
{
fn=paste(”C:/agw/sea ice/north sea ice/nasateam daily/”,filenames[i],sep=””) #folder containing sea ice files
a=file(fn,”rb”)
header= readBin(a,n=102,what=integer(),size=1,endian=”little”,signed=FALSE)
year=readChar(a,n=6)
print(year)
day=readChar(a,n=6)
print(day)
header=readChar(a,n=300-114)
data=readBin(a,n=304*448,what=integer(),size=1,endian=”little”,signed=FALSE)
close(a)
if(as.integer(year)+1900<=2500)
{
date[i]=1900+as.integer(year)+as.integer(day)/365
}else
{
date[i]=as.integer(year)+as.integer(day)/365
}
if(i==1)
{
holemask= !(data==251)
}
datamask=data<251 & data>37 ## 15% of lower values masked out to match NSIDC
trend[i]=sum(data[(datamask*holemask)==1])/250*625
}
###mask out satellite F15
satname=substring(filenames,18,20)
satmask= satname==”f15″
newtrend=trend[!satmask]
newdate=date[!satmask]
After that there is some minor filtering done on 7 day windows to dampen some of the noise in the near real time data.
filtrend=array(0,dim=length(newtrend))
for(i in 1:(length(newtrend)))
{
sumdat=0
for(j in -3:3)
{
k=i+j
if(k<1)k=1
if(k>length(newtrend)-1)k=length(newtrend)-1
sumdat=sumdat+newtrend[k]
}
filtrend[i]=sumdat/7
}
So here is a plot of the filtered data:

Here is the current anomaly.

This compares well with the NSIDC and cryosphere plots. This anomaly is slightly different from some of my previous plots because it rejects data less than 15% sea-ice concentration. Cryosphere rejects data less than 10%. In either case the difference is very slight but since we’ve just learned that the satellites have died and are about 500,000km too low, my previous graph may be more correct. I hope the NSIDC get’s something working soon.
All of that is pretty exciting but the reason for this post is to show the COMPLETE history of the NSIDC arctic sea ice in a video. I used tinypic as a service for this 27mb file so don’t worry, you should be able to see it quite well on a high speed connection. It took my dual processor laptop computer more than 15 hours to calculate this movie, I hope it’s worth it. Brown is land, black is shoreline, blue is water except for the large blue dot in the center of the plot. The movie plays double speed at the beginning because the early satellite collected data every other day. You’ll see the large blue circle change in size flashing back and forth between the older and newer sat data just as the video slows down.
After staring at the graphs above you think you understand what is happening as ice gradually shrinks away. Well the high speed video shows a much more turbulent world with changing weather patterns in 2007 and 2008 summer blasting away at the south west corner of the ice. I’ve watched it 20 times at least, noticing cloud patterns (causing lower ice levels), winds, water currents and all kinds of different things. I’m not so sure anymore that we’re seeing a consistent decline to polarbear doom, with this kind of variance it might just be everyday noise.
Maybe I’m nuts, let me know what you see. 
No Jeff you aren’t nuts. Here is the YouTube Video, suitable for sharing:

Here is another video I posted on You Tube last month which shows the flow of sea ice down the east coast of Greenland. Clearly there is more at work here than simple melting, there is a whole flow dynamic going on.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e956e93f9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"I’m reading one of a small forest’s-worth of beautiful new picture books about the environment with my eight-year-old twins. The Sea, by Miranda Krestovnikoff and Jill Calder, takes us into mangrove swamps and kelp forests and coral reefs. We learn about goblin sharks and vampire squids and a poisonous creature called a nudibranch. Then we reach the final chapter on ocean plastics. When we learn that by 2050 there could be more plastic in the ocean than fish, Esme bursts into inconsolable tears. Since the deserved success of The Lost Words, a large book filled with beautiful paintings of everyday wildlife by Jackie Morris and “spells” by Robert Macfarlane, which my children loved chanting aloud, bookshops have been filled with other gorgeously illustrated oversized tomes about nature and the environment. There’s Ben Hoare’s gilt-edged Anthology of Intriguing Animals, A Wild Child’s Guide to Endangered Animals by Millie Marotta, and other recent classics such as the lovely How Does My Garden Grow by Dutch author Gerda Muller and A First Book of Nature by Nicola Davies. Consumers are spending more money than ever on books for children, and the number of new children’s books about the climate crisis and wildlife has more than doubled over the past year, with sales also doubling, according to data from Nielsen Book Research. The Greta Thunberg effect is creating a new library of children’s books about eco-warriors. Nosy Crow rushed out Earth Heroes four months after signing up author Lily Dyu, selling 9,000 copies in the UK since publication in October. Macfarlane and Morris have just announced a Lost Words sequel, which will be published this autumn. Bloomsbury is publishing what appears to be the apotheosis of right-on environmentalism for kids in February, Fantastically Great Women Who Saved The Planet, by Kate Pankhurst. I love reading with my children and I’m passionate about the environment but my inner sceptic wonders if this worthiness explosion is chasing a phantom market. Who wants heavy-handed moral tales for bedtime? What if eco-doom just generates despair? And how many big picture books are dull-but-worthy Christmas presents that lie unread by real children? Jill Coleman, the director of children’s books at BookTrust, Britain’s largest reading charity, insists the environmental books boom is driven by “a genuine interest and passion from children”, with publishers following their lead. She believes there are many different books that inspire action, rather than despair, and BookTrust offers recommended reading lists of environmental stories for children of all ages. “There have always been lots of nature books for kids – wonderful authors like Nicola Davies help children to connect with and understand the natural world – but the recent trend is books which help children think about what they can do,” she says. “Reading is important because it gives children power and agency in all sorts of ways. We need them to feel they can make a difference. I know there are much bigger issues than not buying a plastic bag in a supermarket but when you are nine you can only do what you can do, and you need to feel that you can do something.” As well as Earth Heroes, Nosy Crow has sold 12,000 UK copies of How To Help A Hedgehog And Protect A Polar Bear, by Jess French and Angela Keoghan, and later this year will publish YouthQuake by Tom Adams and Sarah Walsh, a book about 50 young people who “shook the world”, invariably featuring Thunberg. “It isn’t just a move by publishers, there is a real hunger for it at the moment,” says Kate Wilson, the managing director and co-founder of Nosy Crow. She believes environmentalism is “becoming less siloed” within publishing as environmentally themed books infiltrate many genres, even the cutesy end of fiction for eight-year-olds with Nosy Crow’s reissue of a Holly Webb eco-series (Maya’s Secret, Izzy’s River, Emily’s Dream, Poppy’s Garden). It was originally published in 2012, when “the world was not ready,” as Wilson puts it. Wilson admits there is a risk that children’s publishing becomes overly didactic, pushing a vision of how it wants the world to be “because we have a real commitment to the messages that children are given”. Then again, why wouldn’t we want to encourage care for the planet, Wilson argues: children’s publishers perpetually take moral positions, and “never publish books where bullies win, for instance”. For wildlife journalist and author Ben Hoare, the dilemma is the same as that repeatedly expressed by David Attenborough with his epic natural history TV series. Do writers focus on the wonders of nature and thereby inspire young readers or are they duty-bound to cover the planetary extinction crisis and the climate emergency, even when it risks fuelling climate-anxiety at a young age? Hoare’s lavish Anthology of Intriguing Animals, which has sold in numbers that authors of adult nature writing can only dream of, goes for inspiration with an awareness of global crises. Like many children’s authors, Hoare tested his writing on his own children. “My two girls are adopted and they grew up in totally bookless households for five years. I’ve watched them discover books and reading fairly late compared to their peers. They were reluctant readers so they would’ve found a traditional encyclopedia a turn-off.” Hoare says he tries to add “whimsy, rhyme, alliteration, quirkiness, things that make you smile” to a conventional encyclopedia-type format. “I’ve found that’s what my children really relate to.” There is one type of environmental non-fiction Hoare is not a fan of: “Nature activity books that explain how to roll down a hill, look at a sunset or make things out of sticks. Grandparents and parents buy these books because they want to tackle the dreaded screens but I doubt these books get opened. I’m not sure children need to read about these things – just let them go outdoors and discover nature.” He wonders whether fiction is a more powerful vehicle for raising environmental awareness. His own formative childhood reading included Watership Down, Animals of Farthing Wood and Beatrix Potter, who was never “cutesy”. His daughters have been inspired by Enid Blyton’s The Faraway Tree. “Blyton is laughably dated but this idea of a magical tree really had an impact on my girls. It transformed our walks in the woods. We passed a gnarly oak tree and my daughter said, ‘Look, Dad, there’s the faraway tree’. Fiction is quite a stealthy way of talking to children about big themes.” Lucy Mangan, the author of Bookworm: A Memoir of Childhood Reading, agrees. “Very few children up to the age of 11 can or will read straight non-fiction,” she says. “For the young brain, stories are the way to attract attention and get any message across.” Mangan fears that the moral instruction in environmental non-fiction is too obvious, not least because “there is only one stance that 99% of scientists and writers would want to take so there’s not much room for doing anything with it”. She says: “Children’s literature began with a desire to instruct but if it had stuck to its guns we wouldn’t still be reading it.” The “golden age” of children’s literature only arrived when authors such as J M Barrie to Edith Nesbit cast off Victorian moralising and wrote from a children’s point of view. “Moralising doesn’t get you very far,” argues Mangan. Environmental fiction for children aged 10 and above and dystopian novels for young adults are also proliferating in this age of anxiety. At BookTrust, Coleman has particularly enjoyed The Dog Runner by Australian author Bren MacDibble (who lost her home in a wildfire) and Run Wild by Gill Lewis, a vet who tackles complex wildlife themes in her books, from the persecution of birds of prey as well as bear-bile farming and endangered gorillas. Can this outpouring of environmentalism capture children’s imagination without terrifying them? I test some books on my eight-year-olds. We read Pankhurst’s worthy-sounding Fantastically Great Women Who Saved The Planet. To my surprise, Milly and Esme love it, and I do too: it is funny and interesting, telling stories not just of westerners such as Jane Goodall but also Wangari Maathai, from Kenya, and Isatou Ceesay, from the Gambia. A few days later, Milly tells me they’ve discussed Pankhurst’s story of the Chipko movement in the Indian Himalayas with their teacher. “In the Chipko movement it starts off with one person and then they all protect a whole range of trees from being cut down,” says Milly. “Even a tiny difference makes something big.” A few nights later, Milly announces she has read Hoare’s hefty Anthology of Intriguing Animals in one evening. She adores the pages edged in gold. “You’ll learn hundreds of lovely new things from that book,” she says. Hoare aside, I’m still not convinced by the clunky writing in some of the new eco non-fiction. It seems unfair to pick on one but in Miranda Krestovnikoff’s The Sea, for instance, we learn that kelp, for instance, can grow 30 metres tall. How high is that in real life? How do children relate to that? But my views don’t matter. What do the children think of The Sea? After Esme’s tears over ocean plastics, she consoles herself with its stories of otters. “Everyone should try this book,” she says. “If you read the start you think, ‘meh’ but you’ll learn a lot. It’s really good.”"
nan
"
For those that don’t read a lot of the WUWT comments closely, there has been a scholarly argument going on between  Tom P of the UK and several WUWT commentators over the methodology Steve McIntyre used to illustrate the “breathtaking difference” between the plot of  the hand picked set of 12 Yamal trees and the larger Schweingruber tree ring data set also from Yamal. Tom P. reworked Steve’s R-code script (which he posted on WUWT) to include both the 12 excluded and the Schweingruber and  thought he found “insensitivity to additional data”, saying “There is no broken hockeystick”.
Jeff Id audited the auditor of an auditor and found that Steve’s work still holds up “robustly”. – Anthony

Jeff Id writes on The Air Vent
Just a short post tonight I hope. Tom P, an apparent believer in the hockey stick methods posted an entertaining reply to Steve McIntyre’s recent discoveries on Yamal. He used R code to demonstrate a flaw in SteveM’s method. His post was on WUWT, brought to my attention by Charles the moderator and is copied here where he declares victory over Steve.

Tom P writes on WUWT:
===========
Steve McIntryre’s [sic] reconstructions above are based on adding an established dataset, the Schweingruber Yamal sample instead of the “12 trees used in the CRU archive”. Steve has given no justification for removing these 12 trees. In fact they probably predate Briffa’s CRU analysis, being in the original Russian dataset established by Hantemirov and Shiyatov in 2002.
One of Steve’s major complaint about the CRU dataset was that it used few recent trees, hence the need to add the Schweingruber series. It was therefore rather strange that towards the end of the reconstruction the 12 living trees were excluded only to be replaced by 9 trees with earlier end dates.
I asked Steve what the chronology would look like if these twelve trees were merged back in, but no plot was forthcoming. So I downloaded R, his favoured statistical package, and tweaked Steve’s published code to include the twelve trees back in myself. Below is the chronology I posted on ClimateAudit a few hours ago.
TomP' s plot. Click to enlarge Source: http://img80.yfrog.com/img80/1808/schweingruberandcrud.png
The red line is the RCS chronology calculated from the CRU archive; black is the chronology calculated using the Schweingruber Yamal sample and the complete CRU archive. Both plots are smoothed with 21-year gaussian, as before. The y-axis is in dimensionless chronology units centered on 1.
It looks like the Yamal reconstruction published by Briffa is rather insensitive to the inclusion of the additional data. There is no broken hockeystick.

=============
Jeff Id writes:
He did a fantastic job in reworking R code to create an improved hockey stick graph.  To see his code the link is here.
.




Jeff Id’s version of TomP’s graph – Click to expand



I spent some time tonight looking at his results. Time planned for analyzing Antarctic sea ice. I found that essentially the only difference in the operating functions of the code is the following line.
.
Steve M  —- tree=rbind(yamal[!temp,],russ035)
Tom P —– tree=rbind(yamal,russ035)
.
 
The !temp in Steve’s line removes 12 series of Yamal for the average while Tom’s version includes it. I’m all for inclusion of all data, but I am a firm believer that Briffa’s data is probably a cherry picked set of trees to match temp or something. Therefore by inclusion of the sorted Briffa Yamal version, we have an automatic exclusion of data which would otherwise balance the huge trend. However, this is not the problem with Tom’s result. The problem lies in this plot, also created by Tom P’s code.
Tom P’s Yamal Reconstruction – Count per Year.  Click to Expand

Here is the zoomed in version:

Above we can see that everything in TomP’s curve after 1990 is actually 100%  Briffa Yamal data.
So the question becomes – What does the series look like if the Yamal data doesn’t create the ridiculous spike at the end the curve?
I truncated the black line at 1990 below.


The black line is truncated at the end of the Schweingruber data and it looks pretty similar to the graph presented in the green line by Steve McIntyre again below.

Don’t be too hard on Tom P, he honestly did a great job and took the time to work with the R script which is more than most are willing to. Steve is a very careful worker though and it’s damn near impossible to catch him making mistakes. Trust a serious skeptic, it’s not easy to find mistakes in his work and some of us check him just as I spent over an hour checking Tom’s work. In my opinion Tom deserves congratulations for his efforts and checking, this way we all learn.
I’ve now been all the way through SteveM’s scripts from beginning to end and can’t find any problems with the script, maybe others can!

Steve McIntyre adds in WUWT comments :
 Steve McIntyre (21:35:13)
Here is some conclusive evidence in respect to the following misrepresentation by Tom:
Steve McIntyre said they may well have been just the most recent part of Hantemirov and Shiyatov’s dataset and no selection would have been made.
In my first post in this sequence http://www.climateaudit.org/?p=7142, I identified a common pattern to the IDs for cores and observed:
There are 252 distinct series in the CRU archive. There are 12 IDs consisting of a 3-letter prefix, a 2-digit tree # and 1-digit core#. All 12 end in 1988 or later and presumably come from the living tree samples. The nomenclature of these core IDs url (POR01…POR11; YAD04…YAD12; JAH14…JAH16 – excluding the last digit of the ID here as it is a core #) suggests to me that there were at least 11 POR cores, 12 YAD cores and 16 JAH cores.
It is “possible” that they skipped ID numbers, but this is a farfetched theory even for Tom. As surmised here, the missing ID numbers are “evidence” of at least 39 cores and that the present archive is not only too small, but incomplete.
=========
and also this comment:
 Steve McIntyre (20:13:22)
I am online too much, but I am not online 24/7. I’ve been out playing squash. Surely I’m allowed to be offline occasionally without a poster commenting adversely on this.
While I was out, CA crashed as well.  Thus, it was “quiet.
Contrary to Tom’s speculations and misrepresentation of my statements, it is my opinion that there is considerable evidence that the 12 cores are not a complete population i.e. that they have been picked form a larger population. Rather than quote form actual text, Tom puts the following words in my mouth that I did not say:
Steve McIntyre said they may well have been just the most recent part of Hantemirov and Shiyatov’s dataset and no selection would have been made.
This is not my view.
The balance of Tom’s argument is:
No, they are the twelve most recent cores. There’s been no evidence provided to suggest they are in any way suspect. ..There is no obvious reason to exclude them.
I disagree. I do not believe that they constitute a complete population of recent cores. As a result, I believe that the archive is suspect. There is every reason to exclude them in order to carry out a sensitivity as I did. The sensitivity study showed very different results. I do not suggest that the sensitivity run be used as an alternative temperature history. Right now, there are far too many questions attached to this data set to propose any solution to the sampling conundrum. It’s only been a couple of days since the lamentable size of the CRU sample became known and it will take a little more time yet to assess things.
Reasons why I “suspect” that a selection was made from a larger population include the following. A field dendro could take 12 cores in an hour. We took a lot more than that at Mt Allegre and a field dendro could be far more efficient. Thus, it seems very unlikely that the entire population of cores from the Yamal program is only 12 cores and on this basis, it is my surmise that a selection was taken from the cores. Standard dendro procedures use all crossdated cores and definitely use more than 10 cores if they are available.
This doesn’t “prove” that a selection was made, but it is reasonable to “suspect” that a selection was made and to ask CRU and their Russian associates to provide a clear statement of their protocols. There’s no urgency to do anything prior to receiving a statement of their sampling protocols. For this purpose, it doesn’t matter a whit whether the selection was made by the Russians or at CRU or a combination. In my first post on this matter – which Tom appears not to have read, I canvass the limited evidence for and against. There is certainly evidence supporting the idea that the 12 cores were among 17 selected by the Russians, but in other parts of the data set, the CRU population is larger than that used in the Hantemirov and Shiyatov chronology. The construction of the CRU data set is not described in any literature; the description in Hantemirov and Shiyatov has something to do with it, but doesn’t yield the CRU data data set. Some sort of reconciliation is required.
In addition, the age distribution of the CRU 12 is very different than the age distribution from the nearby Schweingruber population. In my opinion, the uniformly high age of the CRU12 relative to the Schweingruber population is suggestive of selection – in this respect, perhaps and even probably by the Russians. Again this isnt proof. Maybe they were just lucky 12 straight times and, unlike Schweingruber, they got very long-lived trees with every core. Without documentaiton, no one knows. In any event, this doesn’t help the Briffa situation. If these things are temperature proxies, the results from two different nearby populations should not be so different and protocols need to be established for ensuring that the age distribution of the modern sample is relatively homogeneous with the subfossil samples (and they aren’t.)
The prevailing dendro view is that an RCS chronology requires a much larger population than a “conventional” standardization. Thus, even if the data set had been winnowed down to 10 cores in 1990 and 5 cores at the end, this is an absurdly low population for modern cores, which are relatively easily obtained. Use of such small replication is inconsistent with Briffa’s own methodological statements.
Tom also misses a hugely important context. There is a nearby site (Polar Urals) with an ample supply of modern core. Indeed, at one time, Briffa used Polar Urals to represent this region. My original question was whether there was a valid reason for substituting Yamal for Polar Urals. The microscopic size of the modern record suggests that there was not a valid reason. However, this tiny sample size was not known to third parties until recently due to Briffa’s withholding of data, not just from me, but also to D’Arrigo, Wilson et al.
Until details of the Yamal selection process are known, my sense right now is that one cannot blindly assume – as Tom does – that what we see is a population. Maybe this will prove to be the case, but personally I rather doubt it. A better approach is to use the Polar Urals data set as a building block.
As to Tom’s argument that none of this “matters”, the Yamal data set has a bristlecone-like function in a number of reconstructions. While the differences between the versions may not seem like a lot to Tom, as someone with considerable experience with this data, it is my opinion that the revisions will have a material impact on the medieval-modern difference in the multiproxy studies that do not depend on strip bark bristlecones.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92c33482',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Within ten years, the US will phase out fossil fuels and source its energy from 100% renewable sources. That’s what a letter, signed by over 600 people and sent to Congress on January 10 2019, proposes and demands.  The signatories also call for making every building in the US energy efficient and eliminating greenhouse gas emissions from manufacturing, agriculture and transport. Achieving all these targets within the next 25 years would be very ambitious. Within the next ten years requires not just a green transition but a green revolution. The signatories know that, which is why they propose that the federal state should lead this revolution and become the primary funder of this Green New Deal. But how exactly would that work and what would it mean for the economy? The Green New Deal demands the polar opposite of austerity from government. Instead of fiscal frugality, the state would have to pump large sums of money into society and the economy in an effort to overhaul everything. This follows the ideas of Modern Monetary Theory, which sees government debt as normal and possibly even desirable as long as it helps foster an economy that benefits the common good and keeps people economically active.  


      Read more:
      Explainer: what is modern monetary theory?


 Governments and central banks would use state expenditure to keep unemployment low and to subsidise key decarbonisation activities, such as creating  electrified mass transport and making buildings energy efficient, even though this would lead to a noticeable increase in national debt. The government would also have to increase taxes, especially on wealth, capital gains and corporations. In addition to being the main funder of the transition, the state would also act as an employer of last resort, since the Green New Deal promises employment for everybody. This is a huge commitment, considering that there would be significant job losses in fossil fuel dependent industries. It won’t just be coal workers being made redundant – many workers in the car industry, steel manufacturing, large-scale agriculture and food processing would lose their jobs, too. While it’s true that many new jobs will be created as part of the green transition, two issues remain. Can workers be retrained quickly enough to take on these new jobs or are the skills required simply too different? Will new green jobs be sustainable, or will there just be a green boom during the transition followed by a harsh awakening and growing unemployment? In the short and medium term, full employment seems actually quite unrealistic, unless the state forces people to work in jobs they don’t want. Instead, a Green New Deal should prioritise the introduction of a universal basic income to give people the freedom to refuse poorly paid work with bad working conditions. The idea of a universal basic income is simple. Instead of means-tested social welfare payments for people who are outside of paid employment, the state would pay every member of society a monthly basic income allowance. This money would come from absorbing some existing means-tested welfare streams and taxing the new hubs of economic activity, such as green technology manufacturing. In an economy where not everybody might be able to get suitable employment, this basic income allowance would cover essential living costs and allow people to pursue meaningful activities outside of work.  Introducing a universal basic income would have advantages for delivering a Green New Deal. If the radical changes of the Green New Deal aren’t supposed to punish workers in the current fossil-fuel dependent economy, giving these people, who most likely will lose their jobs, a guaranteed alternative would create support for the transition and make sure that those most vulnerable to the proposed changes don’t get left behind. Instead of creating state-funded jobs just for the sake of employing people, the state could empower many of its citizens to lead more sustainable lives as part of the green transition. A universal basic income might offer citizens time to engage in fulfilling community-based work that doesn’t generate profit but which has social value. Taking them out of their cars in long lines of commuter traffic and putting them in allotments growing food or in parks enjoying nature could help usher a whole new way of life.  


      Read more:
      How a basic income could help build community in an age of individualism


 A generous universal basic income would destigmatise work outside of paid employment, such as domestic care work and volunteering in the arts and community sectors and it would allow people to refuse environmentally harmful and badly paid jobs. New income detached from wage labour would mean new flows of money in the economy, breaking the cycle of energy-intensive production and consumption which drives much of emissions.  To win popular support for the Green New Deal its benefits must be truly universal. What better way to guarantee a just, green transition and ensure no one is left behind than universal basic income? Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
In an announcement sure to cause controversy over Svensmark’s theory of cosmic ray to cloud modulation, which is said to be affecting earth’s climate. Svensmark says this is now leading to a global cooling phase. Just a couple of weeks after Svensmark’s bold announcement, NASA has announced that we have hit a new record high in Galactic Cosmic Rays, GCR’s. Apparently, Nature is conducting a grand experiment. – Anthony
Click for larger image - Source: NASA (ACE) spacecraft 
From NASA News: Cosmic Rays Hit Space Age High
Planning a trip to Mars? Take plenty of shielding. According to sensors on NASA’s ACE (Advanced Composition Explorer) spacecraft, galactic cosmic rays have just hit a Space Age high.
“In 2009, cosmic ray intensities have increased 19% beyond anything we’ve seen in the past 50 years,” says Richard Mewaldt of Caltech. “The increase is significant, and it could mean we need to re-think how much radiation shielding astronauts take with them on deep-space missions.”
The cause of the surge is solar minimum, a deep lull in solar activity that began around 2007 and continues today. Researchers have long known that cosmic rays go up when solar activity goes down. Right now solar activity is as weak as it has been in modern times, setting the stage for what Mewaldt calls “a perfect storm of cosmic rays.”
“We’re experiencing the deepest solar minimum in nearly a century,” says Dean Pesnell of the Goddard Space Flight Center, “so it is no surprise that cosmic rays are at record levels for the Space Age.”
An artist's concept of the heliosphere, a magnetic bubble that partially protects the solar system from cosmic rays. Credit: Richard Mewaldt/Caltech
Galactic cosmic rays come from outside the solar system. They are subatomic particles–mainly protons but also some heavy nuclei–accelerated to almost light speed by distant supernova explosions. Cosmic rays cause “air showers” of secondary particles when they hit Earth’s atmosphere; they pose a health hazard to astronauts; and a single cosmic ray can disable a satellite if it hits an unlucky integrated circuit.
The sun’s magnetic field is our first line of defense against these highly-charged, energetic particles. The entire solar system from Mercury to Pluto and beyond is surrounded by a bubble of solar magnetism called “the heliosphere.” It springs from the sun’s inner magnetic dynamo and is inflated to gargantuan proportions by the solar wind. When a cosmic ray tries to enter the solar system, it must fight through the heliosphere’s outer layers; and if it makes it inside, there is a thicket of magnetic fields waiting to scatter and deflect the intruder.
“At times of low solar activity, this natural shielding is weakened, and more cosmic rays are able to reach the inner solar system,” explains Pesnell.
Mewaldt lists three aspects of the current solar minimum that are combining to create the perfect storm:

 The sun’s magnetic field is weak. “There has been a sharp decline in the sun’s interplanetary magnetic field (IMF) down to only 4 nanoTesla (nT) from typical values of 6 to 8 nT,” he says. “This record-low IMF undoubtedly contributes to the record-high cosmic ray fluxes.”
 The heliospheric current sheet is shaped like a ballerina’s skirt.  Credit: J. R. Jokipii, University of Arizona
› Larger image
The solar wind is flagging. “Measurements by the Ulysses spacecraft show that solar wind pressure is at a 50-year low,” he continues, “so the magnetic bubble that protects the solar system is not being inflated as much as usual.” A smaller bubble gives cosmic rays a shorter-shot into the solar system. Once a cosmic ray enters the solar system, it must “swim upstream” against the solar wind. Solar wind speeds have dropped to very low levels in 2008 and 2009, making it easier than usual for a cosmic ray to proceed.
 The current sheet is flattening. Imagine the sun wearing a ballerina’s skirt as wide as the entire solar system with an electrical current flowing along the wavy folds. That is the “heliospheric current sheet,” a vast transition zone where the polarity of the sun’s magnetic field changes from plus (north) to minus (south). The current sheet is important because cosmic rays tend to be guided by its folds. Lately, the current sheet has been flattening itself out, allowing cosmic rays more direct access to the inner solar system.

“If the flattening continues as it has in previous solar minima, we could see cosmic ray fluxes jump all the way to 30% above previous Space Age highs,” predicts Mewaldt.
Earth is in no great peril from the extra cosmic rays. The planet’s atmosphere and magnetic field combine to form a formidable shield against space radiation, protecting humans on the surface. Indeed, we’ve weathered storms much worse than this. Hundreds of years ago, cosmic ray fluxes were at least 200% higher than they are now. Researchers know this because when cosmic rays hit the atmosphere, they produce an isotope of beryllium, 10Be, which is preserved in polar ice. By examining ice cores, it is possible to estimate cosmic ray fluxes more than a thousand years into the past. Even with the recent surge, cosmic rays today are much weaker than they have been at times in the past millennium.
“The space era has so far experienced a time of relatively low cosmic ray activity,” says Mewaldt. “We may now be returning to levels typical of past centuries.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9301c8ce',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The word ""quarantine"" has taken on an extra meaning and been named ""word of the year"" by the Cambridge Dictionary.**
Editors said the word was now ""synonymous with lockdown"" and relates to staying at home to avoid catching the disease.
Previously it was only defined in relation to a person or animal ""suspected of being contagious"".
Other words on the 2020 shortlist included ""lockdown"" and ""pandemic"", it added.
On Tuesday, Oxford Dictionaries said it had expanded its word of the year to encompass several ""Words of an Unprecedented Year"".
Cambridge Dictionary said quarantine was the third most looked-up word overall this year, after ""hello"" and ""dictionary"".
It recorded a surge of searches for ""quarantine"" in March, when restrictions were imposed.
Wendalyn Nichols, its publishing manager, said: ""The words that people search for reveal not just what is happening in the world, but what matters most to them in relation to those events.
""Neither coronavirus nor Covid-19 appeared among the words that Cambridge Dictionary users searched for most this year.
""We believe this indicates people have been fairly confident about what the virus is.""
The new sense of the word, defining it as ""a general period of time in which people are not allowed to leave their homes or travel freely, so that they do not catch or spread a disease"", has been added to the dictionary.
Other news words included ""HyFlex"", which is short for hybrid flexible and denotes a type of teaching where some students are physically present in class and others join from a distance online.
As a result of people stopping shaking hands, kissing or hugging since the outbreak of Covid-19, the phrase ""elbow bump"" has also been added.
It is defined in the dictionary as ""a friendly greeting in which you touch someone's elbow with your elbow"".
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_eastofenglandnews@bbc.co.uk"
"**A secondary school in Powys has been closed for two weeks after a spike in coronavirus cases.**
Welshpool High School will be shut from Tuesday until 7 December, following outbreaks among at least three different year groups.
Head teacher Jim Toal said: ""It saddens me to have to resort to this measure but there really isn't an alternative at this stage.""
The school's 1,000 students have been told lessons will continue online.
Year 7 pupils at the school were told to self-isolate on 15 November, while Year 10 and 11 were told they would also need to stay home on Sunday.
However, a decision was taken on Monday to close the school entirely, following a review by staff and council officials.
""Despite the school's best efforts to mitigate the spread of infection a rapid growth of positive cases in the Welshpool area has led to a tipping point over the most recent weekend, and into this morning making further intervention essential,"" Mr Toal added.
""We continue to work with Environmental Health and Public Health Wales to identify those pupils who may be contactsÂ of a positive case.""
Infection rates in the town now stand at 133.8 per 100,000 of the population over the last seven days.
The move follows decisions to close 13 schools on Monday across Ceredigion and Pembrokeshire.
Phyl Davies, who is responsible for education on Powys council, told the Local Democracy Reporting Service: ""The rising cases at the school and in the community is now causing significant concerns for both the school's senior leaders and the council.
""Due to these concerns, it has been decided to keep the high school closed for two weeks to keep pupils and school staff safe."""
"
Share this...FacebookTwitterSteffen Hentrich here of the Liberal Institute, a think tank of the Friedrich-Naumann-Foundation for Freedom brings our attention here to a PNAS paper written by Glen P. Peters, Jan C. Minx, Christopher L. Weber and Ottmar Edenhofer titled: Growth in emission transfers via international trade from 1990 to 2008.
Transfering efficient manufacturing to developing countries leads to more CO2 emissions, and not less. Photo source: Library of Congress CALL NUMBER LC-USW36-376
Though not saying it directly, the paper calls current climate policy a failure. It’s right there in the very first sentence (emphasis added):
Despite the emergence of regional climate policies, growth in global CO2 emissions has remained strong. From 1990 to 2008 CO2 emissions in developed countries (defined as countries with emission reduction commitments in the Kyoto Protocol, Annex B) have stabilized, but emissions in developing countries (non-Annex B) have doubled.”
That folks, is what we call POLICY FAILURE – period. In fact climate policy has likely produced just the opposite of what was intended, meaning more CO2 and not less.
The authors quantified the growth in emission transfers via international trade. To do this they developed a trade-linked global database for CO2 emissions covering 113 countries and 57 economic sectors from 1990 to 2008. Here’s what they found, taken from the front page:
…emissions from the production of traded goods and services have increased from 4.3 Gt CO2 in 1990 (20% of global emissions) to 7.8 Gt CO2 in 2008 (26%). Most developed countries have increased their consumption-based emissions faster than their territorial emissions, and non–energy-intensive manufacturing had a key role in the emission transfers. The net emission transfers via international trade from developing to developed countries increased from 0.4 Gt CO2 in 1990 to 1.6 Gt CO2 in 2008, which exceeds the Kyoto Protocol emission reductions. Our results indicate that international trade is a significant factor in explaining the change in emissions in many countries, from both a production and consumption perspective.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




What does all that mean? It means that the developed countries are simply moving their CO2-intensive industry off their territory, and placing it on territories that are exempt from mandatory reductions, i.e developing countries. This what they call emissions transfer. The result: less CO2 emissions at home, but huge, greater increases in the less efficient country that took over the industry. This is what the study has confirmed.
Indeed the whole scheme backfires because undeveloped countries often have lower environmental and technical standards, and so produce the goods with considerably higher emissions and real pollution. Then add the transport of these goods from these developing countries back to Europe or North America, which adds even more CO2. And let’s not even look at the biofuels debacle the fossil fuel hysteria led to.
The PNAS paper writes in the discussion part (emphasis added:
Under the IPCC accounting rules of only reporting territorial emissions, many developed countries have reported stabilized emissions. However, our results show that the global emissions associated with consumption in many developed countries have increased with a large share of the emissions originating in developing countries. This finding may benefit economic growth in developing countries, but the increased emissions could also make future mitigation more costly in the developing countries. In addition, we find that the emission transfers via international trade often exceed the emission reductions in the developed countries.”
So what do the environmental and economic masterminds intend to do about it? Not much for now. Solving the emissions transfer problem of course would mean massive interference in global markets and end up punishing developing countries. The authors recommend:
We suggest that countries monitor emission transfers via international trade, in addition to territorial emissions, to ensure progress toward stabilization of global greenhouse gas emissions.”
Monitor? Now they don’t mean that countries start thinking of ways to restrict trade, now do they? The more they meddle with the economy and trade, the more they are going to mess everything up.
Share this...FacebookTwitter "
"

The U.S. Congress finally passed a stimulus package after making sure to strip out elements that would benefit the economy over the long run. Nonetheless, the plan includes an accelerated depreciation provision that will be beneficial if it is later made permanent. But a new corporate tax survey by KPMG makes clear that this is only the first of many needed business tax reforms in the United States.



KPMG found that the U.S. has the fourth highest corporate‐​income‐​tax rate in the 30‐​nation Organisation for Economic Co‐​operation and Development (OECD). The combined U.S. federal and average state rate of 40% is almost 9 percentage points higher than the average OECD top corporate rate of 31.4%. 



This is a dramatic reversal of the U.S. tax situation. After cutting the federal corporate rate from 46% to 34% in 1986, policymakers fell asleep at the switch, perhaps assuming that the U.S. had claimed a low‐​tax advantage permanently. But most industrial countries followed the U.S. lead and cut tax rates in the late 1980s. Then another round of tax‐​rate cuts began in the late 1990s, with the result that the average OECD corporate rate fell from 37.6% in 1996 to just 31.4% by January 2002. The average corporate rate in the European Union is now 32.5%, down from 38.2% in 1996. 



Americans sometimes write‐​off European countries as uncompetitive welfare states, and ignore that many have improved their business climates. But a recent study by the Economist Intelligence Unit placed the U.S. second, behind the Netherlands, for the “best place in the world to conduct business.” And a study by GrowthPlus, a European think tank, compared 10 major countries to determine which had the best environment for entrepreneurial growth companies. Again, the U.S. finished second, this time behind Britain. 



In the last few years, the corporate tax rate was cut in Denmark, France, Ireland, Germany, Poland, and Portugal. Even socialist Sweden has a top corporate tax rate of just 28%. It is certainly true that overall European taxes, as a share of gross domestic product, are much higher than in the United States. But Europe has shifted about one‐​third of its overall tax burden to less distortionary consumption taxes. 



What the Europeans and others are realizing is that countries shoot themselves in the foot by imposing high tax rates on mobile capital. IMF data show that annual global portfolio‐​capital flows rose six‐​fold during the past decade. United Nations data show that direct investment also rose six‐​fold during this period. The U.S. attracts a big share of these flows because of its large economy and stable currency. But investment flows are increasingly sensitive to taxes, so it makes less and less sense to have a high corporate rate. After all, last year’s recession, the Enron collapse, and the high‐​tech bust all show that the U.S. business sector is not as invincible as it seemed in the late 1990s. 



A high statutory rate isn’t the only aspect of U.S. business taxation that needs reform. The new depreciation rules should be made permanent, and the current global reach of the corporate income tax should be replaced with a “territorial” tax. Glenn Hubbard, chairman of the Council of Economic Advisers, has noted that “from an income tax perspective, the U.S. has become one of the least attractive industrial countries in which to locate the headquarters of a multinational corporation.” As a consequence, there has been a “marked increase” in the number of U.S. firms reincorporating abroad, according to a new U.S. Treasury analysis. 



The critics of course will say that big corporations and their shareholders should pay their “fair share” of taxes, and that the government needs to crack down on tax‐​avoiders like Enron. Such views ignore big‐​picture realities. First, the huge rise in global capital flows means that the corporate tax burden falls more on immobile workers, and less on the mobile capital income it is ostensibly placed on. Second, the high U.S. corporate tax rate is the reason why Enron and other firms go to such wasteful lengths to avoid and evade taxes. 



As the world economy changes, so must U.S. tax policy. Pressures to attract mobile capital through international “tax competition” will continue to increase. These trends dictate that the U.S. reform its tax system by moving away from a high‐​rate income tax system to a low‐​rate consumption‐​based tax system.
"
"

Cueball: Catania photosphere image August 31st, 2009 - click for larger image
It has been a strange day. Fires have evacuated the Mt. Wilson Observatory in California, and SOHO images have not been updating all day. Power is down at the mountain and the webcam has gone offline. See status here. Mt. Wilson Observatory is now in the hands of nature and CDF. Let’s hope CDF wins.
The only ""observer"" left at Mount Wilson on Monday afternoon was the automated webcam atop the solar tower. This was its smoky westward view at 6:54 p.m. Pacific time. Still no flames coming over the crests. UCLA Dept. of Physics and Astronomy
It  is about 4 hours now past ooGMT Sept1, 2009 I’ve checked all my sources. Besides the fate of Mt. Wilson, we’ve all been waiting to find out two things:
1- Will we have a spotless calendar month for the sun in August 2009?
2- Do I still have my solar mojo?
The Catania sunspot drawing shows nothing for the 31st.
Catainia Observatory Solar Sketch - click for larger image
Other solar observatories, Uccle in Beligium, Locarno in Germany, both show nothing on August 31st sketches.

This animation from SIDC of the past 30+ days shows nothing for August but DOES show group 1025 popping up on 9/1/2009
http://sidc.oma.be/html/cmap_animator.html
I also checked SIDC’s sunspot report data for August, nothing.
It looks like the spot today, group 1025, squeaked by and was not observed until after August 31st game clock ran out at 00 GMT 91/2009
Then I checked NOAA SWPC….
http://www.swpc.noaa.gov/ftpdir/latest/DSD.txt
Message to NOAA Space Weather: Out damned spot!
And wouldn’t you know it, they have something whereas last year it was the other way around…NOAA had nothing, SIDC (via Catania) did…so where does that leave us?
Leif said last year that SIDC had the last word…so unless they change their report, we may indeed have a spotless calendar month.
We’ll have to see what happens when their report comes out tomorrow. They issue a new report on the first of each month.
http://sidc.oma.be/products/ri_hemispheric/
Watch that space.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93692375',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The word juggernaut derives its meaning from the ancient Hindu god Jagan‐​n, or lord of the world, whose powers crushed all those who stood in his way. Formerly, fanatics sometimes threw themselves under the wheels of carts bearing his image to be crushed as a demonstration of the god’s power. Today, the word juggernaut connotes an overwhelming force that crushes or seems to crush everything in its path.



The word accurately describes the powerful position held by the National Education Association and the American Federation of Teachers. These two unions wield massive power over public school teachers and other school employees. They maintain this power through the inertia of a well entrenched monopoly in representing teachers.



Since the 1960s, the NEA and AFT have dominated the market for teacher representation services. In the 34 states that require school boards to bargain collectively with teachers, the NEA and AFT currently share almost 100 percent of the market for teacher representation services.



Although the AFT and NEA assert that their tenure as exclusive representative demonstrates teacher satisfaction with their services, in reality, teachers desiring a different representative, or none at all, face enormous legal and practical obstacles to achieve any such change. For example, the NEA and AFT have a virtually limitless supply of funds and personnel to thwart any effort to achieve representation by a different entity. But the teachers seeking this objective must finance the effort from their personal funds. Litigation, virtually certain to materialize, can bankrupt the dissidents, whereas knowledgeable staff lawyers are available on call to the unions.



The NEA/AFT monopoly in representing teachers at the local level is a grave disservice to teachers who are forced to pay higher than necessary union dues or who don’t like the policies of the NEA and AFT. But the enormous power and privilege of the incumbent union makes it almost impossible for teachers to change to another union or no union at all. The NEA/AFT juggernaut easily crushes any opposition that threatens their monopoly over teacher representation.



To alleviate this problem, competition is required in the teacher representation market. One way to provide that would be to allow for‐​profit and nonprofit entities as well as solo entrepreneurs, labor lawyers, and collective bargaining companies to represent teachers in collective bargaining. Teachers would retain the right to go without an exclusive representative and each of the representative options would compete against all the others. Teacher representation would not be limited to membership organizations as it is now, and teachers could change their choice of representative periodically, perhaps every three years or at the expiration of a collective agreement covering teachers.



Providing increased competition to the NEA and AFT would require some changes in current labor policy in most states. For example, states legislatures would have to do three things: 1) reduce the number of votes required to trigger an election to select a new teacher representative, 2) explicitly allow individuals, nonprofit and for‐​profit organizations to compete for the right to represent teachers, and 3) enable all members in the bargaining unit to vote on the key decisions affecting their terms and conditions of employment. 



Teachers would experience a number of significant benefits from competition to represent them in collective bargaining. These benefits include lower dues, better service, increased choice, and more input in the key decisions affecting their employment. Allowing solo entrepreneurs, professional negotiators, lawyers, and collective bargaining companies to compete with the NEA/AFT for teacher representation would create a more powerful consumer role for teachers who wish to purchase these services. Introducing competition into teacher representation is the best way to insure that unions work for the benefit of teachers.



Also, considering that the two largest teacher unions are the major opponents of school choice, diminishing the role and influence of the NEA/AFT would contribute to a more positive climate for the school choice movement. Without their opposition to school choice, many more options would be available to parents of K-12 children. Parents and their children, therefore, become the secondary beneficiaries of increased competition for teacher representation.



Undoubtedly, enactment of the proposal in one state would publicize the concept nationally to rank‐​and‐​file teachers who have every reason to support legislation that would expand their choice of exclusive representative. It is difficult to see how the NEA/AFT could successfully persuade most teachers that legislation that expands their choice of exclusive representative is harmful to teachers.



In Economics 101, we are told that ease of entry is the most important requirement for a competitive market to emerge. Perhaps if teachers experience the benefits of competition as consumers, they will recognize its value for parents and students. Rolling back the teacher union juggernaut may be the next critical step in improving schools and creating meaningful educational choice for parents and students.
"
"
Scientists seek to remove climate fear promoting editor and ‘trade him to New York Times or Washington Post’

An outpouring of skeptical scientists who are members of the American Chemical Society (ACS) are revolting against the group’s editor-in-chief — with some demanding he be removed — after an editorial appeared claiming “the science of anthropogenic climate change is becoming increasingly well established.”
The editorial claimed the “consensus” view was growing “increasingly difficult to challenge, despite the efforts of diehard climate-change deniers.” The editor now admits he is “startled” by the negative reaction from the group’s scientific members. The American Chemical Society bills itself as the “world’s largest scientific society.”
The June 22, 2009 editorial in Chemical and Engineering News by editor in chief Rudy Baum, is facing widespread blowback and condemnation from American Chemical Society member scientists. Baum concluded his editorial by stating that “deniers” are attempting to “derail meaningful efforts to respond to global climate change.”
Dozens of letters from ACS members were published on July 27, 2009 castigating Baum, with some scientists calling for his replacement as editor-in-chief.
The editorial was met with a swift, passionate and scientific rebuke from Baum’s colleagues. Virtually all of the letters published on July 27 in castigated Baum’s climate science views. Scientists rebuked Baum’s use of the word “deniers” because of the terms “association with Holocaust deniers.” In addition, the scientists called Baum’s editorial: “disgusting”; “a disgrace”; “filled with misinformation”; “unworthy of a scientific periodical” and “pap.”
One outraged ACS member wrote to Baum: “When all is said and done, and you and your kind are proven wrong (again), you will have moved on to be an unthinking urn for another rat pleading catastrophe. You will be removed. I promise.”
Baum ‘startled’ by scientists reaction.
Baum wrote on July 27, that he was “startled” and “surprised” by the “contempt” and “vehemence” of the ACS scientists to his view of the global warming “consensus.”
“Some of the letters I received are not fit to print. Many of the letters we have printed are, I think it is fair to say, outraged by my position on global warming,” Baum wrote.
Selected Excerpts of Skeptical Scientists: 
“I think it’s time to find a new editor,” ACS member Thomas E. D’Ambra wrote.
Geochemist R. Everett Langford wrote: “I am appalled at the condescending attitude of Rudy Baum, Al Gore, President Barack Obama, et al., who essentially tell us that there is no need for further research—that the matter is solved.”
ACS scientist Dennis Malpass wrote: “Your editorial was a disgrace. It was filled with misinformation, half-truths, and ad hominem attacks on those who dare disagree with you. Shameful!”
ACS member scientist Dr. Howard Hayden, a Physics Professor Emeritus from the University of Connecticut: “Baum’s remarks are particularly disquieting because of his hostility toward skepticism, which is part of every scientist’s soul. Let’s cut to the chase with some questions for Baum: Which of the 20-odd major climate models has settled the science, such that all of the rest are now discarded? […] Do you refer to ‘climate change’ instead of ‘global warming’ because the claim of anthropogenic global warming has become increasingly contrary to fact?”
Edward H. Gleason wrote: “Baum’s attempt to close out debate goes against all my scientific training, and to hear this from my ACS is certainly alarming to me…his use of ‘climate-change deniers’ to pillory scientists who do not believe climate change is a crisis is disingenuous and unscientific.”
Atmospheric Chemist Roger L. Tanner: “I have very little in common with the philosophy of the Heartland Institute and other ‘free-market fanatics,’ and I consider myself a progressive Democrat. Nevertheless, we scientists should know better than to propound scientific truth by consensus and to excoriate skeptics with purple prose.”
William Tolley: “I take great offense that Baum would use Chemical and Engineering News, for which I pay dearly each year in membership dues, to purvey his personal views and so glibly ignore contrary information and scold those of us who honestly find these views to be a hoax.”
William E. Keller wrote: “However bitter you (Baum) personally may feel about CCDs (climate change deniers), it is not your place as editor to accuse them—falsely—of nonscientific behavior by using insultingly inappropriate language. […] The growing body of scientists, whom you abuse as sowing doubt, making up statistics, and claiming to be ignored by the media, are, in the main, highly competent professionals, experts in their fields, completely honorable, and highly versed in the scientific method—characteristics that apparently do not apply to you.”
ACS member Wallace Embry: “I would like to see the American Chemical Society Board ‘cap’ Baum’s political pen and ‘trade’ him to either the New York Times or Washington Post.” [To read the more reactions from scientists to Baum’s editorial go here and see below.]
Physicist Dr. Lubos Motl, who publishes the Reference Frame website, weighed in on the controversy as well, calling Baum’s editorial an “alarmist screed.”
“Now, the chemists are thinking about replacing this editor who has hijacked the ACS bulletin to promote his idiosyncratic political views,” Motl wrote on July 27, 2009.
Baum cites discredited Obama Administration Climate Report
To “prove” his assertion that the science was “becoming increasingly well established,” Baum cited the Obama Administration’s U.S. Global Change Research Program (USGCRP) study as evidence that the science was settled. [Climate Depot Editor’s Note: Baum’s grasp of the latest “science” is embarrassing. For Baum to cite the June 2009 Obama Administration report as “evidence” that science is growing stronger exposes him as having very poor research skills. See this comprehensive report on scientists rebuking that report. See: ‘Scaremongering’: Scientists Pan Obama Climate Report: ‘This is not a work of science but an embarrassing episode for the authors and NOAA’…’Misrepresents the science’ – July 8, 2009 )
Baum also touted the Congressional climate bill as “legislation with real teeth to control the emission of greenhouse gases.” [Climate Depot Editor’s Note: This is truly laughable that an editor-in-chief at the American Chemical Society could say the climate bill has “real teeth.” This statement should be retracted in full for lack of evidence. The Congressional climate bill has outraged environmental groups for failing to impact global temperatures and failing to even reduce emissions! See: Climate Depot Editorial: Climate bill offers (costly) non-solutions to problems that don’t even exist – No detectable climate impact: ‘If we actually faced a man-made ‘climate crisis’, we would all be doomed’ June 20, 2009 ]
The American Chemical Society’s scientific revolt is the latest in a series of recent eruptions against the so-called “consensus” on man-made global warming.
On May 1 2009, the American Physical Society (APS) Council decided to review its current climate statement via a high-level subcommittee of respected senior scientists. The decision was prompted after a group of 54 prominent physicists petitioned the APS revise its global warming position. The 54 physicists wrote to APS governing board: “Measured or reconstructed temperature records indicate that 20th – 21st century changes are neither exceptional nor persistent, and the historical and geological records show many periods warmer than today.”
The petition signed by the prominent physicists, led by Princeton University’s Dr. Will Happer, who has conducted 200 peer-reviewed scientific studies. The peer-reviewed journal Nature published a July 22, 2009 letter by the physicists persuading the APS to review its statement. In 2008, an American Physical Society editor conceded that a “considerable presence” of scientific skeptics exists.
In addition, in April 2009, the Polish National Academy of Science reportedly “published a document that expresses skepticism over the concept of man-made global warming.” An abundance of new peer-reviewed scientific studies continue to be published challenging the UN IPCC climate views. (See: Climate Fears RIP…for 30 years!? – Global Warming could stop ‘for up to 30 years! Warming ‘On Hold?…’Could go into hiding for decades,’ peer-reviewed study finds – Discovery.com – March 2, 2009 & Peer-Reviewed Study Rocks Climate Debate! ‘Nature not man responsible for recent global warming…little or none of late 20th century warming and cooling can be attributed to humans’ – July 23, 2009 )
A March 2009 a 255-page U. S. Senate Report detailed “More Than 700 International Scientists Dissenting Over Man-Made Global Warming Claims.” 2009’s continued lack of warming, further frustrated the promoters of man-made climate fears. See: Earth’s ‘Fever’ Breaks! Global temperatures ‘have plunged .74°F since Gore released An Inconvenient Truth’ – July 5, 2009
In addition, the following developments further in 2008 challenged the “consensus” of global warming. India Issued a report challenging global warming fears; a canvass of more than 51,000 Canadian scientists revealed 68% disagree that global warming science is “settled”; A Japan Geoscience Union symposium survey in 2008 reportedly “showed 90 per cent of the participants do not believe the IPCC report.” Scientific meetings are now being dominated by a growing number of skeptical scientists. The prestigious International Geological Congress, dubbed the geologists’ equivalent of the Olympic Games, was held in Norway in August 2008 and prominently featured the voices of scientists skeptical of man-made global warming fears. [See: Skeptical scientists overwhelm conference: ‘2/3 of presenters and question-askers were hostile to, even dismissive of, the UN IPCC’ & see full reports here & here – Also see: UN IPCC’s William Schlesinger admits in 2009 that only 20% of IPCC scientists deal with climate ]
h/t to ClimateDepot.com go there for links to the above referenced stories.
The ACS letters to the editor are here:  http://pubs.acs.org/cen/letters/87/8730letters.html


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e943625d8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It’s nice to know, even in these troubled times, that eternal verities remain. One is outright lying about environmental issues in order to stampede world leaders who currently have bigger fish to fry.



The latest example of this agitprop was generated in the Oct. 29 issue of The Guardian, the paper of the loony London left. It was timed to coincide with yet another U.N. meeting over the Kyoto Protocol, in, of all places, Marrakech, which began on the same day.



The Guardian describes the plight of all 10,991 poor inhabitants of Tuvalu, an island in the middle of the Pacific Ocean. They have pestered the New Zealand government into accepting each and every one of them as environmental refugees, cast adrift by sea level rise from dreaded global warming. The New Zealand government took the bait. The first evacuees are scheduled to arrive next year.



However, sea level around Tuvalu has been falling precipitously for the last half‐​century. You could look it up in the Oct. 27 issue of Science, which was available for days before The Guardian went to press.



French scientists, led by Cecile Cabanes, used data collected by altimeters aboard the TOPEX/​Poseidon satellite, and then compared them to a longer record of deep ocean temperatures that extends back to 1955. Sure enough, where the data overlapped (the satellite went up in 1993), there was very good agreement. The warmer (or colder) the ocean became, the more sea level rose (or fell).



There’s a beautiful color map of sea level changes in Cabanas’s Science article. It shows that Tuvalu is near the epicenter of a region where the sea level has been declining for nearly 50 years. In fact, the decline is so steep that even using the U.N.‘s lurid (and wrong) median estimates of global warming for the next century will not get the Tuvalus back to their 1950 sea level until 2050.



Unfortunately, The Guardian isn’t the only left‐​wing paper that didn’t do its homework about sea levels around Tuvalu. On Sept. 9, the Travel section of the Washington Post carried a huge article headlined “The End is Near,” complete with that umptysquat‐​point headline drowning in the ocean.



Writer Mike Tidwell includes the following tidbit: “Perhaps the most surreal indication of what might be in store comes from the idyllic, tourist‐​friendly nations of Tuvalu and Kiribati, in the South Pacific. Tuvalu is developing concrete emigration plans to evacuate its islands — perhaps entirely — in this century, migrating en masse to ‘host countries’ like New Zealand. This is because scientists say sea level rise could inundate Tuvalu and other low‐​lying countries almost entirely as polar ice melts and ocean water expands.”



How difficult would it have been to check the facts first? The same “scientists” have published graphs in widely available U.N. reports showing sea level fall in the Pacific and Indian oceans.



What’s the real reason for the Tuvalu exodus? I must be careful here, as my editors caution that a Cato scholar must use precise and non‐​inflammatory language: Tuvalu sucks.



There are no rivers or sources of potable water. Beachheads are eroding because the sand has been removed for building material. Most of the vegetation has been burned for fuel by the environmentally sensitive natives. The soil is poor. There are no mineral deposits and few exports. A large percentage of the GDP comes from licensing its area code for “900” lines and revenue from the sale of its “.tv” Internet domain.



In short, Tuvalu is a Tuvalu‐​made ecological disaster that is now an economic disaster. The natives want out because they wrecked the place. And why does New Zealand want them in? Perhaps because the socialist‐​green coalition government of Prime Minister Helen Clark sees 10,991 votes, largely without skills or jobs, bought and paid for with plane tickets and nurtured with welfare.



The Tuvalu story is an icon of environmental and political deceit, generated by a compliant media that have no regard for inconvenient facts. There are over 2, 000 members of the Society of Environmental Journalists–all attuned to the goings on in Marrakech–and not one of them has the gumption to see if, in fact, Tuvalu is drowning. Instead, they sit quietly, supporting yet another tired attempt to stampede world leaders into an environmental treaty that all competent scientists know has no detectable effect on world climate, even as it costs a fortune to a world fighting for its very civilization.
"
"

 **Introduction**



Although the stew that is the U.S.-China trade relationship has the potential to reach a full boil, it has been on a low simmer since before the start of the financial crisis and subsequent global economic slowdown. Despite pork bans, poultry bans, a steady stream of antidumping and countervailing duty investigations, dispute settlement judgments from the World Trade Organization, accusations of currency manipulation, admonitions regarding China’s dependence on export‐​led growth, and China’s concerns about the impact of profligate U.S. government spending on its U.S. debt holdings, the relationship has held up fairly well.



But that could all change quickly. By September 17 President Obama is required to render a decision in a potentially combustible case concerning automobile tire imports from China. Pursuant to a petition filed by the United Steelworkers of America under Section 421 of the Trade Act of 1974‐​known colloquially as the “China‐​Specific Safeguard”-the U.S. International Trade Commission has already recommended that Obama impose duties of 55 percent on imports of consumer tires from China. Under the law, the president can adopt, modify, or reject that recommendation.



Although this may sound like just another day in Washington, Obama’s decision will be consequential. It will help clarify his administration’s heretofore opaque tradepolicy objectives. It will set the tone for U.S.-China trade relations for the foreseeable future. And it will affect broader international trade relations, for better or for worse, as America honors or disavows its pledge to the Group of 20 nations to avoid new protectionist measures.



Under the statute, the president has discretion to deny import “relief” if he determines that such restrictions would have an adverse impact on the U.S. economy that is clearly greater than its benefits, or if he determines that such relief would cause serious harm to the national security of the United States. The first condition is met overwhelmingly. And, for good measure, there is a very strong argument that the second is met, too.



But at the end of the day the president is a politician, who is presumed to owe Big Labor for his election last November. Will the president do what is overwhelmingly in the best interest of the country? Or will he do what he thinks is best for himself politically? This paper provides some law and case background and then summarizes why the president should reject the recommendations of the USITC and deny import restrictions altogether.



 **The Section 421 Statute and a Brief History**



Section 421 of the Trade Act of 1974, as amended, is a special statute that applies only to imports from China. It became U.S. law as a condition of China’s accession to the World Trade Organization in 2001. The provision aimed to assuage fears about Chinese competition by establishing a special “safeguard” to deal with increased imports from China for the first 12 years after China’s entry into the WTO.1 The law will expire at the end of 2013.



The broader U.S. “safeguard” law, Section 201 of the Trade Act of 1974, authorizes the imposition of temporary trade barriers against increased imports that are a “substantial cause” of “serious injury” to American producers. The China‐​specific safeguard of Section 421, by contrast, sets a lower threshold for imposing trade restrictions. Specifically, the statute provides:



If a product of the People’s Republic of China is being imported into the United States in such increased quantities or under such conditions as to cause or threaten to cause market disruption to the domestic producers of a like or directly competitive product, the President shall, in accordance with the provisions of this section, proclaim increased duties or other import restrictions with respect to such product, to the extent and for such period as the President considers necessary to prevent or remedy the market disruption.2



Under the statute, “market disruption” exists “whenever imports of an article like or directly competitive with an article produced by a domestic industry are increasing rapidly, either absolutely or relatively, so as to be a significant cause of material injury, or threat of material injury, to the domestic industry.“3 And the term “significant cause” refers to “a cause which contributes significantly to the material injury of the domestic industry, but need not be equal to or greater than any other cause.“4



If the ITC renders an affirmative finding (which is decided by majority vote) or if there is an even split among commissioners, the affirming commissioners must submit recommendations for relief to the president and the U.S. Trade Representative within 20 days of the determination. The USTR then has 55 days to advise the president about the ITC’s findings‐​a period during which it must hold hearings on the matter and solicit views from importers, exporters, and other interested parties. It is also authorized to pursue negotiations to address the underlying market disruption with the Chinese government during this period.



Unless an agreeable settlement is reached, the president must announce import relief by the 150th day after the petition’s filing unless he determines that “provision of such relief is not in the national economic interest of the United States or, in extraordinary cases, that the taking of action … would cause serious harm to the national security of the United States.“5 If the president chooses to grant import relief, it must become effective within 15 days of his decision.



It is also important to appreciate what Section 421 is not. It is not an “unfair trade” statute. Unlike the antidumping and countervailing duty laws, a Section 421 case does not include allegations of prices at less than fair value or prices that benefit from countervailable government subsidies. The evidentiary threshold is much lower. All that is alleged‐​and all that has to be established‐​in a 421 petition is that imports from China are increasing in such a manner as to be a cause of market disruption (or threat thereof) to the domestic industry.



Section 421 is not intended to remedy any wrongdoing on the part of Chinese exporters, but is intended rather to give U.S. producers the opportunity to holler “time out!” as they catch their breath, assess prospects, and attempt to adjust to a new level of competition. Of course there are huge costs to this kind of intervention in the marketplace, thus the president is granted discretion, under the law, to deny relief if he determines that the costs to the broader economy clearly exceed any benefits to the petitioning industry. While such discretion provides some comfort that the law’s relaxed evidentiary standards won’t be routinely abused by domestic interests seeking to stifle competition, there are no guarantees that the president’s discretion will be based exclusively on considerations of the national economic interest. If there were, it would be nearly impossible to conjure a scenario in which the concentrated, temporary benefits to a specific industry receiving protection were not overwhelmed by the costs of that protection on the broader economy. Political considerations always influence decisions that lead to protection.



During the Bush administration (the first administration under which the law was in effect), there were six Section 421 cases filed by domestic parties, and in four of those the ITC found market disruption and recommended import restrictions. In each of those four cases, President Bush exercised his discretion to deny relief. Thus, trade restrictions have never been imposed under this statute.



 **Some Specifics of the Tires Case**



The tires case is noteworthy in several respects, starting with the fact that it is the first Section 421 case initiated during the Obama administration. Petitioners came to regard the law as a dead letter under President Bush, but have been anxious to test its viability under a new president, who promised last year to decide Section 421 cases “on their merits, not on the basis of an ideological rejection of import relief like that of the current administration.“17



The petition in the tires case was filed by the United Steel, Paper and Forestry, Rubber, Manufacturing, Energy, Allied Industrial and Service Workers International Union‐​the United Steel Workers, for short‐​on behalf of workers in the U.S. tire industry. However, the USW represents workers accounting for only 47 percent of U.S. tire production capacity, so most workers in the industry are not officially supporting the petition.7 Furthermore, this is the first 421 case that does not have a domestic producer as the petitioner. Out of the 10 firms determined to comprise the entire domestic tire industry, none supports the union’s petition for import restraints. Thus, this case, initiated on behalf of no producers and less than half of the industry’s workers, and given the acrimony it has engendered within the U.S. tire‐​supply chain, is probably not the kind of case that President Obama idealized when he promised to decide these issues “on their merits.”



In reaching its affirmative conclusion of market disruption in June 2009, the USITC cited the 215‐​percent increase in the volume of tire imports from China between 2004 and 2008 as a cause of material injury to the domestic industry. The conclusion of material injury was based on evidence of declining industry capacity, production, shipments, employment, wages, and financial results.8



The argument put forward by respondents in the case (i.e., various producers, exporters, and importers) during the ITC proceeding, which was ultimately rejected in the majority’s determination, is that tire production is stratified among three quality “tiers,” and that competition across tiers is mitigated. Most of the increase in imports from China was of the lowest tier, while most of the tires produced in the United States are of the top two tiers.



Furthermore, 7 of the 10 U.S. tire producers also manufacture tires in China, as well as in other countries.9 And of those 7 firms, 4‐​Goodyear, Michelin, Cooper, and Bridgestone‐​account for almost 90 percent of U.S. production. Thus, the change in composition of domestic and imported tires in the U.S. market is a function of the decisions of these U.S. producers. And it was a deliberate decision of U.S. producers to reduce production of Tier‐​3 tires‐​the lowest‐​end, lowest‐​profit‐​margin tires‐​at their U.S. plants, and increase sourcing of that tier in China and elsewhere, where lower production costs enable the realization of some profit, which in turn helps support continued production of Tier‐​1 and Tier‐​2 tires in the United States. Thus, the declining employment, production, capacity, and shipments are all attributable to intentional, conscious planning on the part of profit‐​maximizing firms.



For a law that is characterized by its champions as a tool to support our producers vis‐​à‐​vis Chinese producers and to ensure a level playing field, this test case for Obama pits American workers against American producers, and American workers against American workers. By going after Chinese producers, petitioners ensnare their own employers, as well as fellow American workers, organized or otherwise. Although the lightning rod is China (with all of the negative perceptions that have been cultivated about its trade practices), this case has little to do with China per se, and everything to do with organized labor begrudging U.S. producers for pursuing profit‐ maximizing strategies in a globalized world. In seeking sanctions, petitioners are asking Obama to indict globalization.



 **Whom Will Protectionism Help, and How**



Duties on imports of tires from China are more likely to lead to greater production in other developing countries than to greater production in the United States. U.S. producers have chosen to outsource production of their lower‐​tier tires to China because producing those tires in that location makes the most sense economically. But raising the costs of producing in China by imposing trade restrictions would not make U.S. production more attractive. It would not bring back U.S. jobs. It would make Indonesian or Mexican or Brazilian production more attractive, and would likely divert jobs from China to those countries.



According to data compiled by the ITC staff, the average unit price (based on the customs value) of a tire imported from China in 2008 was $38.98.10 A 55‐​percent tariff would drive up the unit value to $60.42. But, in 2008, U.S. producers sold 159 million tires, valued at $11 billion, for an average price of $68.60.11 Factoring in mark‐​up of the Chinese price, it is reasonable to conclude that prices of American‐ and Chinese‐​produced tires might retail for about the same price. But that outcome is highly unlikely to be incentive enough for globalized tire producers to divert production from China to the United States. Instead, producers are much more likely to shift production to Mexico, Brazil, or Indonesia, where the unit prices in 2008 (based on customs value) were $56.26, $48.93, and $32.10, respectively.12



Furthermore, the ITC’s recommended remedy would be in place for three years. The statute expires in four years. What kinds of changes should be expected during the interim that would make the United States a more cost‐​effective place to produce Tier‐​3 tires, or any tires for that matter? There are no changes‐​short of technological advances that raise productivity and reduce the demand for labor‐​that could make the United States a better place to produce tires. But this case is about jobs and nothing else, so even that outcome wouldn’t satisfy petitioners. Three years of “relief” will do nothing but perhaps defer the day of reckoning, while imposing heavy costs on the rest of the economy, taxing our relationship with China, and further sullying America’s international standing.



 **Adverse Economic Impact Is Clearly Greater Than any Benefits**



Formal economic models, testimony, anecdotes from representatives of industries in the tire‐​supply chain, and common sense analysis all reveal an excessive cost burden on the economy from the proposed remedy of 55‐​percent duties in year one; 45‐​percent duties in year two, and; 35- percent duties in year three.



In their dissenting opinion, ITC Vice Chairman Daniel R. Pearson and Commissioner Deanna Tanner Okun concluded:



[W]e find that imposing a trade‐​restrictive quota or tariff on the subject imports will be far more likely to cause market disruption than to alleviate it for domestic producers who have already undertaken significant strategic adjustments to adapt to a changing global market.13



Indeed, economist Thomas Prusa estimates that “the tire manufacturing industry will experience little to no job creation as a result of the tariff. Under the best‐​case scenario more than a dozen jobs will be lost for every job protected.” Prusa estimates a net loss of at least 25,000 U.S. jobs if the recommended tariff is imposed. Under the best case, Prusa finds that higher prices and other inefficiencies stemming from the proposed remedies would sap U.S. consumers of $600 to $700 million per year, translating to an annual cost of $300,000 for every job “protected” in the tire industry.14



According to a statement of the U.S. Tire Industry Association, which represents all segments of the tire industry, including those that manufacture, repair, recycle, sell, service, or use new or retreaded tires, and also those suppliers or individuals who furnish equipment, material, or services to the industry:



Our members, by directly importing or contracting with suppliers, are meeting the demands of a segment of the tire consumer market for lower‐​cost tires. No manufacturing uptick would satisfy this product segment, but instead could create a need for product allocation, resulting in shortages and outages. In the best of times, such occurrences are troubling, but in today’s climate, this could inflict severe financial harm on many retailers and on the motoring public.15



Consumer groups and other organizations have also expressed safety concerns about the impact of higher‐​priced tires on increasingly‐​pinched consumers. The likelihood that an increasing number of consumers will forego the replacement of old and worn‐​out tires presents a whole new category of risk and costs that are difficult to quantify economically.



If President Obama imposes trade restrictions in this case‐​regardless of whether those restrictions are as severe as the ITC’s recommendation or if they are milder‐​the United States will have blatantly violated its commitment to the G-20 earlier this year to avoid new invocations of protectionism. That would be a colossal mistake that simultaneously undermines U.S. credibility on trade and invites other governments to indulge their own protectionist lobbies. The consequences for world trade could be severe. There should be no doubt that the demonstration effect would not only influence other governments toward intervention, but would also encourage other U.S. interests to pursue their own protection under Section 421.



The fact that President Bush rejected the ITC’s recommendations to impose restrictions four times reinforces the perspective held by the Chinese government that the imposition of trade restrictions under Section 421 is firmly a matter of presidential discretion. Unlike antidumping or countervailing duties, which run on statutory autopilot without requiring the president’s attention or consent, Section 421 explicitly requires the attention and participation of the U.S. president. In other words, although there are over 90 outstanding U.S. antidumping and countervailing duty orders against various Chinese products, none of them is considered to reflect the direct wishes of the U.S. president, and thus none of them rise to the level of a potentially explosive trade dispute.



Technically, if the United States imposes restrictions under Section 421, the Chinese are not entitled to formally retaliate. But that’s cold comfort when it’s quite obvious that trade restraints under Section 421 will no doubt be considered by the Chinese to be a directive of the U.S. president, and thus the offense taken and the consequences wrought could be profound. U.S. industries across the manufacturing spectrum have written to President Obama, urging him not to impose restraints in this case for fear that they will bear the brunt of the costs through lost export sales. This is a real possibility.



As to the question of national security, the prospect of a spiraling trade war with China, if duties are levied and retaliation ensues, will strain the commercial ties that have been successfully cultivated over the past few decades and will increase the risk that China becomes less helpful on crucial matters of U.S. foreign and security policy.



 **Conclusion**



Through the first eight months of his tenure, the president has avoided making any decisions of consequence on matters of trade policy. While his actions have not been conclusively protectionist, his tepid rhetorical endorsements of trade and his conditional repudiations of protectionism have sown doubts at home and abroad about the direction of U.S. trade policy. A decision to reject trade restraints in the tires case would be reassuring to a world that is struggling to grow out of recession.



The stakes appear to be much higher for Obama than they were for Bush. The unions feel that they have earned the president’s support and are more emboldened in their position now than in the past. Bush didn’t win the near‐​unanimous support of organized labor leaders in his elections, as Obama did. Nor did Bush promise to get tough on Chinese trade practices, as Obama did. Instead, Bush set the precedent of denying relief‐​and he did it four times.



The USITC’s recommendation of a 55‐​percent tariff is a remedy far more restrictive than the quota sought by the USW. The president, then, may be tempted to offer what he might think is a compromise solution, of lower duties or a tariff‐​rate quota. But the costs of any protectionism at this time and under these circumstances could unleash a protectionist backlash in the United States and around the world. It would be far less costly for the president to reject trade restraints altogether and to capitalize on his earned credibility by moving the trade agenda forward.



 **Notes**



1 Daniel Ikenson, “Bull in a China Shop: Assessing the First Section 421 Trade Case,” Cato Free Trade Bulletin no. 2, January 1, 2003, p.1.



2 19 U.S.C. § 2451(a).



3 19 U.S.C. § 2451(c)(1).



4 19 U.S.C. § 2451(c)(2).



5 19 U.S.C. § 2451(k)(1). Note that in cases in which the ITC is evenly split, the president has complete discretion about whether or not to accept the affirming commissioners’ recommendations for relief.



6 Barack Obama (letter to Cass Johnson, President of the National Council of Textile Organizations, October 24, 2008), http://​www​.ncto​.org/​n​e​w​s​r​o​o​m​/​p​r​2​0​0​8​1​0​2​9.pdf.



7 U.S. International Trade Commission, _Certain Passenger Vehicle and Light Truck Tires from China, Investigations No. TA- 421–7_ , Publication 4085, July 2009, p. I-14.



8 For a full discussion of the statutory questions of increasing imports, material injury, and causation, see USITC, _Certain Passenger Vehicle and Light Truck Tires from China_ , pp. 11–29.



9 Those companies are Toyo, Yokohama, Pirelli, Michelin, Goodyear, Cooper, and Bridgestone. USITC, p. IV-3.



10 USITC, Table IV-4, p. IV-10.



11 Ibid., Table III-5, p. III-13.



12 Ibid., Table II-1, p. II-4.



13 Ibid., p.45.



14 Thomas J. Prusa, “Estimated Economic Effects of the Proposed Import Tariff on Passenger Vehicle and Light Truck Tires from China,” American Coalition for Free Trade in Tires (submission to the International Trade Commission, July 26, 2009).



15 Tire Industry Association (position statement, June 17, 2009), http://​www​.tirein​dus​try​.org/​p​d​f​/​n​e​w​s​_​a​r​c​h​i​v​e​s​/​press release061709.pdf.
"
"The British boats were outnumbered by about eight to one by the French. Before long there were collisions and projectiles were thrown. The British were forced to retreat, returning to port with broken windows but luckily no injuries. The conflict behind this skirmish between British and French fishers in the Bay de Seine at the end of August 2018 was quickly dubbed the “scallop war” in the press. The French had been trying to prevent the British scallop dredgers from legally fishing the beds in French national waters. But the incident exposed tensions that have been simmering for many years. Under the European Union’s Common Fisheries Policy (CFP), the British fishers had the legal right to fish in these waters, as did any boats from an EU member state. The complication came from a French regulation that prevented local boats from fishing these waters between May 16 and September 30 each year, in order to allow stocks to recover from the annual harvest. But under the CFP, an EU country has no authority to prevent another member state’s fleet from fishing its waters. This quirk of the CFP left the French fishers unable to dredge for scallops until October 1, and forced to stand by and watch while fleets from other countries harvested what they saw as a French resource from French waters. When the British boats arrived, the French fishers took on the role of guardians of their resource, actions they believed were justified but viewed by the British fishing industry as illegal. This incident in a small corner of a shared EU sea was settled within a few weeks thanks to a new agreement on how the two countries would share the scallop harvest. But the underlying tensions that the CFP has created over the sharing of national resources go much deeper, with a sense that the rules don’t allow for a fair use of the seas. This sense of unfairness was obviously expressed in the role that fishing played in Britain’s decision to leave the EU. Campaigners promised that “taking back control” of British waters would enable the country to revive its long-declining fishing industry and the communities that rely on it. Yet regardless of the impact the CFP has had on Britain’s fishers, their future after Brexit depends very much on any future trade deal that the government negotiates with the EU. And the history of how Britain has responded to conflicts over fishing rights far bigger than the scallop war doesn’t bode well for the industry.  Research into fishing activity shows that the decline of the British fishing industry began well before the creation of any European fishing policy. In fact, its ultimate origins can be traced to a surprising source: the expansion of the railways in the late 19th century.  Trawling, under the power of sail, had existed for more than 500 years. But without refrigeration, fish could only be delivered for sale to areas close to the ports. The coming of the rail network meant that fish could be transported inland to big towns and cities. To further meet this growing demand, steam trawlers started to replace sail trawlers from the 1880s onwards. The power of these steam vessels greatly increased the scope of trawling and allowed them to trawl for longer and further away from port while towing larger nets. British steam trawlers ventured further away from Britain in search of fish, with the fishing grounds expanding to areas as far as Greenland, north Norway and the Barent’s Sea, Iceland and the Faroe Islands. But as early as 1885, concerns were raised that this advance in technology was having a negative impact on both fishing stocks and their habitat. Evidence from records of fishing activity show that this improvement in technology and the increased size of the fishing fleet was squarely behind the increase in landings. The fishing boom that the railways had unleashed proved unsustainable, and the resulting overfishing would ultimately send the industry into a long-term downturn. After decades of more and more fishing, landings eventually started to decline after World War II, a trend that continued through the second half of the 20th century and into the new millennium. To compensate, the size and power of the fleet continued to increase as more effort was required to catch ever-scarcer fish. From the late 1950s, the amount of fish landed per unit of power declined at a faster rate than fish landings, as the fleet continued to expend more and more effort to maintain the size of catches. However, this effort was all in vain and by 1980 catches had declined to their lowest point in a century. Overfishing wasn’t the only reason for decline, however. The falling fish stocks combined with the improvements in the range and power of the fleet in the post-war years led Britain’s fishers to seek new waters, with more boats moving further away from the UK to catch enough fish to meet domestic demand. And this long range trawling brought the UK fleet into conflict with Iceland.  British fishers had fished these waters from the 15th century. However, Iceland’s fishing industry began to resent this as the steam trawlers began fishing off Iceland in the late 19th century. It led to accusations that British trawlers were damaging the fishing grounds and depleting stocks. In 1952, Iceland declared a four-mile zone around their country to stop excessive foreign fishing, although fish don’t stick to human-made boundaries and stocks could still be depleted outside of this zone. Iceland’s decision drew a response from the UK, which banned the import of Icelandic fish. As a major export market for Iceland’s most important industry they hoped that this would bring them to the negotiating table. In 1958, against a background of failed diplomacy, Iceland expanded this zone to 12 miles and banned foreign fleets from fishing in these waters, in defiance of international law. It led to the first of what became known as the Cod Wars – an act in three stages that lasted nearly 20 years.  During the first Cod War, Royal Navy frigates accompanied the UK fleet into Iceland’s exclusion zone to continue their fishing. A game of cat-and-mouse ensued between the Icelandic Coastguard vessels and the British trawlers. In response to attempts to seize them, the trawlers rammed the coastguard vessels and the coastguard threatened to open fire, although major incidents were avoided. In 1961, the two countries eventually came to an agreement that allowed Iceland to keep its 12-mile zone. In return, the UK was granted conditional access to these waters.  By 1972, however, overfishing outside of this limit had worsened and Iceland extended its exclusive zone to 50 miles and then three years later, to 200 miles. Both these moves led to more clashes between Icelandic trawlers and the Royal Navy escort ships, respectively dubbed the second and third Cod Wars.  Icelandic Coastguard vessels towed devices designed to cut the steel trawl wires (hawsers) of the British trawlers – and vessels from all sides were involved in deliberate collisions. Although these clashes were mainly bloodless, a British fisher was seriously injured when he was hit by a severed hawser and an Icelandic engineer died while repairing damage to a trawler that had clashed with a Royal Navy frigate.  In January 1976, British naval frigate HMS Andromeda collided with Thor, an Icelandic gunboat, which also sustained a hole in its hull. While British officials called the collision a “deliberate attack”, the Icelandic Coastguard accused the Andomeda of ramming Thor by overtaking and then changing course. Eventually NATO intervened and another agreement was reached in May 1976 over UK access and catch limits. This agreement gave 30 vessels access to Iceland’s waters for six months.  NATO’s involvement in the dispute had little to do with fisheries and a large amount to do with the Cold War. Iceland was a member of NATO, and therefore aligned to the US, with a substantial US military presence in Iceland at the time. Iceland believed that NATO should intervene in the dispute but it had up until that point resisted. Popular feeling against NATO grew in Iceland and the US became concerned that this strategically important island nation – which allowed control of the Greenland Iceland UK (GIUK) gap, an anti-submarine choke point – could leave NATO and worse, align itself with the Soviets.  Amid protests at the US military base in Iceland demanding the expulsion of the Americans, and growing calls from Icelandic politicians that they should leave NATO, the US put pressure on the British to concede in order to protect the NATO alliance. The agreement brought to an end more than 500 years of unrestricted British fishing in these waters.  The loss of these Atlantic fishing grounds cost 1,500 jobs in the home ports of the UK’s distant water fleet, concentrated around Scotland and the north-east of England, with many more jobs lost in shore-based support industries. This had a significant negative impact on the fishing communities in these areas.   The UK also established its own 200-mile limit in response to Iceland’s exclusion zone. These limits were eventually incorporated in the 1982 United Nations Convention on the Law of the Sea, giving similar rights to every sovereign nation. The creation of these “exclusive economic zones” (EEZ) was the first time that the international community had recognised that nations could own all of the resources that existed within the seas that surrounded them and exclude other nations from exploiting these resources.  The UK now owned the rights to the 200-mile zone around its islands, which contained some of the richest fishing grounds in Europe but up until this point the principle of “open seas” had existed, with Britain its most vocal champion. Fishing nations, had fished the high seas within 200 miles of their own and others coasts for centuries and now were restricted to their own. Britain’s Exclusive Economic Zone (EEZ), however, wasn’t that exclusive.  On joining the European Economic Community (the forerunner to the EU) in 1972, the UK had agreed to a policy of sharing access to its waters with all member states, and gaining access to the waters of other countries in return. The UN convention effectively gave the EEC one giant EEZ.  The UK government was willing to enter into the agreement as fisheries were one part of overall negotiations that would allow the UK to export goods and services to the European continent with significantly reduced trade barriers.  Although the fishing industry is of high local importance to fishing communities, it is relatively unimportant to the UK economy as a whole. In 2016, the UK fishing industry (which includes the catching sector and all associated industries) was valued at £1.6 billion, against £1.76 trillion for the UK economy as a whole – or just under 0.1%. The UK’s trade with the EU, both import and export, stands at £615 billion a year in comparison. In 1983, the Common Fisheries Policy was adopted, introducing management of European waters by giving each state a quota for what it could catch, based on a pre-determined percentage of total fishing opportunities. This was known as “relative stability” and was based on each country’s historic fishing activity before 1983, which still determines how quotas are allocated today.  The formula that the EEC adopted, based on historic catches, is one of most contentious parts of the CFP for the UK. Many fishers have stories of the years running up to 1983, where foreign vessels increased their fishing activity in UK waters in order to secure a larger share of these fish in perpetuity. Although there is little evidence to support these views, it demonstrates the level of distrust in both the system, and foreign fishers, from the outset. As a result, only 32% of fish caught in the UK EEZ today is caught by UK boats, with most of the remainder taken by vessels from other EU states, Norway and the Faroe Islands (who have also joined the CFP). Therefore, non-UK vessels catch the remaining 68%, about 700,000 tonnes, of fish a year in the UK EEZ.
In return, the UK fleet lands about 92,000 tonnes a year from other EU countries’ waters.  Joining the CFP did not cause a decline in UK fish landings. However, in its early days, it did nothing to stop it. Fish landings continued to decline – and along with this, the industry itself contracted, using improved technology to offset the decline in stocks. Through the 1980s and into the early part of this century the imbalance – enshrined in the relative stability measure of the CFP – has led to the view that the CFP doesn’t work in the UK’s interests. Rather it allows the rest of the EU to take advantage of the country’s fish stocks.  The CFP’s quota system, while credited for helping the industry survive (and even reverse the collapse in fish stocks), is now seen as burdensome and preventing further growth.  


      Read more:
      Brexit: what the UK fishing industry wants


 A recent academic analysis of the current performance of the CFP showed it was not improving the management of the fish stock resources in any of its 17 criteria and was actually making things worse in seven areas. For example, a 2013 reform of the CFP introduced the landing obligation, the so-called “discard ban”, that was designed to stop vessels discarding fish (bycatch) caught alongside the species they were targeting. Environmentalists, and campaigns backed by celebrities such as Hugh Fearnley-Whittingstall, have long voiced concerns over incidents of bycatch being dumped by fishers operating under the quota system. This policy is now seen as potentially disastrous by some representatives of Britain’s North Sea fishing fleet, as so many different types of fish live in the waters and bycatch is common and often unavoidable. They are concerned that boats would be forced to fill their holds with commercially worthless fish and return to port early. Or by exhausting their quota for some species early in the season, they would be forced to stay in port for the rest of the year, despite having quotas available for other species. Evidence given to the House of Lords suggests that this situation has not arisen as non-compliance and a lack of enforcement has undermined the discard ban.  When we interviewed fishers in north-east Scotland in 2018, we found many feared such blanket management across the entire EU would continue to damage their industry because it simply does not take into account the local environment that they work in. The depth of feeling among the UK fishing community was illustrated by the voting figures for the EU referendum in June 2016.  In Banff and Buchan, the constituency in Scotland containing Peterhead and Fraserburgh – the largest and third largest fishing ports in the UK respectively – 54% of people voted to leave the EU, with the size of the fishing industry given as the reason for this result. The result compared to 52% for the whole of the UK and just 38% for Scotland. A survey of members of the UK fishing industry before the vote indicated that 92.8% of correspondents believed that doing so would improve the UK fishing industry by some measure.  


      Read more:
      British fishermen want out of the EU – here's why


 But will Brexit really bring the fishing revival so many have promised and hoped for? British politicians have promised a renaissance in UK fishing after leaving the EU. A Fisheries Bill was launched by the environment secretary, Michael Gove, with an aim to “take back control of UK waters”. However, no definitive plan to remove the UK from the CFP in a transition deal has been made, nor has the industry been given any answers on future access for EU vessels, the apportionment of any new quota – if indeed the quota system remains as it is – the rules that they will be operating under, or even a date on which this will come into effect.  The UK government is seen by many in the fishing industry to be acting against their interests in pursuit of wider goals, for example by using the industry as a bargaining chip in wider UK trade negotiations with the EU. The fishing industry’s distrust of the government has a long tail: many believe they were sacrificed in 1973 by the then prime minister, Edward Heath, in order to secure access to the single market.  Ironically, despite the fishing industry’s support for Brexit and the popular campaign promises, our research suggests fishers don’t simply want to close British waters to European fleets. We interviewed people who were sympathetic to their fellow fishers from abroad and did not wish to see businesses and livelihoods lost. They favour a re-balancing of quotas over time to allow EU vessels to adapt to the change, with all vessels having to adhere to UK rules. This would avoid any situations similar to the Scallop War by ensuring that all vessels with a quota have to abide by local restrictions.  The EU is the main export market for UK fish and fisheries products accounting for 70% of UK fisheries exports by value. Valued at £1.3 billion, this trade far exceeds the £980m value of fish landed in the UK, due to the added value from the processing sector. Some of the remaining 30% of exports that go to countries outside of the EU are governed by trade agreements negotiated by the EU that reduce trade barriers. So the single market, and additional trade agreements, are crucial to the success of the UK fishing industry. This reliance on trade into the EU puts the industry in a position where unilaterally preventing access to UK waters would likely be met by reciprocal trade barriers and tariffs. This would increase the cost of their product, while reducing access to their biggest market. The question for the government, then, is how to balance a political issue against an economic one? The issue centres on the word “control”. If the UK has control of its waters that would simply mean that its government has the power to decide on anything from keeping fishing within UK waters purely for UK vessels, to remaining in or re-entering the CFP, or all points in between. Until the deals are negotiated and signed, the industry will remain in a limbo that has reopened old wounds and reignited distrust in the UK government."
"The UK’s efforts to develop facilities to remove carbon emissions from power stations took a step forward with news of a demonstrator project getting underway at the Drax plant in north Yorkshire. Where most electricity carbon capture projects have focused on coal-fired power, the Drax project is the first to capture carbon dioxide (CO₂) from a plant purely burning wood chips – or biomass, to use the industry jargon.  This so-called Bio Energy Carbon Capture and Storage (BECCS) demonstrator is only a pilot project; it covers just a tiny proportion of emissions from the 4GW plant and Drax has no plan yet for storing the captured gas. But coming after a decade in which various other UK carbon capture initiatives and government competitions have ended up scrapped, it is certainly progress.  Some specialists believe this technology has a bright future in the UK, envisaging big wood-fired power plants whose carbon emissions are prevented from returning to the atmosphere. Other countries are looking at it seriously, too, and Drax and its partners have been talking up the prospect of eventually achieving “negative emissions” at the plant in Yorkshire. But this is fundamentally misleading. Without wanting to reject carbon capture out of hand, it is time to get realistic about what can be achieved with this technology.   The logic of the negative emissions argument is that burning wood is “carbon neutral” because trees absorb CO₂ from the atmosphere in the first place, and you are simply releasing it back. When you combine this with a carbon capture facility, it is argued, you are therefore removing CO₂ from the atmosphere overall.    But this view considers the process of burning wood in isolation. It ignores, just as an example, a wider chain of activities including planting and harvesting the trees, converting the wood into chips and then shipping them to the power plant – not to mention storing and using the captured CO₂ once the wood has been burned.  There is also a misconception that burning wood produces only CO₂ – a BBC News reporter was saying as much the other day. But if this were the case, we would not need to separate CO₂ from other flue gases. Some of the carbon in the wood could become carbon monoxide, for instance, which, if not captured, would indirectly contribute to levels of greenhouse gases in the Earth’s atmosphere. The process also produces other noxious emissions, such as volatile organic compounds and oxides of nitrogen, which are responsible for acid rain. Too many people also tend to see wood as better than oil or coal because the amount of CO₂ produced by burning a given unit is much lower for wood. But this overlooks the fact that you get considerably more heat from burning a unit of oil or coal than from wood. In other words, you have to burn much more wood to produce the same amount of heat, so the carbon emissions are actually much more than they appear. This leads people to greatly underestimate the amount of land we will need for trees if biomass power is to become a much bigger part of the energy mix. The Drax plant alone uses more wood than the UK produces every year, for instance.  The blinkered thinking around carbon capture also goes way beyond biomass power plants. There are now 43 carbon capture facilities either operating or in development – ten in the US, followed by Canada and Norway. Very few are attached to power plants so far, with most instead removing CO₂ from oil fields or gas processing plants. But generous new subsidies in the likes of the US are making the industry optimistic about carbon capture in the power sector regardless of which feedstock is burned.  Across the board, there is the same tendency to ignore the carbon emissions in everything from coal/gas/oil extraction to CO₂ storage. We also hear very little about the solvents traditionally used to separate the CO₂ from the rest of the combustion gases. These amines are highly corrosive and bad for the environment, plus there are CO₂ emissions from producing them in the first place.  My point is not that we should be against carbon capture plants; the technology is much needed, and pilots like the one at Drax are important for possibly scaling up the process and measuring what is achievable. But when scientists conduct these measurements, they need to consider the complete chain to look at all of components involved – including, in the case of wood, the land used for the trees, and the consequences of deforestation.  We also need much more discussion and research into which solvents are the most environmentally friendly for gas separation: Drax claims to be using a new solvent with environmental benefits, so it will be interesting to see what the results look like down the line.  Clearly, our society needs energy. We would never be able to sustain ourselves if we eliminated fossil fuels completely. Capturing carbon dioxide emissions certainly has a role to play in the energy systems of the future, but it needs to be appraised in a way that looks at the whole picture.  The reality is that if the UK and EU are serious about being completely carbon neutral by 2050, it will have to use a mixture of methods and cut back more aggressively on the emissions being produced in the first place. This is always going to be more efficient than any attempts to put the genie back in the bottle afterwards. Regardless of what anyone says about technological solutions to the carbon problem, it is almost impossible to get away from this basic fact."
"A new expansion has added environmental challenges to Sid Meier’s Civilization VI, the latest in a popular series of strategy video games that has been running since the 1990s. The expansion – called Gathering Storm – adds new features to the game, most notably anthropogenic climate change and natural disasters.  The game involves developing a civilisation from its humble beginnings in the Stone Age to nowadays and beyond, while choosing from a vast array of technologies and cultural policies. As the game and the ages progress, your energy choices become increasingly important. Indeed, Gathering Storm is based on a simple model of global warming wherein CO₂ emissions from energy sources induce sea level rise, as well as more frequent and intense extreme weather events such as droughts and storms. In turn, these can have potentially devastating effects on your cities and units, pushing the player to think about different adaptation strategies such as flood barriers for coastal cities.  The game even progresses into a “future era”, where players are offered options like carbon capture and storage technologies or “seasteads” to house segments of the population. From early on, this new expansion compels players to think about some of the potential long-term consequences of actions that may offer short-term benefits. One example would be chopping down forests to accelerate production or convert land for other uses which, in the long run, renders a city more vulnerable to flooding and reduces the carbon sink capacity of your civilisation. When asked about whether Gathering Storm was somewhat of a political statement, the lead developer, Dennis Shirk, remained largely agnostic: “No, I don’t think that’s about making a political statement. We just like to have our gameplay reflect current science.” It is certainly true that the game does not coerce players into taking any particular pathway, yet it does include a “World Congress” in which climate or deforestation treaties and humanitarian aid can be ratified. We would also argue that the very inclusion of anthropogenic climate change and an associated system of incentives and punishments is inherently a political act. Moreover, in the social studies of science, what one considers to be “current science” has political ramifications. In the case of Gathering Storm, for example, in most scenarios a player could probably continue to be a “free rider” and rely solely on technological solutions. That is only possible because those technologies are known in advance and players are given virtually perfect information on the different stages of climate change and its effects. One of the consequences is that the game essentially eliminates the very uncertainty which is inherent to the “current science” on climate change and conveys a sense of technological optimism whereby innovations alone can sustain human prosperity. We are not suggesting that the developers are necessarily liable or even responsible for promoting these views. Rather we wish to illustrate how different depictions of the future can restrict or encourage certain courses of action. The developers could have chosen to make the effects of climate change and access to mitigating technologies more random (although we do not know how difficult that would be to implement in practice nor its effects on gameplay). In contrast to this incidentally optimistic outlook, there is an interesting Polish video game by the name of Frostpunk. Frostpunk is set in a dystopian alternate reality in which a volcanic event has triggered a colossal global ice age. The game’s primary scenario consists of surviving the winter – which gets incrementally colder as time progresses – in “New London”: a settlement of survivors clustered around a large coal-powered generator. The player must choose between a number of difficult policies and options to ensure the survival of the population. These include 24 hour shifts, child labour, corpse disposal strategies and, more drastically, whether to welcome refugees or refuse them entry.  While Frostpunk does not directly address the issue of anthropogenic climate change, it evokes extreme scientific scenarios (from the 1970s and 1980s) of global cooling and nuclear winters. The game also takes place in what we understand is Victorian Britain, epitomising the industrial revolution and the onset of the new geological era we now live in: the Anthropocene.  Both these games go a long way in engaging and educating their players on climate change, forcing them to deal with the kinds of political and ethical trade-offs that exist in real world decision-making. We highly encourage these innovations, not just in video games but more broadly in bridging the gap between science and the digital arts.  In the academic journal Environmental Communication, we argue that science and the humanities (including the arts) need to work together in the case of complex issues such as climate change, so as to better communicate scientific thinking and its political ramifications. Video games – as interactive and playful products – offer truly exceptional opportunities to do just that. We welcome these initiatives with open arms, so long as they remain responsible and stimulate critical thinking."
"The Arctic global seed vault has reached the milestone of having 1m varieties stored in its deep freeze. The new deposits are being made after unexpected flooding of its entrance tunnel in 2017 prompted an upgrade. Seeds from 60,000 crop varieties from across the world are being placed in the vault to back up those held in other seed banks.  The €9m (£7.5m) underground facility in the Norwegian archipelago of Svalbard opened in 2008 as a “failsafe” store. But the unexpectedly rapid pace of global heating led to melting of the permafrost that had encased it. Now, a €20m refurbishment by the Norwegian government means the vault is secure for the future and “absolutely watertight”, according to officials. The destruction of nature means vital diversity of crops and their wild relatives are being lost, at a time when the impact of the climate emergency means new varieties are needed to cope with changing weather and pests. Seed banks can also be destroyed by power loss and war, as happened in Aleppo, Syria, making the Svalbard vault crucial. Tuesday’s deposits, from 36 institutions, are the most diverse and include seeds of 27 wild plants from Prince Charles’s Highgrove estateas well as seeds of the candy roaster squash, which are being deposited by the Cherokee Nation in the US. Wild emmer wheat, known as the “mother of wheat” when it was discovered in 1906, is being deposited by Haifa University in Israel, alongside potato varieties from Peru and other crops from Mongolia, Morocco, Myanmar and New Zealand. Each sample contains roughly 500 seeds. The Svalbard vault, which is carved into solid rock, houses samples of about 1,050,000 crop varieties from 5,000 species. This represents two-fifths of the estimated 2.4m varieties in the world, and the vault has plenty of room to accommodate them. “Crop diversity is an essential basis of food production,” said Hannes Dempewolf, a scientist at Crop Trust, which operates the vault alongside the Nordic Genetic Resource Centre. “And the Svalbard vault is the essential backup facility for seed banks around the world, safeguarding the biodiversity they hold.” Many crop varieties have been lost, including 93% of fruit and vegetable varieties in the US. “The issue of some water intrusion in the entrance tunnel was certainly not foreseen during construction,” Dempewolf said. “No one thought summers would be so warm. The physicist Edward Teller tells the American Petroleum Institute (API) a 10% increase in CO2 will be sufficient to melt the icecap and submerge New York. “I think that this chemical contamination is more serious than most people tend to believe.” Lyndon Johnson’s President’s Science Advisory Committee states that “pollutants have altered on a global scale the carbon dioxide content of the air”, with effects that “could be deleterious from the point of view of human beings”. Summarising the findings, the head of the API warned the industry: “Time is running out.” Shell and BP begin funding scientific research in Britain this decade to examine climate impacts from greenhouse gases. A recently filed lawsuit claims Exxon scientists told management in 1977 there was an “overwhelming” consensus that fossil fuels were responsible for atmospheric carbon dioxide increases. An internal Exxon memo warns “it is distinctly possible” that CO2 emissions from the company’s 50-year plan “will later produce effects which will indeed be catastrophic (at least for a substantial fraction of the Earth’s population)”. The Nasa scientist James Hansen testifies to the US Senate that “the greenhouse effect has been detected, and it is changing our climate now”. In the US presidential campaign, George Bush Sr says: “Those who think we are powerless to do anything about the greenhouse effect forget about the White House effect … As president, I intend to do something about it.” A confidential report prepared for Shell’s environmental conservation committee finds CO2 could raise temperatures by 1C to 2C over the next 40 years with changes that may be “the greatest in recorded history”. It urges rapid action by the energy industry. “By the time the global warming becomes detectable it could be too late to take effective countermeasures to reduce the effects or even stabilise the situation,” it states. Exxon, Shell, BP and other fossil fuel companies establish the Global Climate Coalition (GCC), a lobbying group that challenges the science on global warming and delays action to reduce emissions. Exxon funds two researchers, Dr Fred Seitz and Dr Fred Singer, who dispute the mainstream consensus on climate science. Seitz and Singer were previously paid by the tobacco industry and questioned the hazards of smoking. Singer, who has denied being on the payroll of the tobacco or energy industry, has said his financial relationships do not influence his research. Shell’s public information film Climate of Concern acknowledges there is a “possibility of change faster than at any time since the end of the ice age, change too fast, perhaps, for life to adapt without severe dislocation”. At the Rio Earth summit, countries sign up to the world’s first international agreement to stabilise greenhouse gases and prevent dangerous manmade interference with the climate system. This establishes the UN framework convention on climate change. Bush Sr says: “The US fully intends to be the pre-eminent world leader in protecting the global environment.” Two month’s before the Kyoto climate conference, Mobil (later merged with Exxon) takes out an ad in The New York Times titled Reset the Alarm, which says: “Let’s face it: the science of climate change is too uncertain to mandate a plan of action that could plunge economies into turmoil.” The US refuses to ratify the Kyoto protocol after intense opposition from oil companies and the GCC. The US senator Jim Inhofe, whose main donors are in the oil and gas industry, leads the “Climategate” misinformation attack on scientists on the opening day of the crucial UN climate conference in Copenhagen, which ends in disarray. A study by Richard Heede, published in the journal Climatic Change, reveals 90 companies are responsible for producing two-thirds of the carbon that has entered the atmosphere since the start of the industrial age in the mid-18th century. The API removes a claim on its website that the human contribution to climate change is “uncertain”, after an outcry. Exxon, Chevron and BP each donate at least $500,000 for the inauguration of Donald Trump as president. Mohammed Barkindo, secretary general of Opec, which represents Saudi Arabia, Kuwait, Algeria, Iran and several other oil states, says climate campaigners are the biggest threat to the industry and claims they are misleading the public with unscientific warnings about global warming. Jonathan Watts “A major upgrade was the only right thing to do and the Norwegian government has certainly put the resources up to make sure that it is absolutely watertight now.” Hege Njaa Aschim, a spokeswoman for the government, which owns the vault, said: “The entrance tunnel and the upgrade will secure the seed vault for the future.” In 2017, she told the Guardian: “A lot of water went into the start of the tunnel … The vault was supposed to [operate] without the help of humans.” No water reached the seed vaults. The 130-metre entrance tunnel has been fully waterproofed and the cooling equipment that keeps the vault at -18C moved to a new service building, so heat from the machinery can be released outside. The vault is 130 metres above sea level and designed for a “virtually infinite lifetime”. “It is always dangerous to talk about something being completely failsafe and impregnable,” Dempewolf said. “In 20, 30, 40 years down the line, we will continue to monitor the situation to see whether any other upgrades are necessary.”"
"The legal case against Heathrow airport expansion pitted the need to tackle the climate crisis against economic arguments on behalf of UK plc, and for the first time in any major infrastructure project, the planet won. John Holland-Kaye, the chief executive of the airport, made a last-minute PR push on Radio 4’s Today programme on the eve of the appeal court decision, claiming expansion was the “key to delivering the prime minister’s vision of a global Britain” after Brexit.  But the impact on the climate of emissions from a third runway’s 260,000 extra flights a year ultimately held sway in the appeal court. The environmental groups Friends of the Earth and Plan B argued the expansion would jeopardise the UK’s ability to make the very deep reductions in greenhouse gas emissions that are necessary to stop global warming from causing catastrophic and irreversible impacts. What just happened? For the first time judges have said that plans for a major infrastructure project are illegal because they breach the UK's commitments to reduce greenhouse gas emissions to tackle the climate crisis. This is a groundbreaking legal decision that could effect future infrastructure developments and puts the UK’s commitment to cut emission to net zero by 2050 at the forefront of future policymaking. What will happen next? The government has been told by the court of appeal to declare its decision to allow Heathrow airport expansion - contained in its airline national policy statement - illegal. Ministers have two choices now. They can withdraw the whole policy statement or try to amend it to make it compatible with the UK’s commitments to reduce greenhouse gas emissions to net zero by 2050.  Will the runway be built? If the government can prove that expanding Heathrow is compatible with its commitments under the Paris agreement to very radically reduce greenhouse gas emissions, the runway may go ahead. But the prime minister has always been against the third runway, and the government has told the court it will not be appealing against its decision on Thursday.  There now hangs a very big question mark over whether the bulldozers will ever start work on the runway. They argued that the then transport secretary, Chris Grayling, acted unlawfully when he agreed to the expansion of Heathrow in the government’s airports national policy statement in June 2018 – he had failed to take into account the UK’s international obligations under the Paris agreement, and its own domestic law. The judges’ ruling in favour of the campaign groups puts both the need for the UK to make significant reductions in emissions and the requirements of the Paris agreement at the forefront of policymaking. It is a judgment that could have lasting implications for future infrastructure projects. The ruling strikes a warning note that the climate crisis means it can no longer be business as usual. In future, for economic and business decisions to have both legal standing and political credibility they must take into account the impact on global heating. This ruling comes in the year the UK’s trustworthiness on these issues comes under the global spotlight, as the government hosts the key UN international climate talks, Cop26, in Glasgow in November. In the long history of Heathrow’s planned expansion, during which both Labour and Conservative governments have backed a third runway, those protesting that its environmental impact is just too great have seemed to be the underdogs. Arguments about increased air pollution and noise pollution, and the severe negative consequences of, in effect, tacking a new airport with the capacity of Gatwick on to Heathrow, have been met by politicians and business leaders contending that expansion is vital to the UK’s economic prosperity. Nothing, Grayling said last year, must stop this “massive economic boost”. Now it seems the impact of human activity on the planet has outweighed the economic holy grail of Heathrow expansion. But campaigners – some of whom have been fighting for decades to stop the runway – will hold in the back of their minds the words of another Conservative prime minister, David Cameron, even as they celebrate the appeal court ruling. Cameron promised them in 2010: “No ifs, no buts, no new runway,” only to commission a report which paved the way for the runway to be approved. This latest legal challenge – at a time of acute and growing public concern about climate change – might just prove decisive for Heathrow Airport Ltd, and political supporters of its project. The make-up of parliament has also changed considerably since the Commons voted in favour of the third runway in 2018. The judges did not cancel the expansion, but they have thrown a big, perhaps insurmountable, obstacle in its path – and given Boris Johnson, who as a backbench MP threatened to lie down in front of the bulldozers to stop a third runway, a compelling reason to scrap it."
"**Scotland's local Covid-19 alert levels are to remain unchanged, with Nicola Sturgeon saying the government must follow a ""cautious approach"".**
Where each local authority area sits in the five-level system of measures is reviewed every Tuesday.
The first minister said restrictions were ""having an impact"", but that no changes would be made this week.
She said case numbers may be declining across Scotland, but it was important to ""keep the virus at bay"".
And Ms Sturgeon said she was ""hopeful"" that a deal will be agreed later on Tuesday to ease curbs over Christmas in a ""temporary and limited"" way to allow more people to meet up.
A group of 11 council areas in the west of the central belt are to remain in level four - the top tier of curbs - until 11 December.
East Lothian moved down to level two as of Tuesday morning, but plans for Midlothian to make the same move were scrapped amid concerns about a rise in infections.
Ms Sturgeon said she was ""hopeful"" that both Dumfries and Galloway and Argyll and Bute could drop to level one in the coming weeks.
She said there had been rises in case numbers in Aberdeen and Aberdeenshire, but that these had been linked to ""specific outbreaks"" meaning they would not trigger an increase in the local levels.
And the first minister said officials were monitoring Clackmannanshire and Perth and Kinross, currently in level three, ""particularly carefully"" in light of a recent increase in cases.
Scotland moved to a five-level system of localised restrictions earlier in November, with the aim of suppressing the virus in high-prevalence areas but allowing more freedom in places with fewer cases.
Significant changes have been phased in over the past week, with 11 councils around Glasgow and west and central Scotland moving to level four, the top tier of measures.
Ms Sturgeon said this was a bid to ""make sure cases in these areas fall more markedly"", saying people there should ""stay at home as much as possible"".
She said: ""The latest data shows that across the country as a whole and within most local authorities, the restrictions in place are having an impact.
""The number of new cases across the country has stabilised in recent weeks, and we have grounds for cautious optimism that numbers may be declining.
""There is also evidence that admissions to hospitals and intensive care units are declining too, although these do tend to fluctuate on a day to day basis. However the national picture - which is positive - masks some regional variations.""
Midlothian was originally meant to move to level two on Tuesday morning, but this was put on hold after fears about rising cases in the area.
Ms Sturgeon said the number of cases had risen from 61 per 100,000 people to 97, saying that while this was ""well below the national average"" the 50% increase was a concern.
Local council leader Derek Milligan said the move would be ""absolutely devastating"" for local businesses, and Scottish Labour leader Richard Leonard pressed the first minister about the ""eleventh hour decision"".
He said: ""Decisions like this one today need to be a genuine co-production involving the locally elected council and the local business community, workers and their trade unions.
""In Midlothian they have been told for the last week that they would move to level two, including as recently as last Friday. It was only at 10:45 yesterday morning they were told that they may not. And only at 16:29, they were told by the deputy first minister that they definitely were not moving to level two.
""So that means as a result, stock ordered by businesses will go to waste, investment in health and safety measures will lie idle, and staff re-hired will once again be laid off.""
The first minister said she knew the decision would be ""disappointing for individuals and businesses"" in Midlothian, but said it was better than risking having to move the area back up a level in a week's time.
Ms Sturgeon is due to take part in further talks later with ministers from the UK government and the other devolved administrations about easing measures at Christmas.
She told MSPs that she hoped a ""common framework"" would be agreed to allow more people to meet up over the festive period.
However she warned that this would be ""temporary and limited"", and said ""people should use any flexibility carefully and only if they believe it right and necessary for their personal circumstances"".
The Scottish Greens have called for the government to reveal ""how much of an increase in cases"" it expects after loosening the rules, with MSP Alison Johnstone saying ""it's not good enough for the first minister to fall back on personal responsibility"".
Meanwhile Scottish Lib Dem MSP Alex Cole-Hamilton asked Ms Sturgeon if she backed the ""vigilante action"" taken by the SNP's Westminster leader Ian Blackford when he accused a photographer of breaking travel rules.
The first minister praised the MP's ""grace and dignity"" for apologising for ""doing something he recognised he should not have done"".
And Scottish Conservative group leader Ruth Davidson pressed Ms Sturgeon for details about the rollout of Covid-19 vaccines, with the first minister saying there were still details to be ironed out about the certification of different immunisations and how they will be supplied."
"Melting glaciers, rising sea levels, global warming and violent storms: the effects of climate change are well documented. But a growing weather trend that has caused much concern is storm clustering – when three (sometimes more) hurricanes or typhoons group together in a short space of time, gathering strength and unleashing even greater devastation. The development of a tropical depression – a low pressure area with thunderstorms and winds below 39mph – to a tropical storm that attains hurricane strength in less than six hours, shows how quickly these things can intensify. But increased frequency is also a trend, as storms follow each other in quick succession. Those who question the existence of climate change should look at the global hurricane history, or even the hurricane pattern in their own country. If we look at these storms, patterns of increasing intensity and frequency clearly demonstrate how climate change is having a direct impact on the way hurricanes behave. In developed countries coastal residents in affected areas are keenly aware of these hazards and respond well during emergencies by liaising with local agencies and heading to designated shelters during evacuations. But this is not the case in developing and underdeveloped countries, although basic response awareness exists through devastating experience and a degree of public information. Thanks to advances in hurricane forecasting and hindcasting techniques, situations like the Galveston hurricane in 1900, which struck the Texas coast without any official warnings, are happily a thing of the past. The weather prediction models of the National Hurricane Center and the European Centre for Medium Range Weather Forecasting are able to plot the likelihood of impending hurricane paths – known as the “cone of uncertainty” – five days in advance, and are generally accurate. But the real issue is how prepared we are around the world for the increasing frequency of hurricanes and their terrifying “gang” version, hurricane trios. This violent onslaught of hurricane-strength storms batters communities and destroys buildings and infrastructure from the US to the Caribbean to South-East Asia. But should communities on the coast stay and defend, or retreat altogether? Hurricanes hammered the Atlantic from 2016 and 2018, including the Category 5 Matthew (2016), the Harvey-Irma-Maria trio (2017), which registered Category 4, 5 and 4 respectively, and Category 4 Florence and Michael (2018). This not only revealed the rising trend in intensity and frequency, but also alerted the world to the phenomena of clustering.  Critically, predicting the path of a hurricane depends on forecasting the dynamics of its intensity. Understanding the factors that contribute to the sudden changes in the strength (or weakening) of a hurricane is crucial. Changes in wind direction, interaction with the land at the coast, and ocean temperature and depth all play their part in altering the intensity of a hurricane that is highly sensitive to even slight changes. In general, the accuracy of predicting the way a hurricane intensifies and then re-intensifies in less than 24 hours is more challenging than predicting its path. But these dynamics are the underlying factors which compound the threat of hurricane frequency. These dynamics are also capable of further altering storm surge characteristics by triggering coastal and inland flooding – such as abnormal rises in water levels – which often result in shocking devastation. Hurricane Michael in 2018 was the perfect example of the importance of predicting how rapidly a hurricane has intensified before it hits the coast, in this case Florida. The predicted track of the storm was almost accurate but its intensity was more difficult to assess. The National Hurricane Center forecasted Michael’s path by issuing a five-day cone of uncertainty advising of sustained winds of 65mph. However, the sudden change in the storm’s dynamics changed a Category 1 hurricane to a Category 4 with winds of 155mph. This underscores the uncertain and variable nature of hurricane prediction. Despite these emerging and changing weather-related risks, residential and public  buildings are still going up on affected coastal areas. Recent research in China identified a tsunami that swept away the present-day coastal province of Guangdong in 1076AD. It means storm-related surges have been documented in the region for more than 1,000 years – yet still building and expansion goes on heedless of the risk. This is almost the same situation for all vulnerable coastal cities. For instance, Florida has hundreds of thousands of coastal residents living in Low Elevation Coastal Zones – land that is less than ten metres above sea level and within 200km of the coast – but yet again construction there continues despite the threat of hurricanes every season. Developers are already conceiving storm-resilient buildings that can withstand winds of at least 200mph – a Category 5 hurricane. But it’s unlikely many have considered the compounded stress effect on structures having to continuously withstand hurricane force winds more frequently and in quick succession. Building massive sea defences along vulnerable coastlines is practically impossible and isn’t a permanent solution to increasing coastal storm hazards. There is no point in risking lives by remaining, as storm clusters can be unpredictable. It is simply too dangerous, so evacuation is the only option. However, when it comes to coastal assets and investments, defending in a more appropriate and sensible way is required.  Some coastal cities are planning ahead. A recent development of extensive parks in Boston, US, aims to protect the urban shoreline infrastructure from flooding. And a 2009 study revealed the effectiveness of mangrove planting in coastal areas of India to protect the shoreline and reduce cyclone damage. But more practical solutions are needed, especially in more vulnerable developing regions, because cluster storms are not going away any time soon."
"
Now we know why they like global warming so much – it’s hot.

Excerpts below of the original story here: Porn surfing rampant at U.S. science foundation
Jim McElhatton
EXCLUSIVE:
Employee misconduct investigations, often involving workers accessing pornography from their government computers, grew sixfold last year inside the taxpayer-funded foundation that doles out billions of dollars of scientific research grants, according to budget documents and other records obtained by The Washington Times.
The problems at the National Science Foundation (NSF) were so pervasive they swamped the agency’s inspector general and forced the internal watchdog to cut back on its primary mission of investigating grant fraud and recovering misspent tax dollars.
“To manage this dramatic increase without an increase in staff required us to significantly reduce our efforts to investigate grant fraud,” the inspector general recently told Congress in a budget request. “We anticipate a significant decline in investigative recoveries and prosecutions in coming years as a direct result.”
The budget request doesn’t state the nature or number of the misconduct cases, but records obtained by The Times through the Freedom of Information Act laid bare the extent of the well-publicized porn problem inside the government-backed foundation.
For instance, one senior executive spent at least 331 days looking at pornography on his government computer and chatting online with nude or partially clad women without being detected, the records show.
When finally caught, the NSF official retired. He even offered, among other explanations, a humanitarian defense, suggesting that he frequented the porn sites to provide a living to the poor overseas women. Investigators put the cost to taxpayers of the senior official’s porn surfing at between $13,800 and about $58,000.
Read the rest of the article at the Washington Times here
Have a friend or acquaintance that works at NSF? Send them an Ecard here
h/t to Charles the moderator


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e932bb015',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The G20 group of the world’s wealthiest nations have agreed to collectively sound the alarm over the threat to the financial system posed by the climate emergency. Overcoming objections from Donald Trump’s US administration, G20 finance ministers and central bank governors meeting in Saudi Arabia over the weekend agreed to issue their first communique with references to climate change since the beginning of the Trump era, according to reports from Reuters. China produces the most heat-trapping pollution, followed by the US. But historically, the US has contributed more carbon dioxide to the atmosphere than any other nation. The US also has high emissions per capita, compared to other developed countries. And Americans buy products made in China, therefore supporting China's carbon footprint.  Sources told the news agency that the statement of priorities included the importance of examining the implications of global heating for financial stability, as part of the work of the G20’s Financial Stability Board, the steering group for international banking industry rules. The language represented a compromise to overcome opposition from US officials at the first major meeting of Saudi Arabia’s year-long presidency of the G20, according to the sources. An attempt to include references to the downside risks for global growth posed by the climate crisis was dropped. Concerns about the economic damage from rising global temperatures and extreme weather events have risen up the agenda among world leaders, central bankers and financiers in recent years. The financial system continues to fund activities that are inconsistent with meeting climate targets, paving the way for trillions of pounds of financial losses in the future and catastrophic environmental consequences should the world economy fail to adapt. The meetings in Riyadh were attended by Mark Carney, who has driven the climate emergency up the agenda among world leaders and financial regulators to stake a legacy at the Bank of England before he stands down as governor next month. The new chancellor, Rishi Sunak, stayed in London to continue preparing for next month’s budget, instead sending a senior civil servant from the Treasury. Reuters reported that the communique issued at the end of the meetings in the oil-rich Gulf state would be the first to include references to climate change since Trump became president in 2017. The International Monetary Fund included climate-related disasters in a list of the risks facing a highly fragile recovery in the global economy this year. However, the increasing focus comes as US officials resist naming global heating as an economic risk, following Trump’s move at the outset of his presidency to withdraw the world’s largest economy from the Paris climate accords. • This article was amended on 26 February 2020. The communique was the first during the Trump era to mention climate change, not the first since 1999 when the G20 was founded, as an earlier version had said. "
"**Here are five things you need to know about the coronavirus pandemic this Tuesday evening. We'll have another update for you tomorrow morning.**
Up to three households will be able to mix indoors during a five-day Christmas period of 23-27 December, under a plan agreed by the leaders of England, Scotland, Wales and Northern Ireland.
It comes after agreement was reached at a Cobra meeting bringing together the UK government and the devolved administrations.
People will be able to mix in homes but not pubs or restaurants.
Earlier, Transport Secretary Grant Shapps warned Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers.
Wondering how Christmas might look this year? Here are a few things that may be different.
More than one in five secondary school pupils in England missed school last week, figures out today show.
The weekly data from the Department for Education shows overall attendance was down to 83% of pupils, with almost 900,000 children out of school because of either testing positive for coronavirus or coming into contact with a positive case.
The DfE says keeping schools open is a ""priority"". But Mary Bousted, co-leader of the National Education Union said the situation had ""reached crisis point"".
The rules for self-isolating mean pupils have to stay home for at least 14 days. For more on the rules for self-isolating click here.
Latest figures show there were almost 14,000 deaths in the week ending 13 November, with 2,838 of these involving Covid - 600 more than the previous week.
Data compiled by the Office for National Statistics show north-west England and Yorkshire were the worst hit for excess deaths - both regions were more than a third above expected levels.
By comparison, the number in south-east England was 2% above the five-year average.
But there is hope that the rise may soon start slowing as the daily figures show deaths are not rising as quickly as they were.
Click here if you want to know how many cases there have been in your area.
Organised crime groups nearly got away with benefit scams of as much as Â£1bn as they attempted to take advantage of looser benefit rules during the pandemic, BBC News has discovered.
The fraudsters were trying to exploit looser rules introduced to cope with a surge of universal credit claims.
Before the scam was spotted, officials unwittingly confirmed thousands of stolen identities. It was foiled when a junior civil servant working with High Street banks noticed dozens of claims for universal credit had been made asking for money to be paid into the same bank account.
BBC News has asked the Department for Work and Pensions for a response.
The etiquette of the video call has been a steep learning curve for some. Director Tristram Shapeero has had to apologise for failing to hit the mute button while commenting on an actor's ""tiny apartment"".
Shapeero, who has worked on Brooklyn Nine-Nine and Never Have I Ever among others, was auditioning Euphoria actor Lukas Gage via Zoom when he was heard saying: ""These people live in these tiny apartments. Like I'm looking at his, you know, background and he's got his TV... ""
Gage then interjects: ""I know it's a [rubbish] apartment that's why [you should] give me this job so I can get a better one.""
Get a longer daily news briefing from the BBC in your inbox, each weekday morning, by signing up here.
You can find more information, advice and guides on our coronavirus page.
And we've had a look at what England's new lockdown rules will be.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"
Share this...FacebookTwitterJosh weighs in on the sea level debate…

Thanks for making my day, Josh!  See all of Josh’s cartoons HERE.
Share this...FacebookTwitter "
"It’s not news to observe that as soon as anyone mentions climate change policy in Australia, madness generally follows. A fresh outbreak of stupidity in the political debate has been triggered by Labor’s decision last week to sign up to a net zero target by 2050. Given the madness, let’s look at net zero, and establish some basic facts.  Achieving net zero emissions by mid-century is certainly a challenging task, requiring the transformation of Australia’s carbon-intensive economy. But increasingly, net zero is not a controversial ambition. More than 70 countries and 398 cities say they have adopted a net zero position. Every Australian state has signed up to net zero emissions by 2050, and these commitments are expressed either as targets or aspirational goals. Net zero has also been adopted by business groups in Australia who only a few years ago opposed Labor’s carbon pricing scheme. The Australian Climate Roundtable, which includes the Australian Industry Group, the Business Council of Australia, the ACTU, the National Farmers’ Federation and the Australian Council of Social Service, issued a statement late last year supporting policies requiring “deep global emissions reductions, with most countries including Australia eventually reducing net greenhouse gas emissions to zero or below”. The BCA, which represents Australia’s biggest companies, says Australia should legislate a target of net zero emissions by 2050. Just for context, this same organisation characterised Labor’s 45% reduction target by 2030 – the opposition’s policy for the last election – as “economy wrecking”. The point here is: attitudes are shifting, and rapidly. Because the government is speaking out both sides of its mouth, the position is confusing. Let’s step through the sequence. Both the trade minister, Simon Birmingham, and the emissions reduction minister, Angus Taylor, have acknowledged over the past couple of weeks that signatories to the Paris agreement (and that’s Australia) have already signed on to achieving carbon neutrality by mid-century, or not long after. As Taylor told Sky News on Monday: “There is that targeting built into the Paris agreement where the world has agreed to get to net zero emissions in the second half of the century. That’s already there.” And Birmingham, on 10 February: “In signing onto the Paris agreement, Australia has committed to a net zero target for the world by the second half of this century, and we have to then work towards that, and we do that in the bite-size pieces of what we can achieve to 2030.” But while acknowledging the commitment the government has already made for “the world” (and Australia was both a Paris signatory, and part of “the world”, last time I looked) – the government then also seems to suggest it hasn’t committed to anything, and won’t in the absence of a plan to get there – a plan it is yet to specify. Very good question. Last Friday the finance minister, Mathias Cormann, confirmed the government “will be finalising a longer-term target in time for Cop26” (which is the UN-led climate talks scheduled for the end of this year) but Taylor’s language is different. The emissions reduction minister talks about “a long-term strategy” rather than a target, which is highly suggestive that the government won’t adopt a formal target despite having already implicitly signed up to a net zero objective by signing and ratifying Paris. But just to be clear: nobody significant in the government is ruling out adopting a target over the course of the year. I’m sorry if your head is hurting. Bear with me. When the emissions reduction minister was asked about his position on targets on Monday, Taylor said the government would be “focusing on technology” – which translates as the government identifying new technologies than can help Australia’s export industries reduce emissions “without job destruction”. That language doesn’t rule out adopting a target, but suggests a target isn’t that likely. At the risk of making your head explode, Taylor disdained adopting a target in the same interview where he acknowledged that his own government had, in fact, voluntarily signed an international emissions reduction agreement with a net zero objective. Despite this obvious inconvenience, Taylor counselled: “You can’t pursue a target like this without clarity for what it means for Australia.” The government is also arguing at the moment it will not sign up to net zero without knowing what the costs are. Taylor indicated on the ABC on Monday that some work was “going on” to examine these questions – although it was entirely unclear what that work was, or whether it would also examine the cost of inaction. When some other work was put to the minister on Monday, work undertaken by the CSIRO, Taylor didn’t seem that interested. He noted he’d seen “lots of modelling” during the last election. It’s also passing strange that the government has been very attentive to the (absent) costs of Labor’s national commitment to a net zero target, but has not – at least in my hearing – pursued the premiers over the cost of their 2050 commitments. A recent bilateral energy and climate change agreement between the Morrison and Berejiklian governments expressly referenced the state’s “economy-wide target of net-zero emissions by 2050” without any criticism from Canberra."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
The good news: there’s new and exciting opportunities opening themselves to us.The bad news; some people are hilariously unquestioning.

It has been an even more entertaining than usual couple of days in the alarmosphere. I’d been traveling the last week, doing TV station work and station surveys. While on the road I discovered through an email that I was the subject of a YouTube Video called “Climate Crock of the Week”.
The video was about my surfacestations.org project and was titled “What’s up with Watts?”. It was sad and funny at the same time, and as is typically the case with our old friends it was directed at me personally, far more than it tried substance. Equally typically, and sadly, what substance it tried turned out to be wrong. I continued on my travels, my friend Dr. Roger Pielke Sr. posted an opinion on it last week to address some of the issues.
Little did I know bizarro land awaited upon my return home.
Sitting down Saturday night, to watch the video again, detecting through its exquisite subtleties and nuance, I couldn’t help but laugh, because once again I noticed that everything reported in it was just wrong.
In fact, it probably was the worst job of fact-finding I had ever seen, which as WUWT readers know, is a bold assessment. I’ve been involved in broadcast TV news for 25 years, and have seen some really bad work from greenhorns fresh out of reporters school. This video reminded me of those. It was as if whoever put it together had never researched it, but just strung together a bunch of graphics, video, photos, and a monotone voice-over track with ad hominems liberally sprinkled for seasoning. I figured it was probably just an overzealous college student out to save the world and this was some college project. It had that air of  radical burningman quality about it.
Curiosity piqued, I inquired into just who is this climate Solon? To my surprise, he turned out to be an “independent film producer” working out of his house in Midland, MI under the name “Greenman Studio”, one Peter Sinclair, a proud graduate of Al Gore’s Climate Camp. I still figured him to be a kid and imagined his mom was yelling down into the basement “Peter that’s too loud, turn it down!”.
I also wondered if it was the same “Green Man” that had once prompted surfacestations volunteer Gary Boden to create this nifty patch:

This came about because my now defunct local “Alternate Weekly” had a ghost writer named “green man” who penned an unintentionally (I think) hilarious editorial about me and the www.surfacestations.org project back in 2007 in which he wrote the famous line:
“The Reverend Anthony WTF Watts and his screeching mercury monkeys…”
…in response to our daring to survey the weather stations nationwide. The “mercury” is reference to thermometers.
What was funny is that in my original story, one of my commenters posted a silly comment about well, “green stuff” and the editor of the local “Alternate Weekly” went ballistic and demanded I remove it  and gave me a stern lecture on libel. I was happy to comply not out of legal obligation but courtesy and deleted the comment.
Is this Green Man the same guy? Inquiring minds want to know.
OK back to the present. I checked my email for some correspondence from Mr. Sinclair for the past week and found none, and looked back even further to see if he had contacted me about the surfacestations project weeks before in email or in my letters pile. I found nothing and was surprised that he had made a video using my work without at least a basic request or notice.  Normally when somebody wants to publish something in another media type (that is not a blog or webpage) from the surfacestations project or my blog, they contact me and ask permission to use the items. The word normal, however, upon scrutiny really doesn’t apply here.
I’ve gotten dozens of such requests from magazines, newsletters, book publishers, and TV stations. So far, I’ve never said no to any request for such materials or copyright waivers. I’ve filled out lots of forms granting my copyright waiver for the legally skittish that need more than an email or “sure, go ahead” over the phone.
click for PDF
 
But, in the video Mr. Sinclair produced and posted on YouTube, I noticed that he did in fact use photographs and graphics from my published book “Is The U.S. Surface Temperature Record Reliable?”.  I hold the copyright on this book. The notice for copyright is in the inside front cover.  © 2009 Surfacestations.org  All rights reserved, including the right to reproduce this report or portions thereof in any form.  ISBN 13: 978-1-934791-29-5  and ISBN 10: 1-934791-26-6.
There was also a Warner Brothers video clip from the movie “Anchorman” with a segment about the incompetent TV weatherman which I assume was added to portray me in my chosen career, and amazingly (and most amusingly) there was another video clip from the movie “The Adventures of Buckaroo Banzai” which is a campy sendup of “War of the Worlds”. Interestingly in  the credits, and I know this because I happened to watch the movie about two weeks before on Showtime, there is a “John Van Vliet” listed in the credits. It made me wonder if it is the same John Van Vliet that created the “opentemp” program launched just a couple of months after I first started the surfacestations project in an attempt to derail it early on. He made the mistake of using incomplete data. More on incomplete data later.
I noted that neither clip was from the trailers you could find on YouTube and were of high quality, so maybe they were cribbed from a DVD or perhaps an Apple video download, since I recognized from the editing effects that Mr. Sinclair owns a Macintosh. WB has some pretty stringent clip licensing requirements, which I know from doing TV news and a reporter wanting once to use part of a film from WB in a special news report. WB wanted our TV station to pay, but the cost was sky high for our small TV station. They finally whittled it down to something we could afford.
Doing a little more research, I found that Mr. Sinclair does a series of animated online greeting cards, which you can see here: http://www.care2.com/ecards/bio/1023
I thought this one was funny: http://www.care2.com/send/card/0840
The description portrayed him as a pretty nice guy with an alternate minded view of the world like a lot of college students have. He is not a college student, though he has a son who is of college age, a nice Ron Paul supporter, I am told from someone who has met him. His rather conservative son, contrasts the rather left-wing eco-activist ad hominem and rhetorically unrestrained father(see here). It is almost humorous greeting card-worthy, this role reversal.
But since he had used that © symbol, Mr. Sinclair demonstrated awareness of copyright protections, having availed himself of them, e.g., here, right below his own artwork.
With knowledge of this and ad hominem attacks made on me personally, I reasonably presumed his copyright violation on my part was likely intentional. I also figured that this might be a teachable moment, as I was still thinking this is a kid just out of college since there seems to be no business website for Greenman studio in operation yet, it is still “under construction”.
http://www.greenmanstudio.com/
And, I mused, by bringing the copyright issue to his attention, I’d probably be doing him a favor, since I surmised he’d be at risk for using the film clips. I figured anybody working a business out of a house without an operating web page probably can’t afford licensing fees. No deep pockets there. I certainly have no personal beef with Mr. Sinclair, it is just the copyright issue.
But my copyright had been ignored, with evidence that Mr. Sinclair as a publisher himself using the © symbol understands copyrights, and WB’s copyright also looked like it also had been ignored. And well, lets face it, he got the facts wrong about the project and never contacted or interviewed me to get any facts from my side (more on that later). So it could hardly be defined as “journalism” and the protections that such enterprise affords for “fair use”. So I filled out the form for copyright issues on YouTube, and pressed enter.
What I expected to happen is that I’d get an angry email or blog comment from the guy, I’d suggest to him (privately) to make a couple of modifications, grant him a copyright for the factual graphics from the surfacestations project, and tell him to put his video back up on the web. End of story, lesson learned.
What I didn’t expect was the alarmosphere going into berserk overdrive.
After all, this was not yet a “weekday” which it increasingly seems to be what we call those periods when our friends lapse into said mode. It turns out that YouTube put my name and the surfacestations.org URL up on the video pane for the former video, made me a target for hatred by the “scream first, ask questions later” types.
The first hint of this started on Sunday when I got a comment on my blog. The commenter, who obviously didn’t know the difference between copyright law and constitutional law wanted to know why I had “denied free speech” to Mr. Sinclair. Of course, “free speech” protections involve state infringement and,as powerful as our friends do apparently believe I have become, neither am I the state nor was the state involved here, so the angst was yet again rather misplaced. Regardless, I also thought this a pretty odd comment. Since Mr. Sinclair still hadn’t contacted me, I paid no attention to it.
Then I began receiving more odd comments, and I’m thinking; “why are these people making a private copyright dispute their personal business?”
Here’s sampling of  a  few comments I got that never made WUWT:
“Watts you are a coward chickesh** no good dumba** weatherman hiding behind a law that you’ve irrationally applied”
“You can’t handle the TRUTH, if I were Jack Nicholson I’d kick your a**”
“Wattsup, you and your stupid picture book project are toast!”
I even got comments from “Omar” in Finland:
“Looks like your attempt to smother and censor information has fired back badly on you Mr Watts: Do you have – how you say – the cahones to explain yourself? I think not. You appear to be a child coward man.”
Censoring huh? And around the alarmosphere all sorts of curious accusations of censorship — again, with the long arm of the state nowhere to be found, this seemed to be a variant of the Tim Robbins (see also “paranoid” and “uncomprehending”) School of Crying “Censorship”. Even more bizarre, were the demands. On the “DeSmog Blog”, Kevin Grandia lambasted me for not knowing anything about law, and then demanded I email him and explain myself and my reasons for filing a copyright complaint. I’m no lawyer, but clearly giving details of a dispute to an angry third party not involved isn’t right up there with sound legal advice.
Still apparently confused that his dispute lay not with me but with YouTube or the concepts of intellectual property, when that didn’t get the required response, Mr. Grandia posted another angry column over on the Huffington Post, and made the same demand. He’s wondering why I haven’t responded directly to him.
Really.
But being that guardian of smoggy freedom, Mr. Grandia took it a step further, and, in a rather ironic follow-up to his seizing of the mantle of all that conforms to the laws, somehow located the original YouTube video and reposted it to YouTube under the “DeSmog Blog” label:
You can watch it here:
http://www.youtube.com/watch?v=P_0-gX7aUKk 
So much for my “censorship”, feel free to view it. You see, I’ve had lots of angry criticism in the last two years, this is nothing new, so I’m not really concerned about the criticisms.
When viewing, note the graph from NCDC in the video which “proves” my surfacestations project is (choose your own derogatory word). More on that momentarily.
The alarmosphere was reaching a tipping point. I knew it was only a matter of time before somebody would blog the coup de grace, and yet; I still haven’t heard from Mr. Sinclair so I could tell him about what I’d like changed.
OK. But if Mr. Sinclair had contacted me (like a journalist would) before he made his video, instead of simply reading the NCDC Talking points memo (revised version seen here, PDF) he could have found out a few things, such as:

NCDC used an old outdated      version of my data set (April 2008) they found on my website and assumed      it was “current”. Big mistake on their part. Big admission of not overly      concerning himself with first-hand knowledge, or even substance, on his      part.
NCDC did not contact me about      use of the data. The data, BTW is not yet public domain, though I plan to      make it so after I’ve published my paper. So like Mr. Sinclair,      technically they are also in violation of copyright. Surfacestations is a private project, I emphasize, what with the public-private concept being one of      the major precipitors of the alarmosphere’s angst.
That data NCDC found had not      been quality controlled, many of the ratings changed after quality control      was applied, thus changing the outcome.
When notified of this, they      did nothing to deal with the issue, such as notifying readers.
NCDC published no      methodology, data or formula used, or show work of any kind that would      normally be required in a scientific paper.
The author is missing from      the document thus it was published anonymously. Apparently nobody at      NCDC would put his or her name on it.
When notified of the fact      that the author’s name Thomas C. Peterson (of NCDC) was embedded in the  properties of the PDF document (which happens on registration of the Adobe      Acrobat program, causing insertion in all output), NCDC’s only response      was to remove the author’s name from the document and place it back online. It is odd behavior for a scientist to publish work but not put your name on it.
NCDC got the number of USHCN      stations wrong in their original document document graph, citing 1228 when      it is actually 1218 I notified them of this and they eventually fixed it.
That NCDC original document      did not even cite my published work,  or even use my name to credit      me. I have the original which you can view here Note also the name in the document properties and      the number of USHCN2 stations above the graph.

I’m regularly lambasted for publishing things here that are not “peer reviewed”. But, when NCDC does it, and does it unbelievably badly, not only is the “talking points memo” embraced by the alarmosphere as “truth” and “falsification”, but NOT ONE of those embracing it show the remotest interest in questioning why it fails to meet even the basic standards for a letter to the editor of a local newspaper.
My own local paper wouldn’t publish a letter or memo where the author is not identified. Yet an anonymous NCDC memo the author won’t even own up to is considered “climate truth”.
Students of the alarmists may have noticed some time ago, how the burden of proof and quality of publication shifts when the other side of the aisle is doing the talking.  In fact, nobody who has jumped into the fray has asked me any questions, yet take as accurate our gift-card designer cum climate scientist Mr. Sinclair at his word, without asking me a single question.
I guess it doesn’t matter now, The Good Ship Teachable Moment has sailed, now that “Big Smog” has stepped in as the defender of freedom. I think Mr. Grandia is hoping that I’ll file a copyright complaint against him.
But here is the kicker. Once you sort through all the ad homs in the video, you find the nugget. It involves that graph that Mr. Sinclair cites from the NCDC Talking Points Memo. If he had asked, he would have found out that it has some pretty embarrassing flaws.

Figure 1. From the NCDC Talking Points Memo.
As referenced in the text of the NCDC  Talking Points Memo, the Figure1 graph compares two homogenized data sets, and demonstrates an uncanny correlation. Here is what they say:
Two national time series were made using the same homogeneity adjusted data set and the same gridding and area averaging technique used by NOAA’s National Climatic Data Center for its annual climate monitoring.
Seems reasonable, until you understand what “homgenization” really is.
What’s “homogenization” you say? Some kind of dairy product treatment?
Well no, not quite. It is data that has been put through a series of processes that render it so the end result is like comparing the temperature between several bowls of water that have been mixed together, then poured back into the original bowls and the temperature measured of each. What you get is an end temperature for each bowl that is a mixture of the other nearby bowl temperatures.
Here’s another way that is more visual. Think of it like measuring water pollution. Here’s a simple visual table of CRN station quality ratings (as used in my book) and what they might look like as water pollution turbidity levels, rated as 1 to 5 from best to worst turbidity:


In homgenization the data is weighted against the nearby neighbors within a radius. And so a station the might start out as a “1” data wise, might end up getting polluted with the data of nearby stations and end up as as new value, say weighted at “2.5”. Our contributing author John Goetz explains how even single stations can affect many many other stations in the GISS and NOAA data homogenization methods carried out on US surface temperature data here and here.

In the map above, applying a homogenization smoothing, weighting  stations by distance nearby the stations with question marks, what would you imagine the values (of turbidity) of them would be? And, how close would these two values be for the east coast station in question and the west coast station in question? Each would be closer to a smoothed center average value based on the neighboring stations. Of course this isn’t the actual method, just a visual analogy.
So, essentially, NCDC’s graph is comparing homogenized data to homogenized data, and thus there would not likely be any large difference between “good” and “bad” stations. All the differences have been smoothed out by homogenization  pollution from neighboring stations!
The best way to compare the effect of siting between groups of stations is to use the “raw” data, before it has passed through the multitude of adjustments that NCDC does. Admittedly, raw data can have its own problems, but there are ways my friends and I at the Pielke research team can make valid station trend comparisons without making numerical adjustments to the actual data raw data.
And finally for those who say “Watts doesn’t want you to see this video” or “he fears the science”, I direct you to this WUWT entry, dated June 26th, 2009:
NCDC writes ghost “talking points” rebuttal to surfacestations project
I was the first one to report on the NCDC Talking Points Memo. Fearing science, video and all that, I chose to publicly blog on a subject critical and even damaging to my own research, knowing full well others would pick it up, including those who would not treat this even-handedness kindly.
The document is an internal memo for NOAA. It didn’t get wide attention after it was first published on June 9th, in fact I don’t think it got any attention at all.
Without my pulling it out of internal memo obscurity and discussing it on WUWT, Dr. Pielke likely wouldn’t have commented on it, McIntyre wouldn’t have written about it,  twice, and thus from all the pickups from those articles, Mr. Sinclair probably wouldn’t have ever seen it. Surely there would not be this delightfully entertaining, rather revealing, and grade school caliber commentary had I not sought to publish it to a wide audience.
But that’s OK. The result is not something I fear, even if my final analysis shows the USA trends are unaffected. There are other things we know and will learn that are of significance.
In fact I’ve had some very positive things come out of this, both on the media and scientific side. Some offers and ideas have been floated.
But that’s a story that will have to wait. Maybe Mr. Grandia will place an online demand for it. Stay tuned. They rarely disappoint.
Oh, and I got to “meet” Mr. Sinclair, the father of a college-age kid though not quite  the young college kid I expected:

 On Climate, Comedy, Copyrights, and CinematographyThe good news: there’s new and exciting opportunities opening themselves to us.The bad news; some people are hilariously unquestioning.

It has been an even more entertaining than usual couple of days in the alarmosphere. I’d been traveling the last week, doing TV station work and station surveys. While on the road I discovered through an email that I was the subject of a YouTube Video called “Climate Crock of the Week”.
The video was about my surfacestations.org project and was titled “What’s up with Watts?”. It was sad and funny at the same time, and as is typically the case with our old friends it was directed at me personally, far more than it tried substance. Equally typically, and sadly, what substance it tried turned out to be wrong. I continued on my travels, My friend Dr. Roger Pielke Sr. posted an opinion on it last week to address some of the issues.
Little did I know bizarro land awaited upon my return home.
Sitting down Saturday night, to watch the video again detecting through its exquisite subtleties and nuance. I couldn’t help but laugh, because once again I noticed that everything reported in it was just wrong.
In fact, it probably was the worst job of fact-finding I had ever seen, which as WUWT readers know, is a bold assessment. I’ve been involved in broadcast TV news for 25 years, and have seen some really bad work from greenhorns fresh out of reporters school. This video reminded me of those. It was if whoever put it together had never researched it, but just strung together a bunch of graphics, video, photos, and the most monotone Pat Paulsen narration I’d ever heard. I figured it was probably just an overzealous college student out to save the world and this was some college project. It had that air of  radical burningman quality about it.
Curiosity piqued, I inquired into just who is this climate Solon? To my surprise, he turned out to be an “independent film producer” working out of his house in Midland, MI under the name “Greenman Studio”, one Peter Sinclair, a proud graduate of Al Gore’s Climate Camp. I still figured him to be a kid and imagined his mom was yelling down into the basement “Peter that’s too loud, turn it down!”.
I also wondered if it was the same “Green Man” that had once prompted surfacestations volunteer Gary Boden to create this nifty patch:

This came about because my now defunct local “Alternate Weekly” had a ghost writer named “green man” who penned and unintentionally (I think) editorial about me and the www.surfacestations.org project back in 2007 in which he wrote the famous line:
“The Reverend Anthony WTF Watts and his screeching mercury monkeys…”
…in response to our daring to survey the weather stations nationwide.
What was funny is that in my original story, one of my commenters posted a funny comment about well, “green stuff” and the editor of the local “Alternate Weekly” went ballistic and demanded I remove it  and gave me a stern lecture on libel. I was happy to comply not out of legal obligation but courtesy and deleted the comment.
Is this Green Man the same guy? Inquiring minds want to know.
OK back to the present. I checked my email for some correspondence from Mr. Sinclair for the past week and found none, and looked back even further to see if he had contacted me about the surfacestations project weeks before in email or in my letters pile. I found nothing and was surprised that he had made a video using my work without at least a basic request or notice.  Normally when somebody wants to publish something in another media type (that is not a blog or webpage) from the surfacestations project or my blog, they contact me and ask permission to use the items. The word normal, however, upon scrutiny really doesn’t apply here.
I’ve gotten dozens of such requests from magazines, newsletters, book publishers, and TV stations. So far, I’ve never said no to any request for such materials or copyright waivers. I’ve filled out lots of forms granting my copyright waiver for the legally skittish that need more than an email or “sure, go ahead” over the phone.
But, in the video Mr. Sinclair produced and posted on YouTube, I noticed that he did in fact use photographs and graphics from my published book “Is The U.S. Surface Temperature Record Reliable?”.  I hold the copyright on this book. The notice for copyright is in the inside front cover.  © 2009 Surfacestations.org  All rights reserved, including the right to reproduce this report or portions thereof in any form.  ISBN 13: 978-1-934791-29-5  and ISBN 10: 1-934791-26-6.
There was also a Warner Brothers video clip from the movie “Anchorman” with a segment about the incompetent TV weatherman which I assume was added to portray me in my chosen career, and amazingly (and most amusingly) there was another video clip from the movie “The Adventures of Buckaroo Banzai” which is a campy sendup of “War of the Worlds”. Interestingly in  the credits, and I know this because I happened to watch the movie about two weeks before on Showtime, there is a “John Van Vliet” listed in the credits. It made me wonder if it is the same John Van Vliet that created the “opentemp” program launched just a couple of months after I first started the surfacestations project in an attempt to derail it early on. He made the mistake of using incomplete data. More on incomplete data later.
I noted that neither clip was from the trailers you could find on YouTube and were of high quality, so maybe they were cribbed from a DVD or perhaps an Apple video download, since I recognized from the editing effects that Mr. Sinclair owns a Macintosh. WB has some pretty stringent clip licensing requirements, which I know from doing TV news and a reporter wanting once to use part of a film from WB in a special news report. WB wanted our TV station to pay, but the cost was sky high for our small TV station. They finally whittled it down to something we could afford.
Doing a little more research, I found that Mr. Sinclair does a series of animated online greeting cards, which you can see here:
http://www.care2.com/ecards/bio/1023
I thought this one was pretty funny: http://www.care2.com/send/card/0840
The description portrayed him as a pretty nice guy with an alternate minded view of the world like a lot of college students have. He is not a college student, though he has a son who is of college age, a nice Ron Paul supporter, I am told from someone who has met him. His rather conservative son, contrasts the rather left-wing eco-activist ad hominem and rhetorically unrestrained father(see here). It is almost humorous greeting card-worthy, this role reversal.
But since he had used that © symbol, Mr. Sinclair demonstrated awareness of copyright protections, having availed himself of them, e.g., here, right below his own artwork.  With knowledge of this and ad hominem attacks made on me personally, I reasonably presumed his copyright violation on my part was likely intentional. I also figured that this might be a teachable moment, as I was still thinking this is a kid just out of college since there seems to be no business website for Greenman studio in operation yet, it is still “under construction”.
http://www.greenmanstudio.com/
And, I mused, by bringing the copyright issue to his attention, I’d probably be doing him a favor, since I surmised he’d be at risk for using the film clips. I figured anybody working a business out of a house without an operating web page probably can’t afford licensing fees. No deep pockets there. I certainly have no personal beef with Mr. Sinclair, it is just the copyright issue.
But my copyright had been ignored, with evidence that Mr. Sinclair as a publisher himself using the © symbol understands copyrights, and WB’s copyright also looked like it also had been ignored. And well, lets face it, he got the facts wrong about the project and never contacted or interviewed me to get any facts from my side (more on that later). So it could hardly be defined as “journalism” and the protections that such enterprise affords for “fair use”. So I filled out the form for copyright issues on YouTube, and pressed enter.
What I expected to happen is that I’d get an angry email or blog comment from the guy, I’d suggest to him (privately) to make a couple of modifications, grant him a copyright for the factual graphics from the surfacestations project, and tell him to put his video back up on the web. End of story, lesson learned.
What I didn’t expect was the alarmosphere going into berserk overdrive. After all, this was not yet a “weekday” which it increasingly seems to be what we call those periods when our friends lapse into said mode. It turns out that YouTube put my name and the surfacestations.org URL up on the video pane for the former video, made me a target for hatred by the “scream first, ask questions later” types.
The first hint of this started on Sunday when I got a comment on my blog. The commenter, who obviously didn’t know the difference between copyright law and constitutional law wanted to know why I had “denied free speech” to Mr. Sinclair. Of course, “free speech” protections involve state infringement and,as powerful as our friends do apparently believe I have become, neither am I the state nor was the state involved here, so the angst was yet again rather misplaced. Regardless, I also thought it this a pretty odd comment, since Mr. Sinclair still hadn’t contacted me, and I paid no attention to it.
Then I began receiving more odd comments, and I’m thinking; “why are these people making a private copyright dispute their personal business?”
Here’s sampling of  a  few comments I got that never made WUWT:
“Watts you are a coward chickesh** no good dumba** weatherman hiding behind a law that you’ve irrationally applied”
“You can’t handle the TRUTH, if I were Jack Nicholson I’d kick your a**”
“Wattsup, you and your stupid picture book project are toast!”
I even got comments from “Omar” in Finland:
“Looks like your attempt to smother and censor information has fired back badly on you Mr Watts: Do you have – how you say – the cahones to explain yourself? I think not. You appear to be a child coward man.”
And around the alarmosphere all sorts of curious accusations of censorship — again, with the long arm of the state nowhere to be found, this seemed to be a variant of the Tim Robbins (see also “paranoid” and “uncomprehending”) School of Crying “Censorship”. Even more bizarre, were the demands. On the “DeSmog Blog”, Kevin Grandia lambasted me for not knowing anything about law, and then demanded I email him and explain myself and my reasons for filing a copyright complaint. I’m no lawyer, but clearly giving details of a dispute to an angry third party not involved isn’t right up there with sound legal advice.
Still apparently confused that his dispute lay not with me but with YouTube or the concepts of intellectual property, when that didn’t get the required response, Mr. Grandia posted another angry column over on the Huffington Post, and made the same demand. He’s wondering why I haven’t responded directly to him.
Really.
But being that guardian of smoggy freedom, Mr. Grandia took it a step further, and, in a rather ironic follow-up to his seizing of the mantle of all that conforms to the laws, somehow located the original YouTube video and reposted it to YouTube under the “DeSmog Blog” label:
You can watch it here:
http://www.youtube.com/watch?v=P_0-gX7aUKk 
Note the graph from NCDC in the video which “proves” my surfacestations project is (choose your own derogatory word). More on that momentarily.
The alarmosphere was reaching a tipping point. I knew it was only a matter of time before somebody would blog the coup de grace, and yet; I still haven’t heard from Mr. Sinclair so I could tell him about what I’d like changed.
OK Nut if Mr. Sinclair had contacted me (like a journalist would) before he made his video, instead of simply reading the NCDC Talking points memo (seen here, PDF) he could have found out a few things, such as:

NCDC used an old outdated      version of my data set (April 2008) they found on my website and assumed      it was “current”. Big mistake on their part. Big admission of not overly      concerning himself with first-hand knowledge, or even substance, on his      part.
NCDC did not contact me about      use of the data. The data, BTW is not yet public domain, though I plan to      make it so after I’ve published my paper. So like Mr. Sinclair,      technically they are also in violation of copyright. Surfacestations is a private project, I emphasize, what with the public-private concept being one of      the major precipitors of the alarmosphere’s angst.
That data NCDC found had not      been quality controlled, many of the ratings changed after quality control      was applied, thus changing the outcome.
When notified of this, they      did nothing to deal with the issue, such as notifying readers.
NCDC published no      methodology, data or formula used, or show work of any kind that would      normally be required in a scientific paper.
The author is missing from      the document thus it was published anonymously. Apparently nobody at      NCDC would put his or her name on it.
When notified of the fact      that the author’s name Thomas C. Peterson (of NCDC) was embedded in the      properties of the PDF document (which happens on registration of the Adobe      Acrobat program, causing insertion in all output), NCDC’s only response      was to remove the author’s name from the document.
NCDC got the number of USHCN      stations wrong in their original document document graph, citing 1228 when      it is actually 1218 I notified them of this and they eventually fixed it.
That NCDC original document      did not even cite my published work,  or even use my name to credit      me. I have the original which you can view here Note also the name in the document properties and      the number of USHCN2 stations above the graph.

I’m regularly lambasted for publishing things here that are not “peer reviewed”, but when NCDC does it, and does it unbelievably badly, not only is the “talking points memo” embraced by the alarmosphere as “truth” and “falsification”. Not ONE of those embracing it show the remotes interest in questioning why it fails to meet even the basic standards for a letter to the editor of a local newspaper. My own local paper wouldn’t publish a letter or memo where the author is not identified. Yet an anonymous memo the author won’t even own up to is considered climate truth.
Students of the alarmists may have noticed some time ago, how the burden of proof and quality of publication shifts when the other side of the aisle is doing the talking.  In fact nobody who has jumped into the foray has asked me any questions, yet take our gift-card designer cum climate scientist Mr. Sinclair at his word that what he reported, without asking me a single question, is accurate.
I guess it doesn’t matter now, The Good Ship Teachable Moment has sailed, now that “Big Smog” has stepped in as the defender of freedom. I think Mr. Grandia is hoping that I’ll file a copyright complaint against him.
But here is the kicker. It involves that graph that Mr. Sinclair cites from the NCDC Talking Points Memo. If he had asked, he would have found this out.

Figure 1. From Talking Points Memo.
As referenced in the text of the Talking Points Memo, the NCDC graph compares two homogenized data sets. What’s that you say? Some kind of dairy product?
Well no, not quite. It is data that has been put through a series of processes that render it
such that end result is like comparing the temperature of several bowls of water
[need work here and diagram to explain homgenization of data]
And finally for those who say “Watts doesn’t want you to see this video” or “he fears the science”, I direct you to this WUWT entry, dated June 26th, 2009:
NCDC writes ghost “talking points” rebuttal to surfacestations project
I was the first one to report on the NCDC Talking Points Memo. Fearing science, video and all that, I chose to publicly blog on a subject critical and even damaging to my own research, knowing full well others would pick it up, including those who would not treat this even-handedness kindly.
The document is an internal memo for NOAA. It didn’t get wide attention after it was first published on June 9th, in fact I don’t think it got any attention at all.
Without my pulling it out of internal memo obscurity and discussing it on WUWT, Pielke likely wouldn’t have commented on it, McIntyre wouldn’t have written about it,  twice, and thus from all the pickups from those articles, Mr. Sinclair probably wouldn’t have ever seen it. Surely there would not be this delightfully entertaining, rather revealing, and grade school caliber commentary had I not sought to publish it to a wide audience.
But that’s OK. The result is not something I fear, even if it shows the trends are unaffected. There’s other things we know and will learn.
In fact I’ve had some very positive things come out of this both on the media and scientific side. Some offer and ideas have been floated.
But that’s a story that will have to wait. Maybe Mr. Grandia will place an online demand for it. Stay tuned. They rarely disappoint.
Oh, and I got to “meet” Mr. Sinclair, the father of a college-age kid though not quite the young college kid I expected:

 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93feb2d1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"China will not be sending ducks to Pakistan to chomp through a plague of locusts after all, an expert from Beijing’s troubleshooting team has said. A report in the Ningbo Evening News had said 100,000 ducks would be sent from Zhejiang province to Pakistan to deal with its worst locust invasion in two decades, generating 520m views on China’s Weibo social media platform on Thursday and thousands of comments.  China deployed ducks, whose natural diet includes insects, to fight a similar infestation in the north-western Xinjiang region two decades ago, reportedly with considerable effectiveness. Despite the popular support for the idea in a country where cute duck memes have become hugely popular, Zhang Long, a professor from China Agricultural University told reporters in Pakistan the ducks would not be suited to the conditions there. “Ducks rely on water, but in Pakistan’s desert areas, the temperature is very high,” Zhang said. Zhang, part of a delegation of Chinese experts sent to help the south Asian country combat the locusts, advised the use of chemical or biological pesticides instead. The locusts have already caused extensive damage in east Africa and India. Locust swarms can fly up to 150km (90 miles) a day with the wind, and eat as much in one day as about 35,000 people. The Ningbo Evening News had quoted Lu Lizhi, a researcher from the Zhejiang Provincial Institute of Agricultural Technology, as saying the use of ducks was much less expensive and environmentally damaging than pesticides. “Ducks like to stay in a group, so they’re easier to manage than chickens,” he said. A duck is also capable of eating more than 200 locusts per day, compared to just 70 for a chicken, Lu said.  • This article was amended on 27 February 2020 after Zhang Long rejected the Ningbo Evening News report that China was going to dispatch ducks to Pakistan"
"
This makes a lot of sense if you are a rational thinking person. I thought I’d alert WUWT readers to it. Below is a table from the front page of Spaceweather.com today, operated by NOAA and Dr. Tony Phillips. 


And this week, we saw what can happen when PHA’s come calling:

So in light of that, I thought this article was rather interesting.
Death from the Skies = Boring, Sweat from GHGs = Sexy [Jonah Goldberg]
Published at The Corner, part of NRO
From a longtime reader:
Dear Jonah,
I thoroughly enjoyed your article today, and not just because you touched on an area where I worked – at least tangentially – for over a decade.  You are right, virtually nobody is doing the leg work on keeping track of all the debris and potentially nasty sized rocks out there compared to the number of people shrieking about our impending slightly warmer earth.  The big reason is that it isn’t very sexy work, unlike being a proponent of Anthropocentric Global Warming (AGW).  If you work on space debris, minor planet orbits and earth crossing orbits about the best you can hope for is getting to name a new rock nobody else saw, or maybe getting your name in the paper while being misquoted by some reporter who doesn’t have a clue about what preliminary results or margin of error means when he says that your recently discovered rock will destroy the earth in 2029.
By comparison if you use your computer model to predict that according to your model the earth might possibly warm by somewhere between 0.9 and 3.5 degrees Celsius by the year 2100 you get to hang out with Al Gore and Bono and morally scold the ignorant proles for driving their SUVs to pick up the kids from daycare as you jet off to Switzerland for another speaking engagement.  Of course there is one other distinction.  The guy cataloging rocks is actually doing science, and that’s hard work.
One of the problems many people, especially scientists, are starting to have with the AGW proponents is their use of shrill tone and authority of numbers to try to stifle debate.  Science is not consensus, and though there can be a scientific consensus that doesn’t constitute science either.  Computer models predicting conditions 50 years from now in a system as complex as the earth aren’t within spitting distance of science.  To be science something has to be testable and falsifiable. It must produce a predicted data point, interaction or outcome that is unique to the theory and can be verified or falsified.  Would you bet your future on the accuracy of day seven of a seven day weather forecast?  That is essentially what we are being told by the AGW proponents we absolutely must do without delay.  Of course I think the without delay part has more to do with “We must pass the stimulus without delay” or “We must pass healthcare without delay” considerations than any notion that waiting three or four years will actuall make any long term difference.
read the rest of the article at The Corner
h/t to Planet Gore


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94499df5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

While there’s been much carping about the pork‐​laden, recently enacted farm bill, it turns out to be small fry compared to current energy legislation. If passed intact, HR 4, the “Energy Policy Act of 2002,” will begin the stealth enactment of the infamous Kyoto Protocol on global warming, wisely canned by President Bush a year ago.



Of particular concern is Title X, which requires industry to “voluntarily” report its total emissions of greenhouse gases such as carbon dioxide. “Require” and “voluntary” can only coexist in the goofy world of Washington, as the reporting of carbon dioxide becomes mandatory for all industry under this bill if 60 percent of the nation’s total emissions aren’t “voluntarily” reported.



Who’s kidding whom? The purpose of Title X is to establish some type of baseline for carbon dioxide emissions so that some type of arbitrary “cap” can be legislated. Think of this as a Corporate Average Fuel Economy program for me, thee, and everything we own. This is the deceptive atmosphere that pervades HR 4, which is based upon misleading “findings.” If these “facts” are incorrect or incomplete, what does that say about the subsequent regulations? Let’s examine just two of the many “findings” in HR 4, and propose some modest, more factual revisions. Current “Finding #1”: “The Intergovernmental Panel on Climate Change (IPCC) has concluded…that most of the warming of the last 50 years is ‘attributable to human activities’ and that the Earth’s average temperature can be expected to rise between 2.5 and 10.4 degrees Fahrenheit this century.”



Missing Facts: The earth’s surface temperature has warmed a little over 1 degree in the last 100 years. Half of that warming took place before humans could have caused it, and an additional 10 percent or so of the more recent warming has been caused by changes in the sun. Most of that recent warming is in the coldest air of winter, as predicted by greenhouse theory. In other words, the total warming caused by people is a shade less than a mere half of a degree. 



The U.N. made 245 separate forecasts for the next 100 years, based on different assumptions about energy use. The one that warms over 10 degrees predicts unprecedented changes in both per‐​capita emissions of carbon dioxide and the number of people on the planet. Both fly in the face of reality: Carbon dioxide per capita has been basically constant since we started measuring it nearly 50 years ago, and population projections are being scaled down rapidly as the world’s economies develop. Most of the U.N.‘s other 244 forecasts are in the lower half of the predicted range. An extension of current emission trends produces a warming that is slightly beneath their lowest estimate.



Revised “Finding #1”: “The Intergovernmental Panel on Climate Change (IPCC) has concluded that human beings have contributed to a slight warming of planetary mean temperatures in the past 50 years, largely in the coldest air of the winter. Based upon extrapolation of current carbon dioxide emission trends and latest population projections, the warming of the next 100 years is likely to be around 2.5 degrees. Other, less likely assumptions could produce more warming.” 



Current “Finding #4”: “The IPCC has stated that … global average sea level has risen, oceanic heat content has increased, and snow and ice extent have decreased, which threatens to inundate low‐​lying island nations and coastal regions throughout the world.” Missing Facts: Recent studies of satellite data and submarine records reveal that the rise in sea level due to human‐​induced climate change is at best about 2.5 inches, approximately half of what the U.N. has estimated. This forces a halving of the IPCC’s previous 100‐​year average projection, down to nine inches. Much of the U.S. Atlantic coast has seen much larger sea level rises in the last 100 years because of geological activity. Pretty much no one but a few scientists have even noticed it as we happily adapted, building increasingly expensive beach houses.



Melting of sea ice does not change sea level: Pour yourself a drink and prove it. Melting of land ice does. The “ice balance” in Greenland, the largest ice mass in our hemisphere, is neutral. In Antarctica, the continental sheets are growing, not shrinking. Revised “Finding #4”: “Most recent findings reveal a slight rise in sea level as a result of human activities, but there is no evidence for an increasing trend. Observations indicate that sea level will continue to rise, at a rate that most developed economies have easily adapted to.”



Space doesn’t permit an expanded criticism of other findings, but one of them deserves a Dishonorable Mention: HR 4 cites a government report, which turns out to be the “U.S. National Assessment” of climate change. It is based upon two climate models that perform worse than a table of random numbers applied to U.S. temperatures. For this, and for all the other half‐​truths in HR 4, we’re supposed to start down the economically disastrous road to Kyoto?
"
"
Sometimes, seemingly innocuous posts can bring in some oddball commenters. Such is the case this week with a  post I did about a cloud (or lack thereof) spotted by former NWS Lead Forecaster for northern California, Jan Null.

That post brought out the chemtrailers, one of whom insisted that the “hole punch cloud” was not only a new phenonmenon (it isn’t) but made by (you guessed it) chemicals released from airplanes.
In the strictest sense, he’s right. It is caused by airplane exhaust:
This relatively rare occurenvce is the result of an aircraft flying through a  layer of high clouds that have precisely the right temperatre and moisture.  As the jet aircraft flies through the layer it contributes just enough additonal moisture and exhaust particles for the ice crystals in the cloud to grow large enough to fall out as ”fall streaks”.  This happens in a circular pattern around the path of the jet with a hole in the cloud layer being the result.
Jan Null
SF Weather Examiner
There’s nothing nefarious about it.
But when I didn’t allow the discussion of the ridiculous premise of chemtrails, that didn’t sit well.
Oh, and hey: If you people can’t handle the TRUTH about PICTURES >YOUYOU< can’t handle?!?!
.
That you have the freaking temerity to post BALD-FACED LIES from some JERK professing a ~new~ cloud formation, and thence declare that NOBODY may challenge that PROCLAMATION?
.
Science, you say?
.
Science which can’t =OR WILL NOT= be challenged?
.
YOU ARE NO BETTER THAN THE SNAKE OIL SALESMAN YOU PROFESS TO EXPOSE!
.
Go ahead, MR. WATTS, DELETE THIS POST TOO!
I decided to leave it up as an example.
Ric Werme really said it all with this comment about SHOUTING IN ALL CAPS.
Thus our quote of the week:
Good science doesn’t need all-caps.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93be510a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterNTZ contributor and climate observer Matti Vooro provides evidence that the floods of North America are likely linked to cooling, and not warming.
===================================================
Global Cooling – The Real Cause Behind The Catastrophic North American Floods Of 2011
by Matti Vooro
The Canadian Prairies and the United States North Central regions are experiencing one of the worst flood seasons going back some 350 years, read here, here, here, and here. The reason for these floods is not well reported or researched by our media.
These floods stem from the extreme cold and the significant snow extent that fell in the central North America during the past winter. The very significant initial spring snow melt followed by a cool and very wet spring has resulted in more water than the ground could possibly hold. Some areas are having continuous flooding and have received two floods already and may get a third flood as well from new heavy rainfall.
US And Canadian temperatures have been falling 
Unreported by the media are the news that the annual and winter temperatures have been falling over most areas of US and southern Canada [excluding the north] since 1998, and more significantly during the last 4 years [see below]. So how can the floods be due to warming?
Winter temperatures in the USA have been plummeting:
Winter temperatures in the contiguous United States, 1998-2011 (NCDC)
Also in Southern Canada:
Winter temperature departures from 1948-2011 for the Canadian Prairies and Northwest Forest Regions for 1998-2011. Source: Environment Canada
Snow extent is also on the rise:
Snowfall extent is climbing. From Rutgers University
Snow depth in Northern US And Southern Canada in April according to the NOAA:
Source: http://climvis.ncdc.noaa.gov/cgi-bin/cag3/hr-display3.pl
As winters cool, more snow accumulates, which contributes to flooding. As temperatures cool, how can the flooding be due to warming?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




While AGW climate scientists and some world governments are mainly focused on the non-existing global warming, the globe is actually cooling and the impact of this cooling is far greater and more imminent than that of global warming 100 years from now, as these floods clearly illustrate.

This colder and wet weather means that crops cannot be planted and the entire season could be lost for the affected areas. These regions are the bread baskets of the globe and we should all be concerned. Over 1.2 million hectares of farming land may already be lost for farming this year in the Manitoba, and Saskatchewan region alone as recently reported on our news. Similar losses may happen in the United States.
More floods in the future – due to cooling
We are likely to see many more spring floods in the upcoming years like the 2011 floods because the cold winters are returning and could be with us for the next 20-30 years – like we had during the late 1970’s as the Pacific and Atlantic Oceans cool and lose their current heat content. These inland areas of North America are clearly cooling faster than the coastal areas since their climate is not moderated by still warm oceans. This will change in the future as the oceans start to cool and coastal areas will also get colder like Western Europe and Eastern North America.

These events, like the cold and snowy winters, extra flooding and severe tornadoes, have very little to do with man-made carbon dioxide or global warming as the global temperatures have been cooler than normal this winter and the early part of this year, and global and regional temperatures have actually been declining.
The expensive and misguided anti-global warming policies divert valuable funds from other vital areas of our global life, like helping nations who are suffering and experiencing natural disasters, improved flood control, rebuilding homes and infrastructure after tornadoes and major flooding and extra food storage for emergencies as well as job creation, better health care and poverty.
In my judgment this problem could get much worse in the coming years. Like the Pacific Ocean, the North Atlantic Ocean is also cooling again and by 2015 we could begin to feel even cooler weather during the winter and spring especially along the North American eastern coasts and Western Europe. Food and energy could be in short supply unless we all adjust our national and global focus from a non existing global warming threat to a much bigger and very current threat from global cooling for the next 20-30 years.
In summary, the message of this brief article is that we are totally focused on the wrong weather. The impact of global and regional cooling from our winters is much more severe and immediate than any minor impact of global warming 100 years from now, the effect of which may never materialize. Our limited financial resources are being channeled in the wrong direction with little benefit to the planet or to humanity.
Matt Vooro
Share this...FacebookTwitter "
"

My name is Tom Miller. I am director of health policy studies at the Cato Institute, a nonpartisan public policy research institution headquartered in Washington, D.C. I would like to thank Chairman Underheim and Rep. Weickert for inviting me to testify today about what can be done about rising health care costs and, in particular, the possible role of medical savings accounts in restraining the growth of health care costs and making health care more affordable and available.



This committee has already heard quite a bit of testimony regarding the various possible factors behind the return of annual double‐​digit percentage increases in health care spending and health insurance premiums. Some of those factors may remain outside the immediate reach of public policy, such as increased consumer demand for health services in an aging and relatively wealthier society. Other factors include a mix of both higher costs and even more highly valued benefits, such as more effective prescription drugs and innovative technological advances in medical diagnostics and treatment.



Indeed, our society may well decide to spend even higher and higher shares of our nation’s resources on health care in future years — as long as someone, somewhere can be found to foot the bills — but American consumers will receive more value for each dollar they spend only if the distorting effects of government’s multiple role as a regulator, purchaser, and subsidizer of health care are reduced. Our objective should be neither to artificially keep spending levels higher, nor lower, than their market‐​determined costs, but instead to allow individual consumers to seek the best value that balances their spending preferences and priorities with the resources that they can command. I’ll discuss a little later how best to sort out the respective roles of efficient market‐​based mechanisms for delivering health care and societal goals of safety net care, income security, and welfare assistance.



A number of the cost‐​drivers behind higher health care bills are related in part, if not entirely, to current public policies that are outside the primary scope of my remarks today. Mandated health benefits and excessive health services regulation at the state and federal levels unquestionably boost health care costs — estimates might range from a conservative 10 percent to as much as 25 percent, on average — and they contribute significantly to the level of Americans who cannot afford to purchase private health insurance. Medical malpractice costs, including defensive medicine, cannot be eliminated but they can be made better or worse by particular government policies.



Although adoption and dissemination of advanced health care technologies is frequently blamed as the key culprit in the long‐​term secular trend toward rising health care costs, that sort of oversimplified analysis often neglects to ask whether (1) our lives are better off, on balance, even after paying those higher costs, and (2) how the pattern and pace of such technological advancement might change if we relied less on third‐​party payers to fund it and increased the share of health spending that is paid out‐​of‐​pocket by individual consumers.



Another related set of governmental and non‐​governmental factors have combined to diminish the former role of managed care insurers in holding down the costs, if not improving the value, of private sector health care. The managed care backlash, in conjunction with provider pushback on prices, the pendulum swing of the insurance underwriting cycle, and employers’ competition for scarce labor, may have been inevitable to some degree — but public policy efforts to regulate, if not outlaw, many managed care practices and to encourage court challenges to third‐​party restrictions on access to care certainly contributed to a loosening of referral and authorization rules, as well as more inclusive provider networks. As a result, today we have more choice and access to care in many health insurance plans, as long as we (or our employers) still can afford to pay the higher premiums.



 **Medical Savings Accounts & Cost‐​Sharing Clones**



In the current environment of rising health care costs, the fading away of managed care’s third‐​party controls on the supply of health care, and (apparently) continued political resistance to even higher levels of government involvement in the regulation and financing of health care, more employers and their employees are turning to less comprehensive insurance coverage, greater individual cost‐​sharing, and reduced “insured” benefits as the most promising and, until now, relatively less‐​explored means to control health care costs.



Consumer‐​driven health care ranges across a wide continuum of health financing vehicles — from modest increases in cost‐​sharing under more conventional employer‐​sponsored health plans (higher deductibles, multi‐​tiered copayments for types of covered prescription drug purchases, higher out‐​of‐​network coinsurance percentages) to various types of two‐​tiered, defined contribution health plans (combining a higher‐​deductible group insurance policy with an individual health savings account) to the “real thing” — medical savings accounts (MSAs).



In a health care world where “nobody else” seems to managing either the cost or the quality of health care very effectively, MSAs and their cost‐​sharing cousins are picking up the baton and leading employers and their workers toward a consumer‐​driven model of health care purchasing.



Note that a recent Watson Wyatt Worldwide survey found that only 43 percent of workers were satisfied with the overall performance of their health plan. Less than half (48 percent) trust their employer to design a plan that will provide the coverage they need. Approximately the same percentage (47 percent) think that better health plans are available for the same cost. And almost four out of ten employees want their employer to contribute a fixed dollar amount toward the premium for any health plan — even if it means finding their own health plan.



The evidence is overwhelming that increased cost sharing reduces health insurance premiums substantially. For example, Jason Lee and Laura Tollen recently noted in a June 2002 article in Health Affairs that increasing cost sharing from a plan with $ 15 copays and no deductible to one with 20 percent coinsurance and a $ 250 deductible reduces premiums by about 22 percent; and a combination of 30 percent coinsurance and a $ 1000 deductible would reduce premiums by 44 percent. Offering less comprehensive insurance plans with greater enrollee financial responsibility is designed to encourage enrollees to be smarter consumers of health care services, limit demand for less beneficial “discretionary” care, seek out higher‐​value options, and save money for more critical medical needs in the future.



The recent revival of greater cost‐​sharing and so‐​called consumer‐​driven health plan options may provide a partial transition vehicle for employers who are rethinking their health benefits strategies but remain ambivalent about relinquishing most of their role in structuring employees’ choices and monitoring health plan vendors.



The “full‐​strength” version of consumer empowerment, of course, remains an MSA. MSAs provide a health care savings account in combination with a high‐​deductible health insurance policy. The savings account is controlled by the insured person and used to pay routine health care expenses. The accompanying catastrophic insurance policy covers more substantial health care costs. Because the cost of such a policy is usually significantly less than the cost of a low‐​deductible policy, the money saved may be used to increase contributions by an individual (or his employer) to an MSA administered by a designated trustee or custodian.



Unspent MSA funds, including any interest or investment earnings, accumulate from year to year, providing money to cover possible medical expenses in the future.



MSAs help control costs, improve access to health care, expand consumers’ choice in and control of health care, and increase savings.



By putting individuals back in control of more purchasing decisions, MSAs create incentives for individuals to purchase health care more prudently and reduce their overall health care spending in a given year.



Whereas out‐​of‐​pocket payments by individual consumers accounted for about 50 percent of total health care spending in 1960, the share of third‐​party payments (by private health insurers, employers, and government agencies) for health care has grown to about 80 percent. Third‐​party payment of health bills insulates individual consumers from the real cost of their health care decisions and treatment. Consumers have less reason to avoid unnecessary care, question costs, or shop around for the best treatment available at a reasonable price, but they have every incentive to demand more services.



Excessive third‐​party coverage with low deductibles increases administrative costs, because every small bill must be submitted for review and checking for accuracy.



Instead of limiting the supply of desired medical services, MSAs lower the demand for those services by requiring individuals to pay directly and up front for their discretionary health care choices.



The RAND Health Insurance Experiment, conducted from 1974 to 1982, demonstrated that the more people had to pay for medical are without insurance reimbursement, the less they would spend on total medical care.



Because MSA plans are linked to high‐​deductible insurance that covers health claims that are more catastrophic in nature, they make the cost of insurance coverage more affordable for most Americans. Less -comprehensive coverage will mean lower premiums for a larger fraction of people with low incomes. The majority of standardized insurance policies currently available are generous and expensive — making them unaffordable to low‐​income people. On the other hand, catastrophic insurance for very large, less‐​predictable health care expenses forces consumers to bear the full marginal costs of health care up to the point where their use of health care exceeds the deductible.



Under many third‐​party health benefit arrangements, consumers have little incentive or ability to become more knowledgeable about health care. MSAs stimulate consumer demand for information about the quality and price of health care.



A number of studies have illustrated that MSAs improve health plan options not just for affluent and health individuals, but for all Americans.



In April 2000, RAND Corporation researchers examined the effects of making MSA options available to small businesses. RAND rejected the assumption that MSAs appeal most to the wealthiest and healthiest workers. It found that HMOs would remain more attractive to higher‐​income workers, primarily for tax reasons, and exceptionally good health risks would be more likely to decline any insurance at all than to select the MSA option.



A 1996 study by National Bureau of Economic Research analysts concluded that most workers would end up retaining a substantial portion of the contributions they made to MSAs by the time they retired. Approximately 80 percent of employees would have retained over 50 percent of their MSA contributions by the time of retirement, and only 5 percent of workers would have saved less than 20 percent of their contributions. Although workers with high health care expenses in one year tend to have lower but still higher than average expenses in the next few years, the concentration of annual expenditures declines continuously as more and more years of expenditures are cumulated. High expenditure levels typically do not last for many years.



Another 1996 study of Ohio‐​based firms that offered MSAs that did not qualify for tax advantages under the Health Insurance Portability and Accountability Act (HIPAA) determined that the employer’s total cost for family coverage under those MSA plans averaged 23 percent less than traditional family coverage, yet the average employee with family coverage also would be $ 1355 better off under the worst‐​case (maximum out‐​of‐​pocket liability) scenario.



So, if MSAs are so great, why don’t we see more of them in the marketplace? Primarily because the federal MSA program authorized under HIPAA in 1996 has been unnecessarily handicapped, if not permanently crippled, by unreasonable restrictions on what still remains a “demonstration project.” Congress still needs to permanently authorize federally qualified MSAs; lift the enrollment cap and allow an unlimited number of people to have MSAs; expand MSA eligibility to include employees in businesses of all sizes, as well as employees without employer‐​sponsored insurance; allow MSA plans to offer a much wider range of deductibles; allow MSA holders to fund fully their MSAs each year, up to 100 percent of the insurance policy deductible; allow employers and employees to combine their contributions to MSAs at any time within a given year; and either preempt first‐​dollar state‐​mandated benefits or provide the flexibility for MSA plans to adjust to comply with those conflicting mandates.



In the case of MSAs, most of the problems have been caused at the federal policy making level. Aside from making equivalent state income tax benefits available to MSA owners and avoiding or eliminating restrictions on high‐​deductible insurance policies subject to state regulation, state policy makers should continue to press Congress to make MSAs universally available. They also might consider offering MSA‐​like health plan options to state employees as part of their benefits package, in order to boost demand in this currently thin market.



 **Remove Barriers to Growth of Defined Contribution Health Plans**



Despite the potential benefits of two‐​tiered defined contribution (DC) health plans, as well as the recent tax guidance issued by the Internal Revenue Service clarifying how accumulated balances in an individual employee’s health reimbursement accounts may be treated when rolled over at the end of a year, several regulatory barriers to the future growth of DC plans still need to be removed.



First, “pure” DC plans for fully insured employer groups, in which an employer distributes defined health benefits contributions to each eligible employee and allows them to purchase their own individual or non‐​employer‐​group insurance coverage, run the risk of being regulated inconsistently. They might be treated both as employee welfare benefit “group” plans and as “individual” health plans under state law.



To clarify the regulatory treatment of any plan or fund under which medical care is offered to employees by an employer solely through provision of a monetary payment or contribution to a participant or beneficiary and that is used exclusively to purchase individual health insurance coverage, state policy makers and other interested parties should press CMS and members of Congress to clarify that such plans or funds should not be considered an “employee welfare benefit plan” for regulatory purposes under the Employee Retirement Income Security Act (ERISA). However, such plans or funds would retain their “group” tax exclusion benefits under the Internal Revenue Code. Such hybrid treatment (group for tax purposes, individual for regulatory purposes) would be premised on the conditions that (1) only the employer, rather than individual employees, may decide to provide health benefits through defined contribution payments, and (2) such defined contributions must be provided to all employees or all members of a class of employees based on work‐​related distinctions.



Second, the defined contributions employers make to individual employees in pure DC plans, to be used to purchase individual health insurance coverage, should be allowed to vary on the basis of health status in the event the employer uses an approved risk‐​adjustment mechanism. That is, employers would be allowed to make larger contributions to workers with poorer health status to offset the higher premiums they would face when they seek to purchase individual coverage. State insurance regulators would need to approve this exemption from HIPAA non‐​discrimination rules for “group” plans, or press CMS to update and revise its past regulatory interpretations of this provision.



 **Lower Costs Trump Insurance Subsidies**



To achieve better health outcomes, we need to provide individual health care consumers with stronger incentives to be cost‐​conscious in using scarce medical resources. Making the market‐​based cost of care more transparent to all parties involved in health spending decisions will encourage its more efficient consumption and delivery. Reducing the long‐​term rate of growth in the cost of health care remains more important than (and, beyond a certain point, operates at cross‐​purposes to) expanding the scope and scale of subsidized health insurance coverage. Health insurance subsidies increase medical costs and the demand for health insurance, creating net welfare losses estimated at 20 percent to 30 percent of total insurance spending. In the opposite direction, access to “free” care dampens the demand for private health insurance. In striking the necessary balance, the effects of comprehensive third‐​party insurance on raising costs and limiting access to health care substantially outweigh any disincentives to obtain insurance protection that may be caused by direct provision of charity care. When rising health care expenditures outpace wage increases, their strongest effect is to reduce health insurance coverage for low‐​income workers. Hence, at the margin, increasing incentives to purchase less‐​comprehensive health insurance and filling in urgent gaps in direct delivery of health care through safety net mechanisms may produce more affordable and accessible health care.



 **Market‐​Driven Deregulation via Competitive Federalism**



Empowering consumers with a greater diversity of affordable health benefits choices will require exposing exclusive state health care regulation based on geography to competition from market‐​friendly regulation across state lines.



Lower‐​income workers in small firms bear the brunt of excessive state health insurance regulation, because their employers generally are unable to self‐​insure and, thereby, gain ERISA protection from state benefit mandates, restrictions on rating and underwriting, and other regulatory burdens. In general, increased state regulation has raised the cost of health insurance and limited the range of benefits package design. A wide assortment of small‐​group regulatory measures imposed by many states during the 1990s failed to improve levels of insurance coverage and, in some cases, priced low‐​risk consumers out of the small‐​group market. Various state government regulatory attempts to force low‐​risk insureds to subsidize high‐​cost insureds through devices like modified community rating and guaranteed issue often were counterproductive, because they triggered premium spirals that drove younger, healthier, and lower‐​income workers out of the voluntary insurance market. In other words, state health insurance regulation has been part of the problem, not part of the solution.



Rather than try to solve state‐​based regulatory failure with a new round of heavy‐​handed federal rule making or preemption, the better route to restoring a market‐​friendly, consumer‐​empowering environment at the state level is to facilitate competitive federalism‐​revitalized state competition in health insurance regulation that reaches across geographic boundary lines. (The closest successful model for such competitive federalism involves corporate law and the business of corporate charters, in which Delaware has specialized and excelled by consistently producing benefits to its “customers”-investors.) Such regulatory competition would limit the excesses of geographically based monopoly regulation. Currently, insurance consumers (at least in the non‐​self‐​insured market) are subject to a single state government’s “brand” of insurance product regulation. Solely by virtue of where they live, they are stuck with the entire bundle of their home state’s rules. Short of physically moving to another state, they are unable to choose ex ante the type of health insurance regulatory regime they might prefer and need as part of the insurance package they purchase.



Competitive federalism could facilitate diversity and experimentation in health insurance regulatory approaches. It would discipline the tendency of insurance regulation to promote inefficient wealth transfers and promote individual choice over collective decisions driven by interest group politics. In short, it would improve the quality of health insurance regulation, thereby enhancing the availability and affordability of health insurance products.



Insurers facing market competition across state lines would have strong incentives to disclose and adhere to policies that encouraged consumers to deal with them. Employers and individuals purchasing insurance would migrate to state regulatory regimes that did not impose unwanted mandates but, instead, fit the needs of their consumers. State lawmakers would become more sensitive to the potential for insurer exit. At a minimum, interstate regulatory competition would provide an escape valve from arbitrary or discriminatory regulatory policies imposed at either state or federal levels.



Key design requirements for regulatory competition in health insurance would include:



Several mechanisms or paths could lead to vigorous interstate competition in health insurance regulation. A more indirect, but sustainable, approach would involve strategic use of choice of forum clauses, and perhaps choice of law clauses, in health insurance contracts. Insurers would condition sales of a particular policy on a consumer’s consent to the designated litigation forum. That forum would be matched to the state whose regulatory law was selected. This choice of forum would need to be adequately disclosed and executed at the beginning of the contractual period, not just at the time of litigation. Insurers could increase the likelihood that the agreement would be enforced and regulatory competition enhanced by linking the designated forum to their company’s domicile‐​rather than to the site of the sales transaction.



Federal law could provide some shortcuts‐​such as a statute mandating enforcement of choice of forum contracts under the commerce or full faith and credit clauses of the Constitution. Congress also could provide uniform disclosure requirements for choice‐​of‐​forum and the insurer’s domicile in insurance contracts.



A more direct federal statutory approach might set an “insurer domicile” rule, in place of a “site of transaction” rule, for determining applicable state law and regulatory authority‐​at least as a default rule for multi‐​state transactions where the respective parties do not otherwise designate operative law. For example, Rep. Ernest Fletcher (R-KY) recently introduced the “State Cooperative Health Care Access Plan Act of 2002” (H.R. 4170), which would authorize a health insurer offering an insurance policy in one primary state (the primary location for the insurer’s business) to offer the same policy type in another secondary state. The product, rate, and form filing laws of the primary state would apply to the same health insurance policy offered in the secondary state.



Another route to interstate competition in insurance regulation might be built on decisions by individual states to grant regulatory “due deference” to determinations by out‐​of‐​state insurance regulators that a particular insurance company is qualified to conduct such business. Once an insurer submitted evidence of good standing in its domestic jurisdiction and (if different) in the jurisdiction where it conducts the largest share of its health insurance business, it would qualify for licensure in the state granting such regulatory deference. Regulators in secondary states would be most likely to treat proof of licensure and good standing in the primary state as prima facie evidence of qualification for licensure in the secondary state, while still requiring additional routine documents and fees and compliance of the primary states’ insurance department with broadly accepted accreditation standards, such as those maintained by the NAIC. Initially, an individual state’s decision to grant regulatory due deference would be similar to a declaration of unilateral free trade in health insurance products. The state would be eliminating or reducing its own regulatory restrictions on out‐​of‐​state insurance to benefit its citizens and to provide a model for other states to emulate.



 **A Real Safety Net for the Medically Uninsurable**



Medically uninsurable individuals represent a small percentage of the uninsured population (roughly no more than 1 percent to 2 percent of the uninsured have ever been denied health coverage for medical reasons). But they present the strongest case for public assistance. To some degree or another, at least 29 states currently operate high‐​risk pools that make insurance coverage available to the uninsured and subsidize their premiums. States with well‐​structured and adequately financed high‐​risk pools are more successful in keeping their individual health insurance markets competitive and insurance rates affordable. Such pools allow the individual insurance market to operate efficiently, while carving out for special treatment high‐​cost individuals who are beyond the capacity of the individual market to handle on an unsubsidized basis.



However, not all state high‐​risk pools are adequately financed (ideally, the funding should come from general revenues rather than through taxes on insurers within the state), and many states do not provide such subsidized coverage at all. Using the rationale that the “medically uninsurable” (at least to the extent that the unsubsidized price to insure them privately far outstrips their ability to pay) should be considered “medically needy,” mandatory Medicaid coverage and matching federal assistance should be extended to this class of beneficiaries, provided that the federal funds are channeled through state‐​operated high‐​risk pool programs that meet certain minimum criteria (for example, premium ceilings, waiting periods, rejection by at least one insurer, catastrophic conditions allowing automatic pool acceptance without prior carrier rejection) already in practice, but not “new” ones. The scope and scale of this Medicaid‐​financed high‐​risk pool coverage for the medically uninsurable would be capped at an upper ceiling that equals the higher amount of all individuals in a state facing private insurance premiums that are at least 200 percent of standard rates (plus those who cannot obtain any coverage at all, for medical reasons) or 2 percent of all people covered in a state’s individual insurance market.



 **Citizens’ Appropriations for Charity Care**



To bolster financing for charitable safety net care and ensure that it is delivered with private‐​sector efficiency, a new 100 percent, dollar‐​for‐​dollar federal income tax credit (above the line) should be provided for certain charitable contributions to provide health care services to the low‐​income uninsured. These “citizen appropriations” would be modeled in part on the Arizona tax credit for education “scholarships.” The maximum individual credit amount allowed would be no greater than 10 percent of an individual’s federal income tax liability in a given tax year. Eligible donations would have to be made to approved organizations that provide health insurance coverage, health care services, or payment of medical bills to uninsured individuals who are not eligible for optional federal health tax credits or Medicaid assistance. Organizations eligible to receive the donations must either be a non‐​profit, in accordance with section 501(c)(3) of the Internal Revenue Code, or, in the case of health care providers and that who wish to receive direct donations, they must create a separate non‐​profit subsidiary to receive and distribute such funding. Eligible organizations could spend only as much of their donations as they could document were directed toward paying the health care expenses of qualified uninsured individuals. Taxpayers could designate the institution to which their donation would be directed, but they could not pinpoint the individual beneficiary.



States could play a role in jump‐​starting this process, by providing their own state income tax credits along similar lines for such “citizen appropriations,” even in the absence of federal policy action.



 **“Getting Over” Adverse Selection**



Despite hypothetical concerns about adverse selection and risk segmentation in a more competitive, market‐​based private health insurance system, there actually is little evidence that individuals and families can identify and anticipate most of their future medical expenses in ways their potential insurers cannot. A recent study by Cardon and Hendel in The Rand Journal of Economics finds little empirical evidence of information asymmetries, market failure, and adverse selection in health insurance markets. Differences in health expenditures between the insured and uninsured are mostly due to observable differences in demographics (age, gender) and price sensitivities (higher‐​income workers capture more tax subsidies for insurance coverage), rather than unobservable factors related to health status.



Long, Marquis, and Rodgers also find little support for the hypothesis that people anticipate changes in their insurance status and arrange their health care consumption accordingly. The authors also find no evidence that people choose to purchase or drop insurance coverage in anticipation of change in their overall health care needs and conclude that insurer selection is an unlikely explanation for this failure to find quantitatively important transitory demand. However, they observe that recent state reforms aimed at eliminating or limiting some insurer restrictions on coverage of pre‐​existing conditions ironically might increase the ability of patients to adjust their treatment patterns for chronic conditions in anticipation of insurance changes.



Private insurers don’t need to remain helpless and clueless regarding potential adverse selection problems. In competitive markets, they may use a number of tools: set periodic limits on plan switching, vary premiums according to the amount of insurance purchased, underwrite and rate based on risk categories, create more homogeneous risk pools, or rely on the law of large numbers to diversify risks in large pools. Consumer inertia and individual differences in aversion to risk further limit the applicability of adverse selection theory to the real world.



Many difficulties we observe in health care insurance markets are due to government intervention rather than adverse selection or other market failures. If insurers are not allowed to charge different premiums to different risks, price predicted risk appropriately, and match their policy configurations to market demands, they will be more likely to resort to higher uniform prices, less savory practices like excluding or discouraging coverage of high risks, and, ultimately, market exit. Cream skimming (selecting only the best risks) becomes the insurers’ mirror image of adverse selection by insurance customers. Political interventions don’t alleviate underlying differences in risk across customers or eliminate insurers’ knowledge of such differences. They only force insurance companies to cope in inefficient ways and create new problems.



It is preferable to allow private insurers to do what they do best‐​evaluate risk and price it accordingly‐​and then deal with remaining outlier problems (for example, the medically uninsurable) through explicit, transparent public subsidies rather than more camouflaged regulatory cross‐​subsidies. We should separate support for societal objectives of income redistribution and protection against prohibitively expensive, but predictable, health risks from the competitive operations of commercial insurance markets.



Health status information is most likely to be asymmetric when it is scarce and costly. While government mechanisms prefer to ignore, hide, or shift those information costs, markets create proper incentives to discover efficient ways to signal relevant private information and put it to use.



Deregulating insurance choices and providing greater tax parity for all insurance purchasers can fill the real gaps in private insurance coverage, by providing breathing room for further market innovations, such as new forms of voluntary risk pooling and long‐​term insurance contracts. The growing availability of online health information and insurance products further strengthens the case for empowered consumers.



 **Summary**



Market mechanisms can’t eliminate every unfortunate human experience in health care access, affordability, and quality. Private charity and a backup safety net of transparent, direct subsidies have necessary roles to play. Unlike centralized government “solutions,” markets don’t promise perfect outcomes, just better ones.



The current climate of annual double‐​digit percentage increases in health care costs, dissatisfaction with the mature version of managed care, and remaining political resistance to centralized command‐​and‐​control mechanisms points to greater acceptance of the last remaining, relatively unexplored health care reform option‐​putting choices back in the hands of individual consumers and competitive free markets.



In a more market‐​based health care system, you would, to a large degree, get what you pay for, unless someone else wanted to pay for it voluntarily on your behalf. Income redistribution issues should be debated separately and resolved in the larger political arena, while we finally allow health insurance markets to operate more efficiently for the purposes for which they are best suited.
"
"
Share this...FacebookTwitterThe Fukushima nuclear reactor crisis has provided anti-nuclear activists with an assortment of (unsubstantiated) scare stories for turning the tide of public opinion against nuclear energy. So it’s little wonder that few places have been so gripped by hysteria as Germany, home to a large number of eco-stalinists and chronic doomsayers in the media.
German warmist website klimaretter.de (climatesaviour.de in English) writes that this however is not the case in Finland. Despite Fukushima, Finland is still very much in favour of using this plentiful source of zero-emissions energy. Klimaretter writes:
Despite the bad experiences encountered building the Olkiluoto 3 reactor– which will be finally finished 4 years later than planned and at double the cost in 2013 – the government approved the new construction of two additional nuclear reactors. And approved is approved, ways Mari Kiviniemi, head of the government.”
And why not? Nuclear power has been the safest of all sources of energy so far, as I mentioned in a previous post, renewable energy sources have many more deaths per terawatt-hour of electricity produced than nuclear. The dangers of nuclear power are mostly hype, and nothing to do with reality.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
The Finnish government reminds us that in Finland there is practically no danger of earthquakes or tsunamis. And klimaretter reminds us:
The emergency power supply in the event of a disruption of the external power source für is already with the old reactors are designed twice as safely as the Japanese damaged reactors.
One thing that the panic purveyors seem to forget is that there are concepts in engineering and design called “applying lessons learned” and “continuous improvement”. No doubt the Japan earthquake exposed the weaknesses of the older Japanese reactors and safety systems. ´But in the end, this will be a blessing in that the lessons learned can now be implemented into the design of the new generation reactors, thus making the safest source of energy out there even safer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Silly, desperate argument: “What if a plane crashes into it!”
This is a silly argument because planes could crash into anything, and so then nothing ought to be built. What if a plane crashed into a hydro-electric dam? A chemical plant? A football stadium?
Like anything else, all we can do is reduce the risk. And the best way to reduce risk in energy supply is to use the system that has proven to be the safest. That choice is obvious.
Thanks to reader DirkH for this video of a test plane colliding into a conrete wall at 500 mph. Risk dispelled!

Share this...FacebookTwitter "
"What a splendid irony it would be if the enduring legacy of Donald Trump’s presidency was the Green New Deal – a radical, government-directed plan to transition the US to a socially just society with a zero-carbon economy. Of course, it isn’t Trump’s idea. The Green New Deal was first proposed a decade ago, but has only recently captured the public imagination. Environmental activists from the “Sunrise Movement” protested in the office of House speaker, Nancy Pelosi, on November 13 2018, demanding the deal. And they were joined by recently elected congresswoman, Alexandria Ocasio-Cortez, who has argued passionately on behalf of the plan ever since.  Still, it’s partly thanks to Trump and the shock of his election that radical ideas are getting a hearing and his opponents are being forced to think bold. That’s just what is needed if the world is to get serious about tackling climate change. Alongside an aim for net-zero greenhouse gas emissions and 100% renewable energy, the Green New Deal demands job creation in manufacturing, economic justice for the poor and minorities and even universal healthcare through a ten-year “national mobilisation”, which echoes President Franklin Roosevelt’s New Deal in the 1930s. The UK has, for the past decade, thought of itself as a climate leader. It’s true that the 2008 Climate Change Act, which sets a legally-binding framework for carbon reduction, is ambitious compared to legislation in many other countries.  But the UK’s approach – like so many other countries – is based on quiet consensus. So far, climate politics has been a polite conversation between government, industry and researchers, not a subject of heated debate in parliament. My research with UK politicians shows a reluctance to speak out on climate change, as many prefer a low-key approach – dressing up climate action in the language of economic policy and market mechanisms to avoid confrontation with colleagues, the electorate or the industries that risk losing out in the shift to a low-carbon economy.  Some members of parliament even told me that they deliberately avoid mentioning climate change in speeches to the House of Commons or in their constituency, fearing it could backfire. One worried that he would be branded a “zealot”, and marginalised by his colleagues if he argued too vociferously in favour of climate action. This approach is severely limiting. Moving to a zero-carbon society will require changing the way that people live in their homes, travel around, shop, eat and source their food. It’s impossible to do all this without people noticing and attempting to impose change from above, without social consent, may also cause a backlash. The French president, Emmanuel Macron, found this to his cost when he tried to implement fuel tax rises which disproportionately affected poorer consumers. The result was the Gilets Jaunes protests which erupted in France in late 2018. 


      Read more:
      Emmanuel Macron's carbon tax sparked gilets jaunes protests, but popular climate policy is possible


 Climate policies should involve and excite people by addressing their concerns and aspirations. Climate policy proposals have typically centred around technically optimal solutions – trying to establish the least disruptive or costly approach, without paying attention to the question of whether people might vote for them. Barack Obama’s well-intentioned climate policies as US president fitted this mould. His Clean Power Plan, which sought incremental carbon reductions from existing power stations, was a pragmatic response to a divided political scene. After decades of technocratic and consensus-building climate politics, the Green New Deal swaggers onto the scene – an avowedly political and idealistic take on climate action. The Green New Deal was put forward as a Resolution to the House of Representatives, by Ocasio-Cortez and supporters from both houses on February 5 2019. It’s only a non-binding statement of intent at this stage and would require complex legislation. Bold political plans often founder on the rocks of implementation, especially when politics are as fractious as in the current Congress. But the Green New Deal has already succeeded in one important aspect: it puts climate policies on the agenda that are as ambitious as the science of climate change demands. This makes it impossible for opponents to stay silent. The Green New Deal is forcing Democrats and Republicans to consider their own stance on climate change. Some Democrats have branded the plan as unrealistic – a “green dream”, as Pelosi called it. Veteran senator, Diane Feinstein, was similarly dismissive, when young campaigners asked for her support. Republicans, meanwhile, have branded it a socialist takeover to rally their own supporters. But the Green New Deal’s opponents can’t simply criticise. They will need to find their own answer to the climate question. For the Republicans, denying or dismissing the science of climate change is becoming less tenable by the day. The impacts of climate change are mounting, public concern is rising, and schoolchildren are striking.  The Green New Deal has drawn attention to a gaping hole in right-wing politics – the confident articulation of a climate strategy. If you agree with the scientific consensus that rapid action is necessary, but you don’t like the strongly social flavour of the Green New Deal, what do you propose in its place? In the UK, the fog of Brexit has clouded any serious political debate on climate change, but when politicians manage to take a breath, they too will face the same challenge. The Labour Party has promised action but the Conservatives have been told that their own commitments aren’t compatible with the Paris Agreement and so they, too, need a plan. The fight is not nearly won. But the Green New Deal is already succeeding in putting climate action where it belongs, as the defining political issue of our time. How strange that we have dysfunctional US politics to thank for this huge step forward."
"

It’s getting a little hard for the press to keep up with the required twice‐​weekly global warming scare story, so, a few days ago, NBC reached into the recycling bin for an old chestnut: Glaciers are melting at our national treasure, Glacier National Park, and it’s a result of pernicious human influence on the atmosphere. On the program, reporter Jim Avila claimed that “the temperature’s gone up an average 3 1/2 degrees in the park during the last 110 years.”



A closer, more scientific look at the data reveals a different picture. The way we take reliable estimates of temperature is to measure it over a broad area–usually several counties–with an array of thermometers. This is the methodology used by the National Climatic Data Center, the country’s “official” climate repository. They have divided the nation into a couple of hundred “Climatological Divisions” (CDs). Their professionals and volunteers monitor calibrated instruments within each division, and then monthly and annual temperatures are calculated by averaging the readings.



At our latitude, glaciers melt in the summer. Beginning in September and ending in May, it snows frequently at Glacier Park, which is one of the reasons for its glaciers, to begin with. You can download the history of the Western Montana Climatological Division (“Montana CD 01”) for yourself, at http://​www​.wrcc​.dri​.edu. This site is the academically lustrous Desert Research Institute at the University of Nevada and “wrcc” is the Western Regional Climate Center, headed by Dr. Kelly Redmond, one of America’s most careful and savvy climatologists.



All of the CD records have a common starting point in 1895. Inspection of the entire summer history yields no statistically significant warming whatsoever in Montana 01. Ditto for the annual record, although that is not as important as the summer readings. In other words, Glacier’s glaciers are being melted by temperatures that show no summer warming distinguishable from natural year‐​to‐​year variability over the last 107 years.



With climate data, it’s easy to play the standard game of picking a starting point in the record to prove a point. Precisely, one can come up with 3 1/2 degrees of warming by looking at data beginning in 1950, rather than considering the entire history.



The reality is that Glacier Park’s glaciers are melting today because the summer temperature is the same, on average, as it was when the record begins, 107 years ago. They were melting then, too, which was before people burned much fossil fuel, altering the earth’s natural greenhouse effect. They started to melt as the region emerged from what is sometimes called the “Little Ice Age,” a cold period that ended in the middle of the 19th century.



This incident recalls similarly breathless and shoddy reporting two years ago about the melting of the hemisphere’s largest ice complex, the Greenland ice cap, and its outflow glaciers. The overall ice balance turns out to be nearly neutral, but the southern end is melting. We also have a large number of surprisingly good climate records at and near the regions of maximum melting. There hasn’t been any warming for a long time. Every graduate student who has ever passed a comprehensive exam in climatology knows that temperatures have been going down in this region for about 70 years. Like Glacier National Park, southern Greenland also warmed in the 19th century with the climb‐​out from the Little Ice Age.



For what it’s worth, some knowledgeable people think that this decline in regional temperature is caused by global warming (and others do not). But it is getting colder up there, even as the glaciers retreat. As scientists who study Greenland speculated two years ago, the melting rate must have been prodigious in the early part of the 20th century (or nearly 100 years ago) when Greenland was warmer than it is now.



Why didn’t NBC check the regional temperature records for western Montana? Instead of recycling an old story, they could have produced a much better one–real news–by showing that Glacier’s’ glaciers have been melting like this for well over a century and are doing so without any net regional summer warming in the last 100 years.



It’s a fact that many other glaciers around the world are melting because of local and regional changes that correlate with human combustion of fossil fuels, but not the disappearing ice fields of Glacier National Park.
"
nan
"

In an effort to justify its massive global warming regulations, the Obama Administration had to estimate how much global warming would cost, and therefore how much money their plans would “save.” This is called the “social cost of carbon” (SCC). Calculating the SCC requires knowledge of how much it will warm as well as the net effects of that warming. Needless to say, the more it warms, the more it costs, justifying the greatest regulations.   
  
Obviously this is a gargantuan task requiring expertise a large number of agencies and cabinet departments. Consequently, the Administration cobbled a large “Interagency Working Group” (IWG) that ran three combination climate and economic models. A reliable cost estimate requires a confident understanding of both future climate and economic conditions. The Obama Administration decided it could calculate this _to the year 2300_ , a complete fantasy when it comes to the way the world produces and consumes energy. It’s an easy demonstration that we have a hard enough time getting the next 15 years right, let alone the next 300.   
  
Consider the case of domestic natural gas. In 2001, _everyone_ knew that we were running out. A person who opined that we actually would soon be able to exploit hundreds of years’ worth, simply by smashing rocks underlying vast areas of the country, would have been laughed out of polite company. But the previous Administration thought it could tell us the energy technology of 2300. As a thought experiment, could anyone in 1717 foresee cars (maybe), nuclear fission (nope), or the internet (never)?   
  
On the climate side alone, there’s obviously some range of expected warming, often expressed as the probabilities surrounding some “equilibrium climate sensitivity” (ECS), or the mean amount of warming ultimately predicted for a doubling of atmospheric carbon dioxide. In the UN’s last (2013) climate compendium, their 100+ computer runs calculated an average of 3.2°C (5.8°F). A rough rule of thumb would be that this is also an estimate of the total temperature change predicted from the late 20th century to the year 2100.



That forecast is simply not working out. Since 1979, when global temperature-sensing satellites became operational, both satellite and weather balloon data show that the lower atmosphere is warming at about half the rate that was predicted. And in the area that is supposed to show the most integrated warming, in the tropics from about 15,000 to 45,000 feet, there’s two to three times less warming being observed than would be “forecast” by the UN’s models if they are run backwards from today. At the top of the active weather zone there, the forecast warming is a stunning seven _times_ more than has been observed.   
  
Since around the time that the last UN report was being written, a spate of scientific papers has been published showing that the ECS is quite a bit lower than the UN’s number, by 40-60 percent, depending upon the study.   
  
It seems like there’s quite a conspiracy of nature when it comes to observed versus predicted warming, with various measures all telling us that we’re seeing about half as much warming as we are supposed to in the bulk atmosphere. Further, the Obama Administration assumed a distribution of possible warming that was way to hot at the extreme end, 7. 1°C or 12.9°F, a number that _Science_ magazine recently said was “implausibly high” in a different model.   
  
On the economic side, how much something will cost by the year 2300 requires some estimate of economic growth between now and then. It’s called the discount rate, and there are actually guidelines for how to do this put out in 2003 by the Office of Management and Budget. The higher the discount rate, the less that warming costs that far out into the future. OMB says that “you should provide estimates of net benefits using both 3 percent and 7 percent.”   
  
( _Understanding discount rates_ : Imagine someone is going to give you $100 today (which you can invest), or a year from now, when that original $100 hasn’t been able to “work” for a year. If you’re OK either with $100 today, or $105 a year from now, your discount rate is 5 percent; you’re really expecting that much macroeconomic growth in a year. Over the long haul, average inflation-adjusted returns on equity investments are around the OMB’s 7 percent.)   
  
The latter figure drove the cost of warming down too far for the Obama Administration’s liking, and the cost actually went below zero assuming 7 percent and an ECS not far from what may be the most realistic value. That means it could be a net benefit, something the Netherlands’s Richard Tol has been saying for decades, as long as it doesn’t warm too much. The Administration wouldn’t go near that, so, in contravention of the OMB guidance, they simply did not use the 7 percent rate, as Kevin Dayaratna of the Heritage Foundation notes.   
  
For more information on the social cost of carbon, take a look at my testimony from earlier this week before the House subcommittees on the environment and on oversight. A lot came to light in the hearing, which will go a long way towards an EPA justification to cease and desist on its onerous Clean Power Plan and other Obama Administration climate regulations. 


"
"
From EurekAlert
International Greenland ice coring effort sets new drilling record in 2009
Ancient ice cores expected to help scientists assess risks of abrupt climate change in future
















 IMAGE: Atmospheric gases trapped in ancient ice recovered during the international North Greenland Eemian Ice Drilling, or NEEM, project are expected to help scientists better assess the risks of abrupt climate…
Click here for more information.















A new international research effort on the Greenland ice sheet with the University of Colorado at Boulder as the lead U.S. institution set a record for single-season deep ice-core drilling this summer, recovering more than a mile of ice core that is expected to help scientists better assess the risks of abrupt climate change in the future.
The project, known as the North Greenland Eemian Ice Drilling, or NEEM, is being undertaken by 14 nations and is led by the University of Copenhagen. The goal is to retrieve ice from the last interglacial episode known as the Eemian Period that ended about 120,000 years ago. The period was warmer than today, with less ice in Greenland and 15-foot higher sea levels than present — conditions similar to those Earth faces as it warms in the coming century and beyond, said CU-Boulder Professor Jim White, who is leading the U.S. research contingent.
While three previous Greenland ice cores drilled in the past 20 years covered the last ice age and the period of warming to the present, the deeper ice layers representing the warm Eemian and the period of transition to the ice age were compressed and folded, making them difficult to interpret, said White. Radar measurements taken through the ice sheet from above the NEEM site indicate the Eemian ice layers below are thicker, more intact and likely contain more accurate, specific information, he said.
“Every time we drill a new ice core, we learn a lot more about how Earth’s climate functions,” said White, “The Eemian period is the best analog we have for future warming on Earth.”
Annual ice layers formed over millennia in Greenland by compressed snow reveal information on past temperatures and precipitation levels and the contents of ancient atmospheres, said White, who directs CU-Boulder’s Institute of Arctic and Alpine Research. Ice cores exhumed during previous drilling efforts revealed abrupt temperature spikes of more than 20 degrees Fahrenheit in just 50 years in the Northern Hemisphere.
The NEEM team reached a depth of 5,767 feet in early August, where ice layers date to 38,500 years ago during a cold glacial period preceding the present interglacial, or warm period. The team hopes to hit bedrock at 8,350 feet at the end of next summer, reaching ice deposited during the warm Eemian period that lasted from roughly 130,000 to 120,000 years ago before the planet began to cool and ice up once again.
The NEEM project began in 2008 with the construction of a state-of-the-art facility, including a large dome, the drilling rig for extracting 3-inch-diameter ice cores, drilling trenches, laboratories and living quarters. The official drilling started in June of this year. The United States is leading the laboratory analysis of atmospheric gases trapped in bubbles within the NEEM ice cores, including greenhouse gases like carbon dioxide and methane, said White.
The NEEM project is led by the University of Copenhagen’s Centre of Ice and Climate directed by Professor Dorthe Dahl-Jensen. The United States and Denmark are the two leading partners in the project. The U.S. effort is funded by the National Science Foundation’s Office of Polar Programs.
“Evidence from ancient ice cores tell us that when greenhouse gases increase in the atmosphere, the climate warms,” said White. “And when the climate warms, ice sheets melt and sea levels rise. If we see comparable rises in sea level in the future like we have seen in the ice-core record, we can pretty much say good-bye to American coastal cities like Miami, Houston, Norfolk, New Orleans and Oakland.”
Increased warming on Earth also has a host of other potentially deleterious effects, including changes in ecosystems, wildlife extinctions, the growing spread of disease, potentially catastrophic heat waves and increases in severe weather events, according to scientists.
While ice cores pinpoint abrupt climate change events as Earth has passed in and out of glacial periods, the warming trend during the present interglacial period is caused primarily by human activities like fossil fuel burning, White said. “What makes this warming trend fundamentally different from past warming events is that this one is driven by human activity and involves human responsibility, morals and ethics.”
###
Other nations involved in the project include the United States, Belgium, Canada, China, France, Germany, Iceland, Japan, Korea, the Netherlands, Sweden, Switzerland and the United Kingdom.
Other CU-Boulder participants in the NEEM effort include INSTAAR postdoctoral researcher Vasilii Petrenko and Environmental Studies Program doctoral student Tyler Jones. Other U.S. institutions collaborating in the international NEEM effort include Oregon State University, Penn State, the University of California, San Diego and Dartmouth College.
For more information on the NEEM project, including images and video, visit http://www.neem.ku.dk.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e939bc9b1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The World Bank has been criticised for providing $55m (£43m) to aid fossil fuel extraction in Guyana, at the same time that it has pledged to stop direct funding of oil and gas production. The Washington-based institution, which provides loans and grants to aid the development of poorer countries, will provide $20m to pay for the training of Guyanese oil and gas officials, including those involved in the marketing of oil. It will also provide $35m to revamp the banking and insurance sectors in the country, in anticipation of the influx of billions of dollars of oil money from new oilfields. The World Bank gained plaudits from environmental groups in 2017 when it pledged to “no longer finance upstream oil and gas” after 2019. However, the pledge allowed it to finance the strengthening of governance and regulation in poorer countries, in the hope of avoiding the so-called resource curse in which oil wealth brings corruption and misuse of money. Campaigners said the World Bank’s focus on the oil and gas sectors clashed with its commitments “to help countries accelerate the transition to sustainable energy” and to support the 2015 Paris agreement goal of keeping global temperature increases to below 2C. “The World Bank’s public assistance to upstream oil development in Guyana is a blatant contradiction to Guyana’s climate change priorities and the bank’s commitment to the Paris climate agreement,” said Heike Mainhardt, a senior advisor at Urgewald, a German non-governmental organisation that has tracked the projects. “I am perplexed by the World Bank’s disregard for its own warning.” Mainhardt said the World Bank’s provision of assistance to the Guyanese government and general budget support allowed it to get around its pledge not to finance upstream oil and gas projects. The country’s rulers were then free to use the money to finance oil development directly, she said. Guyana is expected to become one of the world’s largest oil producers after US firm ExxonMobil, along with the consortium partners Hess and China’s state-owned CNOOC, found sites that could deliver 8bn barrels of oil, including in the offshore Stabroek block. The discovery could prompt massive change for a country of only 800,000 people and less than $5,000 in GDP per person in 2018. The consultancy Rystad Energy this month forecast that Guyana’s oil production could reach 1.2m barrels per day by the end of the decade, lifting total annual oil revenues well above $20bn at current prices. It said government income – projected to be about $270m in 2020 – could reach nearly $10bn annually within a decade, far outstripping Guyana’s 2018 GDP of $3.9bn. Melinda Janki, an international lawyer challenging oil development in Guyana, asked why the World Bank was not funding cheap renewable energy for the country. She said the institution was “pushing Guyana down a financially disastrous development path”. Oil production has become a significant issue in the run-up to Guyana’s election in March, after criticism of the incumbent government run by President David Granger. This month Global Witness, a corruption monitor, said a 40-year deal agreed between the government and ExxonMobil for drilling rights would deprive the country of $55bn. The Guyanese opposition has maintained that it would not renegotiate the ExxonMobil contracts. A World Bank spokeswoman said: “The World Bank has not provided any financing to develop the Stabroek Block oilfield.” She added that the resources governance project “is aligned with the World Bank’s 2017 One Planet summit announcement that the [World Bank Group] will no longer finance upstream oil and gas, but will continue to help client countries strengthen transparency, governance, institutional capacity, and the energy regulatory environment, including oil and gas.” • This article was amended on 28 February 2020 because the Guyanese opposition has not promised to renegotiate the ExxonMobil contracts, as an earlier version said. This has been corrected."
"**Millions of Americans are already travelling home to celebrate Thanksgiving, despite warnings from health officials amid a significant wave of coronavirus cases and deaths.**
Thanksgiving, traditionally a large family get-together that rivals Christmas in size, is on Thursday.
Three million people are reported to have already travelled through US airports from Friday to Sunday.
But the number is around half the usual figure for Thanksgiving travel.
Dr Anthony Fauci, the country's top infectious diseases expert, told CBS News that people in airports ""are going to get us into even more trouble than we're in right now"".
The number of people flying in the US is the highest since mid-March, when the virus started to spread rapidly in the country.
But millions of Americans are also making huge personal sacrifices to stay at home this year. Some of our readers have been sharing their stories with us.
_We are used to family gatherings. Not being able to spend holidays with family - especially not visiting parents - is hard, but it's the right and responsible thing to do. We want them around._ **Dr Abdul Razzak, Ohio**
_Our families have been understanding about our desire not to travel this year. My husband and I will have just one friend over, who has been part of our 'bubble' all year._ **Meredith Power, Maryland**
_I am staying home and avoiding any unnecessary travel or contact with other people. Normally we travel to spend the Thanksgiving holiday with our family. I'm sad I will not be able to see my grandma. I miss being in their presence._ **Ryan Sedgeley, Wyoming**
Inevitably, however, much of the focus is on those who are travelling, and the fears that many are ignoring the health guidance.
Cleavon Gilman, an emergency doctor in Arizona, tweeted: ""Our pleas for help have fallen on selfish deaf ears.""
On Monday, the US - the worst-hit country in the world - recorded a further 150,000 cases of coronavirus, according to the Covid Tracking Project.
The number of people admitted to hospital with the virus has increased by nearly 50% in the past two weeks, while more than 257,000 have now died of Covid-19 nationwide.
Elsewhere in the US:"
"

Using trade as a weapon of foreign policy has harmed America’s economic interests in the world without significantly advancing national security.



The proliferation of trade sanctions in the last decade has been accompanied by their declining effectiveness. From Cuba to Iran to Burma, sanctions have failed to achieve the goal of changing the behavior or the nature of target regimes. Sanctions have, however, deprived American companies of international business opportunities, punished domestic consumers, and hurt the poor and most vulnerable in the target countries.



According to the president’s Export Council, the United States has imposed more than 40 trade sanctions against about three‐​dozen countries since 1993.



The council estimates that those sanctions have cost American exporters $15 billion to $19 billion in lost annual sales overseas and caused long‐​term damage to U.S. companies–lost market share and reputations abroad as unreliable suppliers.



Economic sanctions are especially damaging when applied to “duel use” technology. U.S. companies face a web of controls that inhibit exporting high‐​speed computers and other high‐​tech goods that, while civilian in nature, could conceivably be used by a hostile regime for military purposes.



Export controls on high‐​tech goods suffer from two fatal flaws: The first is that similar technology can often be obtained off the shelf from foreign competitors. Export controls succeed only in cutting U.S. firms out of fast‐​growing foreign markets without enhancing national security one bit.



The second flaw is that whatever controls are written into law are quickly outdated by Moore’s law of technological advancement. Today’s “supercomputer” inevitably becomes tomorrow’s high‐​end PC.



As well as inflicting economic damage, trade sanctions have been a foreign policy flop. A comprehensive study by the Institute for International Economics found that sanctions have achieved their objectives in fewer than 20 percent of cases. For example, the Nuclear Proliferation Prevention Act of 1994 failed to deter India and Pakistan from testing nuclear weapons in May 1998.



Trade sanctions seldom work because of the competitive global marketplace and the nature of regimes most likely to arouse America’s ire. Although the United States is by far the world’s largest economy, its global economic leverage is limited. The United States accounts for only 13 percent of the world’s merchandise exports and 16 percent of its imports. If Washington seeks to punish another country by unilaterally withholding exports, such as farm products, computers, or oil‐​drilling services, other global suppliers stand ready to fill the gap.



Even if sanctions inflict some pain on the target country, they typically fail because of the nature of regimes most likely to become targets of sanctions. Human rights abuses tend to vary inversely with economic development. Governments that systematically deprive citizens of basic human rights typically intervene in daily economic life, resulting in underdeveloped and relatively closed economies. Such nations are the least sensitive to economic pressure. The autocratic nature of their governments also means that they are relatively insulated from any domestic discontent caused by sanctions. If anything, sanctions tend to concentrate economic power in the hands of the target government and reduce that of citizens.



America’s ongoing embargo against Cuba illustrates the failure of sanctions. When the United States first imposed a comprehensive trade embargo in 1961, Cuba was conducting most of its trade with the United States. Since then, sanctions have utterly failed to influence the government of Fidel Castro, which has used the embargo to excuse its own policy failures and gain international sympathy. Although the embargo once enjoyed a measure of international support, today no other nation stands behind it. The reason is obvious: nearly 40 years after its imposition, the embargo has only hurt American companies and the Cuban people, while leaving the Castro regime firmly entrenched with little prospect of change. The manifest failure of U.S. policy prompted Pope John Paul II during his historic visit to Cuba in January 1998 to declare that sanctions are “always deplorable, because they hurt the most needy.”



Defenders of sanctions often cite South Africa as a success, but sanctions were not the only reason apartheid fell; the fall of the Soviet Union contributed to the climate of reform. Moreover, sanctions against South Africa differed from most U.S. sanctions today in two key respects. One, they were multilateral, while the large majority of sanctions imposed by the United States since 1993 have been unilateral. Second, the apartheid government in South Africa was answerable to a limited but still sizable electorate of about 5 million whites, which made the government more sensitive to outside pressure. Given that multilateral sanctions against a semi‐​democratic government were not sufficient to force change, it is virtually guaranteed that unilateral sanctions against a dictatorship will fail.



U.S. influence around the world is strengthened by the presence of American multinational companies. Foreign direct investment is not only profitable for American shareholders; it also helps foster greater economic growth in less‐​developed nations. American companies introduce new technologies and production methods, while raising wages and labor standards. That creation of wealth helps to advance social, political, and economic institutions that are independent of the ruling authorities. Companies engaged in long‐​term investments in Burma and elsewhere also help to build schools, hospitals, and roads.



China offers a good example of how economic engagement can help to slowly but steadily change a country for the better. Over the past two decades, China has become America’s fourth largest trading partner and the world’s second largest recipient of foreign direct investment behind only the United States, and China will soon be a member of the World Trade Organization.



China’s internal market reforms and increasing openness have fostered rapid growth that has led to rising living standards and greater autonomy for citizens. The share of industry controlled directly by the government has fallen from almost 100 percent two decades ago to less than 50 percent today. Private ownership of homes and businesses is rising dramatically.



Continued economic engagement has also helped open the door to China for a growing number of organizations whose mission is to promote religious and political freedom. For example, East Gates Ministries International, headed by evangelist Ned Graham, has been able to distribute millions of Bibles to Chinese believers. More than a decade after the outrage of Tiananmen Square, the communist government has begun to release political prisoners and allow a small measure of internal criticism. As was the case in Taiwan and South Korea, China’s economic liberalization is creating a foundation for a more vigorous civil society independent of government control.
"
"**A deal to allow families to meet over Christmas has been reached by the leaders of all four UK nations.**
A source told the BBC that details on how Covid restrictions will be relaxed will be announced shortly.
Scotland's first minister said she would ""continue to ask people to err on the side of caution"".
BBC Scotland's chief political correspondent said three households will be allowed to meet indoors over five days between 23-27 December.
Glenn Campbell said they will be able to meet in each other's homes, at a place of worship and in an outdoor public space. But the groupings must be ""exclusive"", meaning you cannot get together with people from more than two other households.
He added the leaders of the nations are expected to urge Britons to use any new flexibility sparingly because public health officials are worried Christmas get-togethers could cause a January spike in Covid cases.
Speaking ahead of a meeting of the UK government's emergency committee Cobra, Welsh First Minister Mark Drakeford cautioned any extra freedoms would not be an instruction to do ""risky things"".
Scotland's First Minister Nicola Sturgeon also stressed any changes would be ""temporary"" and ""limited"".
She said that the ""details"" may differ ""to reflect different circumstances in each nation"", such as what the definition of a ""household"" might be.
She added: ""I know everyone has a desire to see loved ones over the festive period.
""However, there is also a very real and a very legitimate anxiety that doing so could put those we love at risk, set back our progress as a country and result in unnecessary deaths and suffering.""
Meanwhile, the government has recorded another 608 deaths of people in the UK who have died within 28 days of a positive Covid test. There were also a further 11,299 cases of people testing positive for coronavirus.
What to do about Christmas divides opinion.
Increased mixing indoors will certainly mean there is greater transmission of the virus.
But, as chief medical adviser Prof Chris Whitty said on Monday, there is a balance to be struck between the harm the virus can cause and the societal and economic impacts of trying to control it.
He called for a ""public-spirited approach"".
By that he means adhering to the restrictions in the lead up to Christmas, being responsible with the opportunity the relaxation gives people and then immediately switching back to compliance.
If that happens any impact could be minimised - and, of course, it will be up to individuals to decide just how much they mix within the rules.
These are very fine judgement calls by ministers.
They hope Christmas will provide respite and help steel the public for what is clearly going to be a long, hard winter.
They also feel they have little choice, believing large numbers of people would ignore pleas not to mix and this way they can provide advice on how to enjoy Christmas as safely as possible.
But there is also the risk by sanctioning it there will be more mixing than there would have otherwise been.
Prime Minister Boris Johnson has acknowledged there would be risks of letting people meet over Christmas but said families should have the chance to reunite.
Transport Secretary Grant Shapps earlier said Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers.
Referring to domestic travel during the festive period, Mr Shapps urged those travelling on public transport to pre-book tickets as the capacity of services remains reduced to allow for social distancing and as a result of staff self-isolating.
Mr Shapps also highlighted Network Rail's plans for a series of upgrades and routine maintenance across Britain between 23 December to 4 January.
He told BBC Radio 4's Today programme: ""I would appeal to people to think very carefully about their travel plans and consider where they are going to travel and look at the various alternatives available.""
Mr Shapps added that people who live in areas placed in the highest tier of restrictions in England should avoid leaving their region entirely.
Mr Shapps said confirmation of the exact rules would come by Thursday - when people find out which tier their local area will be in - or potentially before then.
It comes after the prime minister confirmed tougher tier curbs once England's lockdown ends.
Gyms and non-essential shops in all parts of England will be allowed to reopen from 2 December under a strengthened three-tiered system.
Areas will not find out which tier they are in until Thursday - and the decision will be based on a number of factors including case numbers, the reproduction rate - or R number - and pressure on local NHS services.
At a Downing Street news conference on Monday to outline a ""Covid-19 winter plan"", Mr Johnson admitted Christmas this year would be very different to normal.
""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none,"" he said.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
Meanwhile, Mr Shapps announced people arriving in England from many countries will be soon able to reduce their quarantine period by more than half if they pay for a coronavirus test after five days.
The rules will come into force from 15 December and the tests from private firms will cost between Â£65 and Â£120.
Elsewhere, Health Secretary Matt Hancock told MPs the UK's new mass testing capacity could be used after the pandemic to diagnose a wider range of illnesses.
He said a British culture of ""soldiering on"" and going to work despite having symptoms of illnesses, including flu, ""should change"".
""In fact, I want to have a change in the British way of doing things where 'if in doubt, get a test' doesn't just refer to coronavirus, but refers to any illness you might have,"" he said.
Latest figures from the Office for National Statistics showed the total number of deaths occurring in the UK is nearly a fifth above normal levels."
"
Here’s an interesting story from the Times. One wonders if the Royal Society is ready to deal with all the unintended and unmodeled consequences of such actions? The last man-made volcano didn’t go over so well. – Anthony
A familiar man-made volcano - The Mirage in Vegas - Image courtesy PDphoto.org
 From The Sunday Times August 30, 2009
Man-made volcanoes may cool Earth
  



   Jonathan Leake, Environment Editor 

    

THE Royal Society is backing research into simulated volcanic eruptions, spraying millions of tons of dust into the air, in an attempt to stave off climate change.
The society will this week call for a global programme of studies into geo-engineering — the manipulation of the Earth’s climate to counteract global warming — as the world struggles to cut greenhouse gas emissions.
It will suggest in a report that pouring sulphur-based particles into the upper atmosphere could be one of the few options available to humanity to keep the world cool.
The intervention by the Royal Society comes amid tension ahead of the United Nations-sponsored climate talks in Copenhagen in December to agree global cuts in carbon dioxide emissions. Preliminary discussions have gone so badly that many scientists believe geo-engineering will be needed as a “plan B”.
Ken Caldeira, an earth scientist at Stanford University, California, and a member of a Royal Society working group on geo-engineering, said dust sprayed into the stratosphere in volcanic eruptions was known to cool the Earth by reflecting light back into space.
“If I had a dollar for geo-engineering research I would put 90 cents of it into stratospheric aerosols and 10 cents into everything else,” said Caldeira.
The interest in so-called aerosols is linked to the eruption of Mount Pinatubo in the Philippines in 1991, the second largest volcanic eruption of the 20th century. The explosion blasted up to 20m tons of tiny sulphur particles into the air, cooling the planet by about 0.5C before they fell back to earth.
The Royal Society is Britain’s premier science institution and its decision to take geo-engineering seriously is a measure of the desperation felt by scientists about climate change.
read the rest of the story here




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e938acb66',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDer Spiegel reports here (Bloggers accuse IPCC of conflicts of interests) on Climategate 2.0, where the IPCC just published a huge new report on renewable energy, claiming that within 40 years nearly 80% of the world’s energy needs could be met, mostly through a massive expansion of wind and solar power. The report, it turns out, was hardly scientific, and was based a lot on a paper co-authored by an employee (Sven Teske) of Greenpeace International and the European Renewable Energy Council.It means the report’s main message came from a full-time environmental activist and that the data underpinning the science was hardly more than a load of activism, and not real data.
Like much climate scandal news, it first got media traction in the Anglo-regions of the world. Now some of the German media are timidly weighing in. Der Spiegel writes:
So far the debate has made waves in the Anglo-Saxon media. That is probably due to the fact that the first to have noticed the possible conflict of interest is Steve McIntyre.  The Canadian is considered as one of the main critics of the established climate sciences. Last week he let loose at his climate blog Climateaudit.org, and spoke of “Greenpeace-Karaoke” in the IPCC report.”
IPCC Report “completely without partisanship”
Much of McIntyre’s critic was aimed at Sven Teske of Greenpeace and Ottmar Edenhofer of the Potsdam Institute for Climate Impact Research (PIK), who is also co-chairman of the WG3 of the IPCC. But Edenhofer has defended the report in comments to Der Spiegel. Der Spiegel writes:
The climate-economist told SPIEGEL ONLINE he defends the entire report. It is “balanced” and referees the state of scientific knowledge ‘without any partisanship’. He points out that the now much criticized study was subjected to ‘a strict scientific reveiw process’. The underlying assumptions of the concerned study have been clearly named. Moreover the IPCC-Report did not  ‘present any special report about renewable energies as being predominant’.”
As a blogger that follows German developments in climate science and politics, it is well known that the cast of characters at the PIK, including Edenhofer himself, are often more than loose with the truth. The PIK is well-known for its activism, let alone its shady brand of science.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But that doesn’t stop Der Spiegel from presenting a defence for Edenhofer and the report, writing that Edenhofer has a point, and even admitted that the costs of renewable energies are fraught with uncertainties, also see original Der Spiegel story .
Teske also offered a defence, telling Der Spiegel that he was not involved in writing up the press release itself. Der Spiegel quotes Teske:
 I first saw it when it was presented the first time at the press conference in Abu Dhabi.”
Teske adds he doesn’t see any problem being involved it the report, claiming that representatives of the US fossil fuels and nuclear industry were there too. Der Spiegel adds:
Greenpeace represents 3 million member worldwide. ‘Why shouldn’t we be involved in such a report?’, asks Teske.”
Der Spiegel then ends the piece by writing about the new IPCC codex that was agreed on, and quotes the last sentence that appears:
To prevent situations in which a conflict of interest may arise, individuals directly involved in or leading the preparation of IPCC reports should avoid being in a position to approve, adopt, or accept on behalf of any government the text in which he/she was directly involved.”
Share this...FacebookTwitter "
"
Spurious SST Warming Revisited
Dr. Roy Spencer August 31st, 2009 

My previous post described what I called “smoking gun” evidence of a spurious drift in the NOAA sea surface temperature (SST) product when compared to SSTs from the TRMM satellite Microwave Imager (TMI). The drift seemed to be mostly confined to 2001, almost a ’step’ jump. The moored buoy validation statistics of the TMI sea surface temperatures from Frank Wentz’s web site (SSMI.com) suggested that the TMI SSTs had good long-term stability.
But 2001 was also the year that the TRMM satellite was boosted into a higher orbit, which concerned me. I asked Frank about the effect of this event on the TMI SSTs (which also come from his web site). Frank couldn’t remember the details, but said he spent quite a bit of time correcting for the altitude change on the retrieved SSTs since the microwave emission of the sea surface depends upon the TMI instrument’s view angle with respect to the local vertical.
I know from our many years of work together on the AMSR-E Science Team that Frank is indeed a careful researcher, yet it seemed like more than a coincidence that the TMI and NOAA sea surface temperatures diverged during the same year as the orbit boost. So, I went back to see what might have caused the problem. I went back and thought about the different ways in which one can compute area averages from satellite data.
To make a long story short, because the orbit boost caused the TMI to be able to “see” to slightly higher latitudes, the way in which individual latitude bands are handled has a significant impact on the resulting temperature anomalies that are computed over time. The previous results I presented were for the 40N to 40S latitude band, which is nominally what the TMI instrument sees today. But before 2001, the latitudinal extent was slightly smaller than it was after 2001.
As shown in the following figure, if I restrict the latitude range to 38N to 38S, which was always covered during the entire TRMM mission, I find that the divergence between the TMI and NOAA average SST measurements essentially disappears.
Click for larger image
Even though I was processing the NOAA and TMI datasets in the same manner, I should NOT have been. This is because there were not as many gridpoints over cooler SST regions going into the ‘global’ averages before the satellite altitude boost as after the boost. So, for example, one must be very careful in computing a latitude band average, say from 39N to 40N, to make sure that there has been no long-term change in the sampling of that band.
Based upon the above comparisons, I would now say there is no statistically significant difference in the SST trends since 1998 between TMI, the NOAA ERSSTv3b product, and the HadSST2 product. And it does look like July 2009 might well have experienced a warmer SST anomaly than July 1998, as was originally claimed by NOAA. (Remember, TMI can not see all of the global oceans, just equatorward of about 40 deg. N and S latitude.)
In the bottom panel of the above figure, I also have a comparison between the TMI and AMSR-E sea surface temperatures, which are available only since June of 2002 from the Aqua satellite. As can be seen, there is no evidence of a calibration (or sampling) drift in that comparison either.
So, what’s the moral of this story? Always question your results…even after finding the obvious errors. And maybe I should eliminate the term ’smoking gun evidence’ from any results I describe in the future.
Oh…and don’t believe everything you read on the internet.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e937cacbd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
How has an image of a reservoir in a desert come to be the best, strongest, and most scientific indication of climate change?
A few months back, I posted a critique titled: Gavin Schmidt’s new climate picture book: Anti-Science?
I found it ironic that Dr. Schmidt used photos to depict climate change, while at the same time promoting open criticism of my surfacestations.org project on realclimate.org.  That project also uses photography, combined with measurements and a NOAA sanctioned rating system, to gauge thermometer siting issues. Oddly, there seems to be no complaints from the usual suspects when Dr. Schmidt uses artistic composition photography to illustrate climate change issues.
It is only fair then that since Dr. Schmidt has responded to the original author of that critical piece, Harold Ambler, that I repost Dr. Schmidt’s response here. Harold has invited me to republish that piece here.
A note to readers, Harold is going through a rough patch financially while waiting for his new book, Don’t Sell Your Coat, is to be published in November 2009. Royalties from it won’t come in until mid-2010. So if anyone is so inclined, please visit his web page and give him a  boost in the tip jar. – Anthony
More About Anti-Science
Guest post by Harold Ambler
As most of my readers know, I posted a critique of Gavin Schmidt’s book, Climate Change: Picturing the Science, not quite three months ago. Dr. Schmidt has responded in the last few days:

The point of a photo is always the context in which it’s seen. Lake Powell is a long way below it’s 1990’s peak, and that is due to a combination of reductions in rainfall upstream and additional demands on it’s water downstream. The last two years have seen a small rise in water level, and as you state correctly, it is important not to read too much into a short term record.
However, the real point of the photo (and as we discuss in the chapter that uses it), is that climate change is really only an issue because of the impacts – whether on human society or ecosystems. Areas that are already under water stress, such as the American South West are very vulnerable to changes in rainfall regime. And in fact, there is some evidence that long-term trends in precipitation in this region are already being affected by ongoing changes.

We have a long discussion in the book about being careful with the problem of attribution in imagery and we try to make that clear in the captions.”
The science concurs:
http://www.csmonitor.com/2008/0213/p25s05-usgn.html
“Last week, Dr. Barnett published additional work in the journal Science attributing 60 percent of the reduction in snowpack, rising temperatures, and reduced river flows over the past 50 years to global warming.
The latest work “not only shows that climate change is a real problem. It also shows it has direct implications for humans – and not just in the third world,” says Peter Gleick, president of the Pacific Institute in Oakland, Calif.”
So yes, it’s a combination of things, as stated in the book (if you bother to read past the cover photo) and in the scientific literature.
My Response to Dr. Schmidt (Plus a Note to Readers):
I grew up in the San Francisco Bay Area and lived through a few droughts, including the very serious one of 1976 to 1978. Again and again, my family and I saw water levels in the local reservoirs (and others in the state) decline to worrisome levels before they were, thankfully, replenished. One perspective on the phenomenon of alternating drought and wet in the West is that it is terrifying, and should be brought to as many people’s consciousness as possible as a new menace, part of global warming, etc. Another, more like my own, would point out that the astonishing agricultural productivity and explosion of population throughout the Southwest are proofs of humanity’s ability to adapt to its natural surroundings in very effective ways.
=====
Please read the remainder of the story at Talking About the Weather and don’t forget the tip jar 😉 – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93dbfa62',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Former vice president and Oscar winner Al Gore is scheduled to testify to both House and Senate committees today about global warming. For the past few years Gore has traveled across America speaking to audiences that range from friendly to worshipful, from journalists in New York and Washington to actors in Hollywood. If he has ever faced skeptical questions, it hasn’t been reported.   
  
  
We have several times invited the former vice president to present his famous slide show at the Cato Institute, in conjunction with a slide show prepared by Patrick J. Michaels, who takes a more benign view of climate change. Michaels is senior fellow in environmental studies at the Cato Institute and research professor of environmental sciences at the University of Virginia. He is the state climatologist of Virginia, a past president of the American Association of State Climatologists, and an author of the 2003 climate science “Paper of the Year” selected by the Association of American Geographers. His research has been published in major scientific journals, including _Climate Research, Climatic Change, Geophysical Research Letters, Journal of Climate, Nature, and Science_. He received his Ph.D. in ecological climatology from the University of Wisconsin at Madison in 1979. His most recent book is _Meltdown: The Predictable Distortion of Global Warming by Scientists, Politicians, and the Media, _which has been number one on Amazon’s global warming bestseller list for months at a time and has been reprinted twice this year.   
  
  
Gore’s office has declined our invitations. If Vice President Gore is committed to public understanding of climate change, why will he not demonstrate to a Washington audience composed of both supporters and skeptics that his ideas can carry the day in a dialogue with a leading critic? He wiped the floor with Ross Perot; does he fear that the case for catastrophic climate change is not as strong as the case for NAFTA?   
  
  
The invitation is still open. Mr. Vice President, please come to the Cato Institute and present your slide show to an audience of journalists and scholars with a knowledgeable climate scientist also on the dais.
"
"
Orland CA and the New Adjustments
by Steve McIntyre on June 29th, 2009
In my last post, I observed that NOAA’s Talking Points applied their new “adjustments” to supposedly prove that NOAA’s negligent administration of the USHCN network did not “matter”.
In order to illustrate the effect of the new methods in this post, I’ll compare the new adjustments (post-TOBS) to the old adjustments (post-TOBS) on a “good” station – Orland CA, a prototype “good” station, discussed at the outset of surfacestations.org, discussed at WUWT here and CA here in early 2007.

The station history for Orland (at CDIAC) says that it has been in its present location for (at least) most of the 20th century and has had minimal changes during that time, other than perhaps time-of-observation (TOBS). The TOBS adjustment is carried forward into USHCN-v2. As I understand it, NOAA’s New Adjustment Method replaces station-history based adjustments for instrumentation changes and station location (the latter formerly done in FILNET).
As a benchmark, here is the difference between FILNET (adjusted) and TOBS for Orland in the “old” USHCN. Adjustments in the 20th century are negligible – in keeping with station history information that indicates no changes in location.

Figure 1. “Old” USHCN Adjustments for Station Location and Instrument Changes
Now here is the net adjustment in the “New” USHCN.
Two points jump out. Look first at the monthly adjustments at the right hand side. In the “old” method, there weren’t any adjustments to recent data – where metadata did not indicate any relevant change. In the “new” method, there are all sorts of jittery little adjustments. They seem to average out, but why introduce these jitters in the first place? It’s starting to look like a pointless Hansen-esque (ROW-style) adjustment that simply distorts the underlying data.
On a larger scale, the new adjustment noticeably increases the 20th century trend at Orland.

These graphics strongly indicate to me that the effect of the algorithm – regardless of whatever good intentions may underlie it – is that data from lower quality stations is being blended into the presently archived Orland data. I presume that something similar is happening to other “good” stations (though I’ve only examined one example so far.) (Note that Orland is a CRN3 station. However, its excellent continuity makes it a pretty attractive station for benchmarking and visually it doesn’t look a “bad” CRN3 station).
Based on this example, it looks like NOAA’s Talking Points comparison is between the overall average and 70 “adjusted” stations – AFTER the good stations have been adjusted. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94cd2f83',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Britons should stop ""soldiering on"" by going to work when sick and making others ill, the health secretary says.**
Matt Hancock said people in the UK were ""peculiarly unusual and outliers"" for still going to work when unwell.
He made the comments in a joint session of the Health and Social Care and the Science and Technology committees.
He also told MPs he would like to see the diagnostic capacity built for Covid used to test for other illnesses like flu once the pandemic had passed.
The UK now has the capacity to carry out over 500,000 tests a day, with new labs to be opened next year to double that number.
He said he wanted to see the ""global-scale diagnostics capability"" continued to be used.
""Afterwards we must use it, not just for coronavirus, but everything,"" he told MPs.
""I want to have a change in the British way of doing things where 'if in doubt, get a test' doesn't just refer to coronavirus but refers to any illness that you might have.
""Why in Britain do we think it's acceptable to soldier on and go into work if you have flu symptoms or a runny nose, thus making your colleagues ill?
""I think that's something that is going to have to change.
""If you have, in future, flu-like symptoms, you should get a test for it and find out what's wrong with you, and if you need to stay at home to protect others, then you should stay at home.
""We are peculiarly unusual and outliers in soldiering on and still going to work, and it kind of being the culture that 'as long as you can get out of bed you still should get into work'. That should change.
""This year there's been far fewer respiratory and other communicable diseases turning up in the NHS.
""I want this massive diagnostics capacity to be core to how we treat people in the NHS so that we help people to stay healthy in the first place, rather than just looking after them when they're ill."""
"
Share this...FacebookTwitterHere’s a must read.
German blogsite Ökowatch here brings our attention to a report appearing in the Smithsonian: America’s First Great Global Warming Debate.
Even Thomas Jefferson was worried about man-made climate change. The Smithsonian writes:
The date was 1799, not 1999—and the opposing voices in America’s first great debate about the link between human activity and rising temperature readings were not Al Gore and George W. Bush, but Thomas Jefferson and Noah Webster.”
Thomas Jefferson, we find out, was a  warmist (who probably had not yet figured out how to make tons of money like Al Gore has done). According to the Smithsonian, Jefferson wrote:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Snows are less frequent and less deep….The elderly inform me the earth used to be covered with snow about three months in every year. The rivers, which then seldom failed to freeze over in the course of the winter, scarcely ever do so now.”
The cause of the climate change back then was man, though not from CO2 emissions, but through deforestation (ironically, today’s efforts to regulate climate are resulting in accelerated deforestation).
Author Samuel Williams claimed  climate change back then was “so rapid and constant.” Unfortunately Williams’ observations are not reflected by Mann’s Hockey Stick chart, which indicates very little climate change back then.
Like today, there were skeptics too, with Noah Webster being among the most vocal  in claiming that the conclusions were mainly based on anecdotes. The Smithsonian writes that Webster eventually prevailed, and quotes Kenneth Thompson, a modern environmental scientist from the University of California at Davis, who praises Websters saying:
,,, ‘the force and erudition’ of Webster’s arguments and labels his contribution to climatology ‘a tour de force.’
The same can be said about today’s skeptics.
Share this...FacebookTwitter "
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Share this...FacebookTwitterBy Matt Vooro
The current year 2011 is a good example of what happens when global temperatures drop and we have cold and snowy winters that stretch well into spring. The current La Nina and the cold PDO brought colder temperatures and extra amount of snow during the past winter to many parts of North America, which means significant spring flooding like we just had in Central US and many parts of Canada.
Even Hadley shows cooling over the last 10 years. Yet policymakers all believe it's getting warmer!
The late and extra snow pack in the Rockies and the colder Pacific air, generally due to the colder North Pacific Ocean surface temperatures as measured by the PDO, also caused extra spring rain and even more flooding as well as severe tornadoes. As the quite cold air from the Rockies (due to the significant snow pack still in the mountains) meets the warm moist air coming from the Gulf of Mexico, severe and frequent tornadoes are spawned in the US tornado alley. This was also the pattern in the 1970’s.
Moreover, there could also be a loss of annual crops this year due to a shortened growing season from the extra wet soil and late planting because of the flooding and cold spring, This pattern could repeat itself many times in the decades ahead similar to the climate we had in the 1970’s. Both the Atlantic and Pacific oceans show strong indications of heading for cooler SST levels.
These alternating cold and warm phases typically last for 20-30 years, as reflected in our past climate records. We just came out of the warm phase and appear to be now headed for a cooler phase. The winters started to get cooler for some parts of the Northern Hemisphere already after 1998, but more noticeably and significantly in many regions including Europe and Asia during the last for 4 years. 
Wasteful anti-global warming policies have resulted in badly needed funds being diverted towards very expensive and unsustainable green energy projects and carbon dioxide storage or sequestering. Without major government subsidies most of these projects would not be viable. Indeed their implementation worsened the financial situation of several poorer European nations. In some countries, fossil fuel plants are being shut down prematurely rather than being converted to cleaner fossil energies resulting in 100 % increase in energy costs due to the more expensive green energy replacements.
These very expensive policies to fight global warming have little effect on global temperatures, if any. These misguided policies divert valuable funds from other vital areas of our global life, like helping nations experiencing natural disasters, job creation, better health care, improved flood control, rebuilding homes and infrastructure after tornadoes and major flooding and extra food storage for emergencies. In my judgment this problem could get much worse in the coming years.
Like the Pacific Ocean, the North Atlantic Ocean is also cooling again and by 2015 we could begin to feel even cooler weather during the winter and spring especially along the North American eastern coasts and western Europe. Food and energy could be in short supply unless we all adjust our national and global focus from a non existing global warming threat to a much bigger and very current threat from global cooling for the next 20-30 years.
The events like cold and snowy winters, extra flooding and severe tornadoes have very little to do with man generated carbon dioxide or global warming as, to the contrary, the global temperatures have been cooler than normal this year and global temperatures have actually been declining since 2001 (see chart above).
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMuch of the German media have been screeching and hyperventilating today about CO2 emissions reaching a record high, see FOCUS or TAZ here or Die Zeit here or Der Spiegel here, to name a few.
All the dregs are at it, making dire 100-year predictions based on silly climate models that have been proven to be wrong time and again. Warmists are gasping in panic screaming “time is running out and they we’ve got to act now!” Where’s the Valium? Take a look at the global temps:
Hadley shows cooling over the last 10 years, even though CO2 emissions have been climbing!
The International Energy Agency (IEA) reports that CO2 emissions rose 1.6 billion tons in 2010, the highest since record keeping began. Total CO2 emissions last year were 30.6 billion tons globally, up 5% from the previous record set in 2008.
Face it – the record emissions are good news and are an indicator of global economic growth and vitality. That’s what normally happens when the economy grows – more energy gets consumed to do more work. Let’s hope the trend continues. Don’t worry, CO2 will not cause the temperature to go up that much. The science behind global warming and tipping points is JUNK.
Indeed the temperatures are not cooperating, and they are not about to for a couple more decades. Time to go back and redo the science.
Share this...FacebookTwitter "
"

Part III: Where does global warming rank among future risks to environmental health?

Guest essay by Indur M. Goklany 
NOTE: Entire 3 part series is now available as a PDF here

In Part 1 of this series we saw that even if one gives credence to the oft-repeated but flawed estimates from the World Health Organization of the present-day contribution of climate change to global mortality, other factors contribute many times more to the global death toll. For example, hunger’s contribution is over twenty times larger, unsafe water’s is ten times greater, and malaria’s is six times larger. With respect to ecological factors, habitat conversion continues to be the single largest demonstrated threat to species and biodiversity. Thus climate change is not the most important problem facing today’s population.
In Part 2 we saw that even if we assume that the world follows the IPCC’s warmest (A1FI) scenario that the UK’s Hadley Center projects will increase average global temperature by 4°C between 1990 and 2085, climate change will at most contribute no more than 10% of the cumulative death toll from hunger, malaria and flooding into the foreseeable future. It would simultaneously reduce the net population at risk of water stress.
Clearly, climate change would, through the foreseeable future, be a bit-player with respect to human well-being.
Here I will examine whether climate change is likely to be the most important global ecological problem in the foreseeable future.
As in Part 2, I will rely on estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs).
The following figure, which presents the FTA’s estimates of habitat converted globally to cropland as of 2100, shows that the amount of habitat lost to cropland may well be least under the richest-but-warmest scenario (A1FI), but higher under the cooler (B1 and B2) scenarios. Thus, despite a population increase, cropland could decline from 11.6% in the base year (1990) to less than half that (5.0%) in 2100 under the warmest (A1FI) scenario.  That is, climate change may well relieve today’s largest threat to species and biodiversity!
One reason for this result is that higher atmospheric concentrations of CO2 might make agriculture more efficient, and this productivity increase would not have been vitiated as of 2100 by any detrimental impacts of higher temperatures.

The next figure shows that in 2085 non-climate-change related factors will dominate the global loss of coastal wetlands between 1990 and 2085.

[In this figure, SLR = sea level rise. Note that the losses due to SLR and “other causes” are not additive, because a parcel of wetland can only be lost once. For detailed sources, see here.]
Thus we see that neither on grounds of public health nor on ecological factors is climate change likely to be the most important problem facing the globe this century.
So the next time anyone claims that climate change is the most important environmental problem facing the globe now or whenever, ask to see their proof that climate change outranks other problems.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9626ae4b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIf you support your enemies, then you ought not wonder why they grow stronger and soon defeat you.
The early results of the Bremen state elections are in, and once again the conservative CDU Party took one on the chin, and in the gut, and again the Greens soared, read here. The German state of Bremen will now be governed by a solid majority coalition of socialist Reds and environmental Greens. As for the conservative CDU party, it has tumbled behind the Greens to 3rd place, picking up only a measly 20% of the vote.
A similar election debacle took place just weeks earlier in the German industrial state of Baden Wurttemberg, where the Greens, who were once just a fringe party, swept to power after 60 years of conservative CDU rule. Angela Merkel’s conservative CDU party is now a collapsing house of cards.
Even the CDU’s coalition partner, the business-friendly FDP Free Democrats, have been reduced to a mere asterisk in the polls, falling well below the 5% hurdle and are now an insignificant political force.
Why are the Greens flying high and the Conservatives and Free Democrats plummeting?
To answer that question, it is helpful to play out a scenario in your mind. Imagine if the Angela Merkel’s conservative CDU one day adopted a new plank in its platform: “Jobs for Germans” and “Foreigners stay out!” What would be the result?
This would tantamount to a mainstream party endorsing and legitimising an extreme fringe ideology, an ideology that deserves defeat and not support. Such an endorsement however would be an immediate boost for Germany’s far-right brownish parties, who would be propelled and zoom in the polls overnight. The CDU on the other hand would deservedly go into a tailspin.
This is what happened with the CDU and the Greens. The Greens in Germany 15 years ago were a just minor party that struggled to reach the 5% hurdle in state and national elections. But over the years, the big parties like the conservative CDU began adopting and endorsing politically-correct green positions on energy and climate change rather than opposing them. The green political-correctness, they thought, would make them more appealing. The result: green fringe positions became viewed by the public as having been legitimised, and so people began to view the Greens as mainstream. Acceptance grew.
Today many people are green to a certain extent – it’s hip after all. So when going to the polls, why vote for the CDU when you can vote for the real deal: the Greens!
Successful politics is not about supporting the kooky fringe positions of your opponents and so legitimising them. No – it is about exposing them for what they really are, and then hanging them around the necks of your opponents and parading them through the public. But if you stupidly support the positions of your opponents, then you ought not wonder why they they keep getting stronger.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The CDU naively believed green voters would gravitate to their ranks if they adopted fuzzy green positions. But that didn’t happen. Indeed the opposite occurred in that the green positions looked more attactive, and so voters migrated to the Greens instead.
The same happened in the USA when Newt Gingrich gave the climate movement his stamp of approval by cozying up with Nancy Pelosi on a sofa. Republican Arnold Schwarzenegger did the same by adopting extreme green beliefs, and even became more green than a number of Democrats. In the end, along with a host of other RINOs, they only became extremely valuable useful imbeciles for the Democrats. The GOP paid a heavy price.
Endorsing the kooky climate positions espoused by Nancy Pelosi and the Democrats neither strengthened Newt nor the Republicans. To the contrary, it glorified the freak positions of the Democrats, who only grew stronger at the GOP’s expense.
“Thanks for the climate support Newt!” —- “Thanks for all the help Arnie!”
German useful political imbeciles for the Greens
Chancellor Angela Merkel took her endorsement of green to an extreme by having Hans Schellnhuber of the crackpot Potsdam Institute for Climate Impact Research (PIK) act as her close advisor on climate change. Merkel even once said that climate change is humanity’s greatest threat and challenge.  Merkel’s Environment Minister Norbert Röttgen (CDU) is also a huge proponent of going green – in order to save the planet.
What better way to make the Greens look like genuine heroes?
Today CDU leaders are scratching their heads wondering why the Greens have passed them in the polls. How much more clueless can one get?
Even the FDP’s new party leader Philipp Rösler has been exposed as a political punk with no qualms about prostituting himself and his FDP party to green industry lobbyists. Where is the FDP today? The party is reduced to just an asterisk in the polls – a political laughing stock. They can’t figure it out.
It’s your constant endorsement of the opposition, stupid!
Getting back to success
To get back to successful politics, the first step is to stop endorsing the absurd positions of your opponents. Then you have reject them and expose the fraud and the corruption that is the science of climate change, and the utter folly of controlling climate by fiddling with one single trace gas. Now is the time to expose the green filth, the junk science, the web of cronyism, and the dead-end it is all taking us to. The first party that does that will be the first to climb back to prominence.
Share this...FacebookTwitter "
"

If we want to understand the current advance of global capitalism, it is worth remembering that a liberal international economic order has actually arisen twice, both at the end of the nineteenth century and now, at the end of the twentieth. 1 In many ways, the world economy has simply caught up to where it was 100 years ago, prompting prominent economists to question whether the level of international integration is as high now as it was before the interruptions of two world wars and the Great Depression.



In a recent study, economists Michael Bordo, Barry Eichengreen, and Douglas Irwin ask whether globalization today is really different from globalization a century ago. 2 Answering that question can help us determine whether we are living in unprecedented times, whether the nation‐​state is becoming obsolete, and whether the new liberal international economic order promises to endure. Indeed, as various observers have noted, the mere fact that the first episode of global capitalism met such a cataclysmic end should force us to reflect on the current features of commercial, financial, and labor integration.



One area in which the world is decidedly less liberal than it was under the Pax Britannica is that of immigration. Although technological advances have made travel far more affordable and convenient than in the nineteenth century when restrictions on immigration were minimal or nonexistent, today hardly a country in the world–certainly not a rich country–does not have an array of labor and immigration regulations. As economist Deepak Lal convincingly explains, such restrictions on the movement of people exist today because citizenship concedes rights to the services provided by the welfare state. 3 Yet even as the welfare state has grown, so have migration flows. From 1965 to 1990, the foreign‐​born population rose from 75 million people to 120 million–with flows from poor to rich countries accelerating the most. The number of people emigrating to the United States has grown about ten times since 1945, for example, but as a percentage of the U.S. population immigration still represents only about one‐​third of its peak at the turn of the century.4



By contrast world merchandise trade reached its 1913 level by the 1970s.5 Global exports as a percentage of global output stood at about 12 percent in the early 1970s and has since risen to about 18 percent. U.S. exports as a share of the country’s economy are only slightly higher today at 8 percent than they were in the late nineteenth century. Including trade in services, however, the U.S. export figure rises to about 11 percent. Indeed, the rise of trade in services such as tourism, finance, insurance, and technical assistance has become far more pronounced today than it was previously. Thus taking into account world exports of both goods and services, global exports rise to about 23 percent of world output today. 6



Two other features distinguish commerce in the new liberal international economic order: the rise of the multinational corporation and the change in the composition of trade. Much trade today involves corporations “slicing up the value chain” and engaging in intra‐​industry trade. As a result, manufactured goods are increasingly exported from developing countries to developed countries. That contrasts sharply with the nineteenth century experience when countries on the periphery exported primary goods to rich countries, which exported manufactured goods made primarily in the center countries to the periphery. Bordo and his coauthors note, for example, that 80 percent of U.S. imports from Mexico are manufactured products while 100 years ago that figure was only 10 percent. And although U.S. exports of goods have not increased dramatically as a share of its economy, a much higher percentage of the production of the United States’ tradable goods is now exported. 7



**Financial Integration**



Is the world more financially integrated than it was in the past? A look at net capital flows suggests that the answer is no. The outflow of capital from Great Britain reached 9 percent of its GDP during the Victorian era with similar figures in Germany, France, and the Netherlands. No country today even comes close to those levels of net flows. In the 1990s, the average capital outflow for leading economies was slightly above 2 percent of GDP. 8



Other differences in capital market integration suggest that the world is indeed more globalized than ever. Gross flows of capital, at about $1.5 trillion per day are much larger than at any time in history, and much of that money represents short‐​term investment. Investors can today react at a moment’s notice to economic and political developments around the world in a market that offers a wide and growing range of international financial instruments. Investors, moreover, are almost equally putting their funds into equity instruments as into debt instruments, which predominated 100 years ago. At that time, international finance concentrated on funding certain sectors, principally railroads and government bonds. 9 Today, with more rapid and perhaps more reliable information about investment opportunities, international funds flow into virtually every sector of countries’ economies. In short, although net capital flows are not as large as those of the nineteenth century, gross flows are unprecedented in size, as are the extent and sophistication of capital markets, suggesting that financial integration is greater today than in the first episode of world capitalism.



 **The Role of Technology and Politics**



Has globalization come about because of political or because of technological change? Here again, Bordo and others suggest important differences between the two episodes of world capitalism. During the last century, technological changes clearly led to globalization. By the 1860s the political bases for a liberal international economic order were already in place. Great Britain repealed the Corn Laws and established its presence in China in the 1840s; it conquered India by 1857, and, with France, defeated Russia in the Crimean War by 1856. Contemporaneous and subsequent advances in technology led to a 40 to 50 percent drop in the cost of shipping in the latter part of the nineteenth century and early part of the twentieth. The transatlantic cable was laid in the 1860s; use of railroads and the telegraph proliferated; the Suez Canal was completed in 1869; and the radio telephone linked Europe and North America by 1900. Those and other innovations led to the first rise of world capitalism. 



Globalization today has benefited from technological improvements but has been almost entirely due to dramatic political changes. Countries around the world have lowered their trade barriers and opened their economies since the 1980s and especially so since the fall of the Berlin Wall. (That actually contrasts with the nineteenth century experience as nations were actually raising tariffs incrementally as the century wore on.) New technology such as air transport may have helped propel today’s globalization, but the role of such change in leading to globalization should not be overstated. On the other hand, technology in the information age may make it more difficult for politicians to reverse the course of world capitalism. 



Although globalization is often said to create inequality and economic volatility, historical evidence points, in fact, to economic convergence in living standards among countries that open their economies. Studies have shown tendencies to converge among countries in the Organization of Economic Cooperation and Development, among U.S. states, and among Japanese prefectures. 10 Significantly, economist Jeffrey Williamson found a decreasing gap in living standards among people living in countries participating in the international economy during both periods of global capitalism. “History offers an unambiguous positive correlation between globalization and convergence. When the pre‐​World War I years are examined in detail,” Williamson adds, “the correlation turns out to be causal: globalization played _the_ critical role in contributing to convergence.” 11



 **The Financial System**



The incidence of financial crises in Asia, Russia, and elsewhere in recent years has also often been treated as novel and as a consequence of globalization. Yet economists have examined the causes of the recent economic turmoil and have formed a consensus that the causes included pegged‐​exchange rates, government‐​directed credit, protected financial systems, moral hazard at the national and international level, and the lack of transparency in official accounts. Despite that consensus, the crises are used by critics of globalization to advocate _moving away from_ a more market‐​based system and toward _more_ interventionism. Thus India is cited as having followed more prudent policies than its East Asian neighbors since its more closed system allowed it to avoid succumbing to the regional financial crisis. The price it has paid for stability, of course, has been enduring poverty. By contrast, Hong Kong has had a volatile economic history but has become one of the wealthiest places on earth. Indeed, even after the financial turmoil, East Asian crisis countries are still eight to 15 times richer than India.



Financial crises also occurred in the first era of world capitalism. One common feature between the two eras is that banking and currency crises occurring at the same time tend to be more common in the periphery, or less‐​developed, countries than in the rich countries, where currency but not banking crises are more common. However, under the gold standard of the Victorian age, crises were resolved differently from how they are resolved under the current system of adjustable exchange rates based on fiat money.



One significant difference was that financial rescues 100 years ago were undertaken by the private sector, while today they are official, usually led by the International Monetary Fund. British Foreign Secretary Lord Palmerston summed up the attitude that prevailed for the rest of the century when U.S. states defaulted in the 1840s: “British subjects who buy foreign securities do so at their own risk and must abide the consequences.” 12 Largely as a result of that approach, economic recovery was more rapid in crisis countries than it is today and crisis countries then did not experience wealth losses as large as those experienced by crisis countries today. 13



 **Liberalism from above or Liberalism from below?**



The distinct institutional framework under which liberalization is taking place worldwide—including the prominence of supranational governmental organizations like the IMF, the World Bank, the World Trade Organization, and the United Nations, and the prominence of welfare and regulatory states—causes both enemies and proponents of globalization to attribute the market revolution to the efforts of those institutions, and to recommend that further developments be managed by international world bodies.



Yet the evidence indicates that such international organizations have at best been marginal to the globalization process and at worst have caused disruptions or delays along the way. Decades of World Bank and IMF lending to inward‐​looking regimes, for example, have certainly slowed the move to world capitalism. 14 Yet countries have _unilaterally_ undertaken economic restructuring, trade and capital account liberalization, and other policy reforms as past policies failed. That is even so of countries that have entered into multilateral free‐​trade agreements, like Mexico, which reduced its trade barriers for years before proposing the North American Free Trade Agreement. China, in its bid to join the WTO, is following the same course. Thus while aid agencies are likely to cause more harm than good in the globalization process (lending to Russia, for example), free‐​trade agreements such as the WTO are likely to be helpful. However, they serve more to preserve trade liberalization reforms than to promote them. 15



In short, the world economy has evolved as a result of changes coming from the national level rather than changes directed at the international level—what German liberal Wilhelm Röpke called an international order “from within and beneath” rather than the “false internationalism” that characterizes supranational organizations. The danger of the constructivist approach to achieving a liberal economic world order is that it may lead to discretionary and arbitrary use of power. Razeen Sally of the London School of Economics describes those hazards. 



Neoliberal institutionalists do not portray international policy coordination in the frame of limiting general rules at the international level that proscribe discretionary government action; rather, they think of it as an apparatus of complicated negotiations on particularistic policies intended to achieve specific results. This is the hallmark not of limited government under the Rule of Law, but of unlimited and discretionary government in an international public policy cartel, avoiding both domestic political accountability and market disciplines. In this context, international regimes are manifestations of government failure transplanted to the international level. Intergovernmental cooperation and international agreements, far removed from public scrutiny and the control of national legislatures and judiciaries, supply extra room for arbitrary activity by politicians and bureaucrats. They exacerbate the malaise of Big Government and political markets within nation‐​states. 16



We have already seen some of that dynamic at work. For example, through international forums, rich countries have pressured poor countries to adopt labor and environmental regulations that did not exist in rich countries at a similar stage in their development. Those impositions have come about against the wishes of developing countries and the vast majority of consumers in rich countries. 



Examples of arbitrariness and lack of transparency are amply provided by the IMF. For instance, the Fund does not tolerate the current account deficits of its member countries exceeding 4 or 5 percent of GDP even though large deficits are beneficial in many cases. Indeed, Australia, Canada, and Argentina had current account deficits greater than 10 percent for decades before 1913. The process by which the IMF decides the bailout amounts nations receive is also unclear. Why did the IMF put together a $57 billion rescue package for Korea as opposed to, say, a $30 billion package? We may never know the criteria or the rationales used in that case or many others. 



In the end, globalization may make such international bureaucracies irrelevant. And efforts to promote international liberalism from above may prove futile. In the meanwhile, we can come to some tentative conclusions. The world has seen global capitalism before; what is unprecedented is not globalization per se, but the extent to which the world is more globalized today than it was 100 years ago. That is especially so in terms of trade and finance. Moreover, no matter how much international agencies would like to take credit for the worldwide market revolution, those changes have emerged at the national level and have not been imposed from above. In that sense, the nation‐​state remains quite relevant. But a backlash against global liberalism is in fact more likely to occur if international agencies increasingly manage the world economy to the detriment of what poor countries consider most important, namely, economic growth.



Happily, one of the biggest differences between the two periods of world capitalism—the ideological environment—portends well for the 21st century. At the end of the nineteenth century, the wave of the future was socialism and its variants, which intellectuals considered held great promise for humanity. That belief system helped destroy the first era of globalization. Today, with socialism thoroughly discredited, basic liberal principles are generally accepted. That current climate of opinion does not make continued globalization inevitable, but it removes a major obstacle on the path toward prosperity that the world has recently resumed. 



**Notes**



1. Jeffrey Sachs and Andrew Warner, “Economic Reform and the Process of Global Integration,” Development Discussion Paper no. 552, Harvard Institute of International Development, September 1996, p. 5.



2. Michael D. Bordo, Barry Eichengreen, and Douglas Irwin, “Is Globalization Today Really Different than Globalization a Hundred Years Ago?” in _Brookings Trade Forum 1999_ , ed. Susan M. Collins and Robert Z. Lawrence (Washington: Brookings Institution Press, 1999), pp. 1–72.



3. Deepak Lal, “The Challenge of Globalization: There Is No Third Way,” in _Global Fortune: The Stumble and Rise of World Capitalism_ , ed. Ian Vásquez (Washington: Cato Institute, 2000), p. 29.



4. Julian L. Simon, _The Economic Consequences of Immigration_ (Ann Arbor, Mich.: University of Michigan Press, 1999), p. 28.



5. Paul Krugman, “Growing World Trade: Causes and Consequences,” _Brookings Papers on Economic Activity_ , vol. 1, 1995, p. 331 and International Monetary Fund, _World Economic Outlook_ (Washington: IMF, May 1997), p. 112.



6. _Economic Report of the President_ (Washington: Government Printing Office, 2000), pp. 306–7, 422 and International Monetary Fund, _World Economic Outlook_ (Washington: IMF, May 2000), pp. 203, 232.



7. Bordo et al.



8. _World Economic Outlook_ , May 1997, p. 114.



9. Bordo et al.



10. Sachs and Warner, p. 39 and Kevin H. O’Rourke and Jeffrey G. Williamson, _Globalization And History: The Evolution of a Nineteenth‐​Century Atlantic Economy_ (Cambridge, Mass: MIT Press, 1999).



11. Jeffrey G. Williamson, “Globalization, Convergence, and History,” _Journal of Economic History_ , June 1996, p. 277.



12. Cited in Harold L. Cole, James Dow, and William B. English, “Default, Settlement, and Signalling: Lending Resumption In A Reputational Model of Sovereign Debt,” _International Economic Review_ , May 1995, p. 369.



13. Bordo, et al., and Michael Bordo and Anna J. Schwartz, “Measuring Real Economic Effects of Bailouts: Historical Perspectives on How Countries in Financial Distress Have Fared With and Without Bailouts,” paper presented at the Carnegie Rochester Conference on Public Policy, November 19–20, 1999.



14. See Doug Bandow and Ian Vásquez, eds., _Perpetuating Poverty: The World Bank, the IMF, and the Developing World_ (Washington: Cato Institute, 1994).



15. Brink Lindsey, “Free Trade From the Bottom Up,” _Cato Journal_ 19, no. 3 (Winter 2000): 363. 



16. Razeen Sally, _Classical Liberalism and International Economic Order: Studies in Theory and Intellectual History_ (London: Routledge, 1998), pp. 196–97.
"
"

The national media have given tremendous play to the claims of Vice President Al Gore, some federal scientists, and environmental activists that the unseasonably warm temperatures of this past summer were proof positive of the arrival of dramatic and devastating global warming. In fact, the record temperatures were largely the result of a strong El Niño superimposed on a decade in which temperatures continue to reflect a warming that largely took place in the first half of this century. 



Observed global warming remains far below the amount predicted by computer models that served as the basis for the United Nations Framework Convention on Climate Change. Whatever record is used, the largest portion of the warming of the second half of this century has mainly been confined to winter in the very coldest continental air masses of Siberia and northwestern North America, as predicted by basic greenhouse effect physics. The unpredictability of seasonal and annual temperatures has declined significantly. There has been no change in precipitation variability. In the United States, drought has decreased while flooding has not increased. 



Moreover, carbon dioxide is increasing in the atmosphere at a rate below that of most climate‐​change scenarios because it is being increasingly captured by growing vegetation. The second most important human greenhouse enhancer — methane — is not likely to increase appreciably in the next 100 years. And perhaps most important, the direct warming effect of carbon dioxide was overestimated. Even global warming alarmists in the scientific establishment now say that the Kyoto Protocol will have no discernible impact on global climate. 
"
"
Share this...FacebookTwitterIn the wake of the massive earthquake and tsunami that devastated Japan, a tsunami of hysteria has swept the German Green Party to a stunning victory in elections in the state of Baden-Württemberg.The Greens also piled on in the Rhineland Palatinate elections as well. One thing is clear:  Angela Merkel’s coalition government with the FDP liberal democrats were punished for their horrendous handling of energy policy in Germany.
One-year ago Merkel and her government approved operating lifetime extensions of nuclear power plants in Germany, giving 8 older reactors the seal approval, certifying they were safe and reliable. That, they were. Indeed German reactors and their management are among the best and safest in the world.
But then came Fukushima, followed by the tsunami of panic generated by an irresponsible media, anti-nuclear activists and opportunistic Greens, who fanned the flames of hysteria with vigor.
But rather than standing up and putting her full faith in German nuclear reactor technology and management, Angela Merkel panicked, blinked and turned her back on the industry, ordering the shutdown of the 8 nuclear reactors that were built before 1981. By turning her back on the nuclear industry, she in the end will have turned her back on the country. The days of cheap, competitive and reliable energy are coming to an end in Germany.
Turning her back on the nuclear industry also made her government’s earlier approval of the reactors appear like a farce. Angela Merkel only succeeded in showing where her priorities are – political survival. Her stance with regards to the Libya war was also pure political calculation.
Now it’s time to pay the piper. Angela Merkel’s government is now collapsing faster than Japan did under it’s 9.0 March 11 earthquake. Rarely does anyone witness ineptitude of this magnitude. She deserved to fail. All of this now clears the way for the SPD socialists and environmental Greens to sweep into power in 2013 in the national elections.
This all clears the way for the German renewable energy experiment, as the reds and the greens are eager to go full throttle with the renewable energy madness, which in the end will make Germany vulnerable and dependent on its neighours. Thanks in part to Merkel, the way is clear to embark on something that has never been tried: an experiment to see if it is possible to power an industrialised and developed country without nuclear and carbon-based energy.
Share this...FacebookTwitter "
"Greta Thunberg, the 16-year-old Swedish climate activist, is calling for system change. At a press conference in Brussels, she told the European Commission that in order to fight climate change we need to change our political and economic systems – a message that has been repeated on signs and in chants in the student climate strikes around the world. The school climate strikes, which she started alone in August 2018, have become a social movement with 1,659 strikes planned for March 15 in 105 countries. But what is system change? How do entire systems change? When we see “save the planet” initiatives, they often look like individual decisions that don’t cost much, like switching to a bamboo toothbrush or washing containers before you recycle them. By all means, do these things, but don’t confuse them with system change. Most people don’t know how to change political, economic and social systems. They end up making token gestures instead that may even perpetuate the problem. There’s also the question of how to overcome powerful vested interests that benefit from the current system. But there is research that can help us understand system change. Neo-institutional theory is one approach to understanding how and why people organise collectively. People create meaning, follow rules and reproduce structures – such as classrooms, businesses, offices and community halls – based on assumptions of what is right and proper. Classrooms look similar, not because each time we set one up we rationally decide how to do so, but because we make assumptions about what a classroom is supposed to look like.  Because we are part of these meaning structures, we reproduce existing norms and beliefs and resist change. System change happens when we don’t take our assumptions for granted, which allows more and more people to question the status quo. Thunberg is telling us that our current political and economic systems are no longer fit for purpose. She is pointing out that the emperor has no clothes.  Changing a system takes time. My research on the LGBT movement in Ireland documented efforts and achievements over 40 years. Homosexuality went from being a crime, to being celebrated in a progressive movement. While the referendum on marriage equality took one day in 2015, the efforts of many to change the system took decades. The Three Horizons Framework can help explain the different factors that lead to changing systems. Horizon one is business as usual – the status quo – and the outgoing institution in times of change. Horizon three is the new institution – with newly legitimised structures and beliefs. The space between them is horizon two, which is occupied by people focused on social change – who lead the transition from an old system to the new.  Most people recognise the problems with the present system and want to help society move to something more sustainable. Products like bamboo toothbrushes exist to monetise that concern, but because they’re sold in plastic and shipped around the world, their production and distribution still consumes fossil fuel and does nothing to change the existing economic or political system that is fuelling climate change. A collective challenge to political and economic elites is likely to be more effective in forcing this transition.  When aspects of horizon three appear – glimpses of a more sustainable system – they are usually rejected as illegitimate or too radical. When Rosa Parks sat down at the front of the bus in a move to promote civil rights in America in 1955, she was condemned. Looking back after system change has happened, these people are seen as leaders. The system that needs to be changed to avert climate disaster is capitalism, which is losing its legitimacy largely due to the system’s failure to respond effectively to climate change. Applying all I’ve learned about how systems change, it’s possible to imagine that the current system which sustains business-as-usual capitalism – horizon one in the framework – is occupied by those who continue to produce, sell and consume products and services that rely on fossil fuels. That’s most of us, but horizon one is also maintained by climate deniers and investors in fossil energy, who, despite the scientific evidence, keep chugging along.  A more sustainable system could include policies we might currently consider “extreme”, like universal basic income. This is a guaranteed payment for all people regardless of their wealth which could help break the cycle of production and consumption that pollutes the atmosphere and fills the ocean with discarded plastic. Evidence suggests there is growing support for this, particularly among young people. Extending human rights to non-humans and even to ecosystems is another idea that seems radical today but is gaining traction and could define an alternative system in future. One thing is for sure, we’ll look back in horror one day at how humans treated the natural world, as many already do in the present. If the climate strikers can continue to grow their movement and sustain momentum, their leadership could be an important part of society’s transition to a more sustainable system in horizon three. Capitalism may seem permanent, but research shows that systems inevitably change over time, and are ultimately created and reinforced by us. But in order to change anything, people must question their own role in the system first. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

If you’ve watched any television in your lifetime, chances are you’ve seen more than a few beer ads. In fact, some of the most memorable advertisements in the history of the medium have been produced by beer makers, as they vigorously compete for customer allegiance. It’s just another part of doing business for beer companies, which depend on TV ads to build brand name recognition.



But if you’re a consumer who enjoys other spirits besides beer, you might be wondering why you never hear anything on TV about your favorite brands, or even competing liquor products. The reason you do not is because, for the past 50 years, the spirits industry has lived under a voluntary ban on the placement of liquor ads on TV. But as revenues have declined gradually over the past two increasingly health‐​conscious decades, the industry has rethought the wisdom of the ban and began cautiously testing the regulatory climate by placing ads on some local TV or cable stations. The debate over the wisdom of this reversal has been heating up nationally since NBC announced recently that they would allow liquor commercials to run during late‐​evening programming, making them the first national network to do so.



Not surprisingly, a lot of social do‐​gooders are up in arms over this and are demanding that federal policymakers take action to halt the practice. NBC “is shirking its public interest responsibility as a broadcaster by putting its bottom line ahead of the health and safety of young people,” says George A. Hacker, director of the Alcohol Policies Project at the Center for Science in the Public Interest. And Joseph Califano, director of the National Center on Addiction and Substance Abuse at Columbia University, told _The Wall Street Journal_ last week, “The only solution now is for federal regulation, just as we have federal regulation prohibiting tobacco ads on television.”



From a public policy perspective, the fear seems to be puritanical in character: If people see booze ads on TV, they will drink more. Such post hoc reasoning could be challenged on a number of grounds. Specifically, it is difficult to believe that Americans are a mindless herd of robots who will make a mad dash to their local liquor stores just because they see a few TV ads. In fact, Dr. Morris E. Chafetz, president of the Health Education Foundation and author of _The Tyranny of Experts_ , argues that “the claim that advertising can lead anyone down the bottle‐​strewn garden path not only to drink alcohol but to abuse it, is pure hokum.” In the mid‐​90s, Dr. Chafetz conducted a review of academic research for the _New England Journal of Medicine_ on the question of how advertising affected alcohol use. His conclusion: “I did not find any studies that credibly connect advertising to increases in alcohol use (or abuse) or to young persons taking up drinking. The prevalence of reckless misinterpretation and misapplication of science allows advocacy groups and the media to stretch research findings to suit their preconceived positions.”



So even though academic evidence suggests that exposure to advertising is unlikely to increase consumption, liquor companies are still willing to run ads, perhaps in an attempt to build brand recognition or attract beer and wine consumers. The question is, is there anything wrong with that?



The answer, of course, is all a matter of personal opinion. In a free society, however, people should be at liberty to make such choices without government entering the picture. Adults should be responsible for their decisions in this regard and they should exercise authority over their children until they reach an age when they can be trusted to make such decisions on their own. Employing the old “It’s about the children!” defense to support an ad ban doesn’t make sense for other reasons. As my Cato Institute colleague Doug Bandow noted in a 1997 _Wall Street Journal_ editorial, “almost every good advertised on the airwaves may have some inadvertent adverse effect on the young,” whether it is cars, riding mowers, high‐​fat food, or computers. “But that’s no excuse for banning ads,” concludes Bandow.



Moreover, children can see liquor ads in magazines and newspapers too, so should we ban liquor ads in print? Which raises another important question: Why is it that we continue to tolerate an artificial regulatory distinction between print and electronic media? For decades, policymakers have imposed the equivalent of second‐​class citizenship on electronic media (television, radio) in terms of First Amendment protections. Unlike their print counterparts, which receive substantial free speech protections, electronic media face numerous speech restrictions that would be unthinkable for newspapers or magazines. So the next time you see a newspaper editorializing about the need to ban liquor ads on TV, fire off a letter to the editor and ask them how they’d feel about a federal ban on all those liquor ads that appear in the paper’s pages and provide them with substantial revenues.



Anyway, a federal ban on televised liquor advertising would probably not pass First Amendment muster today. In the important 1996 decision _Liquormart, Inc. v. Rhode Island_ , the Supreme Court struck down a Rhode Island ban on the advertisement of retail liquor prices outside of the place of sale since such a blanket prohibition against truthful speech about a lawful product betrayed the First Amendment. As Thomas A. Hemphill, a fiscal officer for the New Jersey Department of State, noted in _Regulation_ magazine in 1998: “That landmark decision makes it much more difficult for legislators to restrict truthful commercial speech, thus establishing a precedent for more stringent evidentiary requirements underlying future advertising regulations. Therefore any new law that imposes a comprehensive ban on television or radio liquor commercials will probably not survive First Amendment judicial review.” The Court bolstered this line of reasoning in the subsequent 1999 decision _Greater New Orleans Broadcasting Assn., Inc. v. United States_, which declared that the FCC could not ban casino advertising in states where gambling was legal. The Court declared, “the speaker and the audience, not the Government, should be left to assess the value of accurate and nonmisleading information about lawful conduct.” These decisions also suggests that the Court may finally be getting serious about affording commercial speech the same protections granted to political speech, a move that is long overdue.



A final concern about a federal regulatory response to TV ads relates to its potential applicability to the Internet. As television and the Internet increasingly converge and more Americans gain access to broadband connections, it is likely that more and more television programming will be made available over the Net. So any ban on liquor ads on TV would likely have threatening implications for Internet Webcasting in the long run.



In conclusion, there has never been any logic behind the artificial distinction between liquor and other products, such as beer and wine, when it comes to promotional activities. Alcohol is alcohol. Why should the form in which it is delivered change its legal status? And why place advertising restrictions on _lawful_ products at all? If someone was trying to sell crack cocaine or cruise missiles on TV, it might make for a more interesting debate. But alcohol is a legal product that manufacturers have every right to promote. Policymakers need to take a sober look at these realities before they rush headlong into needless and unconstitutional restrictions on liquor advertisements on TV.
"
"**More than one-third of jobs in arts, culture and heritage are vulnerable as a result of coronavirus restrictions.**
That is according to a new study from Ulster University's Economic Policy Centre (UUEPC).
The report suggests a high proportion of jobs in museums, galleries, theatres and music are particularly at risk.
In general, the creative sector is ""more exposed to the challenges arising from Covid-19 than other sectors and occupations.""
The main factors for that are social distancing measures limiting capacity and a reluctance among audiences to return even when venues can re-open.
The UUEPC report estimated that there are 39,100 people employed in the arts, creative, culture and heritage sector in Northern Ireland.
However, the type of occupation that figure includes is very wide.
It ranges from people working in Information Technology (IT) and architecture to those employed in music, crafts, the performing arts, museums, galleries and libraries.
The study said that while a high proportion of jobs in areas such as IT were not vulnerable, jobs in the other sectors were ""much more at risk.""
It estimated that more than 60% of jobs in museums, galleries and libraries were vulnerable, along with almost half of jobs in music, theatre and visual art.
A significant number of jobs in film and TV production were also at risk.
""The pandemic has caused the immediate closure of non-essential business including the Arts, Culture and Heritage industries resulting in cancelled work and events such as large music events like Belfast Vital and Belsonic which attracted thousands of people to Belfast annually,"" the report said.
""The healthcare situation in NI will be more important in this sector than in the average NI occupation, given the interactive nature of work and a dependence on discretionary consumer spending.""
The authors of the report also make a number of suggestions on how venues and visitor attractions could be helped to recover from the impact of the pandemic.
They include ""visitor vouchers,"" which would subsidise 30-50% of the cost of tickets to venues to encourage audiences to return.
Venues would also be compensated if they had to cancel events at short notice due to new or changing restrictions.
The authors of the study also suggest a bursary of Â£1,000 a month for arts workers who have not been able to benefit from other job support schemes.
Northern Irish artists could also be commissioned to create new public art, the UUEPC report said.
A number of emergency funding schemes for arts and heritage have been opened by the Department for Communities.
The Northern Ireland executive received Â£33m from Westminster in July as part of a UK-wide support package for arts and culture."
"

In an election that promises to be close, command and control of the agenda–from “rats” to prescription drugs–is increasingly important, and timing is paramount. Proof is that in each of President Clinton’s victories, “October surprises” nudged the polls enough to turn squeakers into decisive victories. What’s coming this year, befitting the veep’s proclivities, is the first election year “environmental surprise.”



Those with foresight can already spy it along the planet’s limb. NASA scientists are leaking that Antarctic ozone depletion, which conveniently reaches its seasonal nadir around Oct. 20, is proceeding faster than ever. This doesn’t guarantee record depletion this year (in some years depletion has commenced rapidly, only to fizzle), but it certainly increases the chances. And the fact that NASA is already promoting lurid graphics–like the one that appeared in the Washington Post on Sept. 11–means, as usual, that its political antennae are up.



This is the same agency that discovered life in Martian rocks (later shown to be wrong) right at budget time. Now NASA is betting on Gore, who through his years in the Senate rewarded the space program handsomely. In a 1992 budget hearing, Gore announced that global warming should be “NASA’s number one scientific priority,” and the agency has been cashing the checks ever since.



Along about the time ozone bottoms out, the administration is going to release its “National Assessment of Global Warming,” which will forecast hell and damnation for us in coming decades unless we dramatically reduce our use of fossil energy–coal, oil and natural gas. How hard will it be for Gore to play this for political advantage? Can you hear the rhetoric? “The most eminent scientists in the nation all predict environmental disaster unless we curtail our use of oil, and my opponent’s largest contributors are the corporate polluters who created this problem,” Gore will intone.



Right about then, NASA will announce that the ozone hole has grown so large that it threatens the people of Chile, Argentina and South Africa, including the three million residents of Cape Town.



But how could the ozone hole still be growing, given that production of the putative cause–chlorofluorocarbon refrigerants (CFCs)–was stopped almost 10 years ago by a treaty called the Montreal Protocol? NASA will note that the rules of physics require that surface warming caused by increased greenhouse gases must be accompanied by a cooling of the stratosphere, and the colder it is up there, the more that ozone is depleted. NASA is itching to marry the distant cousins of global warming and ozone depletion before election day.



So what’s wrong with a little public service that just happens to occur at the same time the Clinton administration releases its global warming national assessment?



It turns out that the assessment is based on two computer models that simply do not work. When asked to simulate how the U.S. climate should have changed in the 20th century as a result of greenhouse gases, the computer models do worse than a table of random numbers applied to the problem.



Continuing to use these models to drive a forecast of national Sturm und Drang violates the number one ethic of science: If your theory doesn’t stand the test of the facts, it must be either changed or abandoned. What about oil and global warming? Recently, James Hansen, Gore’s original guru on climate change, said that reducing fossil fuel use is an expensive proposition that will do little to reduce global warming in coming decades. Instead, he argued, we ought to concentrate on other emissions whose control is not so economically detrimental.



Imagine the cost if we had rushed to do what Gore proposed in his book Earth in the Balance. Gas prices would be as high as they are in Britain, where we have witnessed the first riots created by global‐​warming taxes and where we may also witness the fall of a government because of unpopular global‐​warming policies. But that would be the second government to collapse. Last spring, the ruling coalition in Norway went under because it was against building two power plants, on the grounds that they would contribute to global warming.



Ironically, Hansen also works for NASA; but don’t expect to see him on television in October. Nor can we expect to see NASA’s own calculations show that ozone depletion will attenuate in coming decades because of the Montreal Protocol. Instead of these truths, look for environmental gloom and doom to dominate the end of October. And just how will Bush respond, as the clock winds down?
"
nan
"
Share this...FacebookTwitterWhat follows in English is a summary version of a piece appearing here at the European Institute for Climate and Energy (EIKE) based in Germany, written by retired meteorologist Klaus-Eckart Puls.
=================================
PIK report: “Sea level rising fastest in 2000 years” turns out to be a quack! Data shows no change!
Sea levels are now rising faster than at any time in the last 2000 years claims a new hockey stick manufactured by Michael Mann and Stefan Rahmstorf. But that claim has already turned out to be bogus.
As nobody cares much about so-called climate change anymore, the Potsdam Institute For Climate Impact Research (PIK) had to come up with another scare story: rapidly rising sea levels. That claim is supported by a whopping 2 (cherry-picked) North Carolina coastal sediment cores, which the authors claim reflect sea level behavior for the entire globe.  Other scientists have already poured cold water on the paper, like Jens Schröter of the Alfred Wegener Institute, who says Mann’s and Rahmstorf’s paper is “unsuitable for making predictions”.
The opposite is the reality
The new predictions of catastrophe are not based on actual MEASUREMENTS. Actual measurements made by coastal tide gauges and satellites show the opposite is likely happening, i.e. sea level rise is actually decelerating. Presented are 7 datasets that contradict the latest Mannian hockey-stick fantasy.
(1) The US-Coastal Journal reports that sea level rise rate is clearly slowing down – based on tide gauge measurements, full publication here:

It is essential that investigations continue to address why this worldwide-temperature increase had not produced acceleration of global sea level over the past 100 years, and indeed why sea level has possibly decelerated for the last 80 years.”
(2) EUMETSAT recently made public the GLOBAL sea level data/measurements. Result: No trace of an acceleration! See the following graphic:

3) The GFZ Potsdam reached the same result, showing that there is no global uniform trend. Moreover, many locations show a huge sea level drop!

(4) Norderney and Cuxhaven German coastal locations have records going back over 100 years, and so does the NLWKN Lower Saxony State Association and the state of Lower Saxony for the North Sea coast. These MEASUREMENTS too show no acceleration in sea level increase (text translated below):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The text in English:
NLWKN (Annual Report 2005)
“All the discussions and horror scenarios for nothing:
There is no scientific basis for a massive increase in sea level by 2100. The NLWKN has an objective witness for saying this: the tide gauge of Nordeney. It provides a consistent recording of the water level for 100 years. From this data series you can read it: The increase for the time period from 1906 to 2005 is exactly 24.3 cm.
The state government of Lower Saxony:
“Climate change is not detectable:
The state government sees no signs of an increasing sea level at the North Sea coast as a result of climate change. Also the trend of more frequent storm surges is not detectable, says Minister of the Environment Hans-Heinrich Sander (FDP) in the state parliament. The trend remains unchanged at 25 cm per century. A more rapid increase is not observed.”
It needs to be pointed out that since the last Ice Age, North Germany’s coastal land is sinking, while Scandinavia’s is rising.
5) Works from the Institute for Historical Coastal Research Wilhelmshaven also show completely different results from those of PIK and Mann:

The above chart shows that for the period of 1600 to 2000 sea level rose on average 35 cm/century. But from 1900 – 2000, it rose only 25 cm. That is a clear deceleration. Afterwards: The sea level in the recent years has risen even more slowly than in the 20th century! …there is no trace of any acceleration whatsoever.
(6) A NASA team of authors recently published a report that clearly highlighted two points:
(a) No acceleration in sea level exists,
(b) The sea temperature shows a declining trend,
consequently there exists no thermally accelerated sea level increase:
Blue line: global sea level as to AVISO,
Red line: Sea surface temperature since 2004 ARGO (3000 bouys).
(7) Even the IPCC came to the same conclusion that there is no acceleration. With each report the projected sea level for the year 2100 was revised downwards every time – now it is projected to be only 40 cm:
IPCC prognoses: Step-by-step returning to reality.
Finally, when one takes into account that the PIK and the Mann author-team are known worldwide as alarmists, e.g. Michael Mann (hockey stick inventor), Stefan Rahmstorf (PIK) et al., then considerable doubt on the credibility of this doomsday paper is in complete order.
A recent critique of the sea level alarmism can be found here.
Klaus-Eckart Puls – EIKE
(Translation/editing by P. Gosselin)
=======================================================
Share this...FacebookTwitter "
"**The world's largest maker of latex gloves will shut more than half of its factories after almost 2,500 employees tested positive for coronavirus.**
Malaysia's Top Glove will close down 28 plants in phases as it seeks to control the outbreak, authorities said.
The company has seen a huge surge in demand for its personal protective gear since the start of the pandemic.
However, there have been concerns about the working conditions of the low-paid migrant workers it relies upon.
On Monday, Malaysia's health ministry reported a sharp rise in Covid-19 cases in areas where Top Glove factories and dormitories are located.
Nearly 5,800 workers have been screened so far with 2,453 testing positive, it said.
Top Glove operates 41 factories in Malaysia, with many of its workers coming from Nepal and living in crowded dormitory complexes.
""All those who tested positive have been hospitalised and their close contacts have been quarantined to avoid infecting other workers,"" Director-General of Health Noor Hisham Abdullah told Reuters news agency.
It is unclear when the factory closures will begin but they are scheduled to take place in stages.
Top Glove has been in the global spotlight for its record high profits this year, but also over allegations of exploitative labour practices at the firm.
In July, the United States banned the import of gloves from two of the company's subsidiaries following forced labour concerns.
A recent report from the US Department of Labour raised the same issue, pointing to the high recruitment fees overseas migrant workers must pay to secure employment in the rubber glove industry which often results in debt bondage.
In September migrant workers told the Los Angeles Times about difficult working conditions at Top Glove factories, describing 72-hour work weeks, cramped living conditions and low wages.
A few weeks later, Top Glove said it had raised remediation payments to compensate workers for recruitment fees after recommendations from an independent consultant.
Glorene Das, executive director of Tenaganita, a Kuala Lumpur-based NGO that focuses on labour rights, said some Malaysian firms that depend on a migrant workforce were ""failing to meet the basic needs of their workers"".
""These workers are vulnerable because they live and work in congested shared quarters and do work that does not make it possible to practice strict social distancing,"" she told the BBC.
""During these times employers have a huge responsibility towards them but we are hearing of cases where they are not providing workers with sufficient food or even withholding their wages,"" she added.
Shares in Top Glove fell by 7.5% on Tuesday after the factory closures were announced. But despite the slump the company's shares have surged over four fold this year, reported Reuters."
"

Whoever is elected president, global warming legislation is going to be passed in Washington next year. 



Legislation proposed by both John McCain and Barack Obama will require that the cost of energy to become so high that people will avoid using it. The serious question is: why would we do this in the current economic environment? Why would we take away capital that people would otherwise use to invest in companies that produce efficient things when that capital is already being destroyed at an alarming rate?



Other nations that embraced the abject environmental failure known as the Kyoto Protocol and imposed higher energy costs are fleeing from climate change policies as their economies implode. Only the U.S. seems eager to commit economic suicide over global warming. 



Kyoto did nothing measureable about climate change. Global carbon dioxide emissions rose by the same amount they were supposed to fall because of it. All it cost was money. Germany ‘s Chancellor Angela Merkel, who is probably the woman most responsible for the Protocol itself, now calls drastic cuts in carbon dioxide emissions, “ill‐​advised climate policy”. Her foreign minister, Frank‐​Walter Steinmeier, who last year trotted the globe pronouncing global warming a grave threat to world peace, now says that “this crisis changes priorities” and that “interest in protecting the climate will change because of such a crisis”.



A trip around the world (or around the country, or, for that matter, around your city) will demonstrate that economic vitality and environmental protection are highly correlated. The ritzy part of town is neat as a pin, where residents smugly buy (unverifiable) “carbon offsets” to assuage guilt about the four‐​wheel‐​drive behemoth, while bathed in compact‐​fluorescent light. In the poor neighborhoods of the world? Well, they’re cooking indoors with wood or dung, they don’t have a clue what a “carbon offset” or a compact‐​fluorescent is, and the power is out.



As economies suffer increasingly from global warming taxes and regulation, nations can descend from first‐​world energy infrastructure and supply to banana‐​republic like conditions, even without the current economic contraction. 



The first place where this hell is likely to freeze over is going to be in Great Britain this winter. Residential energy costs average $600 per year over where they were a year ago. Britain’s National Housing Administration estimates that 5.7 million British households will spend more than 10% of their income on fuel and energy next year. 



Right now, wholesale power prices in Britain are four times what they are in France. Older coal and nuclear power plants have to be taken out of production for repair and refit. How do energy‐​intensive industries, such as cement, steel, and brickmaking compete in such an environment? They don’t. 



Green policies are sure to make this much, much worse. In large part because of European Union environmental directives, a full 37% of the U.K.‘s electrical generation capacity will be lost by 2015, most of that from mandatory reduction of coal‐​fired plants. Imagine what percent of households will be spending 10% or more of their income on energy 2015. 



Nor will the shortfall to be taken up by solar energy and windmills. Britain is a pretty cloudy place, it isn’t all that big, and windmills produce no power when there is no wind. Last month, Cambridge Econometrics projected that less than 5% of Britain’s total energy will come from these so‐​called “renewables” in 2020.



Before the current financial uncertainty, European governments and the EU environmental bureaucracy thought they could get away with all of this expensive unreality. But, as Angela Merkel and her Foreign Minister now admit, this is beginning to seem “ill advised.”



All of this flags a much larger problem. The only way to reduce emissions enough to have a significant effect on our modest warming trend is to make energy so expensive that people can’t afford it. But, as the current economic situation shows, when people can’t afford it, these policies become “ill advised”. Among other reasons, they are not advisable because they take away capital that is necessary for environmental protection. 



The solution is obvious. Only when technologies are available that produce lower carbon dioxide emissions at a competitive price, will people and politicians really buy in. This requires investment—by individuals—of real money that is currently being confiscated and tilted at windmills. Expensive energy and a financial contraction can only delay this investment, perhaps forever. The United States and the United Kingdom would do well to pay attention to Germany’s newly‐​found realism about global warming policy. 
"
"
Share this...FacebookTwitterIt was supposed to be USA’s Fukushima – an environmental disaster of Biblical dimensions – one that would serve as a watershed in USA’s energy policy development. The nation would now finally start to wean itself off oil and switch to renewables in earnest. When the BP Deepwater Horizon drilling platform exploded, killing 11 workers, and began releasing million of gallons of crude oil into the Gulf of Mexico, outrage spread across the USA and soon worldwide.
Thanks to brave men (and nature) there is little trace of the Deepwater Horizon tragedy today. Thanks to the workers who risked life and limb in doing their jobs honourably. They will not be forgotten. (Photo credit: US Coast Guard)
Like Angela Merkel declaring a moratorium on nuclear energy in Germany after Fukushima, President Barack Obama also declared a moratorium on offshore oil drilling. Indeed it looked as if Deepwater Horizon was the environmental disaster that environmentalists had been waiting for. Environmental horror stories spread like wildfire. Calls to get away from oil grew shrill.
But fortunately for the environment (and unfortunately for the environmentalists) the Deepwater Horizon disaster turned out to be all hype. Today, just a year later, it is difficult to find any of the spilled oil. Within just a few months the true scale of the spill became known, read here, for example. When compared to the shear volume of the Gulf of Mexico, the 800 million liters of spilled oil turn out to be comparable to a few drops of oil in an Olympic swimming pool. Indeed concentrations of greasy, yukky sunscreen lotion in a public pool are much higher.
Now scientists have discovered that bacteria (nature) are devouring the crude. It’s almost all gone.
The German online FOCUS magazine has written the epitiaph on the gravestone of the “Deepwater Horizon Disaster”, which actually died at birth. Focus writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Even the optimists among the biologists were surprised at the speed at which microbes in the water gobbled up the oil in the water.
‘We  misjudged the Gulf ecology’s capability and bacterial ability to process hydrocarbons’, emphasized William Reilly, a leader of the investigation commission that analyzed the oil disaster for US President Barack Obama’s government.”
FOCUS reminds us that “oil spills” are also a natural ongoing phenomena.
160 million litres of oil leak out each year from hundreds of natural fissures in the ocean sea floor. It’s residence time according to studies is anywhere between 10 hours and 5 days. The microbes work so effectively that only a fraction of the oil reaches the sea surface. Obviously crude oil-eating bacteria got along amazingly fine with the oil from Deepwater Horizon.”
Today the stuff is gone, eaten by nature. People are bathing at the beach, and life goes on.
Let’s recall that tens of thousands of underwater volcanoes globally spew out all sorts of nasty things into the oceans. One quickly realises that Mother Nature is in fact a long way from being clean and gentle. When you get down to it, Mother Nature is a cocktail of physical, biological, and chemical weapons – she is always trying to kill us. She is not this silly paradise of a picture that Google uses for Earth Day.
Happy animals living in a tropical paradise with no disease or danger. This is what the planet would be like if we switched off all the lights. 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterPeople are discussing the instructions made by CERN Head Rolf-Dieter Heuer not to interpret the results of the CLOUD experiment on cloud formation, read here.
What follows is a translation of the relevant text in the article that appeared in DIE WELT here.
Welt Online: Also the results of the so-called Cloud experiment are being awaited with much excitement, where cloud formation is being researched. These results could indeed be important for understanding global climate change?
Heuer: Indeed this is about better understanding cloud formation. In nature there are many parameters that influence this, among them temperature, air humidity, aerosols and cosmic rays. The Cloud experiment examines the effects of cosmic rays on cloud formation. The rays for studying this come from the accelerator. And inside the experimentation chamber it can be examined under controlled conditions how the formation of droplets depends on radiation and floating particles. The results will be published soon. I’ve requested my colleagues to clearly present the results, but not to interpret them. That would mean immediately entering the highly political arena of climate change discussion. You have to be clear that concerning cosmic rays it is only one parameter of many.”
Personally I agree with Dr. Heuer. CERN is not a climate research institute, and so the interpretation of results (with regards to impact on climate) ought to be left to experts of atmospheric and climate sciences. Last I heard is that CERN is a research institute for particle physics, and not climate. And we’ve all seen what happens when other kinds of physicists start acting like climate experts and start modelling future climates.
If they do not wish to interpret the results, other scientists from the appropriate fields certainly will. And if they don’t, the bloggers will.
Heuer also admits that climate science is highly politicized, and so one can’t blame them for not entering this poisoned arena. After all, telling the truth would mean a cut-off in funding. Finally it doesn’t hurt to remind some us that a vast number of parameters are involved in cloud formation and climate change, and that it doesn’t all get boiled down to a single dominant factor (like CO2).
Thanks for reminding us of that, CERN.
Share this...FacebookTwitter "
"We are living through a period of unprecedented environmental breakdown which is increasingly being referred to as “the Anthropocene”. As the term becomes more and more pervasive, I want to explain why, as a psychologist and a committed environmentalist, I think it is a highly problematic way of framing our predicament.  Originally proposed by atmospheric scientists and then geologists, the Anthropocene has come to the fore as a powerful if perplexing way of talking about our current era. This is a period in which, for the first time in its history, the Earth is being deeply transformed by one species – humans. The word Anthropocene refers to the idea that the Earth’s geological record has been transformed by humanity: Anthropos is Greek for human and -cene is a substantial geological time period within the current 65 million year old Cenozoic era.  It is remarkable how quickly this idea has become ubiquitous. It is now the subject not just of academic texts and conferences, but art, fiction, magazines, travelogues, poetry, even an opera.  While I agree that this is an important and timely provocation, I want to pause here for a moment, and consider whether the Anthropocene narrative really does capture our predicament and our prospects.  There is already plenty of criticism of the Anthropocene idea. Alternative terms like Capitalocene (which attempts to highlight the detrimental forces of capitalism), and Plantationocene (which emphasises the role of colonialism, the plantation system and slave labour) have been offered as a way of doubling down on the elements of human history responsible for environmental crises, rather than lumping all humans, and their responsibility, together. But I want to concentrate on the idea of time itself. “Deep time” is the concept of geological time that is used “to describe the timing and relationships between events that have occurred throughout Earth’s history”. That’s a 4.54 billion year history. We struggle to grasp the huge scale of a sense of time that is so, well, deep. There are numerous analogies for helping us comprehend this enormity, like the 24-hour clock – that humans have only been on the planet for 19 seconds of it. I like the one below, as you can visualise it simply enough by holding out your arm.  If the Earth formed about 4.54 billion years ago at the shoulder, animals of any kind appear within the palm, and more familiar (to us) lifeforms originate at the first knuckle. Movement along the fingers represent the periods that followed, incorporating, for example, the Jurassic. And humans? The 11,700-year-old Holocene marks the start of a global spread of homo sapiens – “a microscopic sliver at the tip of a fingernail”. The beginning of the proposed Anthropocene, whether we go with a starting point of a mooted 400 years, 70 or somewhere in between, is a tiny speck within this sliver. So, have homo sapiens created a new geological era? In simple terms, there is something of a case here – there’s plenty of evidence for human impact in the geological record, from signatures of human-induced climate change, atomic testing, and much more. But a fuller appreciation of deep time should actually make us wary of the Anthropocene label, maybe even shift our image of ourselves and what it means to inhabit the Earth at this time. Here’s why.  


      Read more:
      A glass of whisky could help you get your head around deep time


 Around 66m years ago, a mass extinction event took place, wiping out around three quarters of all species. This was most likely the result of an enormous asteroid impact – a conclusion reached after the discovery of a thin but distinct layer of sediment in the geologic record from this time, containing elements abundant in asteroids.   Mass extinction offered an opportunity for the rise of mammals as dominant lifeforms – ushering in the Cenezoic (“new life”) era. This thin layer of comet dust in the rock record represents a brief but vital transition between much thicker preceding and subsequent layers. But no one refers to what followed the mass extinction event as the “Cometocene”. That just wouldn’t make sense – the impact was a one-off event, significant in the context of deep time only in that it ushered in new foundations for life that then stretched out for millions of years into the far future.  What if the same could be said of our influence? What if, even with the well-documented effects of an Anthropocene still accumulating, we are talking about human impacts as a mere blip in the context of deep time? This is likely true. The spread of industrialism has aggressively and rapidly extracted and used up a finite supply of resources. The fact of finiteness, coupled with unprecedented environmental breakdown, fundamentally circumscribes the long-term viability of any possible era of human dominance.  This is what the American writer John Michael Greer claims when he says that all forms of industrial civilisation combined, in the context of geological time, are unremarkably short-lived and “self-terminating” – simply a transition between eras. This is why he considers the Holocene-Neocene transition, H-N transition for short, as a more accurate term, with Neocene being a placeholder name for whatever emerges next.  Our geological legacy will probably be like the comet dust – “a slightly odd transition layer a quarter of an inch thick”. As a remarkably adaptive species, humans may find ecological niches to survive and flourish in this far future, but we will not be dominant.  This does not mean we are heading towards some kind of one-off cataclysm – another extinction event. It means we are already living through one. But rather than being remembered as something grandiloquent and portentous – like the Anthropocene – it is more likely that some far future species would think of us as what historian Stephen Kern calls “a parenthesis of infinitesimal brevity”. In the context of deep time, the Earth will continue to meander on without us, and it will hardly notice we’re gone, just as it hardly knew we were here. This sojourn into deep time is not intended to be depressing or defeatist, certainly not to rule out hope, or to avoid acknowledgement of the damage humans can do. I think its psychological relevance is to offer a reminder of life itself as something to approach with reverence and awe; our species as interdependent and interconnected, not somehow apart; and to chip away at any residual hubris in the idea of the Anthropocene. Locating humanity in an even deeper story can seem scary. But it might also be liberating. For countless cultures around the world of course, this is nothing new – many Indigenous worldviews embrace nature, have a reverence for it and a deep sense of time and place. While being historically displaced from those places by the forces of colonialism and industrialism, these voices are often neglected.   The history of our far future, if we have one, will be one where we learnt to recognise interdependence with nature, with other species. In the end, it is about what it means to be human. As the late environmental philosopher Val Plumwood warned: “We will go onwards in a different mode of humanity, or not at all.”"
"
WUWT readers may remember last year that we had an early outbreak of Tornado season, and media opportunist Senator John Kerry immediately jumped at the chance to blame the weather event on “global warming” as we reported here on WUWT:
Kerry appeared on MSNBC on February 6 to discuss storms that have killed at least 50 people throughout the Southeastern United States. So, of course, Kerry used the platform to advance global warming alarmism.
“[I] don’t want to sort of leap into the larger meaning of, you know, inappropriately, but on the other hand, the weather service has told us we are going to have more and more intense storms,” Kerry said. “And insurance companies are beginning to look at this issue and understand this is related to the intensity of storms that is related to the warming of the earth. And so it goes to global warming and larger issues that we’re not paying attention to. The fact is the hurricanes are more intensive, the storms are more intensive and the rainfall is more intense at certain places at certain times and the weather patterns have changed.”
See the original WUWT report here.
So, this year is a little bit different. We have a late and slow start to tornado season. Always a good thing. That being said, this report from Joe D’Aleo discusses why its been slow, and debunks a recent Weather Channel claim that the current deficit of tornadoes has something to do with “global warming”.  Seeing how global warming causes both individual tornado events and decreased tornado events, I’ve apparently terribly underestimated its omnipotent power to influence weather. 😉

Graph from NWS/NOAA. Smaller (F1) tornadoes seem to be on the increase, but not larger ones.
Even though tornado reports seem to be on the rise, the larger damaging tornados, F2-F5 don’t seem to be. There are some good reasons for this, and it might be a good primer for readers to revisit this report I made about the issue of tornado reporting:
Increasing tornadoes or better information gathering?
BTW, if anyone wants a really cool weather radar program for tracking severe storms, please see my StormPredator program here – Anthony

Tornado Season So Far Not as Bad as 2008
By Joseph D’Aleo, CCM, on Intellicast
After another La Nina season with again a lot of snow and precipitation in the north central, another relatively active tornado season was expected and so far it has delivered on that promise. However given the La Nina was not as strong and the rebound in the Pacific towards El Nino is a month earlier than last year, the number of storms so far, have been less. It looks like May will fall well short of last May’s 461 tornadoes.

Cedar Hill, Texas, Photo credit Pat Skinner, TTU
The annual summary to date can be found here. The tornadoes so far in 2009 have been in the southeast quadrant of the nation. Climatologically, that is where the season normally begins.

See larger image here.
In 2008, the tornadoes when all was said and done, were found in all but 4 of the lower 48 states.

See larger image here.
As we move into summer, expect the activity to shift north with the jet stream. The march of the season – climatology of tornadoes normally follows this depiction (source here).
We are below the 5 year average for tornadoes for the season to date, below all but 2005.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf. You can see in 2008 that the daily events peaked in May with the biggest day on the 23rd of May before falling off in summer as El Nino like conditions developed in the eastern Pacific. That is occurring a month earlier this year and perhaps, that explains the quieter May.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf.

The reason that this region is most vulnerable is that this is where the combination of moisture from the Gulf of Mexico, dry air in mid levels from Mexico, a strong jet with cold air aloft coming out of the Rockies and a boundary between still cool air to the north and the warm humid summer like air in the south all come together.  Read more and see some useful links here. 
By the way, last year, the alarmist media blamed the increase in tornados on global warming. Well guess what this year, stormchasers across Tornado Alley have been frustrated this spring by what seems to be a lack of tornadoes and severe weather.  Indeed, VORTEX2, the largest tornado field study ever, has been running for more than two weeks now and has not seen one twister.  Last week, Weather Channel Senior Meteorologist Stu Ostro speculated that global warming was the cause. Of course this is the normal, mother nature has a perverse sense of humor – projects to study east coast storms had no east coast storms that winter, just one passing front. You just knew that VORTEX2 would lead to a lack of storms to study. We need them to schedule a massive study of hurricanes and we will surely have a dud season.
As to Stu’s reasoning, a big ridge would lead to heat and dryness across the central states. Instead the region has been hit hard with a steady stream of progressive troughs with heavy rainfall and very little warmth (fuel for storms). These storms brought more toirnadoes in April 2009 than in April 2008.

See larger image here.
The real issue may instead be that the El Nino like conditions that came in late May last year shutting off severe weather activity in June 2008 came on a month earlier this year and severe weather activity has diminished in May.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e961a4c69',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"When they start mining the seabed, they’ll start mining part of me. These are the words of a clan chief of the Duke of York Islands – a small archipelago in the Bismarck Sea of Papua New Guinea which lies 30km from the world’s first commercial deep sea mine site, known as “Solwara 1”. The project, which has been delayed due to funding difficulties, is operated by Canadian company Nautilus Minerals and is poised to extract copper from the seabed, 1600m below the surface. Valuable minerals are created as rapidly cooling gases emerge from volcanic vents on the seafloor. Mining the seabed for these minerals could supply the metals and rare earth elements essential to building electric vehicles, solar panels and other green energy infrastructure. But deep sea mining could also damage and contaminate these unique environments, where researchers have only begun to explore. The industry’s environmental impact isn’t the only concern. It’s been assumed by the corporate sector that there is limited human impact from mining in the deep sea. It is a notion that is persuasive especially when compared with the socio-ecological impacts of land-based mining.  But such thinking is a fallacy – insights from my research with communities in Papua New Guinea over the past three years highlight that the deep sea and its seabed should be thought of as intimately connected to humanity, despite the geographical distances involved. For the people of the Duke of York Islands, deep sea mining disturbs a sense of who they are, including the spirits that inhabit their culture and beliefs.  In Western thought, the sea has not only been considered to be marginal to politics, but also as entirely distinct from the land. Separating nature from humanity has proved useful in enabling exploitation of the natural world for human means. Deep sea mining, with all its material connections between a dynamic seabed and sites of consumption on land, provokes new questions. If humanity can’t physically encounter the deep seabed, then how are we to treat it ethically?. By conceptually “distancing” the deep ocean, who is being marginalised? For the people who live close to Solwara 1, the answer is pointed. These communities have long understood the world as a connection between “nature”, “spirits” and “beings”. Central within this cosmology are the spirits – masalai – some of which are understood as guardians of the seabed and its resources. Masalai are a fundamental part of the islanders’ world. Thus, the prospect of deep sea mining means not just social and economic disruption, but spiritual turmoil. The digging up of the seabed and the extraction of its resources cuts through the very fabric of their spiritual world and its sacred links to the sea and land.  As the historian Neil Macgregor put it in the Radio 4 series “Living with the Gods”, masalai are not  out there… [like] tourists in the human realm, from somewhere else … but in a world in which we co-inhabit.  The political implication for island communities here is clear. The copper which might be mined from the seabed is effectively constituted by these spirits. Thus, as copper “resurfaces” in the objects and technologies of the future – in batteries and wiring – it also carries a spirituality from the region where it originated. Spirits infuse the traditions and everyday practises of the people on the Duke of York Islands. “Shark calling” is one such example which is practised along parts of the west coast of New Ireland Province – the closest point on land to Solwara 1.  Every few weeks, when the sea conditions allow, “shark callers” attempt to attract sharks to their hand-carved wooden canoes by rattling a mesh of coconut shells in the water, before capturing them by hand. Shark meat is a key part of local diets that generally lack protein. 


      Read more:
      Deep sea mining could help develop mass solar energy – is it worth the risk?


 Shark callers communicate with spirits which are “resident” in stones found on local beaches prior to their expeditions. It’s no surprise then, that these communities fear noise pollution generated by deep sea mining and the physical disturbance of the seabed which could sever the cultural connections they have with the ocean. Deep sea mining companies should consider the spirituality of the people their work affects and other kinds of environmental knowledge as important in their own right. As this new industry collides with cultural belief systems in different parts of the world, it will be essential to understand the complex ways in which deep sea mining does have “human” impacts after all. Culture is a key part of any understanding of environmental politics, no matter how extreme the environment in question."
"
If you are just joining us, first you should read about what started it all here.
While Realclimate.org continues deleting the ongoing river of comments posted on their threads ( Note: Any of you who find that your posts to those sites are being rejected {as usual without any explanation} can keep a copy of the post, and post it at http://rcrejects.wordpress.com if you want. Keep those screencaps going folks) asking about the McIntyre Yamal data development, Jennifer Marohasy of Australia is drawing a bit of a line in the sand. Given the churlishness of the Team and the blockades put up by Hadley, I can’t say that I blame her stance. – Anthony

Leading UK Climate Scientists Must Explain or Resign
By Jennifer Marohasy
MOST scientific sceptics have been dismissive of the various reconstructions of temperature which suggest 1998 is the warmest year of the past millennium.    Our case has been significantly bolstered over the last week with statistician Steve McIntyre finally getting access to data used by Keith Briffa,  Tim Osborn  and Phil Jones to support the idea that there has been an unprecedented upswing in temperatures over the last hundred years –  the infamous hockey stick graph.
Mr McIntyre’s analysis of the data – which he had been asking for since 2003 – suggests that scientists at the Climate Research Unit of the United Kingdom’s Bureau of Meteorology  have been using only a small subset of the available data to make their claims that recent years have been the hottest of the last millennium.   When the entire data set is used, Mr McIntyre claims that the hockey stick shape disappears completely. [1]
Red - before new data Black - after new data
Mr McIntyre has previously showed problems with the mathematics behind the ‘hockey stick’.   But scientists at the Climate Research Centre, in particular Dr Briffa, have continuously republished claiming the upswing in temperatures over the last 100 years is real and not an artifact of the methodology used – as claimed by Mr McIntyre.     However, these same scientists have denied Mr McIntyre access to all the data.    Recently they were forced to make more data available to Mr McIntyre after they published in the Philosophical Transactions of the Royal Society  –  a journal which unlike Nature and Science has strict policies on data archiving which it enforces.   
This week’s claims by Steve McInyre that scientists associated with the UK Meteorology Bureau have been less than diligent  are serious and suggest some of the most defended building blocks of the case for anthropogenic global warming are based on the indefensible when the methodology is laid bare.
This sorry saga also raises issues  associated with how data is archived at the UK Meteorological Bureau with in complete data sets that spuriously support the case for global warming being promoted while complete data sets are kept hidden from the public –  including from scientific sceptics like Steve McIntyre.
It is indeed time leading scientists at the Climate Research Centre associated with the UK Meteorological Bureau explain how Mr McIntyre is in error or resign.
***********
Notes and Links
[1] Yamal: A “Divergence” Problem, by Steve McIntyre, 27 September 2009
http://www.climateaudit.org/?p=7168
The above chart shows the difference when the entire data set (black line) as opposed to a subset (red line) is used to reconstruct temperature.   The chart is accompanied by the following comment from Mr McIntyre:  “The next graphic compares the RCS chronologies from the two slightly different data sets: red – the RCS chronology calculated from the CRU archive (with the 12 picked cores); black – the RCS chronology calculated using the Schweingruber Yamal sample of living trees instead of the 12 picked trees used in the CRU archive [leaving the rest of the data set unchanged i.e. all the subfossil data prior to the 19th century]. The difference is breathtaking.”
Mann, Michael E.; Bradley, Raymond S.; Hughes, Malcolm K. (1998), “Global-scale temperature patterns and climate forcing over the past six centuries” (PDF), Nature 392: 779–787, doi:10.1038/33859, http://www.caenvirothon.com/Resources/Mann,%20et%20al.%20Global%20scale%20temp%20patterns.pdf
Wikipedia http://en.wikipedia.org/wiki/Hockey_stick_controversy#cite_note-17
CRU Refuses Data Once Again
http://www.climateaudit.org/?p=6623
http://climateresearchnews.com/2009/09/the-hockey-stick-is-dead/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92d8c348',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels and Paul C. “Chip” Knappenberger, from Cato’s Center for the Study of Science, review interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
\---------   
  
  
When it comes to global warming, facts often take a back seat to fiction. This is especially true with proclamations coming from the White House. But who can blame them, as they are just following the lead from Big Green groups (aka, “The Green Blob”), the U.S. Climate Change Research Program (responsible for the U.S. National Climate Assessment Report), and of course, the United Nations' Intergovernmental Panel on Climate Change (IPCC).   
  
We have documented this low regard for the facts (some might say, deception) on many occasions, but recently we have uncovered a particularly clear example where the IPCC’s ideology trumps the plain facts, giving the impression that climate models perform a lot better than they actually do. This is an important façade for the IPCC to keep up, for without the overheated climate model projections of future climate change, the issue would be a lot less politically interesting (and government money could be used for other things … or simply not taken from taxpayers in the first place).   
  
The IPCC is given deference when it comes to climate change opinion at all Northwest Washingon D.C. cocktail parties (which means also by the U.S. federal government) and other governments around the world. We tirelessly point out why this is not a good idea. By the time you get to the end of this post, you will see that the IPCC does not seek to tell the truth—the inconvenient one being that it dramatically overstated the case for climate worry in its previous reports. Instead, it continues to obfuscate.   
  
This extracts a cost. The IPCC is harming the public health and welfare of all humankind as it pressures governments to seek to limit energy choices instead of seeking ways to help expand energy availability (or, one would hope, just stay out of the market).   
  
Everyone knows that global warming (as represented by the rise in the earth’s average surface temperature) has stopped for nearly two decades now. As historians of science have noted, scientists can be very creative when defending the paradigm that pays. In fact, there are already several dozen explanations.   
  
Climate modelers are scrambling to try to save their creations’ reputations because the _one_ thing that they do not want to have to admit is that they exaggerate the amount that the earth’s average temperature will increase as a result of human greenhouse gas emissions. If the models are overheated, then so too are all the projected impacts that derive from the model projections—and that would be a disaster for all those pushing for regulations limiting the use of fossil fuels for energy. It's safe to say the number of people employed by creating, legislating, lobbying, and enforcing these regulations is huge, as in “The Green Blob.”   




In the Summary for Policymakers (SPM) section of its _Fifth Assessment Report_ , the IPCC pays brief attention to the recent divergence between model simulations and real-world observations:   




There are, however, differences between simulated and observed trends over periods as short as 10 to 15 years (e.g., 1998 to 2013).



But, lest you foolishly think that there may be some problem with the climate models, the IPCC clarifies:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Whew! For a minute there it seemed like the models were struggling to contain reality, but we can rest assured that over the long haul—say, since the middle of the 20th century—according to the IPCC, model simulations and observations “agree” as to what is going on.   
  
The IPCC references its “Box 9.2” in support of the statements quoted above.   
  
In “Box 9.2” the IPCC helpfully places the observed trends in the context of the distribution of simulated trends from the collection of climate models it uses in its report. The highlights from Box 9.2 are reproduced below (as our Figure 1). In this figure, the observed trend for different periods is in red and the distribution of model trends is in grey.   






  
  
_Figure 1. Distribution of the trend in global average surface temperature from 114 model runs used by the IPCC (grey) and the observed temperatures as compiled by the UK’s Hadley Center (red). (Figure from the IPCC Fifth Assessment Report.)_   
  
As can be readily seen in Panel (a), during the period 1998-2012, the observed trend lies below almost all the model trends. The IPCC describes this as:   




111 out of 114 realizations show a GMST [global mean surface temperature] trend over 1998–2012 that is higher than the entire HadCRUT4 trend ensemble.



That gives rise to the IPCC SPM statement (quoted above) that   




There are, however, differences between simulated and observed trends over periods as short as 10 to 15 years (e.g., 1998 to 2013).



No kidding!   
  
Now let’s turn our attention to the period 1951-2012, Panel (c) in Figure 1.   
  
The IPCC describes the situation depicted there as:   




Over the 62-year period 1951–2012, observed and CMIP5 [climate model] ensemble-mean trends agree to within 0.02°C per decade.



This sounds like the model are doing pretty good—only off by 0.02°C/decade. And this is the basis for the IPCC SPM statement (also quoted above):   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Interestingly, the IPCC doesn’t explicitly tell you how many of the 114 climate models are greater than the observed trend for the period 1951-2012. And it is basically impossible to figure that out for yourself based on their Panel (c) because some of the bars of the histogram go off the top of the chart and the x-axis scale is so large as to bunch up the trends such that there are only six populated bins representing the 114 model runs. Consequently, you really can’t assess how well the models are doing and how large a difference of 0.02°C/decade over 62 years really is. You are left to take the IPCC’s word for it.   
  
Don’t.   
  
The website Climate Explorer archives and makes available the large majority of the climate model output used by the IPCC. From there, you can assess 108 (or the 114) climate model runs incorporated into the IPCC graphic—a large enough majority to quite accurately reproduce the results.   
  
We do this in our Figure 2. However, we adjust both axes of the graph such that all the data are shown and you can see the inconvenient details.   






  
  
_Figure 2. Distribution of the trend in the global average surface temperature from 108 model runs used by the IPCC (blue) and observed temperatures as compiled by the UK’s Hadley Center (red) for the period 1951-2012 (the model trends are calculated from historical runs with the RCP4.5 emissions scenario results appended after 2006). This presents the nearly identical data in Figure 1 Panel (c)._   
  
What we find is that there are 90 (of 108) model runs that simulate more global warming to have taken place from 1951 to 2012 than actually occurred and 18 model runs simulating less warming to have occurred. Which is another way of saying the observations fall at the 16th percentile of model runs (the 50th percentile being the median model trend value).   
  
So let us ask you this question, on a scale of 1 to 5—or rather, using the descriptors, “very low,” “low,” “medium,” “high,” or “very high”—how would you describe your “confidence” in this statement:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



OK. You got your answer?   
  
Our answer is, maybe, “medium”, and there is plenty of room for improvement.   
  
The model range should be much tighter, indicating that the models were in better agreement with one another as to what the simulated trend should have been. As it is now, the model range during the period 1951-2012 extends from 0.07°C/decade to 0.21°C/decade (with the observed trend at 0.107°C/decade). And this is from models that were run largely with observed changes in climate forcings (such as greenhouse gas emissions, aerosol emissions, volcanoes, etc.) and for a period of time (62 years) during which short-term weather variations should all average out. In other words, they are all over the place.   
  
Another way the agreement between model simulations and real-world observations could be improved would be if the observed trend fell closer to the center of the distribution of model projections. For instance, the agreement would be better if, say, 58 model runs produced more warming and the other 50 produced less warming.   
  
What would lower our confidence?   
  
The opposite set of tendencies. The model distribution could be even wider than it is currently, indicating that the models agreed with each other even less than they do now as to how the earth’s surface temperature should evolve in the real world (or that natural variability was very large over the period of trend analysis). Or, the observed trend could move further from the center point of the model trend distribution. This would indicate an increased mismatch between observations and models (more similar to what has taken place over the 1998-2012).   
  
Unfortunately, that’s what is happening.   
  
Figure 3 shows at which percentile the observed trend falls for each period of time starting from 1951 and ending each year from 1980 through 2013.   








_Figure 3. The percentile rank of the observed trend in the global average surface temperature beginning in the year 1951 and ending in the year indicated on the x-axis within the distribution of 108 climate model simulated trends for the same period. The 50 th percentile is the median trend simulated by the collection of climate models._   
  
After peaking at the 42nd percentile (still below the median model simulation, which is the 50th percentile) during the period 1951-1998, the observed trend has steadily fallen in the percent rank, and currently (for the period 1951-2013) is at its lowest point ever (14th percentile) and is continuing to drop. Clearly, this “tendency within a trend” (as Casey Stengel or Yogi Berra might have called this “trendency”) is looking bad for the models, as the level of agreement with observations is steadily decreasing with time.   
  
In statistical parlance, if the observed trend drops beneath the 2.5th percentile, it would be widely considered that the evidence was strong enough to indicate that the observations were not drawn from the population of model results. In other words, statisticians would say the models disagree with the observations with “very high confidence.” Some researchers use a more lax standard and would say that falling below the 5th percentile would be enough to consider the observations not to be in agreement with the models. We could consider that case to be described as “high confidence” that the models and observations _disagree_ with one another.   
  
So, just how far away from either of these situations are we?   
  
It all depends on how the earth’s average surface temperature evolves in the near future.   
  
We explore three different scenarios between now and the year 2030:   
  
Scenario 1: The earth’s average temperature during each year of the period 2014-2030 remains the same as is average temperature observed during the first 13 years of this century (2001-2013). This scenario represents a continuation of the ongoing “pause” in the rise of global temperatures.   
  
Scenario 2: The earth’s temperature increases year-over-year at a rate equal to the observed rise in temperature observed during the period 1951-2012 (a rate of 0.107°C/decade). This represents a continuation of the observed trend.   
  
Scenario 3: The earth’s temperature increases year-over-year during the period 2014-2030 at a rate equal to that observed during the period 1977-1998—the period often identified as the 2nd temperature rise of the 20th century. The rate of temperature increase during this period was 0.17°C/decade. This represents a scenario in which the temperature rises at the most rapid rate observed during the period often associated with an anthropogenic influence on the climate.   
  
Figure 4 shows how the percentile rank of the observations evolves under all three scenarios from 2013 through 2030. Under Scenario 1, the observed trend (beginning in 1951) would fall below the 5th percentile of the distribution of model simulations in the year 2018 and beneath the 2.5th percentile in 2023. Under Scenario 2, the years to reach the 5th and 2.5th percentiles are 2019 and 2026, respectively. And under Scenario 3, the observed trend would fall beneath the 5th percentile of model-simulated trends in the year 2020 and beneath the 2.5th percentile in 2030.   








_Figure 4. Percent rank of the observed trend within the distribution of model simulations beginning in 1951 and ending at the year indicated on the x-axis under the application of the three scenarios of how the observed global average temperature will evolve between 2014 and 2030. The climate models are run with historical forcing from 1951 through 2006 and the RCP4.5 greenhouse gas scenario thereafter._   
  
**It is clearly not a good situation for climate models when even a sustained temperature rise equal to the fastest yet observed (Scenario 3) still leads to complete model failure within two decades.**   
  
So let’s review:   
  
1) Examining 108 climate model runs spanning the period from 1951 to 2012 shows that the model-simulated trends in the global average temperature vary by a factor of three—hardly a high level of agreement as to what should have taken place among models.   
  
2) The observed trend during the period 1951-2012 falls at the 16th percentile of the model distribution, with 18 model runs producing a smaller trend and 90 climate model runs yielding a greater trend. That's not particularly strong agreement.   
  
3) The observed trend has been sliding further and further away from the model median and toward ever-lower percentiles for the past 15 years. The agreement between the observed trend and the modeled trends is steadily getting worse.   
  
4) Within the next 5 to 15 years, the long-term observed trend (beginning in 1951) will more than likely fall so far below model simulations as to be statistically recognized as not belonging to the modeled population of outcomes. This disagreement between observed trends and model trends would be complete.   
  
So with all this information in hand, we’ll give you a moment to revisit your initial response to this question:   
  
On a scale of 1 to 5, or rather, using these descriptors “very low,” “low,” “medium,” “high,” or “very high,” how would you describe your “confidence” in this statement:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Got your final answer?   
  
OK, let’s compare that to the IPCC’s assessment of the situation.   
  
The IPCC gave it “very high confidence”— _the highest level of confidence that they assign_.   
  
Do we hear stunned silence?   
  
This, in a nutshell, sums up the IPCC process. The facts show that the agreement between models and observations is tenuous and steadily eroding and will be statistically unacceptable in about a decade. And yet the IPCC tells us with “very high confidence” that models agree with observations and therefore are a reliable indicator of future climate changes.   
  
Taking the IPCC at its word is not a good idea.   
  
_[This is a major revision of a post that first appeared at_ _Watts Up With That_ _on April 16, 2014.]_


"
"The building of tens of thousands of homes on flood-prone land is worsening the damage to surrounding areas, Conservative MPs have said, as the head of the Environment Agency warned against new developments on floodplains. Tory backbenchers called on Boris Johnson to review the government’s housing policy over concerns that new homes were either not flood-proof or were exacerbating issues in neighbouring communities.  John Redwood, the MP for Wokingham, said building on land most at risk of flooding was “a very foolish thing to do and it’s obviously making the problem considerably worse”. He said the risk to residents had been “greatly increased” by building on floodplains in his Berkshire constituency, and added: “I think [the government] should certainly review their planning policy and I think they should take the Environment Agency’s advice more seriously on appeal and regard it as a very important factor.” Two severe flood warnings remained in place on Tuesday night, in Hereford and Ironbridge where homes were evacuated as the River Severn was expected to reach near-record levels. Residents were also evacuated in the town of Snaith, east Yorkshire, after the River Aire burst its banks. There were 250 flood warnings and alerts in place across England, from Devon to Cumbria, with a further 10 in Wales following one of the wettest Februarys in 254 years of records. The government has come under pressure over its aims to build 300,000 homes a year by the mid-2020s to help ease a chronic shortage across the UK. Local authorities say they are struggling to meet these demands because of a shortage of available land, leading to one in 10 of new homes in England being built on high-risk flood sites since 2013. The Guardian revealed on Sunday that more than 11,000 homes were planned in areas the government considers a high flood risk in the seven English regions swamped by Storms Ciara and Dennis. The government says its planning policy is clear that housing should be located in the areas least at risk of flooding and, when development in a risk area is absolutely necessary, “sufficient measures should be taken to make sure homes are safe, resilient and protected from flooding”. However, a series of experts, MPs and local authorities have said that these new developments often increase the flood risk to surrounding areas because water that would be otherwise absorbed by the land instead runs off more quickly into rivers that then burst their banks. The Tory MP Laurence Robertson said two huge housing developments were under construction in his Tewkesbury constituency, comprising 2,500 homes, and that one of them was currently under water. “All of that [new housing] is just going to make [the flooding] so much worse,” he said. “I don’t think there’s been any adequate demonstration that they can contain the water in the new buildingwork. I’ve got other examples in my constituency where houses have been built, particularly on slightly elevated land, which throw the water downhill. They suffer [in surrounding areas] and that’s what I fear.” Kieran Mullan, the newly elected Conservative MP for Crewe and Nantwich, said housebuilding on floodplains was “asking for trouble and I would need a lot of convincing to find that is ever justified”. He said a recently built housing estate had caused a road in his constituency to flood and that residents’ concerns about the dangers had been ignored. “It is no surprise concreting over fields can make localised water drainage worse,” he said. Ahead of a speech in London on Tuesday, the head of the Environment Agency, Sir James Bevan, said properties should not be built on the floodplain “as far as possible” and that some developments should never have been approved. Speaking on BBC Radio 4’s Today programme, Bevan also raised the possibility that some vulnerable communities on the coast and in river valleys may have to move to avoid repeated flooding. He said those communities should not be “forced out” but that there needed to be a conversation about how they can be protected in the long term. In a speech at the World Water-Tech Innovation summit in central London, Bevan said it was unrealistic to ban all housebuilding on floodplains given England’s geography. However, he added: “The clue is in the name: floodplain. So we can and should insist that development only happens there if there is no real alternative, that any such development doesn’t increase other people’s flood risk … and that properties built on the floodplain are flood resilient, for example with the garages on the ground floor and the people higher up.” Labour has called for an immediate end to building on land considered to be at high risk of flooding, which equates to 10% of land in England. Analysis by the Guardian found that more than 84,000 homes had been built in these high-risk flood zones between 2013 and 2018, with the annual total having doubled in that time. The government also came under fire from the National Farmers’ Union, which blamed a “third world” approach to water management for the devastation. The NFU president, Minette Batters, said the government had done “nothing” in the last eight years to act on its 2012 manifesto involving more reservoirs and a national plan to transport water elsewhere to the country to meet needs. “Years of neglect has created an urgent problem,” she said, adding that farmers’ efforts with wood dams or the introduction of beavers to naturally manage water flow were part of the solution."
"
Share this...FacebookTwitterThe warmist German klimaretter.info site has a piece about sequestraton of carbon dioxide, which reports that Swedish power company Vattenfalls plans have a CCS plant near Berlin ready by 2015.
The plant would remove CO2 from Vattenfall’s brown coal power plant and pump it into the earth for high-pressure underground storage. But an expert geological assessment shows that could lead to problems. According to klimaretter.info:
Storing carbon dioxide underground could however have negative impacts beyond Brandenburg. A geological expert assessment for the community of Barnim-Oderbruch made available to klimaretter.info states that because of the overpressure in the bedrock strata, a salinisation of groundwater has to be expected within a radius of 100 km from the injection borehole. That would affect Mecklenburg Western Pommerania and Poland.
That means the entire Berlin metropolitan area would be impacted. Geology expert Ralf Krupp studied the underground geology in the area and concludes that the ground structure may not be able to securely store the CO2 because the 20-meter salt layer is not thick enough, and so fears that the high pressure could lead salt water carrying strata to mix in with drinking water – causing it to become saline. Kilmaretter also writes:
Especially problematic for Krupp is that saltwater probably is laden with heavy metals. ‘This could be an acute hazard for many water utilities,’ the geologist descríbes.”
In the meantime Vattenfalls calls such scenarios “purely speculative” and that there a number of technical factors that have to be considered. Water utility companies, however, find the scenarios plausible and not without risk.
In the meantime, the uncertainty is already having a powerful impact on public opinion. Activist and cititens groups are already mobilising to stop the CCS technology from being employed not only near Berlin, but at a number of locations throughout Germany. So add another technology that is too risky to be used – along with nuclear power, GMO’s, high speed trains, coal power plants, shale gas, oil, internal combustion engines, bottled water, fireplaces, toilets…
Reading up on CCS technology, I find that it involves a lot work (consumption of energy) and will provide no benefit. Seems to be yet another superstition-driven folly. Watch this Alberta video on how it works: http://www.youtube.com/watch?v=R0i6dhEPSwU.
Share this...FacebookTwitter "
"“Nurdles” may sound cute but they pose a huge risk to the marine environment. Also known as “mermaid tears”, these small plastic pellets are a feedstock in the plastic industry. Instead of being converted into household items, many end up in the ocean, collecting toxins on their surfaces and being eaten by marine wildlife. Not so cute now, are they? Nurdles are the building blocks for most plastic goods, from single-use water bottles to televison sets. These small pellets – normally between 1mm and 5mm – are classed as a primary microplastic alongside the microbeads used in cosmetic products – they’re small on purpose, as opposed to other microplastics that break off from larger plastic waste in the ocean. The small size of nurdles makes them easy to transport as the raw material which can be melted down and moulded into all kinds of plastic products by manufacturers. Unfortunately, mismanagement of these little pellets during transport and processing leads to billions being unintentionally released into rivers and oceans through effluent pipes, blown from land or via industrial spillage. “Mermaid tears” is an appropriate nickname when we consider the potential harm that nurdles have on marine life. Their small size, round shape and array of colours make them attractive food – easily mistaken for fish eggs and small prey. This “food” has an extra problem – it comes with a side of noxious chemicals.  The large surface area to size ratio and polymer composition of the nurdle pellets allow persistent organic pollutants (POPs) in seawater to build up on their surfaces. These toxins then transfer to the tissue of organisms which eat them.  The problem is in the name – POPs are “persistent”, meaning they don’t go away easily and can remain on the surface of nurdles for years. Nurdles can also be colonised by microbes that are dangerous to humans. A study investigating nurdles on bathing beaches in East Lothian, Scotland, found that all five beaches tested had nurdles that were covered with E. coli – the bacterium responsible for food poisoning.  Nurdles can be so noxious that people cleaning beaches or recording pellets in scientific surveys are advised not to touch them with their bare skin – which makes sun bathing on many beaches in the summer an unattractive prospect. So how many nurdles are out there in the ocean and on coastlines? It’s estimated that up to 53 billion nurdles are released annually in the UK from the plastic industry. That’s the same amount of nurdles that it would take to make 88m plastic bottles. So why are nurdles rarely discussed in the plastic pollution debate? Luckily, there are organisations raising awareness of nurdles and their prevalence in marine pollution. The Great Global Nurdle Hunt started by Fidra – a charity based in Scotland that addresses environmental issues – and the Marine Conservation Society encourages people to become citizen scientists and gather data on how common these pellets are on beaches around the world.  Data collection helps identify the main sources of this pollution from the plastic industry, which can use the information to improve management of the problem. As there are so many nurdles present in the environment, it takes an army of people to gather information about them. The Hunt takes place over ten days in February each year.  Citizen scientists log their nurdle findings onto a global map that shows the extent of nurdle pollution worldwide and how it’s changed over time. Since 2012, the number of beaches being searched has reached 1610 across six continents, 18 countries and with over 60 organisations involved. This year, Staffordshire University’s Microplastic and Forensic Fibre Research Group took part in efforts to estimate the concentration of nurdles on Hightown beach in Liverpool, UK. An average of 139.8 nurdles per square metre were found. That’s around 140,000 nurdles over 1km of hightide line.  If you’d like to become a citizen scientist and collect nurdle data at your local beach, there are a few useful tips. Have a look at one of the online nurdle ID guides online so that you don’t mistake a polystyrene ball, BB gun pellet or ancient fossil for a nurdle. Make sure to check seaweed and other marine debris when on the beach – these act like large nurdle nets. Once you’ve collected data, don’t forget to submit your findings to a suitable survey so that that they can be used to fight the pollution problem. And if you don’t live near the coast, don’t worry – nurdles have been found in most environments, including rivers, lakes and even far inland and away from water. We even found them in soil in our campus. So let’s get nurdle hunting – but don’t forget your gloves."
"**As much as Â£1bn in benefit fraud has been prevented from being paid to organised-crime groups in recent months, BBC News has learned.**
But before the scam was spotted, officials unwittingly confirmed thousands of stolen identities.
Fraudsters took advantage of looser rules introduced to cope with a surge of universal credit claims during the pandemic.
BBC News has asked the Department for Work and Pensions for a response.
In May, a junior civil servant working with High Street banks noticed dozens of claims for universal credit had been made asking for money to be paid into the same bank account.
Further investigation identified more than 100,000 fraudulent claims.
And officials admit they had confirmed thousands of people's identities to the gangs that had stolen them - and passed on their National Insurance numbers.
The Department for Work and Pensions wants to write to those whose data has been compromised.
But BBC News has learned it is struggling to identify many of them and is wary of sending out letters to last known addresses in case they end up in the wrong hands, exacerbating the data breach.
Claimants whose identities have been stolen can face real hardship, as it can be months before their accurate benefits are paid.
Currently, 5.7 million people receive universal credit, almost double the figure for March.
To cope with the surge, identity checks were processed online, rather than face-to-face, and information such as the cost of rent and whether someone had been self-employed taken on trust.
Criminal gangs have attacked several government Covid-19 schemes.
DWP officials have asked the Treasury for Â£200m over three years, in this spending round, calculating it would enable it to prevent such mass scams and save taxpayers about Â£500m each year.
It is estimated more than a million claims for universal credit have still to be properly checked, with additional rising concerns tens of thousands of people may have claimed the benefit without declaring they had received government grants to help the self-employed.
However, the Treasury has turned down the request."
"
Here is the current Pacific satellite image, note the lower right.
Click for a larger image
I might add that the likelihood of a hurricane strength storm striking Southern California is low. Since 1900, only four tropical cyclones have brought gale-force winds to the Southwestern United States. They are an unnamed tropical storm that made landfall near San Pedro in 1939, the remnants of Hurricane Joanne in 1972, the remnants of Hurricane Kathleen in 1976, and Hurricane Nora in 1997 which entered California as a tropical storm.
The storms that do make it close enough to be a threat are often weakened by two facts: cold sea surface temperatures and upper level steering winds that tend to take them away for SoCal. But it’s a fun exercise to discuss the possibility. – Anthony
Hurricanes in Los Angeles?
Guest post by Roger Sowell

A hurricane hitting Los Angeles. No, it hasn’t happened yet, but it could. I am using the same reasoning as the Carbon Is Going to Kill Us crowd, where it is deemed prudent and even mandatory that we take action now to prevent a future catastrophe. AGW believers insist that all mankind (well, except for developing countries, of course!) curtail or stop altogether emitting carbon dioxide, as that may cause ice caps to melt and oceans to rise and population disruption.
There is a hurricane in the Pacific Ocean, headed directly toward Los Angeles. It’s name is Jimena (pronounced him -ay – nuh, accent on the ay). Jimema currently has winds of 135 miles per hour, and is just south of the tip of Baja, California. Its course is to the northwest, up the Baja peninsula.
Judging from the mass confusion a couple of years ago when Houston evacuated ahead of hurricane Rita, Los Angeles might want to start packing and driving today. Houston only had around 1 million people exiting the city, and had at least five freeways on which to do it. Los Angeles has approximately 3 million people, probably closer to 4 million, but the metropolitan area has 18 million, and only three ways out. There is the Interstate 10, going due East; Interstate 5 going North; and highway 101, also going north. I-5 also goes south, but little good that will do since one runs into San Diego and the hurricane.
A hurricane hitting Los Angeles. We must take prudent steps to avoid the certain disaster and destruction from a hurricane. We will not be required to wait 100 years for the results to be in. This hurricane will be here in less than 10 days. We must act today, while there is still time. The science is settled. Hurricanes hitting major population centers are a serious threat. Remember New Orleans and Hurricane Katrina. Houston and Hurricane Rita. We must mobilize FEMA so they can get their red tape all in order, ready to send trailers and water and food packs to Los Angeles.
The low-lying areas of Southern California are at risk of inundation from the storm surge. Ports and river basins will be swamped with seawater, causing un-told devastation to precious seashore that is a national treasure, as the California Coastal Commission regularly reminds us. A storm surge from a hurricane can be several feet. The California Coastal Commission was in a tizzy recently over the prospect of the ocean rising just one foot, in the next century. Where is the alarm, the hysterical and frantic activity, over a storm surge of 5 to 10 feet in the space of 24 hours?
Where is the clarion call to action from our state and city leaders? Governor Schwarzenegger, Mayor Villaraigosa, are you watching this hurricane? Have you prepared the state and city and county to deal with this?
Or, are you hoping the hurricane does arrive, and right away, so that the wildfires will finally be put out and the firefighters get some much-needed rest?
Stay tuned, sports fans.  This is about to get interesting.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93cef8c5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Congress finally passed a stimulus package after making sure to strip out elements that would benefit the economy over the long run. The plan includeed an accelerated depreciation provision that will be beneficial if it is later made permanent. But a new corporate tax survey by KPMG makes clear that this is only the first of many needed business tax reforms in the United States.



KPMG found that the United States has the fourth highest corporate income tax rate in the 30‐​nation Organisation for Economic Co‐​operation and Development (OECD). The combined U.S. federal and average state rate of 40 percent is almost 9 percentage points higher than the average OECD top corporate rate of 31.4 percent.



This is a dramatic reversal of the U.S. tax situation. After cutting the federal corporate rate from 46 percent to 34 percent in 1986, policymakers fell asleep at the switch, perhaps assuming that we had claimed a low‐​tax advantage permanently. But most industrial countries followed the U.S. lead and cut tax rates in the late 1980s. Then another round of tax rate cuts began in the late 1990s, with the result that the average OECD corporate rate fell from 37.6 percent in 1996 to just 31.4 percent by January 2002 (see chart). The average corporate rate in the European Union is now 32.5 percent, down from 38.2 percent in 1996.





We sometimes write‐​off European economies as uncompetitive welfare states, and ignore that many countries have improved their business climates. In fact, a recent study by the Economist Intelligence Unit placed the United States second, behind the Netherlands, for the “best place in the world to conduct business.” And a study by GrowthPlus, a European think tank, compared 10 major countries to determine which had the best environment for entrepreneurial growth companies. Again, the United States finished second, this time behind Britain. 



In the last few years, the corporate tax rate was cut in Denmark, France, Ireland, Germany, Poland, and Portugal, and in many countries outside of Europe. Even socialist Sweden has a top corporate tax rate of just 28 percent. It is certainly true that overall European taxes, as a share of gross domestic product, are much higher than in the United States. But Europe has shifted about one‐​third of its overall tax burden to less distortionary consumption taxes.



What the Europeans and others are realizing is that countries shoot themselves in the foot by imposing high tax rates on mobile capital. IMF data show that annual global portfolio capital flows rose six‐​fold during the past decade. United Nations data show that global direct investment also rose six‐​fold during this period. The U.S. attracts a big share of these flows because of its large economy, stable currency, and strong growth. But investment flows are increasingly sensitive to taxes, so it makes less and less sense to have a high corporate rate. After all, last year’s recession, the Enron collapse, and the high‐​tech bust all show that the U.S. business sector is not as invincible as it seemed in the late 1990s. 



A high statutory rate isn’t the only aspect of U.S. business taxation that needs reform. Aside from making the new depreciation rules permanent, we need to switch to a territorial tax system from the complex worldwide system that makes U.S. firms less competitive abroad. Glenn Hubbard, chairman of the Council of Economic Advisers, has noted that “from an income tax perspective, the United States has become one of the least attractive industrial countries in which to locate the headquarters of a multinational corporation.” As a consequence, there has been a “marked increase” in the number of U.S. firms reincorporating abroad, according to a new U.S. Treasury analysis. Shouldn’t the U.S. be trying to attract business rather than drive it away?



The critics of course will say that big corporations and their shareholders should pay their “fair share” of taxes, and that the government needs to crack down on tax‐​avoiders like Enron. Such views ignore big picture realities. First, the huge rise in global capital flows means that the corporate tax burden probably falls more on immobile workers, and less on the mobile capital income it is ostensibly placed on. Second, the high corporate tax rate is the reason why Enron and other firms go to such wasteful lengths to avoid and evade taxes. If the U.S. cut its corporate rate to say 20 percent, not only would real capital investment increase, but firms would financially restructure in order to shift more of their global tax base into this country.



As the world economy changes, so must U.S. tax policy. Pressures to attract mobile capital through international “tax competition” will continue to increase. These trends dictate that we reform our tax system by moving away from a high‐​rate income tax system to a low‐​rate consumption‐​based tax system. 
"
"
Share this...FacebookTwitterBy Ed Caryl
Pierre and I have both written about the effects of soot on Arctic and sub-Arctic ice and glaciers, read here Glaciers – The Dark Side and Half Of Arctic Warming Caused By Soot. Scientists are recognising that CO2 is becoming less of a factor and that black carbon soot instead is being recognised increasingly as the a driver of warming in the Arctic. Time to go back and revamp the models – again.
Source: http://atmoz.org/blog/2007/06/12/global-melting-big-thaw/
Researchers from the Arctic Council, representing the eight countries that border the Arctic, are now seriously studying the subject. See the Associated Press article here. The photo and caption from the article reads (emphasis added).
This undated handout photo provided by NOAA-STAD, Soot Transport and Deposition Study, shows Trish Quinn of NOAA in a first snow pit. An international research team is in the land of snow and ice in search of soot. Though the Arctic is often pictured as a vast white wasteland, that can be deceiving. And carbon deposited there as a result of activities elsewhere can have a long-term impact on climate (AP Photo/NOAA-STAD).
‘The Arctic serves as the air conditioner of the planet,’ explained Patricia Quinn of the National Oceanic and Atmospheric Administration, one of the research participants. Heat from other parts of the Earth moves to the Arctic in the circulating air and ocean water, and at least some of that warmth can radiate into space.”
At the same time, some of the incoming heat from the sun that tends to be absorbed in other locations is reflected by the ice and snow, allowing the polar regions to serve as cooling agents for the planet.”
Of course they need to insert the obligatory nod to CO2.
Cutting carbon dioxide and other greenhouse gases is the backbone of any effort to combat warming, both globally and within the Arctic, Quinn said.”
The group has not published yet, as the research will not end until the end of May, and to have any hope of getting their paper or papers past “peer review” they need to get CO2 into the mix. But this study is a few steps in the right direction.
Share this...FacebookTwitter "
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. What forms a current under water? – Natalie, age 11, Melksham, UK. Thanks for your question, Natalie. Underwater currents can form in lakes, rivers and oceans, and there are many reasons why they happen. Since I’m an ocean scientist, I’m going to explain the currents you find in the sea.  Some ocean currents are very large, and the biggest one – called the “global conveyor belt” – moves water very slowly all the way around the world. In fact, it takes water in the global conveyor belt about 1,000 years to get right around the planet.  Because the global conveyor belt and other big ocean currents move so slowly, we don’t notice them when we go to the beach. But we might feel some other types of currents when we go for a swim. When ocean waves get to a beach, they turn white at the top and crash onto the sand – this is called “breaking”. Swimming or surfing in breaking waves can be good fun, but we need to remember that these waves cause currents to form.  When waves break on the shore, the sea water in them gets pushed up against the beach. This water must get back out to sea somehow, otherwise we’d expect the water level at the beach to rise and rise forever. Of course, the water can’t get back out to sea near the surface, because that’s where the breaking waves are busy moving water toward the shore. So, two different currents form, to help take the water back out.  One of these currents is called the “undertow”. It forms beneath the breaking waves, and pulls the water back toward the sea, across the sandy seabed, out past where the waves are breaking.  Though the undertow helps to get some of the water back to sea, it’s not usually very strong. So, some of the work has to be done by another type of current, called a “rip” current. Rips are much stronger, narrow currents that run straight out to sea. Rip currents don’t happen all the way along the beach. They only form at certain “weak spots” along the beach where waves are not breaking, and the water is a bit deeper. This makes it easier for the water to flow back out to sea.  Here’s how it works: after water is brought in toward the shore by breaking waves, it can’t turn around and go straight out again, so it runs sideways along the beach in what we call a “feeder current”. As soon as it finds a weak spot, where the waves aren’t breaking, the water flows back out to sea in a rip current.  It’s very handy to know how to spot a rip current when you go to the beach, because they are much stronger than undertow currents and can sweep people out to sea. 


      Read more:
      Rip currents are a natural hazard along coasts – here's how to spot them


 When there are lots of waves breaking on the beach, it’s tempting to swim in places where the water looks calmer. But we know that rips form at the places where the waves aren’t breaking – so this is actually the worst place to swim!  Rip currents sometimes leave another tell-tale sign: because they’re so strong, they can churn up the sand on the seabed, making the water look brown and murky. Even if we know how to spot a rip current, it is always best to swim at beaches where there is a lifeguard, because they’re specially trained to know the best places to swim, and will always be on the look out to make sure everyone is safe. More Curious Kids articles, written by academic experts: How does heat travel through space if space is a vacuum? – Katerina, age ten, Norwich, UK. What makes a shooting star fall? - Katelyn, age seven, Adelaide, Australia. Is there a place in the middle of the English Channel where the waves change direction? – Sebastian, age 12, Kent, UK."
"**Six residents of a Dumfries and Galloway care home have died after a Covid outbreak, it has been confirmed.**
Dumfries and Galloway Health and Social Care Partnership is working with the operators of Alma McFadyen Care Centre in Dalbeattie.
A spokesperson for the health and social care partnership said it was an ""upsetting and concerning"" situation.
They added that the correct protocols for dealing with the pandemic were in place at the 24-bed home.
""This is obviously a very upsetting and concerning situation,"" the spokesperson said.
""We want to credit and pay tribute to the operators of Alma McFadyen and their extremely dedicated staff for their response.
""Covid-19 is incredibly infectious, and containing its spread is not at all easy - even when all the correct protocols are in place to address the virus.
""The coronavirus can result in mild symptoms and sometimes none at all, potentially masking its spread to more vulnerable individuals where it can pose a high degree of risk."""
"**Deaths involving Covid-19 have risen again in Wales to the highest weekly total since early May.**
A total of 190 deaths were registered in the week ending 13 November, according to the Office for National Statistics (ONS).
This is 24 more than the previous week and account for a quarter of all deaths in Wales.
Meanwhile, 19 more deaths have been reported linked to hospital infections at Cwm Taf Morgannwg health board.
There have been 10 more deaths at the Princess of Wales hospital, Bridgend, five at the Royal Glamorgan hospital and four deaths at Prince Charles Hospital in the last week.
It takes the total deaths to 177. That includes six deaths at Maesteg hospital reported a few weeks ago.
The number of cases linked to the outbreaks has slowed down, rising from 597 to 628.
The ONS figures show deaths in care homes involving the virus have also risen to their highest total - 36 - for five months.
The proportion of Covid deaths compared to all deaths is higher in Wales than in England in this latest week.
There were 56 deaths registered across the Cwm Taf Morgannwg health board area, which covers Rhondda Cynon Taf, Bridgend and Merthyr. Of those, 44 were in hospital.
There were also 51 deaths in the Aneurin Bevan health board area, across all settings, 29 deaths in Swansea Bay, 27 in Betsi Cadwaladr and 14 in Cardiff and Vale.
There were 10 deaths in Hywel Dda and three hospital deaths involving Powys residents.
The figures show:
So-called 'excess deaths', which compare all registered deaths with previous years, are above the five-year average.
Comparing current figures with the number of deaths normally seen at this point in the year is regarded as a useful measure of how the pandemic is progressing.
In Wales, the number of deaths fell to 742 in the latest week, but this was still 84 deaths (12.8%) higher than the five-year average.
In the latest week, England had 2,274 deaths involving Covid, followed by Scotland (278), Wales (190) and Northern Ireland (96)."
"Scott Morrison says he’s happy to work with the New South Wales government on its ambition to hit net zero emissions by 2050 because the premier, Gladys Berejiklian, “has a plan” – although the plan the prime minister referenced on Tuesday ends in 2030. Despite blasting Labor federally for adopting the same net zero target as the Berejiklian government, the prime minister told parliament circumstances were different in NSW because there was a strategy in place to deliver a transition.  “We have a plan,” Morrison told parliament on Tuesday. “That’s what the leader of the opposition doesn’t understand. He doesn’t have a plan, he just has some sort of vague commitment to something 30 years from now.” In declining to criticise NSW, while criticising Labor federally, Morrison cited a recent $2bn agreement his government signed with NSW as evidence there was a plan in the state. “Just a few weeks ago, the premier and I stood together and we agreed a plan, some $2bn, to invest in what is happening in New South Wales and in Australia, to achieve important targets,” the prime minister said. But the text of the agreement says the memorandum of understanding between the two governments commences on the day of signing, which was 31 January 2020, and continues until 30 June 2030. The text does not purport to be an agreement covering actions to 2050, although it does reference the NSW commitment. Morrison said on Tuesday the agreement with NSW was “about getting access to the gas that this country needs to ensure that we can firm up renewable investments in this country, which is at record levels, to put stability into our electricity grid”. “The problem with what the Labor party is proposing with their 2050 commitment, net zero target of 2050, is they have no plan.” While blasting Labor for adopting net zero, the government continues to leave its options open about whether to adopt a 2050 target down the track. Earlier in the day, the prime minister told colleagues in their weekly joint party room meeting it wasn’t about “being for or against the target”. “I won’t commit to anything that I don’t know the cost of, if I don’t know the impact on jobs,” Morrison told colleagues. “The leader of the opposition has got no plan he has got no clue what the impact would be.” Morrison also used a warning about the economic consequences of the rapidly-spreading coronavirus to criticise Labor for signing up to net zero. “Given the economic challenge, Labor’s reckless approach on the economy is particularly troubling, signing up to a target with no knowledge of what it would cost,” he said. While the political focus remained on 2050, Labor on Tuesday signalled it would adopt a more ambitious emissions reduction target for 2030 than the Coalition’s. Labor took a 45% emissions reduction target by 2030 to the last federal election, and is yet to determine its new medium term target. The process of revising the medium term target will be contentious in Labor ranks. The NSW right winger Joel Fitzgibbon has already argued publicly that the ALP should consider adopting a bipartisan position with the Coalition. The shadow climate change minister, Mark Butler, has rejected Fitzgibbon’s proposition, and told reporters on Tuesday Labor would base all of its proposed emissions reduction targets, including a new medium-term target, on scientific advice. He said the government’s 2030 target was consistent with 3C warming “which would be catastrophic for a vulnerable continent like Australia, and for the rest of the world”. “That’s why we have consistently opposed Tony Abbott and now Scott Morrison’s target for 2030,” Butler said. “The government’s current target, we still don’t know where that came from. “Tony Abbott plucked it out of the air. He’s never produced scientific evidence or modelling to show it’s consistent with what we need to do on climate change. “That will be our approach though, to take scientific advice, to work with the community, to work with businesses, on a proper pathway to net-zero emissions by 2050.”"
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Bloomberg: U.S. Northeast May Have Coldest Winter in a Decade
By Todd Zeranski and Erik Schatzker
The SOI has gone weakly positive again - click for larger
Sept. 28 (Bloomberg) — The U.S. Northeast may have the coldest winter in a decade because of a weak El Nino, a warming current in the Pacific Ocean, according to Matt Rogers, a forecaster at Commodity Weather Group.
“Weak El Ninos are notorious for cold and snowy weather on the Eastern seaboard,” Rogers said in a Bloomberg Television interview from Washington. “About 70 percent to 75 percent of the time a weak El Nino will deliver the goods in terms of above-normal heating demand and cold weather. It’s pretty good odds.”
Warming in the Pacific often means fewer Atlantic hurricanes and higher temperatures in the U.S. Northeast during January, February and March, according to the National Weather Service. El Nino occurs every two to five years, on average, and lasts about 12 months, according to the service.
Hedge-fund managers and other large speculators increased their net-long positions, or bets prices will rise, in New York heating oil futures in the week ended Sep. 22, according to U.S. Commodity Futures Trading Commission data Sept. 25.
“It could be one of the coldest winters, or the coldest, winter of the decade,” Rogers said.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92eb6f17',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Britain’s oil industry watchdog plans to overhaul its mission to wring as much value from the North Sea’s oil reserves as possible before the UN climate talks this year. The Oil and Gas Authority (OGA) was due to meet a government minister on Wednesday afternoon to discuss how the regulator, which was set up to extend the life of the North Sea, could play a part in tackling the climate crisis.  Andy Samuel, the OGA’s chief executive, told delegates at an industry conference that the watchdog was considering how it could help the UK meet its climate targets and would open a consultation on how to redefine the OGA’s strategy within the coming months. “We will be discussing this with a minister this afternoon,” he said. The overhaul follows a pledge by Ofgem, the energy regulator, to play a bigger role in helping to meet the government’s climate targets, after coming under fire for failing to prioritise the climate emergency. The OGA was established five years ago by the government to extend the life of the North Sea, after the oil price crash cast doubt over the future of the UK’s ageing oil basin. Since then, the statutory duty has raised questions over whether it is compatible with the government’s commitment to tackling the climate crisis. The regulator’s new strategy is expected to align its work safeguarding North Sea jobs and investment with the UK’s legally binding ambition to cut carbon emissions virtually to zero by 2050. This is likely to pile pressure on North Sea firms to shrink their carbon footprints by reducing flaring and methane leaks, and even use renewable energy to run the rigs. The OGA is also likely to push for companies to collaborate on big investments in carbon capture technology and clean hydrogen production, which could help the UK meet its net zero targets. The strategy may also mean the North Sea produces fewer barrels of oil if companies shift their portfolios from oil towards gas in line with future demand forecasts. Bob Ward, a director at the Grantham Research Institute on Climate Change at the London School of Economics, welcomed the OGA’s attempt to “reconcile the UK’s target for net zero emissions by 2050 with its strategy for maximising economic recovery” from the North Sea. “If the world is serious about achieving the Paris Agreement’s goal, it should mean a global decline in demand for oil and gas. The North Sea has relatively high operating costs compared with other basins, so its oil and gas might be among the first to be priced out of global markets if overall demand falls,” he said, speaking on the sidelines of the International Petroleum Week conference. The Department for Business, Energy and Industrial Strategy did not respond to a request for comment."
"School students across the UK (and the world) went on strike on February 15, leaving their lessons to protest the lack of effective action on climate change. Coordinated school strikes may be a novel tactic, but mass environmental activism isn’t. So will things be any more successful this time around? The first big global wave of ecological concern began in the late 1960s and involved fears of overpopulation, air and water pollution and the extinction of species. It peaked with the 1972 Stockholm Conference on the Human Environment, which kicked off international environmental politics. The next mass movement began in the late 1980s with concerns over the ozone hole, Amazonian deforestation and newly-voiced fears of climate change – then known as the “greenhouse effect”. That wave peaked with the 1992 Rio Earth Summit, which sought to tackle both global warming and biodiversity, and marked the beginning of coordinated climate action through the UN. That conference was addressed by a passionate and articulate young woman representing “ECO” – the Environmental Children’s Organization: From about 2006 to 2010 there was another, climate specific wave, beginning with Al Gore’s An Inconvenient Truth documentary, and groups like Climate Camp in the UK. It climaxed (or fizzled out) with the 2009 UN climate summit in Copenhagen. This wave saw the creation of various “Youth Climate Coalition” organisations in Australia and the UK. In academic terminology these periods of concern and relative indifference are known as the “Issue Attention Cycles”. This latest wave of climate action emerged in 2018, in the shape of Extinction Rebellion and its French cousin (or inverse) the gilets jaunes. Earlier in the year, Swedish schoolgirl Greta Thunberg had begun her solo “school strike” in Stockholm while, more or less simultaneously, activists in America launched the “Zero Hour” youth climate march. Alongside this activism, the IPCC released its report on what it would take to keep global warming below 1.5℃, and Mother Nature lent a hand with blistering hot summers in the UK, California and (more recently) Australia.  Previous bursts of environmental activism occurred before climate breakdown had been quite so obvious and severe. This time round, the heatwaves, hurricanes and floods will keep coming, perhaps making the latest wave of enthusiasm last longer. But what goes up must come down, and the students will find that it is very hard indeed to sustain emotional and physical mobilisation for a prolonged period. Right now, this issue is roughly where the Parkland shooting protests were last year – newsworthy for now, but the media caravan will inevitably move on.  That has consequences: when protests and actions stop getting the same amount of attention, and it seems that momentum is stalling, internal disagreements as to what is the best way forward, beyond a cycle of marches and symbolic strikes, will emerge, and will need to be managed skilfully. Some will want to work “within the system” and get invited onto advisory panels and into consultative processes. Others will have to get on with real life (university, paying the rent, working on, ah, zero-hour contracts). On one front, the young are lucky – their age means it is hard to see any direct infiltration and “strategic incapacitation” by undercover police. But the flip side is that social media offers virtually limitless surveillance possibilities. One possibility is an attempt to discredit and demoralise those who seem vulnerable. Elements of special interests like the oil and gas industry often try to “pick off” individual scientists or activists rather than take on a whole field – climate scientist Michael Mann has dubbed this the Serengeti Strategy as it resembles lions hunting the weakest zebras. We are already seeing this strategy in the latest wave of climate activism: recently Greta Thunberg had to address some rumours being circulated about her. Youth activists also face the problem that they may annoy their parents and grandparents. Yet before offering advice to the young, we older people have to ask ourselves, why should they listen to us? We’ve known about the problem and either been ineffective or done nothing. It is children who are owed an enormous apology and expression of humility. So for the latest generation of climate campaigners, my top four pieces of advice (see here for a longer list), based on both my activism and my time in academia, are as follows: Be aware of emotions. People won’t be persuaded just by being given more information on global temperatures or carbon budgets – psychological skills will matter too. Your parents are probably wrestling with fear (aren’t we all?) and guilt for not having sorted this out before you had to. Fear and guilt make can make people oscillate from action to inaction, pessimism to optimism. Traditional “social movement” activities (marches, petitions, protests, camps) have a short shelf-life. The media gets bored and stops reporting. Meanwhile, those in power learn how to cope with the pressure. Be very careful about getting drawn into the Big Marches In London syndrome. You’re going to need to innovate, repeatedly. Even though time is short, this is still a marathon, not a sprint. But what would you say? How should we older people offer advice, when, who to, and about what? Suggestions in the comments please."
nan
"
Share this...FacebookTwitterIt’s a slow climate news day here in Germany, and this story at chinadaily.com happened to catch my eye.
Russian scientists expect to meet aliens by 2031
So there you are. Kook scientists (funded by the poor taxpayers) are also to be found in fields other than climate science. To be honest, I’d say the odds these aliens being discovered are likely greater than some of the goofy climate predictions we’ve heard from GISS or the PIK coming true. The China Daily writes:
Russian scientists expect humanity to encounter alien civilizations within the next two decades, a top Russian astronomer predicted on Monday.
‘The genesis of life is as inevitable as the formation of atoms… Life exists on other planets and we will find it within 20 years,’ Andrei Finkelstein, director of the Russian Academy of Sciences’ Applied Astronomy Institute, was quoted by the Interfax news agency as saying. Speaking at an international forum dedicated to the search for extraterrestrial life, Finkelstein said 10 percent of the known planets circling suns in the galaxy resemble Earth.
Where does he come up with 10%? Fat chance of that being true. And planets that resemble earth? Right!



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




If water can be found there, then so can life, he said, adding that aliens would most likely resemble humans with two arms, two legs and a head. “They may have different colour skin, but even we have that,” he said.
Gee, and I thought alarmist climate scientists were kooks and losing it completely.
I agree that there is a chance that there is “other life” out there – maybe way way out there, like 100 million light years away, and in some “weird” form. But the chances that there is a planet with creatures with “two arms, two legs and a head” living on it, and close enough to be discovered, and that in the next 20 years, is statistically zero. That would require a planet whose numerous physical features would be incredibly similar to the earth’s, and one that would also have followed a similar geological, climatic multi-million year history and evolution. We’re talking zillions of factors here.
Forget it.
But let’s say we did miraculously discover such creatures. Then the planet on which they live would very very very likely be thousands if not millions of light years away, meaning the images and signals that we would be receiving would be thousands or millions of years old, meaning the creatures would be long dead anyway.
Sorry, but we’re on our own here, and our stay is temporary. Just be glad you even got the precious chance to know the earth.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterTuvalu is saved! What follows is a press release from the Leibniz Institute for Marine Science (IFM-GEOMAR) on a new paper appearing in the GRL, which shows sea level changes are far more complex than first thought. It’s back to the drawing board for climate and sea level modellers. (Hat-tip Science Skeptical)
==================================================
Quo Vadis Sea Level? New Study Shows Ocean Currents Lead To Strong Regional Fluctuations
Dr. Andreas Villwock
Scientists of the Leibniz Institute for Marine Science (IFM-GEOMAR) have now shown that there are large regional variations when it comes to sea level change. The causes are due to changes in ocean currents, which lead to varying sea levels, especially in the tropical Pacific and Indian Oceans.



Chart above: Sea level fluctuations caused by wind and ocean currents (relative to mean global sea level rise) for the period 1958-2007 (in cm). The model simulation shows regions with sunken sea level (blue) in the tropical Pacific and Indian Ocean. Graphic from IFM-GEOMAR.
Why has the sea level in some regions of the tropical Indian Ocean and Pacific risen strongly over the last 15 years, while in the decades before the sea levels at these locations dropped? The ocean scientists from Kiel are uncovering why by using computer simulations. A paper now appearing in the Geophysical Research Letters shows that fluctuations in ocean currents, caused by trade winds in the tropical Pacific, play an important role.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The impact of wind and ocean currents are prevalent in the tropical Pacific especially in the wake of the El Niño phenomena. “The associated swashing back and forth of the warm surface water leads to a continuous rise and drop in sea level of up to 20 cm within just a few years“, explains oceanographer Franziska Schwarzkopf of the Leibniz Institute for Marine Science (IFM-GEOMAR) and author of the study.
While these short term fluctuations are well documented by modern satellite measurements, little was known about the long-term pattern of changes. “Our computer simulations which use current models show that regional water levels also over time periods of several decades are affected by wind changes and ocean currents“, says Professor Claus Böning, director of Kiel Ocean-Modelling and co-author of the study. A surprising finding from the scientists in Kiel:
In the middle of the last 50 years, some areas in the tropical Pacific and Indian Ocean experienced a drop in sea levels, contrary to the global trend.”
These new results on sea level rise of the last decades mean an additional challenge for climate modeling. “Whether a group of islands has to reckon with a greater increase in sea level with respect to the average, or can reckon with a temporary drop over the next decades depends decisively on the development of the wind systems and ocean currents“, says Böning. “Future research programs will put increasing focus on the regional fluctuations in the oceans.“
The paper: Schwarzkopf, F.U. and C.W. Böning, 2011: Contribution of Pacific wind stress to multi-decadal variations in upper-ocean heat content and sea level in the tropical south Indian Ocean. Geophysical Research Letters, 38, L12602, doi: 10.1029/2011GL047651.
Contact:
Prof. Dr. Claus Böning, Tel: (+49) 431 600-4003, cboening@ifm-geomar.de
Dr. Andreas Villwock (public relations), Tel: (+49) 431 600-2802, avillwock@ifm-geomar.de
================================================
Other reading (h/t: Krishna Gans): Meteorologically driven trends and worldclimatereport.


Share this...FacebookTwitter "
"The Beast from the East, a polar vortex which brought freezing conditions to the UK, arrived on February 26 2018. Two days later there was a minimum temperature of -11.7°C (10.9˚F) at South Farnborough in Hampshire, and a maximum of only -4.8°C (23.4˚F) at Spadeadam in Cumbria. In sharp contrast, on February 26 2019, temperatures reached 21.2˚C (70.2˚F) at Kew Gardens in south-west London – the warmest winter day since records began. In February 2019, bumblebee queens were out looking for nest sites, adult butterflies were emerging from their winter hibernation and blossom appeared on some trees and shrubs. But what will be the long-term effects of 2019’s early spring? The relatively new science of phenology examines the timing of the seasons by plotting the calendar records of first plant bud, first flower, first nesting behaviour and first migrant arrivals. Over the past three decades, these records have confirmed that spring temperatures are generally arriving earlier in the year. As the days get longer and warmer in the northern hemisphere, bird species such as the barn swallow follow these natural cues to depart for British habitats, where they nest and rear their young. These insectivorous migratory birds time their breeding season to coincide with insects being present in sufficient numbers to feed their young.  While the main trigger for birds to migrate from their overwintering grounds to Britain is the length of daylight, temperature will also fine-tune the arrival date. An early spring means that insects could emerge and breed before migratory birds arrive. Once in the UK, the birds may find there are fewer insects to eat and this results in fewer chicks fledging, which leaves their predators, including the sparrowhawk and the stoat, with less to eat. The disconnect between the arrival of insectivorous birds and the abundance of insects ripples through the ecosystem, affecting other animals and plants that at first sight may not seem linked to this seemingly benign change. Of course, much depends on how long the warm weather lasts and what follows. The mild conditions in 2019 have prompted buds to burst on some plants – this makes the flowers and leaves vulnerable if the weather reverts to colder conditions. If the temperature drops to low single figures in degrees Celsius but remains above freezing, growth rate will slow and the plant’s growth will be stunted. A hard frost, on the other hand, would damage or even kill any of the flowers and leaves that have emerged during the warm spell. At the end of twigs is the apical meristem – the site where rapid cell division generates new plant material which results in the twigs growing longer each year. No longer protected from frosts within a bud, apical meristems are vulnerable to damage by frost. The result is that twigs will stop growing. Rather, a new apical growth point will establish itself from a bud nearer to the main trunk of a tree or the main stem of a bush. The long-term effect is that a twig develops in a different direction, and the plant carries this signature of frost damage for the rest of its life. An early spring might also accelerate climate change. Scientists at the Vienna University of Technology and the University of Leeds studied satellite data for the northern hemisphere – from southern Europe and Japan up to and including the Arctic tundra – and demonstrated that in many regions of the Earth, an early spring leads, counter-intuitively, to less plant growth.  This may be because certain plants have a predetermined growth period. Growth in early spring means an early cessation of growth later in summer or early autumn. Greater plant growth in the spring could also result in increased transpiration – the process by which moisture is drawn through plants from roots to small pores on the underside of leaves, where it becomes vapour which is released to the atmosphere. This causes a higher demand for water during the growing season which cannot be met if the summer and autumn are also dry. The result of this early growth is limited plant growth overall through the entire year. Plants that do not grow as big as they might absorb less carbon, so reduced plant growth means less carbon stored in vegetation, and that in turn means more carbon dioxide in the atmosphere, more warming and even earlier springs – a positive feedback loop. 


      Read more:
      The Arctic is turning brown because of weird weather – and it could accelerate climate change


 Many people have worried about the unseasonable warmth and spring-like conditions of February 2019. As unseasonably mild weather brings about changes in plant growth that could accelerate climate change and widen the disconnect between elements of ecosystems, this unusual week may leave an even more worrying legacy. However, even in the midst of a winter warm spell, the Met Office forecast predicts less mild and more changeable conditions to come – showery rain in some parts and stormy conditions elsewhere, with overnight frosts still possible. There is even a mention of snow on high ground. Such are the vagaries of the British weather."
"**Ministers are going to be asked to re-think the plan for gym classes in England from next week.**
Under the new three-tier proposals announced yesterday, they won't be allowed in the very high alert areas.
From 2 December gyms can re-open no matter what tier they're in - but if you're a fan of HIIT or yoga, they'll be banned in tier three.
UKactive, which represents a lot of the fitness industry, says it's ""disappointed"" and wants this changed.
Ministers say group classes will have to close in tier three because worries coronavirus could quickly spread in them.
This announcement shows a positive shift in the Government's desire to strengthen our nation's physical and mental resilience,"" says Huw Edwards, CEO of UKactive.
""However, we are disappointed to note that indoor group exercise is included in tier three restrictions and will not be permitted unless in household groups or bubbles.""
The body says it wants the government to change its decision and carry out a review into the role physical activity plays in society.
""The sector has proven this activity can be undertaken in a manner that is safe,"" says Huw.
""Using a combination of social distancing, sanitisation and increased ventilation.""
The government has yet to respond to this call by the fitness industry, but the Prime Minister Boris Johnson has said he's ""very sorry"" for the problems these measures are causing for business owners.
But he added that ""things will look and feel very different"" after Easter as a vaccine is rolled out.
Measures in Scotland, Wales and Northern Ireland will continue to be decided by its local governments, but a joint approach to Christmas is set to be announced.
_Follow Newsbeat on_Instagram _,_Facebook _,_Twitter _and_YouTube _._
_Listen to Newsbeat_live _at 12:45 and 17:45 weekdays - or listen back_here."
"**A high street voucher scheme will see pre-paid cards issued to individuals in Northern Ireland, not to each household as previously thought.**
The scheme is still being worked on, but BBC News NI understands the amount given to each qualifying person could be between Â£75 and Â£100.
It was previously suggested that Â£200 would be awarded to each household.
The Department for the Economy has now provided more detail on the scheme that it expects to roll out early next year.
The voucher scheme was announced as part of a raft of financial measures outlined on Monday.
The aim of the scheme is to support bricks and mortar retail which has been adversely affected as a result of coronavirus restrictions.
Non-essential retail in Northern Ireland will be shut from Friday as part of a two-week circuit breaker.
The budget for the vouchers is Â£95m and that will also cover any administration fees a chosen provider may charge.
About 1.1m people could be issued a pre-paid card, which suggests there will be some sort of age limit on who will be eligible.
The money can be spent in shops, but not online.
It is based on similar schemes in Jersey and Malta.
Earlier this year, Jersey introduced an Â£11m scheme through which residents received Â£100 each to stimulate the island's economy.
Northern Ireland's First Minister Arlene Foster has previously said she was impressed by the initiative.
Murray Norton, chief executive of Jersey's Chamber of Commerce, told BBC NI's Good Morning Ulster the initiative had been ""good news for the consumer and the High Street"".
""Jersey is an island, so you can't get off it to go and spend it anywhere else for starters, but there were limits on the pre-paid card that everyone received,"" he said.
""Yes it did help consumers and yes it did help shopkeepers locally, by how much compared to how much was spent?
""The jury is still out on that.""
Northern Ireland's Finance Minister Conor Murphy said on Monday that the pre-paid card issued through the voucher scheme would be worth Â£200 per household.
Clarification that it would be paid per individual was issued on Tuesday by Diane Dodds' Department for the Economy."
