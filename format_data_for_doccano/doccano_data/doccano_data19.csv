nan
"It’s going to be a rough year. The fatal combination of escalating climate breakdown and the capture of crucial governments by killer clowns provokes a horrible sense of inevitability. Just when we need determined action, we know that our governments, and the powerful people to whom they respond, will do everything they can to stymie it. Witness the disasters in Australia. In mid-December, on the day the nation’s lethal heatwave struck, Rupert Murdoch’s newspaper the Australian filled its front page with a report celebrating new coal exports and a smear story about the chiefs of the state fire services, who were demanding an immediate end to the burning of fossil fuels. The response of the prime minister, Scott Morrison, to the escalating catastrophe was to embark on a holiday overseas as his country burned.  Some of the Earth’s largest land masses – Australia, Russia, the US, Brazil, China, India and Saudi Arabia – are governed by people who seem to care little for either humankind or the rest of the living world. To maintain their grip on power, which means appeasing key oligarchs and industries, they appear prepared to sacrifice anything – including, perhaps, the survival of humanity. I know that the protesters who made 2019 the year of climate action will continue to step up. We will do all we can to focus the world’s attention on the greatest crisis human beings have ever faced. But with hostile governments blocking a collective international response to this emergency, the struggle will feel increasingly desperate. I admit that I’m feeling quite close to burnout. I believe resilience is the most useful human quality, and I’ve sought to cultivate it, but in 2019 I felt my resolve begin to weaken at times as it has never done before. Part of the reason is doubtless my continuing health issues: the repeated complications and procedures that have followed my cancer treatment two years ago. Sometimes it’s hard to disentangle the external and internal sources of despondency. For many people, there is no such separation. We now know that people living in heavily polluted places have higher levels of depression and suicide: air pollution has been implicated in brain inflammation and damage to nerve cells, both of which have been linked to mental illness. Research has also linked wellbeing with environmental quality: a recent study in Denmark suggests that people who grew up in places with plenty of green space have a lower chance of developing a psychiatric disorder than those who were surrounded only by artificial surfaces, even when all other factors have been taken into account. I have tried to keep my eco-anxiety at bay, to box it into my working life. But every month this becomes more difficult. The rising sense of panic I feel is entirely rational; we should all be feeling it. But we can’t live with it through every hour of every day. So my new year’s resolution is to spend more time on my sea kayak. It possesses almost miraculous properties: it is a four-metre, plastic rejuvenation machine. After a day on the water, ideally paddling as far as I can, sometimes until the coast is out of sight, I feel ready for anything. But even across this experience a shadow now falls: my gathering awareness of what I should be seeing at sea, and its resounding absence. The shocking and distressing fact is that the waters around the UK were once among the most abundant on Earth, and are now among the least. Armadas of bluefin tuna once stormed our coasts, harrying shoals of mackerel and herring many miles long. Halibut the size of barn doors and turbot like tabletops came into shallow water to feed. Cod commonly reached almost two metres; haddock grew to a metre. Plaice were the size of road atlases. Pods of fin whales and sperm whales could be seen from the shore, while Atlantic grey whales, now extinct, roamed our estuaries. Gigantic sturgeon poured up the rivers to spawn, pushing through traffic jams of salmon, sea trout, lampreys and shad. On some parts of the seabed the eggs of the herring lay six-feet deep. Much of the sea floor was covered by a continuous crust of life: reefs of oysters and mussels, soft corals and sea pens, sea fans and sponges, peacock worms and anemones, stabilising the sediments and filtering the water column, with the result that our seas might have been crystal clear. The abundance of everything, if we were to transport ourselves back a few hundred years, would blow our minds. Now, on some days, it’s a surprise to see anything. I might, if I’m lucky, spot a flock of shearwaters, skimming the waves with their velvety wings, a couple of gannets, a solitary razorbill, the occasional small bait ball. When I kayak in Cardigan Bay, in Wales, what I hope to find above all else is dolphins. Sometimes I do, and these days are the waymarks of my life. But my sightings seem to have become less common since scallop dredgers were allowed back into even the most “strictly protected” parts of the bay by the Welsh government, ripping up the seabed and destroying most of the life it harbours. The same applies to nearly all the “marine protected areas” around the UK’s coasts. But these zones amount to little more than lines on a map. While 36% of England’s waters are theoretically set aside for wildlife, commercial fishing – which has by far the greatest impact on the life of the seas – is excluded from less than 0.1% of that area. In fact, the trawling intensity in “protected” zones is higher than in unprotected places. It’s all so stupid. Commercial fishing is by far the greatest cause of ecological destruction at sea, but produces less income and employment in the UK than the industries it wrecks. Recreational angling alone, which is perpetually threatened by the absence of fish, generates more jobs and money than commercial fishing. Whale and dolphin watching, diving and snorkelling would, if allowed to prosper, greatly enhance the livelihoods of coastal people. And this is to say nothing of the immeasurable improvements in the lives of everyone connected to a thriving, abundant living system. If we stop dragging trawls and dredges through it, the life of the seas would recover with astonishing speed. Because most marine animals are highly mobile during at least one stage of their development, the rewilding of the seas needs little help from humans. But we could make a few useful interventions, such as the possibly crazy but wonderful idea once proposed by two researchers at the University of Central Lancashire of transporting Pacific grey whales to the Atlantic; and the less crazy but equally wonderful idea of reintroducing the Dalmatian pelican – a species that was native to the UK until the middle ages. Both species play a crucial role in marine food webs, and can fill our lives with wonder. Recharging nature recharges the human spirit. In 2020, we could all do with some of that. • George Monbiot is a Guardian columnist"
"Prince William has announced what was described as “the most prestigious environment prize in history” to encourage new solutions to tackling the climate crisis. The “Earthshot prize” will be awarded to five people every year over the next decade, the Prince said on Tuesday, and aims to provide at least 50 answers to some of the greatest problems facing the planet by 2030.  They include promoting new ways of addressing issues such as energy, nature and biodiversity, the oceans, air pollution and fresh water. The prize, inspired by US president John F Kennedy’s ambitious “Moonshot” lunar programme and backed by Sir David Attenborough, promises “a significant financial award”, a statement said. The Duke of Cambridge, a grandson of the Queen and second in line to the throne, said the Earth was “at a tipping point” and faced a “stark choice”. “Either we continue as we are and irreparably damage our planet or we remember our unique power as human beings and our continual ability to lead, innovate and problem-solve,” he said. “Remember the awe-inspiring civilisations that we have built, the life-saving technology we have created, the fact that we have put a man on the moon. People can achieve great things. “The next 10 years present us with one of our greatest tests – a decade of action to repair the Earth,” he said. ""The earth is at a tipping point and we face a stark choice: either we continue as we are and irreparably damage our planet or we remember our unique power as human beings and our continual ability to lead, innovate and problem-solve."" — The Duke of Cambridge @EarthshotPrize pic.twitter.com/SfGaKY9qsG The award, which will be launched later in the year and bestowed from 2021, is open to individuals as well as communities and businesses. It has the support of conservation campaigners, groups and scientists, including the veteran British natural history broadcaster Attenborough. “The spirit of the Moonshot can guide us today as we confront the serious challenges we face on Earth,” Attenborough said in a film to mark the launch. “This year Prince William and a global alliance launch the most prestigious environment prize in history ... designed to motivate and inspire a new generation of thinkers, leaders and dreamers to think differently.” Both Prince Charles and Prince Philip have campaigned for environmental causes and against the illegal trade in wildlife around the world. William is hoping to build on their work through the prize, which will initially be run by his and his wife’s own charitable foundation."
"No one enjoys choking on smog, but are more trees really the answer for polluted city air? It’s not as clear-cut as you might think. Air pollution is clearly a problem for health and well-being – and as more and more people across the world move to live in megacities, they could miss out on the fresh air associated with the green countryside. So far, many strategies have been put in place to try and mitigate urban air pollution: from introducing congestion charges  to imposing car bans, promoting electric vehicles to providing more car-free zones. One group has even suggested making London a “national park city”. If you can’t escape to the countryside, then you can instead bring the countryside feel to the city. Urban planners seem to be increasingly focused on promoting more green spaces in our towns and cities, creating a truly “urban jungle”. Why not? After all, trees really do make you feel better, according to recent research published in the journal Scientific Reports. The study, in Canada, found ten extra trees in a city block meant local people’s health perceptions improved an amount comparable to being given a US$10,000 raise or suddenly being seven years younger. One of the reasons for this, the researchers suggested, was that “trees reduce air pollution”. No one can deny that finding a quiet space in a bustling city is challenging, and that city parks offer a place for you to catch a breath of fresh air. Thus given how trees improve our wellbeing, supporting the concept of making a city greener seems like a no-brainer … right? Work in which I have been involved considers how air flows in and around city streets, dispersing vehicle emissions on innocent pedestrians and cyclists. If you consider any wall, parked cars, hedges or trees as barriers that cause the natural pattern of air flow to be diverted, then you can see how trees may not always point transport pollutants in the “right” direction. On the extreme side of things, your typical street with avenue trees, can almost lead to a “green” roof effect, when the canopy in full bloom. This can prevent pollutants from escaping the street and air quality can be greatly impacted. In less extreme circumstances, a single tree in a street corner may break the wind flow and lead to pollution dropping into the breathing zone of pedestrians walking by. To cut a long story short, trees can be as detrimental to air quality as a Slipknot concert in your apartment is to noise pollution. It’s all about their location. Carbon dioxide is usually labelled as the “bad guy”, and it is – in terms of climate change at least. However when it comes to our health the range of other pollutants emitted from cars such as nitrogen oxides (NOx) and particulate matter (PM) present the greatest risk. Young children, the sick and the elderly are particularly vulnerable to harmful gases and particles released by fuel combustion in cars, lorries and buses. Whatever impact trees may have on reducing carbon dioxide in cities, how they may, or may not, control these other pollutants is not too clear. The importance of trees is not in question, as protecting our green spaces is vital for the environment on a global scale. However, if we choose to plant trees in our city streets, at a local level, the outcome on air quality may be somewhat different. It is important to consider the type of tree you wish to plant, the shape of the street, what direction the wind blows, and where your pollutant source (cars) and receptors (pedestrians) are located. Work by a Belgian research group entitled “Improving local air quality in cities: to tree or not to tree?” (… that is the question) sums things up. Sometimes, planting trees in cities is driven by people who may be informed of the benefits, but not all of the facts. I just want to make sure all the facts are looked at to make a well-informed decision. It’s what research is about. It is the responsibility of urban planners and local authorities to ensure trees are not just planted where they look nice, but perhaps where they can do more good than bad for air quality on city footpaths. I’m in no way saying trees in cities are bad, they do add some colour to our otherwise grey landscape, but I will say that “terms and conditions apply”. Before planting those urban trees, make sure you read the small print."
"Although Tesla’s Powerwall battery storage is likely to be a disruptive force for electrical energy systems around the world, it is not going to supplant the major forms of electrical energy storage anytime soon, and is ill-suited to storing energy over longer timeframes such as between seasons. When powering a modern economy, electrical energy comes with one major disadvantage – it requires that supply and demand must be balanced within strict limits at all times, as an imbalance will lead to voltage changes that can damage connected equipment and even the networks themselves. This challenge of network balancing has conventionally been managed by keeping the electricity “stored” in fossil or nuclear fuels until it is required.  This presents a storage problem for renewables, especially weather-dependent wind and solar. As we cannot hope to store the weather, we have to instead store the generator’s output, which is the electricity. This is what Tesla’s Powerwall and the bigger Powerpack both intend to help with. At full capacity Tesla’s first Gigafactory is planned to produce batteries totalling 35 GWhe (gigawatthours electrical) each year. It’s an impressive figure – enough to power England, Scotland and Wales for about an hour. However, to put this into perspective, Great Britain (Northern Ireland is counted separately) stores on average 800 times as much energy in its coal stockpiles alone. The chart below shows the total electrical energy equivalent stored in its coal stockpiles over the past two decades, with the amount stored tending to peak each autumn in preparation for an increase of electrical demand over winter. (The 2009 spike was partly caused by power plants stocking up on cheap coal after the financial crash). Over these two decades Great Britain has averaged just over 30,000GWhe of electrical energy stored this way. Great Britain also has up to 32,000GWh of natural gas in long-term storage (which dropped from 41,400GWh earlier this year). While most is used directly for cooking or heating, the rest is consumed to make electricity. It’s tough to know exactly what proportion this is, since it’s difficult to apportion gas that came directly from storage as opposed to gas that came via liquified natural gas ships or through pipelines. However if we estimated that 30% ends up in power plants with an efficiency of 50% then the electrical energy stored is around 4,800GWhe. Again, far more than Tesla’s Gigafactory annual 35GWhe output. It’s even harder to tell how much energy is stored in Great Britain’s reserves of nuclear fuels of uranium and plutonium – the information is classified. These simple calculations show Great Britain remains overwhelmingly dependent on fossil fuels to store its electrical energy. The country needed nearly 800 GWhe of electrical energy in an average day last year, which suggests that batteries would have some way to go to supplant fuels, even for a day. This storage of vast amounts of energy in fossil fuels is typical of many modern economies as we have become accustomed to the flexibility that they allow us; they are able to decouple supply from demand by location as well as by time, and at a phenomenally low cost for storage. Regardless of the falling price of batteries (Tesla’s or anyone else’s) they are always likely to be ill-suited to the long-term seasonal storage of thousands of GWhs of energy. Batteries are simply the wrong technology. But the fuels we mainly rely on for electrical storage – coal and gas – are not ultimately sustainable, and contribute to increased CO2 emissions if burned without carbon capture. So if fossil fuels and batteries are not the answer to the seasonal storage of electricity – what is? Carbon capture and storage holds out the promise of the continued use of fossil fuels, with all the fuel storage advantages this would allow. Additionally, fuels such as hydrogen or synthetic methane could be generated using low-cost, low-carbon energy and stored at scale too.  A continued use of fuels isn’t necessarily competition for Tesla’s disruptive new battery packs; the two complement each other. They are different horses for courses, as not all energy storage is the same, and in the long run we are going to require both. Tesla’s batteries aren’t about to take over the world just yet, especially if you count fuels as “energy storage”. But if the company goes on to produce a similarly disruptive force in the area of power-generated fuels, that really would be a gamechanger. Lets hope it or some other firm does soon – then we would truly have the components to decouple our economies from carbon emissions."
nan
"
Share this...FacebookTwitter
Source: American Geophysical Union Fall Meeting
At next month’s American Geophysical Union (AGU) Fall Meeting in New Orleans (US), an independent researcher named Trevor Underwood will be presenting an equation-rich analysis that thoughtfully undermines the perspective that increases in CO2 concentrations are a fundamental variable affecting climate.
Instead, Underwood argues that the absorption band where CO2 emissivity could have an effect is likely already saturated, precluding the capacity of increased CO2 concentrations to produce atmospheric warming.
He also advances the position that solar irradiance changes can explain modern temperature variations, which is consistent with other recent analyses.
It seems that more and more of these papers questioning the “consensus” view on the efficacy of the CO2 within the greenhouse effect are being considered in scientific circles.  Several previous examples are listed below.
The volume of contrarian analyses would seem to suggest that the climate’s specific sensitivity to CO2 concentration changes is not yet settled.
And so the debate rages on.

•   Another New Paper Dismantles The CO2 Greenhouse Effect ‘Thought Experiment’
•   New Paper: CO2 Has ‘Negligible’ Influence On Earth’s Temperature
•   3 Chemists Conclude CO2 Greenhouse Effect Is ‘Unreal’, Violates Laws Of Physics, Thermodynamics
•   Ph.D. Physicist Uses Empirical Data To Assert CO2 Greenhouse Theory A ‘Phantasm’ To Be ‘Neglected’
•   Swiss Physicist Concludes IPCC Assumptions ‘Violate Reality’…CO2 A ‘Very Weak Greenhouse Gas’
•   Recent CO2 Climate Sensitivity Estimates Continue Trending Towards Zero
•   A Swelling Volume Of Scientific Papers Now Forecasting Global Cooling In The Coming Decades
•   Russian Scientists Dismiss CO2 Forcing, Predict Decades Of Cooling, Connect Cosmic Ray Flux To Climate
•   2 New Papers: Models ‘Severely Flawed’, Temp Changes Largely Natural, CO2 Influence ‘Half’ Of IPCC Claims
•   Leading Heat Transfer Physicists/Geologists Assert The Impact Of CO2 Emissions On Climate Is ‘Negligible’
•   New Atmospheric Sciences Textbook: Climate Sensitivity Just 0.4°C For CO2 Doubling
•  U of Canberra Expert: Doubling Atmospheric CO2 Would Increase ‘Heating By Less Than 0.01°C’
•   Uncertainties, Errors In Radiative Forcing Estimates 10 – 100 Times Larger Than Entire Radiative Effect Of Increasing CO2
•   New Paper Documents Imperceptible CO2 Influence On The Greenhouse Effect Since 1992



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Underwood, 2017
No Increase in Earth’s Surface Temperature From Increase in Carbon Dioxide 
A critical look at these different in situ measures of the Earth’s surface temperature identified a divergence between land and marine surface temperatures, with land surface air temperatures showing a significant and increasing rate of warming of around 0.5°C between 1880 and 1981, and 0.7°C between 1982 and 2010, whilst marine air temperatures show little if any change between 1880 and 2010 (Underwood (1) 2017). Recent academic literature is also beginning to question the accuracy of the adjusted in situ data (Kent et al. 2017).
In order for an increase in carbon dioxide or other greenhouse gas concentration in the atmosphere to result in an increase in the surface temperature of the Earth, it must be able to increase the absorption of infrared radiation emitted from the surface. This would result in an increase in the absorption factor, f. However, as seen above f is currently around 0.9444. Absorption of infrared radiation by molecules of greenhouse gases, involves increasing the internal energy of the molecule by changing the quantum state of the molecules, which can only occur at particular wavelengths, known as absorption bands. 
These absorption bands can be extended by what is referred to as pressure broadening (Strong and Plass 1950; Kaplan 1952), but when all of the emitted infrared radiation within these absorption bands has been absorbed by greenhouse gas molecules in the atmosphere, no further absorption of the terrestrial radiation is possible. The radiation with wavelengths falling outside of the absorption bands passes through the atmosphere and escapes into space.
Absorption of solar radiation in in the stratosphere is almost 100% efficient in the ultraviolet due to electronic transitions of oxygen (O2) and ozone (O3) and a significant amount of solar radiation is absorbed by water vapor (H2O) in the lower atmosphere. It is primarily the visible radiation that is absorbed at the Earth’s surface. In the infrared, absorption is again almost 100% efficient because of the greenhouse gases, but there is a window between 8 and 13 mm, near the peak of terrestrial emission, where the atmosphere is only a weak absorber except for a strong ozone feature at 9.6 mm. This atmospheric window allows direct escape of radiation from the surface of the Earth to space and is of importance in determining the temperature of the Earth’s surface (Jacob 1999).
Additional leakage could occur if the greenhouse gas concentration in the atmosphere were insufficient to absorb all of the infrared radiation in the absorption bands emitted by the Earth’s surface, but due to the extent of the atmosphere and its known unsaturated state, it is more likely that the current leakage corresponds to radiation in the part of the infrared spectrum that does not fall in the greenhouse gas absorption and emission bands, referred to as the “infrared window”. As a consequence, even in the case where there is leakage of infrared radiation from the Earth’s surface directly into space, as long as the atmosphere is able to absorb all of the upwelling infrared radiation in the greenhouse gas absorption bands, neither the amount of this leakage nor the amount of the absorption will depend on concentration of greenhouse gases in the atmosphere. From the emission spectra (a) and absorption percentages (b) in the diagram above (Fig. 2.6, Yang 2016), where the 255°K blackbody curve represents the terrestrial radiation, it appears that at the current surface temperature and absorption factor of 0.9444 all of the radiation within the emission bands is fully absorbed, and that the remaining 5.56 percent of the infrared emission represents radiation with wavelengths within the atmospheric window. If this is true, there can be no further increase in f [absorption], and no increase in the surface temperature with an increase in carbon dioxide.
Increase In Solar Forcing Explains Recent Warming
The difference between the minima showed an increase of 0.2812 W/m-2 for VIRGO; 0.4701 W/m-2 for ACRIM; and 0.2650 W/m-2 for ACRIM + TIM over the 11.6 year solar cycle 23 beginning in May 1996 and ending in January 2008; or 0.24 W/m-2 per decade for VIRGO, 0.40 W/m-2 per decade for ACRIM, and 0.23 W/m-2 per decade for ACRIM + TIM (pmodwrc website
 2016).
These decadal increases in TSI [Total Solar Irradiance] from ACRIM, SARR, VIRGO and ACRIM + TIM are sufficient to explain the whole of the increase in surface temperature estimated from in situ data during the last 100 years. They compare with the six published model-based estimates of forcing examined in Schwartz (2012) that showed forcing by incremental greenhouse gases and aerosols over the twentieth century ranging between 0.11 and 0.21 W/m-2 per decade.
Summary
Solution of the Greenhouse Effect equations based on a more realistic atmospheric model that includes absorption of solar radiation by the atmosphere, thermals and evaporation, and an examination of the fraction of terrestrial infrared radiation absorbed whilst passing through the atmosphere, suggests that the contribution of greenhouse gases to the surface temperature is close to its upper limit. Any further contribution would depend on an increase in the infrared absorption factor of the atmosphere from its current level of around 0.9444, which seems unlikely. As this appears to correspond to total absorption of all black body infrared emission from the Earth’s surface at wavelengths at which there are greenhouse gas absorption bands, including for water vapor, it seems likely that we are close to the thermodynamic limit of greenhouse warming for the current luminosity of the sun, and that any further increase in carbon dioxide concentration in the atmosphere will have little or no effect on the surface temperature of the Earth. Questions about the reliability of in situ measurements of surface temperatures also raise questions about current estimates of global warming. Moreover, recent evidence from satellite measurements of solar irradiance, indicate that any recent warming could be due to increasing solar irradiance.

A conference paper with a similar conclusion regarding the emissive/warming limitations of increased CO2 concentrations was presented by a molecular physicist, Dr. N. Doustimotlagh, at the World Conference On Climate Change in October, 2016.



Doustimotlagh and Mirzaee, 2016


So because of the limited values of electromagnetic waves that come from Earth and limitation of  absorption of greenhouse gasses, the greenhouse effect of greenhouse gasses should be limited.  In other words, after absorbing of all the IR waves that come from Earth by greenhouse gasses in the atmosphere, there are no IR waves to cause greenhouse effect. It means that “two things that cause greenhouse effect are greenhouse gasses and IR waves in absorption spectrum of these gasses, so if greenhouse gasses increases but there are no IR waves, it is natural that there is no greenhouse effect”.

If the concentration of CO2 in atmosphere increases until absorbs all the values of  electromagnetic waves that are absorbable for CO2, additional values of CO2 should not have greenhouse effect.



Share this...FacebookTwitter "
"
Share this...FacebookTwitterDr. Sebastian Lüning and Prof. Fritz Vahrenholt show that sea level rise at the Fiji Islands is being hyped up in order to generate money.
====================================================
Fijigate
Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(Translated/edited by P. Gosselin)
The COP23 climate conference in Bonn had originally been planned to take place in the Fiji Islands. But in order to comfortably accommodate the approximately 25,000 representatives(!) from every country in the world, it was decided to hold it in Bonn.
It was reported in Spiegel about how the islands are becoming victims. At the start of the article author Axel Bojanowski referred to the rise in sea level and linked to an NOAA-website. But later throughout the rest of the article there was no mention of climate change submerging the islands.
Bojanowski was completely correct to emphasize that the most important reasons for the erosion of the islands is solely the fault of the island inhabitants. The uncontrolled deforestation reduces stability and resistance to the sea. Even persons who sail in the area report that there are 3-meter waves even in the absence storms in the region.
But let’s get back to sea level rise in the area. At the NOAA website there is also the possibility to download the data. And that is what we did.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Figure 1: Sea level rise at the Fiji Islands, 1990-2011. Data: NOAA. The linear trend is 6 mm per year.
The available NOAA data go back (with some gaps) to 1972, but the station was moved in March 1989 and this led to an upwards jump of about 10 cm. Thus we look only at the period from 1990 to the end of 2011, where unfortunately the data series ends. However we supplement the data for the area from satellites (see here:

Figure 2: Sea Surface Height (SSH at the Fiji Islands from satellite measurement. Source here.
After a peak in 2012 the level went down by about 10 cm by mid 2017. It is very much related to natural variations, in sync with the El Ninos (low levels) and La Ninas (high levels).
So what remains of the climate change horror stories in connection to the Fiji Islands? In the article, a 40-year old woman tells about her youth (i.e. around 1990), when she viewed the water as her friend and how today (2017) she regards it as an enemy. But just what should an approximately 8 cm rise (and not the often cited 17 cm that was generated by the powerful 2011/12 La Nina) lead to in 27 years with waves of 3 meters?
The contribution to erosion coming from climate change is certainly hardly noticeable by the residents. However for PR work, it works great for shaking down money.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter 

A remarkably unsophisticated paper was published a few months ago curiously entitled Internet Blogs, Polar Bears, and Climate-Change Denial by Proxy.  Among the list of co-authors of Harvey et al. (2017) are two rather familiar names in climate science circles: Michael E. Mann and Stephan Lewandowsky.
The 14 authors liberally utilize name-calling and broad-brushed accusation (i.e., “unsubstantiated opinions of climate-change deniers”) to make the claim that “climate-change deniers” have no scientific backing for their “opinions”, and so they consequently use the same scare-mongering rhetorical devices and tactics as creationism apologists to advance their cause.
“Proponents of creationism and intelligent design use the same strategy [as climate-change deniers]: Instead of providing scientific evidence in favor of their opinions, they instead focus selectively on certain lines of evidence for evolution and attempt to cast doubt on them (Nisbet 2009).”
“Rhetorical devices to evoke fear and other emotions, such as implying that the public is under threat from deceitful scientists, are common tactics employed by science-denier groups (Barry et al. 2008).”
The purpose of their paper is to make the case that widely-read “denier blogs” like Watts Up With That and Climate Depot have cherry-picked an anthropogenic global warming (AGW) icon, the polar bear, and then proceeded to hand-wave by denying the “well established” science that says these bears’ survival and ability to obtain food (i.e., hunt seal) is threatened by reductions in sea ice.   In denying that these animals are endangered by sea ice losses, the polar bear has become a “proxy” or “keystone domino” for denying all the other dire consequences associated with AGW.
“Here, focusing on Arctic sea ice and polar bears, we show that blogs that deny or downplay AGW disregard the overwhelming scientific evidence of Arctic sea-ice loss and polar bear vulnerability. By denying the impacts of AGW on polar bears, bloggers aim to cast doubt on other established ecological consequences of AGW.”  
Indeed, Harvey et al. (2017) authors claim that the evidence is both “overwhelming” and “well established” that polar bears can only hunt and catch their main prey, seals, “from the surface of the sea ice”.   They can not catch seals in the open water.  Consequently, as long as there is less sea ice available, “AGW assures that all polar bears ultimately will be negatively affected.”
Inuit Observations, And Scientists Who Record Them, Are Now ‘Climate-Change Denial’
The native Inuit peoples who have lived in the Arctic and observed polar bear hunting practices for generations are apparently deserving of the “climate-change denier” moniker.
For that matter, the audacious scientists who risk the ire of the AGW gatekeepers to interview these community leaders and then publish their results in scientific journals apparently must be classified as “climate-change deniers” too.
Why?  Because there appears to be widespread agreement among Inuit observers that polar bears are skilled swimmers who can catch seals in open water (and not just from sea ice surfaces).   This observation wholly contradicts the “well established” and “overwhelming” scientific evidence identified in Harvey et al. (2017) that says polar bears can only catch seals from a sea ice platform.
“The [native populations’] view of polar bears as effective open-water hunters is not consistent with the Western scientific understanding that bears rely on the sea ice platform for catching prey (Stirling and McEwan, 1975; Smith, 1980). The implications of this disagreement are paramount, given that scientists suggest that the greatest threat to polar bears associated with a decrease in sea ice is a significant decrease in access to marine mammal prey (Stirling and Derocher, 1993; Derocher et al., 2004).” — Laforest et al., 2018
‘There’s Too Many Polar Bears Now’
Not only do the generational observations indicate that polar bears’ hunting practices are not duly harmed by sea ice reduction, but community participants consistently report thriving and growing polar bear populations — especially in recent years.
An extensive analysis by York et al. (2016), relying heavily on native reports, concluded that 12 of 13 Canadian Arctic sub-populations have been stable or growing in recent decades.   Wong et al. (2017) recorded Inuit community members reporting “there’s too many polar bears now.”
Even aerial analysis has revealed stable to growing polar bear populations across wide swaths of the Arctic.  Aars et al. (2017), for example, report that there is “no evidence” that reduced sea ice has led to a reduction in polar bear population size.  To the contrary, these scientists found that polar bears living near the Barents Sea increased in number by 42% — from 685 to 973 — between 2004 and 2015.
Unconvincing Claims Of ‘Overwhelming Scientific Evidence’ 
The fact that the real-world observations of seal-hunting in open water can be collaborated by stable to growing population sizes would appear to support the Inuit version of polar bear science and to simultaneously undermine the Harvey et al. (2017) version of polar bear science.
If Michael E. Mann, Stephan Lewandowsky, and the other authors of the Harvey et al. (2017) polemic wish to characterize those who reject observational evidence (i.e., science) as “deniers”, perhaps they should first get their own “facts” straight.
As a requisite, Mann and his colleagues should seek to persuade Inuit community members that they have not actually witnessed polar bears hunt seals in open waters, or that they have not actually observed an increase in polar bear population size in recent decades.
After this insidious “denialism” permeating Inuit communities has been eradicated, the 14 authors of Harvey et al. (2017) might then have a leg to stand on in going after the “denier blogs” and their creationist-style tactics and scare-mongering rhetorical devices.

Aars et al., 2017
The number and distribution of polar 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




bears in the western Barents Sea
“In August 2015, we conducted a survey in the Norwegian Arctic to estimate polar bear numbers and reveal population substructure. … Mainly by aerial survey line transect distance sampling methods, we estimated that 264 (95% CI = 199 – 363) bears were in Svalbard, close to 241 bears estimated for August 2004. The pack ice area had an estimated 709 bears (95% CI = 334 – 1026). The pack ice and the total (Svalbard + pack ice, 973 bears [in 2015], 95% CI = 334 – 1026) both had higher estimates compared to August 2004 (444 and 685 bears [in 2004], respectively), but the increase was not significant.”
“There is no evidence that the fast reduction of sea-ice habitat in the area has yet led to a reduction in population size.”


Laforest et al., 2018
Traditional Ecological Knowledge 
of Polar Bears […] Québec, Canada
“Communities also differed in their perception of the prevalence of problem polar bears and the conservation status of the species, with one-third of participants reporting that polar bears will be unaffected by, or even benefit from, longer ice-free periods. A majority of participants indicated that the local polar bear population was stable or increasing.”
“[Participants] indicated that polar bear body condition is stable; they cited the fact that polar bears are capable of hunting seals in open water as a factor contributing to the stable body condition of the bears. … None of the participants explicitly linked the effects of a warming climate to specific impacts on polar bears.”
“Five participants indicated that polar bears are adept swimmers capable of hunting seals in open water. Residents of communities along Baffin Bay have also expressed this viewpoint (Dowsley and Wenzel, 2008), whereas Inuvialuit of the Western Arctic had variable perceptions of the ability of bears to catch seals in open water (Joint Secretariat, 2015). The [native populations’] view of polar bears as effective open-water hunters is not consistent with the Western scientific understanding that bears rely on the sea ice platform for catching prey (Stirling and McEwan, 1975; Smith, 1980). The implications of this disagreement are paramount, given that scientists suggest that the greatest threat to polar bears associated with a decrease in sea ice is a significant decrease in access to marine mammal prey (Stirling and Derocher, 1993; Derocher et al., 2004).”
“A recent aerial survey of the Southern Hudson Bay subpopulation concluded that the abundance of polar bears has remained steady since 1986 (943 bears; SE: 174) (Obbard et al., 2015). The survey included the entire coastal range and offshore island habitat of the Southern Hudson Bay subpopulation, except for the eastern James Bay coast. Taken together, the results of the aerial survey and the participant responses from Wemindji and Chisasibi indicate that the local population has remained stable. However, the unanimous responses from participants in Whapmagoostui/Kuujjuarapik suggest that there has been a localized increase in the number of bears near Whapmagoostui/Kuujjuarapik.”

Wong et al., 2017
Inuit perspectives of polar bear research:
Lessons for community-based collaborations
“All [Inuit] participants reported having more bear encounters in recent years than in the past. Some participants indicated that the bears they have encountered are healthy.”
Inuit observations: “Last year he said that there’s more bears that are more fat … they rarely see unhealthy bears … the only time they would see one is when it’s pretty old … it won’t hunt—hunt as much … and it’s skinny. (AB9)  … Our elders, they say, they migrate, into other area… for years, and then they come back … that’s what we’re experiencing now … back in early 80s, and mid 90s, there were hardly any bears … there’s too many polar bears now.  Bears can catch seals even—even if the—if the ice is really thin … they’re great hunters those bears … they’re really smart … they know how to survive”

York et al., 2016
Demographic and traditional knowledge perspectives on 
the current status of Canadian polar bear subpopulations
“Considering both [observations from native populations] and scientific information, we suggest that the current status of Canadian polar bear subpopulations in 2013 was 12 stable/increasing and one declining (Kane Basin).”
“We do not find support for the perspective that polar bears within or shared with Canada are currently in any sort of climate crisis.”
“We show that much of the scientific evidence indicating that some polar bear subpopulations are declining due to climate change-mediated sea ice reductions is likely flawed”
“Reduction in the heavy multiyear ice and increased productivity from a longer open water season may even enhance polar bear habitat in some areas.”
“It seems unlikely that polar bears (as a species) are at risk from anthropogenic global warming.”

Dowsley and Wenzel, 2008
“The Time of the Most Polar Bears”
“In the interviews, Inuit reported numerous changes in polar bears over the past 10 – 15 years [1990s to 2000s].”
“During the Inuit knowledge survey in the Baffin Bay area, Inuit knowledge varied significantly between communities on whether there was any change in the population of polar bears (p = 0.010) (Dowsley, 2005, 2007). In the northern community of Pond Inlet, all 14 respondents indicated a population increase. In the central community of Clyde River, 16 of 17 respondents reported an increase. In the most southern community of Qikiqtarjuaq, 9 of 15 reported an increase. The other six respondents in Qikiqtarjuaq reported either that they did not know, or that no change was observed. No respondent in any of the communities reported a decrease in the bear population.”

Dowsley, 2007
Inuit Perspectives on Polar Bears (Ursus maritimus)
and Climate Change in Baffin Bay, Nunavut, Canada
“A significant difference between communities was also observed regarding the number of polar bears. Only 60% of respondents in Qikiqtarjuaq felt the bear population had increased over the past 10-15 years, compared to over 90% of respondents in the other two communities.”
“No, because polar bears can go and follow the seals further [if sea ice retreats], so they won’t have trouble hunting. Also the snow covers the [seals’] breathing holes but polar bears can still hunt, it’s just for people.”
“There is more rough ice, more thin ice. But it won’t affect polar bears’ hunting.”
Share this...FacebookTwitter "
"For nearly 100 years, Mount Everest has been a source of fascination for explorers and researchers alike. While the former have been determined to conquer “goddess mother of the world” – as it is known in Tibet – the latter have worked to uncover the secrets that lie beneath its surface. Our research team is no different. We are the first group trying to develop understanding of the glaciers on the flanks of Everest by drilling deep into their interior. We are particularly interested in Khumbu Glacier, the highest glacier in the world and one of the largest in the region. Its source is the Western Cwm of Mount Everest, and the glacier flows down the mountain’s southern flanks, from an elevation of around 7,000 metres down to 4,900 metres above sea level at its terminus (the “end”). Though we know a lot about its surface, at present we know just about nothing about the inside of Khumbu. Nothing is known about the temperature of the ice deeper than around 20 metres beneath the surface, for example, nor about how the ice moves (“deforms”) at depth. Khumbu is covered with a debris layer (which varies in thickness by up to four metres) that affects how the surface melts, and produces a complex topography hosting large ponds and steep ice cliffs. Satellite observations have helped us to understand the surface of high-elevation debris-covered glaciers like Khumbu, but the difficult terrain makes it very hard to investigate anything below that surface. Yet this is where the processes of glacier movement originate. Scientists have done plenty of ice drilling in the past, notably into the Antarctic and Greenland ice sheets. However this is a very different kind of investigation. The glaciers of the Himalayas and Andes are physically distinctive, and supply water to millions of people. It is important to learn from Greenland and Antarctica, – where we are finding out how melting ice sheets will contribute to rising sea levels, for example – but there we are answering different questions that relate to things such as rapid ice motion and the disintegration of floating ice shelves. With the glaciers we are still working on obtaining fairly basic information which has the capacity to make substantial improvements to model accuracy, and our understanding of how these glaciers are being, and will be, affected by climate change. So how does one break into a glacier? To drill a hole into rock you break it up mechanically. But because ice has a far lower melting point, it is possible to melt boreholes through it. To do this, we use hot, pressurised water.  Conveniently, there is a pre-existing assembly to supply hot water under pressure – in car washes. We’ve been using these for over two decades now to drill into ice, but our latest collaboration with manufacturer Kärcher – which we are now testing at Khumbu – involves a few minor alterations to enable sufficient hot water to be pressurised for drilling higher (up to 6,000 metres above sea level is envisioned) and possibly deeper than before. Indeed, we are very pleased to reveal that our recent fieldwork at Khumbu has resulted in a borehole being drilled to a depth of about 190 metres below the surface. Even without installing experiments, just drilling the borehole tells us something about the glacier. For example, if the water jet progresses smoothly to its base then we know the ice is uniform and largely debris-free. If drilling is interrupted, then we have hit an obstacle – likely rocks being transported within the ice. In 2017, we hit a layer like this some 12 times at one particular location and eventually had to give up drilling at that site. Yet this spatially-extensive blockage usefully revealed that the site was carrying a thick layer of debris deep within the ice. Once the hole has been opened up, we take a video image – using an optical televiewer adapted from oil industry use by Robertson Geologging – of its interior to investigate the glacier’s internal structure. We then install various probes that provide data for several months to years. These include ice temperature, internal deformation, water presence measurements, and ice-bed contact pressure. All of this information is crucial to determine and model how these kinds of glaciers move and melt. Recent studies have found that the melt rate and water contribution of high-elevation glaciers are currently increasing, because atmospheric warming is even stronger in mountain regions. However, a threshold will be reached where there is too little glacial mass remaining, and the glacial contribution to rivers will decrease rapidly – possibly within the next few decades for a large number of glaciers. This is particularly significant in the Himalayas because meltwater from glaciers such as Khumbu contributes to rivers such as the Brahmaputra and the Ganges, which provide water to billions of people in the foothills of the Himalaya. Once we have all the temperature and tilt data, we will be able to tell how fast, and the processes by which, the glacier is moving. Then we can feed this information into state-of-the-art computer models of glacier behaviour to predict more accurately how these societally critical glaciers will respond as air temperatures continue to rise.  This is a big and difficult issue to address and it will take time. Even once drilled and imaged, our borehole experiments take several months to settle and run. However, we are confident that these data, when available, will change how the world sees its highest glacier."
"Residents of London, Los Angeles and Beijing often complain about air pollution. And they’re right to – their concerns are backed by lots of data. However, not all cities are measured as rigorously. Notably, the air quality in many African cities is almost completely unmonitored. By 2050, both Lagos and Kinshasa will exceed 30m people – shouldn’t we know more about pollution in this fast-growing part of the world? The World Health Organisation calculates air quality is responsible for more than 500,000 deaths a year in Africa from both indoor and outdoor air pollution. To put this into perspective, around 11,000 people died in the recent Ebola epidemic. Yet the WHO’s ability to make these estimates is limited both by the lack of air measurements and the lack of medical studies linking pollution to deaths in Africa. It seems unlikely that the current air quality impact studies based on the populations of Los Angeles or London can be directly transferred to  Lagos or Kinshasa. London and Lagos have entirely different air quality problems. In cities such as London, it’s mainly due to the burning of hydrocarbons for transport. A complicated problem for sure, but one that can be addressed by tackling petrol usage through electric vehicles, car free zones, and so on. African pollution isn’t like that. There is the burning of rubbish, cooking indoors with inefficient solid fuel stoves, millions of small diesel electricity generators, cars which have had the catalytic converters removed and petrochemical plants, all pushing pollutants into the air over the cities.  It’s not even obvious what source to tackle first. Compounds such as sulfur dioxide, benzene and carbon monoxide that haven’t been issues in Western cities for decades may be a significant problem in African cities. We simply don’t know. Nature isn’t helping here either. Compounds such as hydrocarbons which may be inoffensive in themselves are emitted into the atmosphere and a complex web of chemical reactions process them into harmful products such as ozone and aerosols. These reactions are driven by the sun, and Africa has that in spades.  Natural sources of harmful compounds also abound. Sahara sand storms can cloak cities with choking dust. Chemicals emitted by trees in Africa’s vast forested areas may magnify the impact of human emissions in the same way as they do in the southern US, and smoke from seasonal forest fires can drift over population centres.  The relative importance of these natural sources compared to the human sources, and, even how we separate out the natural versus human are hotly debated by scientists. How much of the forest burning we see is due to natural courses such as lightning strikes and how much is linked to agricultural practices? Again without improved observations it is hard to tell. These air pollutants also harm vegetation and crops. In Asia it is estimated that around 10% of the food crop is destroyed by pollutants. For Africa we’ve got no good idea. First because we don’t have the same controlled field experiments on Africa’s staple crops such as cassava or millet as we do for Europe and North America’s crops. Second, because if we know little about air quality in cities, we know even less about what is happening in agricultural areas. The need to focus on air quality in Africa’s new megacities is the topic of a new paper, which I co-authored, in the journal Nature Climate Change.  Not only is pollution in these cities killing local residents, we found these emissions may even be altering the climate along the coast of West Africa, leading to changes in the clouds and so potentially to rainfall with devastating effects. Things aren’t going to get better any time soon. Half of the global population growth between 2015 and 2050 will occur in Africa, and the continent is becoming increasingly urbanised. Economic development will put increased strain on resources. A 2012 OECD report suggests successes in dealing with other problems such as access to drinking water and malaria is likely to make air quality the dominant environmental risk for premature deaths globally by 2030, if it isn’t already. Africa will not be far behind. Scientists can help. The latest generation of satellites is providing high-resolution information about these pollutants on an unprecedented scale, and cheap new sensors can monitor the composition of the air over cities.  Couple this with the revolution in big data and the decades of research that has been undertaken in North American, European and now Asian cities and we should soon be able to understand the air quality problems of African cities. And once we’ve understood the problem, science will be able to suggest solutions. Then it will be up to African cities to implement changes needed to prevent the deaths of thousands of their citizens."
"In the dying minutes of winter’s shortest days, something magical happens to the trees that cling to the steep slope on the eastern edge of Tunstall reservoir. Just before the setting sun dips behind Wolsingham Park Moor, the water surface becomes a mirror that bounces glancing sunbeams into the tree canopy. Seen from across the reservoir, trunks and branches are bathed in a golden glow, every twig etched with startling clarity against the gathering dusk within the wood. This afternoon, as I walked among those trees, the lighting was pure theatre. My vision struggled to accommodate its extremes, of dazzling white trunks of silver birch, rough-textured grey bark of ancient oaks, vivid green mosses and deep black shadows.  A nuthatch appeared as a fleeting monochrome silhouette, hanging on the shadowy underside of a branch, dagger beak wheedling out something from a fissure. It vanished behind the trunk, then reappeared in bright sunlight, in full colour, unmistakable in its bandit-mask black eye-stripe, slate-blue back, and apricot chest feathers. Just another nuthatch. But when I first came to live hereabouts, more than 40 years ago, these were rare birds in County Durham. Reading through my natural history notebooks from the 1970s I see that they merited a specific mention on the few occasions when I saw one. Now they are common. Their relentless northerly advance is often attributed to milder winters, brought about by climate change, though it’s far from clear exactly what factors have changed in their favour. It would now be noteworthy to walk in deciduous woodlands in Weardale and not see – or, more often, hear – these birds; they have a piercing call, especially in late winter and early spring when courtship begins. Personal nature notebooks, often ledgers of profit and loss, can be emotive documents, with the growing awareness of shifting baseline syndrome, where succeeding generations are denied the pleasure of seeing once-commonplace species that disappear during their parents’ lifetime. In the 1970s I recorded red squirrels here; I would now need to travel much further afield to see one. But in the case of the nuthatch, in this precious fragment of ancient woodland, the syndrome seems to have worked in reverse."
nan
"We are not far from the ocean here. The air smells of salt and sulphur, of marine life. But the square of black, cracked mud in front of us, bounded by its four crumbling walls of sand, is no place for living things. It was previously a pond for cultivating tiger prawns, the lucrative species that was the reason for cutting the lush mangrove forest that once covered this area. The recent history of this abandoned place is sadly representative of the story of thousands of hectares in this region in the west of Sri Lanka.  A swelling appetite for shrimps and prawns in America, Europe and Japan has fuelled industrial farming of shellfish in the past few decades. The industry now has a farm-gate value of $10bn (£6.4bn) per year globally and the prawn in your sandwich is much more likely to have come from a pond than from the sea. While the industry is dominated by the likes of China, Vietnam and Thailand, a large number of other countries have invested heavily in cultivation too.  One is Sri Lanka, which saw the industry as a passport to strong economic growth and widespread employment. Just outside the world’s top ten producers, it accounts for approximately 50% of the total export earnings from Sri Lankan fisheries. More than 90% of the harvested cultured prawns are exported, going mostly to Japan.  Yet the picture is decidedly mixed on a closer inspection. The country saw an explosion of unregulated aquaculture on the island in the 1980s and 1990s, bringing riches to a few and the hope of riches or at least an income to many more. But poor coastal management also brought white spot syndrome virus, a virulent disease that spreads in water and on the feet of birds, and can kill all the prawns in a pond in under a week.  Crowding shrimp together in warm little pools full of nutrients creates the perfect conditions for an outbreak. It contributes to the fact that here and elsewhere in the tropics, most intensively farmed ponds remain productive for only five to ten years (the other main reason is the build-up of an organic ooze, rich in uneaten food and prawn faeces). Such ponds are then abandoned in favour of new areas of wetland to convert for another brief harvest. The disease kills off prawns in the wild in large numbers too.   To get a sense of how bad the problem has been in Sri Lanka, I was one of a group of researchers who studied the Puttalam area on the west coast, one of the first in the country where large-scale aquaculture was introduced.  We looked at satellite imagery from 1992 to 2012, which showed an explosion in prawn farms from less than 40ha in our study area to over 1,100ha (a rise of over 2,700%). This combined with a decline in natural habitats – mangroves lost some 36% of their area over the period. Yet most of these historic ponds are now unproductive or abandoned.  The evidence from the satellite images combined with interviews with local people suggest that a staggering 90% of ponds are lying idle. The story is unlikely to be quite as bad across the country as a whole, since Puttalam was one of the early areas to be cultivated. Detailed figures are thin on the ground, but certainly overall shrimp exports in 2012 were 65% below their 1999 peak.  Prawn aquaculture has been likened to slash-and-burn cultivation – find a pristine spot, remove the vegetation and farm it for a few years before moving on. But the analogy is misleadingly benign. Slash-and-burn systems on a small scale can be sustainable, since the cut plots can recover afterwards.  In the case of prawn farming, a better phrase would be “slash and sink”. Mangroves are among the most carbon-dense of all ecosystems, often storing more than 2,000 tonnes of carbon per hectare in sediments beneath the forest floor, according to research that our group has yet to publish. Cut them down and this carbon is oxidised and emitted into the atmosphere as CO2.  We estimate that nearly 192,000 additional tonnes of carbon have been added to climate change as a result of these land-use changes in Puttalam, Sri Lanka alone. And that of course excludes any emissions during farm operations and the potential for the lost mangroves to capture carbon in future.  An additional issue is the sinking shoreline. In the face of global rising sea levels of more than 3mm a year, healthy mangrove forests are among the best protection since they bind together sediments and even elevate their soils to match the rising tide. Lose them and the chances of coastal subsidence, erosion and storm damage goes up.  In fact, mangroves are such useful ecosystems that destroying them almost never makes sense, even from a narrow economic perspective. A recent analysis in southern Kenya showed that conserving and restoring the forests was worth at least $20m more in present value than allowing current cutting to continue. So what about Sri Lanka? A positive recent development was that the government announced that it would protect all of its remaining mangroves, totalling some 8,800 hecatares. It also promised to replace a further 3,900 – a task that will require careful restoration of the right tidal conditions and planting trees where necessary. Another positive sign is that there are now local movements that are coordinating production among zones and farms to avoid disease and achieve better sustainability. This is on the back of a commitment by the government in 2010 to expand the industry.  The country should also look to return some of its abandoned ponds to production, provided producers are supported to adopt best practice and work together to avoid disease outbreaks and pollution in future. As for us in the West who import these shellfish in vast quantities each year, we need to think harder about the real costs of that cheap prawn sandwich. Without knowing where it has come from and what farming practices have been used, we would do well to steer clear."
"
Share this...FacebookTwitterIn the wake of a fall storm ‘Xavier’ that struck Germany and claimed 7 lives, one of Germany’s most popular TV Talkshows, Maischberger 1 on ARD German public television, recently featured climate change in discussion round bearing the title: “Xavier and the weather extremes: has our climate reached the tipping point?”

Cologne and Berlin under water! Backdrop on set for ARD German television discussion round on climate change. Image cropped from Maischberger 1 here.
The discussion round included, among others, Prof Hans-Joachim Schellnhuber, high profile Swiss meteorologist Jörg Kachelmann, who by the way is a warmist, and Swiss science journalist Alex Reichmuth.
Not surprisingly, talk show moderator Sandra Maischberger introduced the show with dramatic scenes of a climate in collapse, and then asked the round if this if the recent storms Xavier and now Orphelia are unprecedented. So dramatic in fact were the images of Maischberger’s intro that even German daily Die Welt here commented that “ARD had allowed itself to be inspired a bit by Hollywood“.
When asked by Maischberger about the recent storms, Kachelmann immediately dumped cold water on the notion that they were unusual, noting that storm Xavier seemed worse because it hit in October when trees are still fraught with foliage and thus cause far more wind resistance and cause tress to fall more often. Overall, Xavier was just a normal fall storm, Kachelmann told the audience.
“Storms don’t come with a label”
When asked if this year’s heavy rains were due to global warming, Kachelmann responded:
“We don’t know case by case because all the thunderstorms and storms don’t come with a label stating: ‘I’m here only because of you, or your actions, to say it correctly’. The problem is that we don’t know.”
No detectable increase in storm frequency
Kachelmann went on to explain that experts evaluated the data from the German Weather Service and concluded there has been “no increase in frequency in these events“. He added:
Also with tropical storms, looking at it globally, from the data of the American weather agencies, up to now we see no increase in frequency.”
A few seconds later he responded to Maischberger’s inquiry about the “monster hurricanes” hitting the Caribbean and USA:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Yes, this is an active season. But it is not a record season. It is not anything that has not happened before. When we look back at the last 50 or 60 years, we see no trend.”
During the course of the Talkshow, moderator Maischberger globetrotted across the entire planet, it seemed, going from one weather disaster to the next. She pressed Kachelmann about the fires in northern California. Here too the Swiss veteran meteorologist dismissed the notion that it’s unusual, reminding the audience that California is a dry state and these things have always happened before:
When you look at the archives, we see no increase in frequency.”
Kachelmann also reminded that anyone can make a list of 100 disasters in any year.
Like Jehovah Witness
At about the 16-minute mark, Maischberger turned to Swiss journalist Alex Reichmuth, who like Schellnhuber studied physics and mathematics. Journalist Reichmuth, however, is far more critical of climate change,and told the audience that climate science is more a religion than science and that it all reminded him of the Jehovah Witness sect.
This is about a religious conversion – ride your bicycle more and we’ll be redeemed.”
Reichmuth reminded the audience that even the IPCC stated that there is no clear trend regarding weather extremes.
From 97% to 99%?
Reichmuth then slammed Schellnhuber for his outlandish predictions of the future, telling the “renowned” Potsdam professor that he “has clearly deviated from the scientific approach“. Just a minute earlier Schellnhuber had seemed to claim that 99% of the scientists agree with him.
Surprisingly guest Dorothee Bär of the conservative CSU party said she doubted that man was all responsible for the 1°C warming of the past century and that economy and well-being of the citizen had to be placed at the forefront of any energy policy. But later in the show the CSU politician hopped on the politically correct “we have to do something” facade – as did Kachelmann.
At the 34-minute mark, the talk switched to Trump’s backing out of the Paris Accord and whether the fight against the climate would hurt the economy. Most of the discussion was filled Marxist-brand utopian platitudes with few in the round grasping the technical implications of green energies. For example, suddenly Schellnhuber poased as an expert and leading authority on transportation technology, agriculture and economics, and gave the impression storage systems are all ready to go!
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGlobal Warming Theory ‘Completely 
Disconnected From the Observations’


Extensive analysis of temperature trends in the Arctic reveals that there has been no detectable long-term change since the beginning of the 20th century, and thus predictions of a sea ice-free Arctic in the coming decades due to dramatically rising temperatures are not rooted in observation.


Butina, 2015  
IS THE ARCTIC MELTING?  
THEORY VS. OBSERVATIONS
Abstract
[T]he Arctic Circle is the most extreme place on our planet where seasonal changes can range from +35.0°C in July and -65.0°C in February; […] on average 75% of the year is spent below the melting point of water [and] on average the Arctic will be covered by ice/snow for the same proportion of time, i.e., 75% or 9 months of the year.
The same seasonal extreme variations in air temperatures are also observed in ice cover variations observed in the Arctic where the winter‘s ice cover can be between 14- 16 million km2, while during summer the area covered can vary between 4 and 8 million km2.  Based on observations, dating back to 1900, it can be concluded that it is physically impossible for the Arctic to be ice/snow free in the foreseeable future since the air temperatures were as cold in 2013 as they were in 1900.
Since ice cannot melt below 0.0°C, all these observations point towards the Arctic remaining ice-covered for the next 100 years. It must also follow that any theory predicting imminent melting of the Arctic ice cap cannot be based on thermometer-recorded data and, therefore, must be wrong and will merely be an artefact of using the term temperature where there is no true association with the calibrated thermometer, the instrument used to measure temperature in all physical, medical and engineering sciences.
Conclusion
So, what are the hard facts about Arctic that are based on the observations made by calibrated thermometers at 20 stations across the Arctic Circle and which conclusions can be made based on those observations?
1. Temperatures in the Arctic between 1900 and the present day are a long distance below 0.0°C for at least 9 months per year and can be as low as -64.0°C
2. It is impossible to separate the youngest from the oldest years using thermometerbased daily or monthly Tmax/Tmin data
3. The total ranges observed in daily Tmax/Tmin data can be as high as 100.0°C and as low as 75.0°C making the Arctic Circle the most variable and extreme area on our planet therefore making any accurate forecasting of future temperature patterns and trends impossible
4. The switches between the extreme hot to extreme cold temperatures are very frequent and very unpredictable and can occur within the same month, same year or between two consecutive years
5. The large observed ice gain/loss variations are pre-determined by the large observed variations in air temperatures
6. Since the air temperatures are chaotic in nature it must follow that the extent of the ice cover has to be chaotic as well and, since we cannot predict future events of a chaotic system, we cannot predict future trends of either air temperatures or ice cover patterns
Based on the facts above only one conclusion can be made in reference to the putative melting of the Arctic: historical thermometer-based data tells us that between 1900 and 2014 arctic temperatures were for 75% of the time consistently long distance below 0.0°C; the ice cover in the winter months is still consistently more than 14,000,000km2 and, therefore, it is physically impossible for the Arctic to be already melting since nothing has changed since 1900 till present day. The only sensible forecast for the future would be to expect the same extreme events to continue until thermometer-based evidence tell us otherwise.
Let me conclude this paper by answering the question asked in the first part of the title by a categorical No, the Arctic is not melting. As long as temperatures remain the same as they have been for the last 100 years the Arctic will remain frozen in the long winter months and partly melt during very short summer months.
The answer to the second question is that the theory of global warming is completely disconnected from the observations since their definition of temperature is based on some theoretical number that has nothing to do with the temperature that is measured by calibrated thermometer and, most importantly, used as an international standard by the scientific community. Since the theory is clearly wrong about forecasting the temperature patterns in the Arctic, all other predictions made by the theory must be wrong too.


New Paper: No  Greenland Temperature Or Sea Ice Changes Since 1600 Either

Kryk et al., 2017     
“Our study aims to investigate the oceanographic changes in SW Greenland over the past four centuries (1600-2010) based on high-resolution diatom record using both, qualitative and quantitative methods.  July SST during last 400 years varied only slightly from a minimum of 2.9 to a maximum of 4.7 °C and total average of 4°C. 4°C is a typical surface water temperature in SW Greenland during summer. … The average April SIC was low (c. 13%) [during the 20th century], however a strong peak of 56.5% was recorded at 1965. This peak was accompanied by a clear drop in salinity (33.2 PSU).”


Greenland Ice Sheet In Balance…Melt Added  Just 1.5 cm (0.6  Inch) To Sea Levels Since 1900

Fettweis et al ., 2017
“[T]he integrated contribution of the GrIS [Greenland Ice Sheet] SMB [surface mass balance] anomalies over 1900–2010 is a sea level rise of about 15 ± 5 mm [1.5 cm], with a null contribution from the 1940s to the 2000s“

Like The Arctic, Antarctica Has Not Warmed In The Last Century Either

Stenni et al., 2017
“[N]o continent-scale warming of Antarctic temperature is evident in the last century.”

Antarctica’s Ice Sheet Has Been Gaining Mass Since 1800

Thomas et al., 2017
“Our results show that SMB [surface mass balance] for the total Antarctic Ice Sheet (including ice shelves) has increased at a rate of 7 ± 0.13 Gt decade−1 since 1800 AD…”

Antarctica’s Mass Gains Have Reduced Sea Levels By -0.04 mm-¹ Per Decade Since 1900

Thomas et al., 2017
“…representing a net reduction in sea level of ∼ 0.02 mm decade−1 since 1800 and ∼ 0.04 mm decade−1 since 1900 AD.  The largest contribution is from the Antarctic Peninsula (∼ 75 %) where the annual average SMB during the most recent decade (2001–2010) is 123 ± 44 Gt yr−1 higher than the annual average during the first decade of the 19th century.”

In sum, there is nothing thermally unusual occurring today in either the Arctic or Antarctic, precluding the clear detection of an anthropogenic temperature or ice-melt signal in the polar regions.
Share this...FacebookTwitter "
"Britain’s countryside is becoming ever more socially exclusive as spiralling house prices turn once-normal villages into rich ghettos. Contrary to the popular imagination, few of the cast of The Archers could now afford to live in Ambridge, or anywhere else in rural Britain for that matter. It’s getting ever harder for people on medium and lower incomes to buy a first home, and there is very little so-called “affordable housing”. The proposed extension of the Right to Buy to tenants of housing associations will only make matters much worse. As the internet explodes with articles focused on London, it’s is an ideal moment to take stock of the situation and ensure people in the countryside don’t get drowned out. The UK is unique in having higher house prices in its rural areas than in its towns and cities. Rural homes now cost 26% more on average than those in urban England (London aside), and work out to around 11 times the average local salary. These prices are well beyond the means of most families living and working in the countryside, who are outbid by wealthy commuters, retirees and second home owners who earn their living elsewhere. For people unable to afford to buy a home, renting should be viable alternative, but this is also a problem. Private rents are just as high as in urban areas, despite lower earnings in rural communities (local earnings in rural England average £19,700 compared to £26,900 in major urban areas). Rented social housing provided by housing associations and councils could help out, but again rural areas are lacking: 12% of rural housing is social compared to 19% in urban areas. Even this small stock of social housing has been further depleted under the Right to Buy scheme, and in some areas affordable housing has all but disappeared. The few opportunities which do arise for people with local connections and ordinary incomes to share in rural life, helping keep schools and services going, derive primarily from the efforts of housing associations. Where these associations have built small-scale developments, shops remain open, buses keep running, and local communities thrive. The government is now proposing to force housing associations against their will to sell their stock to tenants at heavily discounted prices. The bill, estimated at £5.8 billion or more, will partly be met by forcing local authorities to sell their most valuable council houses. The government argues that this will allow housing associations to build new affordable housing, even though since 2012 just 46% of houses sold under the Right to Buy scheme have been replaced. Unless the government ensures replacements are built in the same settlement, affordable homes will inevitably be lost in larger numbers from attractive villages and hamlets (as happened under the original Right to Buy of council houses) and that they will be replaced, if at all, elsewhere where sites are cheaper and planning regulations more sympathetic. Lord Kerslake, until recently head of the civil service, described this proposal as “wrong in principle and wrong in practice”. We are already seeing those on low and medium incomes, and especially young people, priced out of small towns and villages across the country. With housing association properties sold off at great cost to the taxpayer, and unlikely to be replaced in any substantial quantities, the wealth divide in rural communities will deepen even further. Not only will the forced sale of housing association properties affect the social and demographic make-up of rural communities; it will also have an effect on local employers. With rural areas becoming increasingly socially exclusive, local businesses – from farms and shops to accountants and software developers – will find it even harder to attract the young, skilled, ambitious people they need. We urgently need more affordable homes to be built, not the disposal of the few that remain in rural areas. The government must reconsider the proposed Right to Buy extension, at least exempting rural areas. Instead they should implement the recommendations made by the Rural Housing Policy Review, to provide more affordable rural housing. This would not only provide much-needed housing supply, but would help rural economies contribute more fully to the government’s growth agenda."
"
Share this...FacebookTwitterIn an opinion piece at the Mittel Bayerische daily, Harry Neumann, National Chairman of the environmental group Naturschutzinitiative e.V. declares Germany’s Energiewende (transition to renewable energies) a failure and writes: “The wind power industry and nature protection cannot be reconciled.”
Moreover Germany’s EEG green energy feed-in act is doing more harm than good, writes Neumann: “The EEG is impeding the research of environmentally compatible technologies.”
Neumann also notes that despite having installed close to 30,000 wind turbines, Germany’s “CO2 emissions are not dropping, but rather are rising again.” He adds:

During the expansion of renewable energies, they failed from the start to set impact limits too protect nature, species, forests and landscapes.”

He also blasted what he calls the “political-industrial complex“, which he says has nothing to do with nature and climate protection, “but rather with the full exploitation of billions in subsidies“.
In Neumann’s view, the wind industry and nature protection “cannot be reconciled” and thus calls for the immediate repeal of the EEG feed-in act.
Veteran journalist: German energy policy fraught with absurdities


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On another note, veteran German science journalist Michael Miersch explains in an interview conducted by Dr. Benny Peiser of the Global Warming Policy Foundation the sheer absurdity and widespread damage German renewable energies are having on the environment.

When asked about the current status and dialogue surrounding the Energiewende, Miersch tells Peiser:
I would like the debate to be less ideological and to be held with less moral rigor. Nowadays you cannot criticize the Energiewende without being put into a corner and being accused of not caring about global climate change.”
He cites the U.S. as a model of how to go about energy policy:
If you think about the US for example, they have achieved a lot in terms of CO2 reduction with gas power plants. There are very few gas power plants in Germany. They are building hardly any new ones.”
 
Share this...FacebookTwitter "
nan
"The average British person will have emitted more carbon dioxide in the first two weeks of this year than a citizen of any one of seven African nations does in an entire year. This is the key finding of an Oxfam project, published on Sunday, which discovered that someone in the UK will take just five days to emit the same carbon as someone in Rwanda does in a year.  And by next Sunday, 12 January, the average Briton’s emissions will have overtaken the annual per capita emissions of a further six African countries: Madagascar, Malawi, Ethiopia, Uganda, Guinea and Burkina Faso. The study revealed that annual emissions of carbon dioxide, per head of population, is 0.09 tonnes in Rwanda, 0.19 in Malawi and 0.25 in Burkina Faso. Further up the scale, it was found that Nigeria emits 0.49 tonnes of carbon per person every year while in India the figure is 1.68. These figures compare with a global average of 4.7 tonnes per person per year. In Britain the figure is 8.3. Danny Sriskandarajah, the chief executive of Oxfam GB, described the scale of global inequality revealed by the study as staggering. “It’s a shock to realise that in just a few days our high-carbon lifestyles here in the UK produce the same emissions as the annual footprint of people in some poor countries. However, the encouraging thing is the willingness of the British public to take action.” This view is supported by the results of a YouGov poll, carried out for Oxfam and also published on Sunday. It shows that 61% of people in Britain want the government to do more to address the climate emergency. Most respondents also say they would be willing to act to reduce their own carbon footprints. The poll found that 55% of Britons say they worry about the impact of global heating and as many as 79% said they were likely to take one of a number of actions to reduce their carbon footprint. Responses ranged from 79% of people who said they were likely to recycle more, down to 38% who were likely to change their diet, such as by eating less meat or dairy. More than two-thirds (68%) said they were likely to use energy-efficient products or utility providers, and almost half to limit their air travel (49%) or buy ethically made or second-hand products (49%). “Just as large numbers of the public are resolving to reduce their carbon footprint, we need a bold new year’s resolution from the prime minister to get us on track to net-zero emissions much earlier than the current 2050 deadline,” added Sriskandarajah. “As the UK government gets ready to host global climate talks later this year, it needs to show that it is deadly serious about leading the fight against climate change.”"
"
Share this...FacebookTwitterOutgoing director of the Potsdam Institute for Climate Impact Research (PIK) believes mankind, through its activities, is headed for a “mass extinction” event and an anthropogenic calamity comparable to a geological scale asteroid hit.

Outgoing Potsdam Institute director Hans-Joachim “John” Schellnhuber tells German national daily that mankind has run out of time and faces “mass extinction” and calamity of geological proportions. Photo: PNAS. 
In an online article titled Climate Change Like An Asteroid Strike appearing in Germany’s national daily Süddeutsche Zeitung (SZ), journalist Alex Rühle reports on the outgoing director of Germany’s alarmist Potsdam Institute for Climate Impact Research (PIK), Prof. Hans-Joachim “John” Schellnhuber.
Prof. Schellnhuber, widely known in Germany as the Climate Pope, is considered in Europe as the leading climate science authority and the architect of the Master Plan for transforming global society – dubbed The Great Transformation – which aims to make global society climate compatible by applying draconian, surrealistic measures. Moreover the plan calls for all of it to happen in just a matter of a couple of decades!
A number of critics have characterized the whole idea as detached from economic reality and dystopian.
Risks mounting “by the hour”
Schellnhuber is also the father of the “2°C climate target”, a warming he claims that the world must never exceed, lest it’ll tip hopelessly into a state of rapid, irreversible climatic collapse.
The SZ article reports how the outgoing Schellnhuber believes the planet is in fact approaching the climatic catastrophe at “a crazy speed” and that the risks “are mounting quasi by the hour”.
The alarmist Potsdam Institute professor has been known for a number of shrill comments made in the past, but in this most recent SZ article, Schellnhuber’s shrillness arguably reaches a whole new level that shoots beyond the realms of reason.
Rapid, epic disaster of geologic dimensions
Now that Schellnhuber is stepping down as director of the Potsdam Institute after 25 years as its director, he reflects back, telling the SZ how he feels about the overall public “disinterest with regards to the consequences of climate change” that has taken hold globally.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Schellnhuber’s view, world leaders are moving far too sluggishly to avert what he sees is a climatic disaster of epic geological proportions, and compares it to earlier calamities which the earth witnessed tens of millions of years ago.
100 times faster than Permian–Triassic extinction event
The Potsdam director tells the SZ that the globe’s temperature today is rising at a rate of 5°C per century! – or “100 times faster than during the time of the Permian–Triassic extinction event” which occurred some 250 million years ago and is believed to have wiped out 90% of all land life on the planet at the time.
According to Rühle, Schellnhuber compares today’s climate change to the “largest mass extinctions in the history of the earth.”
Comparable to the Cretaceous–Paleogene extinction event!
If one mass extinction event was not enough to illustrate what in Schellnhuber’s mind is climatically in store for humanity, the former PIK director piles on with yet another geological catastrophe to describe the course on which he claims man has put the planet on. He tells the SZ:
What man is doing today is similar to the asteroid strike known as the Cretaceous–Paleogene extinction event.”
According to the SZ, Schellnhuber says, “We’ve shirked our responsibility much too long”.
He then compares the situation to a sinking ship that urgently demands top priority:
If we don’t get climate change under control, and if we can no longer keep the ship afloat, then we won’t need to think about distribution of income, racism and good taste any longer.”
Embarrassing climate science even more
Luckily Schellnhuber’s dire claims are very much in dispute and considered extreme outliers, but do provide much fodder to the media. If anything is certain, it is that Schellnhuber is definitely not leaving his post a minute too soon. His most recent claims are an embarrassment to science.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAustralian Bushfires Have Become Less Frequent Over The Past 15 Years
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P Gosselin)
In the Australian state of Victoria, 50,000 km² of land have burned and 12 people and 1 million sheep and thousands of heads of cattle lost their lives. The regions hit by fire were near Portland, Westernport and in the Plenty Ranges, as well as the Wimmera and Dandenong districts. The burned regions extended over a quarter of the state. Conditions for the fires were made favorable by the long-lasting drought period, which changed the landscape into a tinderbox. Finally the fire was exacerbated by strong winds, which carried off a glowing ember from a campfire and ignited the adjacent grassy region.
What role did climate change play in the fire disaster? The Potsdam Institute for Climate Impact Research (PIK) and other alarmists remained surprisingly moot here. Normally climate alarmists rush to the microphones and claim that although such single events are not easily linked to climate change, the probability is in any case is much higher. Loaded dice.
Proponents of a climate catastrophe kept silent in the case of these Victoria fires because they had not been born yet.
The described above fires occurred in February, 1851 and are known as the ‘Black Thursday Bushfires‘.
There have always been bushfires in Australia. For example at the end of the 19th century and the start of the 20th century in New South Wales. Apparently that fact was not even known by the former General Secretary of the Climate Framework Convention of the United Nations (UNFCCC), Christiana Figueres, who in 2013 described in knee-jerk fashion that the fires in New South Wales (NSW) were a consequence of climate change. A classic gaffe – one that should never happen for someone occupying such a position.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The provincial government defended itself against such misinterpretations. The German business daily Handelsbatt wrote on October 25, 2013:
The new conservative government accuses environmental protection activists are exploiting the fires to oppose the planned repeal of of the CO2-tax. ‘Some people are trying to profit from all the tragedy and suffering of this week,’ said Environment Minister Greg Hunt. By the way, the CSIRO research authority just explained that there have been bushfires in Australia for millions of years.”
If one counts the damage from bushfires in NSW compared to the number of homes, then there has been no recognizable trend over the past 90 years.  In The Conversation, John McAneney presented the facts. Foremost he sees deficits with regards to land-use planning, which made the extent of the fire damage possible.
In July 2017 a study by Nick Earl und Ian Simmonds appeared in the Journal of Geophysical Research. The authors analyzed the Australian bushfire statistics from 2001-2015 and found a reduction in fires. Yet, they did find a large temporal and spatial variability which in part was controlled by ocean cycles such as the El Nino or the Indian Ocean Dipole. Abstract:
Variability, trends, and drivers of regional fluctuations in Australian fire activity
Throughout the world fire regimes are determined by climate, vegetation, and anthropogenic factors, and they have great spatial and temporal variability. The availability of high-quality satellite data has revolutionized fire monitoring, allowing for a more consistent and comprehensive evaluation of temporal and spatial patterns. Here we utilize a satellite based “active fire” (AF) product to statistically analyze 2001–2015 variability and trends in Australian fire activity and link this to precipitation and large-scale atmospheric structures (namely, the El Niño–Southern Oscillation (ENSO) and the Indian Ocean Dipole (IOD)) known to have potential for predicting fire activity in different regions. It is found that Australian fire activity is decreasing (during summer (December–February)) or stable, with high temporal and spatial variability. Eastern New South Wales (NSW) has the strongest decreasing trend (to the 1% confidence level), especially during the winter (JJA) season. Other significantly decreasing areas are Victoria/NSW, Tasmania, and South-east Queensland. These decreasing fire regions are relatively highly populated, so we suggest that the declining trends are due to improved fire management, reducing the size and duration of bush fires. Almost half of all Australian AFs occur during spring (September–November). We show that there is considerable potential throughout Australia for a skillful forecast for future season fire activity based on current and previous precipitation activity, ENSO phase, and to a lesser degree, the IOD phase. This is highly variable, depending on location, e.g., the IOD phase is for more indicative of fire activity in southwest Western Australia than for Queensland.”
 
Share this...FacebookTwitter "
"At the end of a tumultuous decade for biodiversity, in which a report based on the most comprehensive study of life on Earth warned that “nature is declining globally at rates unprecedented in human history”, we spoke to some of the world’s leading voices on the environment about their greatest fears for the next decade – and also their hopes. As the IPBES report’s authors noted: “It is not too late to make a difference, but only if we start now at every level from local to global.” We asked three questions:  Chair of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) 1. The species I am most directly worried about is our own! Humanity has reached a point that has enabled us to inflict large-scale and lasting damage on our natural world – destroying ecosystems, driving species extinctions and even changing our global weather patterns. Food security, energy, health and livelihoods all depend on nature’s contribution to people.  2. Humanity never seems to miss the opportunity of missing an opportunity. My greatest regret of the last 10 years is how many chances we have collectively missed to make better choices and drive better policies. We have known enough about the risks of our damage to nature, yet the determination to act and to change seems to be constantly deferred to a later date. Before we know it, we will run out of time and the question will no longer be what we regret most, but rather how could we have been so foolish. 3. The conservation work I am most excited by goes beyond traditional conceptions of conservation – acknowledging that the nature crisis must be understood as one that is also a social, ethical, economic, health and security crisis. This is the basis for the next work programme of the IPBES (until 2030). It has potential to finally tackle the root causes of the loss of nature – values and behaviours that drive the destruction of biodiversity and nature’s contributions to people. 17-year-old British Bangladeshi activist 1. The species I am most worried about is the critically endangered spoon-billed sandpiper (SBS), which breeds in the Arctic Russian tundra, migrates 8,000 km along the East Asian-Australasian Flyway, with a significant percentage wintering on intertidal mudflats on Sonadia Island in southern Bangladesh. My mother’s family are Bangladeshi and so I feel a huge connection to this tiny, captivating bird, with its minute spoon-shaped bill. In 2010 the population was down to 200 birds, the combined weight of which is less than a single mute swan. The SBS is still a long way from being out of danger, despite a captive breeding programme. With many current threats, from mudflat reclamation in China to a sea-level rise wiping out their wintering areas, the survival of this species is still on a knife-edge. 2. Ten years ago, we understood more than ever before about the habitats across the world, the life they support and their importance to our planet. This gave us a huge opportunity to conserve the rarest habitats by working with indigenous peoples who understand them best. However, these were opportunities either squandered or exploited by the large global conservation organisations, such as the WWF and Wildlife Conservation Society, who have instead continued to remove indigenous peoples from their land in Africa and Asia, leaving the land more vulnerable to commercial poaching. Poaching in Africa and Asia is now pandemic, with wildlife no longer being protected by indigenous peoples who would previously have been able to identify poachers before they killed, but are instead excluded from having an input in conserving their animals. 3. I see a change coming in the way that international conservation organisations work with local and indigenous peoples around the world to save habitats, species and retain biodiversity. The decolonisation of conservation is happening, with a final realisation that indigenous peoples who have been successfully managing their land for thousands of years will be able to stop the mass commercial poaching and hunting taking place in national parks. I have seen examples of indigenous peoples in Africa, South America, and Asia using conservation to support their heroic efforts to save rainforests and species like orangutan in Borneo and yellow-headed picathartes in Ghana. I have recently become a global ambassador for Survival International, which fights for the human rights of indigenous peoples. I am very hopeful about seeing this new type of conservation work rise and dominate over the next decade. I hope that all of this will finally stop the assumption that white, ‘educated’ people understand and can look after lands, habitats, and wildlife better. I intend to be at the forefront of that campaign, which is the international side of the coin to my campaigning so far which has been making the sector ethnically diverse in the UK, rather than 99.4% white as it is now. Naturalist and television presenter1. I am worried about those species that are highly specialised in terms of their habitats. The more specialised you are as an animal, requiring unique resources, diet and habitat to survive, the greater effect any rapidly changing circumstances will have. It doesn’t just have to be animals either, it also includes things like chalk streams and sandy lowland heath – in the UK we have more than anywhere else in the world and this habitat is rarer than tropical rainforests. Those things that are least resilient and least common need to be our focus. 2. Activism is growing now and we are seeing positive results with Extinction Rebellion and School Strikes for Climate having a big impact. But it has been dormant for some time and that’s disappointing. Second, our NGOs aren’t showing their muscle. Many are very broadly supported and have an enormous legacy of credibility: the RSPCA, the RSPB and Wildlife Trust for example have vast public support but they’ve not been wielding that to best benefit and that’s been disappointing. And lastly we have an enormous armoury of technologies, capabilities and tried-and-tested methods to implement practical conservation with almost immediate success but we haven’t been doing enough of it. 3. I’m interested in new techniques and also divesting our trust into younger ecologists and conservationists who are brave and take risks and who know that there are enormous gains to be had if we get off the fence and start actually calling some shots. I’m looking for a new attitude in conservation. In terms of the practicalities: rewilding. Through this there are opportunities for carbon capture and planting trees, landscape resilience in terms of preventing flooding, and biodiversity generating a more natural mosaic and ecosystems which would be good. Primatologist and founder of the Jane Goodall Institute 1. While governments delay or ignore their commitment to reducing emissions it is vitally important to protect and restore our forests. We are destroying these precious forests at a terrifying rate, thus releasing stored CO2 from the trees and the forest soils into the atmosphere. The forest habitat is home to many of the most endangered specieson the planet so protecting and restoring forests is an imperative for us all. 2. We have missed so many opportunities – introducing environmental education into schools around the world, doing more to alleviate poverty (the really poor destroy the environment as they try to make a living, buy the cheapest food to survive). But perhaps the biggest opportunity missed is the lack of government subsidies for innovative technologies that will help us to live in better harmony with the natural world. Instead of subsidising clean green energy – solar, wind and tide – it is the big oil and gas companies, with their billions of dollars, that are receiving the support and tax breaks that enable them to continue to pollute and destroy. 3. I am really excited about some of the technology that is already making a huge difference, for example radio collars and microchips that enable scientists to accurately map animal movements. Also drones that can help to map habitats and plot out the range of individual animals, and a newly developing field of thermal cameras on drones that can detect wildlife under forest cover … helping rangers to better protect wildlife from poachers. Director of Survival International 1. I am most worried about human beings, and especially those who live most differently to “ourselves”. Many, especially non-European, peoples hunt, herd or grow their own food and are, at least in part, self-sufficient. We can’t all live like this, but we can learn from them. If the keys to real change lie anywhere, it lies with them. 2. The greatest missed opportunity has been the complete failure of the billion-dollar conservation industry to do anything more than pay lip service to tribal, indigenous and local peoples, whilst at the same time continuing to steal their lands and destroy them, and failing to “conserve” nature as well as they had been!  3. My hope is for the exposure of the hypocrisy behind the conservation industry, which hopefully will force it to accept the rights of tribal, indigenous and local peoples to manage their own environments, as they have done for generations. This is a struggle between those who want to kick people off their land, and those who want and need to live sustainably on and from it. If the fundamentalist environmentalists win, it will lead to yet more pollution and destruction. Ecologist and writer 1. It’s hard to beat coral reefs for the prize of global catastrophic rapid decline of a habitat. But it’s not a contest, and globally, all habitat types are in declining health. Our footprint continues to expand. It’s been calculated that 70% of all birds on Earth are now poultry, about 60% of all non-human mammals are farmed cattle, pigs, sheep, and goats. If you think of all those deteriorating wild habitats as proxy for all plants and animals living in them, you come to one uncomfortable conclusion: the human species is no longer compatible with the rest of life on Earth. 2. We have proven ourselves quite capable of creating global problems but have not shown an ability to control or reverse those problems. As long as we must maintain a global human population growing at roughly the rate of a new Mexico annually (around 70 million), getting in front of problems of the environment and the extinction crisis is like running after a hot-air balloon. The biggest missed opportunity is our failure to have an intelligent and compassionate conversation about human population. 3. Because an expanding human population is the greatest driver of environmental degradation as well as a major driver of poverty and injustice, the greatest lever for conservation is women’s empowerment. The only thing that has worked to ease population is women making their own voluntary choices. On a different front we must transition rapidly away from fossil fuels and the greatest lever there is financial, divesting from fossil fuels, divesting from banks that finance fossil fuels, and investing in clean energy technologies. Natural England chair and author 1. I’ve spent much of my career working for the conservation and restoration of the tropical rainforests and I have been utterly mortified to see what has recently been happening, especially in Brazil this year, where vast fires have wiped out thousands of square kilometres of habitat. These ecosystems are vital not only for wildlife and human cultural heritage, but also in the battle against climate change and for water security. 2. In 2010 countries agreed an ambitious set of 10-year goals to conserve the Earth’s incredible wildlife riches. These Aichi Targets have been largely sidelined to the point where they will be missed. A meeting of the United Nations Convention on Biological Diversity will take place in China in 2020, where the global compass on this agenda will be reset for the coming decade. Countries really need to up their ambition in advance of that meeting, otherwise the mass extinction of species that is now gathering pace will soon turn into a tragedy of truly colossal proportions. 3. I am very excited about how in some countries the conservation agenda is beginning to shift, moving from the rather grim task of hanging on to the last remnants of wildlife-rich habitat and rare species, to increasingly being about large-scale nature recovery. This is happening here in England, and I hope and pray that during the coming few years we will be able to show the kind of leadership that is so desperately needed on the global stage, as we are doing on climate change. If we can begin to rebuild habitats and species here in our densely populated islands, then that will be a real beacon of hope, showing that it is possible to reverse historic trends, in the process setting a new path for the future. Now is the time to do this. Director of Strong Roots Congo 1. I am worried about all endemic species of animals and plants, but mostly about great apes; and especially the eastern lowland gorillas and mountain gorillas. Their numbers have dropped dramaticallyin the last two decades. 2. Separating human from nature has been the biggest missed opportunity! The contribution and role of local communities and indigenous peoples in sustainable conservation is well-documented worldwide. This was not taken into account when “modern conservation”, imported by colonialism, was imposed on mostindigenous lands. Driven mostly by big organisations, this type of conservation has invested enormous amounts of money, energy and time into conservation and the result is a high rate of species extinction and deforestation globally. We missed considering local communities and indigenous peoples in our conservation efforts. We missed an opportunity to understand the spiritual and cultural values of nature. 3. I am most excited about models that consider the rights of natural resources governance and the management of indigenous people! This is the only way we can reverse the damages we brought into “conservation” for lucrative benefits.  Environmental activist at Youth 4 Nature 1. I am honestly worried about all sorts of ecosystems, because man has driven them to the brink of collapse. But most definitely, I am most worried about forest habitats in the sub-Saharan Africa. We are losing our forests at a faster rate, through deforestation and land degradation.  2. The biggest missed opportunity of the last 10 years is not working with nature, indigenous frontline communities, and with youth. World leaders have wasted so many years talking and “negotiating” for their stomachs and unsustainable economic powers while ignoring the role of nature and indigenous communities in addressing climate change. They seem to be waking up now, but time is running out. 3. I am most excited about on-the-ground implementation of nature-based solutions, and especially land restoration across Africa.There is no sidelining of young people anymore. They need us, and we have real experience, not the boring conference reports and speeches. There is no 1.5°C without nature, and there is no 1.5°C without youth. Former IPBES chair 1. Coral reefs are incredibly susceptible to small changes in ocean temperature, and exacerbated by land-based pollution and ocean acidification.Other highly sensitive ecosystems include cloud forests and those at high latitudes. The species most at risk are endemic species who occupy small climatic zones and are unable to migrate fast enough to survive. 2. I would highlight three missed opportunities – failure to address climate change, which will become the greatest threat to biodiversity in the coming decades; failure to remove perverse agricultural subsidies that lead to loss of biodiversity; and failure to transform unsustainable agricultural production systems into sustainable agricultural systems using agro-ecological processes, coupled with reduced food waste and healthier diets. 3. We need to rethink protected areas, and a redesign of corridors that allow for migration under a changing climate. This should be accompanied by large-scale restoration projects. Sailor and founder of the Ellen MacArthur Foundation 1. All across the world, in habitats of every region, biodiversity is being destroyed. Not a single region of the planet is escaping the onslaught. Globally, a million animal and plant species are facing extinction. The driving force behind this mass extinction is our current linear economy. This “take-make-waste” system sees us remove ever more materials from the ground, make products from them that we mostly use for only a short period of time, and then throw them away as waste. Each element of this approach represents a direct threat to biodiversity. 2. The past 10 years have seen huge conservation efforts, including ocean clean-ups and protected zones – but, while necessary and laudable, they are not enough. The most effective strategy is one that prevents damage in the first place. In short, protecting biodiversity requires a fundamental shift in the way our economy works.  3. This vision is becoming reality. To secure long term economic development we need a system fit for the future, not one stuck in the past. What if our aim was not simply to do less harm, but to actively regenerate our natural world? We know what the solutions look like, and that the opportunities are out there. All we have to do is grasp them. Director of Science at the Royal Botanic Gardens, Kew 1. The overwhelming evidence makes me really, really concerned about the future of tropical rainforests. They are the most biologically diverse ecosystem on Earth, containing millions of species – many of them providing crucial benefits to us and playing essential roles in nature. Increased degradation of natural environments is not restricted to the Amazon: Madagascar lost some 366,000 hectares of forest in 2018, which is 4.3% of all its original rainforest. It is clear to see that this is insanely unsustainable. 2. To stop and revert this trend, we need radical changes across all segments of society. We should all stop and review our consumer choices – it could be by avoiding buying furniture made of non-certified wood, or avoiding products that contribute to deforestation, like meat and dairy produce fed with Brazilian soybeans. The media attention on the climate crisis has unfortunately overshadowed another huge, and arguably even more significant crisis: the loss of biodiversity. The problem with biodiversity loss is that if we lose a species to extinction, it is gone forever – and right now, currently one in five plants are threatened with extinction. 3. There are many great opportunities for tackling the climate and biodiversity crises at the same time, but we must get it right from the beginning. For instance, companies and governments have never been so keen to invest in carbon offsetting, in particular afforestation. But it is important we plant the right trees in the right place. If we do so, we can combine long-term carbon storage goals with habitat restoration, increasing opportunities for wildlife to re-establish and regain stable population sizes.The most important action, however, is to conserve what we already have whenever there’s a choice. Not destroying a native habitat is immensely better than trying to restore it afterwards. Indian activist, 15, who started a battle against plastic straws in 2018 1. The Aravalli biodiversity habitat is the only forest around Delhi and acts as its lungs. It is a haven for more than 900 species of terrestrial plants, 208 species of birds and at least 113 species of butterflies. The Aravallis offer a mosaic of micro habitats for a variety of species – from big mammals to small birds and even microbes. I am worried about many species like the Indian Tiger, great Indian bustard, one-horn rhinoceros, Indian vulture. 2. In India, we have missed the opportunity to implement extended producer responsibility at the time of opening up the economy in the late 1990s. We missed imbibing the fact that development should not be at the cost of the environment. These two factors have been missed not just in India but in all developing nations. 3. The River Cauvery is a forest-fed perennial river which is fast becoming a seasonal stream as 87% of tree cover has been removed in 50 years. “Cauvery Calling” is a campaign setting the standard for how India’s rivers can be revitalised. It will initiate the revitalisation of the Cauvery river and transform the lives of 84 million people. The project aims at helping farmers plant over 2.4bn trees through agro-forestry programmes. Marine conservationist at the University of York 1. Coral reefs are the richest, most vibrant and best loved of all ocean ecosystems. They provide habitat for countless species, from enormous whale sharks to vanishingly small snails, crabs and worms. On coral reefs, a quarter of all shallow-water marine species are crammed into an area of only one tenth of one percent of the surface of the ocean. But all is not well. Corals are incredibly fussy about temperatures, liking it hot but not too hot. As global warming has gripped the planet, there have been repeated mass die-offs of coral spanning the globe. There are dire but scientifically credible predictions of the near complete loss of coral if we stay on the current path to more than 2C of warming by the end of this century. 2. Ten years ago I participated in a meeting at the Royal Society of London to consider the future of coral reefs. What we concluded was that they were in critical trouble. Reefs had become the first habitat on the planet for which anthropogenic greenhouse gas emissions, then at 383 parts per million, had already overshot their safe zone (350 ppm). Not only would we have to bring down emissions to save them, we would have to recapture some of the carbon already emitted. We fed the findings of our meeting into the Copenhagen climate conference a year later. To little avail. We are still scrambling to mount an adequate response. 3. If we are to save reefs in any semblance of their present state we will have to give it everything we have. The world community is near achieving its goal of protecting 10% of the sea by 2020. But science tells us we need to protect at least 30% from extractive and damaging uses to safeguard wildlife and habitats. Coral rich countries around the world, like the UK and the Seychelles, have already committed to such protection. There is a good chance this target will soon be adopted across the planet, helping keep coral reefs on life support while fossil fuels are phased out. Sued the Indian government over climate crisis inaction 1. The model development governments have taken up is out of balance. Mass tree-felling is common to make way for dams, infrastructure and road projects, leading to a big fall in the species count throughout India due to habitat loss.  2. Kedarnath flash floods, Kerala floods, Bihar floods, Assam floods and many such disasters have taken hundreds of lives and continue to do so every year, but as a country we haven’t learned from these disasters. The government itself is saying that the spring water channels are drying up, the rivers are getting contaminated every day, the piles of solid waste are growing like a mountain in every corner of the country, biodiversity and habitat are being lost and still there no plans to mitigatethe situation. 3. The ongoing efforts to clean and rejuvenate the Ganges River and its tributaries have excited me the most."
"Royal Dutch Shell is at risk of falling short on plans to invest up to $6bn (£4.6bn) in green energy projects between 2016 and the end of 2020, with its slow progress likely to raise concern that oil companies are not moving fast enough to help tackle the climate crisis. The Anglo-Dutch oil company has spent an estimated $2bn on building a low-carbon energy and electricity generation business since setting up its “new energies” division in 2016. With a year to go, the sum is well below Shell’s own guidance that the total investment between 2016 and the end of 2020 would be between $4bn and $6bn. Shell’s green energy plans are some of the most ambitious in the oil industry, despite assigning just a 10th of its spending pot to “new energies”. Shell told investors in 2017 it would spend between $1bn to $2bn a year developing a clean energy business up to the end of 2020, up from a previous plan to spend up to $1bn a year in the same period. Under the plans Shell would spend up to $6bn on green investment , but instead it is on track to meet a third of this, with only a year left for the company to meet its guidance. Up to the end of 2019, Shell’s guidance suggests it should have spent at least $3bn. In the same four years the company spent more than $120bn developing fossil fuel projects and set out plans to increase its total spending to $30bn a year in the early 2020s. A spokesman for Shell declined to comment. Shell is considered a climate leader within the oil industry despite spending a fraction of its total budget on new energies, which include biofuels, hydrogen and electricity investments. Data from Rystad Energy, a Norwegian consultancy, shows that Europe’s five largest oil companies – Shell, BP, Total, Eni and Equinor – together spent a total of $5.5bn on renewable energy projects to date, comparedwith a combined total budget of almost $90bn last year alone. Stephen Kretzmann, the executive director of Oil Change International, said executives “trumpet their relatively tiny investments in renewables” but continue to “pour more fuel on the fire of global warming every day”. He said: “It used to be the case that some people believed that an oil company that invested even only a small portion of their resources in renewable energy was worthy of praise… because it makes us feel better to believe that the people who run these powerful companies get it.” Oil bosses have voiced support for global climate targets in public but the industry continues to invest an estimated 1% of its annual spending budget on clean energy while producing more fossil fuel products than the Paris Climate Agreement allows. “The executives that run the carbon companies definitely do not get the part about the need for them to make less of the thing that is driving climate disaster,” Kretzmann added. “The big problem isn’t too little investment in renewables - it’s too much investment in, and government support for, fossil fuels.” Shell’s green spending plans were dealt a blow earlier this year when the company missed out on a multibillion dollar race to buy Dutch utility Eneco, which has a large renewable energy portfolio. Shell and its pension fund partner lost out to a consortium of investors led by Japan’s Mitsubishi, which paid $4.5bn for the company. The deal might have pushed Shell’s green investment towards its planned spending range. Shell said it was disappointed it lost the bid, and said that it would continue to invest growing gas and electricity generation from renewable sources. Shell’s previous acquisitions have included UK energy supplier First Utility, a 49% stake in Australian solar company ESCO Pacific, and Eolfi, a French renewable energy developer that specialises in floating wind projects. Shell plans to spend $2bn to $3bn through its “new energies division” every year between 2021 to 2025. The company said it plans to become the world’s biggest electricity company by the 2030s, and hopes to bring a reliable electricity supply to 100 million people in developing countries by 2030."
"By the end of the century, the world’s remaining tropical forests will be left in a fragmented, simplified, and degraded state. No patch will remain untouched – most remnants will be overrun by species that disperse well, which often means “weedy” plants like fast-growing pioneer trees and small rodents that thrive in disturbed areas. Most of the rest will be “the living dead” – tiny remnant populations of plants and animals hanging on with no future. There is no cast-iron law that dictates this scenario – but it appears likely unless we see a series of major policy changes. What could unfold? In research published in the journal Science, colleagues and I outline an all too common chain of events. The first cut of timber from any natural forest is the most lucrative. The most remote places, in the interior of Amazonia, in central Congo and the heart of Borneo are all coveted by industrial loggers. The logging frontier marches relentlessly on. They selectively take the biggest trees and along with them the habitat of species that rely them.  Today, less than 25% of tropical forests have escaped industrial logging and each year new concessions are given to industrial loggers in forests that had hitherto never been logged. While parts of the forest remain following logging, truly intact tropical forests may soon become a thing of the past. Logging pushes roads into the forest. It’s estimated that an astonishing 25m kilometres of road will be built in the tropics by 2050. Roads begin to isolate fragments of forest, and some ground-dwelling specialist species fail to cross even small openings.  Roads also bring hunters and markets together: in the decade to 2011 some 62% of Africa’s forest elephants were killed for their tusks. Usually international logging companies cut first, for export, and then they sell on their concession. This encourages a second cut of less desirable timber species, without waiting for the forest to recover, and further degradation ensues. This degraded forest is more susceptible to forest fires which kill trees and drive out many species. Heavily logged and degraded forest is then often slated for conversion to agricultural plantations. About 100m hectares of tropical forest – four times the size of the UK – has been converted over the past 30 years. Ominously, the palm oil industry which devastated much of Indonesia’s and Malaysia’s forests is now moving into Africa, which up to now has had relatively low deforestation rates. With food demand set to double, this pressure on tropical forests will intensify. Much more forest looks set to be lost. The remaining forest is fragmented and surrounded by other agriculture. The trend is towards small patches of isolated, fire-impacted, logged-over forests, with no large animals due to overhunting.  Now add a new pressure to this process: climate change. On the positive side, more carbon dioxide in the atmosphere increases tree growth and their resilience to drought. Yet, on current emissions scenarios, tropical forests are set to warm by about 4℃ this century. With hotter temperatures and increased frequency of the most extreme El Niño events that cause droughts, huge forest fires would rage, turning even areas that escape conversion to agriculture into savanna-type vegetation rather than tropical forest. As the climate rapidly changes, plants and animals will need to move to continue to live within their ecological tolerances – one study calculated they would need to keep moving 300 metres each year through this century to keep in the current temperature they live in.  How are organisms supposed to move through fragmented, isolated and degraded forest patches? Climate change and forest fragmentation together is a recipe for the mass extinction of tropical forest species this century. How might such a fate be avoided? Aside from a rapid global shift to low-carbon energy, two changes of policy direction would help. First, given widespread poverty in tropical forest regions, policies that enourage “development without destruction” are needed to increase prosperity without undermining the forest and the services it provides. Unfortunately, most of the benefits from logging, mining and intensive agriculture flow away from local people. Giving forest-dwellers long-term collective legal rights over their land would mean benefits flow to them. Importantly, studies show local people with legally recognised land rights preserve forests. A study of 292 protected areas in Amazonia showed that indigenous reserves were highly effective at avoiding deforestation in high pressure areas. A study of 80 forest commons across Asia, Africa, and Latin America showed forest was maintained when local people managed them. Of course, forest-dwellers won’t be perfect managers of forests, but they won’t look for a quick profit and then move on, as big businesses often do. And they represent a win-win situation for human rights and conservation. Unbroken forested corridors linking tropical forest landscapes with those 4℃ cooler will also be necessary to reduce levels of extinction. So landscape planning is required on a massive scale – and new areas will require restoring to provide links between forest areas. For example, those rare tracts of intact forest in Southeast Asia need connecting all the way to the foothills of the Himalayas. This sounds dramatic, but neither climate change nor forest wildlife stick within political borders.  Is development without destruction an academic dream? There is good news among the bad: the UN New York Declaration on Forests is a promising start – more than 100 signatories, including governments, businesses and indigenous peoples groups have pledged to halve deforestation by 2020 and ensure palm oil, soy, paper and beef production is deforestation free. The declaration also includes promoting land rights. The UN climate talks in Paris will also show whether institutions can rise to the challenges of our globally changing environment. Agreements on reducing deforestation, including durable finance, could play an important role in keeping forests standing, as would allocating funds for land-use planning to retain forest connectivity. There are signs of changes in policy to avoid a global simplification of tropical forests, but the window of opportunity is closing."
"The death of a celebrity often makes the headlines, but it is less common that the death of wild animal has the same effect. However, it appears that the entire world has mourned the loss of Cecil the lion, killed on a private game reserve bordering a national park in Zimbabwe. But is the recent barrage of attacks on trophy hunting, and the US dentist who killed Cecil, justified? Let’s be clear: Cecil was killed illegally, which we don’t condone. The landowner who allowed the hunt on his reserve without the necessary permit should face the justice system. But this one bad apple should not tarnish an entire industry. Legally hunting lions in Zimbabwe is highly regulated: it requires various permits and licenses from the client, professional hunter and hunting reserve owner. National quotas aim to ensure sustainable off-take of the species and, in western Zimbabwe, lions are only killed once they have reached a certain age to make sure they’ve had the chance to pass their genes on. As a result, lion populations in Zimbabwe are either stable or increasing.  So if hunts are conducted following these rules, can trophy hunting really help conserve lions? Some argue that even if this were the case, the practice still shouldn’t be allowed because it involves killing a charismatic and threatened animal for fun. Opponents suggest that non-lethal alternatives such as photographic tourism should be the main way in which conservation is funded.  But there are a number of problems with this argument. Hunters are willing to go to remote and unstable areas into which most photographic tourists are unwilling to venture. Far more photographic tourists than hunters would have to travel to Africa to make up the same level of revenue, so the carbon footprint from all that air travel would surely have a significant environmental impact. It should also be noted that the potential for nature tourism is not equally distributed, with the industry often focused only around a few locations. This leaves other regions without access to tourism revenue. Oh, and let’s not forget that wildlife reserves can also kill lions. If the goal is to preserve populations and species (as opposed to the welfare of individual animals), countries with healthy wildlife populations should be able to use their natural resources to cover the costs of management. This is particularly the case in countries such as Zimbabwe, one of the poorest places in the world. Zimbabwe has a tradition of using trophy hunting to promote wildlife conservation. Through the CAMPFIRE programme, which ran from 1989 to 2001, more than US$20m was given to participating communities, 89% of which came from sports hunting. In more recent times, populations of elephants and other large herbivores have been shown to benefit from trophy hunting. Zimbabwean trophy hunting generates roughly US$16m of revenue annually. While it has been rightly pointed out that only 3% of this goes towards local communities, the ethical implications of removing this money without a clear alternative need to be examined.  The economic impact of trophy hunting in comparison to tourism as a whole may not be huge, but what is the alternative if it is made illegal?  Zambia banned trophy hunting of big cats in 2013, only to reverse it earlier this year because the government needed the money to fund conservation.  Conservation costs money – so does the damage done by lions killing livestock.  It is not clear whether photographic tourism alone could cover these financial burdens. If trophy hunting is to continue, how can we make it more sustainable?  One study suggested we need to enforce age restrictions on trophy animals throughout the entire country, improve monitoring, change quotas over time depending on environmental conditions and ensure that lion hunts are at least 21 days long.  Another study found that trophy hunting can be beneficial to lion conservation when the income is shared with locals who live with this species (and have to deal with the negative consequences of their presence).   While it is sad that we sometimes have to resort to killing animals for conservation, let’s not allow emotions to overtake our arguments. Conservation is a complex, difficult industry and needs all the financial help it can get: we are after all living through the sixth mass extinction. How much money will that take to fix?"
"Greta Thunberg has said she wouldn’t have wasted her time talking to Donald Trump about climate change at the UN climate change summit in New York earlier this year – the same event she was pictured glaring at the one of the world’s leading climate-change deniers.  The Swedish climate activist made the comment during an interview on BBC Radio 4 on Monday morning, where she had been invited to guest-edit the programme. Thunberg, 16, was asked what she would have said to the leader who pulled the US – one of the world’s leading carbon emitters – out of the Paris climate accord, and who has taken radical steps to undo decades-old US pollution standards. She said: “Honestly, I don’t think I would have said anything. Because obviously he’s not listening to scientists and experts, so why would he listen to me?” She added: “So I probably wouldn’t have said anything, I wouldn’t have wasted my time.” Thunberg’s comments came several weeks after Trump attacked her for being named Time magazine’s person of the year. “So ridiculous. Greta must work on her Anger Management problem, then go to a good old fashioned movie with a friend! Chill Greta, Chill!” Trump tweeted at the time. She has also been attacked by Brazil’s far-right president Jair Bolsonaro. “It is staggering, the amount of coverage the press gives that brat,” Bolsonaro said at the time. Invited to respond to her critics, Thunberg told the program “those attacks are just funny because they obviously don’t mean anything”. She said: “I guess of course it means something – they are terrified of young people bringing change which they don’t want – but that is just proof that we are actually doing something and that they see us as some kind of threat.”"
"
Share this...FacebookTwitterConservationist/wind-energy protest group Rettet den Odenwald (Save the Forest of Odes) here writes that yet another endangered stork nest was recently destroyed at the forested location near a proposed JuWi wind park.
Controversy swirls over German wind park builder JuWi 
Normally the clearing of forest land to make way for industry is required to undergo an extremely strict permitting process involving very detailed environmental impact studies. Violations are usually punished extremely harshly. But when it comes to wind parks in Germany, the fox in the henhouse often seems to rule. This also may be the case at a proposed JuWi wind park location in the area of Donnersberg (Palatinate).
Back in 2016 the existence of a nest at the location of interest was proven. The nest belonged to a pair of rare black storks that later gave birth to three offsprings that year, and to four more in 2017. Normally with such a nest in the area, obtaining a permit to clear away forest and to set up an industrial complex would be totally out of the question.
Rare and legally protected black stork nest gets allegedly destroyed in what is suspected to be a criminal attempt to clear the way for a wind park construction permit. Photo see: Rettet den Odenwald
According to Rettet den Odenwald, the nest belonging to the pair of rare black storks appears to have been recently willfully and criminally destroyed.
Earlier, local citizens had worked closely with authorities to stop the construction of five JuWi wind turbines, which had been permitted to be built right close to what later was discovered as the nest belonging to the pair of protected black storks. The black stork pair had been expected to return to its nest by early March to produce offsprings.
Tree and nest destroyed
But then on February 10, 2018, Rettet den Odenwald broke the tragic news: the tree in which the nest had been perched had been singled out and  illegally cut down “by unknown attackers” using a power saw, thus preventing the stork pair from returning and successfully nesting this year.
The obstacle blocking the construction of the JuWi windpark in the area was in effect disposed of.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Destroyed tree in the Forest of Odes (Odenwald). Home to the nest of a pair of rare, legally-protected black storks was illegally cut down. Conservationists and wind park opponents suspect foul play by the wind industry. Photo see: Rettet den Odenwald
Path “reopens for greed”
The conservationist Rettet den Odenwald site writes:
This lawbreaking allows the permitting process for the planned and halted wind park to now appear in a new light. … The lawful protection that was established by the provision of facts was illegally undone and thus has again reopened the door for the greed of those with a stake in the wind park.”
The outrage by conservationists and wind park opponents came swiftly and loudly. Already on February 12 Rettet den Odenwald issued a press release in which they demand the Environment Ministry to assure that no permit be granted in the event of such criminal acts and that they take swift action.
JuWi condemns destruction
In a press release, JuWi stated that it “condemns the criminal act in the harshest terms”. Moreover the press release adds: “JuWi is filing criminal charges against unknown perpetrators for violating federal nature protection laws”.
The latest in a series of criminal environmental destruction acts
This is not the first time that nests and homes for protected species located in proposed wind park areas have been destroyed in Germany. Der Spiegel has reported on this before, e.g. see here.
Also read “wind power mafia” destroys stork’s nest here.  
When it comes to saving the planet, wind parks seem to get away with everything nowadays. Often times wind turbines get installed right up close to residents and thus make them sick from infrasound, or they ruin idyllic landscapes, destroy biotopes, cause hazards in the North Sea, shred migrating birds, etc. Environmental concerns from citizens be damned!
As far as the wind park in Odenwald is concerned, don’t be surprised if its construction ends up getting permitted soon. Greed disguised as green always gets its way in Germany.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterElectric Vehicle Emissions 27-50% Greater
Than Internal Combustion Engine Vehicles

Image: Qiao et al., 2017
Sales of electric vehicles (EV) in China have exploded in recent years.
According to the New York Times (October, 2017), between 2014 and 2017, annual EV purchases by China’s citizens more than doubled, from 145,000 in 2014 to 295,000 (projected) for 2017.   By 2019, the annual sales of EVs are expected to swell to 814,000 for China alone, which will eclipse the expected EV sales for the rest of the world combined (602,000).
Good news for the climate, right?  After all, driving an EV is green.  Driving an EV reduces CO2 emissions.   Driving an EV is sustainable.  Right?
Well, no.  According to recently published scientific papers, driving an EV in China dramatically increases CO2 emissions relative to driving an internal combustion engine vehicle (ICEV).
Why?  Because China’s electricity grid is overwhelmingly powered by fossil-fuels (i.e., 88% of China’s energy consumption  (2015) is derived from coal, oil and gas).   Therefore, the energy used to charge up an electric vehicle in China is derived from a rapidly growing fossil fuel-based electrical grid.
Fossil fuel-powered electricity grids are growing in prevalence across the world.  And this will continue to be the case as “1,600 coal plants are planned or under construction in 62 countries” which will “expand the world’s coal-fired power capacity by 43 percent” (New York Times, July, 2017).
As long as EVs continue to be predominantly powered by the growing fossil fuel infrastructure in China (“Chinese corporations are building or planning to build more than 700 new coal plants at home and around the world”), driving EVs will not reduce CO2 emissions relative to driving ICEVs.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Put another way, purchasing and driving a conventional internal combustion engine vehicle will actually reduce China’s CO2 emissions.
According to Barkenbus (2017), “when EVs receive electricity with emission levels exceeding 559 gCO2/kWh, they, unfortunately, are net contributors to climate change when compared with conventional vehicles.”
China’s EVs receive electricity with emissions levels of 712 gCO2/kWh, which is 27% greater than the emissions associated with driving the average ICEV.

Image: Barkenbus, 2017
Not only that, but as the introductory image above indicates, the manufacture of battery-powered EVs emit 50% more greenhouse gas emissions (CO2) than ICEVs do.
Qiao et al., 2017
“In this study, the life cycle energy consumption and greenhouse gas emissions of vehicle production are compared between battery electric and internal combustion engine vehicles in China’s context. … Greenhouse gas emissions of battery electric vehicles are 50% higher than internal combustion engine vehicles.”
“Electric Drive Vehicles (EDVs) are considered to be environmentally-friendly and have attracted much attention worldwide, and Battery Electric Vehicles (BEVs) are the most popular vehicles among all kinds of EDVs. In China, the country with the world’s largest automotive market, the government is determined to develop BEV industry and produced over 250 thousand BEVs in 2015, and the annual growth rate was 420%. In addition, according to the production plan, the cumulative output of BEVs in China will reach 5 million in 2020, meaning that BEVs will gradually replace Internal Combustion Engine Vehicles (ICEVs).  BEVs [Battery Electric Vehicles] are designed to obtain more environmental benefits, but the energy consumption and GHG emissions of BEV production are much larger than those of ICEV [Internal Combustion Engine Vehicles] production in China.”
So why is it that advocates of CO2 emissions reductions so readily extol the explosion of EV purchases and use worldwide?
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAt Die kalte Sonne site here, geologist Dr. Sebastian Lüning and Prof. Fritz Vahrenholt comment on the controversy surrounding allegations of Big Oil “covering up” knowledge of the impacts their products could have on climate.
For example on April 16, 2018, renowned German weekly Spiegel reported on how “a confidential Shell study” showed the oil company “kept knowledge over climate change secret” and how “Shell knew already in detail 30 years ago about the greenhouse gas effect – and decided to keep silent.”
Now just a couple of weeks later, we find out that back then oil companies like Shell in fact didn’t know any more about climate change than other climate experts. The clandestine “Shell study” summarized:
–A thorough review of climate science literature, including acknowledgement of fossil fuels’ dominant role in driving greenhouse gas emissions. More importantly, Shell quantifies its own products’ contribution to global CO2 emissions.
–A detailed analysis of potential climate impacts, including rising sea levels, ocean acidification, and human migration.
–A discussion of the potential impacts to the fossil fuel sector itself, including legislation, changing public sentiment, and infrastructure vulnerabilities. Shell concludes that active engagement from the energy sector is desirable.
–A cautious response to uncertainty in scientific models, pressing for sincere consideration of solutions even in the face of existing debates.
–A warning to take policy action early, even before major changes are observed to the climate.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the time of the Shell study, the history of the earth’s climate over the past 2000 years had been poorly understood, and so Lüning and Vahrenholt find the cover-up accusations against Shell a bit strange. Overall the two German scientists say the whole story tells us just how weak the accusations of the activist side really are.
Natural factors have been proven
Some three decades later, the two German scientists note that far more is known today: for example the important roles played by natural ocean cycles and the global phenomenon of the Medieval Ice Age, which now is acknowledged and make the climate models look obsolete. Moreover, CO2 climate sensitivity has been dialed back with each passing year.
Lüning and Vahrenholt add:
Over the coming months and years the IPCC will have to admit to some changes in its understanding of the climate.”
The real scandal is the cover-up of natural factors
But the real intent of the climate activists turning to litigation is about generating publicity, as plainly admitted by German site Klimafakten on 16 April 2018:
Going to court for more climate protection – and for more transparency
Everywhere across the world activists are fighting against climate change by using litigation. Experts already count about one thousands court proceedings in 24 countries. For the litigants it’s not only about getting a ruling, but about publicity: The suits are  a means for strategic communication.”
The real deal, say Lüning and Vahrenholt, is that natural climate factors have been known as real drivers for over a decade, and activists, scientists and the IPCC continue to cover them up.
In fact, Lüning and Vahrenholt say that this is the real story that needs to be the subject of litigation. The real scandal is
How activists and the IPCC covered up knowledge of natural climate change.”
Share this...FacebookTwitter "
"Moths are the insect we truly love to hate. The press report almost annually on the looming threat of clothes moths. I have previously written in defence of diamondback moths, a migratory pest of cabbage crops, and highlighted the quirks of biology that drove the spectacle of thousands of Silver Y moths gatecrashing the Euro 2016 final without tickets. I am absolutely unapologetic about my love for these diverse and intriguing cousins of the much better-loved butterflies. Moths get a bad press thanks to a few species which negatively affect our lives (this also applies to other insects, such as wasps), but most are harmless (or beneficial), fascinating, and often even beautiful. And so we come to recent news reports of a plague of toxic caterpillars descending on London. The caterpillars in question are those of the oak processionary moth (Thaumetopoea processionea, or just OPM) – just about the only species for which I struggle to summon up much sympathy. So what’s the issue? It’s not to say that this is an unattractive moth. The grey-black colour scheme of the adults, active in late summer, lends them the look of having been delicately sketched in pencil. You are much more likely to encounter the caterpillars, which are covered in very long, white hairs. Colonies of OPM caterpillars form white silk nests on oak trees and can be spotted moving about in remarkable nose-to-tail processions. Other moth species form similar nests in the UK, including occasionally on oak: if such a nest is found outside London (especially in Essex or Cambridgeshire), it is more likely to be the Brown-tail moth. Unlike the Brown-tail, however, the oak processionary moth is not native to the UK. It was first recorded in Britain in 1983, but the species established properly in around 2006, when it’s believed some eggs arrived on imported oak trees. This isn’t in itself a reason to dislike OPM, as conservationists (including myself) can sometimes be hypocritical about non-native species: for instance, we are vocally concerned about the arrival of the horse-chestnut leaf-miner moth because it harms horse chestnut trees – even though the trees themselves are non-native. However, OPM is also a potentially a public health problem. Each of the caterpillar’s hairs contains a toxin called thaumetopoein. Touching an OPM caterpillar directly could bring you out in a rash – in fact, as a general rule it’s always best to avoid hairy caterpillars unless you know what you’re dealing with. The hairs of OPM caterpillars can also break off and drift on the air and, if there are sufficiently high densities of caterpillars, these hairs can cause rashes and respiratory problems even to bystanders.  Besides affecting people, OPM can also impact the oak trees on which it feeds. A particularly severe infestation could strip a tree completely bare of its leaves, though few cases of this taking place in the UK have been reported. Most people agree that something needs to be done, although the NGO Butterfly Conservation argues that, rather than tackling the moth wherever it appears, control efforts should focus on areas where the threat to human health is high or large numbers of trees are at risk of death. Nevertheless, controlling OPM outbreaks is difficult. The nests are constructed in the crowns of oak trees when they are in full leaf, and even if they can be reached, removing them manually requires full protective equipment to ward off the toxic hairs. For that reason the preferred approach is currently to tackle nests remotely, spraying trees with insecticide when the moth is most vulnerable – as a young caterpillar, between April and June.  There is currently no insecticide that is specific to OPM, so a bacterium known as “Bt” is used. Unfortunately, Bt is toxic not just to OPM, but to the caterpillars of all moths and butterflies. The financial cost of these control efforts is astronomical – estimated at around £1.2m per year in 2016-17. The uncounted, and incalculable, cost to the oak woodland ecosystem could be greater still. The loss of much of the insect biodiversity from our woodlands would be tragic in itself but is likely to have further implications for the bats, birds and other wildlife that rely on these insects for food during their breeding seasons – a study of an OPM control programme in woods near Pangbourne, Berkshire, suggested that blue and great tits were breeding less after spraying took place. It’s these losses that have put me off OPM. But let’s not panic – there are plenty of reasons to feel hopeful about the future of Britain’s oak woodlands. It’s true that the moth has been recorded “across vast regions of the south-east”, but that mostly only refers to the highly-dispersive males. To spread the outbreak requires the egg-laying females to travel, and they don’t fly nearly as far. This means that, for now, the toxic caterpillars are mainly confined to London. The outbreak has crossed the M25 ringroad in just a few places, and is still only expanding at a slow pace. Encouragingly, some of the more isolated sections of the outbreak also appear to be coming under control. New outbreaks in Watford, Barnet, and Pangbourne all appear to have been successfully removed. An outbreak at Bethlem Hospital, Croydon, estimated to contain 4,000 nests in 2012, was confined to just four trees by 2016. Vigilance is key, and this year the Forestry Commission is once again asking the public to report any potential sightings of OPM through its Tree Alert scheme. Finally, we may have some unexpected allies on our side. In its native southern and central Europe, OPM is not especially problematic because it rarely reaches sufficiently large population densities. That’s partly because its numbers are kept in check by its natural enemies – parasitoids. This is a catch-all term for various insects with a rather gruesome life-cycle: eggs are laid inside caterpillars and other insects, before the larvae eat their victim from inside out (killing it in the process) and emerge as a fully-formed fly or wasp ready to seek out new prey.  Often, when an insect expands its range by artificial means (as OPM did, entering the UK on imported trees), it can take some time for its parasitoids to catch up, and in this lag period the insect may do particularly well. However, a recent study found nearly half of the OPM caterpillars sampled from the Croydon outbreak in 2014 were infested by one such natural enemy, the Carcelia iliaca tachinid fly. This suggests the oak processionary moth may be reaching the end of its lag period in the UK, and as the flies attack more caterpillars, this could help the control efforts. My enemy’s enemy is truly my friend, and in this case, perhaps it is a tiny fly."
"
Share this...FacebookTwitterGeologist Dr. Norman Page left a comment which I’ve decided to upgrade to a post. In it he writes solar and La Nina observations fit well with his recent paper showing that climate is controlled by natural orbital and solar activity cycles.
Dr. Page is among a growing number of scientists who share the general view that natural solar and oceanic cycles are mostly driving the climate, just as they always have in the past.
Warming has already peaked, cooling ahead
And as a result, Dr. Page believes that the millennial temperature cycle peaked at about 2003/4 and the earth is now in a cooling trend, which will last until about 2650. Read background here.
Recently he published a paper titled: “The coming cooling: usefully accurate climate forecasting for policy makers“.
Models “unfit for purpose”
His paper argues that the methods used by the establishment climate science community are not fit for purpose and that a new forecasting paradigm should be adopted. A number of papers have been published over the recent years pointing out that climate models have been far short of reliable.
In the paper’s abstract Dr. Page writes that the Earth’s climate is the result of resonances and beats between various quasi-cyclic processes of varying wavelengths and that it is not possible to forecast the future unless there’s a good understanding of where the earth is in time in relation to the current phases of those different interacting natural quasi periodicities.
“Temperature decline in the coming decades and centuries”
He presents evidence specifying the timing and amplitude of the natural 60+/- year and the all important 1,000 year periodicities (observed emergent behaviors), which he and other scientists maintain are so obvious in the temperature record.

Fig. 8, HadSST3 temperature anomaly: “Over the last 135 years an approximate 60 year periodicity is clearly present in the temperature data.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




He projects cyclic trends forward and predicts a probable general temperature decline in the coming decades and centuries.
Large divergence by 2021
He also estimates the timing and amplitude of the coming cooling, writing: “If the real climate outcomes follow a trend which approaches the near term forecasts of his working hypothesis, the divergence between the IPCC forecasts and those projected by his paper will be so large by 2021 as to make the current, supposedly actionable, level of confidence in the IPCC forecasts untenable.”
The 1991 millennial solar activity peak is seen in Figure 10 neutron data.
Dr. Page notes that there is a varying lag between the solar activity peak and the corresponding peak in the different climate metrics because of the thermal inertia of the oceans. In the abstract he writes:
It has been independently estimated that there is about a 12-year lag between the cosmic ray flux and the temperature data – Fig. 3 in Usoskin (28).”
Page says this correlates with the millennial temperature peak seen at 2003/4 in the RSS data in Fig 4,

Fig 4. RSS trends showing the millennial cycle temperature peak at about 2003 (14)
Page also says that since the strong El Nino peak anomaly of 2016, the temperature has “declined rapidly” and: “The cooling trend is likely to be fully restored by the end of 2019.”
======================================
Dr. Norman J. Page    
 Email: norpag@att.net
 DOI: 10.1177/0958305X16686488
 Energy & Environment
 0(0) 1–18
 (C )The Author(s) 2017
 journals.sagepub.com/home/eae
Share this...FacebookTwitter "
"The Israeli public broadcaster has come under fire from angry listeners after broadcasting an interview with Tony Abbott in which he said the world was “in the grip of a climate cult”. During the interview, recorded on 15 December while his home state of New South Wales was fighting terrifying bushfires, Abbott denied that carbon dioxide was driving global warming. The interview was broadcast on New Year’s Eve in a special show reviewing key international issues of the decade. Abbott said: “While we still seem to be in the grip of a climate cult, the climate cult is going to produce policy outcomes that will cause people to wake up to themselves.” After claiming, incorrectly, that a focus on emissions reduction in Australia had caused blackouts and rising power prices, Abbott said: “Sooner or later, in the end, people get hit over the head by reality.” The host of the show, Israel Public Broadcasting Corporation foreign editor Eran Mor-Cicurel, interjected part-way through the interview, pointing out Australia had been hit by fires and disasters “happening again and again”. The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. Mor-Cicurel asked Abbott if he was “denying that fact we are in the process of global warming and global change”. Abbott responded that while there was “no doubt that climate has changed”, previous warming events had happened before the industrial revolution. Abbott’s points are contradicted by every major science academy in the world, as well as the expertise of government science agencies in his own country. His anti-science views on climate change are well known in Australia and have been corrected many times by leading climate scientists but Mor-Cicurel was not aware of his views before the interview. Mor-Cicurel told Guardian Australia the radio network had been reporting regularly on Australia’s bushfire crisis and was not aware of Abbott’s views before the interview. He said: “Personally I was surprised [by Abbott’s views on climate change]. I did not expect a former prime minister of Australia to be so blunt about environmental issues in the middle of an environmental crisis in Australia. “When we put it to air, people were terribly angry at us for airing such extremist views, especially from environmental organisations that were annoyed that we had given the stage to these kinds of views.” Well, Israel is a small country with big problems. However it has a large and vocal environmental movement. They were outraged. Some have blamed us for spreading lies as they failed to differentiate between the broadcaster and the interviewee. Abbott said changes to climate in the past “makes me think as a matter of simple logic that carbon dioxide emissions, particularly human carbon dioxide, are not the only, or even the main factor here”. He said that “all things being equal of course we should try and reduce our carbon dioxide emissions”. But, he said: “The last thing we should do is drive our industries offshore and be putting pressure on household budgets and risk third world-style blackouts all in the name of climate change. We have got to be sensible and balanced and proportionate about these things and I don’t think other policy makers are right now.” Australia’s bushfire season began in August, much earlier than usual, and has claimed at least 19 lives, destroyed more than 1,600 homes and burned at least 4.6m hectares of land – an area more than double the size of Israel. Earlier in the interview, Abbott named three key themes for the globe over the past decade: the “challenge of China”, the “climate cult” and the “ongoing Islamist challenge”. He said: “I think the western world has continued to suffer from serious self-doubt over the last decade and it’s exacerbated by the rise of what are effectively new religions like the climate cult and there is an ongoing Islamist challenge … and it’s been obvious since September 2001.” Abbott was prime minister from September 2013 to 2015. He lost his Sydney seat of Warringah at the 2019 general election."
"For those that haven’t noticed, it’s been rather warm in Europe. In fact, last Wednesday was the UK’s hottest July day since records began. The temperature at Heathrow soared to 36.7°C, approximately 15°C higher than the average maximum daily temperature for the month. In mainland Europe it has been even hotter, with much of Spain well into the 40s and Paris seeing its highest temperature since 1947. However, the extreme conditions haven’t been limited to high temperatures – thunderstorms were generated that produced fantastical lightning shows, heavy downpours and hail stones the sizes of ping pong balls.  The World Meteorological Organisation defines a heatwave as five or more consecutive days when the maximum daily temperatures are at least 5°C above average for that time of year.   Persistent hot weather of this type is linked to high pressure systems, where air descends, heats up and drys out. This suppresses the formation of clouds and allows for clear conditions for days or even weeks.  Western Europe’s weather is largely governed by the jet stream. This high-altitude, high-velocity river of air meanders around the globe and is constantly changing position. When an omega-shaped wave is present on the jet stream which arcs over Europe, warm dry air from southern Europe and Africa can be pulled north, pushing temperatures higher than normal. If this upper level feature coincides with high pressure at the surface with relatively low pressure to the east and west an omega block is formed.  This pattern of flow can be very persistent and lead to long periods of fine weather in summer. Decreased cloud cover and transport of warm air over a number of days can produce very high temperatures. Such a situation occurred last week, replacing the usually cool westerly air stream with warm air from over continental Europe. An important consequence of this situation has been the formation of a Spanish plume, which transports warm dry air from over Spain to the UK (sometimes North Africa, which explains the red dust left on your car after it rains).  This air rises as it travels and acts as a lid under which energy is trapped and builds. This energy is then suddenly and dramatically released through thunderstorms, as the warm air bumps into the colder from the jet steam and further north. The strongest thunderstorms are generated at the edge of the high pressure region – hence the storms over the UK last Wednesday. Europe has been affected by two notable heatwaves in recent memory, during the summers of 2003 and 2006. Both were caused by very similar meteorological conditions to those occurring now.  While hot, dry weather might seem like perfect summer conditions, such severe heatwaves can be deadly. The “lid” in the atmosphere causes pollutants to build up near the surface. Exposure to these, together with dehydration and heat stress pose a very real risk. In the heatwave of 2003 almost 15,000 deaths were attributed to the weather conditions in France alone. Heatwaves can also be responsible for destruction of crops, widespread drought, power cuts due to lightning strikes, accelerated glacier melt and destructive forest fires – governments across Europe have plans specifically designed for reducing the impact. What impact will climate change have on this? It will no doubt lead to higher temperatures in Europe, and with hotter conditions the air is able to hold more water. This means there is more energy that can be released by thunderstorms, which is expected to lead to heavier downpours and other more severe weather.  However, it is impossible to attribute a particular event to climate change. Just like rolling a loaded die, you cannot say whether any particular six you roll was because the die is loaded, only that the chance of a six was higher.  The key components of a heatwave are the flow patterns on a continental-scale, and whether the frequency of these patterns will be significantly changed in an altered climate is still uncertain. As such, it is an important and interesting topic of debate among scientists."
nan
"
Share this...FacebookTwitterAnthropogenic Influence On Arctic Climate
 ‘Too Small To Be Detected’

Source: Haine, 2016

The evidence compiled in scientific papers continues to rapidly accumulate.
An anthropogenic signal in the regional Arctic climate is still too small to be detected.
Temperature, glacier melt, and sea ice changes are all well within the range of natural variation for the Arctic region.  The changes that do occur have identifiable origins that are unrelated to atmospheric CO2 concentrations or human emissions.
Below is a brief summary of some of the latest research that underscores the lack of connection between anthropogenic influences and climate-related changes in the Arctic.

Arctic Temperature And Ice Retreat Mechanisms
1. Arctic Warming Since 1990s ‘Dominated By Natural Variability’ (NAO)
Orsi et al., 2017
The recent warming trend in North Greenland  … We find that δ 18O [temperature/climate proxy] has been increasing over the past 30 years, and that the decade 1996-2005 is the second highest decade in the 287-year record (Figure 4). The highest δ 18O values were found in 1928, which is also an extreme year in GISP2 and NGRIP ice cores, and in a coastal South Greenland composite [Vinther et al., 2006; Masson-Delmotte et al., 2015], but the decadal average (1926-1935) is not statistically different from the decade (2002-2011).
The surface warming trend has been principally attributed to sea ice retreat and associated heat fluxes from the ocean [Serreze et al., 2009; Screen and Simmonds, 2010a, b], to a negative trend in the North Atlantic Oscillation (NAO) since 1990, increasing warm air advection on the West Coast of Greenland and Eastern Canada [Hanna et al., 2012; Fettweis et al., 2013; Ding et al., 2014], and to an increase in the Greenland Blocking Index [Hanna et al., 2013]. These latter mechanisms could be dominated by natural variability rather than forced response to the anthropogenic increase in greenhouse gases [Fettweis et al., 2013; Screen et al., 2014].
2. Arctic Ice Melt Since 1995 Due To Natural Cloud Cover Decrease, NAO
Hofer et al., 2017
Decreasing cloud cover drives the recent mass loss on the Greenland Ice Sheet … The Greenland Ice Sheet (GrIS) has been losing mass at an accelerating rate since the mid-1990s. … We show, using satellite data and climate model output, that the abrupt reduction in surface mass balance since about 1995 can be attributed largely to a coincident trend of decreasing summer cloud cover enhancing the melt-albedo feedback. Satellite observations show that, from 1995 to 2009, summer cloud cover decreased by 0.9 ± 0.3% per year. Model output indicates that the GrIS summer melt increases by 27 ± 13 gigatons (Gt) per percent reduction in summer cloud cover, principally because of the impact of increased shortwave radiation over the low albedo ablation zone. The observed reduction in cloud cover is strongly correlated with a state shift in the North Atlantic Oscillation promoting anticyclonic conditions in summer and suggests that the enhanced surface mass loss from the GrIS is driven by synoptic-scale changes in Arctic-wide atmospheric circulation. … Th[e] strong correlation between summertime NAO index and the MAR-based cloud cover could be used to forecast whether the observed reduction in cloud cover during summer, and the associated increase in GrIS melt, is likely to continue.

3. Geothermal Heat Flux From ‘All Over’ Greenland The ‘Primary Process’ Behind Temperature Changes
Rysgaard et al., 2018
The Greenland ice sheet (GIS) is losing mass at an increasing rate due to surface melt and flow acceleration in outlet glaciers. … Recently it was suggested that there may be a hidden heat source beneath GIS caused by a higher than expected geothermal heat flux (GHF) from the Earth’s interior. Here we present the first direct measurements of GHF from beneath a deep fjord basin in Northeast Greenland. Temperature and salinity time series (2005–2015) in the deep stagnant basin water are used to quantify a GHF of 93 ± 21 mW m−2 which confirm previous indirect estimated values below GIS. A compilation of heat flux recordings from Greenland show the existence of geothermal heat sources beneath GIS and could explain high glacial ice speed areas such as the Northeast Greenland ice stream. … Geothermal springs with source water temperatures above 0 °C have been found all over Greenland, especially around Disko Island in West Greenland, where several thousands of such springs have been identified. … Therefore, we assume that vertical turbulent mixing and GHF [geothermal heat flux] are the primary processes behind the observed salinity and temperature change.
4. Recent Winter Arctic Warming Driven By Planetary Scale Waves
Baggett and Lee, 2017
The dynamical mechanisms that lead to wintertime Arctic warming during the planetary-scale wave (PSW) and synoptic-scale wave (SSW) life cycles are identified by performing a composite analysis of ERA-Interim reanalysis data. The PSW life cycle is preceded by localized tropical convection over the Western Pacific. Upon reaching the mid-latitudes, the PSWs amplify as they undergo baroclinic conversion and constructively interfere with the climatological stationary waves. The PSWs [planetary scale waves] flux large quantities of sensible and latent heat into the Arctic which produces a regionally enhanced greenhouse effect that increases downward IR and warms the Arctic two-meter temperature. The SSW life cycle is also capable of increasing downward IR and warming the Arctic two-meter temperature, but the greatest warming is accomplished in the subset of SSW events with the most amplified PSWs. Consequently, during both the PSW and SSW life cycles, wintertime Arctic warming arises from the amplification of the PSWs [planetary scale waves].
5. Recent Canadian Arctic Warming (1988-1996) And Cooling (1997-2016) Driven By The AO
Mallory et al., 2018
The AO [Arctic Oscillation] has positive and negative phases that infuence broad weather patterns across the northern hemisphere (Thompson et al. 2000). For example, during the positive phase of the AO, atmospheric pressure over the Arctic is lower than average, which tends to result in warmer and wetter winters in northern regions as warmer air is able to move further north (Thompson et al. 2000; Aanes et al. 2002). …  From 1988 to 1996, the summer intensity of the AO was largely in the positive phase, with a mean value of 0.207 (± 0.135 SE), and this was a period of population stability or growth for each of the three herds that we examined here. In contrast, from 1997 to 2016 the summer AO has remained largely in the negative phase [cooling], with a mean value of − 0.154 (± 0.077 SE), and over this period the Bathurst, Beverly, and Qamanirjuaq herds declined in abundance. … We found that positive intensities of the Arctic Oscillation (AO) in the summer were associated with warmer temperatures, improved growing conditions for vegetation, and better body condition of caribou.
6. Greenland Glacier Retreat, Growth Linked To The NAO
Bjørk et al., 2017     
Changes in Greenland’s peripheral glaciers linked to the North Atlantic Oscillation … [W]e map glacier length fluctuations of approximately 350 peripheral glaciers and ice caps in East and West Greenland since 1890. Peripheral glaciers are found to have recently undergone a widespread and significant retreat at rates of 12.2 m per year and 16.6 m per year in East and West Greenland, respectively; these changes are exceeded in severity only by the early twentieth century post-Little-Ice-Age retreat. Regional changes in ice volume, as reflected by glacier length, are further shown to be related to changes in precipitation associated with the North Atlantic Oscillation (NAO), with a distinct east–west asymmetry; positive phases of the NAO increase accumulation, and thereby glacier growth, in the eastern periphery, whereas opposite effects are observed in the western periphery. Thus, with projected trends towards positive NAO in the future, eastern peripheral glaciers may remain relatively stable, while western peripheral glaciers will continue to diminish.
7. Arctic’s Polar Vortex Changes ‘Primarily A Result Of Natural Internally-Generated Climate Variability’
Seviour, 2017
Weakening and shift of the Arctic stratospheric polar vortex: Internal variability or forced response? … By comparing large ensembles of historical simulations with pre-industrial control simulations for two coupled climate models, the ensemble mean response of the vortex is found to be small relative to internal variability. There is also no relationship between sea-ice decline and trends in either vortex location or strength. Despite this, individual ensemble members are found to have vortex trends similar to those observed, indicating that these trends may be primarily a result of natural internally-generated climate variability.
Arctic Temperature Changes In Recent Decades
8. No Net Warming Since 1940s/1950s In Alaska, Subarctic North Atlantic, Siberia…Climate Trends Consistent With 50-90 Year AMO
Nicolle et al., 2018
Persistent multidecadal variability with a period of 50– 90 years is consistent between the subarctic North Atlantic mean record and the AMO over the last 2 centuries (AD 1856–2000). … In the North Atlantic sector, instrumental sea surface temperature (SST) variations since AD 1860 highlight low-frequency oscillations known as the AMO (Kerr, 2000).  …  The LIA is, however, characterized by an important spatial and temporal variability, particularly visible on a more regional scale (e.g., PAGES 2k Consortium, 2013). It has been attributed to a combination of natural external forcings (solar activity and large volcanic eruptions) and internal sea ice and ocean feedback, which fostered long-standing effects of short-lived volcanic events (Miller et al., 2012).



9. Greenland Has Been Cooling Since 2001
Westergaard-Nielsen et al., 2018
Here we quantify trends in satellite-derived land surface temperatures and modelled air temperatures, validated against observations, across the entire ice-free Greenland. … Warming trends observed from 1986–2016 across the ice-free Greenland is mainly related to warming in the 1990’s. The most recent and detailed trends based on MODIS (2001–2015) shows contrasting trends across Greenland, and if any general trend it is mostly a cooling. The MODIS dataset provides a unique detailed picture of spatiotemporally distributed changes during the last 15 years. … Figure 3 shows that on an annual basis, less than 36% of the ice-free Greenland has experienced a significant trend and, if any, a cooling is observed during the last 15 years (<0.15 °C change per year).

10. Greenland Has Been Cooling Since 2005
Kobashi et al., 2017 
For the most recent 10 years (2005 to 2015), apart from the anomalously warm year of 2010, mean annual temperatures at the Summit exhibit a slightly decreasing trend in accordance with northern North Atlantic-wide cooling.  The Summit temperatures are well correlated with southwest coastal records (Ilulissat, Kangerlussuaq, Nuuk, and Qaqortoq).

11. No Net Warming In Greenland For The Last 90 Years
Kobashi et al., 2017


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Arctic Sea Ice Changes 
12. Arctic Sea Ice Expanding Since 1988 (Bohai Sea), AO & NAO ‘Primary’ Climate Factors 
Yan et al., 2017
Afforded by continuous satellite imagery, evolution of sea ice cover over nearly three decades from 1988 to 2015 in the Bohai Sea [North China] as a peculiar mid-latitude frozen sea area is reported for the first time. An anomalous trend of slight overall increase of 1.38 ± 1.00% yr–1 (R = 1.38, i.e. at a statistical significance of 80%) in Bohai Sea ice extent was observed over the 28 year period. …  Correlation with decreasing Arctic Oscillation (AO) index (r = –0.60, p < 0.01) and North Atlantic Oscillation (NAO) index (r = –0.69, p < 0.01) over the study period suggested AO and NAO as the primary large-scale climate factors for Bohai Sea ice.

13. Arctic Sea Ice Oscillates…Not Significantly Lower Now Than In The 1940s
Connolly et al., 2017
According to this new dataset, the recent period of Arctic sea ice retreat since the 1970s followed a period of sea ice growth after the mid 1940s, which in turn followed a period of sea ice retreat after the 1910s. Our reconstructions agree with previous studies that have noted a general decrease in Arctic sea ice extent (for all four seasons) since the start of the satellite era (1979). However, the timing of the start of the satellite era is unfortunate in that it coincided with the end of several decades during which Arctic sea ice extent was generally increasing. This late-1970s reversal in sea ice trends was not captured by the hindcasts of the recent CMIP5 climate models used for the latest IPCC reports, which suggests that current climate models are still quite poor at modelling past sea ice trends.

14. Arctic Sea Ice Extent Only Slightly Lower Now Than During Little Ice Age, Much Higher Now Than Most Of Last 7,000 Years
Perner et al., 2018
[W]e find evidence of distinct late Holocene millennial-scale phases of enhanced El Niño/La Niña development, which appear synchronous with northern hemispheric climatic variability. Phases of dominant El Niño-like states occur parallel to North Atlantic cold phases: the ‘2800 years BP cooling event’, the ‘Dark Ages’ and the ‘Little Ice Age’, whereas the ‘Roman Warm Period’ and the ‘Medieval Climate Anomaly’ parallel periods of a predominant La Niña-like state. Our findings provide further evidence of coherent interhemispheric climatic and oceanic conditions during the mid to late Holocene, suggesting ENSO as a potential mediator.

15. Solar Forcing Drives Arctic Sea Ice Trends, Sea Ice Higher Now Than Nearly All Of The Last 8,000 Years
Yamamoto et al., 2017
Millennial to multi-centennial variability in the quartz / feldspar ratio (the BG [Beaufort Gyre] circulation) is consistent with fluctuations in solar irradiance, suggesting that solar activity affected the BG [Beaufort Gyre] strength on these timescales. … Multi-century to millennial fluctuations, presumably controlled by solar activity, were also identified in a proxy-based BSI [Bering Strait in-flow] record characterized by the highest age resolution. … Proxy records consistent with solar forcing were reported from a number of paleoclimatic archives, such as Chinese stalagmites (Hu et al., 2008), Yukon lake sediments (Anderson et al., 2005), and ice cores (Fisher et al., 2008), as well as marine sediments in the northwestern Pacific (Sagawa et al., 2014) and the Chukchi Sea (Stein et al., 2017).

16. Southwest Greenland: Sea Ice Increasing Since 1930s, No Net Change In Temperature Since 1600
Kryk et al., 2017     
Our study aims to investigate the oceanographic changes in SW Greenland over the past four centuries (1600-2010) based on high-resolution diatom record using both, qualitative and quantitative methods.  July SST during last 400 years varied only slightly from a minimum of 2.9 to a maximum of 4.7 °C and total average of 4°C. 4°C is a typical surface water temperature in SW Greenland during summer.
The average April SIC [sea ice concentration] was low (c. 13%) [during the 20th century], however a strong peak of 56.5% was recorded at 1965. This peak was accompanied by a clear drop in salinity (33.2 PSU).

17. Arctic Sea Ice Trends Linked To The AMO, NAO, Sea Ice Lower Than Today During Medieval Climate Anomaly
Kolling et al., 2017     
[O]ur reconstructions reveal several oscillations with increasing/decreasing sea ice concentrations that are linked to the known late Holocene climate cold/warm phases, i.e. the Roman Warm Period, Dark Ages Cold Period, Medieval Climate Anomaly and Little Ice Age. The observed changes seem to be connected to general ocean atmosphere circulation changes, possibly related to North Atlantic Oscillation and Atlantic Multidecadal Oscillation regimes. Furthermore, we identify a cyclicity of 73–74 years in sea ice algae and phytoplankton productivity over the last 1.2 kyr, which may indicate a connection to Atlantic Multidecadal Oscillation mechanisms.

18. Arctic Amplification, Sea Ice Loss Not Explained By CO2 Forcing
Kim et al., 2017
Understanding the Mechanism of Arctic Amplification and Sea Ice Loss
Sea ice reduction is accelerating in the Barents and Kara Seas. Several mechanisms are proposed to explain the accelerated loss of polar sea ice, which remains an open question. … [T]he role of upward and downward longwave radiations in Arctic amplification is vague and not fully understood.
[CO2 is not mentioned in the paper as a mechanism responsible for Arctic amplification or sea ice loss.]
Long Term Changes In Arctic Region Temperatures
19. Greenland 1°C to 3°C Warmer Than Now For Most Of The Last 8,000 Years
Kobashi et al., 2017
After the 8.2 ka event, Greenland temperature reached the Holocene thermal maximum with the warmest decades occurring during the Holocene (2.9 ± 1.4 °C warmer than the recent decades) at 7960 ± 30 years B.P.

20. Arctic-Wide Temperatures Warmer Than Now During The Medieval Warm Period
Werner et al., 2017
[S]tatistical testing could not provide conclusive support of the contemporary warming to supersede the peak of the MCA [Medieval Climate Anomaly] in terms of the pan-Arctic mean summer temperatures.

21. Northern Alaska Warmer During Medieval Times
Hanna et al., 2018
Here, we utilize one such sediment archive from Simpson Lagoon, Alaska, located adjacent to the Colville River Delta to reconstruct temperature variability and fluctuations in sediment sourcing over the past 1700 years. Quantitative reconstructions of summer air temperature […] reveal temperature departures correlative with noted climate events (i.e. ‘Little Ice Age’, ‘Medieval Climate Anomaly’). … Reconstructed temperatures are generally coolest between 300 and 800 CE (Tavg = 2.24 ± 0.98°C), displaying three temperature minima centered at 410 CE (1.34 ± 0.72°C), 545 CE (1.91 ± 0.69°C), and 705 CE (1.49 ± 0.69°C). Temperatures then rapidly increased, reaching the warmest interval (800–1000 CE) in the approximately 1700-year record. During this interval, average temperatures were 3.31 ± 0.65°C, with a maximum temperature of 3.98°C.

Share this...FacebookTwitter "
"The most extreme weather of all rarely gets a mention, even in the UK where we’re famous for our weather talk. Far above our heads the Earth is regularly hit by colossal, tsunami-like waves of scorching gas and savage, supersonic winds from space.  The culprit for this extra-terrestrial weather is sat at the centre of our solar system. The familiar pictures of our Sun that portray a plain, incandescent orb, serenely holding the planets in place, couldn’t be further from the truth. The Sun is a rowdy place. One of the most spectacular forms of space weather are Coronal Mass Ejections, where the Sun sporadically throws out billions of tonnes of hot gas and magnetic field into space.  The Sun also generates its own wind, which ranges from “breezes” to “hurricanes”. It’s all on a much bigger scale though – even average solar winds are much more ferocious than anything we could ever experience, with speeds varying between a gentle 500,000 miles per hour to a gusty 2,000,000 mph.  These winds carry with them a part of the Sun’s atmosphere, a million-℃ gas composed of highly energetic electrons, protons and alpha particles. The winds are accelerated along the sun’s outstretched, tentacle-like magnetic field, which originates deep under its surface and extends out past Earth to the edges of the solar system. Being able to forecast the solar wind has its problems though. For example, we know they predominantly originate in darker, less dense patches of the Sun’s atmosphere known as coronal holes, however we are still unable to locate the other significant sources that must contribute to the wind. More importantly, we don’t have a clear explanation of how the winds are heated and accelerated. My colleagues and I were interested in the processes underlying these tempestuous winds. In a study published in the journal Nature Communications, we investigate powerful magnetic waves, known as Alfvén waves, located in the regions where the solar wind originates. These waves cause the Sun’s magnetic field to violently sway back and forth at tens of thousands of miles per hour, transporting energy around the star’s atmosphere and out into space. It is this role as a magnetic energy carrier that means the Alfvén waves are often responsible for accelerating the solar wind to such monstrous speeds. We found that some of the necessary conditions exist for the waves to break down their energy to smaller scales and supply some of it to the wind (potentially via the interactions of the waves with particles)  – something predicted for a couple of decades but never observed. Future studies of Alfvén waves should reveal how much energy they feed to the solar wind and may even allow us to forecast wind speeds.  “Space weather forecasts” may seem like one for the future but such reports are already used by a host of agencies. The UK’s National Grid, for instance, relies upon daily updates to avoid overloads of the electrical grid due to resulting geomagnetic storms (an extreme case in 1989 cut off power for six million people in Quebec). We don’t often encounter the consequences of space weather in our day-to-day lives, but as society becomes increasingly dependent on technology it will surely be felt more keenly. Recent reports have demonstrated how wide-ranging this can be, for example, disrupting radio communications, damaging satellites and causing increased radiation levels on commercial flights. The UK government is concerned enough that space weather was added to the National Risk Register and, in late 2014, it set up the Met Office Space Weather Operations Centre to monitor it and provide an assessment of the risks. Looking forward, if we are to regularly engage in space tourism, asteroid mining or manned trips to other worlds, then travellers and technology will be exposed to the elements once they leave the safety of Earth’s protective magnetic field. While space weather may be hazardous, there are some suggestions we can exploit the solar wind to power spacecraft using magnetic or electric sails or even harvest some of its energy using a so called Dyson-Harrop satellite. Our findings are an example of one of the many advances being made in understanding the origins of space weather, although much more is needed to bring our predictive abilities in line with those of our meteorological friends. Space weather forecasters will also need to continue popularising the lexicon of extra-terrestrial weather and raising awareness of its impact here on Earth. Then, maybe one day, people will tune in to the morning’s space weather report to see whether they should take that trip to Mars."
nan
"Amazon has threatened to fire employees for speaking publicly about the company’s role in the climate crisis, tech workers at the retail giant have revealed.  An email shared with the Guardian shows Amazon’s human resources department launched an “investigation” into one employee, Maren Costa, over comments made to the media that called for the company to do more to tackle the climate crisis. In the email, Costa is told she will not face punishment at this point – but that any future comments unauthorized by Amazon “may result in formal corrective action, to and including termination of your employment with Amazon”. A group of Amazon employees who banded together to call for stronger climate action by the company said several members have been questioned by legal and HR representatives about their public comments. Some received follow-up emails similar to Costa’s that threaten dismissal for speaking out in the future. Costa said four employees have been questioned and two have been threatened with termination if they continue speaking up about Amazon’s role in the climate crisis without seeking approval. Costa, a user experience principal designer, said: “It was scary to be called into a meeting like that, and then to be given a follow-up email saying that if I continued to speak up, I could be fired. “But I spoke up because I’m terrified by the harm the climate crisis is already causing, and I fear for my children’s future. Any policy that says I can’t talk about something that is a threat to my children – all children – is a problem for me.” According to Amazon, it started updating its external communications policy for staff in spring last year. It said it was not aimed at any particular group of employees. A company spokeswoman said: “Our policy regarding external communications is not new and, we believe, is similar to other large companies. We recently updated the policy and related approval process to make it easier for employees to participate in external activities such as speeches, media interviews, and use of the company’s logo. “As with any company policy, employees may receive a notification from our HR team if we learn of an instance where a policy is not being followed.” Amazon’s threats to Costa and other employees occurred after it announced a “climate pledge” in September. The plan commits Amazon to using 100% renewable energy by 2030, before becoming carbon neutral by 2040. To help achieve this, Amazon has ordered 100,000 fully electric delivery vehicles for its fleet. Jeff Bezos, Amazon’s chief executive, said he was “done being in the middle of the herd” on climate policies at the policy’s unveiling, which took place just a day before 1,500 Amazon employees planned to walk out of work to join a wave of global climate strike rallies inspired by Swedish teenager Greta Thunberg. Amazon’s shift on climate change represented a victory for its employee group, which is called Amazon Employees for Climate Justice. The group had previously pushed, unsuccessfully, a shareholder resolution to set a climate change plan. A further 8,000 Amazon employees subsequently signed an open letter to Bezos calling for concrete climate goals; to cancel contracts with oil and gas companies; and to stop donations to politicians who deny the reality of the climate crisis. Amazon employees said the company updated its policy on staff speaking to the press and on social media in early September, a day after the plan to join the climate walkout was announced. The new policy requires staff members to seek permission from Amazon prior to talking in a public forum while identified as an employee. Victoria Liang, a software engineer at Amazon, said: “Amazon’s newly updated communications policy is having a chilling effect on workers who have the backbone to speak out and challenge Amazon to do better. This policy is aimed at silencing discussion around publicly available information. It has nothing to do with protecting confidential data, which is covered by a completely different set of policies.” Bezos, the world’s richest person, has said he understands the concerns of employees and has promised to review Amazon’s political donations. He has rejected, however, calls to sever ties with oil and gas companies. Amazon Employees for Climate Justice said it will continue to push the company to do more on climate, despite the threat of recriminations for speaking out publicly. Justin Campbell, a data engineer at Amazon, said: “Amazon’s policy is not going to stop the momentum tech workers have built over the past year at Amazon. The climate crisis is the greatest challenge we face, and the only way we can find solutions is by protecting people’s right to speak freely and disrupting the status quo.”"
nan
"Russia has published a plan to adapt its economy and population to climate change, aiming to mitigate damage but also “use the advantages” of warmer temperatures. The document, published on the government’s website on Saturday, outlines a plan of action and acknowledges changes to the climate are having a “prominent and increasing effect” on socioeconomic development, people’s lives, health and industry.  Russia is warming 2.5 times faster than the planet as a whole, on average, and the two-year “first stage” plan is an indication the government officially recognises this as a problem, even though Vladimir Putin denies human activity is the cause. It lists preventive measures such as dam building or switching to more drought-resistant crops, as well as crisis preparations including emergency vaccinations or evacuations in case of a disaster. The plan says climate change poses risks to public health, endangers permafrost, and increases the likelihood of infections and natural disasters. It also can lead to species being pushed out of their usual habitats. Possible “positive” effects are decreased energy use in cold regions, expanding agricultural areas and navigational opportunities in the Arctic Ocean. Among a list of 30 measures, the government will calculate the risks of Russian products becoming uncompetitive and failing to meet new climate-related standards, as well as prepare new educational materials to teach climate change in schools. Russia is one of the most vulnerable countries to climate change, with vast Arctic regions and infrastructure built over permafrost. Recent floods and wildfires have been among the planet’s worst climate-related disasters. Moscow formally adopted the Paris climate accord in September last year and criticised the US withdrawal from the pact. Putin, however, has repeatedly denied the scientific consensus that climate change is primarily caused by emissions deriving from human activity, blaming it last month on some “processes in the universe”. He has also criticised the Swedish climate campaigner Greta Thunberg, describing her as an uninformed, impressionable teenager possibly being “used” in someone’s interests. He has also voiced scepticism on numerous occasions about solar and wind energy, expressing alarm about the dangers of turbines to birds and worms, causing them to “come out of the ground” by vibrating. While there is evidence that large wind-power installations can pose a risk to birds, known research does not suggest they harm worms. On Sunday, Russia’s meteorological service predicted temperatures up to 16C higher than normal for Monday and Tuesday, when Russia celebrates Orthodox Christmas."
"
Share this...FacebookTwitterI thought the following paper was interesting. 
No, lead-author Prof. Pierre Gosselin is not me from NTZ. But he very likely is a descendent the same family line. The first Gosselin (Gabriel) left Normandy-France and landed in Quebec City way back in 1653. As a devout Catholic, Gabriel earnestly started what was the population of Gosselins over North America and beyond over the next 364 years.
=============================================
Effects of climate and fine particulate matter on hospitalizations and deaths for heart failure in elderly: A population-based cohort study
In a recent study a team of scientists led by Prof. Pierre Gosselin assessed 112,793 people aged 65 years and older who had been diagnosed with heart failure in Quebec between 2001 and 2011. Over an average of 635 days, the researchers measured the mean temperature, relative humidity, atmospheric pressure and air pollutants in the surrounding environment and studied the data to see if there was any relationship.
Their results: for each decrease of 1°C in the daily mean temperature of the previous 3 and 7 days, the risk of heart failure events is increased of about 0.7%. In other words, a drop of 10°C in the average temperature over 7 days, which is common in the province of Quebec because of seasonal variations, is associated with increased risk to be hospitalized or to die for the main cause of heart failure of about 7% in elderly diagnosed with this disease.
The paper’s abstract:
We measured the lag effects of temperature, relative humidity, atmospheric pressure and fine particulate matter (PM2.5) on hospitalizations and deaths for HF in elderly diagnosed with this disease on a 10-year period in the province of Quebec, Canada.
Our population-based cohort study included 112,793 elderly diagnosed with HF between 2001 and 2011. Time dependent Cox regression models approximated with pooled logistic regressions were used to evaluate the 3- and 7-day lag effects of daily temperature, relative humidity, atmospheric pressure and PM2.5 exposure on HF morbidity and mortality controlling for several individual and contextual covariates.
Overall, 18,309 elderly were hospitalized and 4297 died for the main cause of HF. We observed an increased risk of hospitalizations and deaths for HF with a decrease in the average temperature of the 3 and 7 days before the event. An increase in atmospheric pressure in the previous 7 days was also associated with a higher risk of having a HF negative outcome, but no effect was observed in the 3-day lag model. No association was found with relative humidity and with PM2.5 regardless of the lag period
Lag effects of temperature and other meteorological parameters on HF events were limited but present. Nonetheless, preventive measures should be issued for elderly diagnosed with HF considering the burden and the expensive costs associated with the management of this disease.
Lower risk of death in summer
The authors also found:
The results showed a higher risk of hospitalization or death in the winter period of the year (October to April) compared to the summer period (May to September).”
Share this...FacebookTwitter "
"The idea of a deep-frozen world, “snowball Earth”, has captured the imagination since first proposed in the 1990s. On several occasions in history, long before animals evolved, apparently synchronous ice sheets existed on all the continents. However, much like falling into a crevasse on a glacier, it’s easy enough to enter such an ice age, but very difficult to escape. The snowball Earth theory came from climate modellers who found that low carbon dioxide levels could trigger the growth of ice sheets. The whole planet would become glaciated and its mean temperature drop to as low as -45°C. As ice is much more reflective than the sea, or bare land, the Earth at that point would have been bouncing nearly all of the sun’s radiation back into space. So how could the planet ever emerge from such an ice age?  Volcanoes had to be the answer. Only they could emit enough carbon dioxide into the atmosphere to overcome the effects of Earth’s cool reflective surface. But climate models still found it difficult to plausibly describe how the Earth could have shed its glaciers. We now have the first full explanation for how the best-known snowball event, the Marinoan, finished 635 million years ago with a several hundred metre rise in sea level. The study is the result of work by an international team of scientists, including myself. Our results are published in the journal Nature Geoscience. We found slight wobbles of the Earth’s spin axis caused differences in the heat received at different places on the planet’s surface.  These changes were small, but enough over thousands of years to cause a change in the places where snow accumulated or melted, leading the glaciers to advance and retreat.   The Earth was left looking just like the McMurdo Dry Valleys in Antarctica – arid, with lots of bare ground, but also containing glaciers up to 3km thick. Such an Earth would have been darker than previously envisaged, absorbing more of the sun’s radiation; it was easier to see how the escape from the snowball happened. Today, to find exposed rocks that can tell us about the carbon dioxide content of the atmosphere in the Marinoan, you have to go to the Norwegian Arctic island of Svalbard. In 2009 snowball theory was vindicated after we found the telltale signal of high carbon dioxide levels in Svalbard limestone that formed during the ice age.  Immediately underneath the Marinoan deposits are some beds of rocks deposited at very regular intervals – so regular that they must have formed over thousands of years, influenced by wobbles in the Earth’s orbit. Since Svalbard was near the Equator at the time, the most likely type of wobble is caused by the Earth slowly shifting (“precessing”) its axis on cycles of approximately 20,000 years. Researchers also found evidence of the same process in the Snowball deposits themselves. Fluctuations in ice in relation to the Earth’s orbit are a feature of our modern ice ages over the past million years, but had not been found in such an old glaciation. For a long time the Earth was too cold for glaciers to erode and deposit sediment – the main snowball period. The sediments then show several advances and retreats of the ice. When the glaciers retreated, they left behind a patchwork of environments: shallow and deep lakes, river channels, and floodplains that appeared as arid as anything known in Earth’s history. Carbon dioxide appears to have remained at the same high level throughout the deposition of these sediments. Since it takes millions of years for CO2 to build up in the atmosphere, this implies the sediment layers must have formed quickly – on the order of 100,000 years.  All this fits with the idea of 20,000 year precession cycles.  A group of climate modellers from Paris tested the theory. The rocks and the models agreed: wobbles in the Earth’s axis had caused the planet to escape its snowball phase. So after several million years of being frozen, this icy Earth with a hot atmosphere rich in carbon dioxide had reached a Goldilocks zone – too warm to stay completely frozen, too cold to lose its ice. This transitional period lasted around 100,000 years before the glaciers fully melted and present-day Svalbard was flooded by the sea."
"
Share this...FacebookTwitterRecently German SAT1 television broadcast a documentary on the state of the European and German increasingly green power grid: “How secure are our power grids?” Due to the volatile and unpredictable supply of wind and solar energy, the grid has become far more unstable, the documentary warns. The news is not good.
At best: the consumers are getting a far lousier product at a much higher price.
At the 17-minute mark, Bernd Benser of GridLab-Berlin tells viewers that while grid operator Tennet had to intervene only 3 times in 2002 to avert grid instability, last year he says the number was “over 1000” times — or “three times daily”.
These intervention actions, known as redispatching, cost the consumer about a billion euros last year alone, says Benser.  The SAT 1 voice-over warns that more power transmission lines are urgently needed if the Energiewende is to avoid “becoming a sinking ship“. However over the years acceptance by citizens has swung from a generally warm welcome to ferocious opposition. Politicians need to start noting that green energies have overstayed their welcome.
Major grid instability
And as wind and solar power capacity gets added to the grid without expanding transmission capability to offset the ever more wild fluctuations, grid operators are now constantly scrambling to keep the grid from spiraling out of control. At the 21-minute mark, Klaus Kaschnitz of the operations management of Austrian Power Grid remarks:
These fluctuations in the system that we now see have increased dramatically and are ultimately a product of weather events.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The fluctuations are having a profound impact, Kaschnitz explains. It is especially difficult to keep the grid at 50 Hz frequency, which makes keeping the grid from collapsing harder and the powering of modern industrial systems highly challenging. At the 25-minute mark, the report then switches the focus to the grids’ vulnerability to hackers.
Swiss daily: “danger of a blackout rising”
Also the Swiss online Baseler Zeitung (BaZ) here reports on major power grid woes in Switzerland, warning: “The danger of a blackout is rising” and that power grid operator Swissgrid “must intervene increasingly more often in the power grid“.
According to the BAZ, in 2011 Swissgrid had to intervene only twice over the entire year. But since then the grid has become far more unstable, and that at the current rate it will be necessary to intervene 400 times in 2017!
In summary, the green energies have resulted in two outcomes for citizens: 1) a supply that is now far more unstable and 2) power that is far more expensive. In a nutshell: Far less quality for a lot more money.
That’s the expected result whenever you have the wrong people (activists and politicians) deciding how to run complex technical systems.
 
Share this...FacebookTwitter "
"The UK is a rich, stable country with oil and gas reserves, lots of wind, and more than enough scientists and engineers to make the most of its resources. So why do so many people in the country continue to tolerate high energy bills and poor service? The question of what can be done to encourage energy consumers to become more active and engaged has come to a head after the Competition and Markets Authority (CMA) published provisional findings from its investigation into the UK’s energy market.  One of the things the report considers is the attitudes and behaviour of the energy consumer – ordinary bill-payers like you or I. And one of the findings is that inactive, disengaged consumers are partly responsible for tolerating weak competition. Energy suppliers exploit this inertia to increase their profits.  Only a minority of the UK’s 27m electricity customers, or the 23m gas customers, are regularly shopping around for the best deal, or switching their energy supplier. In fact, 19m buy both their electricity and gas from the same supplier and most of them remain on open-ended, standard variable tariffs (SVTs), rather than fixed term, non-standard tariffs, which are generally (but not always) cheaper.  While this consumer apathy persists, the “Big Six” energy suppliers have little incentive to compete aggressively on either price or service. The first point to make here is that not all energy consumers are the same. However, it could be argued that a failure to acknowledge this has been one of the key problems with the energy market. Privatisation promised increased choice and competition, but the constraints of the supply chain and a shared infrastructure have worked against the development of a truly free market. How different do you really expect EDF and npower to be when they both carry the same electricity on the same grid?  Instead, the UK energy consumer has been bewildered by complicated pricing and paralysed by the belief that changing supplier involves too much hassle and carries the risk of ending up worse off in the long run. Previous sharp practice, using door-to-door sales techniques (which ended in 2012), has left a legacy of mistrust towards the whole industry. Some regulations designed to protect consumers, such as a ban on regional price discrimination and an enforced reduction in the number of tariffs, may have actually weakened competition. There are consumers who are prepared to search for the best deal, either by using a price comparison website, or by joining a buying consortium such as a community energy scheme. They may also have invested in technology to monitor their energy consumption patterns. However, as the latest energy market report points out, these consumers tend to be relatively wealthy and highly educated. They may be motivated by a desire not just to save money, but also by environmental concerns to reduce their carbon footprint. Many other consumers do not or cannot search for the best deal. This may be because they don’t have the necessary numeracy skills, internet access or confidence to make an informed decision. They may be poor, old or living in rented accommodation and more pre-occupied with their immediate short-term finances, leaving them unable or unwilling to make long-term financial choices. Another cause of inaction and apathy is the fact that gas and electricity are perceived as rather boring, undifferentiated commodities; necessary evils that are difficult to get excited about.  For the energy market to work more efficiently and fairly, both the regulators and the energy providers must understand and serve the different types of energy consumer, by segmenting the market in an appropriate way. One study of the Swiss energy market identified six clusters of energy customer, based on their knowledge, values, capabilities and habits. These segments were characterised as “idealistic energy-savers”, “convenience-oriented indifferent energy consumers”, and so on.  The researchers concluded that each group needs to be targeted with different messages and incentives, tailored to their particular characteristics and circumstances. This makes a lot of sense – a wealthy person obsessed with cutting their carbon footprint is hardly likely to be swayed by an advert promising cheap energy aimed at an entirely different demographic. Another method of segmentation based on cost-of-service has been proposed as a possible basis for supplier strategy in the future. The authors of the study, two Stanford scientists, pointed out that two different consumers of energy may have the same overall power usage, but the cost of supplying them can vary greatly, because of their different consumption patterns, such as usage at peak and off-peak times. They argue detailed information on the presence of certain household appliances such as tumble dryers is a strong predictor of energy usage. Combined with detailed data from smart meters (which are planned to be installed in all UK households by 2020), energy suppliers could gain valuable insights into each customers’ energy needs and behaviour. This knowledge could allow them to offer individualised tariffs, perhaps to incentivise off-peak usage of expensive appliances such as tumble dryers. At the same time, data from smart meters, combined with greater trust in the information provided by comparison websites, will be key to consumers becoming more engaged in understanding their own energy usage and encouraging them to switch suppliers on a regular basis."
"The European Union’s common agricultural policy is a sprawling programme of farming subsidies that covers everything from income support for farmers to supporting the promotion of products such as wine. No wonder then that the European Commission, the EU’s executive branch, wants to “modernise and simplify” the policy. This is why the EC has just published its legislative proposals for the common agricultural policy (CAP) after 2020. Its aim to make sure that the CAP continues to support farmers and rural communities, that it leads the sustainable development of EU agriculture, and that it reflects the EU’s ambitions on the environmental and climate change. Across the years 2021-2027, the proposed CAP’s total budget will be around €365 billion. However, the food system in Europe and the UK faces some critical sustainability challenges. And, despite some welcome new objectives – particularly on the environment – and new support payments for young farmers, the proposals clearly do not go far enough on areas such as health.  Europeans deserve an agricultural policy that addresses their health. Too much red and processed meat, and food with lots of fat, sugar and salt means more than 20% of the continent is now obese. Poor diets are also responsible for half of the burden of cardiovascular disease, which remains the leading cause of death in the EU.  However, the CAP announcement contains little on health measures. The EC recently called for better access to “nutritious valuable products such as fruit and vegetables”, yet the new CAP contains no new policy instruments and no specific targets for fruit and veg.  This is disappointing and, unless addressed in the final legislation, will not go unnoticed. From a public health perspective, this really is “low-hanging fruit” as the need for more fruit and vegetables is uncontroversial: the World Health Organisation’s 400 grams per day is a widely accepted minimum standard and Eurostat already provides comparable data on consumption in the EU.  Implying that the school scheme will do the trick is a severe disappointment. The scheme promotes the benefits of healthy eating to children and encourages them to increase their consumption of fruit, vegetables and milk – yet its current budget is just 0.33% of the CAP.  The CAP’s sometimes incoherent position on the use of public money is nicely illustrated by its support for the wine sector. Excessive alcohol consumption is a well known public health problem, and wine subsidies themselves have often been criticised. Yet the draft proposal for CAP reform accords considerable attention to wine promotion measures. Most perversely, the increased value of wine sales may well be one of the policy’s indicators of success. The CAP reform does outline new environmental initiatives such as a new agri-environmental scheme and the first ever initiative to address the decline of pollinating insects. This is welcome, bearing in mind the overuse of pesticides means three-quarters of flying insects have disappeared, jeopardising pollination and yields.  Taking into account that global food and farming production contributes 30% of all greenhouse gas emissions – with 18% from livestock – it will be important to see the details of how the new environmental scheme incentivises climate action. It was disappointing so see nothing specific on soils, as Europe loses 970m tonnes of topsoil every year to degradation and erosion. Europeans also waste 71kg of food per person every year costing €143 billion (2012 figures) in wasted resources and environmental impact – and there is nothing in today’s announcement on waste measures, which is a missed opportunity. What Europe desperately needs is a new comprehensive food policy – one that actually tackles these huge challenges to human health and society, or the environment, across the food system not just at farm level. The EC could start by making healthy and sustainable choices easier for regular people. That might involve new guidelines on public procurement, a continent-wide child obesity strategy or more CAP money set aside for promoting fruit and vegetables.  Europe could also set targets for using less antibiotics and pesticides and could integrate healthy and sustainable nutrition into school curricula. Rising food insecurity across the UK and Europe also means measures must be targeted at those vulnerable groups who aren’t able to access healthy diets. Protecting soils in the face of degradation and nutrient loss could deliver major environmental and health benefits, but the EU and member states have failed to act on this basis, and a proposed directive on soil has remained stalled since 2006. Targeting CAP payments for ambitious crop rotations with a minimum share of legumes would be a more positive approach. Perhaps the new environmental scheme could be more ambitious and include such measures. Incentives should be targeted at those practices across the food system that positively pursue improvements in the soil, water and biodiversity. Let’s also target specific payments for environmental services that favour mixed crop-livestock farms and grassland systems."
nan
"Australians should be proud of the country’s achievements on climate change, energy minister Angus Taylor has argued in a newspaper column that claims “quiet Australians” don’t accept the “shrill cries” of the government’s climate critics. The column, published in The Australian, makes a series of claims about Australia’s emissions and how they compare to other countries, as well as highlighting exports such as LNG that are “dramatically reducing emissions” in other countries.  So is Australia really a paragon of climate virtue – cutting emissions at home while helping the world to cut emissions? As is always the case when it comes to climate and energy policy, there is much to check and understand in Taylor’s article. Prof Frank Jotzo, director of the Centre for Climate and Energy Policy at the ANU Crawford School of Public Policy, told Guardian Australia: “I would characterise [Taylor’s article] as a selective use of statistics that make Australia’s emissions trajectory look good, when in reality it does not look good at all.” Taylor writes that Australia is “responsible for only 1.3 per cent of global emissions, so we can’t single-handedly have a meaningful impact without the co-operation of the largest emitters such as China and the US.” In the context of global emissions, there is much that Australia can, and does, do that has a meaningful impact. The 1.3% figure does not account for Australia’s contribution to global emissions from the fossil fuels we dig up and export. If this exported coal and gas was accounted for, one analysis suggests Australia would be responsible for almost 5% of the global carbon footprint from fossil fuel burning. When countries report their emissions to the United Nations Framework Convention on Climate Change, they only report emissions occurring inside their borders, so it could be argued that using this larger number is unfair. But the problem is that elsewhere in Taylor’s article, he says Australia’s exporting of LNG is helping countries cut emissions. Jotzo says: “If we are going to talk about impacts on global emissions of Australia’s energy exports, then we need to consider all fuels, including coal. Any exporting of coal will result in higher global emissions because it increases the availability and lowers the price of coal, and encourages the use of coal.” While Taylor admits that LNG processing in Australia has pushed domestic emissions higher, he claims that “our LNG exports are dramatically reducing emissions in customer countries such as Japan, South Korea and China — the equivalent of up to 30 per cent of our emissions each year”. But Jotzo says this claim depends heavily on what the LNG displaces. He says the “lion’s share” of the exports will actually replace gas from other sources, rather than displacing coal generation. There is also a risk, he says, that increasing LNG exports also encourages countries to build more gas infrastructure, making it harder to move away from the fossil fuel. He adds: “It is not clear that the availability of Australian LNG decreases emissions internationally.” “Australia meets and beats its emission-reductions targets, every time,” writes Taylor. “We beat our first Kyoto targets by 128 million tonnes. We ­expect to beat our 2020 targets by 411 million tonnes.” The key reason why Australia has easily beaten its targets, is that they were very low to begin with. Australia’s first Kyoto target allowed it to increase emissions by 8% between 1990 and 2010. The second target period required a 5% cut below 2000 levels by 2020. Much of Australia’s cuts to emissions in recent decades, says Jotzo, has been achieved through drops in land clearing, rather than reductions in other parts of the economy the government could have influence over. Australia wants to use some 411 million tonnes of CO2 “credits” amassed over the Kyoto periods against future targets under the separate Paris agreement, even though it admits it is probably the only country looking to use these “carryover credits”. Using carryover credits would cut the amount of emissions reductions Australia would need to find to meet its Paris target by about a half. At the latest UN climate talks in Madrid, Australia came under harsh criticism from more than 100 countries for its desire to use the credits, which some analysts say is a proposal with no legal basis. Australia was accused of “cheating” at the talks, but refused to back down on the carryover issue, leaving it unresolved. In his article, Taylor says “when you compare Australia’s emission-reduction track record with nations such as Canada and New Zealand”, Australia comes out on top. While Australia’s emissions have dropped 12.9% since 2005, writes Taylor, New Zealand’s have risen by 4% and Canada’s have dropped only 2%. Jotzo says the 12.9% figure Taylor is using includes changes to land use, such as land clearing, which are not major issues for other developed countries. As an example, Australia’s reporting to the UN shows that in 2005, emissions from land use, land-use change, and forestry (known as LULUCF) were +88mt. In 2017, LULUCF emissions were -19mt. That’s a net drop of 107mt. Using the same periods for New Zealand, the difference is a net increase of 4.8mt. A fairer global comparison, says Jotzo, is to use figures that remove these LULUCF emissions. This, he says, turns Australia’s 12.9% drop between 2005 and 2018 into a 6% rise. In his article, Taylor repeats a point that he made during his official speech to the Madrid climate talks that technological innovation would be a key to fighting climate change. In the article, Taylor points to the new national hydrogen strategy as an example of innovations with “enormous potential” for cutting future emissions. Jotzo says there is potential for an Australian hydrogen export industry to have a positive impact on global emissions. However, he says this comes with large caveats. Hydrogen can be produced using renewable energy, but also by using fossil fuels. If Australia was to use coal or gas, it would need to be able to capture most of the waste CO2 to claim the fuel as green. But analysis by Jotzo and colleagues shows that while rates of up to 95% carbon capture might be “technically possible” they have not yet been achieved. Only two plants – in Canada and the UK – currently capture CO2 when producing hydrogen from fossil fuels. The best capture rate is 80%. If the carbon capture rates were at 60%, then Jotzo says the net greenhouse gas footprint of hydrogen would be the same as just burning gas. According to Taylor, “Australia has strong targets, clear plans, an enviable track record” on climate change, and Australians should be proud of it. But when overseas groups look at Australia’s record compared to the rest of the world, the assessments come out differently. An analysis by Climate Action Tracker says Australia’s Paris targets are “insufficient” and inconsistent with the Paris goal of keeping global warming well below 2C. Australia has been placed consistently towards the bottom in the annual Climate Change Policy Index analysis of the world’s top 57 emitting nations. The most recent analysis ranked Australia as the sixth worst country on climate change overall. Jotzo, who attended the Madrid climate talks as an observer, said: “Australia was highly regarded at the talks for its technical competence, and it always has. But Australia is not highly regarded at all for its policies or for its efforts to water down effective ambition of the Paris agreement.” He said speaking with observers from other countries, Australia’s position was seen “with quite some bewilderment” especially with the backdrop of the current devastating fire season. Jotzo adds: “They are flabbergasted that Australia is digging in to its stance of getting an easier deal when it would so obviously be in its national interest to encourage strong global action.”"
"At close to 90 years old, Brazil’s most venerated indigenous leader, Raoni Metuktire, has returned to the spotlight to challenge the man he calls the worst president of his lifetime, Jair Bolsonaro. In an interview with the Guardian, the Kayapó chief said he wanted to speak out about the far-right administration’s plans to allow mining in indigenous territory and he warned that Brazil’s Amazon policies threatened global efforts to protect nature and address the climate emergency.  “Ï have seen many presidents come and go, but none spoke so badly of indigenous people or threatened us and the forest like this,” he said. “Since he [Bolsonaro] became president, he has been the worst for us.” Raoni has lived through 24 administrations since first making contact with the world outside his rainforest home, and is at the forefront of a reinvigorated indigenous movement in South America’s biggest nation. Along with Davi Kopenawa Yanomami, he is leading the resistance against government plans to open up the rainforest to land speculators, cattle ranchers, loggers and gold miners. With his lip disc, beads, earrings and flowing grey hair, Raoni is probably the best-known Amazonian in the world. But he spent the first 18 or so years of his life unknown to anyone outside his forest community. Raoni was a young, jenipapo-painted warrior when his tribe, the Metuktire Kayapó, was first contacted by non-indigenous invaders in the early 1950s, according to a new book by the veteran British explorer John Hemming. The intruders brought gifts of metal blades and beads but left behind European diseases such as malaria, influenza and measles that decimated the population. In the 1970s and 1980s, Raoni was among the leaders of the often deadly fight against the BR-080 road, cattle ranchers and the Belo Monte dam. He rose to international prominence thanks to his friendship with the rock star Sting. In the years that followed, he was feted by world leaders and met the pope, gaining a level of prestige and leverage that challenged the prejudices of the many Brazilians who see indigenous people as poor and uneducated. This helped the Kayapó to secure government recognition of their territorial rights across a vast chain of reserves, which formed the spine of a north-south firewall against deforestation. “From many years ago, I fought in campaigns and appeared in the media. Then, when we won the victory of having our lands demarcated, I stopped because everything seemed fine, everything was tranquil,” he recalled. “But the new president threatens indigenous people, so I came back to fight again.” Recent government figures show Amazon deforestation has surged to the highest level in a decade. Farmers and land-grabbers have started more fires to clear land, which is pumping huge quantities of carbon dioxide into the atmosphere, disrupting the water cycle and destroying the world’s most biodiverse land habitat. They have been emboldened by a government that has spent its first year weakening environmental protections, encouraging loggers and heaping scorn on conservation groups and forest dwellers. Even before entering office, Bolsonaro frequently abused indigenous groups as an obstacle to economic development. “It’s a shame the Brazilian cavalry hasn’t been as efficient as the (North) Americans who exterminated the Indians,” he said in 1998. Now in power, he has promised to halt demarcation of new reserves and to open up territories to mining and agriculture businesses. Anthropologists have warned these actions will result in the genocide of uncontacted tribes. Among the greatest threats is encroachment and environmental destruction by Brazil’s tens of thousands of garimpeiros (artisanal gold miners). Almost all are illegal, but Bolsonaro has expressed far more support for this group than previous state leaders. For him it is partly a personal issue. Bolsonaro’s father was a part-time gold miner, and the president has said he himself panned for gold while serving in the army. “Bolsonaro is a garimpeiro. It explains the way he thinks, always trying to explore more land,” said Davi Kopenawa Yanomami. “He has a sickness in his head. He doesn’t think about others, or about the future.” An author, shaman and environmentalist, Kopenawa is arguably the most prominent intellectual voice of the more than 300 different indigenous groups in Brazil. His book The Falling Sky outlines the very different cosmology of traditional forest peoples and warns that humankind is breaking the forest pillars that hold up the sky – an allusion that stretches beyond the climate crisis. He said that in the past year Yanomami lands (which stretch across Brazil’s border with Venezuela) had been invaded by the biggest wave of illegal miners since the 1980s. “They are poisoning our rivers, killing our fish, and our people are starting to get sick with malaria again,” he told the Guardian. Quietly spoken but defiant, Kopenawa said the problem was greater than Bolsonaro. Although the president had made matters worse, he said, mining companies from Canada, China and Japan were behind the push for resources. “Our politicians are selling our wealth. This brings no benefit to our people, just destruction. Who is getting rich? It’s the foreigners. The big companies are behind this.” The threats are not just to the forest. Raoni has two bodyguards and is a target for attention-seeking nationalists who are trying to ingratiate themselves with Bolsonaro. At a recent gathering of forest defenders in Altamira, a small group of land grabbers and farmers attempted to disrupt proceedings by surging towards the top table, prodding and shouting in the face of a young indigenous woman who was speaking about the killings of her people. Raoni wagged his finger reprovingly, a sign for half a dozen Kayapó warriors to push the intruders back to their seats. The scuffle prompted exaggerated claims on rightwing social media that Raoni had “ordered an attack”. In fact, it was a defence – and a reminder of what has been happening across the Amazon for decades. The jostling is now in the courts. The academic who organised the disruption has filed a criminal accusation against the Kayapó chief. The organisers of the event had already lodged a complaint against the protest organisers for making threats. Raoni said the fracas should not distract from the more important issue of how to save the Amazon. “I was very sad at what happened. The people who want to destroy the forest came to disrupt things. I felt it was important to talk so I asked people to hold them back.” The landowners were much quieter from that moment on. Civil society organisers claimed this as a victory for the majority in Brazil who want to protect the rainforest. They hope to build alliances across the Amazon and throughout the world to counter the threat posed by Bolsonaro and extractive industries. There are signs this may be happening under the leadership of Raoni, Kopenawa and others. Indigenous tribes once fought each other as well as riverine settlers and quilombolas (descendants of runaway slaves who moved into the forest). Today, however, many of these different groups are allied against tree-clearing and river-poisoning intruders. Raoni invited people across the world to join a peaceful resistance against the forces threatening indigenous territory, the Amazon and the world. “They have the money and the guns. We don’t have that. I don’t have that,” he said after the interview. But with temperatures climbing and the forest under increasing threat, he said, it was necessary to act to help Brazil and avert a grimmer future for people around the globe. “Nature is essential for us to breathe,” he said. “I hope people, not just in Brazil, will take my hand and join our forces to save nature, the forest and everything inside it, including the animals and the people.”"
"Sometime around 3600BC, people in the Balkan peninsula reached a major milestone: their mining and metal smelting created enough pollution for us to detect it today. Our research has revealed this was the beginning of the Bronze Age in the region, and the birth of large-scale metallurgy in Europe. To give some context for how early this was: at the same time, the first ever writing was just being developed in Sumer, Mesopotamia, while Britain was still in the Stone Age. Egypt’s first pyramids were still a thousand years in the future. We already knew about these Bronze Age Balkans from patchy archaeological records of axes, adzes and beads. But we are now able to learn more about them thanks to traces of pollution they left behind.  Metal is extracted from its ore through a process known as smelting. This releases microscopic particles of lead into the atmosphere, which are then transported long distances by winds until they settle on the ground.  Peat bogs are ideal repositories for these particles because atmospheric transport is the main pathway by which pollutants can reach these sites. As peat bogs grow in small layered increments each year, they can give us a clear history of the environment in which they grew. When many such chemical analyses of known ages from the many layers of the bog are put together, a sequence of changing pollution can be developed. In our new paper published in PNAS, we present the first such record of changing pollution from south-eastern Europe, reconstructed from the changing concentrations of lead in a peat deposit from western Serbia.  We found evidence of raised lead levels dating back to 3600BC. This is the oldest known environmental metal pollution on Earth, and places the Balkans very much at the forefront of the period of metallurgical discovery and development in the very earliest Bronze Age. Previously the oldest known European environmental pollution happened about 3000BC in southern Spain. Our findings pushes this back by more than 500 years. Indeed, it would take western Europe another 1,000 years to catch up to the same level of metallurgical development.  Lead pollution in the Balkans has continued almost ever since. Those same peat bogs show spikes in the level of pollution during the Late Bronze Age, and unsurprisingly, during the Roman period. At this time, the Balkans were known around the Roman world as one of the main sources of silver used for coins. Since silver is regularly found in ores alongside lead, silver smelting releases lots of lead into the atmosphere.  What is more interesting, is what happened after the Roman Empire fell in the 3rd and 4th centuries CE. In Serbia at least, it appears lead pollution continued, and even increased, indicating that local people continued the strong mining and smelting culture developed by the Romans.  This goes against the long-held view of barbaric hordes with little technological know-how ousting the Romans, leading to the “Dark Ages”, as we term the 1,000 years following the fall of Rome. This may have been true in much of western Europe, but the Balkans were in fact rather well-lit. The culture of metalworking and mining continued into the medieval period in Serbia, as the peat shows almost constant increases in the amount of lead pollution until the 17th century. Periods of pollution reduction often coincide with periods of plague or pestilence, but they are always short-lived, suggesting metallurgy was a key feature of local populations recovering after such periods of strife.  Our data suggests the Balkans played a major role in medieval mining and metallurgy right up until the Ottoman invasions, whereupon steadily increasing taxation and bureaucracy in the region caused many mines to close. This leading role is evidenced by levels of lead pollution in the 17th century comparable with known centres of medieval western mining such as the Black Forest in Germany and the north-west of England. Our work presents an alternative view on how the hugely socio-economically important metallurgical industry developed in Europe. People in the Balkans were clearly pioneers of very early metalwork, and remained at the forefront through the Dark Ages and medieval period."
"
Share this...FacebookTwitterWeather and climate analyst Schneefan here writes that the 2017/18 winter in Europe could be one of the coldest of the last 20 years.
In mid September NOAA’s CFSv2 weather model once again crunched out a cold temperatures across Europe for all three winter months (December (left), January (center), February (right)) for the coming 2017/18 winter:

Meteociel/CFS prognosis dated 1 September 2017 for the temperature deviation from the long-term mean at 850 hPa (approx. 1500 m) in Europe for the 2017/18 winter. Source: http://www.meteociel.fr/modeles/cfsme_cartes.php
Schneefan writes one has to go back to the 1990s to find a negative 2.0°C deviation from the 1961-1990 mean that is projected for Germany. That deviation translates to almost 3°C when compared to the 1981-2010 mean. That would would be awfully cold.
The following chart shows the winter temperature anomalies for Germany for each year since 1901:

If projections come true, Germany would face one of its coldest winters in the last 50 years. Source: http://www.wzforum.de/forum2/read.php?6,3260663,3260663#msg-3260663
The latest CSFv2 model run confirms the earlier cold projections that have been calculated since mid June, 2017.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Cooler than normal autumn
Projections for this fall (September, October, November) are also on the cool side. An analysis from 17 September shows that Central Europe will see temperatures that are about 1°C below the 1981-2010 mean. So far the first three weeks have been right on the money.
Schneefan warns that it’s still too early to rely on the latest trend and to bank on it, but adds: “If these cold projections for the 2017/18 winter keep appearing in the next model runs this fall, then the probability increases.” 

Also Schneefan writes that we should not expect any general warming trend soon after the coming winter, due to the lowest solar activity is 200 years, the cooling La Niña that is beginning to take hold, and the already falling temperatures taking place in the wake of the 2015/15 El-Niño.
There are other signs that change is possibly in the works:
After the ice mass growth in Greenland for the first time in the current century and a new record cold July temperature (-33°C) set in Greenland, no one should be surprised that the 2017/18 winter will be the coldest in Europe and other parts of the northern hemisphere this century.
And to potentially make matters worse, the Bali volcano Agung is now at warning level “orange”. The last eruption was in 1963 with a VEI of 5!. So rapidly could global climate unexpectedly and naturally change.
Readers need to note that the projections involve considerable uncertainty, and the winter of course may develop completely differently. Yet, many meteorologists had projected earlier this year a severe hurricane season this year based on oceanic patterns, and that has come true.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterFirst a note:
If you haven’t already picked up a copy of the The Politically Incorrect Guide to Climate Change, please do get your hands on one.
According to its author Marc Morano, people have been snatching them up and a third printing has started. The book even made the Amazon top 100 best selling books for awhile.
It also ranked first in a number of categories. In his book, NoTricksZone gets mentioned 4 times and even took up one full page at one spot!
So now on to today’s post…
Less heat days near Tokyo
As in Germany, a heat day in Japan is defined as one reaching 30°C or higher.
And according to the manmade CO2 theory, global temperatures are supposed to be rising rapidly and hence we should be seeing many more “heat days” than say 50 or 100 years ago.
Yet Japanese blogger Kirye presents data over Hachijo Island, out to sea east of Tokyo, a location shielded from the urban heat island effect, which tell us that more heat days is not the case at all:

On Hachijo Island, Tokyo, the number of days over 30℃ has not trended since 1926. Source: www.data.jma.go.jp/
Examining the above chart, we see that the number of “heat days” since 2000 is a bit below that of the period from 1940 to 1960. Note the cool 1970 to 1990 period, which likely can be explained by natural oceanic oscillations.
Another chart Kirye provides at Twitter breaks it down in more detail:
 Source: www.data.jma.go.jp/




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Kirye writes, when looking at the past 111 years, the trend in Hachijo Island’s number of days over 30℃ from 1906 to 2017 “denies the anthropogenic global warming hypothesis” and that it does not support the myth of a widespread man-made climate change.
No trend in the city of Tokyo
Also the last 24 years in the city Tokyo show no trend, and even a declining trend since 2010:


Data source: Japanese Meteorological Agency (JMA)
What follows is the month-by-month breakdown, May to October:

Data Source: www.data.jma.go.jp/
Urban heat island
Kirye also presents here a comparison of the Tokyo and Hachijo Island temperature course over the past 111 years. Note urban Tokyo has risen while the island off the coast has risen only very slightly:

Data source: JMA
Urban heat island effect? Kirye notes:

The mean daily maximum temperatures for Tokyo and Hachijo Island differed by more than 2C with the early trend during the period from 1907 to 2015, but temperature difference of close to 0.7C with the trend in the latter period.”




Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
German text edited/translated by P Gosselin)
Satellite measurements of Antarctic sea ice do not go back even 40 years. That’s not very much, especially when we consider that many natural climate cycles have periods of 60 years and more.
Luckily we have the field of climate reconstruction. Using historical documents and sediment cores, the development of ice cover can be estimated. In November, 2016, Tom Edinburg and Jonathan Day examined shipping log books from the time of Antarctic explorers and published on ice extent in The Cryosphere:
Estimating the extent of Antarctic summer sea ice during the Heroic Age of Antarctic Exploration
In stark contrast to the sharp decline in Arctic sea ice, there has been a steady increase in ice extent around Antarctica during the last three decades, especially in the Weddell and Ross seas. In general, climate models do not to capture this trend and a lack of information about sea ice coverage in the pre-satellite period limits our ability to quantify the sensitivity of sea ice to climate change and robustly validate climate models. However, evidence of the presence and nature of sea ice was often recorded during early Antarctic exploration, though these sources have not previously been explored or exploited until now. We have analysed observations of the summer sea ice edge from the ship logbooks of explorers such as Robert Falcon Scott, Ernest Shackleton and their contemporaries during the Heroic Age of Antarctic Exploration (1897–1917), and in this study we compare these to satellite observations from the period 1989–2014, offering insight into the ice conditions of this period, from direct observations, for the first time. This comparison shows that the summer sea ice edge was between 1.0 and 1.7° further north in the Weddell Sea during this period but that ice conditions were surprisingly comparable to the present day in other sectors.”
The surprising result: with respect to sea ice extent 100 years ago things looked similar to what we have today, with the exception of the Weddell Sea. A study by Hobbs et al. 2016 also looked back at the last century, here using geoscientific sea ice reconstructions. Once again the strong discrepancies between the real ice development and model simulations were criticized:
Century-scale perspectives on observed and simulated Southern Ocean sea ice trends from proxy reconstructions
Since 1979 when continuous satellite observations began, Southern Ocean sea ice cover has increased, whilst global coupled climate models simulate a decrease over the same period. It is uncertain whether the observed trends are anthropogenically forced or due to internal variability, or whether the apparent discrepancy between models and observations can be explained by internal variability. The shortness of the satellite record is one source of this uncertainty, and a possible solution is to use proxy reconstructions, which extend the analysis period but at the expense of higher observational uncertainty. In this work, we evaluate the utility for change detection of 20th century Southern Ocean sea ice proxies. We find that there are reliable proxies for the East Antarctic, Amundsen, Bellingshausen and Weddell sectors in late winter, and for the Weddell Sea in late autumn. Models and reconstructions agree that sea ice extent in the East Antarctic, Amundsen and Bellingshausen Seas has decreased since the early 1970s, consistent with an anthropogenic response. However, the decrease is small compared to internal variability, and the change is not robustly detectable. We also find that optimal fingerprinting filters out much of the uncertainty in proxy reconstructions. The Ross Sea is a confounding factor, with a significant increase in sea ice since 1979 that is not captured by climate models; however, existing proxy reconstructions of this region are not yet sufficiently reliable for formal change detection.”
A paper published by Ellen & Abrams 2016 even looked back 300 years ago and showed that the increase in sea ice from 1979-2016 has been part of a long-term growth trend of the 20th century:
Ice core reconstruction of sea ice change in the Amundsen-Ross Seas since 1702 A.D.
Antarctic sea ice has been increasing in recent decades, but with strong regional differences in the expression of sea ice change. Declining sea ice in the Bellingshausen Sea since 1979 (the satellite era) has been linked to the observed warming on the Antarctic Peninsula, while the Ross Sea sector has seen a marked increase in sea ice during this period. Here we present a 308 year record of methansulphonic acid from coastal West Antarctica, representing sea ice conditions in the Amundsen-Ross Sea. We demonstrate that the recent increase in sea ice in this region is part of a longer trend, with an estimated ~1° northward expansion in winter sea ice extent (SIE) during the twentieth century and a total expansion of ~1.3° since 1702. The greatest reconstructed SIE occurred during the mid-1990s, with five of the past 30 years considered exceptional in the context of the past three centuries.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA documentary dubbed “The Weather Machine” produced in 1975 – long before NASA fiddled with the data – warned of an impending ice age (10:35), and maintained that the globe is cooling. Hat-tip: reader The Indomitable Snowman.
The documentary attempted and succeeded at presenting the latest on climate change at the time.
Changing climate accepted as normal
It is true that back in 1975 climatologists already knew that the climate behaved cyclically, as evidenced by the ice cores and tree ring sets extracted from the American Southwest.
Climate change back then was known to be a normal, natural phenomenon. Moreover, after 3 decades of temperature decline, scientists indeed were concerned that the globe was cooling at a worrisome rate.

Part 1: Weather Machine. Exiled Czech climate scientist Dr. George Kukla said in the 1970s: “The ice age is now due any time.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also, contrary to what was suggested by Michael Mann”s notorious hockey stick chart, the little Ice Age did in fact exist and was clearly evidenced by old historical records from ships, The Weather Machine documentary tells us.
And note that the documentary stated that the Jet Stream also changed its course naturally, just as it does today, and that there was much more to it than just Arctic sea ice extent, on which today some scientists are trying to blame for the frigid winter we are now witnessing.
In Part 2, viewers are told how the ocean cycles have a major impact on the weather cycles, something today that is ridiculously being blamed on trace gas CO2 from human activity.
Little Ice Age warnings…
At the 6 minute mark of Part 2 again we are warned of cooling and the potential of a little ice age, or worse.

Prof. George Denton, University of Maine at Orono, warned we could easily return to Little Ice Age conditions.
Humans may be causing cooling
Later into Part 2 Dr. Reid Bryson of the University of Wisconsin claims that man’s activity may be contributing to the cooling through the “Human Volcano” spewing aerosols into the atmosphere that “blots out” the sun.
Share this...FacebookTwitter "
"At the outset of the 2020s, the UK economy embarks on a new decade with little momentum. Growth has stalled, not least because Brexit uncertainty and a slowdown in the global economy has served as a handbrake on business investment. Meanwhile, consumers have begun tightening their belts and the job market boom of the past decade has petered out. For at least the duration of 2020 another groundhog year awaits thanks to the ongoing Brexit saga, despite the promise Boris Johnson made before the election. But as the economist Mat Lawrence has argued, Brexit is just the firing gun on a wider decade of disruption. The prime minister will not be able to declare the job done and his mission accomplished after the UK’s formal exit later this month. Instead it will mark the first steps towards redefining the nation’s place in a rapidly-changing world. In a prescient report for the Institute for Public Policy Research three years ago, Lawrence outlined five powerful trends that will drive change in the 2020s: The aftershock of Brexit, a demographic tipping point from an ageing society, the rise of new technologies, a shifting global economic order and the existential threat of global heating. These trends aside, much about the coming decade will be tough to predict from today’s vantage point. Johnson says his 10-year plan for Britain will usher in a “new golden age”. Yet, some on the left think Labour could remove him from office in half the time. Donald Trump could be deposed this year. Rising tension in the Middle East could spark a fresh Gulf war, sending oil prices soaring.. One hope in the wake of the election is that the decade ahead will not lead to another 10 years of crippling austerity. The British state is set to grow, with spending as a share of national income due to rise under Johnson’s Tories to 1970s levels according to the Resolution Foundation. This shift is vital to begin the lengthy job of fixing the battered public realm. But the 2020s will still be largely defined by the cost-cutting of the past given the extent of the harm it caused. Britain has three primary economic problems to address from the outset, all arguably made far worse by the spending constraint of the 2010s. First will be to reboot the flatlining productivity growth of the 2010s. Second is the task of rebalancing a fractured nation, by addressing inequalities of wealth, gender, race, education, geography and opportunity. Third will be to decarbonise the UK’s economic model, to prevent the climate emergency from turning into catastrophe. Johnson will need to address each point while also taking on the complex task of redrawing Britain’s trading relationships with the EU, as well as striking new trade deals with other countries. All this comes as the economic world order of the past century faces substantial change that will shape the coming decade for Britain. Asia’s GDP is expected to overtake that of the rest of the world this year, while China could replace the US as the planet’s largest economy by the end of the decade. Asia at large is expected to contribute to roughly 60% of global economic growth by 2030, and should be home to almost all the world’s 2.4 billion new members of the middle class. Britain will fall down the international league table, according to the City bank Standard Chartered, out of the top 10 to rank behind nations such as India and Indonesia by 2030. The emergence of Asia as an economic powerhouse has so far been against a backdrop of ever increasing globalisation. But the 2010s probably marked the high point for the economic integration of recent decades. Trade policy uncertainty reached unprecedented levels in 2019. Phase one of a resolution in the US-China trade dispute that provided much of that uncertainty is expected next week, but economists expect the world to increasingly fragment over the next decade. At the same time as our economies look set to slip further apart, global heating and a new mass extinction mean greater political cooperation will be required to tackle the climate emergency. Amid these competing forces, there are warnings from the World Bank that doing nothing could mean climate impacts pushing an additional 100 million people into poverty by 2030. The poorest regions of the world – Sub-Saharan Africa and south Asia – are expected to be hit the hardest, prompting a migration crisis as people flee increasingly extreme weather events. To respond to the threat, the decade ahead will be pivotal. World leaders have targeted the 2020s as a “decade of delivery” to reach the UN’s sustainable development goals – 17 targets to eliminate extreme poverty, cut inequalities and combat the climate emergency – adopted by the UN general assembly to hit by 2030. Progress is being made, but much more needs to be done. A major risk will be that countries get distracted from the task by another global recession. Given rising geopolitical tensions, and the simple fact that national recessions tend to take place roughly ever ten years, the chances are rising. For the US, the current expansion is the longest on record, while in the UK the 2010s marked the first decade since reliable records began – in the 1700s - in which recession was completely avoided. Either way economists expect growth over the 2020s in wealthy nations to be much slower than average in the decades before the 2008 financial crisis amid an ongoing demographic shift. Working-age populations are expected to shrink, including in the UK, where the number of over 85-year-olds is projected to almost double in the next 25 years – raising questions for growth and the funding of public services. There are hopes that technological advances could address both the demographic shift and the climate emergency. But like a common thread running through these issues facing Britain in the 2020s, greater government coordination and funding will be required to realise the ambition. Central banks head into the 2020s with interest rates close to, at, or below zero, meaning that the effectiveness of tweaking interest rates to stimulate demand is effectively neutered. Andrew Bailey will have an important role to play to continue the work of Mark Carney at the Bank of England, becoming governor for eight years of the 2020s starting in March, but low inflation and lack of room for manoeuvre mean the government must take the primary role. Voters are increasingly fed up with kick-the-can policymaking. At the outset of the 2020s, Britain and the world are entering a disruption decade. • Sign up to the daily Business Today email here or follow Guardian Business on Twitter at @BusinessDesk."
nan
"Inside Digital Realty’s Dublin data centre, racks of shiny black servers throb and whirr as unseen fans cool machines that steadily process unending data. It operates 24 hours a day from the business park, sited on a former orchard, and the data joins a digital torrent in an underground fibre ring network that sweeps around the Irish capital and connects to undersea cables – the physical backbones of the digital world.  It is not just for Ireland. This is also how the UK and continental Europe accesses a lot of email, social media, online shopping, Netflix and other internet services. “Everything with the word smart in front of it has a data centre behind it,” said Ben Bryan, Digital Realty’s technical operations manager in Dublin. But there is a catch. The surge in Irish data processing will require significant new energy infrastructure and increase emissions, complicating Ireland’s response to the climate crisis. The cloud can create carbon: it is estimated that when the music video Despacito reached 5bn streamed YouTube views in 2018, the energy consumption was equivalent to powering 40,000 US homes a year (it has now exceeded 6.5bn views). By 2028 data centres and other large users will consume 29% of Ireland’s electricity, according to EirGrid, Ireland’s state-owned transmission system operator. Worldwide data centres consume about 2% of electricity, a figure set to reach 8% by 2030. Few countries, if any, will match Ireland’s level. It is already Europe’s data centre capital, with Amazon, Google and Microsoft siting operations there. Dozens of centres have opened in recent years, bringing the total to 54, with a combined power capacity of 642MW. Once a leading exporter of floppy discs and CD-Roms, Ireland has successfully transitioned to the big data era. It is just the beginning. With the state’s blessing another 10 centres are under construction, including a €1bn (£845m) Amazon hub in Mulhuddart, west Dublin, that together will add 202MW. Another 31 centres have planning permission, which would add 629MW. There has been one setback: planning approval delays prompted Apple to scrap a planned €850m (£743m) centre last year. “The data centre industry is growing so fast it’s hard to fathom,” said Patrick Bresnihan, a geography professor at Maynooth University in Ireland. “But somehow the tech companies get far less attention than aviation or fossil fuel companies.” Ireland faces a dilemma. The expanding web of data centres is part of a strategy to anchor tech companies that drive economic growth. They have been designated as “critical infrastructure”, facilitating planning approval. “Data centre presence in Ireland raises our visibility internationally as a technology-rich, innovative economy,” said a spokesperson for the Department of Business, Enterprise and Innovation (DoBEI). But the boom will exact a price. Ireland is one of the EU’s worst carbon emission offenders and faces fines of more than €250m for missing 2020 targets on reducing greenhouse gas emissions. Missing later targets will trigger steeper fines. A report by the Irish Academy of Engineering (IAE) has estimated data centre expansion will require almost €9bn in new energy infrastructure and add at least 1.5m tonnes to Ireland’s carbon emissions by 2030 – up 13% spike on current electricity sector emissions. Ireland’s data centres have a low profile. They tend to operate from anonymous-looking business parks with high-security perimeter fences and intruder detection alarms. A Google centre has colourful murals but the rest are grey and nondescript, with discreet signage. Digital Realty, whose headquarters is in San Francisco, has a relatively small centre in Profile Park, part of a cluster in west Dublin. Its two data halls are powered by 9MW, a fraction of some neighbours but enough to require an electricity substation. It powers the servers and fans which during winter suck in cool air from outside. If the power – and cooling – stopped the machines would swiftly overheat, said Bryan. That has never happened but just in case there are backup diesel generators on the roof. The electricity came from 100% renewable sources, said Valerie Walsh, a company vice-president. “There is a huge amount of thought to make sure we’re sustainable and do the right thing.” Google and Amazon representatives also said their Irish data centres were energy efficient and entirely supplied by – or soon would be - renewable energy. Earlier this year Amazon Web Services announced backing for a 91.2MW windfarm in Donegal and a 23.2MW windfarm in Cork. Such deals could act as a catalyst for renewable energy investment, said the DoBEI, citing a 2018 government report. Asked about the estimated 1.5m tonnes of carbon emissions, it replied: “The department is not in a position to comment on the accuracy or otherwise of the estimates of the IAE study, which it is not a party to.” Bresnihan, the academic, said Ireland’s dependence on big tech companies should not obscure their environmental cost. “If they left Ireland would be in a pretty bad situation but there’s only so long you can put off these contradictions.”"
"
Share this...FacebookTwitterBefore getting to the subject of climate models, first two small points worth bringing up:
Eco-Trumpism spreading
Firstly, it appears that Trump’s policies are sending powerful political impulses worldwide. For example ultra-alarmist German climate and energy site klimaretter here bemoans that leading socialist Sigmar Gabriel seems to be turning into an “Eco-Trump”. Gabriel actually had the audacity to remind Germany that economics need have as great as or greater priority than climate change does, something causing a bit of political indigestion at klimaretter.
Fears of German companies moving to USA
Secondly, German business daily Handelsblatt here cites a study that tells us Germany will likely see jobs lost due to Trump’s tax reforms. It is feared that a number of German companies may opt to flock over to USA to take advantage of lower taxes, cheaper energy and less stringent regulation. Germany helping MAGA!
===================================
Climate models totally fail in practice: Can atmospheric circulation be simulated at all?
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated / edited by P Gosselin)
A large part of international climate policy is based on prognoses delivered by climate models. Here the key players act as if they are highly robust and thus serve as a good basis for policy decision making. But what hardly ever makes it through the media filter is the rather hectic discussion taking place behind the scenes among climate modelers.
In September 2014 Theodore Shepherd of the University of Reading summarize the entire extent of the problems in an article published in Nature Geoscience. The models simply fail to grasp the atmospheric circulation. And Shepard feels that will remain the case also in the future:
Atmospheric circulation as a source of uncertainty in climate change projections
The evidence for anthropogenic climate change continues to strengthen, and concerns about severe weather events are increasing. As a result, scientific interest is rapidly shifting from detection and attribution of global climate change to prediction of its impacts at the regional scale. However, nearly everything we have any confidence in when it comes to climate change is related to global patterns of surface temperature, which are primarily controlled by thermodynamics. In contrast, we have much less confidence in atmospheric circulation aspects of climate change, which are primarily controlled by dynamics and exert a strong control on regional climate. Model projections of circulation-related fields, including precipitation, show a wide range of possible outcomes, even on centennial timescales. Sources of uncertainty include low-frequency chaotic variability and the sensitivity to model error of the circulation response to climate forcing. As the circulation response to external forcing appears to project strongly onto existing patterns of variability, knowledge of errors in the dynamics of variability may provide some constraints on model projections. Nevertheless, higher scientific confidence in circulation-related aspects of climate change will be difficult to obtain. For effective decision-making, it is necessary to move to a more explicitly probabilistic, risk-based approach.”
Also accounting for solar irradiance is causing a lot of problems, as Zhou et al. 2015 point out:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the incident solar radiation in CMIP5 models
Annual incident solar radiation at the top of atmosphere should be independent of longitudes. However, in many Coupled Model Intercomparison Project phase 5 (CMIP5) models, we find that the incident radiation exhibited zonal oscillations, with up to 30 W/m2 of spurious variations. This feature can affect the interpretation of regional climate and diurnal variation of CMIP5 results. This oscillation is also found in the Community Earth System Model. We show that this feature is caused by temporal sampling errors in the calculation of the solar zenith angle. The sampling error can cause zonal oscillations of surface clear-sky net shortwave radiation of about 3 W/m2 when an hourly radiation time step is used and 24 W/m2 when a 3 h radiation time step is used.”
Currently the author teams for the planned 6 IPCC climate report are getting together. Are the considerable problems surrounding climate models resolved? No sign of that. On October 11, 2017, Stony Brook University set off the alarms: The models still are not running properly! And the German press prefers to keep silent about this. The Stony Brook press release follows:
Study Reveals Need for Better Modeling of Weather Systems for Climate Prediction
Computer-generated models are essential for or scientists to predict the nature and magnitude of weather systems, including their changes and patterns. Using 19 climate models, a team of researchers led by Professor Minghua Zhang of the School of Marine and Atmospheric Sciences at Stony Brook University, discovered persistent dry and warm biases of simulated climate over the region of the Southern Great Plain in the central U.S. that was caused by poor modeling of atmospheric convective systems – the vertical transport of heat and moisture in the atmosphere. Their findings, to be published in Nature Communications, call for better calculations in global climate models.
The climate models analyzed in the paper “Causes of model dry and warm bias over central U.S. and impact on climate projections,” included a precipitation deficit that is associated with widespread failure of the models in capturing actual strong rainfall events in summer over the region. By correcting for the biases, the authors found that future changes of precipitation over the US Southern Great Plain by the end of the 21st Century would be nearly neutral. This projection is unlike what has been predicted as a drying period by the majority of current climate models. The correction also reduces the projected warming of the region by 20 percent relative to projections of previous climate models.
“Current climate models are limited by available computing powers even when cutting-edge supercomputers are used,” said Professor Zhang. “As a result, some atmospheric circulations systems cannot be resolved by these models, and this clearly impacts the accuracy of climate change predictions as shown in our study.” Professor Zhang and colleagues believe climate models will become more accurate in the coming years with the use of exsascale supercomputing, now in development worldwide.”
Already in 2014 Mauri et al complained of enormous discrepancies between the real and simulated developments for precipitation and temperature in Europe 5000 years ago. Modelling of the past, i.e. the calibration, didn’t work at all. With so much disappointment one has to ask where all the confidence surrounding models being reliable forecasters comes from.
The paper’s abstract follows:
The influence of atmospheric circulation on the mid-Holocene climate of Europe: a data–model comparison
The atmospheric circulation is a key area of uncertainty in climate model simulations of future climate change, especially in mid-latitude regions such as Europe where atmospheric dynamics have a significant role in climate variability. It has been proposed that the mid-Holocene was characterized in Europe by a stronger westerly circulation in winter comparable with a more positive AO/NAO, and a weaker westerly circulation in summer caused by anti-cyclonic blocking near Scandinavia. Model simulations indicate at best only a weakly positive AO/NAO, whilst changes in summer atmospheric circulation have not been widely investigated. Here we use a new pollen-based reconstruction of European mid-Holocene climate to investigate the role of atmospheric circulation in explaining the spatial pattern of seasonal temperature and precipitation anomalies. We find that the footprint of the anomalies is entirely consistent with those from modern analogue atmospheric circulation patterns associated with a strong westerly circulation in winter (positive AO/NAO) and a weak westerly circulation in summer associated with anti-cyclonic blocking (positive SCAND). We find little agreement between the reconstructed anomalies and those from 14 GCMs that performed mid-Holocene experiments as part of the PMIP3/CMIP5 project, which show a much greater sensitivity to top-of-the-atmosphere changes in solar insolation. Our findings are consistent with data–model comparisons on contemporary timescales that indicate that models underestimate the role of atmospheric circulation in recent climate change, whilst also highlighting the importance of atmospheric dynamics in explaining interglacial warming.”
 
Share this...FacebookTwitter "
"The restoration of natural ecosystems – “rewilding” – ought to be a chance to create inspiring new habitats. However the movement around it risks becoming trapped by its own reverence of the past; an overly nostalgic position that makes rewilding less realistic and harder to achieve. The recent launch of Rewilding Britain is certainly exciting and timely. However George Monbiot’s vision of bringing back 15 iconic species falls short of the rewilding visions being discussed in universities.  These are emerging from advances in functional ecology and Earth system science. The vision of rewilding is more ambitious: it is about restoring ecological processes through reassembling the species that drive them. For example rooting by wild boars has repercussions throughout a woodland ecosystem. Such animals shouldn’t be reintroduced simply because they were once there, but because they could do something productive in future. Monbiot’s quest to restore “lost” species harks back to a past age. However many conservation scientists are more relaxed concerning the question of “nativenes”. They are willing to consider introducing non-native species if they contribute a  functional role in ecosystems, and they view the past not as a benchmark to preserve or replicate but as an inspiration for ecosystem restoration.  For instance, “Monbiot’s 15” omits the auroch  and tarpan which are classed as extinct. However in the 1980s progressive Dutch ecologists realised that their functional analogues survived as cattle and ponies and their ecological role could be restored through “de-domestication”. They set about de-domesticating them at the famous Oostvaardersplassen reserve located a 40 minute drive from Amsterdam. This produced a “Serengeti-like” landscape: a type of nature unknown to Europe since humans settled down and started farming.  The OVP, as it is known, made nature conservation political again and has become a landmark public experiment in ecology. I first visited it with a group of students in 2003 when we travelled to the Netherlands to meet the radical ecologist Frans Vera and engage with the controversies created by rewilding.   The OVP is created on reclaimed land and opponents argued that the fences and flood control created an artifical landscape that undermined any claims to its authenticity as a restored ecosystem. More seriously the policy of allowing the cattle and ponies to die of “natural” starvation enraged animal welfare and farmer groups who believed they should be subjected to the same welfare standards applied to animals in labs, farms and zoos. The controversies surrounding the experiment, Vera’s hypothesis that Europe’s original vegetation was wood-pasture rather than high-forest, and other radical rewilding visions are inspiring a re-examination of the fundamental premise of nature conservation. I recently published a Rewilding agenda for Europe in the journal Ecography, as my contribution to the European Council’s “fitness check” of its nature legislation. The Birds and Habitats directives under review derive from the science and policy context of the 1970s. They are ageing. Both science and society have moved on.  Any revisions to European nature legislation should support the creation of experimental rewilding sites. Across the UK we could imagine the creation of wild cattle and pony step-lands on the Ridgeway, wild boar and deer-driven woodland ecosystems in Wales, and a Scottish arcadia of bison, moose, wolves and pine forest.  We also need many more OVP-like public rewilding experiments close to urban areas. These would be contained sites that inspire and inform the public about scientific advances, and provoke us all to ask: what sort of nature do we want for the future? Rewilding might offer fresh solutions to intractable conservation problems. For example, conservationists want to remove pine trees introduced to the Sefton Coast dune system near Liverpool but local residents love them for their scenic grandeur and red squirrels.  The famous Formby footprints dating from 2,500 BC show that humans, wild cattle, deer and wolf once inhabited these coastal areas. Suggesting the reintroducing of wild cattle and companion herbivores and seeing what happens might prompt a unified vision for the dunes. In practice rewilding is constrained by regulations on biohazards, public access and animal husbandry – and rigid and powerful 20th century conservation legislation and agencies which have no real incentive to innovate. Conservation institutions need to modernise but no one wants to dismantle them and start over. We need designated spaces with regulatory flexibility – experimental rewilding sites – where we can plan future natures that will improve the quality of life for people and the planet. Ordinary people are disenfranchised. Conservation policy is influenced by a coordinated lobby of a few big charities who have built their organisational models on the institutional structures of the late 20th century.  George Monbiot’s vision catches the attention but advocates of rewilding need to develop realistic policy mechanisms to take their ideas forward. Rewilding experiments would give space for wider reflection and debate and give our conservation institutions time to adapt. Crucially they would reinvigorate conservation as a cultural force in the 21st century."
"It’s hard to be optimistic about British homes in the future. The lack of accommodation and the “broken” housing market are perpetually in the news. Millennials, even middle earners, are unlikely to own their own home.  The country’s new-builds are the smallest in Europe, with families “so cramped there isn’t enough space for them to live comfortably, sit down and eat together or even store necessities such as a vacuum cleaner”. As for the ageing population, the UK is said to be 
“woefully underprepared” for offering them appropriate housing and care.  These woes are variously blamed on “greedy developers” or Margaret Thatcher’s Right to Buy policy for council houses. Many commentators believe we need to build around 240,000 homes a year to solve the problem, while praising government schemes to increase ownership, such as the Help to Buy policy.  I want to argue for a different approach to making housing more viable. It focuses on a major cultural shift that has contributed to the housing problem, but too often gets overlooked.  Nearly one-third of the population now live on their own. Key demographic changes, such as young people deferring marriage and children until later in life and older women outliving their spouses and living on their own, has resulted in the average UK household size falling from almost three per household in 1970 to 2.4 for the past decade. This is part of why the number of households is rising, currently at about 1% a year.  UK household composition 1961-2011 Accompanying these shifts have been changes in people’s expectations. Householders nowadays consider a spare bedroom a necessity, while children are less likely to share a room with siblings than was once the case. Despite all the talk of “rabbit hutch homes”, domestic space per person is actually increasing. With this in mind, here are a couple of alternative responses to the housing crisis that begin to make sense:  The 1850 British census defined the family as “the wife, children, servants, relatives, visitors, and persons constantly or accidentally in the house”. This is an interesting reminder that past home life was much more communal than today.  The UK government already incentivises renting to lodgers through its Rent a Room scheme, offering homeowners up to £7,500 tax-free income per year if they let space. Taking a more creative approach to our perception of family and who we are willing to live with could make a big difference to the housing crisis. It would also be good for our wallets, not to mention wellbeing, helping people with loneliness and filling empty nests.   While the tiny house movement is becoming known for freeing people from mortgages and giving them more time to do what they love, this does not mean everyone needs to move into a 25 square metre home. Just downsizing from a large family house once you reach a certain age would help address our “ticking household bomb”.  The trouble is that people’s willingness to move house declines drastically after the age of 45 as they become more attached to their homes and wider community. Older householders often only move when they are forced by factors such as injury, illness or the death of their partner.  To combat this, organisations such as this one in the US, where there is a similar debate taking place, are already working to put a positive spin on downsizing and to market “downsizer homes” to people of a certain age. If we want to do something about the housing crisis, putting more emphasis on the upside of downsizing cannot be understated.  My research compares experiences of living in different sizes of house and household, and what motivates them. People expect more space for different reasons. Sometimes it makes it easier to enjoy living with your family members: more bathrooms per household reduces the potential for conflicts over who’s next in the shower, for instance. Or it may be because people have become accustomed to having a bedroom or study where they can do what they want and retreat from the company of others. We need to play up the counterarguments: why is bigger always better, for example, when we know it often locks us into unaffordable mortgages and more housework and gardening?  We also need to encourage people to accommodate these desires in different ways. Soundproofing walls can create a better sense of privacy than having more rooms. Sofa beds can create temporary guest bedrooms that need not be empty for the majority of the year.  The answer to the housing crisis is not to build vast numbers of new homes and help people to own more space than they need. Instead, we need to make do with less and learn to appreciate it."
"The plan to ban the growing of genetically modified crops is disappointing to many scientists. It would be highly unsatisfactory if, as it appears, such an important decision has been made by the Scottish government without a proper informed debate that takes the scientific evidence fully into consideration. It is not enough for the rural affairs secretary, Richard Lochhead, to say that he is not prepared to “gamble” with the future of Scotland’s £14bn food and drink sector.  What we are talking about is simply biological technology with potentially wide and varied applications. Our work at the University of Stirling’s Institute of Aquaculture is a case in point. We have been testing and assessing oils from genetically modified (GM) oilseed crops developed to provide sustainable sources of long-chain omega-3 fatty acids. These nutrients are recommended as part of a healthy diet because they can protect against cardiovascular diseases and promote heart health.  Marine microalgae make most of the world’s omega-3, allowing it to work its way up the marine food chain as they are consumed. As a result, it can only be obtained in any significant amount from fish and seafood. This is why oily fish such as Atlantic salmon are among the best sources of the nutrient.   When it comes to farmed fish, the omega-3 has to be included in their diets, both for the good of their own health and to ensure that they have the high levels required to pass on to the consumer. This means that the feeds must mimic their wild cousins’ natural diet – hence the historic use of fishmeal and fish oil in “traditional” feeds. These tend to be imported at present, particularly from the west coast of south America, from Peru and Chile.  Unfortunately there is insufficient omega-3 of the type required available in the world to satisfy human dietary requirements. As fishmeal and especially fish oil supplies are finite and limited, they are being spread thinner in feeds, and the levels of omega-3 in farmed fish are declining. Without new sources of omega-3, the absolute levels of the nutrient will fall below those of wild fish.   The oils that we are developing from GM oilseed crops – in collaboration with crop scientists led by Professor Johnathan Napier at Rothamsted Research – offer a new and sustainable source of omega-3 that can be used to replace the wild fish oil. Having proven the concept, we are now seeking funding for commercial-scale trials. With a fair wind, the work will foreseeably be ready for full-scale commercialisation in the next two or three years.  The project addresses not only an important aspect of population health but also issues of environmental impact, sustainability and food security. When you consider that Scotland has a high death rate from heart disease – one third of all deaths – it is ironic that that we are also a nation producing many thousands of tonnes of farmed salmon that can be a rich source of the beneficial omega-3 fatty acids.  Yet the Scottish government would not permit these GM crops to be grown in the very country where the oils the crops produce can be applied most effectively. Assuming our work reaches the market, this would mean that Scotland would lose the financial benefits from growing the oilseed. Neither is it environmentally sound to grow crops elsewhere and ship the oils around the world when they could be grown locally. These extra costs could undermine the sustainability of the aquaculture industry in Scotland, one of the key segments of the country’s food and drink sector. This is of direct relevance to the health and welfare of its people, not to mention consumers of Scottish farmed salmon all over the world.  Obviously this is not to suggest that omega-3 or GM are panaceas for all our ills. Our research simply highlights one application of GM technology to solve a critical problem, and the context within which it was developed. But while few would disagree that Scotland has a beautiful natural environment or that seeking to protect it is a good policy, what exactly are the risks that growing GM crops actually pose? The Scottish government’s announcement is rather unclear when it comes to this question.  In September 2014, Scotland showed the world how to have a truly public and inclusive debate on a subject of massive national and international importance, make a decision based on that debate, and then accept and live with that decision. If the true lesson of that was not to have a debate that you think you might lose, the Scottish government appears to have learned it all too well."
"Most people enjoy the warmer, longer days that summer months bring – but plant allergy sufferers will have mixed emotions. Roughly one in five Europeans suffers from allergic reactions to tree, grass and weed pollen causing pollinosis, hay fever and allergic asthma. Allergies to substances such as pollen are driven by errors in the body’s immune system, which means it mounts a response to otherwise benign substances from plants. On first exposure to pollen, the body decides if some of the otherwise harmless proteins in the pollen are dangerous. If it decides they are, the immune system produces immunoglobin E (IgE) antibodies in a process called sensitisation.  The next time the body is exposed to pollen, it remembers the proteins and mounts another response. The IgE antibodies detect the pollen in, or on, the body, and cause cells to release histamine and a variety of other chemicals. This results in symptoms ranging from itchy eyes and nose, to production of mucous, inflammation and sneezing fits.  But while we know that “pollen” causes this response, at present we still don’t know all the types of pollen that cause the body to react.  In the UK, a daily pollen forecast is generated by the UK Met Office in collaboration with the National Pollen and Aerobiology Unit (NPARU), to help allergy sufferers. This forecast is created using data from a network of pollen traps which operate throughout the main pollen season (March to September) and measure how many pollen grains are present on a daily basis.  Pollen from different types of tree can be identified using microscopes, but grass pollen grains all look the same. As a result the pollen forecast for grasses (of which there are 150 types in the UK alone) is based on the broad, undifferentiated category of “grass”. That is despite grass pollen being the single most important outdoor aeroallergen.  We already know that different species of grass pollinate at different times in the year, and allergic reactions can occur at different times throughout the allergy season. What we need to figure out is whether allergies are caused by all species, specific species, or a combination of species of grasses. We also need to learn how pollen grains change in composition in time and space. While pollen is known for being very tough and is often well preserved in sediments, it can be very fragile in certain circumstances, such as bursting when in contact with rain drops.  To find out which grasses are linked to the allergic response, we need to know many things, such as where and when species of grass are releasing pollen. We also need to uncover how the pollen moves through the atmosphere, quantify the exposure of grass pollen species in time and space, and work out how allergies develop across broad geographical and temporal scales. Our Natural Environment Research Council (NERC) PollerGEN project team is now working on a way to detect airborne pollen from different species of allergenic grass. We’re also developing new pollen source maps, and modelling how pollen grains likely move across landscapes, as well as identifying which species are linked with the exacerbation of asthma and hay fever. We’re going to be using a new UK plant DNA barcode library, as well as environmental genomic technologies to identify complex mixtures of tree and grass pollens from a molecular genetic perspective. By combining this information with detailed source maps and aerobiological modelling, we hope to redefine how pollen forecasts are measured and reported in the future. We have just started the third year of pollen collection and hope to road test the combined forecasting methods over the next year. In the long run, our vision is to be able to provide specific pollen forecasts for grass, and unravel which species of grass pollen are most likely causing allergic responses. More broadly, we also want to provide information to healthcare professionals and charities, who can translate this information to help pollen allergy sufferers live healthier and more productive lives. In the meantime, if you suffer from pollen allergies, sneeze or wheeze during spring, speak to a doctor or pharmacist to prepare an action plan. You can also get support from Allergy UK, and information about the pollen forecast from the UK Met Office."
"Earlier this year, Trudi Beck, a general practitioner from Wagga Wagga, wrote to councillors across New South Wales urging them to acknowledge the climate crisis and declare a local emergency. Some responses were positive. Others less so.  Mark Hall, a Lachlan shire councillor and Baptist pastor, told Beck: “Stick to medicine – you have utterly no clue about climate science. Your email intrusion is truly not welcome.” So far, 84 jurisdictions in Australia covering about a quarter of the population – mostly cities and local government areas – have declared a climate emergency. The first elected body in the world to act, Darebin council in Victoria, is credited with starting a movement that is now supported by governments representing 800 million people worldwide, including the European Union and Bangladesh. In Australia, as ever when it comes to climate policy, the process has been polarising and frustrating. The leaders of one town might have recognised the climate crisis and committed to developing adaptation measures to help the community deal with the impacts of global heating. The next town over might have decided that climate change has nothing to do with local government business such as carting rubbish or fixing potholes. “We went from talking about the climate emergency, to now all of a sudden we’re living in it,” says Sarah Mollard, a general practitioner from the coastal NSW town of Port Macquarie. “It was incredibly unsettling to experience the sky going from blue to red in the space of a few hours. It’s extraordinarily unsettling to be in your home and see smoke haze in your home. This is my home, this is my safe space, and I can’t keep my children safe in it.” A few months ago, Mollard and other community members began to lobby for the Port Macquarie council to declare a climate emergency. In September, a relatively benign council motion to develop a “climate change action plan” was deadlocked at four-all. The mayor’s casting vote shelved the idea indefinitely. Since the vote, and since the November bushfire crisis that blanketed Port Macquarie in an orange haze, community members have turned up to council meetings, where residents are allowed to take the floor before formal debates, to discuss the climate change impacts of relevant items of business. In November, Mollard spoke about the need for the council to develop a heat plan. “It’s constructive in a sense. At the moment the council does not have someone on their payroll who is looking at the actions of council through a climate lens,” Mollard says. “I prefer gardening to public speaking, and would rather spend my day off work with the kids at the beach than rallying for our government to simply do its job. “As a doctor I am familiar with the term emergency. An emergency is a threat to people, property or society that has the potential to overwhelm them. “An emergency requires action to stop the problem from getting out of control and then return to safety. In an emergency, timing is critical – if you wait to act, the problem gets worse, more damage is done, the cost of repair is increased. “The more involved I’ve been getting the more I’ve had people coming up to me in the street and saying thank you. That’s a really strong indicator that people feel strongly about an issue.” One of the most remarkable aspects of the climate emergency movement is how it has put debate on the agenda in places that might have otherwise buried their heads in the nearest sandy riverbed. The Glen Innes Severn council has made a declaration and the mayor, Carol Sparks, has emerged from the bushfire crisis as a credible voice for regional people demanding climate action. Newcastle, the home of the world’s largest coal export port, has declared an emergency and has a policy to work towards a just transition. The Wollongong City c-ouncil – which along with Newcastle was for decades an industrial and steelmaking hub – has also recognised the climate crisis. In Queensland, where climate politics is most fraught amid a rush to support coal exports, only the Noosa council has declared an emergency. It also set a zero net emissions target by 2026. “I see it as both symbolic but also practical for sure,” Noosa mayor Tony Wellington says. “Of course, when we declared a climate emergency I did receive some hate mail. Let’s just say I did expect that. There wasn’t a large amount of pushback but there are inevitably in our community a number of people who [don’t accept climate change]. “Noosa has a history of being somewhat adventurous and pioneering as a council. We’re also for example the only council that has joined the alliance for gambling reform. We take a rather intrinsic view to development per se. We have a proud history of environmental conservation.” Two communities in the area, Noosa North Shore and Peregian Beach, were evacuated under threat from bushfires earlier this year. Wellington says the incidents were a “wake-up call” for the need to adapt the council’s plans and operations for a changed climate. “We’re acutely aware the impacts of climate change will resonate,” he says. “The costs of not preparing are far greater than doing something now.” Conservative Wagga Wagga, home of the deputy prime minister, Michael McCormack, earlier this year declared a climate emergency. A few weeks later, after an increasingly nasty debate, councillors rescinded that declaration. Outraged councillors would later demand the mayor, Greg Conkey, drive an electric vehicle to Sydney and back. He did and has said the journey was a success. Beck had been instrumental in building local support in Wagga Wagga, and in July, while the city was locked in debate about the declaration, she contacted other council areas soliciting support. “No way!” replied the Tenterfield deputy mayor, Don Forbes. When Beck responded by referring to the water security issues facing the Tenterfield community, which have got worse in the months since, Forbes asked not to receive further emails. When the same email reached Wollondilly shire councillor Simon Landow, a former candidate for Liberal preselection, he replied to say it was not the council’s role. “The term ‘climate emergancy’ (sic) … is very misleading to my residents of Wollondilly,” Landow wrote. “I respect your view to have an opinion on the theory that man is causing catastrophic global warming. “I would like you to respect my view that there is none, and I won’t be deviating from a stance thats is filled with so may (sic) flaws and misconceptions.” Another councillor, Murray Thomas from the Bland Shire, said in response that climatic changes had no relationship to carbon dioxide and would soon be proved to be caused naturally. “How do you propose explaining that [to] hordes of angry rorted,” Thomas said. “You’ve obviously made your choice, suggest you reconsider while you have the opportunity. Just tell ’em you were experimenting with psychotic drug samples … or in a moment of stress you fell victim to carbon rort hype.”"
"
Share this...FacebookTwitter Already 14 New (2018)
Non-Hockey Stick Papers

During 2017, there were 150 graphs from 122 scientific papers published in peer-reviewed journals that indicated modern temperatures are not unprecedented, unusual, or hockey-stick-shaped — nor do they fall outside the range of natural variability.
Less than 3 weeks into the new publication year, the explosion of non-alarming depictions of modern climate change continues.



Blarquez et al., 2018


 Magyari et al., 2018
…its climatic tolerance limits were used to infer July mean temperatures exceeding modern values by 2.8°C at this time [8200-6700 cal yr BP] (Magyari et al., 2012).


White et al., 2018
Our data, together with published work, indicate both a long-term trend in ENSO strength due to June insolation [solar] forcing and high-amplitude decadalcentennial fluctuations; both behaviors are shown in models. The best-supported mechanism for insolation-driven dampening of ENSO is weakening of the upwelling feedback by insolation-forced warming/deepening of thermocline source waters. … Another potential source of decadal-centennial forcing is total solar irradiance, which varied more in the early Holocene than the mid- to late Holocene [Marchitto et al., 2010]. Changing solar irradiance is theoretically capable of affecting ENSO via ocean dynamical cooling [Emile-Geay et al., 2007], and is correlated with centennial-scale variations in early Holocene ENSO [Marchitto et al., 2010].


Song et al., 2018
[A] general warm to cold climate trend from the mid-Holocene to the present, which can be divided into two different stages: a warmer stage between 6842 and 1297 cal yr BP and a colder stage from 1297 cal yr BP to the present. … The general cooling trend may represent a response to decreasing solar insolation; however, the relative dryness or wetness of the climate may have been co-determined by westerlies and the East Asian summer monsoon (EASM). The climate had a teleconnection with the North Atlantic region, resulting from changes in solar activity.


Huang et al., 2018
A period of weak chemical weathering, related to cold and dry climatic conditions, occurred during the Little Ice Age (LIA), whereas more intense chemical weathering, reflecting warm and humid climatic conditions, was recorded during the Medieval Warm Period (MWP). Besides, an intensification of chemical weathering in Poyang Lake during the late Holocene agrees well with strong ENSO activity, suggesting that moisture variations in central China may be predominantly driven by ENSO variability. … Rao et al. (2016b) demonstrated that a humid late-Holocene in central China and an arid late-Holocene in southern and northern China were significantly related to strong ENSO activity. Thus, it seems that ENSO forcing may be likely dominant factor controlling moisture variations in central China.


Perner et al., 2018
[W]e find evidence of distinct late Holocene millennial-scale phases of enhanced El Niño/La Niña development, which appear synchronous with northern hemispheric climatic variability.
Phases of dominant El Niño-like states occur parallel to North Atlantic cold phases: the ‘2800 years BP cooling event’, the ‘Dark Ages’ and the ‘Little Ice Age’, whereas the ‘Roman Warm Period’ and the ‘Medieval Climate Anomaly’ parallel periods of a predominant La Niña-like state.
Our findings provide further evidence of coherent interhemispheric climatic and oceanic conditions during the mid to late Holocene, suggesting ENSO as a potential mediator.





<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->






Maley et al., 2018



Polovodova Asteman et al., 2018
The record demonstrates a warming during the Roman Warm Period (~350 BCE – 450 CE), variable bottom water temperatures during the Dark Ages (~450 – 850 CE), positive bottom water temperature anomalies during the Viking Age/Medieval Climate Anomaly (~850 – 1350 CE) and a long-term cooling with distinct multidecadal variability during the Little Ice Age (~1350 – 1850 CE). The fjord BWT [bottom water temperatures] record also picks up the contemporary warming of the 20th century, which does not stand out in the 2500-year perspective and is of the same magnitude as the Roman Warm Period and the Medieval Climate Anomaly.

 


Papadomanolaki et al., 2018  (Baltic Sea)
A large fraction of the Baltic Proper became hypoxic again between 1.4 and 0.7 ka BP, during the Medieval Climate Anomaly (MCA), when mean air temperatures were 0.9–1.4 °C higher than temperatures recorded in the period 1961–1990 (e.g. Mann et al., 2009; Jilbert and Slomp, 2013).

20th/21st Centuries Non-Warming

Yi, 2018
As measures of climate response, temperature and precipitation data from the north, east, and south-facing mountain ranges of Shennongjia Massif in the coldest and hottest months (January and July), different seasons (spring, summer, autumn, and winter) and each year were analyzed from a long-term dataset (1960 to 2003) to tested variations characteristics, temporal and spatial quantitative relationships of climates. The results showed that the average seasonal temperatures and precipitation in the north, east, and south aspects of the mountain ranges changed at different rates. The average seasonal temperatures change rate ranges in the north, east, and south-facing mountain ranges were from –0.0210 ℃/yr to 0.0143 ℃/yr, –0.0166 ℃/yr to 0.0311 ℃/yr, and  –0.0290 ℃/yr to 0.0084 ℃/yr, respectively, and seasonal precipitation variation magnitude were from –1.4940 mm/yr to 0.6217 mm/yr, –1.6833 mm/yr to 2.6182 mm/yr, and –0.8567 mm/yr to 1.4077 mm/yr, respectively. The climates variation trend among the three mountain ranges were different in magnitude and direction, showing a complicated change of the climates in mountain ranges and some inconsistency with general trends in global climate change.


Bereiter et al., 2018
Our reconstruction provides unprecedented precision and temporal resolution for the integrated global ocean, in contrast to the depth-, region-, organism- and season-specific estimates provided by other methods. We find that the mean global ocean temperature is closely correlated with Antarctic temperature and has no lead or lag with atmospheric CO2, thereby confirming the important role of Southern Hemisphere climate in global climate trends. We also reveal an enigmatic 700-year warming during the early Younger Dryas period (about 12,000 years ago) that surpasses estimates of modern ocean heat uptake.

(press release)
“Our precision is about 0.2 ºC (0.4 ºF) now, and the warming of the past 50 years is only about 0.1 ºC,” he said, adding that advanced equipment can provide more precise measurements, allowing scientists to use this technique to track the current warming trend in the world’s oceans.


 Purich et al., 2018
Observed Southern Ocean changes over recent decades include a surface freshening (Durack and Wijffels 2010; Durack et al. 2012; de Lavergne et al. 2014), surface cooling (Fan et al. 2014; Marshall et al. 2014; Armour et al. 2016; Purich et al. 2016a) and circumpolar increase in Antarctic sea ice (Cavalieri and Parkinson 2008; Comiso and Nishio 2008; Parkinson and Cavalieri 2012).  … [A]s high-latitude surface freshening is associated with surface cooling and a sea ice increase, this may be another factor contributing to the CMIP5 models excessive Southern Ocean surface warming contrasting the observed surface cooling (Marshall et al. 2014; Purich et al. 2016a), and sea ice decline contrasting the observed increases (Mahlstein et al. 2013; Polvani and Smith 2013; Swart and Fyfe 2013; Turner et al. 2013; Zunz et al. 2013; Gagne et al. 2015) over recent decades. … Our results suggest that recent multi-decadal trends in large-scale surface salinity over the Southern Ocean have played a role in the observed surface cooling seen in this region. … The majority of CMIP5 models do not simulate a surface cooling and increase in sea ice (Fig. 8b), as seen in observations.


Cerrone and Fusco, 2018
Compelling evidence indicates that the large increase in the SH sea ice, recorded over recent years, arises from the impact of climate modes and their long-term trends. The examination of variability ranging from seasonal to interdecadal scales, and of trends within the climate patterns and total Antarctic sea ice concentration (SIC) for the 32-yr period (1982–2013), is the key focus of this paper. The results herein indicate that a progressive cooling has affected the year-to-year climate of the sub-Antarctic since the 1990s. This feature is found in association with increased positive SAM and SAO phases detected in terms of upward annual and seasonal trends (in autumn and summer) and upward decadal trends. In addition, the SIC [sea ice concentration] shows upward annual, spring, and summer trends, indicating the insulation of Antarctica from the warmer flows in the midlatitudes.

Palmer et al., 2018

Share this...FacebookTwitter "
"Nearly a century has passed since Europe’s last major tsunami, a 13m wave caused by an earthquake off the coast of Sicily that was responsible for around 2,000 deaths. Sometimes tsunamis in the Mediterranean can be even more destructive – a large volcanic eruption on the island of Thera (Santorini) around 3500 years ago generated a wave that decimated an entire civilisation, the Minoans, and may have led to the legend of Atlantis. Millions more people live along the Mediterranean coastline these days, of course, and the volcanoes and earthquakes haven’t gone anywhere. Indeed a new study in the journal Ocean Science suggests even a moderate earthquake in the eastern Mediterranean could set off a tsunami with the potential to affect a large proportion of the 130m people who live on its coastline. The devastating tsunamis that hit Indonesia and surrounding countries in 2004 and Japan in 2011 were a wakeup call. Since the turn of the century 177 tsunamis have actually been recorded and of these, four occurred within the Mediterranean basin. These four were all relatively small, and no one died. But history – and seismology – suggests more destructive waves are inevitable. Are we prepared for the “big one”?  The Mediterranean is ultimately prone to tectonic (and volcanic) activity as a result of the collision of the African plate into the western portion of the Eurasian plate. For the past 65 million years or so, this collision has proceeded, producing the Alps, which are still growing, and closing the Tethys Sea which once separated both continents.  Today, the Mediterranean Sea is the remnant of the Tethys and it too is shrinking as the African plate continues to drive north at about 2.5cm per year. The boundary between these plates is not clear cut however, and as a result, the Mediterranean region is criss-crossed with active fault-lines and it is these, along with plate movements, which create a complex tectonic setting and produce the region’s earthquake risk.  Significantly however, the tectonics of the region are not at all that similar to those in Indonesia or Japan. In the Pacific and Indian Oceans, the tectonic hazard results largely from subduction, where one plate is driven beneath another. Large earthquakes are common at subduction boundaries, and often result in massive displacement at the ocean bed which generates very large tsunamis.  Although there are areas of subduction in the Mediterranean, the scale is much smaller meaning less displacement and smaller tsunamis. Scientists have, in fact, suggested that the 1908 Sicily tsunami was not directly a result of displacement at all, but rather the result of an earthquake-generated landslide on the sea bed. Often, it is not the size of a tsunami (or indeed any natural hazard) that results in human devastation but rather where it is focused. In 1958 for example, the largest tsunami on record struck Lituya Bay in Alaska. The 30 metre high wave was powerful enough to travel more than 500m up the valley sides but, given the remote location, just five people were killed. In contrast, the 2004 Indonesian tsunami reached around 24m in some places, but by striking in a densely populated region, the human impact was unimaginable. With this in mind, Mediterranean tsunamis pose a significant risk. Around 130 million people live on the coastline, often in big cities: Barcelona and Algiers in the west (both with a population of 1.6 m), Naples and Tripoli in the central region (both 1m) and Alexandria (4m) and Tel Aviv (400,000) to the east.  The risk is further compounded by the fact the Mediterranean is relatively small and enclosed, meaning any tsunami could spread throughout the whole basin. Warning times, essential for minimising human losses, would also be small. Economic impacts could also be significant, with the Mediterranean home to some large industrial centres and ports. Little can be done about the hazard itself – seismic and volcanic activity can neither be prevented nor (accurately) predicted. However there are steps which can, and in some cases have, been taken to reduce the potential impact of Mediterranean tsunamis.  Following the 2004 Indonesian tsunami, UNESCO set up the (take a deep breath) Intergovernmental Coordination Group for the Tsunami Early Warning and Mitigation System in the North-eastern Atlantic, the Mediterranean and connected seas (ICG/NEAMTWS). This group is responsible for monitoring seismic activity, sea levels and other relevant data, and disseminating warnings when necessary. Such warnings saved many lives in Japan in 2011.  Development of early warning systems is progressing, but workable and widespread availability of such warnings is likely to be far off. Given this, education of vulnerable communities is key so that they can identify early warning signs and act accordingly.  Unfortunately it might well take a large, devastating tsunami in the Mediterranean before warning and defences are taken seriously. We can only hope that the wave, when it comes, is not as destructive as could be."
"Imagine a world populated by woolly mammoths, giant sloths and car-sized armadillos – 50,000 years ago more than 150 types of these mysterious large-bodied mammals roamed our planet. But by 10,000 years ago, two-thirds of them had disappeared.  Since the end of the 19th century, scientists have puzzled over where these “megafauna” went. In 1796, the famous French palaeontologist Georges Cuvier suggested a global catastrophe had wiped them out. Others were appalled. The great Thomas Jefferson was so against Cuvier’s idea he sent an expedition to try to find vast herds of these animals grazing contentedly in the American interior. The only thing anyone could say with certainty was there should be a lot more of them than we see today.  Alfred Wallace, who wrote the first paper on evolution by natural selection with Charles Darwin, noted that “we live in a zoologically impoverished world, from which all the hugest, and fiercest, and strangest forms have recently disappeared”.  It’s one of the great historical whodunnits: what happened to the megafauna, and when did they disappear? As with any good mystery, there are two main suspects: climate and humans. The idea that our ancestors may have hunted the huge beasts to extinction has long been a popular view, particularly as the spread of humans around the world appears closely associated with their demise. Several major criticisms continue to be levelled at this theory, the most popular being that many large animals are still present in Africa, despite it having the longest record of occupation by people. Others in turn argue that humans co-evolved alongside megafauna in Africa for millions of years, giving animals time to learn from human behaviour. The alternative is that a rapidly changing climate caused the habitat of the megafauna to shrink or disappear. As the planet warmed out of the last ice age 12,000 years ago, many animals would have struggled to adapt to the new environment. A major criticism here is that there have been other major climatic changes in the past, some of which have been equally extreme and rapid. What could have been so different with this most recent warming? In a research paper published in the journal Science, we report new advances in ancient DNA, carbon dating and climate reconstruction that finally give some answers. Previously, as long as species appeared to survive in the fossil record the interpretation had been that nothing significant had happened for tens of millennia.  But thanks to ancient DNA analysis of megafaunal bones we now know that this approach has missed a series of events throughout the past 50,000 years when major parts of a species’ genetic diversity, or even the whole species itself, disappeared. Alongside this, more accurate carbon dating of the fossil remains shows these extinctions did not all happen at a single time but were staggered through time and space. It’s important to realise the backdrop to these extinctions was a wildly fluctuating climate. The ice age of the northern hemisphere was not one long frigid wasteland.  Instead, frozen conditions were punctuated by many short, rapid warming periods, known as interstadials, where temperatures would soar from 4 to 16˚C within just a few decades and last for hundreds to thousands of years. They represent some of the most profound climate changes detected in the recent geological past.  When we precisely compared the dates for European and American extinctions with climate records, we were amazed to find they coincided with the abrupt warming of the interstadials; in stark contrast there is a complete absence of extinctions at the height of the last ice age.  As temperatures rose during the interstadials, dramatic shifts in global rainfall and vegetation patterns would have placed the megafauna under immense stress. Those that could not adapt to the rapidly changing conditions would have quickly succumbed. The European cave lion, for instance (Panthera leo spelaea in the chart below), survived through periods when much of the continent was covered in ice, only to go extinct during relatively benign conditions around 14,500 years ago. There seems little doubt humans would have contributed to extinctions, however. While the dramatic climate shifts were the major driver in megafaunal extinction events, humans would have applied the coup de grâce to populations already suffering major stress.  In one likely scenario, humans would have concentrated their hunting efforts along dispersal routes, killing the few bold individuals moving out to re-establish an extinct population, causing localised extinctions to expand into larger and larger areas, that would have eventually led to an irreversible ecosystem collapse. It’s likely the scattered pattern of extinctions and the difficulty of detecting them from fossils alone is why the relationship with warming events has not been detected before. So what does this mean for the future? Well for a start, rapidly increasing temperatures are not good news for the megafauna that survived the last warming. In many ways the rise of atmospheric CO2 levels and resulting warming effects are expected to have a similar rate of change to the onset of past interstadials, heralding another major phase of large mammal extinctions.  This seems all the more likely thanks to our “success” in developing the planet’s surface, breaking up areas of natural habitat and disrupting any connectivity that once existed between areas. Migration is becoming increasingly less of an option for species struggling to adapt to changing temperatures with little chance of back filling from neighbouring areas for re-establishing populations. Even after all these years, megafauna are providing a precious lesson from the past."
"
Share this...FacebookTwitterIn Germany there is one weather station that has be intact and unchanged for some 138 years. 
It has never been moved and never been corrupted by the urban heat island (UHI) effect. Moreover it has consistently used the same instrumentation and computation method over the entire period, thus making it rare indeed. Few station can boast having those instrumentation qualities.
That measurement station is one operated at the Klostergarten of the St. Stephan Abbey in Augsburg just northwest of Munich.
44-year veteran German meteorologist Klaus Hager reports the following results of this station (reproduced with permission):
======================================
A look at the January mean temperature in Augsburg from 1879 – 2017 ( 138 years)

The chart below shows the chaotic ups and downs of the mean value of the January temperatures measured in the Klostergarten of the St. Stephan Abbey in Augsburg:

First it’s important to note:

The measurement location has not changed since 1879, nor has it been relocated. The garden area remains 1 hectare in size, and thus is completely representative of the Augsburg inner city area.
The mean temperature was computed by halving the sum of the high temperature and the low temperature.
The measurements were always done using glass thermometers – a mercury thermometer for the maximum and an alcohol thermometer for the minimum – inside an official so-called Stevenson screen.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Thus real continuity with respect to measurement is assured.
Over the entire 138-year observation period the measurements were carried out according to the same technical requirements, which is something that unfortunately can only be said about very few measurement stations.
As the chart above shows, one can see considerable temperature fluctuations from year to year. The deviations from the 138-year mean of -1.0°C are shown in red for above-mean temperatures and in blue for those below the mean. Of course since the end of the 1980s, positive deviations have been far more common, but there remains no detectable relationship with the continuously rising CO2 in the air.
Because nature – as is the case with temperature – is constantly undergoing fluctuations, one ought to be especially careful when it comes to making projections into the future. Unfortunately nature’s complexity also does not allow it to be modelled adequately.
January of 2017 was the seventh coldest since 1879, posting a mean of – 6.1°C.
Finally let’s not forget to thank the fathers of the St. Stephan Abbey for having recorded the temperatures every day until today.
 

Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe’re always hearing from the European media how winters supposedly have been getting warmer. Yet when we look out the window and look at the hard data, the claim crumbles.
Atlanta ice box, 8-15 cm of snow!
Today snow is forecast across much of Europe. In the US Dr. Ryan Maue tweets if there is any place other than southern Florida where it is not snowing, and: “Atlanta suffering from 3-6″ of global warming today.”
Record low in Hokkaido
Kirye from Japan tweeted telling us that Ikutahara, Hokkaido Prefecture saw with a reading of -24°C the coldest December 9 temperature in at least 40 years:

Hokkaido temperatures giving CO2 the cold shoulder. Data Source: JMA.
-12°C and snow in the UK!
Meanwhile the UK Met Office has issued the amber warning as temperatures are forecast to plummet to -12°C and snow to pile up to 20 cm.
Of course all of the above do not come as any surprise to those who ignore the media climate propaganda and focus on the real data.
Cooling Central Europe winters past 30 years
The European Institute for Climate and Energy Friday posted on Germany mean winter temperature over the past 30 years. A vastly huge majority of Germans would tell you that Germany winters have gotten warmer – because of “global warming” – and so confirm they are only parroting the fake German news.
Yet, EIKE writes what the reality in Germany really is: “Winter has been giving global warming the cold shoulder“.
A chart of the last 30 winters from data from the German DWD national weather services show that German winters in fact have been cooling:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Chart: Josef Kowatsch.
Going back more than 100 years, since 1910, it is also clear that German mean winter temperatures have nothing to do with CO2:

Germany mean winter temperatures with smoothed polynomial trend curve. Chart: Josef Kowatsch.
So before the German DWD weather services and media blurt claims that winters are becoming warmer, they first ought to look at their own data. There hasn’t been any warming since “global warming” started being an issue at the end of the 1980s.
No winter warming elsewhere
The following charts of winter mean temperature depict locations well beyond Germany, showing Central England, Sapporo Japan, Östersund in Central Sweden, and Oymyakon, Eastern Siberia respectively:



No warming trends seen over the past 30 years at these locations. And the eastern Siberian permafrost is not going anywhere anytime soon, Stefan Krämpfe’s chart tells us.
Of course global temperatures as a whole have warmed since 1980, but it’s nothing that comes close to what the alarmists would like to have us believe. The truth is that there’s been very little warming at all since the late 1990s.
 
Share this...FacebookTwitter "
"Tens of thousands of people remained stranded on Thursday evening while attempting to flee bushfire-ravaged areas of the south-east Australian coast – having earlier been urged to leave before the return of extreme and dangerous weather conditions. The mass evacuation of communities in New South Wales and Victoria is among the largest ever emergency movements of people in Australia. The numbers fleeing the bushfire crisis remain unclear, but are expected to compare to the 60,000 people who were flown out of Darwin after Cyclone Tracy in 1974. Visitors told to flee a vast evacuation area along the NSW south coast reported sitting in gridlock for up to 10 hours after responding to the order to evacuate, as further outbreaks of fire and sheer weight of traffic blocked escape routes north of Ulladulla and near Cooma in the Snowy Mountains. The prime minister, Scott Morrison, urged people to be patient, as he again deflected criticism about his government’s policies to address the causes of climate change. “I know you can have kids in the car and there is anxiety and there is stress and the traffic is not moving quickly but the best thing to do – the best thing that helps those out there volunteering, out there trying to restore some order to these situations, is for everyone to be patient,” Morrison said. On Thursday afternoon an angry protester told Morrison he should be “ashamed of himself” and that he had “left the country to burn” during a tour of the burnt out town of Cobargo. Authorities in Victoria hold grave fears for 17 people missing across the state, and advised anyone who could do so to leave fire-affected places in the East Gippsland region. The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. At the Victorian coastal inlet of Mallacoota, which is among 20 towns that have been isolated by the fires since Tuesday, between 3,000 and 4,000 people were facing an impending food and water shortage as they waited to be evacuated by sea. On Thursday afternoon, Australian defence force officials said they expected to relocate 800 people using the naval ship HMAS Choules, while others will be airlifted out. Evacuation plans for #Mallacoota are underway, as #YourADF works with emergency services to prepare transferring people to HMAS Choules tomorrow AM. Commanding Officer of Choules, CMDR Houlihan, led the ADF team onshore today to provide timely and relevant support. pic.twitter.com/ecphjfLg9f New South Wales has declared a seven-day state of emergency before extreme conditions, including temperatures exceeding 40C, return to the south coast, Snowy Mountains and the outskirts of Sydney on Friday and Saturday. Evacuation orders were issued to holidaymakers from Batemans Bay to the Victorian border and people were advised that two highway escape routes were opened – north to Sydney along the Princes Highway, and south, via the Monaro Highway, hooking back to Canberra through Cooma. But on Thursday evening both routes were at a standstill as the fire threat lingered, with tailbacks of up to 25km reported. The northern route was blocked from about midday by a bushfire that closed the Princes Highway. Motorists heading north were advised to delay their trip or take shelter at Ulladulla. The road remained closed about 6pm. Motorists were reported abandoning their cars or heading to the south. Gridlock all the way through Ulladulla, similar scenes in Milton, Nowra and Batemans Bay. Just met a woman who’s arrived in Ulladulla after leaving Depot Beach - 30km south - at 7am. I’ll have more on @abcnews The World Today @ 12.10pm on Local Radio, 1pm on @RadioNational pic.twitter.com/iIWnTxPTIv The southern route was also hampered by a bushfire that came close to the Monaro Highway and caused delays north of Cooma. Witnesses said the lines of stationary traffic near Cooma were more than 30km long. Monaro Hwy from Cooma to Canberra. Drive carefully folks. pic.twitter.com/Ltbd5xjU9o Residents in the town of Batlow, famous for its apples, were advised to leave by Friday morning because of forecast fire danger in the Kosciuszko national park and surrounding areas. The entire park has been evacuated and authorities warned conditions were likely to be so severe that surrounding towns could not be defended. One account from a friend trying to evacuate from Malua Bay today:Headed north at 6am. Took 6.5 hours to do 80kms.... then highway closed at Benalong and we were turned back. so we headed south. now at a standstill coming into Bega 10 hours in the car and counting. NSW Rural Fire Service commissioner Shane Fitzsimmons said conditions on Saturday were forecast to be worse than those on New Year’s Eve. “Those fires have spread at the absolute worst-case scenario, which typically is not what happens when it plays out on the ground,” Fitzsimmons said. “The conditions on Saturday are likely to be worse than New Year’s Eve and a lot of those areas in the south-east quadrant of the state have the potential to be impacted and impacted very heavily.” The New South Wales premier, Gladys Berejiklian, said the state of emergency from Friday would give emergency services the authority to undertake forced evacuations and road closures at short notice. “We don’t take these decisions lightly but we also want to make sure we’re taking every single precaution to be prepared for what could be a horrible day on Saturday,” she said. Since Christmas Day, nine people have been killed in bushfires across New South Wales and Victoria, while 17 people have died since the season began. About 150 fires continued to burn in Victoria and NSW on Thursday afternoon and officials warned they would be unable to control the blazes before conditions worsen. “We have no capacity to contain these fires … the fires are going to do what they are going to do, and people have to get out of that area,” NSW Rural Fire Service deputy commissioner Rob Rogers said. Earlier on Tuesday, Morrison attended the funeral of Geoffrey Keaton, a volunteer firefighter who died on 19 December when his truck rolled at Green Wattle Creek, south of Sydney. At a press conference afterwards, Morrison was repeatedly asked about his government’s climate policy. “I understand the anxiety and I understand the fear that is there for many and I understand the frustration, but this is a natural disaster,” he said. “What we are saying is we cannot control the natural disaster but what we can do is control our response. “The drought has created a tinder box around the country and that has, through various forms of ignition, has seen these fires run for long periods of time.”"
"
Share this...FacebookTwitterUPDATE:Hat-tip: Kenneth
Graph showing increasing Adélie penguin numbers during 1982-2015
Che-Castaldo et al., 2017
“We found a marked and steady increase in [Adélie penguin] abundance around the rest of the Antarctic continent, including both Eastern Antarctica and the Ross Sea [during 1982-2015].”
Scientists have historically determined that increasing Adélie penguin numbers seem to coincide with warm periods, whereas cooling periods elicit population declines (Emslie et al., 2007; Huang et al., 2009).
===============================================
Once again I’m bringing you today another one from Lüning’s and Vahrenholt’s Die kalte Sonne, this one concerning a recent study claiming the “king penguin populations are at heavy extinction risk under the current global warming predictions“.

With dire (phony) study claims of king penguins being threatened by global warming, climate science and media once again do their usual number on the public. Image source here. CC BY-SA 3.0
=====================================================
What did the king penguins do 1500 years ago when it was warmer than it is today?
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
The online Swiss daily Luzerner Zeitung reported on 27 February 2018:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




King’s penguin threatened by climate change
Climate change is threatening more than 70 percent of the king penguin colonies. So writes an international team of scientists in the journal ‘Nature Climate Change’. The animals have to move further south and thus into colder areas. […] The king penguin (aptenodytes patagonicus) is the second largest type of penguin after the emperor penguin. According to the study, the population is currently 1.6 breeding pairs. Over thousands of years the king penguins have been able to rely on the Antarctic polar front, the researchers write. That is a system of currents the transports masses of water from the depths to the surface and so provides for a large fish supply for a relatively small area. Because of climate change, the polar front is shifting to the south and is leaving the Crozet Islands, the Kerguelen and Marion Island on which the penguins live.”
Read more at the Luzerner Zeitung.
Once again we have the fairy tale that everything in the past was stable and that today it’s warmer than ever before. But just one look at Stenni et al. 2017 would have sufficed:
Antarctic climate variability on regional and continental scales over the last 2000 years
[…] Our new reconstructions confirm a significant cooling trend from 0 to 1900 CE across all Antarctic regions where records extend back into the 1st millennium, with the exception of the Wilkes Land coast and Weddell Sea coast regions. Within this long-term cooling trend from 0 to 1900 CE, we find that the warmest period occurs between 300 and 1000 CE, and the coldest interval occurs from 1200 to 1900 CE. Since 1900 CE, significant warming trends are identified for the West Antarctic Ice Sheet, the Dronning Maud Land coast and the Antarctic Peninsula regions, and these trends are robust across the distribution of records that contribute to the unweighted isotopic composites and also significant in the weighted temperature reconstructions. Only for the Antarctic Peninsula is this most recent century-scale trend unusual in the context of natural variability over the last 2000 years. […]”
Let’s take a look at the temperature chart for all of Antarctica over the past 2000 years:

Temperature curve for Antarctica over the past 2000 years. Source: Stenni et al. 2017.
As we see, it has not even been 1000 years ago since the penguins were forced to leave the area where they currently find themselves due to oncoming cold. We would gladly provide the activists with a new pair of spectacles that would correct their climate-historical myopia.
 
Share this...FacebookTwitter "
"Wolves and foxes are closely related and share many of the same characteristics. But look at their eyes – where wolves have rounded pupils like humans, foxes instead have a thin vertical line. But it isn’t just canines –across the animal kingdom, pupils come in all shapes and sizes. So why the differences? It’s a question that has long interested scientists working on vision and optics. In a new study published in the journal Science Advances, colleagues from Durham, Berkeley and I explain why these pupil shapes have developed. Goats, sheep, horses, domestic cats, and numerous other animals have pupils which vary from fully circular in faint light to narrow slits or rectangles in bright light. The established theory for this is that elongated pupils allow greater control of the amount of light entering the eye. For instance, a domestic cat can change its pupil area by a factor of 135 from fully dilated to fully constricted, whereas humans, with a round pupil, can only change area by a factor of 15. This is particularly useful for animals that are active both day and night, allowing for much better vision in low light conditions. However, if the only reason for elongated pupils was to control the amount of light entering the eye, the orientation would not be important: horizontal, vertical, or diagonal would all offer the same advantages.  Instead, the pupils are almost always horizontal or vertical, which suggests there must be other benefits which explain this orientation. Our work has focused on the visual benefits of vertical and horizontal pupils in mammals and snakes. One of the most interesting factors we found is that the orientation of the pupil can be linked to an animal’s ecological niche. This has been described before, but we went one step further to quantify the relationship.  We found animals with vertically elongated pupils are very likely to be ambush predators which hide until they strike their prey from relatively close distance. They also tend to have eyes on the front of their heads. Foxes and domestic cats are clear examples of this. The difference between foxes and wolves is down to the fact wolves are not ambush predators – instead they hunt in packs, chasing down their prey. In contrast, horizontally elongated pupils are nearly always found in grazing animals, which have eyes on the sides of their head. They are also very likely to be prey animals such as sheep and goats. We produced a computer model of eyes which simulates how images appear with different pupil shapes, in order to explain how orientation could benefit different animals. This modelling showed that the vertically elongated pupils in ambush predators enhances their ability to judge distance accurately without having to move their head, which could give away their presence to potential prey. Grazing animals have different problems to deal with. They need to check all around for prey and they need to flee rapidly in case of attack. Having eyes towards the side of their head helps them to see nearly all around them. Having a horizontal pupil enhances the amount of light they can receive in front of and behind them while reducing the amount of light from above and below. This allows them panoramic vision along the ground to help detect potential predators as early as possible. The horizontal pupil also enhances the image quality of horizontal planes and this enhanced view at ground level is also an advantage when running at speed to escape. So, vertically elongated pupils help ambush predators capture their prey and horizontally elongated pupils help prey animals avoid their predators. We realised our hypothesis predicted that shorter animals should have a greater benefit from vertical pupils than taller ones. So we rechecked the data on animals with frontal eyes and vertical pupils and found that 82% are what is considered “short” (which we defined as having a shoulder height of less than 42cm) compared with only 17% of animals with circular pupils.  We also realised that there is a potential problem with the theory for horizontal elongation. If horizontal pupils are such an advantage to grazing animals, what happens when they bend their head down to graze? Is the pupil no longer horizontally aligned with the ground?  We checked this by observing animals in both a zoo and on farms. We found that eyes of goats, deer, horses, and sheep rotate as they bend their head down to eat, keeping the pupil aligned with the ground. This remarkable eye movement, which is in opposite directions in the two eyes, is known as cyclovergence. Each eye in these animals rotates by 50 degrees, possibly more (we can only make the same movement by a few degrees). There are still some unexplained pupils in nature. For example, mongooses have forward-facing eyes but horizontal pupils, geckos have huge circular pupils when dilated which reduce down to several discrete pinholes when constricted and cuttlefish have “W”-shaped pupils. Understanding all these variations is an interesting challenge for the future."
"Most life in the sea ultimately depends on photosynthetic plankton. Also known as microalgae, these tiny or microscopic organisms live near the surface and take their energy from the sun and pass it on through the marine food chain.  But these plankton have a big role to play above the surface of the sea too. In new study published in the journal Science Advances, colleagues and I found that plankton help to control clouds over remote seas far from land. These clouds in turn bounce the sun’s energy back into space, regulating the Earth’s climate and keeping temperatures cooler than they would otherwise be without them. Clouds are made up of many tiny droplets of water that have condensed from water vapour onto microscopic particles floating in the Earth’s atmosphere. These particles are known as cloud condensation nuclei. Plankton essentially help provide clouds with these nuclei to form around.  The number of these particles in a given volume helps to determine the number of droplets in a cloud, which can have a big influence on how much sunlight a cloud reflects back into space. The more droplets a given mass of cloud water is broken up into, the more sunlight is reflected, as the overall surface area of the cloud’s droplets increases. Since a significant portion of the planet’s reflectivity, or albedo, is due to clouds, this can have a major impact on the energy balance of the Earth. To investigate the link between plankton and clouds, we looked at the Southern Ocean. This sea, encircling Antarctica, is one of the most remote places on the planet and far from any man-made sources of particles. And yet it is also one of the cloudiest places on Earth. What then are these clouds clinging on to? We analysed satellite cloud data in a section of the Southern Ocean spanning right around the globe between the 35th parallel south (which passes through Australia and just south of South Africa) and the 55th (which just clips the bottom of South America).  We found that more cloud droplets tended to occur above patches of the ocean with more plankton, indicated by increased concentrations of a type of chlorophyll used in photosynthesis. This means plankton are likely to influence cloud albedo and the amount of energy from the sun that is reflected to space.  Most of this is down to plankton releasing gases either through cell ageing, or when they are broken open and eaten by their microscopic animal counterparts zooplankton. Some of this gas is then converted into new microscopic solid particles, or adds to existing particles, which act as extra condensation nuclei. However, we also found that some organic material – which can come from the bodies of the plankton, other sea creatures, viruses, bacteria and so on – is emitted directly into the atmosphere through sea spray. Water can condense around these tiny particles, forming extra cloud droplets (although organic material may affect droplet numbers in other ways too – the science is still hotly debated). We also estimated how much extra solar energy was prevented from reaching the surface due to the extra cloud droplets formed by phytoplankton – up to ten watts per square metre in summer. That’s comparable to similar estimates of the annual mean effect on clouds from man-made particles downwind of highly polluted regions. Thus, in a lifeless ocean without phytoplankton it is likely that the Southern Ocean’s surface would be somewhat warmer.  Many climate models underestimate the amount of sunlight reflected back into space by clouds in the Southern Ocean. This can lead to errors in regional sea surface temperature predictions and incorrect large-scale circulation patterns both locally  and in far afield regions such as the tropics, and so it is important that they are corrected.  These biases might partly be down to an unrealistic representation of the link between phytoplankton and cloud formation. Somewhat ironically, uncertainties in our knowledge of the “baseline” effect of these natural condensation nuclei are also one of the biggest causes of uncertainties in how anthropogenic aerosols are affecting the climate. Due to its remoteness and inhospitality there is, however, very little in-situ data available from within Southern Ocean clouds with which to study what is going on. This makes satellite data very valuable, but also emphasises the need for more direct observations in this region."
"Since the dawn of aviation, planes have primarily been powered by carbon-based fuels such as gasoline or kerosene. These contain a lot of energy for their weight, providing the vast power required to lift large commercial airliners on journeys across the globe. But with oil resources declining and penalties on greenhouse gas emissions increasing, the future of aviation is dependent on finding an alternative power source. Is electricity the answer? A first step is to develop “more electric aircraft” – jet-powered planes that maximise the use of electricity for all the other aircraft systems. The idea is to significantly reduce fuel consumption by improving overall energy efficiency. In practice, this means reducing the weight of the aircraft, reducing drag with improved aerodynamics and optimising the flight profile to use less fuel.  But though these improvements can save on fuel, that alone isn’t enough. The shift to more sustainable aircraft requires major, longer-term solutions. Such significant innovations have often been driven by military requirements. The jet turbine engine was developed during World War II and the US Air Force’s Chuck Yaeger first broke the sound barrier in the Bell X-1 as part of the Cold War race to achieve supersonic speeds. The drive for new technologies led to massive improvements in performance and reliability, which has since filtered through to commercial aviation and made mass intercontinental air travel a reality. Concorde was the ultimate expression of this transformation from military to high-performance commercial aircraft, but despite its phenomenal performance it was plagued by complaints of excessive noise and pollution. Modern jet air travel still consistently raises such environmental concerns and, while the military has an obvious incentive to design the fastest aircraft, its motivation to go green is less obvious. We may need to look elsewhere for the next big innovation. Solar-powered endurance aircraft have received a lot of attention recently, with the Solar Impulse team attempting to make the first round-the-world flight. But solar power, while an interesting technical challenge, is not a particularly realistic option for mass transit of passengers. As can be seen from the Solar Impulse aircraft, the power output from the Solar Panels on a very wide wingspan is able to transport only the aircraft and the pilot for any significant distance. Battery storage is the key limiting factor for electric aircraft. If electric aircraft are held back by either weight or fuel restrictions, it’s probably down to the battery. Aircraft typically have a longer fuelling time than a car, so rapid recharging is possible and effective, as current jet aircraft take about the same time to refuel (and also for passenger and cargo turnaround) so electric charging of about 1hr is reasonable, however the critical problem is energy density – how much energy does the battery provide for its weight?  Typical lithium-ion batteries in use today have a maximum energy density of around 1,000,000 joules of energy per kilogram, and while newer research promises the possibility of higher densities, these are not available commercially. A million joules sounds like a lot. However, compare this with 43 million joules per kilogram for aviation fuel. Swapping the fuel tanks for a battery weighing 43 times as much isn’t a viable option – clearly there’s a significant storage problem to be solved before electricity can power large aircraft over long distances. So where does electric power fit in the long-term vision for consumer air travel? Despite the obvious technical challenges, The Airbus prototype E-Fan aircraft is due to be put into production by 2017. The E-fan is a very light two-seater plane powered by two electric motors, with a relative speed and carrying capacity far lower than those required by commercial carriers. However,  Within the next decade, this technology may extend to short-range commuter and business aircraft – especially targeting routes that still use conventional propeller propulsion. Airbus has medium-term plans for such an aircraft, with a target capacity of perhaps 60 passengers – making it a suitable platform for short-haul commuter flights. Safety and reliability must be addressed before electric aircraft are adopted by commercial airlines. Much as the electric car still has to achieve a critical level of public confidence, perceived reliability will have a significant impact on consumer trust in new aircraft.  If prototypes such as the E-Fan can build public confidence, this may mark a “tipping point” in overcoming the technical challenges inherent in any new form of transportation, especially in aviation which has a track record of rapid innovation. Advances – particularly in new materials, storage and power electronics technology – may offer the prospect of purely electric commercial aircraft within the next two decades."
nan
"Dairy farmers were racing to shore up supplies of fodder and fuel on Friday as they prepared for a hot weekend that could see the return of fires that have already ravaged much of Australia’s east coast. Two key dairy areas, East Gippsland in Victoria and the New South Wales south coast, were heavily burned during fires over the new year, adding to the woes of an industry already suffering from a crippling drought and persistently low milk prices.  Supermarkets say the fire crisis has not curbed the supply of fresh milk but the head of NSW farmers’ body Dairy Connect, Shaughn Morgan, said it could do so if it continued. “It could have an impact, depending on the amount of milk that’s not collected,” he said.  He said it was hard to get information but the fires were “impacting quite heavily on the south coast from Nowra down”. “These guys aren’t able to get the milk from their farms, they’re spilling their milk,” he said. “We’re very grateful to the processors, who are continuing to pay the farmers.” He said he hoped the declaration of a state of emergency by the state government on Thursday would allow roads to open and farmers to bring in fodder. “If there’s no fodder to feed them [the cows],” he said. “It raises serious questions about the viability of their farms. “Saturday’s a real concern because it is another flashpoint – it’s going to be something that we need to monitor really closely. “The people down there are at their wit’s end and have been under enormous pressure for days.” The president of the United Dairyfarmers of Victoria, Paul Mumford, said there had been reports of pasture damage and some stock losses in East Gippsland and up into the state’s north-east, but information was “still reasonably sketchy coming out of both areas”. “The big problem farmers are having is fire damage not only to pastures but infrastructure – but more importantly getting services back on to the farm.” He said farmers needed fodder to feed their cows, and fuel to power milking equipment. “The cows have to be fed and the cows also have to be milked,” he said. “Some farms may not have been able to milk their cows since the fire went through their district. “Because tomorrow and Sunday are going to be such problem days for heat, from what I understand today no fodder or services will be allowed in or out of those districts until the worst of the danger has passed.” Max Roberts, the chairman of milk processor Bega, said it had been difficult to collect milk. “If they’re not on fire, we can’t get to them,” he said. “There’ve been a number of farms that haven’t milked for up to 50 hours, 60 hours, and that’s an issue for cow health.” He said the company was working on getting fodder and diesel to farms, while emergency services were helping to get milk tankers out to dairy farms. “It’s highly unusual to get a milk tanker turning up with a police escort but that’s what’s happening.” He was “not sure” how much it was costing Bega to pay for milk it could not collect. “It’d be a bigger cost if the farm went broke,” he said. “You take a longer-term view on these things.” Production at the company’s factory in Bega will grind to a standstill over the weekend. “The factory will close down all but a skeleton operation all through Saturday and Sunday to allow people to stay home and look after themselves,” Roberts said. “There will be milk tanker pickups but again the instruction is that, if it isn’t safe to do it, don’t.” Steve Guthrey, a former dairy farmer who now grows fodder and agists livestock at his property near Bega, said the community was bracing for the weekend. “Pretty much everybody’s just watching and waiting for the moment,” he said. “We know we’ve got a pretty serious day tomorrow. “We’re all on tenterhooks, preparing our houses and farms as best we can.” He said he feared fires to the north-east and north-west could join together. “We haven’t gotten any aircraft down here to help us,” he said. “A lot of the vehicles down here are really limited in what they can do. “We’ve run out of irrigation water now. The dams are virtually empty … We’ve still got a long summer ahead of us now.”"
"
Share this...FacebookTwitter
Comedy clubs aren’t usually thought of as venues for serious debate about controversial topics like climate change.
And yet in a rare debate opportunity, Harvard-Smithsonian Center astrophysicist Dr. Willie Soon took full advantage of the short time he had available to him.  He critiqued “consensus” science, the ocean acidification narrative, the poverty-inducing reliance on wind and solar energies, and climate change alarmism in general.
Dr. Jon Christensen, his opponent, an adjunct assistant professor in the Institute of the Environment and Sustainability, emphasized the “consensus” and the “existential threat” of climate change, extolled the expansion of renewable energy sources like wind and solar in California, and insisted that politicians in the Golden State are focused on not burdening poor people with their “green” policies.
A summary highlighting some of the more interesting exchanges and their corresponding timelines follows.

(1) Dr. Soon: CO2 a benefit, minimal sea level rise awaits
Dr. Christensen: CO2 rise an existential, apocalyptic threat
2:25 Dr. Willie Soon “They try to demonize carbon dioxide as if this is something that’s going to kill everybody.  Which is really not true. …  CO2 has a lot of potential benefit[s].  There are some potential negatives.  If it’s [CO2] going to cause sea levels to rise,  we’re going to have 4 inches or 8 inches or 12 inches…per century.”
3:30 Dr. Jon Christensen “The way I like to think about all this is…sunny with a chance of apocalypse. … (3:59) There is an existential threat out there. (5:40)  Everybody…who is under 40 is going to experience the effects of climate change, global warming, increase in sea level rise, flooding…in their lifetime.”
What the science says…
1. During 1958 to 2014, global sea levels rose at a rate of 1.3 mm per year to 1.5 mm per year, which is a rate of just over 3 inches per century; the Greenland and Antarctica ice sheets have combined to add just 0.59 of an inch of melt water to sea level rise since 1958 (Frederiske et al.,2018).   There has been “a recent lack of any detectable acceleration in the rate of sea level rise” (Parker and Ollier, 2017).  
2. Since the 1980s, coastal land area across the globe has been expanding, meaning that more land are is above sea level today (2015) than in 1985 (Donchyts et al., 2016).
“Coastal areas were also analysed, and to the scientists’ surprise, coastlines had gained more land – 33,700 sq km (13,000 sq miles) – than they had been lost to water (20,100 sq km or 7,800 sq miles). ‘We expected that the coast would start to retreat due to sea level rise, but the most surprising thing is that the coasts are growing all over the world,’ said Dr Baart.  ‘We were able to create more land than sea level rise was taking (press release).'”
3. Hurricane frequencies and intensities have been declining (Truchelut and Staeling, 2018, Zhao et al., 2018, Klotzbach et al., 2018).
4. Extreme weather events (floods, droughts) have decreased in frequency and intensity (or showed no detectable change) (Zhang et al., 2017, McCabe et al., 2017, Cheng et al., 2016, Hodgkiins et al., 2017, McAneney et al., 2017).
5. In recent decades 92% of Canadian polar bear subpopulations have remained stable or increased, leading scientists to conclude that “it seems unlikely that polar bears (as a species) are at risk from anthropogenic global warming” (York et al., 2016).  Local Inuit populations even report that there are “too many polar bears now” (Wong et al., 2017).   There has also been a “marked and steady increase“ in penguin populations between 1982 and 2015 (Che-Castaldo et al. 2017). 

(2) Dr. Soon: Ocean acidification is a myth.
Dr. Christensen: Ocean acidification is science.
11:21 Dr. Willie Soon “Can I say something about ocean acidification?  It’s a myth. … The ocean has something you can measure.  Basically, it’s called ion of the hydrogen.  It’s called [the] pH scale. You have 0 to 14.  Seven is neutral.  Seven to 0 is acidic.  Seven to 14 is called basic.   The ocean is right about 8.03, 8.04 [non-acidic].  But deep inside the ocean, about 2,000 meters down, it’s actually very acidic.  If you wanna talk about ocean acidification – it’s one of the most dangerous myths that there is.  A very radical one.  It’s not sensible.  Who created this myth, actually…?”
12:23 Dr. Jon Christensen “They call it [ocean acidification] science.”
12:25 Dr. Willie Soon “No, it’s not even science, excuse me, because… Do you know what the pH of rainwater is?  It’s 5.5. (Wikihow.com:  “Ordinary rainwater is naturally acidic with a pH between 5.0 and 5.5.”) …  [T]hat means you have to outlaw all the [naturally acidic] rain that’s falling down?  You want to outlaw all the slightly acidic water that is sitting on the bottom of the ocean?”
13:05  Dr. Jon Christensen “No….It’s not that hard to actually read the science.  It can seem a little bit daunting but anybody who can read can work their way through many of these papers and you can see that there’s a wide variety of findings and results and conclusions…  There is in science a fair degree of certainty on a lot of things. …  What we know is that there’s a wide spectrum of results here and we need to look at the data and the whole big picture of the science and not just write it off one way or the other.”
15:08 Dr. Willie Soon “Jon, my whole point is that ocean acidification is an extreme.  It is one of the most extreme things they could come up with because they are not able to find the fingerprint of the carbon dioxide warming of the atmosphere so then they started to come up with this new scheme [ocean acidification].  Next thing…they’re going [to claim] carbon dioxide is killing all of the polar bears.  It’s going to melt all the ice sheets.  It’s completely not even true.”
What the science says…
McElhany, 2017
“Documenting an effect of OA [ocean acidification] involves showing a change in a species (e.g. population abundance or distribution) as a consequence of anthropogenic changes in marine carbonate chemistry. To date, there have been no unambiguous demonstrations of a population level effect of anthropogenic OA [ocean acidification], as that term is defined by the IPCC. … [I]t is important to acknowledge that there are no studies that directly demonstrate modern day effects of OA [ocean acidification] on marine species.”
Duarte et al., 2014
“[T]here have been a few claims for already realized impacts of ocean acidification on calcifiers, such as a decline in the number of oysters on the West Coast of North America (Barton et al. 2012) and in Chesapeake Bay (Waldbusser et  al. 2011). However, the link between these declines and ocean acidification through anthropogenic CO2 is unclear.    Corrosive waters affecting oysters in hatcheries along the Oregon coast were associated with upwelling (Barton et  al. 2012), not anthropogenic CO2. The decline in pH affecting oysters in Chesapeake Bay (Waldbusser et al. 2011) was not attributable to anthropogenic CO2 but was likely attributable to excess respiration associated with eutrophication. Therefore, there is, as yet, no robust evidence for realized severe disruptions of marine socioecological links from ocean acidification to anthropogenic CO2, and there are significant uncertainties regarding the level of pH change that would prompt such impacts.  [D]espite the strong mechanistic or physiological basis for a role of warming in coral bleaching and coral growth, a robust demonstration of a direct causal link between global warming and global coral bleaching over decadal time scales has not yet been produced.”
Wei et al., 2015
“It is worth noting that the errors of these estimates are fairly large with RSD of 65% for that these two time-series do not show significant decreasing trend for pH. Despite of such large errors, estimated from these rates, the seawater pH has decreased by about 0.07–0.08 U over the past 200 years in these regions. … The average calculated seawater pH over the past 159 years was 8.04 [with a] a seawater pH variation range of 7.66–8.40.”


(3) Dr. Soon: Thank God for fossil fuels.
Dr. Christensen: We may fly planes with bio-gas.
8:27 Dr. Willie Soon “You gotta stick to the facts.  Wind and solar – oh boy, so useful.  Every time I look at this sad situation of all these landfalling hurricanes (Irma, Harvey)…the only thing I have to say about that is ‘Thank God for fossil fuels’.  … Fossil fuels offer the most energy density.  There is no viable energy replacement.  Wind and solar could never do anything…”
9:25 Bryan Dey “You’re never gonna get a plane off the ground with wind and solar.”
9:30 Dr. Jon Christensen “You may [get a plane off the ground] with bio-gas, though.”
What the science says…
DeCicco et al., 2016
“Biofuels increase, rather than decrease, heat-trapping carbon dioxide … The researchers conclude that rising biofuel use has been associated with a net increase—rather than a net decrease, as many have claimed—in the carbon dioxide emissions that cause global warming.”

(4) Dr. Christensen: In CA, we believe, we’re
doing something, and we’re helping the poor
17:03 Dr. Jon Christensen “The majority of people in every congressional district in the United States believes that climate change is real, it’s caused by people, it will harm people in the future, and we should do something about it.  […] Great vast majorities of people in California [are believers], where we have decided we’re going to do something about it…we’re on that path which I call the California way or the Paris way where we continue to make commitments, continue to raise our commitments, continue to ratchet down…”
17: 49 Bryan Dey “But all the [green] solutions that come from the Left are really going to be punishing poor people the most…”
17:54  Dr. Jon Christensen “No, they’re not, actually…”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




17: 56 Bryan Dey (adamant) “Yes, they will!  It’s poor people that are going to suffer the most!”
Californians in the audience clap and cheer
18:00 Panelist “We’ve got people [in the audience] who love poor people.”
18:03 Dr. Willie Soon “Oh, poor people are clapping.  Can I say something?”
18:12 Dr. Jon Christensen “Look at the laws in California.  Every single law that is being passed about climate change, cap-and-trade, environmental… has explicit language that is to make those benefits go to poor, disadvantaged communities…”
Californians in the audience shout “No, not true!”
18:30 Bryan Dey “If your electric bill goes up by 20 percent…”
18:34 Dr. Jon Christensen (to audience) “I don’t know who’s saying that [green policies don’t help poor people] but I can show it to you — this is what I do my research on.”
28:20 Question from audience “Do you think it’s appropriate for the government to use force and tax penalties which really affect middle income and lower income people based on climate science that is obviously hotly contested?”
28:50 Dr. Jon Christensen “In California, anyway, … [politicians] make sure that those burdens do not fall on lower income people.”
What the science says…
Environmental Progress (February, 2018)
“The burden of higher cost electricity and benefits of renewable energy subsidies fall unevenly on Californians. Between 2007 and 2014, the highest-income 40 percent of California households received three times more in solar subsidies — valued between $10,000 and $20,000 per household — as the lowest-income 40 percent. California households with over $100,000 in annual income benefited from energy efficiency subsidies at twice the rate of households whose income was under $50,000.”
Poorest households hit hardest by UK climate change levy despite using least energy (March, 2018)
“We found that, in a year, the richest households each consumed on average the same amount of energy that would be produced by 12.7 tonnes of oil, compared to 3.3 tonnes for the poorest households. But the poorest spent a much greater proportion of their income (10%) on energy than the richest (3%). And the energy used for heating and powering their homes – the part that their climate change levy bill is measured on – represented a much greater proportion of their overall energy use.”
“This means that adding the climate change levy to household energy bills hits the poorest households hardest. Energy bills account for a much greater share of their household income and more of their energy use is charged. In fact, the levy only affects a quarter of the total energy consumption of the richest households, compared to 53% for the poorest households. As a result, the richest homes use nearly four times more total energy than the poorest but only pay 1.8 times more towards energy policy costs.”

(5) Dr. Christensen: CA economy is growing
25:55 Dr. Jon Christensen “California’s economy has been growing while emissions have been decreasing.”
What the science says…
Los Angeles Times (January, 2018) 
“Guess which state has the highest poverty rate in the country? Not Mississippi, New Mexico, or West Virginia, but California, where nearly one out of five residents is poor.”

(6) Dr. Soon: Science does not work by consensus
19:13 Dr. Willie Soon “I’m very, very sorry.  Science does not work by consensus.  …  This nonsense about consensus…”
19:50 Dr. Willie Soon “This 97 percent consensus… We have published a peer-reviewed paper (Legates et al., 2013) that shows that it’s only 41 papers out of 12,000.  So it’s only 0.3 percent.”
What the science says…
Legates et al., 2013
“Cook et al. (2013), after a subjective review of only the abstracts of 11,944 papers on climate change which ‘‘matched the topics ‘global climate change’ or ‘global warming’’’ (p. 1), conclude that 97.1 % of those that expressed an opinion endorsed the hypothesis as defined in their introduction (i.e., the standard definition). However, 66.4 % percent of the abstracts had expressed no position. Thus, 32.6 % of the entire sample, or 97.1 % of the 33.6 % who had expressed an opinion, were said to be in agreement with the standard definition. However, inspection of the authors’ own data file showed that they had themselves categorized only 64 abstracts, just 0.5 % of the sample, as endorsing the standard definition [a majority of the warming since 1950 was human-caused]. Inspection shows only 41 of the 64 papers, or 0.3 % of the sample of 11,944 papers, actually endorsed that definition.”

(7) Dr. Soon: CO2 increase is greening the planet
21:20  Dr. Willie Soon “One of the most powerful effects of carbon dioxide is not on temperature because if you talk about greenhouse gases it’s water vapor that’s more important [than] CO2. […]  The only proof we have so far is that it [CO2] is greening the planet.  Twenty to fifty percent of the [Earth’s] vegetated area has been greening, only 4% has been showing a little browning.  That tell[s] you that the overwhelming effect of this [increase in CO2] is fertilization of the atmosphere. […] We’re [currently] in a CO2 starvation state.  Today our air has only 400 parts per million.  If you don’t know what that means, it’s 4 cents for every hundred dollars.”
What the science says…
Zhu et al., 2016
“Global environmental change is rapidly altering the dynamics of terrestrial vegetation, with consequences for the functioning of the Earth system and provision of ecosystem services.  Yet how global vegetation is responding to the changing environment is not well established. Here we use three long-term satellite leaf area index (LAI) records and ten global ecosystem models to investigate four key drivers of LAI trends during 1982–2009. We show a persistent and widespread increase of growing season integrated LAI (greening) over 25% to 50% of the global vegetated area, whereas less than 4% of the globe shows decreasing LAI (browning). Factorial simulations with multiple global ecosystem models suggest that CO2 fertilization effects explain 70% of the observed greening trend, followed by nitrogen deposition (9%), climate change (8%) and land cover change (LCC) (4%). CO2 fertilization effects explain most of the greening trends in the tropics, whereas climate change resulted in greening of the high latitudes and the Tibetan Plateau.”

(8) Dr. Christensen: The goal is to increase the cost of carbon
29:30 Dr. Jon Christensen “A lot of what’s happening is figuring out ways to use the market to increase the cost of carbon…  (Audience: No! No!) …to take into account the cost that we’re paying in health and environmental effects…which are externalities that have not been factored in. … Those revenues [from increasing the cost of carbon] are used to benefit the whole state of California with particular attention to people who are lower income.”
What the science says…
Environmental Progress (February, 2018)
“Between 2011 and 2017, California’s electricity prices rose five times faster than they did nationally. Today, Californians pay 60 percent more, on average, than the rest of the nation, for residential, commercial and industrial electricity.”

“California’s high penetration of intermittent renewables such as solar and wind are likely a key factor in higher prices.”
“Economists agree that “the dominant policy driver in the electricity sector [in California] has unquestionably been a focus on developing renewable sources of electricity generation.”
“High levels of renewable energy penetration make electricity expensive around the world, not just in California. As Germany deployed high levels of renewables over the last 10 years it saw its electricity prices rise 34 percent. Today, German electricity costs twice as much as that in neighboring France.”
“As wind and solar capacity climbs, the returns of usable power diminish because of increasing curtailment during surges that the grid cannot absorb. More and more intermittent capacity has to be pushed onto the grid to get less and less additional renewable electricity. The dynamic of soaring overcapacity and falling prices is the inevitable result of the fundamental inability of intermittent wind and solar generators to efficiently match supply to demand.”
Share this...FacebookTwitter "
"As I write, immense protests are taking place in India against the new anti-Muslim law and Hong Kong activists, who have been protesting for their own rights for months, stand in solidarity with the Uighur people being persecuted on the other side of China. The decade will end in protest. But who can look back a decade when a week in Trump time is like a century, and hardly anyone can remember the overstuffed chaos of the month before, let alone 2017, to say nothing of the remote era before he was president? Seriously, people keep forgetting what came before, which is why they fail to recognise patterns, consequences and the real power of movements. For instance, the wave of feminism called #MeToo is often treated as a sudden eruption out of nowhere when in fact it came out of a very specific somewhere: a ferocious upsurge of global feminism over the past decade that had been spawning news, protests, hashtags and action about feminism before #MeToo in 2017. That upsurge was itself the culmination of feminist analysis and action for decades before. All that happened in October of 2017 was that movie stars got involved.  But my real fear is that the 2010s will, like the 1980s, be misremembered through oversimplification. People dismissively say the 1980s were “Reagan”, as though several billion people on several continents were one reactionary old white man in America. Ronald Reagan was horrible, and his regime launched the reversal of decades of progress towards economic equality and security in the United States. But beyond and all around, the 1980s saw remarkable activism with immediate consequences – the overthrow of the Marcos regime in the Philippines through people power in 1986, the overthrow of the South Korean military dictatorship in 1987, the toppling of the whole eastern bloc of Soviet states in 1989, the beginning of the end of the apartheid era in South Africa (and powerful but unsuccessful uprisings in Burma and China). But a lot of groundwork was also laid for what was to come, with feminism, Aids activism and queer rights organising, and the beginning of a profound shift toward recognising racial and social issues in the environmental movement. Even deeper than that was the evolution of new, inclusive, less hierarchical, nonviolent organising strategies that rejected some of the failed tactics and principles of past activism and have been important ingredients in movements ever since. So one could dismiss this decade as the rise of Donald J Trump and authoritarians around the world (and yes, there have been plenty of them, from the Philippines to Hungary). But there have also been plenty of moves in the opposite direction. If protests had a slow start in the teens, they woke up fast with the Arab spring in January of 2011, one of the most powerful waves of anti-authoritarianism the world has ever seen. Regimes toppled in Tunisia, Egypt and Libya, with protests spreading from Sudan to Iraq. Of course the Syrian version turned into the long nightmare of civil war, and many of the nations involved in the Arab spring did not end up better in any simple way. But the protests made clear that even dictators backed by armies are not invulnerable, that ordinary people together sometimes have extraordinary power, that the longing for democracy is powerful in the Islamic world and that history is sometimes written by the vanquished when they cease to be vanquished. In October of the same year came Occupy Wall Street. The feminist upheaval has been global, with significant eruptions in Chile, Mexico, South Korea, Japan, Pakistan, Kenya and beyond in this decade. And Occupy was influenced by the Arab spring and anti-capitalist movements in Greece and elsewhere. And eventually outposts of Occupy were established in cities from Kyoto to Auckland to small towns in Alaska. The climate movement grew in power, reach and sophistication, often led by indigenous people, from the Arctic to Ecuador to the South Pacific and beyond. It became a powerful force that needs to grow yet more so in the next year and needs to win in the next decade. But what has been more important than any single movement in this decade is disillusionment – and I mean that in the positive sense: of letting go of illusions. Black Lives Matter, founded in 2013, and other anti-racist movements around the world shattered the sense that racism was over and linear progress was trustworthy and inevitable. Feminism went deeper into the nature of oppression and rose higher in its demands for equality. Same-sex marriage came into law in Argentina, Mexico, Iceland and Portugal in 2010, followed by many other countries including Britain and the US later in the decade. And the question of what equality meant for LGBTQ people also went deeper. So did questions about how gender is constructed and deconstructed as trans rights grew in visibility. What lay underneath all this disillusionment was a readiness to question foundations that had been portrayed as fixed, inevitable, unquestionable – whether that foundation was gender norms, heterosexuality, patriarchy, white supremacy, the age of fossil fuels or capitalism. To see beyond what we had seen before, or to change the “we” whose perceptions define the real, the important and the possible. With this came a capacity to understand more complex, subtle and hidden forms of oppression, and to think – encapsulated in that beautifully valuable word, contributed in 1989 by law scholar Kimberlé Crenshaw intersectionally – about how multiple identities overlap (and thus do multiple forms of oppression or privilege). The decade began in the wake of global economic collapse, and Occupy Wall Street was one of the reactions to the sheer greed, destructiveness and shortsightedness of the financial system. That the current economic arrangements don’t work for ordinary people has also prompted protests that don’t fit into a left framework. These included the gilets jaunes protests in France, the people who voted for Trump in the belief that he was an economic populist and the British voters who said yes to Brexit because they felt the system didn’t work for them. A surprising late-in-the-decade form of resistance has arisen among the employees at Facebook, Amazon and Google, protesting aspects of their corporations’ amorality. Employees at all three walked out as part of the September climate strike. The climate movement is inevitably an anti-capitalist movement. That capitalism is the best or only way to do things was, in the triumphalism after the collapse of the Soviet Union, affirmed again and again. That mood fell apart in the wake of episode after episode of corruption, destruction and failure – and the rise of a young generation ready to rethink the alternatives and, often, embrace versions of socialism. The nonviolent strategist George Lakey argues that polarisation brings clarity and a volatility that makes positive change more possible. We have the polarisation and the disillusionment, and with perspective about how we got here and when we won, we can claim the possibilities in the decade to come. • Rebecca Solnit is an author and journalist. Her latest book is Whose Story is This? Old Conflicts, New Chapters"
"
Share this...FacebookTwitterThe article that follows below is from the UK online Independent here.
Regrettably the editors screwed up and inserted the wrong English text in the video and misidentified the participants as well. The discussion is in fact about CO2 being the sole real driver of global climate. Moreover, the talk round took place in London, and not Egypt.
This is not fake news, but merely a case of human error on the part of the Independent. It happens. 🙂
As a public service, we at NTZ have corrected the article by the Independent (below). Again, pay no attention to the erroneous English text in the video, which is really about a climate skeptic, James Delingpole, at a Talkshow presenting an alternative theory on why the globe is warming a bit.
================================
Corrected version of the Independent article: 
Climate denialist kicked off live Climate TV show for ‘inappropriate’ ideas: ‘Go straight to a psychiatric hospital’
‘We cannot promote such destructive ideas…you set a very bad example for the World’s youth’

A climate skeptic has been kicked off a live TV show taped in London after the host accused him of being ‘confused and unreliable’ and being in need of psychiatric treatment.
James Delingpole was presenting his reasons for being climate science skeptic on Current TV (co-founded by Al Gore) when host George Abd Al-Halim Monbiot told him he was being ‘inappropriate’.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sheikh of Al Mahmoud Gore, who was also on the programme, agreed, telling him: ‘Look dear James, you need psychiatric treatment. Many young people today suffer from mental illnesses due to material or mental circumstances.’
When Delingpole suggested ‘the sun‘ as an explanation for climate change, George Abd Al-Halim Monbiot exploded and demanded that he ‘stop using difficult words’ and reminded him that he was ‘addressing simple people and to not use big words for no reason’.
George Abd Al-Halim Monbiot then accused James: ‘You deny the existence of CO2 as the climate driver and reject our religion and science.’
Next Mr. Al-Halim Monbiot demanded Mr. Delingpole to leave, saying: ‘We cannot promote such destructive ideas…you set a very bad example for the world’s youth.’
He added: ‘I advise you to leave the studio and go straight to a psychiatric hospital.’
Blasphemy and denialism are illegal in climate science, and prosecution is routinely recommended by NASA GISS, PIK and NOAA if people should insult or defame Climatism and CO2 under proposed climate science RICO laws.
======================================
We at NoTricksZone are glad to have been able to get this cleared up. As you can see, the video now makes perfect sense.
 
Share this...FacebookTwitter "
"Almost every climate scientist agrees human-caused climate change is a major global threat. Yet, despite efforts over the past 30 years to do something about it, emissions keep increasing. Any successful coordinated international response will require action from businesses. However, some organisations, especially those in sectors that significantly contribute to environmental degradation such as the oil industry, seem rather reluctant to embrace the challenge. Those climate initiatives they have embraced were more often than not prompted by litigation risks or enforced by governmental policies rather than a result of an intrinsic “green” commitment. This isn’t the impression the industry likes to give off, of course, and it’s no wonder oil companies’ statements on corporate social responsibility and environmental reporting tend to highlight their greenest side. Yet the fact these documents give the oil firms the opportunity to construct their own narrative means they are a useful source for my research in applied linguistics. When a huge volume of language is analysed, features and patterns can emerge that would be invisible to the casual human reader.  My latest study looked at the “climate change reality” constructed by oil industry in its corporate reporting, what language was used to create this reality, and how this changed over time. This sort of analysis of language is important. Language not only mirrors the social world but acts as a lens through which objects, situations and people are given meaning. Features and associations that are foregrounded can point to some level of significance, while what is kept in the background or not mentioned at all can highlight a lack of interest.  This is why I used corpus-linguistic tools – essentially, using a computer to analyse vast amounts of text for certain patterns – to investigate nearly 500 corporate documents produced between 2000 and 2013 by major oil companies (including all the big names). This comprised some 14.8m words published in corporate social responsibility and environmental reports and relevant chapters in annual reports. That’s a lot of words – roughly equivalent to 25 copies of War and Peace.  Using software program Sketch Engine, I looked at how frequently the key corporate terms “climate change”, “greenhouse effect”, and “global warming” were used in each year to reveal how patterns of attention changed over time. My analysis shows that the most frequently adopted term in the studied sample is “climate change”, while other terms such as “global warming” and “greenhouse effect” are rarely used. The preference for “climate change” and near absence of “global warming” reflects patterns observed in public and media discourse, too. The use of the term “climate change” experienced peaks and troughs over time, with most mentions between 2004 and 2008, and fewer and fewer mentions since 2010. Less attention to climate change in public debates and overt anti-climate change attitudes on the parts of some governments in recent years might have contributed to the decline in attention given to climate change in corporate reporting. I then looked at words used alongside “climate change” to gather clues as to the company’s attitude towards it. This showed a significant change in the way it has been portrayed. In the mid-2000s, the most frequent associated terms were “tackle”, “combat” and “fight”, showing climate change was seen as a phenomenon that something could be done about.  However, in recent years, the corporate discourse has increasingly emphasised the notion of “risks”. Climate change is portrayed as an unpredictable agent “causing harm” to the oil industry. The industry tends to present itself as a technological leader, but the measures it proposes to tackle climate change are mainly technological or market-based and thus firmly embedded within the corporate world’s drive for profits. Meanwhile, social, ethical, or alternative solutions are largely absent. It seems that climate change has become an elusive concept that is losing its relevance even as an impression management strategy. The proactive stance of a decade earlier is now offset by a distancing strategy, often indicated through the use of qualifying words like “potential” or “eventual”, which push the problem into the future or pass responsibility to others. In doing so, the discourse obscures the oil sector’s large contribution to environmental degradation and “grooms” the public to believe that the industry is serious about tackling climate change."
"An American trophy hunter has kicked off another social media furore after defending a recent giraffe kill in South Africa by claiming they were “very dangerous animals”. In one sense she is right – giraffes are big and strong and you certainly wouldn’t want one kicking you. But attacks on humans are very rare. A more relevant question is whether hunting is a key threat to giraffes.
The International Union for the Conservation of Nature (IUCN) Red List assessment does not list legal hunting as a threatening process at all. However illegal hunting for meat and trophies is listed as threatening as it reduces the effective size of their protected areas and, if allowed to proceed unchecked, can cause the collapse of wildlife populations. Giraffes are popular among bushmeat poachers because of their size, high meat yield and the ease with which they can be hunted. The giraffe is currently listed as “least concern” on the IUCN Red List, but this doesn’t present the full picture. Back in 1999 wildlife expert Rod East estimated there were 140,000 in Africa – today the Giraffe Conservation Foundation estimates there are only 80,000 left. Such a rapid decline suggests they may soon qualify as being vulnerable to extinction. But why does a 40% drop in giraffe numbers not resonate worldwide? After all, everyone knows African elephants are threatened yet there are still 500,000 left in the world. So why is the giraffe being ignored? Normally, it is the uncharismatic species that decline without much public sympathy, but that doesn’t apply here. Giraffes are one of the megastars of the African savannah. Tourists love them. Children who have never been to Africa know what a giraffe looks like. It is the world’s tallest animal despite having the same number of bones in its neck as we do. It is almost comical in appearance with its orange dappled pyjama onesie – although when you feel it, giraffe skin is thick and tough. A drive through a well-managed protected area, such as Kruger National Park in South Africa, gives the impression that both elephants and giraffes are secure. You can sit at a waterhole and watch elephants cavorting in the water while a lone giraffe browses peacefully on the acacias nearby. In Zimbabwe’s Hwange National Park I once saw 32 giraffes without even turning my head. It could be that this familiarity has blinded society to the decline of the species, in addition to a lack of well-publicised trafficking busts that occurs with elephant ivory or rhino horn. But the rapid decline of giraffes isn’t the only story – because in southern Africa, populations are increasing. A major reason for this increase has been the development of wildlife ranches and the reintroduction and protection of giraffes on those lands. There are significant numbers on wildlife ranches in South Africa, Botswana, Zimbabwe, and a recent study estimated that 23,000 giraffes occupy such lands in Namibia. Ironically, many of those ranches only developed because there was potential for deriving income from trophy hunting, including giraffes. Elsewhere, though, other sub-species are faring far worse. The reticulated giraffe from Somalia, Kenya and Ethiopia has been reduced to just 5,000 individuals through illegal poaching and war. The West African giraffe in Niger had only 50 animals in the mid-1990s, but robust environmental protection has resulted in an increase to around 400 today.  The taxonomy of giraffes is currently being studied, and it may be that the dozen or so giraffe sub-species are elevated to distinct species, which would totally reform their conservation status assessments. It seems clear that to protect giraffes, we need to prevent both habitat loss and illegal hunting. These targets can be achieved through adequate management of protected area estates and through the creation of incentives for conservation on lands outside of protected areas. Trophy hunting contributes to both in some countries by generating income from and for wildlife.  The controversy over the killing of Cecil the lion highlights how much is needed to make sure legal hunting industries are adequately managed. However, until an alternative to the income from trophy hunting is found, the answer lies not in banning the practice or on clamping down on trophy imports, but in helping African countries manage the industry better."
"
Share this...FacebookTwitterWhat follows is one example why caution is absolutely essential when dealing with results and findings issued by (activist) government agencies.
Once popular diesel engines now public enemy no. 1
Nowhere in the world have the diesel engines enjoyed so much popularity as in Germany. Diesel engines had long been considered in Germany as being more environmentally friendly then the Otto type engines due to their much higher fuel mileage. Taxes on diesel fuel were and are today much lower.
But Germany has withdrawn its welcome mat for diesel engine. Like CO2, the government and environmental groups recently began waging full-scale war on diesel engines. The official reason for the crack down on diesel is the alleged high levels of dangerous emissions of nitrous oxides, and is what many suspect is mostly part of what is the overall war on the internal combustion engine and thus the effort to get people to switch to “clean” electric cars.
Ministry of Environment’s, media’s absurd claims
To underscore the risks of diesel fumes and to spread fear of diesel engines, Germany’s Ministry of Environment (UBA) recently released “new findings” claiming diesel engines are responsible for 6000 premature deaths every year. Unsurprisingly: the German press and activist groups went bananas uncritically reporting the findings in the most spectacular ways they could imagine.
For example: the Frankfurter Rundschau wrote:
Also diseases such as diabetes mellitus, high blood pressure, stroke and asthma are connected to irritant gas concentration. Eight percent of the diabetes mellitus illnesses in Germany in 2014 can be linked to nitrous oxide in the air outside: ‘That corresponds to some 437,000 cases,‘ said Myriam Tobollik, health researcher at the Ministry of Environment.”
“A political number” that “sounds like science”
Fortunately the hysteria and gross exaggerations did not escape the attention of the German press consumers, who have recently seen the value of their diesel engine vehicles plummet, and the few, still responsible journalists out there. It turns out the UBA report was based on exceptionally terrible science and the claims bordering on the absurd.
For example, Spiegel’s Jan Fleischhauer wrote here:
The made-up dead
Every year 6000 premature deaths from nitrous oxide – that’s how the Environment Ministry panicked the German citizenry. What sounds like science in truth is  a political number from a completely politicized government administration.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the Spiegel article Fleischhauer asks why aren’t other devices not targets: “A gas cooking stove during reaches peak values of 4000 micrograms per cubic meter. Where’s the campaign against the gas stove?”
It is a fact that many workplaces see routinely far higher nitrous oxide concentrations than what is measured near streets.
Measurement station folly (again), fake crisis
Fleischhauer also reminds readers that the EU directives specify that limit values for exhaust concentrations be measured at a distance of 25 meters from a busy intersection. After having looked through the UBA report, the Spiegel journalist adds:
Now I read the the measurement instruments in Germany are placed directly next to the roadway. I have not verified that. But if it’s true, then it should not be a surprise we find ourselves in a state of a diesel alarm.”
Diesel study “botched”
That the 6000 deaths a year figure was a fraud came to the attention of German mass daily Bild from its own readers. Bild was compelled on March 11 to publish the following headline:

Bild daily headline: “Reader anger over the botched diesel study”
The European Institute for Environment and Climate (EIKE) here commented that the German Environment Ministry “irrevocably ruined the reputation of the 1500-employee large agency behemoth”.
“Politicians lying, playing games”
One Bild reader, Wolfgang Bügener of Oberhausen, wrote: “It is peculiar how our politicians are playing games with and lying to us.”
Another added: “The real scam not only happened in Wolfsburg [VW headquarters] but also at the environmental organizations and Ministries, who throw around false and unproven claims.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitter‘Marked And Steady Increase’ 
In Modern Penguin Abundance

Perhaps because of their unique visual appeal and heavy representation in children’s books and movies (and climate blogs), penguins may subjectively rank second only to polar bears in their polar popularity.
The Polar Bear As ‘Global Warming’ Icon
Advocates of climate alarm have historically used images of forlorn and starving polar bears stranded on melting ice floes to spur human guilt and policy action.  In 2008, polar bears were even classified as endangered due to modeled expectations of their imminent demise.
According to recently published peer-reviewed scientific papers, however, polar bears have been defying the narrative that says dangerous anthropogenic global warming (DAGW) is targeting them for extinction.
That’s because in recent decades 92% of Canadian polar bear subpopulations have remained stable or increased, leading scientists to conclude that “it seems unlikely that polar bears (as a species) are at risk from anthropogenic global warming” (York et al., 2016).  Local Inuit populations even report that there are “too many polar bears now” (Wong et al., 2017).
How About Penguins?
Since the Arctic’s polar bears have not been cooperating with the DAGW narrative (by failing to die off in greater numbers), perhaps penguins, another beloved polar species, could take their place.  After all, the plight of Antarctica’s penguins has not received nearly as much worldwide attention or sympathy.
But scientists have found that penguins have not been cooperating with DAGW expectations either.
In recent decades, and over the course of the last 200 years, penguin numbers have either increased or remained stable.
Penguin Population Dynamics And Climate
Scientists have historically determined that increasing Adélie penguin numbers seem to coincide with warm periods, whereas cooling periods elicit population declines (Emslie et al., 2007;  Huang et al., 2009).
According to Yang et al. (2018), however, increases in penguin abundance coincide with cooling periods.  They note that there were higher Adélie penguin numbers in the Ross Sea region during the Little Ice Age (1600s to early 1800s) than during the 19th and 20th centuries.
Interestingly, though, these scientists also found that there has been no net change in penguin population since the 1800s, a determination that would not appear to fit the perspective that modern climate changes are unprecedented or even unusual.
Furthermore, when it’s considered that there has been no significant regional temperature change between the 1880s and mid-2000s, and that the Ross Sea has undergone a dramatic cooling trend (-1.59°C per decade) since 1979 (Sinclair et al., 2012), any decreasing population trend in recent decades would necessarily coincide with a cooling rather than warming climate.

In another paper published in the journal Nature Communications a few months ago, Che-Castaldo et al. (2017) analyzed 267 Adélie penguin colonies residing on the Antarctic continent and found their numbers have undergone a “marked and steady increase“ between 1982 and 2015.
Reindeer, Perhaps?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




With both polar bears and penguins perpetually failing to support the narrative invoking deep concern about the species-depleting effects of anthropogenic global warming, perhaps a new animal icon foreshadowing the dangers of climate change will emerge at some point.
Reindeer are polar animals that are seasonally quite popular.   Perhaps they could take the place of polar bears and penguins.
Or not.
Bårdsen et al., 2017     The Pursuit of Population Collapses: Long-Term Dynamics of Semi-Domestic Reindeer in Sweden … We investigated the population dynamics of Swedish semi-domestic reindeer from 1945 to 2012 at the reindeer herding district-level (Sameby) to identify possible population collapses or declines […] but found no evidence of large-scale reindeer population declines and no visible synchrony across adjacent populations. Our findings were unexpected as both reindeer populations and the pastoral lifestyle face increased habitat loss, predation, fragmentation and climate change.

Pan-Antarctic analysis aggregating spatial
estimates of Adélie penguin abundance…
Che-Castaldo et al., 2017
[A]ggregated abundance [for 267 Adélie penguin colonies] across all sites in this region showed extended periods of both increasing and decreasing abundance over the last three decades [1982-2015]. 
We also find a long-term decline in abundance in the South Orkney Islands, following an initial period of increase in the early 1980s. In contrast, we found a marked and steady increase in abundance around the rest of the Antarctic continent, including both Eastern Antarctica and the Ross Sea.
Commensurate with other studies [Lynch et al., 2013], we find that the population of Adélie penguins on the Antarctic Peninsula declined between 2000 and 2008, though we found an unexpected rebound in abundance starting in 2008. This regional increase in abundance may, in part, be driven by sites in the Marguerite Bay area, where Adélie penguins are stable or even increasing. However, this increase may also reflect a cessation of regional warming on the Antarctic Peninsula since the late 1990s [Turner et al., 2016], which may benefit ice-dependent species like the Adélie penguin. 
We find that while Eastern Antarctica appears to have been increasing steadily in abundance since at least 1982, the increasing abundance of Adélie penguins in the Ross Sea is more recent, beginning in 2002.
 


Oceanographic mechanisms and penguin population increases 
during the Little Ice Age … southern Ross Sea, Antarctica
Yang et al., 2018
Adélie penguin populations as inferred from […] southern Cape Bird declined slightly from ∼1450 to ∼1600 AD, began to rise afterward and reached their highest level in ∼1700 AD, then declined with fluctuations to the lowest levels through ∼1900 AD. For the past 100 yr, Adélie penguin populations experienced a sharp rise and drop. 
Monitoring data have shown that Adélie penguins at Cape Bird had an increasing trend in the 1970s, likely linked with changes in sea-ice extent and polynya size, but also with variation in competition with minke whales (Ainley et al., 2005; Wilson et al., 2001). Our study suggests that the penguin populations increased in the 1960s as well, consistent with their research.
Over the past 500 yr at Cape Bird, Adélie penguin populations increased during the cold period (∼1600–1825 AD), which is inconsistent with the general pattern in other studies, for example, penguin populations increased when climate became warmer, and vice versa (Emslie et al., 2007; Huang et al., 2009; Sun et al., 2000).


Share this...FacebookTwitter "
"
Share this...FacebookTwitterFirst, at Twitter here Swiss high-profile meteorologist Jörg Kachelmann presented a video on how he thinks German public television failed to adequately warn the public before North Sea storm Xavier barreled through northern Germany on October 5.

Swiss meteorologist Jörg Kachelmann says he believes German media (image above) inadequately informed the public of the danger of storm ‘Xavier’. Seven Germans lost their lives to a storm that was “nothing out of the ordinary”. Image: ZDF German Public Television.
Media warnings of storm were inadequate
As a result 7 people were killed by falling limbs and trees. Some of these deaths could have been prevented had the media issued stronger warnings of the storm’s danger before it hit, believes Kachelmann, who in his twitter video pointed out that Xavier was not an unusual storm by any means and that there should not have been so many deaths.
Kachelmann said:
Why it happened has a bit to do with the media and what they could do. That’s the big difference to the USA when you look at the reporting there concerning hurricanes or even tornadoes, where the main reporting does not come after everything has happened — after all the deaths, injuries and everything laying around — but the main reporting is before where people are helped and told what to do and accompanies the people during this time with reporters out there in the storm with microphones, which here is ridiculed. But it helps.”
Kachelmann says the media here could have done this too to make the danger clear to people: “They could have done this here too. The storm did not just come as a surprise.”
The media hype comes afterwards, when it’s too late
As to why the German press did so little to warn the people of the storm, Kachelmann can only speculate: Maybe they were just “infinitely lazy“. Kachelmann thinks that had the media given stronger warnings, some of the lives would have been spared.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The average observer could say that German public media seems to have a habit of underrating storms before they hit, and then exaggerating them after they leave. For example, on Monday earlier this week – after Ophelia had already hit Ireland – North German NDR radio presented Ophelia as something that no one in Ireland “could recall ever happening“.
Certainly a bit of hyperbole here.
Here one could successfully argue that this is an exaggeration and that the German media are simply just too lazy to look back into the archives, or just aren’t interested in presenting accurate reports. Joe Bastardi at Weatherbell on Tuesday highlighted Ophelia in his Daily Update and showed that Ophelia “was not as bad as Debbie in 1961“.

Track of Hurricane Debbie in 1961, which was worse than Ophelia. Source: cropped from Weatherbell Saturday Summary.
Maybe the pre-storm downplaying and post-storm hyping is unwittingly intended by the German media. By neglecting to warn people beforehand, they get to blare out bigger, more spectacular headlines of death and destruction after the storm passes.
Of course no one seriously thinks it’s intentional on the German media’s part, yet the bottom line is that the German public is getting a distorted reporting both before and after the storm. They deserve far better for their exorbitant mandatory public television and radio fees.
Ophelia not unprecedented
And at his Saturday Summary of October 14, the veteran meteorologist blasted the hurricane hysteria coming from the usual US activists. He shows how Hurricane Faith in 1966 remained a hurricane far north of Ireland, and didn’t peter out until it reached the North Pole! There’s nothing unusual about Ophelia. Bastardi added:
I feel very strongly about these people who are using these storms […] for their agenda, and so what I’m doing here is that I’m letting you know that I’m showing you beforehand that there is visible evidence that this has happened before.”
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterPresident Donald Trump once again rejected “global governance” and rigid multi-nation trade deals before a packed and highly enthusiastic rally in Pensacola, Florida, yesterday.

Donald Trump leaves door wide open to a fair climate deal. Photo credit: Shealah Craighead, public domain photo.
The US President re-emphasized the importance of American national sovereignty and independence from “global bureaucrats” residing in foreign countries.
In his speech the President brought up the Paris Climate Accord, a deal he refused to sign early this year, thus setting off outrage among the UN, climate activists, global bureaucrats and Accord cash-beneficiary countries worldwide.
“Would have been one of the great catastrophes”
He reminded the packed audience that he had promised “to withdraw…from the horrible Paris climate accord” — another promise he has kept.
He said: “It costs us a fortune. China doesn’t start until 2030, I think. Russia doesn’t have to go back to like a recent date; they go back to somewhere like the 1990s, which was a high pollution time. Other countries we end up giving money to. This would have been one of the great catastrophes.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“I could come back to the Paris Accord”
But Trump left the door open for a better deal, one that would be much fairer: “And I can come back into the deal at a much better price. I could come back to the Paris Accord.”
Trump then repeated his commitment to “clean air” and “crystal clean water”. He said that if the USA pledged to accept the numbers set forth by the Paris Accord, “We would have to close factories and businesses in order to qualify by 2025. […] In the meantime some of those countries are spewing out stuff like you wouldn’t believe.”
US has already significantly cut emissions
Over the past decade the USA has already substantially cut back its “greenhouse gas” emissions, by an amount that equals Germany’s total annual output – something that never gets mentioned in the media. Paris Climate Accord-promoting Germany on the other hand, has not cut its emissions in almost a decade. Currently the USA emits roughly half as much as China does.
Performance by other climate protection-preaching western countries is not always what it is made out to be. Many western countries are meeting their targets by merely offshoring their CO2-emitting industries over to low environmental standards regions like China and India. The result: global CO2 emissions are in reality higher rather than lower.
Inefficient wind energy
Trump then mocked wind energy as being unreliable and inefficient, saying: “Windmills are wonderful. But you know when the wind doesn’t blow, they really do cause problems. ‘We have no energy this week! Well hopefully the wind will start blowing pretty soon!'”
Share this...FacebookTwitter "
nan
"After years studying the climate, my work has brought me to Sydney where I’m studying the linkages between climate change and extreme weather events. Prior to beginning my sabbatical stay in Sydney, I took the opportunity this holiday season to vacation in Australia with my family. We went to see the Great Barrier Reef – one of the great wonders of this planet – while we still can. Subject to the twin assaults of warming-caused bleaching and ocean acidification, it will be gone in a matter of decades in the absence of a dramatic reduction in global carbon emissions.  We also travelled to the Blue Mountains, another of Australia’s natural wonders, known for its lush temperate rainforests, majestic cliffs and rock formations and panoramic vistas that challenge any the world has to offer. It too is now threatened by climate change. I witnessed this firsthand. I did not see vast expanses of rainforest framed by distant blue-tinged mountain ranges. Instead I looked out into smoke-filled valleys, with only the faintest ghosts of distant ridges and peaks in the background. The iconic blue tint (which derives from a haze formed from “terpenes” emitted by the Eucalyptus trees that are so plentiful here) was replaced by a brown haze. The blue sky, too, had been replaced by that brown haze. The locals, whom I found to be friendly and outgoing, would volunteer that they have never seen anything like this before. Some even uttered the words “climate change” without any prompting. The songs of Peter Garrett and Midnight Oil I first enjoyed decades ago have taken on a whole new meaning for me now. They seem disturbingly prescient in light of what we are witnessing unfold in Australia. The brown skies I observed in the Blue Mountains this week are a product of human-caused climate change. Take record heat, combine it with unprecedented drought in already dry regions and you get unprecedented bushfires like the ones engulfing the Blue Mountains and spreading across the continent. It’s not complicated. The smoke is so thick in Katoomba tourists are opting for photos with billboards, rather than the Three Sisters themselves. @abcsydney pic.twitter.com/MOWvH8UBgs The warming of our planet – and the changes in climate associated with it – are due to the fossil fuels we’re burning: oil, whether at midnight or any other hour of the day, natural gas, and the biggest culprit of all, coal. That’s not complicated either. When we mine for coal, like the controversial planned Adani coalmine, which would more than double Australia’s coal-based carbon emissions, we are literally mining away at our blue skies. The Adani coalmine could rightly be renamed the Blue Sky mine.  In Australia, beds are burning. So are entire towns, irreplaceable forests and endangered and precious animal species such as the koala (arguably the world’s only living plush toy) are perishing in massive numbers due to the unprecedented bushfires. The continent of Australia is figuratively – and in some sense literally – on fire. Yet the prime minister, Scott Morrison, appears remarkably indifferent to the climate emergency Australia is suffering through, having chosen to vacation in Hawaii as Australians are left to contend with unprecedented heat and bushfires. Morrison has shown himself to be beholden to coal interests and his administration is considered to have conspired with a small number of petrostates to sabotage the recent UN climate conference in Madrid (“COP25”), seen as a last ditch effort to keep planetary warming below a level (1.5C) considered by many to constitute “dangerous” planetary warming. But Australians need only wake up in the morning, turn on the television, read the newspaper or look out the window to see what is increasingly obvious to many – for Australia, dangerous climate change is already here. It’s simply a matter of how much worse we’re willing to allow it to get. Australia is experiencing a climate emergency. It is literally burning. It needs leadership that is able to recognise that and act. And it needs voters to hold politicians accountable at the ballot box. Australians must vote out fossil-fuelled politicians who have chosen to be part of the problem and vote in climate champions who are willing to solve it. Michael E Mann is distinguished professor of atmospheric science at Pennsylvania State University. His most recent book, with Tom Toles, is The Madhouse Effect: How Climate Change Denial Is Threatening Our Planet, Destroying Our Politics, and Driving Us Crazy (Columbia University Press, 2016)."
"Puffins are facing a perilous future. Population numbers have fallen sharply, and there are even fears the sea bird could be heading towards extinction within the next 100 years.  A much loved and enigmatic creature, puffins are easily identified by their wonderfully coloured beaks. They waddle around in a characterful fashion and make the strangest of noises. Their endearing features have been used as the symbol of children’s books, and to illustrate many stamps – but they are now also appearing on lists of endangered species.   On Britain’s Farne Islands, numbers have gone down 12% on average over just five years, with one island’s population falling by 42%. The common puffin, named after its puffed-up swollen appearance (although its scientific name, Fratercula arctica, arises from its resemblance to a friar wearing robes) has an extensive range across the northern hemisphere, with breeding colonies from Norway to Newfoundland.  Around 90% of the global population is found in Europe, with 60% of the population breeding in Iceland (which is also home to a tradition which involves children rescuing young, wayward puffins – “pufflings” – and returning them to the safety of the sea). The UK is home to 10% of the global puffin population, breeding on many islands and mainland coastal areas. Although there are around 450,000 puffins in the UK, the species is threatened with extinction due to their rapid and ongoing population decline. Recent surveys of the Farne Islands revealed that despite a steady increase over the previous 70 years, numbers have declined by as much as 42% over the past five years. Unfortunately, we know very little about the ecology of the puffin outside the breeding season. Although the birds amass in large numbers to breed, they spend two-thirds of their life alone, out in the north Atlantic sea. Consequently, they are very difficult to monitor. Firstly, although puffins live for a fairly long time (the oldest recorded so far reached the age of 34), their breeding population is limited to a small number of sites. They also have a low reproductive rate, laying just one egg a year, which makes them particularly vulnerable to adverse changes in the environment and means they can take a long while to recover from negative impacts. They are also hunted – by humans and other animals. Smoked or dried puffin is considered a delicacy (or a flavouring for porridge) in some places, such as Iceland and the Faroe Islands. But although they were once over harvested by people, hunting is now maintained at a sustainable level. During the breeding season, puffins nest in burrows on clifftops. Although this offers the nest protection from aerial predators, such as gulls, chicks and eggs are not safe from mammals, including weasels and foxes. On Lundy Island in the Bristol Channel, the population of puffins fell to just 10 pairs, but since the eradication of rats there, things are looking up. Nevertheless, the Arctic skua can be a particular problem as it steals food from adult puffins which is intended for their young. Living on the open ocean makes the puffin highly susceptible to pollution such as oil spills. After the Torrey Canyon oil spill in 1967, the number of puffins breeding in France the following year decreased by a massive 85%. The puffin feeds almost entirely on small fish, including sandeels, herring and capelin, which make up over 90% of the diet of pufflings.  The birds have a specialised beak with backwards facing spines, which prevents their prey (up to around 60 fish at a time) from falling out of their mouths when foraging. But in years where the main food source is low, many chicks starve to death.   Puffins have also suffered increased mortality from the rising frequency and intensity of extreme weather events associated with climate change. A recent succession of severe storms caused 54,000 seabirds, half of which were puffins, to be washed up along coasts. Starvation was cited as the main cause of death.  Sea temperatures have increased over the past 30 years, causing indirect effects on puffin survival. The rise in temperature decreases the abundance of plankton, which in turn leads to a reduction in the growth and survival of young sandeel and herring on which the puffins rely, particularly during the breeding season. Conditions in the North Sea are even causing some puffins to travel into the Atlantic, rather than the North Sea, in search of food – a perilous trek involving greater distances and different habitats. It seems that a combination of factors are to blame for the decline in puffins, but the reduction in their food supply, particularly as a result of increased sea temperatures, appears to be the main culprit.  We need to continue monitoring puffins worldwide to better understand factors affecting populations. Hopefully, we can put measures in place to minimise pollution, reduce introduced predators and promote sustainable harvesting to try and ensure that the fate of this wonderful bird is not the same as that of the dodo."
"The UK government has published a new clean air strategy for consultation. The document sets out plans to tackle emissions from a range of sources, including agriculture, industry and even wood-burning stoves. It all adds up to a subtle but important shift in emphasis away from simply meeting air quality targets to also reducing wider impacts on health and the environment. In Britain, much of the debate about air quality has focused on roads and the issue of local roadside hotspots where air pollution exceeds legal limits. It is no surprise that the government’s critics have repeatedly pointed to a lack of action on diesel cars.  While this is understandable, it is one area where things are slowly getting better, partly through local actions and partly because, on average, newer vehicles actually do emit less pollution than the vehicles they are replacing. The highest concentrations of nitrogen dioxide (NO₂) in UK cities were typically seen in 2010 and in many places have declined since then. Some of the forecasts of future road transport emissions may also have been overly pessimistic. The strategy is banking on this trend continuing, and it restates the long-term target of phasing out fossil fuel-only vehicles by 2040.  This is part of a wider focus which acknowledges that air pollution is much more than just a roadside problem. One eye-catching component concerns fine particulate matter, known as PM2.5 since the particles are less than 2.5 micrometers across. These are really too tiny to see with the eye, yet present a major health risk as particles this small can easily find their way deep into the lungs and finally the bloodstream.  The strategy now recognises the most stringent World Health Organisation limits for PM2.5, and includes an ambition to halve the number of people living in areas with concentrations above that limit. This would mean the UK was working toward meeting tougher PM2.5 standards than virtually every other industrialised nation. The strategy sets out plans for reducing emissions across different source types rather just tackling each pollutant in isolation, in some cases with proposals for specific actions, in others more general ambitions for reductions. Interesting examples in the strategy include a proposed gradual retirement of diesel trains and the voluntary labelling of solvents in consumer products. It also looks serious about finally tackling the long-standing issue of ammonia emissions from agriculture, and it raises the emerging health issue of managing indoor air quality. This sort of multi-pollutant approach makes sense, since different classes of chemical can interact with one another to form secondary air pollution. For instance, nitrogen oxides (NOx) from combustion combine with ammonia from farming to create a substantial fraction of the particulate matter that is found in the air. That same NOx can also combine with gaseous solvents to form ozone. Real improvements can only be achieved by simultaneously reducing emissions from these disparate sectors. While the government will always be measured first against its ability to deliver good air quality at a local level, the strategy reflects that the health and ecological impacts do not stop at the borders of individual countries. Although less reported on than the ambient air quality standards, the EU also sets specific limits on the total emissions that each country can make, to minimise the spread of pollution between countries.  The need to manage air pollution at an international level is frequently cited in the new strategy. Limits on pollution emissions from each EU country are set in the National Emissions Ceiling Directive (NECD) and are straightforward to understand, if complex to actually estimate or measure. On this issue Brexit will not change things since the UK’s commitments to reduce emissions are mirrored in the standalone UNECE Convention on Long Range Trans-boundary Air Pollution, to which the UK (along with 50 other countries, including the US and Canada) is a signatory.  The location of the UK means that it has the good fortune to be less affected by trans-boundary pollution than many other countries – there is a very large and generally very clean Atlantic ocean upwind. But even Britain feels the effects on occasion and many of its smoggiest days, particularly in southern England, are exacerbated by pollution flowing in from mainland Europe. A focus in the strategy on reducing emissions, even when local air quality targets have been met, reflects that UK has signed up to new binding limits for trans-boundary pollution in 2020 and 2030, and these will require close to a halving of emissions of some pollutants. One reason why NECD has rarely been in the headlines is that up until now, meeting these national targets has been quite straightforward, as compared to ambient concentrations where limits have been frequently exceeded. By the 2020s it seems likely that ambient concentrations will have fallen further. At this point, meeting the increasingly tough international emissions targets may become the new legal compliance challenge. There will no doubt be debate over whether the strategy moves far or fast enough to clean up the outstanding urban hotspots. Quantifying the broader success, or failure, of the strategy over the long-term will however be complex. It’s relatively easy to measure the concentration of pollutants in the air, but it is more difficult to devise metrics that capture all the intended benefits, such as improvements to health, productivity and to the wider environment. This is where the government will need to work hard to convince those that may have to pay, that investment in emissions reduction is money well spent."
"Earlier this year, an unusual weather pattern dubbed the Beast from the East covered much of Britain in heavy snow. But once the beast had passed, things soon returned to normal and, at the beginning of March, the temperature in London jumped by more than 10℃ in just two days. Water pipes that had been frozen solid quickly thawed, and the sudden flood soon overwhelmed the capital’s creaky infrastructure, causing many pipes to burst. More than 20,000 homes in the city were left without water, and residents had to queue for handouts.  Could this become a common sight in future? The UK’s Environment Agency certainly thinks so, as it warns in a new report that England could suffer major water shortages by 2030 and that London is particularly at risk. The BBC agrees, placing London on its recent list of 11 cities most likely to run out of drinking water along with the likes of Cape Town, where an ongoing water crisis has caused social and economic disruption.  London is unlikely to experience such shortages this summer. It is the winter (not the summer) weather that determines whether or not the city runs out of water, and winter 2017/18 had plenty of rain. But what happens after a dry winter? At Oxford’s Environmental Change Institute, colleagues and I have addressed the question of how to prevent London from becoming the next Cape Town. Our research shows that if no action is taken the city is indeed set to experience more frequent and severe water shortages in the future. This is mainly down to population growth, but climate change complicates things further as it will mean more frequent and intense droughts. In agreement with the plans developed by Thames Water – the private utility responsible for providing water and sewage services for most Londoners – our research shows that aggressive demand management to reduce consumption and losses in the distribution system (called leakage) is a priority to be implemented immediately. But reducing leaks from London’s old water pipes is not an easy task. Over the past few years, Thames Water has missed its leakage reduction targets. In 2017, the failure to meet these leakage reduction commitments cost the water company an £8.55m fine from the water regulator, only a fraction of the £100m the water company paid investors in dividends in the same year. Recognising the scale of investment and effort required, the company now says it is directing all its resources towards upgrades and maintenance rather than dividends. But there are limits to what can be achieved just by fixing leaky pipes or getting people to water their lawns less often. Though such measures are useful, they will not safeguard London’s water supplies against the more extreme combinations of growth and climate change.  Instead, the city’s water managers have been thinking about innovative ways to augment supplies. Potential solutions include building new reservoirs or transferring water from other parts of the country. More radically, London could start recycling its wastewater back into the river Thames. This would involve advanced treatment of wastewater from a sewage treatment works that is then returned to the Thames river downstream of an abstraction point. This would allow for more abstraction upstream, without compromising the environment’s water needs. How should London choose between these different alternatives? The city needs something that’s not too expensive, that keeps residents happy with the price, taste and appearance of the water, while also reducing the risk of the taps running dry.  My colleagues and I looked at the various options – new reservoirs, water transfers, desalination and recycling – and the model we developed shows that the recycling of treated wastewater back into the river makes most sense from an economic and risk reduction standpoint. Water recycling works in Singapore, where water is reused time and again, thus closing the loop between supply and demand – an example of the circular economy. Yet all this requires a change of thinking. Traditionally, investments in new pipes or reservoirs are based on estimates of future water availability and needs. These estimates are based on past observations, which means that water engineers look at how much rain there was in the past and then assume that there will be as much in the future. Typically, this results in infrastructure that delivers a secure supply of water at the lowest cost possible – under “normal” conditions. However, the future will be significantly different from anything imagined when water supply systems were first built. We will have to leave more water in the rivers for aquatic ecosystems to thrive. We will have to deal with more erratic rainfall. To prevent London from becoming the next Cape Town, individual residents will have to use water as wisely as possible. And their water managers will have to focus on what will work even in an era of significant climate change."
"In the mid-20th century pilot whaling still took place in many north Atlantic nations such as the US and Canada. Now, only the Faraoese have a dedicated pilot whale hunt, the grindadráp. Many of us don’t like the idea of this. I am a scientist. I do not profit from the pilot whale hunt nor do I have anything to gain by writing this article. Indeed, I risk retaliation from those that feel what I say departs from the accepted mantra. I study and work with dolphins and whales and for a while I spent more time around dolphins than people. For no logical reason, these animals are special to me, and that they are hunted upsets me. But these are personal opinions which have no place in this debate – a debate that is too easily ruled by emotions. The Faroese catch around 900 pilot whales, actually a type of dolphin, every year. This catch level does not threaten the conservation status of this population estimated to have more than 750,000 whales. Often forgotten or ignored is that an estimated several hundred pilot whales from the same populations are drowned every year in the nets of our fishing fleets. The scale of the Faroese pilot whale hunt is very different to the industrial whaling led by the UK and Norway during the 19th and 20th centuries which, in only 50 to 70 years, over-exploited whales in the Antarctic Ocean and drove them almost to extinction. Nor is it comparable to the commercial pilot whaling in Newfoundland from the 1950s and 1960s which over-exploited the stock. In comparison, the Faroese pilot whale hunt has continued for close to 1,000 years without over-exploitation, with records going back to 1584.  Since pilot whales are top predators in the north Atlantic, they accumulate levels of heavy metals and other pollutants that make their meat hazardous to eat. Yet the hunt is part of the social fabric of the islands, and the meat is eaten nevertheless. The Faroese pilot whale hunt is a dramatic sight. The animals are driven close to the shore in shallow bays and slaughtered with knives and lances. It results in a lot of blood in the water, clearly visible from the shore where many often gather to watch. The need for animals that we eat to be killed quickly and humanely is well understood and agreed. The pilot whale killing method was chosen to ensure that the whales die as quickly as possible, considering all the factors in the hunt.  Killing an animal is not a pleasant business, be it a whale, a deer, or a chicken. However, all welfare issues considered, I do not see how the pilot whale hunt is different from non-stalking hunts for animals on land, many of which take place in countries where opponents to the whale hunt live. Time-to-death is kept as short as possible, even if sometimes it’s longer than we would like. One thing is certain: it’s much shorter than the time it takes a pilot whale to drown in a fishing net that we use to catch our daily fish. The hunt itself is a different story. We have very recently stopped hunting foxes with dogs in the UK on welfare grounds. Driving pilot whales into bays to kill them takes time and is not unlike the process of hunting with dogs, and I think it raises welfare questions that need to be discussed. I personally have difficulties weighing these welfare questions against those raised by the industrial farming which generates most of the meat we consume in anti-whaling nations. Anyone that signs a petition to stop this hunt only to go home and roast a chicken that never saw daylight or moved much when it was reared is a hypocrite. Would it be more ethical of the Faroese to replace the wild-caught meat they have available to them with imported, industrially produced meat? Many of the arguments against the Faroese subsistence whaling should equally apply to the subsistence whaling that goes on in other countries, such as among the Inuit and Eskimo of the US and Canada and the Siberian peoples in Russia. One argument against subsistence hunting is that as the world develops, access to other food sources increases. But alternative food sources are as prevalent in these other countries as they are in the Faroe Islands. Yet the Intuit and Eskimo for example are not subject to the same criticism, and are even lauded for protecting their cultural traditions – are Faroese traditions somehow less worthy of protection? We need an unemotional public debate about all forms of whaling, and a commonly agreed definition of subsistence whaling, dietary or cultural, that is more tightly defined and less open to interpretation. The debate is too driven by emotions, with too many groups that stand to gain while whaling remains a Punch and Judy show. As Gandhi said: “Anger and intolerance are the enemies of correct understanding.” We must never again allow whaling on an industrial scale. But I enjoy my venison and I have no problem with deer hunts. I am one of the millions of hypocrites that eat meat but cannot bear the idea of killing an animal myself. I eat tuna despite its health risks – if I was born in the Faroe Islands, wouldn’t I equally enjoy my pilot whale?"
"We all know about the obvious dangers of DIY and construction work – smashed thumbs, stubbed toes and so on. Even hanging wallpaper results in 1,500 British people going to hospital every year. But the biggest threat might be invisible. Research by my team has revealed that drilling, cutting and sawing releases into the air dangerous levels of ultrafine particles. These tiny specks of dust that are 700 to 70,000 times thinner than human hair are small enough to slip through most masks and could cause serious heart and lung diseases due to prolonged exposure. Our latest study, published in the Journal of Nanoparticle Research indicates peak concentrations during drilling, cutting or sawing could reach as high as 4,000 times as much as the level at the same site when there is no work going on.  We measured the particles released by a variety of different DIY and construction activities – wall chasing (cutting grooves into a wall using an electrical tool, for example to lay electrical cables) released the most particles but, on average, up to 40-times higher concentrations were recorded during refurbishment work compared with local background levels (when no construction work was taking place). These tiny particles are especially dangerous as their large surface area relative to their size increases their potential chemical reactivity and ability to be absorbed into the human body. They can pass deep into the respiratory system, reacting with the lung tissues and potentially entering the blood stream. This isn’t an issue with larger particles (such as regular, visible dust), which cannot pass through cell membranes. We’ve previously calculated traffic-related emissions of these nanoparticles are responsible for around 40,000 deaths per year in Delhi alone, and roughly 300,000 deaths per year in Asian megacities. The problems with traffic emissions are well known – construction-related nanoparticles, less so. Yet it’s an issue that won’t go away. By 2030 the world population will hit 8.3 billion, and all those extra people will need more urban infrastructure – new construction, demolition of old buildings and renovation of existing ones. The DIY market is growing in the UK and is expected to grow even further as existing buildings show their age. At the University of Surrey we’ve looked at a series of health risks associated with this topic, highlighting for instance the high exposure of construction workers to ultrafine particles.  Our latest observations clearly indicate workers on refurbishment sites are even more exposed than those working on the roadside, even given the already high concentrations of ultrafine particles (from vehicle emissions) you’d expect to find beside a road. We need to limit the occupational hazards on refurbishment sites – builders work long hours and end up experiencing more exposure to ultrafine particles than equivalent workers outdoors or in cars. These particles were also found to travel further than their larger counterparts, putting even passers by and the occupants of nearby buildings at risk. Water sprays do a good job of suppressing dust emissions. Respiratory masks are also effective at blocking out coarse (visible) dust – however they weren’t so effective with ultrafine particles because they simply aren’t designed to protect against nano-size dust particles. While renewing rather than replacing is great for sustainability, it is less positive for those working in and around these sites. With the potential to breathe in harmful dust particles including silicon, copper and aluminium, our research shows that we need more regulatory guidelines, not only to protect construction workers, but to protect the general public. In the meantime, construction workers and those undertaking their own building projects, should always err on the side of caution and wear face masks when undertaking activities that could throw out dust. Some of the most dangerous particles are invisible and we shouldn’t underestimate the effect on our health – and on the health of those around us."
"
Share this...FacebookTwitterOne thing is clear: Germans were fooled and deceived by politicians and activists into thinking that the transition to renewable energies would not cost much, reduce pollutants, create a clean environment, improve the climate and create many jobs.
None of these have come true.
Electricity prices have skyrocketed, the landscape is being industrialized and Germany has not reduced its greenhouse gases in more than 7 years. Moreover the climate is still the same. Now Germany’s industrial base is eroding.
Today we will look at the first point: cost. Yesterday the online industry journal Deutsche Mittelstand Nachrichten (Midsize Company News) here carried the headline:
“Association of Energy: Electricity Prices To Rise Significantly”
So the bad news continue, and this will further adversely impact consumers, small businesses and the all-important Mittelstand.
And because the Mittelstand employ some 70% of all workers in Germany, most of them highly skilled and well-paid, the news is bad. The Mittelstand is already facing crisis on a number of fronts. First is the lack of skilled workers on the labor market. Second: many of these companies are now being handed down to the next generation, but there are no successors. In fact Chinese companies have been busily snapping up the companies along with their patents and technical expertise.
Now, thirdly, comes the extreme energy prices (and volatile supply) – thanks to Germany’s mad and poorly thought-out rush into utopian green energies.
Rising feed-in costs


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The main factor driving the higher prices remains the EEG green energy feed-in act, the site reports. Association head Christian Otto told German daily Bild that the feed-in subsidy will rise to 7 cents per kwh (currently 6.88 cents). 
Three of the four power transmission grid operators have already announced an increase in the grid fees.”
The four German grid operators are Amprion, TransnetBW, Tennet and 50Hertz. Only East Germany-based 50Hertz does not plan to raise the fees for the time being.
Uncompetitive
The higher feed-in surcharges make push German electricity prices to 30 cents a kwh, almost three times more expensive than power in the USA, for example. Little wonder that some are now calling to “make Germany great again”.
Moreover, citing analysts, the site reports that heating oil as well has become 11 percent more expensive, and warns more increases lie ahead as winter approaches.
Grid instability adding to costs
The Deutsche Mittelstand Nachrichten site cited the head of the BDEW energy association, Stefan Kapferer, who blasted the “constantly more frequent and expensive interventions that are needed to keep the grid stable due to the fluctuating feed-in of renewable energies“.
 
Share this...FacebookTwitter "
"…all progress in capitalistic agriculture is a progress in the art, not only of robbing the labourer, but of robbing the soil; all progress in increasing the fertility of the soil for a given time, is a progress towards ruining the lasting sources of that fertility. – Karl Marx, Capital vol 1 Following the collapse of the Soviet Union and an economic shift in China it seemed that capitalism had become the only game in town. Karl Marx’s ideas could safely be relegated to the dustbin of history. However the global financial crash of 2008 and its aftermath sent many rushing back to the bin.  For good or ill, the German philosopher’s ideas have affected our world more profoundly than any other modern social or political thinker. Yet on Marx’s recent 200th birthday, discussion of his continuing relevance was still dominated by “traditional” understandings of Marxism. Commentators, whether hostile or sympathetic, focused on his critique of the exploitation and inequality of capitalism and imperialism, and the struggle to transform society in a socialist direction.  Sadly, there was little – far too little – on Marx’s thinking on the relations between humans and nature.  After all, the steady but accelerating destruction by modern capitalism of the very conditions which sustain all life, including human life, is arguably the most fundamental challenge facing humanity today. This is most widely recognised in the shape of one of its most devastating symptoms: climate change. But there is much more to it, including toxic pollution of the oceans, deforestation, soil degradation and, most dramatically, a loss of biodiversity on a geological scale. Some will say that these are new problems, so why should we expect Marx, writing more than a century ago, to have had anything worthwhile to offer to us today? In fact, recent scholarship has demonstrated that the problematic, often contradictory relationship between humans and the rest of nature was a central theme in Marx’s thinking throughout his life. His ideas on this remain of great value – even indispensable – but his legacy is also quite problematic and new thinking is needed.  Marx’s early philosophical manuscripts of 1844 are best known for developing his concept of “alienated labour” under capitalism, yet commentators hardly ever noticed that for Marx the fundamental source of alienation was our estrangement from nature. This began with enclosure of common land, which left many rural people with no means of meeting their needs other than to sell their labour power to the new industrial class. But Marx also talked of spiritual needs, and the loss of a whole way of life in which people found meaning from their relationship to nature. The theme running through his early manuscripts is a view of history in which exploitation of workers and of nature go hand-in-hand. For Marx, the future communist society will resolve the conflicts among humans and between humans and nature so that people can meet their needs in harmony with one another and with the rest of nature: Man lives on nature – means that nature is his body, with which he must remain in continuous interchange if he is not to die. That man’s physical and spiritual life is linked to nature means simply that nature is linked to itself, for man is a part of nature. In these writings Marx makes vital contributions to our understanding of the human-nature relationship: he overcomes a long philosophical tradition of viewing humans as separate from and above the rest of nature, and he asserts the necessity for both survival and spiritual well-being of a proper, active relationship with the rest of nature. At the same time he recognises this relationship has gone wrong in the capitalist epoch. In his later writings Marx develops this analysis with his key concept of “mode of production”. For Marx, each of the different forms of human society that have existed historically and across the globe has its own specific way of organising human labour to meet subsistence needs through work on and with nature, and its own specific way of distributing the results of that labour. For example, hunter-gatherer societies have usually been egalitarian and sustainable. However feudal or slave-owning societies involved deeply unequal and exploitative social relations, but lacked the limitlessly expansive and destructive dynamic of industrial capitalism.  This concept of “modes of production” immediately undermines any attempt to explain our ecological predicament in such abstract terms as “population”, “greed” or “human nature”. Each form of society has its own ecology. The ecological problems we face are those of capitalism – not human behaviour as such – and we need to understand how capitalism interacts with nature if we are to address them.  Marx himself made an important start on this. In the 1860s he wrote about soil degradation, a big concern at the time. His work showed how the division of town and country led to loss of soil fertility while at the same time imposing a great burden of pollution and disease in the urban centres.  Modern writers have developed these ideas further, including the late James O’Connor, the sociologist John Bellamy Foster, who identified an endemic tendency of capitalism to generate an “ecological rift” with nature, and those in the UK associated with the Red Green Study Group. I suggested above that Marx’s ideas were indispensable but also problematic. There are places where he appears to celebrate the huge advances in productivity and control over the forces of nature achieved by capitalism, seeing socialism as necessary just to share the benefits of this to everyone. Recent scholarship has challenged this interpretation of Marx, but historically it has been very influential. It is arguable that the disastrous consequences of the Stalinist drive for rapid industrialisation in Russia came from that interpretation.  But there is another point. The newer ecological marxists argue, rightly, that capitalism is ecologically unsustainable, and that socialism is necessary to establish a rational relationship to the rest of nature. However, to build a movement capable of transforming society in this way, we need to recall Marx’s early emphasis on both the material and spiritual needs that can be met only by a fully rewarding and respectful relationship to the rest of nature: in short, we need a Marxism that is green, as well as ecological."
"Martin Jacques writes about the “world-transforming” effect of China’s economic growth on Europe and the US (This decade belonged to China. So will the next one, 1 January). Arguably, he’s referring to states and corporations rather than citizens, though individuals and companies are likewise enmeshed in global capitalism. Recently, Arsenal’s distancing from Mesut Özil after he raised awareness of the persecution of Uighurs in Xinjiang, and Tesco’s complicity in alleged forced prison labour, have shown how western corporations have disdained humanitarian basics in favour of international profit – perhaps an indication of what to expect from future power shifts.  However, the “existential crisis” of climate collapse might prompt global bodies to respond to the growing number of world citizens who are demanding entirely new structures. It may be that “we” westerners are less afraid of economic fragmentation and more eager to meet with the best of China’s “rich and intellectually endowed civilisation”, even as we hold the country to account.Libby RuffleWaldringfield, Suffolk • Martin Jacques’s paean to the Chinese ruling class noticeably fails to mention “democracy”, “human rights” and “free trade unions”. Nor does it mention a million Uighurs in internment camps. None of this may matter to Mr Jacques, because he believes the global dominance of China is “ultimately irresistible”.  I am reminded of Orwell’s comment about James Burnham (an intellectual who admired authoritarian regimes): “Power worship blurs political judgment because it leads, almost unavoidably, to the belief that present trends will continue. Whoever is winning at the moment will always seem to be invincible.”Jim DenhamBirmingham • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"
Share this...FacebookTwitterAtmospheric Scientists Slam Fundamentals
of the Anthropogenic Global Warming Theory

Scafetta et al., 2017    Natural climate variability, part 1: Observations versus the modeled predictions
[T]he AGWT [Anthropogenic Global Warming Theory] was globally advocated by the IPCC in 2001 because it appeared to be supported by the ‘infamous’ Hockey Stick temperature reconstructions by Mann et al. [10]* and by specific computer climate models mainly based on radiative forcings [4,11]. Those temperature reconstructions claimed that only a very modest change in the Northern Hemispheric climate had occurred during the pre-industrial times from A.D. 1000 to 1900, while an abrupt warming did occur just in the last century. Energy balance and general circulation climate models (GCM) were used to interpret the Hockey Stick climatic pattern as due mostly to anthropogenic greenhouse gas emissions such as CO2 because of coal and oil fuel consumption, which has been accelerating since the beginning of the 20th century [11].
However, since 2005 novel Northern Hemisphere proxy temperature reconstructions were published revealing the existence of a large millennial oscillation that contradicts the Hockey Stick temperature pattern
* see reference list



Wilson et al., 2016

Wilson et al., 2016


Abrantes et al., 2017

The new findings were consistent with alternative climatic and solar activity records showing that a quasi-millennial oscillation occurred throughout the entire Holocene for the last 10,000 years [16, 17].
The severe discrepancy between observations and modeled predictions found during the 1922-1941 and 2000-2016 periods further confirms, according to the criteria proposed by the AGWT advocates themselves, that the current climate models have significantly exaggerated the anthropogenic greenhouse warming effect.
In 2009 AGWT advocates acknowledged that: “Near-zero and even negative trends are common for intervals of a decade or less in the simulations, due to the model’s internal climate variability. The simulations rule out (at the 95% level) zero trends for intervals of 15 year or more, suggesting that an observed absence of warming of this duration is needed to create a discrepancy with the expected present-day warming rate” [24]. Thus, according to the AGWT advocates’ own criteria, a divergence between observations and climate models occurring at the bi-decadal scale would provide strong convincing evidences that the GCMs used to support the AGWT are severely flawed.
In conclusion, the temperature records clearly manifest several fluctuations from the inter-annual scale to the multidecadal one. Detailed spectral analyses have determined the likely existence of harmonics at about 9.1, 10.5, 20 and 60- year periods [7, 8, 9]. By contrast, the CMIP5 GCMs simulations used by the IPCC (2013) to advocate the AGWT show a quite monotonic accelerating warming since 1860, which is at most temporarily interrupted by volcano eruptions and only slightly modulated by aerosol emissions. Thus, the models are not able to reproduce the natural variability observed in the climate system and should not be trusted for future energy planning [33].


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It has been suggested that non-radiative physical processes connected with solar activity and the “resonant” orbital motions of the moon and the planets can cast light on the otherwise incomprehensible temperature fluctuations [34, 35]. In fact, the magnetic activity of the sun and, probably, also the planetary motions modulate both the solar wind and the flux of the cosmic rays and interstellar dust on the earth with the result of a modulation of the clouds coverage.

Scafetta et al., 2017  Natural climate variability, part 2: Observations versus the modeled predictions

Several studies based on general circulation model (GCM) simulations of the Earth’s climate concluded that the 20th century climate warming and its future development depend almost completely on anthropogenic activities. Humans have been responsible of emitting in the atmosphere large amount of greenhouse gases (GHG) such as CO2 throughout the combustion of fossil fuels. This paradigm is known as the Anthropogenic Global Warming Theory (AGWT).
[S]ince 2001 AGWT was actually supported by the belief that the “hockey stick” proxy temperature reconstructions, which claim that an unprecedented warming occurred since 1900 in the Northern Hemisphere, were reliable [2,5] and could be considered an indirect validation of the available climate models supporting the AGWT [6]. However, since 2005 novel proxy temperature reconstructions questioned the reliability of such hockey stick trends by demonstrating the existence of a large millennial climatic oscillation [7-10]. This natural climatic variability is confirmed by historical inferences [11] and by climate proxy reconstructions spanning the entire Holocene [12, 13]. A millennial climatic oscillation would suggest that a significant percentage of the warming observed since 1850 could simply be a recovery from the Little Ice Age of the 14th – 18th centuries and that throughout the 20th century the climate naturally returned to a warm phase as it happened during the Roman and the Medieval warm periods [9, 11, 14-16].
We … critically analyze the year 2015-2016, which has been famed as the hottest year on record. We show that this anomaly is simply due to a strong El-Niño event that has induced a sudden increase of the global surface temperature by 0.6 °C. This event is unrelated to anthropogenic emissions. In fact, an even stronger El-Niño event occurred in 1878 when the sudden increase of the global surface temperature was 0.8 °C.
Herein, the authors have studied the post 2000 standstill global temperature records. It has been shown that once the ENSO signature is removed from the data, the serious divergence between the observations and the CMIP5 GCM projections becomes evident. Note that Medhaug et al. [28] claim that the models agree with the post 2000 temperature trend. However, these authors did not remove the ENSO signal and used annual mean temperature records up to 2015 that camouflage the real nature of the 2015-2016 ENSO peak.
Moreover, a semi-empirical model first proposed in 2011 based on a specific set of natural oscillations suggested by astronomical considerations plus a 50% reduced climatic effect of the radiative forcing, which includes the anthropogenic forcing, performs quite better in forecasting subsequent climate changes. Thus, the GCMs used to promote the AGWT have been also outperformed [by a natural oscillation/astronomical/anthropogenic “semi-empirical” model][15]. This result is indeed consistent with recent findings. In fact, although the equilibrium climate sensitivity (ECS) to CO2 doubling of the GCMs vary widely around a 3.0°C mean [3,4], recent studies have pointed out that those values are too high.
Since 2000 there has been a systematic tendency to find lower climate sensitivity values. The most recent studies suggest a transient climate response (TCR) of about 1.0 °C, an ECS less than 2.0 °C [20] and an effective climate sensitivity (EfCS) in the neighborhood of 1.0 °C [29].
Thus, all evidences suggest that the IPCC GCMs at least increase twofold or even triple the real anthropogenic warming. The GHG theory might even require a deep re-examination [30].

Share this...FacebookTwitter "
"Japanese knotweed (Fallopia japonica var. japonica) was introduced into Europe in the mid-19th century by Philipp Franz Balthasar von Siebold, a German botanist and physician living in The Netherlands. In 1850, von Siebold sent a specimen of Japanese knotweed to Kew Gardens in London and by 1854, knotweed had travelled as far as the Royal Botanical Gardens in Edinburgh. Just over 30 years later, in 1886, Japanese knotweed was found growing in the “wild” for the first time, in Maesteg, south Wales. Now, the plant is found in over 70% of UK hectads – 10km × 10km grid squares used to measure animal and plant distributions – although it is worth noting that this does not necessarily indicate high abundance in all areas. It is also established across mainland Europe, North America and the southern hemisphere. This global spread is astonishing – particularly as, to date, it has only occurred via plant fragments (vegetative) and not from (viable) seed. In the UK alone, it is estimated that controlling Japanese knotweed costs the economy around £170m every year. There are at least 15 different active control methods and herbicides used in the country, and an extensive control industry has built up around the plant.  But, until now, there has never been a study of the scale needed to truly test how effective these treatments are. They are being sold to home and land owners with no unbiased research to back up their worth. However, we have recently completed the largest Japanese knotweed field trial ever conducted globally, and working with academic and industry partners, found the best way of treating the plant long term. The key to our approach was to understand the plant, in order to control it. Japanese knotweed’s ease of spread and rapid growth from a deep rhizome (root) system was initially prized for planting schemes. However, from an ecological perspective, these plant traits are precisely why it has become a huge problem for native biodiversity and, increasingly, wider society. Rapid growth from early in the growing season (February onwards in the UK) excludes most native plants from well-established Japanese knotweed patches (known as “stands”). This is because the dense canopy of leaves shades out other species. This shading effect is amplified as insects do not graze on knotweed plants, and native diseases don’t keep the plant in check either. Knotweed also produces a thick leaf litter, and chemicals that inhibit the germination and growth of native plants. It dominates non-native habitats, displacing native plants and altering how local ecosystems function – for example, in soil nutrient cycling. During our research, it became apparent that because a Japanese knotweed stand contains significant underground and spreading biomass, we would need to do large field trials, to reflect real world conditions. So, we set up 58 different 15 metre × 15 metre (225 square metre) field trial plots, located in south Wales (UK), and repeated each method three times in these areas.  Between 2011 and 2016, we tested all control methods and herbicides used for controlling knotweed in the UK, Europe and North America – 19 in all. This experiment continues to be unique in terms of scale, duration and scientific rigour. But it is plain to see why this research has not been conducted before – the commercial cost has been (conservatively) estimated at £1.2m. However, given the ongoing costs of managing knotweed in the UK, the value of the experiment is self-evident.  Our research has highlighted the most appropriate way to treat established Japanese knotweed stands and, surprisingly, a number of other methods which are poor or totally ineffective at field scale. We now know that glyphosate-based herbicides are significantly better than all other herbicide groups currently used for knotweed control, and that physical methods such as covering up and cutting down knotweed simply do not work. Importantly, we are not describing eradication (which is almost impossible to acheive), but rather a type of extended “dormancy” where the plant does not grow above ground. Additionally, we have also found that understanding when to apply the herbicide by considering the biology of the plant, specifically the seasonal surface-rhizome resource flows, is critically important. From this, we have defined a new patent pending approach to Japanese knotweed treatment, The 4-Stage Model™, which links herbicide selection and application with the seasonal surface-rhizome flows in the knotweed plant.  We are now working to replace outdated guidance based on short-term experiments and anecdotal information. We’re discovering how best to tackle invasive plants in real world conditions, informed by the evidence of what actually works.  While we acknowledge the current political debate surrounding glyphosate use and licence renewal for this herbicide, the effective outcomes of using glyphosate-based treatment seasonally requires lower doses of herbicide across the whole treatment life cycle. It is also more sustainable than other control methods that do not work.  All in all, our ongoing experimental approach delivers a more affordable knotweed treatment that is also more environmentally friendly than traditional, blanket application of herbicides."
"
Share this...FacebookTwitterThe Sun in November 2017
By Frank Bosse and Prof. Fritz Vahrenholt
(Translated and edited by P Gosselin)
In November the sun was unusually quiet with respect to activity. The observed sunspot number (SSN) was merely 5.7, which is only 14% of what is typically normal for month number 108 into the cycle. The current cycle number 24 began in December 2008. The sun was completely spotless 19 of 30 days in November.
At the end of the month some activity appeared, but only at a very low level. The following chart depicts the current cycle’s activity:

Figure 1: The monthly SSN values for the current solar cycle 24 (red) 108 months into the cycle, the curve for the mean of the previous 23 cycles (blue), and the similar solar cycle number 5 (black).
The next chart shows a comparison of all observed solar cycles thus far:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Figure 2: The monthly accumulated anomalies of the cycles up to 108 months into the cycle. Cycle number 24 has taken third place for the most inactive.
The situation thus remains unchanged: such a weak solar cycle has not been witnessed in 200 years. It is anticipated with quite high certainty that also the upcoming solar cycle number 25 will be about as weak, because the sun’s polar fields are about as strong as they were during the minimum between cycle number 23 and cycle number 24.
The very weak solar north pole so far has recovered significantly over the past few months since June. What this means now and for the future can be seen graphically at the chart posted here. You can find the latest information at www.solen.info/solar.
LaNina is here
An update to our last post here is surely of interest. We were sure of a La Nina by the end of December, and in the meantime the Australian Bureau of Meteorology officially announced a La Nina in its most recent bulletin. The current model forecast shows continued falling sea surface temperatures along the equatorial eastern Pacific until about February, 2018:

Figure 3: The model for El Nino/La Nina in the Pacific, Source: NOAA. All forecasts point to a moderately strong La Nina event until spring. A powerful La Nina such as the one observed in 2011/12 is currently not projected by the models (which incidentally did not even forecast a La Nina just a few months ago).
The impacts on global temperatures lag behind by about 3 to 4 months, and so we should expect a La Nina dip by spring.
Share this...FacebookTwitter "
"If you are very lucky you might have seen an orangutan in the wild. Most people have only seen them on television. In either case the animal was probably deep in some remote forest, as yet untainted by people. This is the image we associate with these critically endangered animals: vulnerable, dependent on pristine habitats, and incapable of coexisting with people. But that view may be wrong. Until recently, our ideas about conservation were constrained by romantic notions of “wild” nature and our limited grasp of just how adaptable and robust nature can be. Yet understanding how prolonged exposure to humans has impacted even well-studied species can help overturn assumptions about them and make conservation more effective. The orangutan is a good example. Orangutans are the largest mammals to primarily live in trees, and they have few predators aside from humans. They generally live at low densities and are unique among apes in being largely solitary. Though orangutan species were once widespread in mainland South-East Asia, the three that remain are restricted to small populations on Sumatra (Pongo abelii and the newly described P. tapanuliensis) and Borneo (Pongo pygmaeus). All orangutans are critically endangered, but it was assumed that significant human impacts mostly took place within the past 60 years, leading to the view of orangutans as “untouched” and lacking the capacity to adapt to humans. But we may have misjudged the orangutan. That’s the conclusion of research we have just published, together with co-authors, in Science Advances. Rather than being an ecologically-fragile ape, there is evidence that orangutans have long been adapting to humans. The modern orangutan is the product of both environmental and human impacts, and where they live and how they act appear to reflect our shared history. We are not implying that orangutans aren’t endangered by current human activities – they are. For example, between 1999 and 2015, Borneo’s orangutan population plummeted by about 50%, a loss of an estimated 100,000 individuals in 16 years. The main factor responsible for this was likely hunting. But if major threats like hunting are controlled – an important “if” – then orangutans may be better able to coexist with people than is widely thought. This opens up opportunities for conservation beyond simply protecting remote forests. People and orangutans have been in contact ever since modern humans made their home in the wet tropics some 70,000 years ago. At that time orangutans were widespread and abundant. Their teeth are relatively common among animal remains found in China, Vietnam, and Thailand suggesting they were easy pickings for prehistoric hunters. Orangutans underwent a precipitous decline around 20,000 years ago, resulting in a restricted distribution and low densities even before mass deforestation over the past century. While the climate likely had some effect, evidence from fossils, archaeology and genetics strongly suggests a human role. Specifically, we found that the arrival of humans – and especially advances in their hunting technologies, such as projectile weapons and, later, blowguns and guns – match up with orangutan declines.  Indeed, it appears that ancient humans nearly wiped out the orangutan, as they did the woolly rhino, giant ground sloths, and other Pleistocene megafauna. Surviving orangutans probably modified their behaviour to counter this threat, perhaps retreating further into the thickest forests to avoid human hunters. That ability to adapt is still present in orangutans, and is also why they are still around today. Recent studies have found they can get by reasonably well in logged forests, and they even inhabit fragmented forest landscapes dominated by oil palm and other crops, although they still need access to natural forest. When their preferred foods (ripe fruit) are not available, orangutans are even able to eat a wide variety of “fallback foods” like bark.  The realisation that orangutans have already adapted to a world dominated by humans has implications for conservation. The fact that these animals can survive relatively well in plantations and farmland outside pristine forests – as long as they are not being hunted – means that these areas should be integrated into conservation strategies. This is especially important given that most orangutans today do not live in protected forests, but in areas open to human use. The line between nature and human-dominated world is increasingly blurred. Most species have adapted to human activities in some manner. This isn’t always a good thing, but with the orangutan it allows us to see conservation opportunities that were previously invisible."
"The Organisation of the Petroleum Exporting Countries, better known as OPEC, meets in Vienna on June 22. OPEC is a group (many call it a cartel) of 14 of the biggest oil nations, representing most of the world’s reserves and just under half of current oil production. For the first time in many years, the cartel will assemble against a background of tightening supply and significantly rising oil prices.  Though the emergence of Russia and the US as major oil producers means OPEC no longer wields quite as much power as it did in the 1970s, its announcements are still big news. So will OPEC choose to raise production? Will its members even be able to agree at all? These are questions we have looked at in our recent research on OPEC’s decision making. We found that whether or not all cartel members agreed on what to do significantly affected how the market reacted to an OPEC announcement.  This latest meeting comes as the price of oil is fast increasing, several years after it crashed from around $100 a barrel (bbl) to $50/bbl. OPEC members, who generally produce their oil relatively cheaply, will also be aware that long-run prices above $50/bbl will motivate shale production in rivals, including Canada and the US.  Commentary on OPEC decisions usually focuses on “spot prices” – that is, how much a barrel of oil would cost if you actually wanted it right away. But trading is also possible where we assume delivery at some specified future date, that is “forward prices”. The other way to assess OPEC decisions is to look at these forward prices – and especially at the relationship between forward and spot prices. This relationship is known as the oil market’s “term structure”. The graph below provides a simple demonstration of the importance of term structure in the oil markets. The blue line shows the familiar story of spot prices, including the 2014 crash and the recent rally. However, the red line, showing the price of three-year forward oil, tells a very different story. There was no distinct fall in forward prices around the OPEC policy change of 2014 and no recent recovery. Rather, there was a continuous and gradual decline followed by an ongoing period of stability.  Given OPEC decisions have long-term ramifications, an analysis that looked only at spot prices would certainly not tell the whole story. This is why our recent study also examined the impact of OPEC decisions on how forward prices relate to spot prices – the term structure.  We found that decisions to raise production tend to be followed by forward price increases, while decisions to cut tend to be followed by forward price decreases. These findings are seemingly counter-intuitive – after all, basic economics tells us that more supply leads to lower prices and vice versa. So why would additional OPEC production actually make oil more expensive in the future? Things are best explained by viewing OPEC decisions as something of a too-little-too-late response to market fundamentals. If you look back at the graph above, you’ll see that the OPEC decision to allow (spot) prices to fall in 2014 appears to have been anticipated by the forward market over several years. Most OPEC meetings aren’t as dramatic as 2014, of course. The most frequent outcome is “no change” in production quotas and previous research has found that prices rise following such decisions. We however have divided the “no change” decisions into those which follow a unanimous decision by members to maintain production unchanged, and those which follow a failure by OPEC members to make a unanimous decision (leading to no change by default). For example during the two OPEC meetings in 2015, both decisions were no change in production. However, the June meeting was based on “agreement to maintain”, while the December meeting was based on “failure to agree”. We found that the market reacts significantly differently to the two types of no-change decision. Failure to agree decisions lead to higher spot prices and expectations of higher prices, over a prolonged period. This is not the case for agreement to maintain decisions. The implication is that there is more market optimism following an agreement to maintain production and more caution following a failure to agree.  So OPEC members should not be fooled by the current short-run price increases. Though reports suggest an increase in production is likely, they may yet (and, from a purely self-interested point of view, should) decide not to increase their production quotas, at least so long as forward prices remain low.  But there will no doubt be some voices at the table looking to boost production because prices in the short term suggest that more oil could be absorbed by the market.  If there is no quota change, it will be interesting to watch whether members agree to maintain production unchanged, or whether they maintain production unchanged as a result of a failure to agree. Our study shows that when it comes to OPEC decisions, agreement matters."
nan
"Passenger cars are still the most popular transportation mode. In 2014 nearly 68m were produced globally. They’re not only a vital part of our economy and our personal lives but also an important social and cultural tool, used to present a certain image and status – real or imagined.  Our entrenched reliance on – and attachment to – this method of travel means that, even if we shift away from such widespread car ownership, we need to change our perception of what cars are if we want to mitigate their high environmental costs. This doesn’t just mean moving to electric vehicles. Just at the resource extraction level, roughly five tons of materials are needed to produce a 1.2 ton car, creating ten tons of effluents and 2.5 tons of emissions. Processing these materials into components, assembling and distributing the cars around the world – and then using, servicing and disposing of them generates even more emissions. In total, a typical mid-size car is responsible for around 17 tons of CO₂. The total embodied emissions for alternatively fuelled vehicles such as hybrids, electric and fuel-cell vehicles may even be higher than normal internal combustion engines – even when they produce no tail-pipe emissions (based on the as-yet unpublished Greet2 study). This is perhaps because such technologies are more energy intensive to produce due to the materials that compose them. So what is the alternative to the current system? If car travel is going to remain common, perhaps we need to be smarter about how we build and use them. Our cars currently spend 92% of their time parked – and, when driving, most of their weight is used only to carry one person most of the time. Cars could be produced in fewer numbers, to be smaller, longer-lasting and shared by more people. And instead of focusing on turning out as many new cars with relatively short lifespans as possible, manufacturers could provide more services to keep vehicles on the road for longer and deal with their disposal. The role of car designers could also change. First by designing simpler basic cars, without “gimmicks” such as mood lights or massaging seats. Timeless lines rather than subject to the fads of the day. Instead of working on one project after another, the designer could be involved in an upgrading process that would see each model evolve through re-manufacturing in a more direct interaction with consumers. Other changes in the features of the cars themselves could also produce more sustainable models. For example, safety standards today are driven by electronic systems such as collision-avoidance and pedestrian-detection systems. These could be upgraded during service life more easily than physical features. Based on my own (as yet unpublished) research, I believe that if these systems prove to be highly reliable, there will be no need for low-speed impact structures, reducing the use of materials. This model might be easier to move to than it first sounds. So-called millennials are less interested in cars than previous generations, applying for driver’s licences later in life and more likely to live in highly congested cities where access to public transport is easier. They are also used to sharing or renting services, for example with taxi-hailing or liftsharing apps such as Uber. Owning a car, on the other hand, is seen as an expensive liability. The car manufacturing industry is also at a cross-roads. Powertrain options are multiplying, driverless technology is poised to make big changes and non-automotive companies including Google want a share of the market. As materials become more expensive, relying on cars with relatively shorter lifespans to flood the market is not in anyone’s interest. Not even the car makers, who at best can only make 5% annual profit. The current business model may not survive in the longer term. It may naturally make more sense for manufacturers to build and service cars as long-lasting rental products. Some electric vehicle manufacturers have already introduced rental schemes for their batteries, which are likely to need replacing far quicker than the rest of the car. Extending the lifespan and the product life cycle will impact on production. Fewer cars means that the return on investment may take longer. But it could also mean less need to update costly manufacturing tools – and factories could be made more modular and flexible to produce different types of cars in one assembly line. Plants could be more localised to meet the different needs of the different megacities of the future. And redundant assembly workers could be retrained into servicing and maintenance or other car-related services. This model would require us to think differently about cars, redefining terms such as “old” and “used” and educating consumers, especially those from older generations who are unfamiliar with sharing systems. Not all cars will survive into the future, but if we are better stewards of what we have now and learn to cherish products in a more subjective way than the market does, cars can definitely last for longer."
"We’re increasingly aware of how plastic is polluting our environment. Much recent attention has focused on how microplastics – tiny pieces ranging from 5 millimetres down to 100 nanometres in diameter – are filling the seas and working their way into the creatures that live in them. That means these ocean microplastics are entering the food chain and, ultimately, our bodies. But fish and shellfish aren’t our only food sources that can contain microplastics. And, in fact, other sources that don’t come from the sea might be much more worrying. A portion of consumer-grade mussels in Europe could contain about 90 microplastics. Consumption is likely to vary greatly between nations and generations, but avid mussel eaters might eat up to 11,000 microplastics a year. It’s harder to know how many microplastics we might be consuming from fish. Most studies to date have only analysed the stomach and gut content of these organisms, which are usually removed prior to consumption. But one study has found microplastics in fish liver, suggesting particles can get from digestive tissues to other body parts. Microplastics have also been found in canned fish. Numbers identified were low, so the average consumer might only eat up to five microplastics from a portion of fish this way. The particles found might also come from the canning process or from the air. Another marine food source of microplastics is sea salt. One kilogram can contain over 600 microplastics. If you eat the maximum daily intake of 5 grams of salt, this would mean you would typically consume three microplastics a day (although many people eat much more than the recommended amount).  However, other studies have found varying amounts of microplastics in sea salt, possibly because of different extraction methods used. This is a widespread problem in microplastics research that makes it hard or impossible to compare studies. For example, one study seems to only have looked for microfibres (tiny strands of artificial materials such as polyester) while a further study only looked for microplastics larger than 200 micrometres.  The sea salt study mentioned above didn’t attempt to remove and count all the microplastics from its salt samples and instead gave an estimate based on the proportion of particles that were recovered. This means it showed 1 kilogram of salt contained at least 600 microplastics – but the actual figure could be a lot higher. Despite these findings, other research demonstrates that far more microplastics in our food are likely to come from other sources than the sea. Land animals also eat microplastics although – as with fish – we tend not to eat their digestive systems. There’s limited data about this part of the food industry, but a study of chickens raised in gardens in Mexico found an average of 10 microplastics per chicken gizzard – a delicacy in some parts of the world. Scientist have also found microplastics
in honey and beer. We might be swallowing tens of microplastics with each bottle of the latter. Perhaps the biggest known source of microplastics that we consume is bottled water. When researchers examined a variety of types of glass and plastic water bottles, they found microplastics in most of them. Single-use water bottles contained between two and 44 microplastics per litre, while returnable bottles (designed for collection under a deposit scheme) contained between 28 and 241 microplastics per litre. The microplastics came from the packaging, which means we could be exposing ourselves to more of them every time we fill up a plastic bottle in order to reduce waste. There is also evidence that microplastics in food come from indoor dust. A recent study estimated that we could get an annual dose of almost 70,000 microplastics from the dust that settles on to our dinner – and that is only one of our daily meals.  So, yes, we are eating small numbers of microplastics from marine products. But it may only take drinking a litre of bottled water a day to consume more microplastics than you would from being an avid shellfish eater. And the other question scientists have yet to answer when it comes to microplastics in our food is how much harm they actually do."
"Madagascar is in the midst of a toxic invasion. Since around 2010, an army of invasive Asian toads (Duttaphrynus melanostictus) has gained a foothold in and around the eastern port of Toamasina after they were accidentally introduced from South-East Asia. This has dismayed conservationists who worry about the island’s already beleaguered endemic fauna.  Now, our worst fears have been confirmed by recent findings by a research team led by Bangor University masters student Ben Marshall and including myself. In our new study published in Current Biology, we show that most of Madagascar’s unique native wildlife can indeed be poisoned by the introduced toad. Invasive species are one of the major drivers of extinction worldwide. Most impact native species by eating them (cats, rats), competing with them (grey squirrels in the UK) or altering local vegetation (rabbits, goats). Invasive toads, on the other hand, primarily affect native predators through their skin toxins, poisoning any animal that takes a toad into its mouth. “True toads”, of the family Bufonidae, synthesise potent toxins in large prominent parotoid glands on their backs. These “bufotoxins” impede the regulation of sodium and potassium levels in cells, leading to rapid heart failure and death. Where toads occur naturally, some predators avoid them, but others have evolved resistance to their toxins, allowing them to routinely eat toads. Recent investigations have revealed the mechanism of resistance: two specific amino acid substitutions in the target molecule of bufotoxin, the ATPase sodium-potassium pump, are all that is required to change a dangerously poisonous toad into a potential meal. Remarkably, the same solution has evolved in a diverse array of species, from rats to lizards to the toads themselves, and this appears to be the only way for any vertebrate to resist the toads’ toxins. The problem for Madagascar’s animals is they developed on an island with plenty of frogs but no toads. Native species therefore had no reason to evolve any resistance to toad toxins nor any sense that they should avoid toads. And when toads do invade these previously toad-free areas, it may result in mass mortality among some naive predators. For instance the infamous cane toad invasion of Australia caused larger apex predators, such as monitor lizards, some snakes and the marsupial quolls to become locally rare or extinct. This in turn affected numerous other species in the food web, though some, such as some smaller lizards and snakes, actually flourished due to the removal of the larger predators. So will Madagascar’s predators suffer just like Australia’s? As we now know that there is a single molecule responsible for toad resistance in vertebrates, this gives biologists an invaluable tool: simply examining the relevant gene will reveal whether an animal can eat a toad with impunity or is liable to be poisoned. In our study, we used this approach on a range of Malagasy predators. The results confirmed our worst fears: out of 29 reptiles, eight mammals, 12 frogs and 28 birds tested, all except one rodent (the white-tailed antsangy) lack resistance to toad toxins. This presents an immediate conservation concern in an already troubled biodiversity hotspot, and has serious implications for the many species that exist only on Madagascar. For instance, all Malagasy snakes tested are vulnerable to toad toxins, and anecdotal reports have already documented snakes dying from eating toads.  It gets worse: in the past, a loss of snakes has led to booms in rodent populations and fears over public health. Such fears are warranted once again, given that non-native rats are resistant to bufotoxins. However, rodents appear to be unique in this respect. Among the toad-sensitive mammals are Madagascar’s most charismatic and widely recognised residents. Three representatives from three lemur families are vulnerable. Fortunately lemurs eat plants and sometimes insects, and only rarely prey on small vertebrates, which will likely limit the toad’s impact on them. The same cannot be said for the carnivores of Madagascar: the habits of the enigmatic fossa and others make them extremely likely to encounter and consume the toxic invader. As cane toads in Australia highlighted, it’s tough to predict exactly which species will be most affected by an invader. The sharing of habits and habitats will likely be the biggest factor influencing how much the toad will impact a species, but subtle differences in their interactions can make big differences to the outcome. For instance, small variations in breeding times made large differences in the relative success of native and invasive tadpoles in Australia. Possible hope comes in the form of animals’ unrelenting ability to adapt. Australia offers many examples of native species evolving or learning quickly to avoid toads, or to only eat the least toxic parts of toads. Whether vulnerable Malagasy species, restricted to much more fragmented habitat patches serving as their final refuges, can display similar resilience remains to be seen. The invasion of Madagascar by these toads has attracted considerable attention, but initial efforts to eliminate them have faltered. We hope that the confirmation that these toads pose a real threat to native species will reinvigorate efforts to protect native species from the toad’s further expansion."
"
Share this...FacebookTwitterWhile global warming alarmists continue to fantasize crude oil use getting drastically reduced already starting next year, OPEC sees it totally differently. German online. center-left weekly Die Zeit reports “OPEC anticipates growing oil demand until 2040.”
This poses a huge dilemma for the activists and alarmists who are urgently pressing to transform society –based on the fear and belief that the globe will warm rapidly if we don’t act now.
The other fear is that rising oil consumption over the next 25 more years accompanied stalling global temperatures will forever expose climate science as a ruse.
Still decades away from peak oil consumption
Die Zeit writes that OPEC is sure that the planet is still years away from peak oil demand, and the reason is because cars will continue to be mostly powered by petroleum even beyond 2040. Obviously a number of leading energy experts believe electric cars will remain a pipe dream for quite some time.

Chart: OPEC
Oil consumption will climb almost 20%
Firstly OPEC sees the world population growing from 7.6 billion today to 9.2 billion by 2040 and global GDP growing by a whopping 126%. That’s all going to require lots of reliable and efficient energy. The good news is that overall energy efficiency will increase, as only 96% more energy will be needed to power the 126% GDP boost.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Most of the energy increase will come from India and China. Moreover, the global average standard of living will also grow strongly as the per capita energy use will be much higher. OPEC projects that much of the added energy demand will be supplied by natural gas (see chart above). Part of the increased demand will be met by coal and oil.
OPEC also foresees continued domination by the internal combustion engine for passenger cars, even in 2040:

Chart: OPEC
Electric mobility still decades away
OPEC also projects there will be more than 2 billion automobiles on the world’s roads by 2040, almost double today’s number. Over 80% of these will continue to be powered by internal combustion engines. The electric vehicle age appears to be something for the late 21st century. Visions of a near zero carbon society within the next 30 years are more fantasy than reality.
Not surprisingly, most of the increased energy demand will come from Asia and Africa. The richer, developed OECD countries will see little growth in energy demand.
The OPEC video summarizes by saying that it is “committed to reducing energy poverty“. That target of course would make a huge contribution to alleviating the overall misery that still plagues many poor countries. Oil, coal and gas will continue to play the leading role.
Of course, jet-setting climate activists and alarmists would like to deny poor countries access to affordable and reliable energy.
Good luck with that.
 
Share this...FacebookTwitter "
"The Guardian has always been a purpose-led organisation, and this year we committed to focusing on that even more. B Corporations are companies that value purpose as much as profit, so it felt like a natural fit with our own values. Being a B Corp means a few different things: it’s a way of having a rigorous, objective view of where we are doing well and where we need to work harder when it comes to our environmental and social impact. It’s a way of demonstrating to our readers what type of business we are and it’s also a community of like-minded businesses that want to share ideas and encourage each other.  A lot of companies are talking about the idea of “purpose” at the moment so it’s important to us that we can demonstrate to our readers that we are doing more than just talking – we are actually acting on things like our working practices, how we recruit, our environmental performance and how we support local communities. We believe we are the first major media organisation to become a B Corp anywhere in the world so we are proud to have taken this step. There are currently about 3,000 certified companies globally. The certification started in the US and around half of all certified companies are based there, but it is growing quickly in Europe and beyond. The outdoor clothing brand Patagonia is one of the best known B Corps, the Body Shop recently certified and food company Danone is in the process of certifying all of its subsidiaries. There are also lots of smaller companies doing everything from law to teabags to wetsuits! It feels like consumer and business attitudes are changing really quickly and people increasingly want to know that companies are behaving in an ethical way. We have committed to being carbon neutral by 2030 and we already have a target of eliminating the gender pay gap in the top half of our organisation by 2022. We’re already doing a lot of work to be a more diverse and representative employer, which will continue to be a priority. We’re currently working on a plan for what other specific improvements we want to achieve by the time we get reassessed in three years. The B Corp assessment measures our performance across five areas - governance, workers, community, environment and customers. We have to recertify every three years so we will be able to track our progress. Our climate pledge was really driven by our reporting on the severity of the climate crisis, and the strength of feeling among our readers and our staff. We believe that the world is in the grip of an environmental emergency but too many governments and businesses are not taking meaningful action to address it. The biggest way that we can play a role is through our reporting, highlighting the impact of the climate crisis on communities around the world and helping our readers to understand the issue. But it was also important to us that we held ourselves to account and made a commitment to improve our own performance. The more companies that commit to net zero, the more momentum there will be to develop solutions, and hopefully the more pressure it will put on governments to take meaningful action. We are currently in the process of doing a full audit of our carbon emissions, both within our own operations but also across our supply chain. We hope to complete that soon and then identify where we can make the biggest reductions in our footprint. One of the things we will be looking at is how we reduce the amount of air travel. We’ll also be looking at things like where all our energy supplies come from and how we could reduce emissions associated with our printing operations. For example, we previously switched the packaging for some of our weekend papers from plastic to compostable materials, but we are now looking at other alternatives that are easier to recycle. Our priority is to reduce the emissions that we cause. We will also look at ways that we can offset the emissions that we can’t remove. We want to approach that in a really thoughtful way as there are a lot of conflicting views about the merits of different offsetting options, and growing cynicism about the ways some companies are using it as a way of appearing green instead of making meaningful change to the way they operate. That doesn’t mean that we shouldn’t do it, but we want to make sure that we support schemes that are verifiable and have a real benefit. You can encourage the companies you buy from to become B Corps – it’s a rigorous certification and if all major companies did it then everything from labour standards to environmental performance would be a lot better! You can advocate for the government to implement stronger corporate standards so that companies are obliged to consider the long-term impact of their actions on employees, the environment and society alongside the impact on shareholders. And we hope that our climate pledge will inspire readers to take action in their own lives, whether it’s taking fewer flights, shopping more locally or reducing the amount of plastic in your life. Julie Richards is the delivery portfolio director at Guardian News & Media Support Guardian journalism today, by making a single or recurring contribution, or subscribing "
"
Share this...FacebookTwitterEngineering Prof. Questions Temperature
Record, Models, CO2 Climate Sensitivity 

Photo California Baptist University

 Pontius, 2017  
Sustainable Infrastructure: 
Climate Changes and Carbon Dioxide 
Temperatures Record ‘Unreliable’, ‘Arbitrarily Adjusted’, And Of ‘Poor Data Quality’
Temperature measurement stations have been installed at various locations across the globe. The number of temperature monitoring stations is decreasing and many areas across the globe do not have any temperature monitoring stations. Consequently, average surface temperature is an unreliable metric for assessing global temperature trends.
Computer models are used to analyze data sets. In science and engineering (and this paper) the term “data” refers to actual physical measurement at a point in time and space. In some temperature data sets, however, computer simulated values have been added in or data may have been arbitrarily adjusted long after the physical measurement was taken. Such practices undermine the credibility of the data set.   Computer generated values are estimates, projections, or simulations and are of a different quality than physical measurements. Physical measurements represent a physical quantity whereas computer simulations represent numerical calculation.
The HADCRU, GISTEMP, and NOAA surface temperature archives rely on the same underlying input data and therefore are not independent data sets. Limitations of the GHCN affect all data sets. Sampling discontinuities, urbanization and land use changes have decreased the quality of GHCN data over time. Differences in data processing methods between research teams do not compensate for poor underlying data quality inherent in the GHCN data. A similar situation exists with historical Sea Surface Temperature (SST) data sets which are derived primarily from the International Comprehensive Ocean-Atmosphere Data Set (ICODADS).
Climate Models ‘Unreliable For Long-Term Climate Prediction’
Computer simulations involve mathematical models implemented on a computer imitating one or more natural processes. Models are based on general theories and fundamental principles, idealizations, approximations, mathematical concepts, metaphors, analogies, facts, and empirical data (Peterson, 2006, Meehl et al., 2012). Judgments and arbitrary choices must be made in model construction to apply fundamental laws to describe turbulent fluid flow. The large size and complexity of the atmosphere prohibit the direct application of general theory.
In general, ensemble model forecasts have been found unreliable for long-term climate prediction (Green and Armstrong, 2007, Mihailović et al., 2014).
“The forecasts in the [IPCC] Report were not the outcome of scientific procedures. In effect, they were the opinions of scientists transformed by mathematics and obscured by complex writing. Research on forecasting has shown that experts’ predictions are not useful in situations involving uncertainly and complexity. We have been unable to identify any scientific forecasts of global warming. Claims that the Earth will get warmer have no more credence than saying that it will get colder.”  –  Green and Armstrong, 2007.
“This analysis, set into context of the climate modeling, points out the fact that there exists set of domains where the environmental interface temperature cannot be calculated by the physics of currently designed climate models.” – Mihailović et al., 2014

Climate Sensitivity To Changing CO2 Concentrations
Global
The global atmospheric system is dynamic and is constantly in a state of change and adjustment. The sun is the primary climate change driving force.   
Using a Climate Sensitivity best estimate of 2°C, the increase in [global] temperature resulting from a doubling of atmospheric CO2 is estimated at approximately 0.009°C/yr which is insignificant compared to natural variability.
CO2 is a non-toxic trace gas constituting approximately 0.04% of the earth’s atmosphere. The global atmospheric concentration of CO2 increased from a pre-industrial value of about 280 ppmv to 379 ppmv in 2005 . The average CO2 concentration at the monitoring station at Mauna Loa, Hawaii for May 2017 is 409.65 ppmv.  A rising concentration of atmospheric CO2 will contribute to warming of the Earth’s atmosphere. The physics of CO2 in the atmosphere is very different than the physics of the heating effect occurring in a physical “greenhouse” for growing plants. The term “greenhouse effect” is commonly used to refer to the warming of the earth from “greenhouse” gases such as CO2 in the atmosphere. The term “greenhouse” is not used here to refer to the Earth’s warming to avoid equivocation.
Estimates of climate sensitivity differ widely suggesting that this characteristic of the climate system is not well-understood (Schwartz et al., 2014). 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A simple model predicts that a doubling of the CO2 concentration in the atmosphere would result in a small increase of the Earth’s surface temperature, from approximately 0.[5] to < 0.7°C  (Kissin, 2015).
“[A] doubling the CO2 concentration in the Earth’s atmosphere would lead to an increase of the surface temperature by about +0.5 to 0.7 °C, hardly an effect calling for immediate drastic changes in the planet’s energy policies.” – Kissin, 2015
A best estimate of 2.0°C (Otto et al., 2013) is assumed here. If CO2 increases at the current rate of approximately 2 ppmv per year, a temperature increase of approximately 0.009°C/yr could be expected.
To date the impact of CO2 is assessed universally within a global reference frame. Although atmospheric CO2 has steadily increased the average satellite global temperatures have flattened since approximately 1995. 

 From such trends, it must be inferred that changes in global lower troposphere average temperature correspond to fundamental changes in the climate system beyond internal variability.
Riverside, California
The impact of future atmospheric CO2 warming on the Riverside locational reference frame must be estimated.  GCMs [climate models] could be applied to project future global temperatures and those projections could be downscaled to the Riverside area. However, such efforts would be potentially misleading because of the limitations of GCMs discussed previously.  Detailed assessments of the CO2 effect have been performed analyzing the Earth’s energy balance in the total atmosphere column and the reduction of the upward infrared radiation emission at the tropopause. The impact of CO2 on warming of the Earth is expressed in terms of “climate sensitivity,” which is the amount of warming that could be  expected as a result of doubling of the CO2 concentration.
Available temperature data from both the Riverside Fire Station No. 3 and the Riverside Municipal Airport demonstrate horizontal trends within a wide band of  variability. Historical evidence of a significant increase in surface temperatures due to increases in atmospheric CO2 is absent from these data.   [C]limate models are useful but limited in their representation of underlying physical processes.  Uncertainties and other limitations discussed previously render such models unreliable for long-term global temperatures or local climate change prediction.
Climate sensitivity may be applied to estimate the warming effect of CO2 on the locational reference frame.  Factors affecting Climate Sensitivity are not well-understood and estimates differ among researchers. Alternatively, a site-specific model could be developed to estimate the future impact of CO2 warming on a particular location. If atmospheric CO2 continues to increase at its current rate the small annual temperature increase expected at Riverside will likely be insignificant (e.g. < 0.01°C/yr) compared to natural temperature variability.
A slight increase in minimum daily temperature is noticeable at Riverside Fire Station No. 3 after 1998 (Figure 8, lower) with a corresponding slight decrease in the daily temperature range (Figure 9). This trend is most likely due to the urban heat island effect (Tam et al., 2015) resulting from increased development within and around downtown Riverside over this extended period.

Share this...FacebookTwitter "
"The discovery of 26 bodies with lethal injuries in a 7,000 year old mass grave in Germany provides more evidence of organised large-scale violence in Neolithic Europe. The findings, reported in the journal PNAS, also help us understand the sudden and perhaps brutal ending of central Europe’s first farming culture. The Linear Pottery (Linienbandkeramik, or LBK) culture which dominated central Europe between 5600 and 4900 BC was once depicted as peaceful and pioneering – farmers who cleared land and carved new communities out of the heavily-forested “wilderness”. This view began to crumble with the discovery in the late 1980s of a mass grave at Talheim in southern Germany, containing the remains of 34 men, women and children, many of whom showed evidence of lethal injuries caused by stone axes.  This was followed not long after by the findings from an enclosure (a precursor to fortified Roman army camps) at Asparn/Schletz in Austria. Here, the remains of 67 people were found lying in haphazard positions in the bottom of just one section of the enclosure ditch, the implication being that many more individuals are represented across the site as a whole. It is clear that they too died violently, with many skulls showing signs of multiple blows.  That fact both of these massacres – no other word can be applied – fell near the end of the Linear Pottery culture, around 5000 BC, raised the possibility that things ended less than peacefully. In the latest paper, a German team led by Christian Meyer present the recently discovered third instalment in this story. A long trench at the site of Schöneck-Kilianstädten, central Germany, held the remains of at least 26 individuals, found commingled in a mass grave, again with evidence of multiple injuries showing no signs of healing. Most were caused by stone axes, but there were also arrowhead wounds. This was almost certainly a single event, again dating to around 5000 BC.  Intriguingly, as at Asparn/Schletz, the graves contained no children aged 9-16 or young women. This suggests the capture of children and young women may have been one of the motivations for conflict, as it has been in more recently recorded societies around the world. These events were devastating not only to those involved, but to the entire society. While we have only the vaguest idea of the total population of Neolithic Europe at any particular point, we do have some sense of the size of local villages – usually around 50 to 100 inhabitants. Thus, the deaths of even 26 or 34 people represents an event that, scaled up for an appropriate comparison with modern population levels, would entail killing on a scale seen today only in the most war-torn countries. Archaeology deals with fragments of the past, and there is always the possibility of bias in what survives and what does not, as well as in what is found and what remains hidden. Add to this the fact that radio-carbon estimates provide a date range rather than a specific year and the discovery of one massacre falling at approximately the same time as the disappearance of Linear Pottery may be no more than a coincidence.  The finding of a second example begins to suggest a pattern, however tentatively. The discovery of a third case looks very suspicious indeed. The question that naturally arises is why this particular point in time should see such a widespread outbreak of conflict, involving the killing of what could easily be the entire populations of small hamlets. While there is certainly evidence of conflict both before (including among the hunter-gatherers that preceded the Neolithic) and after 5000 BC, this usually takes the form of isolated incidents involving relatively few individuals. These mass graves were the result of something larger and more organised. One theory blames the environment. A period of climatic instability led to increased competition for resources and eventually to conflict – including the extermination of some entire communities. This interpretation very much divides the room. Many researchers take exception to what they see as an overly simplistic, environmentally deterministic explanation, and favour internal causes for conflict. Other strategies could have been employed to cope with shortages, emphasising greater cooperation rather than competition.  There is also the problem of precisely correlating climatic records and archaeological events. While there is some evidence of a climatic downturn at the end of the 6th millennium BC, there is still considerable leeway in the dating of both this downturn, and in the massacres discussed here, making it very difficult to link them in a causal way. The findings at Schöneck-Kilianstädten will no doubt fuel this debate, and rightly so, since it is an important one that is not without implications for our own future. Some studies have suggested that global warming is likely to lead to a massive increase in levels of conflict worldwide. If such a link does turn out to have been the case in Neolithic Europe, it would be depressing if we have not learned anything in the intervening millennia that would enable us to avoid a similar fate."
"
Share this...FacebookTwitter‘Two-Thirds Of Climate Warming’ 
Since 1750 Due To ‘Solar Causes’
– Dr. Alan D. Smith, Geoscientist

Though advocates of the dangerous anthropogenic global warming (AGW) narrative may not welcome the news, evidence that modern day global warming has largely been driven by natural factors – especially solar activity – continues to pile up.
Much of the debate about the Sun’s role in climate change is centered around reconstructions of solar activity that span the last 400 years, which now include satellite data from the late 1970s to present.
To buttress the claim that solar forcing has effectively played almost no role in surface temperature changes since the mid-20th century, the IPCC has shown preference for modeled reconstructions of solar activity (i.e., the PMOD) that show a stable or decreasing trend since the 1970s.  Why?  Because if the modeled results can depict steady or decreasing solar activity since the last few decades of the 20th century – just as surface temperatures were rising – then attributing the post-1970s warming trend to human activity becomes that much easier.
The trouble is, satellite observations using ACRIM  data (which have been affirmed to be accurate by other satellite data sets and are rooted in observation, not modeled expectations) indicate that solar activity did not decline after the 1970s, but actually rose quite substantially.  It wasn’t until the early 2000s that solar activity began to decline, corresponding with the denouement of the Modern Grand Maximum.

ACRIM Composite Is ‘Data Driven’, While The PMOD Composite Is ‘Model Driven’

Willson, 2014
• Comparison of the results from the ACRIM3, SORCE/TIM and SOHO/VIRGO satellite experiments demonstrate the near identical detection of TSI variability on all sub-annual temporal and amplitude scales during the TIM mission.   A solar magnetic activity area proxy [developed in 2013] for TSI has been used to demonstrate that the ACRIM TSI composite and its +0.037 %/decade TSI trend during solar cycles 21–23 [1980s-2000s] is the most likely correct representation of the extant satellite TSI database. 
• The occurrence of this trend during the last decades of the 20th century supports a more robust contribution of TSI variation to detected global temperature increase during this period than predicted by current climate models.
• One of the most perplexing issues in the 35 year satellite TSI database is the disagreement among TSI composite time series in decadal trending. The ACRIM and PMOD TSI compostite time series use the ERB and ERBE results, respectively, to bridge the Gap. Decadal trending during solar cycles 21–23 is significant for the ACRIM composite but not for the PMOD.  A new [2013] TSI-specific TSI proxy database has been compiled that appears to resolve the issue in favor of the ACRIM composite and trend. The resolution of this issue is important for application of the TSI database in research of climate change and solar physics.
• The ACRIM TSI composite is data driven. It uses ACRIM1, ACRIM2, ACRIM3 and Nimbus7/ERB satellite results published by the experiments’ science teams and the highest cadence and quality ACRIM Gap database, the Nimbus7/ERB, to bridge the ACRIM Gap. 
• The PMOD TSI composite, using results from the Nimbus7ERB, SMM/ACRIM1, UARS/ACRIM 2 and SOHO/ VIRGO experiments, is model driven. It conforms TSI results to a solar-proxy model by modifying published ERB and ACRIM results and choosing the sparse, less precise ERBS/ERBE results as the basis for bridging the ACRIM Gap (Frohlich and Lean 1998).
• The Earth’s climate regime is determined by the total solar irradiance (TSI) and its interactions with the Earth’s atmosphere, oceans and landmasses. Evidence from 35 years of satellite TSI monitoring and solar activity data has established a paradigm of direct relationship between TSI and solar magnetic activity. (Willson et al. 1981; Willson and Hudson 1991; Willson 1997, 1984; Frohlich and Lean 1998; Scafetta and Willson 2009; Kopp and Lean 2011a, 2011b)  This paradigm, together with the satellite record of TSI and proxies of historical climate and solar variability, support the connection between variations of TSI and the Earth’s climate.   The upward trend during solar cycles 21–23 coincides with the sustained rise in the global mean temperature anomaly during the last two decades of the 20th century. 

Assessment Of The Sun’s Climate Role Largely Depends On The TSI Model Adopted

Van Geel and Ziegler, 2013
• [T]he IPCC neglects strong paleo-climatologic evidence for the high sensitivity of the climate system to changes in solar activity. This high climate sensitivity is not alone due to variations in total solar irradiance-related direct solar forcing, but also due to additional, so-called indirect solar forcings. These include solar-related chemical-based UV irradiance-related variations in stratospheric temperatures and galactic cosmic ray-related changes in cloud cover and surface temperatures, as well as ocean oscillations, such as the Pacific Decadal Oscillation and the North Atlantic Oscillation that significant affect the climate.
• [T]he cyclical temperature increase of the 20th century coincided with the buildup and culmination of the Grand Solar Maximum that commenced in 1924 and ended in 2008.
• Since TSI estimates based on proxies are relatively poorly constrained, they vary considerably between authors, such as Wang et al. (2005) and Hoyt and Schatten (1997). There is also considerable disagreement in the interpretation of satellite-derived TSI data between the ACRIM and PMOD groups (Willson and Mordvinov, 2003; Fröhlich, 2009). Assessment of the Sun’s role in climate change depends largely on which model is adopted for the evolution of TSI during the last 100 years (Scafetta and West, 2007; Scafetta, 2009; Scafetta, 2013). 
• The ACRIM TSI satellite composite shows that during the last 30 years TSI averaged at 1361 Wm-2, varied during solar cycles 21 to 23 by about 0.9 Wm-2, had increased by 0.45 Wm-2 during cycle 21 to 22 [1980s to 2000s] to decline again during cycle 23 and the current cycle 24 (Scafetta and Willson, 2009). 
• By contrast, the PMOD TSI satellite composite suggests for the last 30 years an average TSI of 1366, varying between 1365.2 and 1367.0 Wm-2 that declined steadily since 1980 by 0.3 Wm-2.

Total Solar Irradiance Increased By 3 W m-2 Between 1900 And 2000

Van Geel and Ziegler, 2013 (continued)
• On centennial and longer time scales, differences between TSI estimates become increasingly larger. Wang et al. (2005) and Kopp and Lean (2011) estimate that between 1900 and 1960 TSI increased by about 0.5 Wm-2 and thereafter remained essentially stable, whilst Hoyt and Schatten (1997) combined with the ACRIM data and suggest that TSI increased between 1900 and 2000 by about 3 Wm-2 and was subject to major fluctuations in 1950-1980 (Scafetta, 2013; Scafetta, 2007). 
• Similarly, it is variably estimated that during the Maunder Solar Minimum (1645- 1715) of the Little Ice Age TSI may have been only 1.25 Wm-2 lower than at present Wang et al., 2005; Haig, 2003; Gray et al., 2010; Krivova et al., 2010) or by as much as 6 ± 3 Wm-2 lower than at present (Shapiro et al., 2010; Hoyt and Schatten, 1997), reflecting a TSI increase ranging between 0.09% and 0.5%, respectively.

Graph Source: Soon et al., 2015

After Removing Instrumental ‘Adjustments’, Urban Bias, Temperatures Follow Solar Activity

The combined Hadley Centre and Climatic Research Unit (HadCRUT) data set — which is featured in the Intergovernmental Panel on Climate Change (IPCC) reports — underwent a revision from version 3 to version 4 in March of 2012.  This was about a year before the latest IPCC report was to be released (2013).  At the time (early 2012), it was quite inconvenient to the paradigm that HadCRUT3 was highlighting a slight global cooling trend between 1998 and 2012, as shown in the graph below (using HadCRUT3 and HadCRUT4 raw data from WoodForTrees).  So, by changing versions, and by adjusting the data, the slight cooling was changed to a slight warming trend.

Source: WoodForTrees
As recently as 1990, it was widely accepted that the global temperature trend, as reported by NASA (Hansen and Lebedeff, 1987), showed a “0.5°C rise between 1880 and 1950.”
Pirazzoli, 1990
This 0.5°C rise in global temperatures between 1880-1950 (and 0.6°C between 1880 and 1940) can clearly be seen in the NASA GISS graph from 1987:

Schneider, S. H. 1989. The greenhouse effect: Science and policy. Science 243: 771-81.
Today, it is no longer acceptable for the NASA global temperature data set to graphically depict a strong warming trend during the first half of the 20th century.  This is because anthropogenic CO2 emissions were flat and negligible relative to today during this abrupt warming period.
So as to eliminate the inconvenience of a non-anthropogenic warming trend in modern times, NASA has now removed all or nearly all the 0.5°C of warming between 1880 and 1950.

NASA GISS graph

 

Soon et al., 2015   
• [B]etween 65-80% of the apparent warming trend over the 1961-2000 period for the Beijing and Wuhan station records was probably due to increasing urban heat islands.  [T]he temperature trends increase from +0.025°C/decade (fully rural) to … +0.119°C/decade (fully urban). … If we assume that the fully rural stations are unaffected by urbanization bias, while the other subsets are, then we can estimate the extent of urbanization bias in the “all stations” trends by subtracting the fully rural trends. This gives us an estimate of +0.094°C/decade urbanization bias over the 1951-1990 period [+0.38°C of additional non-climatic warmth]– similar to Wang & Ge (2012)’s +0.09°C/decade estimate.
•We have constructed a new estimate of Northern Hemisphere surface air temperature trends derived from mostly rural stations – thereby minimizing the problems introduced to previous estimates by urbanization bias.  
• Similar to previous estimates, our composite implies warming trends during the periods 1880s-1940s and 1980s-2000s. However, this new estimate implies a more pronounced cooling trend during the 1950s-1970s. As a result, the relative warmth of the mid-20th century warm period [1930s-1950s] is comparable to the recent [1980s-2000s] warm period – a different conclusion to previous estimates. Although our new composite implies different trends from previous estimates, we note that it is compatible with Northern Hemisphere temperature trends derived from (a) sea surface temperatures; (b) glacier length records; (c) tree ring widths.
• However, the recent multi model means of the CMIP5 Global Climate Model hindcasts failed to adequately reproduce the temperature trends implied by our composite, even when they included both “anthropogenic and natural forcings”. One reason why the hindcasts might have failed to accurately reproduce the temperature trends is that the solar forcings they used all implied relatively little solar variability. However, in this paper, we carried out a detailed review of the debate over solar variability, and revealed that considerable uncertainty remains over exactly how the Total Solar Irradiance has varied since the 19th century. 
• When we compared our new composite to one of the high solar variability reconstructions of Total Solar Irradiance which was not considered by the CMIP5 hindcasts (i.e., the Hoyt & Schatten reconstruction), we found a remarkably close fit. If the Hoyt & Schatten reconstruction and our new Northern Hemisphere temperature trend estimates are accurate, then it seems that most of the temperature trends since at least 1881 can be explained in terms of solar variability, with atmospheric greenhouse gas concentrations providing at most a minor contribution. 
• This contradicts the claim by the latest Intergovernmental Panel on Climate Change (IPCC) reports that most of the temperature trends since the 1950s are due to changes in atmospheric greenhouse gas concentrations (Bindoff et al., 2013).


New Paper: Since 1750, About 0.8°C – 0.9°C Of CET Increase Is Due To Solar Forcing

Smith, 2017
Yearly mean temperatures in the CET [Central England Temperature] record show an increase in temperature of approximately 1.3°C degrees from the end of the 17th Century to the end of the 20th Century/beginning of 21st Century.  …  Subtle difference in timing between the warming/cooling phases between the Central England record and the other localities may reflect local climate variation, but the similarity in events between continents suggests the CET [Central England Temperature] record is recording global temperature patterns.
Records of sunspot numbers began in 1610 such that detailed estimates of solar variation for the years covered by the CET record can be made without resort to the use of proxy data. Reconstructions of TSI [e.g. 16-18] differ in magnitude (Table 1), but there is agreement in form with 4 peaks and 4 to 6 troughs occurring over the time-scale of the CET record (Fig. 4). These are: a minimum in TSI associated with the Maunder Sunspot Minimum in the latter half of the 17th Century; a peak, possibly bi-modal approaching modern TSI values during the 18th Century; a well-defined trough corresponding with the Dalton Sunspot Minimum between 1800- 1820; a poorly defined TSI peak in the mid 19th Century; a reduction in TSI during the late 19th Century; increasing TSI during the early 20th Century; a decrease in TSI from around 1950- 1975; and a second phase of TSI increase in the late 20th Century [1980s-2000s]. There is good correspondence with TSI throughout the CET record, with warm events correlating with high TSI and cool phases correlating with plateaus or decreases in TSI .
However, for temperature increases from the beginning of the Industrial Revolution (Maunder Minimum and Dalton Minimum to end of 20th Century), high TSI models can account for only 63-67% of the temperature increase. This would suggest that one third of Global Warming/Climate Change can be attributed to AGW. … Approximately two-thirds [0.8°C to 0.9°C] of climate warming since the mid-late 18th Century [1.3°C] can be attributed to solar causes, suggesting warming due to anthropogenic causes over the last two centuries is 0.4 to 0.5°C.


All Over The Globe, Trends In Solar Forcing Correlate With Temperature Changes



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Christiansen and Lungqvist (2012)


Stoffel et al., 2015


Schneider et al., 2015  and  Wilson et al., 2016


Kim et al., 2017


Yamanouchi, 2011


Box et al., 2009


Southern Hemisphere

Schneider et al., 2006


De Jong et al., 2016
“[T]he period just before AD 1950 was substantially warmer than more recent decades.”


de Jong et al., 2013


Zinke et al., 2016


Turney et al., 2017


Elbert et al., 2013


“[I]n the framework of empiric [observable] models, the estimate of the solar activity contribution in the variation in the air global temperature in the 20th century is about 70%.” – Kovalenko and Zherebtsov, 2014
Share this...FacebookTwitter "
nan
"With just under a week to go, more than £750,000 has been donated to the Guardian and Observer climate emergency charity appeal, which supports projects planting and protecting trees, woodlands and forests. More than 10,000 readers have contributed to the appeal, which promotes environmental and social justice through natural climate solutions, from safeguarding rainforests in the Amazon to rewilding the Scottish Highlands and planting trees in Britain’s towns, cities and countryside. The charities are: Woodland Trust, Trees for Life, Trees for Cities and Global Greengrants Fund UK. Responding to the news, the chair of Global Greengrants Fund UK, Stephen Pittam, said: “Thanks to Guardian and Observer readers we can enable more local communities in the Amazon region of Brazil to continue their struggles for environmental justice. Supporting these frontline defenders of the largest rainforest on earth is crucial for the future of every single person on this planet.” Scores of messages left by donors show many felt compelled to give in part to signal their frustration at the failure of many elected politicians to take the climate crisis seriously, as well as the need to take personal action in the absence of large-scale solutions to the most important issue facing the planet. One reader said: “I feel that large-scale government action is necessary in order to avert climate disaster. But that shouldn’t stop each of us individually trying to change our behaviours. We see this donation as a way of mitigating some of the climate damage we have been responsible for over the last year as a family.” Another said: “Trees can save the earth. Let’s stop cutting the ancient ones down, and cover bare land with young trees to help and beautify the future.” Introducing the appeal in December, the Guardian editor-in-chief, Katharine Viner, said although the onus was on governments and corporations to take major steps to avoid global climate catastrophe, this year’s charity appeal “highlights ways we as citizens can support practical, natural solutions to climate change”. The appeal continues until midnight on Sunday 12 January. Readers can donate online here or send a cheque (payable to the Guardian and Observer charity appeal 2019) to: The Guardian and Observer charity appeal 2019, Charities Trust, Suite 20-22, Century Building, Tower Street, Liverpool, L3 4BJ."
"Across the North Atlantic, shipwrecks scatter the seabed like the carcasses of prehistoric creatures. Bygone relics of sea exploration, trade, migration and conflict, these historical monuments are important sites of cultural interest. But they also form the basis of a burgeoning recreational dive tourism industry, and contribute substantially to the biodiversity and abundance of marine life. The stories behind how these ships came to rest on the sea bed are intriguing. In the Orkney Isles off the north coast of Scotland, for example, a major naval event at the close of World War I set the wheels in motion for the creation of a world-renowned dive site of considerable historic and environmental value. The captured German warship fleet (comprising 74 vessels) was interned in Scapa Flow, a sheltered body of water between the Orkney Mainland and the South Isles. In June 1919, as its fate was debated by the allied powers, Rear Admiral Ludwig von Reuter gave the order to his German crews to scuttle all ships to prevent the flotilla falling into enemy hands. A total of 52 ships were successfully sunk, leaving Scapa Flow an undersea graveyard of WWI German shipwrecks. But this is a graveyard full of life. A hundred years later, marine habitats around the Scapa Flow wrecks are thriving. The waters teem with an abundance of sea creatures including crab, lobster, starfish, sea urchin and a variety of fish species. Recent surveys around the Orkney wrecks by Seasearch volunteer divers have also reported rarities such as the fan mussel and the common skate, now internationally scarce. As well as being habitats where marine species can establish and flourish, these wrecks may also be some of the most biodiverse habitats of northern waters. Take the Karlsruhe for instance, a 112-metre German light cruiser, once operated by a crew of 475 men, now lying abandoned on its starboard side in 25 metres of water in Scapa Flow. Its toppled foredeck guns and armoured control tower can still be seen, creating an eerie but captivating scene. Nearby, off the deck side of the resting hull, the seabed is covered by a horse mussel bed that hosts a range of species including brittle stars, herringbone hydroids, large predatory spiny starfish and seven-armed starfish. Sea cucumbers can also be found here among the dense clumps of horse mussels. Horse mussels are bivalve molluscs similar to the well-known, edible blue mussel, but much larger and longer-lived. Their notable size and longevity, along with an ability to bind to one another via thin secreted threads called byssus, allow these creatures to create extensive reefs that provide habitat for a wealth of other marine creatures. This makes horse mussel reefs biodiversity hotspots, with some supporting hundreds of other species. These reefs also provide a number of beneficial ecosystem “services” including the provision of nursery grounds for commercial fisheries species. Unfortunately, these reef habitats are currently listed as endangered in European waters, in part due to their historical destruction by bottom-towed fishing equipment like trawlers. But under climate change these optimal habitat conditions will vastly dwindle and push these reef habitats to their northern limits as more southern temperature conditions become intolerable. Research into the specific impact of climate change on horse mussels is only beginning to emerge, but studies have shown that the effects on other bivalve species such as the blue/common mussel  include changes in shell strength, disease and reproduction issues.  Marine biologists believe wrecks like the Karlsruhe may be instrumental in creating ideal hydrodynamic conditions for vital habitats like horse mussel reefs, but this is yet to be fully investigated.  Wrecks may help to produce good feeding conditions, that is, sufficient water flow to maintain a supply of algal feed but not too vigorous that animals are unable to filter feed, and natural breaks and eddies in high current flow that allow for the settlement of mussel larvae on the sea floor. Larvae are the microscopic offspring of invertebrate species which use ocean currents to move across large areas. In doing so, they ensure important genetic mixing between populations. Recent research by Heriot-Watt University researchers in collaboration with Scottish Natural Heritage has shown that the Karlsruhe horse mussel reef is likely genetically connected (via the transfer of genetic material between sites, termed “gene flow”) to horse mussel reefs hundreds of miles away along the west coast of Scotland. Whether settlement of larvae arriving from far-off sites could occur without the wreck in place remains unknown, but it’s certainly an interesting question to ponder. Unfortunately, deterioration of these sites is a major challenge faced by local, recreational and scientific diving operations in northern communities whose livelihoods rely heavily on these underwater habitats. The marine environments of northern regions like the Orkneys also face a serious threat from global climate change due to the accelerated rates of change occurring in polar regions. However, a lack of research on impacts has meant that communities face huge challenges in finding ways to react and adapt to these problems. Dive operators, researchers and volunteers continue to work together to document these historic relics and their ecological associations. June 2019 marks the centenary of the sinking of German WWI warships in Scapa Flow. We should mark this important moment in history by drawing attention to these precious marine habitats so they may be protected for hundreds of years to come."
"
Share this...FacebookTwitter Cooling, Not Warming, Leads To
  Weather and Climate Instability 

Image Source: Loisel et al., 2017

1. Significant Decreasing Trend In Severe Weather Since 1961
Zhang et al., 2017
Based on continuous and coherent severe weather reports from over 500 manned stations, for the first time, this study shows a significant decreasing trend in severe weather occurrence across China during the past five decades. The total number of severe weather days that have either thunderstorm, hail and/or damaging wind decrease about 50% from 1961 to 2010. It is further shown that the reduction in severe weather occurrences correlates strongly with the weakening of East Asian summer monsoon which is the primary source of moisture and dynamic forcing conducive for warm-season severe weather over China.

2. Most Frequent Climate Instability During Global Cooling/Reduced CO2 Periods
Kawamura et al., 2017
Numerical experiments using a fully coupled atmosphere-ocean general circulation model with freshwater hosing in the northern North Atlantic showed that climate becomes most unstable in intermediate glacial conditions associated with large changes in sea ice and the Atlantic Meridional Overturning Circulation. Model sensitivity experiments suggest that the prerequisite for the most frequent climate instability with bipolar seesaw pattern during the late Pleistocene era is associated with reduced atmospheric CO2 concentration via global cooling and sea ice formation in the North Atlantic, in addition to extended Northern Hemisphere ice sheets.

3. Hurricane Activity Is ‘Subdued’ During Warm Periods (1950-2000)
Heller, 2017


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The hurricane analysis conducted by Burn and Palmer (2015) determined that hurricane activity was subdued during the [warm] Medieval Climate Anomaly (MCA) (~900-1350 CE) and became more produced during the [cold] Little Ice Age (LIA) (~1450-1850 CE), followed by a period of variability occurred between ~1850 and ~1900 before entering another subdued state during the industrial period (~1950-2000 CE). In general, the results of this study corroborate these findings … [W]hile hurricane activity was greater during the LIA, it also had more frequent periods of drought compared to the MCA (Burn and Palmer 2014), suggesting that climate fluctuations were more pronounced in the LIA compared to the MCA. The changes in the diatom distribution and fluctuations in chl-a recorded in this study starting around 1350 also indicate that variations in climate have become more distinct during the LIA and from ~1850-1900.
[C]limate variability has increased following the onset of the Little Ice Age (~1450-1850 CE), however it is difficult to distinguish the impacts of recent anthropogenic climate warming on hurricane activity from those of natural Atlantic climate regimes, such as ENSO.

4. Surface Warming Weakens Cyclone Activity
Chen et al., 2017
Results indicate that the midlatitude summer cyclone activity over East Asia exhibits decadal changes in the period of 1979–2013 and is significantly weakened after early 1990s. …  Moreover, there is a close linkage between the weakening of cyclonic activity after the early 1990s and the nonuniform surface warming of the Eurasian continent. Significant warming to the west of Mongolia tends to weaken the north–south temperature gradient and the atmospheric baroclinicity to its south and eventually can lead to weakening of the midlatitude cyclone activity over East Asia.

5. More Hydroclimatic Variability During Cold Periods…Models Say Warming Causes More Instability, So The 21st Century Will Be Like The Little Ice Age, With More Instability/Megadrought
Loisel et al., 2017
Our tree ring-based analysis of past drought indicates that the Little Ice Age (LIA) experienced high interannual hydroclimatic variability, similar to projections for the 21st century. This is contrary to the Medieval Climate Anomaly (MCA), which had reduced variability and therefore may be misleading as an analog for 21st century warming, notwithstanding its warm (and arid) conditions. Given past non-stationarity, and particularly erratic LIA, a ‘warm LIA’ climate scenario for the coming century that combines high precipitation variability (similar to LIA conditions) with warm and dry conditions (similar to MCA conditions) represents a plausible situation that is supported by recent climate simulations. … Our comparison of tree ring-based drought analysis and records from the tropical Pacific Ocean suggests that changing variability in El Niño Southern Oscillation (ENSO) explains much of the contrasting variances between the MCA and LIA conditions across the American Southwest. The Medieval Climate Anomaly (MCA, ~950–1400 CE) is often used as an analog for 21stcentury hydroclimate because it represents a warm (and arid) period. The MCA appears related to general surface warming in the Northern Hemisphere, prolonged La Niña conditions, and a persistent positive North Atlantic Oscillation mode. It has been referred to as a stable time interval with ‘quiet’ conditions in regards to low perturbation by external radiative forcing. In this study, we demonstrate that the Little Ice Age (LIA, ~1400–1850 CE) might be more representative of future hydroclimatic variability than the conditions during the MCA megadroughts for the American Southwest, and thus provide a useful scenario for development of future water-resource management and drought and flood hazard mitigation strategies.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany Temperatures Baffle: September Mean Shows Hardly Any Warming In 70 Years
By Josef Kowatsch and Dr. Sebastian Lüning
(Translated and edited by P Gosselin)
Temperatures are rising and rising and rising. That’s what we read in any case in the daily newspaper, and that’s what some television professors, activists and climate scientists are telling us. Strangely rarely are temperature curves ever shown. Why is this so? One example is the September mean temperature for Germany, which we use to illustrate this peculiar media documentation gap.
Here we use the official DWD German Weather Service data. When we look at the past 100 years we see a very modest warming of just a few tenths of a degree (Fig. 1). This is no surprise as we find ourselves in the warming phase since the Little Ice Age, the coldest phase of the last 10,000 years. It would have been terrible had the climate stayed at this non-representative low level.

Figure 1: Chart depicting Germany September mean temperature over the past 100 years. Data source: DWD. 
It is easy to see the long cycles in the temperature curve. Above we a cold phase between 1920 and 1930, followed by a warm period during the Nazi time, and then followed by a long-term cold dip.
Beginning in 1985, September began to warm up again before reaching a plateau that took hold just before the year 2000 and at which we currently find ourselves. Based on the past development one could speculate that we are headed towards a slight cooling.
Now let’s look at the period from the end of WWII until today, more than 70 years, the time of the last temperature plateau until today. Immediately we see that we are far from worrisome climate warming (Fig. 2):


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2: Chart depicting Germany September mean temperature over the past 70 years. Data source: DWD. 
Finally we take a look at the past 13 years (Fig. 3), i.e. the development since 2004. Again there has not been any significant warming. In fact there’s been some cooling. Everything other than a climate catastrophe.

Figure 3: Chart of September mean temperatures in Germany over the past 13 years. Data source: DWD. 
Getting back to the primary question of why isn’t the German media showing the real German temperature curve, obviously the real facts are just too inconvenient. A pert of the public could even lose its faith in the much-preached climate catastrophe and end up sharply criticizing the harsh sacrifices now being made because of the climate fear that has been instilled by policymakers.
It’s high time for the issue to be made transparent and to push back against the activism. What’s needed is a new environmental protection ethic, one which addresses all the problems.
The excessive focus on the climate question is no longer sustainable and is even counterproductive. Other more important problems that can be solved over the short term require greater attention — clean water, clean air and clean food being evenly distributed — would be a common ethical goal for mankind to strive for. The fear-mongering climate protection issue is a repeat of the earlier business model of sin and the sale of indulgences.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterCO2 Climate Sensitivity So Low It’s ‘Impossible 
To Detect Or Measure In The Real Atmosphere’

“In particular, formula 5 (and 6) as presented here, totally rules out
any possibility that a 33°C greenhouse effect of the type proposed
by the IPCC in their reports can exist in the real atmosphere.”
– Holmes, 2017

In a new peer-reviewed scientific paper published in the journal Earth Sciences last December (2017), a Federation University (Australia) Science and Engineering student named Robert Holmes contends he may have found the key to unlocking our understanding of how planets with thick atmospheres (like Earth) remain “fixed” at 288 Kelvin (K), 740 K (Venus), 165 K (Jupiter)…without considering the need for a planetary greenhouse effect or changes in atmospheric CO2 concentrations.
The Greenhouse Effect ‘Thought Experiment’ 
Perhaps the most fundamental conceptualization in climate science is the “thought experiment” that envisions what the temperature of the Earth might possibly be if there was no greenhouse effect, greenhouse gases, or atmosphere.
Dr. Gavin Schmidt, NASA  
“The size of the greenhouse effect is often estimated as being the difference between the actual global surface temperature and the temperature the planet would be without any atmospheric absorption, but with exactly the same planetary albedo, around 33°C. This is more of a ‘thought experiment’ than an observable state, but it is a useful baseline.”
Simplistically, the globally averaged surface temperature clocks in at 288 K.   In the “thought experiment”, an imaginary Earth that has no atmosphere (and thus no greenhouse gases to absorb and re-emit the surface heat) would have a temperature of only 255 K.  The difference between the real and imagined Earth with no atmosphere is 33 K, meaning that the Earth would be much colder (and uninhabitable) without the presence of greenhouse gases bridging the hypothetical “heat gap”.
Of that 33 K greenhouse effect, 20.6 K is imagined to derive from water vapor droplets in the atmosphere (1,000 to 40,000 parts per million [ppm] by volume), whereas 7.2 K is thought to stem from the “natural” (or pre-industrial) 200-280 ppm atmospheric CO2 concentration (Kramm et al., 2017).
As a “thought experiment”, the critical heating role for water vapor droplets and CO2 concentrations lacks real-world validation.  For example, the Earth’s oceans account for 93% of the planet’s heat energy (Levitus et al., 2012), and yet no real-world physical measurements exist that demonstrate how much heating or cooling is derived from varying CO2 concentrations up or down over a body of water in volume increments of parts per million (0.000001).  Consequently, the CO2 greenhouse effect is a hypothetical, model-based conceptualization.
And in recent years, many scientific papers have been published that question the fundamentals of not only the Earth’s hypothetical greenhouse effect, but the role of greenhouse gases for other planets with thick atmospheres (like Venus) as well Hertzberg et al., 2017, Kramm et al., 2017, Nikolov and Zeller, 2017 , Allmendinger, 2017, Lightfoot and Mamer, 2017, Blaauw, 2017, Davis et al., 2018).   The Holmes paper highlighted here may just be among the most recent.
‘Extremely Accurate’ Planetary Temperature Calculations With Pressure/Density/Mass Formula
Holmes has argued that the average temperature for 8 planetary bodies with thick (0.1 bar or more) atmospheres can be precisely measured with “extreme” accuracy — an error range of just 1.2% — by using a formula predicated on the knowledge of 3 parameters: “[1] the average near-surface atmospheric pressure, [2] the average near surface atmospheric density and [3] the average mean molar mass of the near-surface atmosphere.”
Holmes used the derived pressure/density/mass numbers for each planetary body.   He then calculated the planets’ temperatures with these figures.
Venus’ temperature was calculated to be 739.7 K with the formula.  Its measured temperature is 740 K.  This indicates that the formula’s accuracy is within an error range of just 0.04% for Venus.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Given Earth’s pressure/density/mass, its calculated temperature is 288.14 K using Holmes’ formula.  Earth’s measured temperature is 288 K, an exact fit.
Saturn’s calculated temperature is 132.8 K.  Its measured temperature is 134 K — an error range of only 0.89%.
The impressive accuracy of the formula is illustrated below in Table 1. and Figure 2.

Atmospheric Pressure/Density And Surface Temperature
In large part, the density of a planet’s atmosphere is a primary determinant of its temperature.   Planets with thick atmospheres are hotter.  Planets with thin atmospheres are cooler.  The further away from the surface, the less gravity/pressure there is and the cooler it gets.  And vice versa.
Sciencing.com
“In general, the weaker the gravitational pull of a planet, the thinner the atmosphere will be. A planet with weak gravity will tend to have less mass and allow more atmosphere to escape into space. Thus the thickness or thinness of the atmosphere depends upon the strength or weakness of gravity. For example, the gravity on Jupiter is 318 times greater than Earth, and thus Jupiter’s atmosphere is much thicker than Earth’s. Gravity gets weaker the further away it is from a planet, so the atmosphere will be thicker near the surface.”
A facile illustration of the effects of atmospheric pressure on the surface temperatures of a planet like Earth can be found in the Grand Canyon, Southwestern U.S.  There, the North Rim is about 1,000 feet (305 meters) higher in elevation than the South Rim.  Interestingly, the North Rim is also about 9 degrees Fahrenheit colder than the South Rim due to the influence of atmospheric pressure/gravity.   The bottom of the canyon reaches temperatures 20-25 degrees warmer than the top.  The stark temperature difference is unrelated to the greenhouse gas concentrations for the two locations, nor is it connected to sunlight.   It’s the gravitational pressure that creates the heat divergence.
Subia, 2014
“Elevation and season of the year determine average temperatures at the the Grand Canyon. Elevations at top of the South Rim average around 7,000 feet. The North Rim averages about 8,000 feet. The higher the elevation, the cooler the temperature. At any given time, the North Rim will average 8-10 degrees Fahrenheit cooler versus the South Rim. … [T]he very bottom of the canyon can increase 20 to 25 degrees warmer than the top of the respective rims.”
Sensitivity To CO2 Concentration Changes ‘Extremely Low’
Holmes points out that the implications of his precise calculations for planetary temperatures necessarily lead to the conclusion that there is no need to have a greenhouse effect or greenhouse gases to bridge a hypothetical “heat gap.”  Instead, he writes that “planetary bodies with thick atmospheres cannot be mainly determined by the ‘greenhouse effect’, but instead most likely by an effect from fluid dynamics, namely, adiabatic autocompression.”
This effectively rules out the possibility that CO2 is a predominant climate driver.
In fact, Holmes’ calculation for CO2 climate sensitivity (doubling the atmospheric CO2 concentration from 0.03% to 0.06%) is -0.03°C.
As he ostensibly understates in his conclusion, “This climate sensitivity is already so low that it would be impossible to detect or measure in the real atmosphere.”

Holmes, 2017
Molar Mass Version of the Ideal Gas Law 
Points to a Very Low Climate Sensitivity
Introduction
Presented here is a simple and reliable method of accurately calculating the average near surface atmospheric temperature on planetary bodies which possess a surface atmospheric pressure of over 10kPa [a thick atmosphere, 0.1 bar or more]. This method requires a gas constant and the knowledge of only three gas parameters: [1] the average near-surface atmospheric pressure, [2] the average near surface atmospheric density and [3] the average mean molar mass of the near-surface atmosphere. The formula used is the molar version of the ideal gas law.
It is here demonstrated that the information contained in just these three gas parameters alone is an extremely accurate predictor of atmospheric temperatures on planets with atmospheres >10kPa. This indicates that all information on the effective plus the residual near-surface atmospheric temperature on planetary bodies with thick atmospheres, is automatically ‘baked-in’ to the three mentioned gas parameters.
This formula proves itself here to be not only more accurate than any other method heretofore used, but is far simpler to calculate.  It requires no input from parameters previously thought to be essential; solar insolation, albedo, greenhouse gas content, ocean circulation and cloud cover among many others.
Given this, it is shown that no one gas has an anomalous effect on atmospheric temperatures that is significantly more than any other gas.
In short, there can be no 33°C ‘greenhouse effect’ on Earth, or any significant ‘greenhouse effect’ on any other planetary body with an atmosphere of >10kPa.
The Formula: An ‘Extremely Accurate Predictor’ Of Planetary Temperatures

[T]he hypothesis being put forward here is that in the case of Earth, solar insolation provides the ‘first’ 255 Kelvin – in accordance with the black body law [11]. Then adiabatic auto-compression provides the ‘other’ 33 Kelvin, to arrive at the known and measured average global temperature of 288 Kelvin. The ‘other’ 33 Kelvin cannot be provided by the greenhouse effect, because if it was, the molar mass version of the ideal gas law could not then work to accurately calculate planetary temperatures, as it clearly does here.
It is apparent that this simple formula calculates the ‘surface’ temperatures of many planetary bodies in our Solar System accurately (Figure 2).
Specifically, those which have atmospheres thick enough to form a troposphere (i.e. possessing an atmospheric pressure of over 10kPa or 0.1bar). These are: Venus, Earth, Jupiter, Saturn, Titan, Uranus and Neptune. All calculated temperatures are within 1.2% of the NASA reported ‘surface’ temperature (except for Mars, which is excluded because it has a much lower atmospheric pressure than 10kPa).
This accuracy is achieved without using the S-B black body law, or the need to include terms for such parameters as TSI levels, albedo, clouds, greenhouse effect or, for that matter, adiabatic auto-compression. All that is required to be able to accurately calculate the average near-surface atmospheric temperature, is the relevant gas constant and the knowledge of three variable gas parameters.
The Implications: CO2 Climate Sensitivity (-0.03°C) ‘Extremely Low’
Some reflection upon the simplicity and accuracy of these results will bring an unbiased person to the obvious implications of this work. These are that the residual (residual being the difference between S-B law results and actual) near-surface atmospheric temperatures on planetary bodies with thick atmospheres cannot be mainly determined by the ‘greenhouse effect’, but instead most likely by an effect from fluid dynamics, namely, adiabatic autocompression.
Another implication leads directly to the conclusion that the climate sensitivity to, for example, a doubling of the atmospheric carbon dioxide concentration has to be operating instantaneously, and also must be extremely low. Under this scenario, the climate sensitivity to CO2 cannot be very different to the addition of a similar quantity of any other gas.
In particular, formula 5 (and 6) as presented here, totally rules out any possibility that a 33°C greenhouse effect of the type proposed by the IPCC in their reports [23] can exist in the real atmosphere. The reason is that the IPCC state in their reports that a 0.03% [300 ppm] increase in atmospheric CO2 (i.e. a doubling from pre-industrial levels) must result in a global temperature rise of ~3°C; (a range of 1.5°C to 4.5°C, which has hardly changed since 1990) [24]. This is the so-called ‘climate sensitivity’. Anything like this magnitude of warming caused by such a small change in gas levels is completely ruled out by the molar mass version of the ideal gas law.
Calculate for a doubling of CO2 from the pre-industrial level of 0.03% [300 ppm]:
Calculated temperature after doubling of CO2 to 0.06% ≈ 288.11K. Climate sensitivity to CO2 is ≈ 288.14 – 288.11 ≈ – 0.03K.
The change would in fact be extremely small and difficult to estimate exactly, but would be of the order -0.03°C. That is, a hundred times smaller than the ‘likely’ climate sensitivity of 3°C cited in the IPCC’s reports, and also probably of the opposite sign [cooling]. Even that small number would likely be a maximum change, since if fossil fuels are burned to create the emitted CO2, then atmospheric O2 will also be consumed, reducing that gas in the atmosphere – and offsetting any temperature change generated by the extra CO2. This climate sensitivity is already so low that it would be impossible to detect or measure in the real atmosphere, even before any allowance is made for the consumption of atmospheric O2.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterParts of Germany’s political leadership appear to be waking up to the harsh realities of green energies (wind and sun) and their inefficiencies.
Hat-tip: Die kalte Sonne
“Economic Minister accepts true condition of Energiewende”
The website of German national daily “Welt” here reported last month that Germany’s powerful Minister of Economics, Peter Altmaier “accepts the true condition of the Energiewende [transition to green energies]”.
In the Welt commentary, veteran journalist Daniel Wetzel wrote that Altmaier “avoided every mention of Germany functioning as a leader or role model”” for the world when it comes to green energies today. That’s change of course from what we used to hear.
Some ten years ago Germany boasted non-stop about being the global leader in green energies. Today, after seeing years of skyrocketing electricity prices and an increasingly destabilized power grid, the country has visibly backed off its once lofty green goals, which were aimed at making Germany 90% reliant on green energies by 2050. Lately it’s been dawning that this target was far too utopian.
Energiewende “no solution for single countries”
Wetzel quotes Altmaier, who was speaking before dozens of green business leaders at the international Energiewende Conference, stated that “the Energiewende will survive only if it is global” and that it is “no solution for single countries.”
In a nutshell, Altmaier admitted the Energiewende is a failure because it is already known that many other countries, like USA and China” are not going to adopt it and so will always have access to cheap, reliable energy and Germany will thus have no chance to compete internationally should it opt to stay on the green course.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Altmaier said it only made sense if it’s implemented worldwide. But today everyone knows worldwide implementation is a pipe dream and so Germany needs to start forgetting about its once ambitious Energiewende..
Wetzel then comments:
Altmaier’s sober message to the international eco-electricity scene: An Energiewende is more difficult than one thinks, and it takes longer than many think it does.”
Only efficient when everyone else accepts being inefficient
So why would Altmaier state that only a global Energiewende would make sense? To answer that one has to read between the lines. His claim in fact confirms that green energies are terribly inefficient, and thus uncompetitive, which means going it alone only makes the country inefficient and uncompetitive.
So according to Altmaier in order for the Energiewende to be “efficient in a country, all othe rcountries must adopt it and become energetically inefficient. Only when all countries become inefficient can Germany’s Energiewende be “efficient”.
No hurry to go green
Under the bottom line: Germany is no longer in a hurry to transform its energy supply system into a green one because it knows big competing coutries aren’t going to do it.
 
Share this...FacebookTwitter "
"It is not exactly glamorous work. Measuring sheep farts is smelly, time consuming and expensive. But for Dr Suzanne Rowe, a scientist who is breeding strains of sheep that emit less methane than regular flocks, there is nothing more important she feels she could be doing.  “New Zealand has really become a global leader in this space and there’s a lot of buzz around at the moment – it’s hugely exciting,” she says. The release of methane gas from New Zealand sheep and cattle accounts for one third of the country’s greenhouse gas emissions, the single largest contributor in the country. Since 1990, methane gas from stock has increased by 10%, according to the ministry of the environment, along with a 70% increase in dairy cattle and a 44% drop in sheep numbers. Accordingly, methane has found itself in the crosshairs of the government’s climate action policy, and scientists around the country are being given the green light to run free with their best and brightest ideas to lower emissions. In November, the prime minister, Jacinda Ardern, pushed the Zero Carbon Act through parliament with cross-party support, saying the world was “undeniably warming” and all greenhouse gases must be reduced to net zero by 2050 in order to honour agreements made under the Paris climate accords. “We have to start moving beyond targets,” Ardern said. “We have to start moving beyond aspiration. We have to start moving beyond hope and deliver signs of action.” Under the zero carbon legislation, the methane targets are separated out from other greenhouse gases, with the goal of reducing biogenic methane by 10% by 2030, and 24-47% by 2050.  “Agriculture is incredibly important to New Zealand,” she said. “But it also needs to be part of the solution. That is why we have listened to the science and also heard the industry and created a specific target for biogenic methane.” In Nelson, at the top of the South Island, the Cawthron Institute has recently been awarded government funding to cultivate and research a red native seaweed known as Asparagopsis armata. The agriculture minister, Damien O’Connor, has said if this seaweed is able to be mass produced, it could be a “game-changer for farmers here and around the world” as it has been proven to reduce stock methane emissions by as much as 80% when added as a feed supplement at quantities as low as 2%. “The holy grail is going to be getting enough of the stuff,” Cawthron chief executive Prof Charles Eason told the Guardian. “It’s got a complicated life cycle. One of the barriers to getting this to market is growing enough of the stuff in ways that are cost effective to make it commercially viable.” O’Connor said the potential of the seaweed not only in New Zealand but around the globe was “huge”. “Other products typically provide reductions of between 10% and 20%,” he said. “Australian research estimates that if just 10% of global ruminant producers adopted Asparagopsis as an additive to feed their livestock, it would have the same impact for our climate as removing 50 million cars from the world’s roads.” Several hundred kilometres north in the Waikato city of Hamilton – the heart of dairy country – Dr Bjorn Oback from AgResearch has been given NZ$10m in government funding to design a “climate-smart cow”. There are more cows than people in New Zealand and while they have become a mainstay of the economy they also draw the ire of environmentalists for their high emissions profile, intensive impact on the land and muddying and polluting waterwarys. Oback says specialised breeding programmes can “take decades” to reach their desired outcome, but using genetic modification his team can design a climate-smart cow much more quickly and precisely – though the process remains controversial. As well as working towards manipulating the genes that control methane admissions , Oback is also focused on designing cattle that will thrive in a climate-altered future – animals that are more heat tolerant, hardy and productive. The five-year programme is only three months in, but is already targeting coat colour adapation – creating cows that are lighter in colour, making them more heat-tolerant. An important feature of the programme was using “elite” cattle, Oback said, so not only would they emit less methane and be climate adaptive – they’d also be super performers. “We’re taking a high-performing elite dairy background, and then we’re putting mutations on top of it.” Oback’s work is intertwined with Rowe’s research into low emitting sheep in the south of the country. For Oback to study – and try to manipulate – the genes of low-emitting stock, he is invested in the survival of Rowe’s flocks, who are now in their third generation. Rowe and her team’s work is creating a buzz in the agricultural community, because not only do her sheep have low methane emissions, they also produce more wool and are shown to be hardier and healthier than normal sheep. At this point no-one knowns why, but 20 major breeders have already signed on to produce flocks, and the low-emission sheep have the backing of industry body Beef and Lamb New Zealand. “We have demonstrated a 10% difference in methane produced between the average sheep in both the high and low methane breeding lines,” says Rowe. “It is a very natural system, it’s been used for eons. Domestication and breeding animals is something everybody is familiar with and it’s cumulative. If you breed an animal tomorrow the affect is always there and the offspring that come after them always have that effect.” Many of the methane reduction programmes in New Zealand are in their nascency, with climate smart cows at least five years away and the mass roll-out of Asparagopsis armata at least five to ten. But agricultural scientists and farmers say after decades of indecision and uncertainty concrete action is finally underway in New Zealand – and it is drawing global attention. Myles Allen is a climate change scientist at Oxford University and head of its Climate Dynamics group. He recently visited New Zealand and said he was impressed with how engaged industry leaders, government and the farming community were. “I only wish New Zealand could convince others … to take such a sensible approach.” he said.  "
"
Share this...FacebookTwitter…EU enacts law to regulate the color of potato and grain-based foods with the aim of protecting public from high cooking temperatures

Too dark! EU now regulating bread color from baking at too high temperatures. Photo credit: Fritzs, Creative Commons Attribution-Share Alike 3.0 Unported.
The following should be a viewed as a shot across the bow concerning the extent and zeal to which the EU is capable of when it comes to regulation. Just imagine if they were given a free hand to regulate all things related to CO2 and climate change. It’s getting frightening.
I’ve gotten used to the high levels of regulation here in Europe, and it’s gotten tough to surprise me. Yet, EU bureaucrats never fail at finding new ways to do so.
Nanny state
The latest pertains to the color of bread. The most recent news on EU regulation is reported for example by the Austrian online Wochenblick here, which writes: “EU regulation: Effective immediately our bread is not allowed to be too dark”!
The risk, according to the EU, is acrylamide, a carcinogenic compound that can form on starchy food if the cooking temperature is too high (some studies suggest).
European nannies are afraid some people could get sick from eating overly dark bread.
Meanwhile, it’s business-as-usual for the obviously dangerous product sugar, whose consumption in Europe has reached dangerous levels with diabetes becoming an epidemic. But the days of unregulated sugar use may also be coming to an end. Regulating sugar would make sense.
Aim is to curb acrylamide from high temperature cooking


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The dark-bread regulation went into effect last Wednesday, and appears to be a part of the regulation package to ban golden, crispy french fries as well.
Wochenblick reports that the regulation are for all products based on potatoes, grains, and also coffee. The EU intends to conduct food inspections in the future.
Regulating living down to the detail
The new highly intrusive regulations are the latest rules aimed at limiting coffee machines, restricting wood-burning stoves, vacuum cleaner power ratings, and lawn mower emissions, to name a few.
A Google search already shows that EU regulations for outdoor barbecues are likely in the works. Readers can look into that themselves to determine if that’s the case or not. Just imagine what that would look like.

The EU is only just getting started with all the regulation-mania.
More background here.
 
Share this...FacebookTwitter "
nan
nan
"
Share this...FacebookTwitter 
In assessing the global-scale trends in near-surface (0-20 m) ocean temperatures between 1900 and 2010, Gouretski et al. (2012) determined that the world’s oceans warmed by about 1.1°C between 1900 and 1945 (~0.24°C per decade), but then only warmed by an additional net 0.3°C between 1945 and 2010 (~0.046°C per decade), including a cooling trend between 1945 and 1975.
The early 20th century warming was therefore about 4 to 5 times greater both in magnitude and rapidity as the post-1945 warming.
Gouretski et al., 2012
“Both time series show a temperature increase from 1900 to about 1945, a slight decrease to the mid-1970s, and a temperature rise to the end of the record.”


Image: University of Hamburg, Gouretski et al., 2012
Interestingly, Gouretski et al. (2012) also point out that large regions of the oceans have been cooling since the 1990s.
“Decadal mean SST and 0–20 m layer anomalies calculated relative to the reference decade 2001–2010 give evidence of the general warming of the global ocean since 1900. However, large regions of the oceans have experienced cooling since the 1990s. Whereas cooling in the tropical Eastern Pacific ocean is associated with frequent La Nina events in the past decade, the cause of the cooling within the Southern Ocean remains unknown.”
According to Riser and 26 co-authors (2016), the globe’s oceans have warmed in some places, cooled in others, and the overall net change has been a warming of a little less than 0.2°C (0-1000 m) since about 1950, or about 0.03°C per decade.

The achievement of a few tenths of a degree of added warmth over the course of the last 6 ½ decades has been realized largely because the regions of the world where the oceans have been warming have slightly exceeded the cooling regions in volume.
The net difference between the warming and cooling trends for the globe is oddly referred to as global warming even though the warming trends have not been global, but regional.
Riser et al., 2016
“Most regions of the world ocean are warmer in the near-surface [0-700 m] layer than in previous decades, by over 1° C in some places. A few areas, such as the eastern Pacific from Chile to Alaska, have cooled by as much as 1° C, yet overall the upper ocean has warmed by nearly 0.2° C globally since the mid-twentieth century.”  
According to climate models and anthropogenic global warming theory, it has been expected that a long-term, gradually rising warming trend in the world-wide ocean would follow the trends associated with the rise of CO2 emissions.
The the oceans have not cooperated.
Instead, the world’s regional oceans have followed a decadal-scale variability, with pronounced warming and cooling episodes.   The lack of consistency with climate models has thus led scientists to conclude that it is “very difficult to determine whether significant anthropogenic change in [regional 0-2000 m ocean temperatures] … have occurred” (Yashavaev and Loder, 2017).
Below are several examples of the wide swaths of the Earth where ocean cooling (or non-warming) has been ongoing for at least the last decade to last several decades, including the North Atlantic Ocean, Pacific Ocean, Southern Ocean, and Indian Ocean.

North Atlantic

Yashayaev and Loder, 2017
“As a result of this intermittent recurrence of intensified Labrador Sea Water formation, the annual average temperature and density in the region’s upper 2000m have predominantly varied on a bi-decadal time scale, rather than having a long-term trend as might be expected from anthropogenic climate change. … [I]ntermittent recurrence of enhanced deep convection periods in the Labrador Sea, and the associated formation of major LSW classes, are contributing to a predominant decadal-scale variation in hydrographic properties which makes it difficult to determine whether anthropogenic changes are occurring. … This strong, apparently natural, decadal-scale variability makes it very difficult to determine whether significant anthropogenic changes in LSW formation and properties have occurred.”



Robson et al., 2017     
“In the 1990s anomalously strong ocean heat transport convergence dominates the SPG [Subpolar Gyre, North Atlantic] warming. … The cooling of the SPG [Subpolar Gyre, North Atlantic] after 2005 is dominated by a reduction in ocean heat transport convergence, particularly in the eastern SPG. The reduced ocean heat transport is largely due to a weakening ocean circulation.  By focusing on three independent case-studies of North Atlantic decadal change events the analysis presented here gives further support to the important role of ocean heat transport and ocean circulation in driving the observed changes in North Atlantic ocean heat content in the recent past.”


Gladyshev et al., 2017 
“After 2010, a sharp and stable freshening and cooling of SPMWs [Subpolar Mode Water] started in the eastern part of the North Atlantic. In the years 2010–2016, the mean temperature of the SPMW [Subpolar Mode Water] core in the Rockall Trough dropped by -0.73°C (-0.12°C/yr); in the Iceland Basin it dropped by -2.12°C (-0.35°C/yr), and salinity decreased by 0.12 psu (0.02 psu/yr) and 0.23 psu (0.04 psu/yr), respectively.”

Kim et al., 2017


de Jong and de Steur, 2016


Rosenthal et al., 2017


Pacific Ocean

Cheung, 2017


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->






Wilson et al., 2017


Li, 2017     
“In the Southern Ocean, the increasing trend of the total OHC slowed down and started to decrease from 1980, and it started to increase again after 1995. In the warming context over the whole period [1970-2009], the Pacific was losing heat, especially in the deep water below 1000 m and in the upper layer above 300 m, excluding the surface 20 m layer in which the OHC kept increasing through the time.”


Southern Ocean

Kusahara et al., 2017 
“In contrast to a strong decrease in Arctic sea ice extent, overall Antarctic sea ice extent has modestly increased since 1979. Several hypotheses have been proposed for the net Antarctic sea ice expansion, including atmosphere/ocean circulation and temperature changes, sea ice-atmospheric-ocean feedback, increased precipitation, and enhanced basal meltwater from ice shelves. Concomitant with this positive trend in Antarctic sea ice, sea surface temperatures (SSTs) over the Southern Ocean south of approximately 45°S have cooled over this period [since 1979].”

Latif et al., 2017
“The Southern Ocean featured some remarkable changes during the recent decades. For example, large parts of the Southern Ocean, despite rapidly rising atmospheric greenhouse gas concentrations, depicted a surface cooling since the 1970s, whereas most of the planet has warmed considerably. In contrast, climate models generally simulate Southern Ocean surface warming when driven with observed historical radiative forcing. The mechanisms behind the surface cooling and other prominent changes in the Southern Ocean sector climate during the recent decades, such as expanding sea ice extent, abyssal warming, and CO2 uptake, are still under debate. Observational coverage is sparse, and records are short but rapidly growing, making the Southern Ocean climate system one of the least explored. It is thus difficult to separate current trends from underlying decadal to centennial scale variability.”

Turney et al., 2017     
“Occupying about 14% of the world’s surface, the Southern Ocean plays a fundamental role in ocean and atmosphere circulation, carbon cycling and Antarctic ice-sheet dynamics. … As a result of anomalies in the overlying wind, the surrounding waters are strongly influenced by variations in northward Ekman transport of cold fresh subantarctic surface water and anomalous fluxes of sensible and latent heat at the atmosphere–ocean interface. This has produced a cooling trend since 1979.”


Jones et al., 2016


Indian Ocean

Zinke et al., 2016


Yan et al., 2015


Both The Cooling And The Warming Trends For Recent Decades Follow Natural Oscillatory Patterns, Not Trends In Anthropogenic CO2 Emissions

Gong et al., 2017
“The inter-annual relationship between the boreal winter Arctic Oscillation (AO) and summer sea surface temperature (SST) over the western tropical Indian Ocean (TIO) for the period from 1979 to 2015 is investigated. The results show that the January–February–March AO [Arctic Oscillation] is significantly correlated with the June–July–August SST and SST tendency. … The multi-month SST tendency, i.e., the SST difference of June–July–August minus April–May, is correlated with the winter AO at r = 0.75.Investigation of the regional air–sea fluxes and oceanic dynamics reveals that the net surface heat flux cannot account for the warming, whereas the oceanic Rossby wave plays a predominant role. During positive AO winters, the enhanced Arabian High causes stronger northern winds in the northern Indian Ocean and leads to anomalous cross-equatorial air-flow. … The winter AO-forced Rossby wave propagates westward and arrives at the western coast in summer, resulting in the significant SST increase.”


Belohpetsky et al., 2017
“It is well known that most short term global temperature variability is due to the well-defined ENSO natural oscillation (see: Wang and Fiedler, 2006). During strong El Niño events global average temperature rises by a few tenths Kelvin and reverts back subsequently. … The residual dynamics left after adjusting global surface temperature anomalies (1950-2014) for short-term variability from El Niño Southern Oscillation (ENSO) and volcanic eruptions have a staircase pattern. Linear trends for three quasi-stable periods 1950-1987, 1988-1997 and 1998-2014 are near zero with nearly all warming occurring during two step-like shifts in the years 1987/1988 and 1997/1998.  A notable consequence of the staircase dynamics of recent warming is that observed temperature anomalies (HadCRUT4.5) from 1950 till 2014 could be almost reproduced as the linear sum of only two factors(!) : ENSO variability and the staircase function.”

Gong et al., 2017
“During the past three decades, the most rapid warming at the surface has occurred during the Arctic winter. By analyzing daily ERA-Interim data, we found that the majority of the winter warming trend north of 70°N can be explained by the trend in the downward infrared radiation (IR). This downward IR trend can be attributed to an enhanced poleward flux of moisture and sensible heat into the Arctic by poleward propagating Rossby waves, which increases the total column water and temperature within this region.”

He et al., 2017 
“As pointed out by Cohen et al. (2014) that continental winter SAT [surface temperature] trends since 1990 exhibit cooling over the midlatitudes. The negative trends extend from Europe eastward to East Asia, with a center of maximum magnitude to the west of the Baikal.  As reviewed above, the AO/NAO [Arctic Oscillation/North Atlantic Oscillation] shows an in-phase relationship with the SAT [surface temperatures] over Eurasia. … [T]he negative trend in the AO/NAO might explain the recent Eurasian winter cooling. … Additionally, the relationship between the winter AO and surface-climate anomalies in the following spring might be modulated by the 11-year solar cycle (Chen and Zhou, 2012). The spring temperature anomalies in northern China related to the previous winter AO were larger and more robust after high solar cycle winters. However, spring temperature anomalies became very small and insignificant after the low solar cycle winters. … Numerous atmospheric scientists have documented that the AO could impact significantly the climate over Europe and Far East. …  It is evident that a positive winter AO causes warmer winters over East Asia through enhancing Polar westerly jet which prevents cold Arctic air from invading low latitudes.”


Wu et al., 2017
“The enhanced warming observed in the Eastern China Coastal Waters (ECCW) during the last half-century has received considerable attentions. However, the reason for this warming is still a subject of debate. Based on four different Sea Surface Temperature datasets, we found that the most significant warming occurred in boreal winter during 1982–1998, although the warming trends derived from these datasets differ in magnitude. We suggest that the rapid warming during winter is a result of the asymmetry in the El Niño–Southern Oscillation teleconnection, through which El Niño events induce significant warming over the ECCW at its peak, whereas La Niña events fail to do the opposite that would completely reverse the trends; in addition, there were more El Niño than La Niña events during the recent decades. All these contribute to the winter warming during 1982–1998.”

Mermelstein, 2017     
“[T]he 1940-1978 decrease in CONUS [continental U.S.] temperatures was caused more by the negatively trending oscillatory modes of the AMO/PDO than other factors, and the 1978-2001 increase in temperatures was caused more by the positively trending oscillatory modes of the same oscillations. The small increase, or rather stagnant nature in U.S. CONUS temps since 2001, was likely due to peaking positive modes of the AMO/PDO. In the same way that the AMO and PDO can modify the regional temperatures, we see the same types of effects on precipitation, snowfall and drought in the different regions of the U.S. … It was not until 2003 (Anastasios, Swanson, & Kravtsov, 2003, 2007) that models were created that suggested that these cycles, namely the Pacific Decadal Oscillation (PDO) and the Atlantic Multidecadal Oscillation (AMO) synchronized with each other. Using this as a base, we can explain the major climate shifts that have occurred since scientists began collecting data in the late 1800’s: 1908, 1932, 1973, and 2000. While the most noticeable change in these shifts was on global temperature, effects on the regional, sensible weather in the U.S. were also identified in these same time frames. Through analysis it has been theorized that these shifts are caused by the oceans, and are in fact the main drivers of the climate, and the sensible weather experienced in the United States (Klotzbach & Gray, 2009).”

Fan and Yang, 2017
“The wintertime Arctic temperature decreased from 1979 to 1997 and increased rapidly from 1998 to 2012, in contrast to the global mean surface air temperature [which] increased between 1979 and 1997, followed by a hiatus… A recent study suggests a possible role of the Pacific Ocean decadal oscillation in regulating wintertime climate in the Arctic (Screen and Francis 2016).  … The ‘‘greenhouse effect’’ of water vapor and clouds [CO2 not mentioned as contributing to the GHE] may amplify the effect of winds on Arctic winter climate. …  The objectives of this study are to assess how much natural–internal variability has contributed to climate changes in these [Arctic] regions from 1979 to 2012 … In summary, the correlation analyses presented in this paper shows a natural mode of Arctic winter variability resulting from the Nordic–Siberian seesaw of meridional winds […] is associated with two-thirds of the interannual variance [cooling-warming] of winter-mean Arctic temperature between 1979 and 2012, and possibly contributed a substantial fraction of the observed Arctic amplification [1998-2012 warming] in this period.”


Piecuch et al., 2017
“The subpolar North Atlantic (SPNA) is subject to strong decadal variability, with implications for surface climate and its predictability. In 2004–2005, SPNA decadal upper ocean and sea-surface temperature trends reversed from warming during 1994–2004 to cooling over 2005–2015. … Over the last two decades, the SPNA has undergone a pronounced climate shift. Decadal OHC and SST trends reversed sign around 2004–2005, with a strong warming seen during 1994–2004 and marked cooling observed over 2005–2015. These trend reversals were pronounced (> 0.1 °C yr−1 in magnitude) in the northeastern North Atlantic (south and west of Iceland) and in the Labrador Sea. … To identify basic processes controlling SPNA thermal variations, we diagnose the SPNA heat budget using ECCOv4. Changes in the heat content of an oceanic control volume can be caused by convergences and divergences of advective, diffusive, and surface heat fluxes within the control volume.  [Advective heat convergence] explains 87% of the total [ocean heat content] variance, the former [warming] showing similar decadal behavior to the latter [cooling], increasing over 1994–2004, and decreasing over 2005–2015. … These results demonstrate that the recent SPNA decadal trend reversal was mostly owing to advective convergences by ocean circulation … decadal variability during 1993–2015 is in largest part related to advection by horizontal gyres.”


Cheung, 2017
“The sea surface temperature (SST) of the Eastern Equatorial Pacific (EEP) exerts primary control on global surface temperature (e.g. Halpert and Ropelewski 1992; Wigley 2000) and regional climate (e.g. Ropelewski and Halpert 1987) through different modes of climate variability including the El Niño Southern Oscillation (ENSO) and the Pacific Decadal Oscillation (PDO). With such profound impacts, it is important to understand the evolution of SST in EEP, specifically the dynamics of these climate modes. Rigorous studies over the past decades have shed insights on these two climate modes. ENSO is known to affect regional and global climates on interannual timescales. During an El Niño event, a weakening of easterly trade wind stimulates propagation of Kelvin waves from the western equatorial Pacific to the EEP, which in turn reduces the slope of the thermocline and suppresses upwelling. The decrease in pressure gradient reinforces the weakening of the trade winds through the Bjerknes feedback and ultimately creates an El Niño condition (e.g. Collins et al. 2010). The reorganization of the ocean and the atmosphere due to El Niño raises the global mean surface temperature (e.g. Halpert and Ropelewski 1992; Wigley 2000) and alters regional climate, for example causing drought in Australia (Cai et al. 2011), pluvial in Southwest United States (Ropelewski and Halpert 1987), and changing tropical cyclone frequencies in the Western North Pacific (Camargo and Sobel 2005; Chan 1985). The opposite spatial pattern and teleconnections happen during a La Niña event.”

Wang et al., 2017
“The driving forces of climate change were investigated and the results showed two independent degrees of freedom —a 3.36-year cycle and a 22.6-year cycle, which seem to be connected to the El Niño–Southern Oscillation cycle and the Hale sunspot cycle, respectively. … Solar variability has been shown to be a major driver of climate in central Europe during the past two millennia using Δ14C records. Furthermore, this result is essentially in good agreement with the findings of Scafetta e.g. refs 17, 18, 19, who found that the climate system was mostly characterized by a specific set of oscillations and these oscillations (61, 115, 130 and 983 years) appeared to be synchronous with major astronomical oscillations (solar system, solar activity and long solar/lunar tidal cycles).”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterPublic opinion of wind energy in Germany, once unanimously high, has eroded considerably over the past years as more people begin to realize that the country’s once idyllic countryside is turning into a blighted industrial landscape.
The earlier visions of environmental purity are instead turning out to be illusions by con-artists. The reality is an environmental hell.
In an opinion piece at German flagship daily Die Welt, Wolfgang Büscher wrote:
Wind energy is destroying the country more than any other industry.” 
Blighted landscape…everything “blinking and rotating”
And the irony could not be more glaring. German environmentalists used to be devoted to all things that protected forests and rural countryside against the ravages of industrialization. But now, Büscher writes, “It’s the same activists who are blandishing the blighting of the landscape.”
Büscher describes the views of wind parks in Bavaria as “massively appalling”. While German industry once only ruined local areas of the country, such as the Ruhr industrial region, Büscher adds:
The wind industry is not satisfied with that. It wishes to subject the entire country to its moral galvanized industry. Whether it’s Magdeburg or Warburg Saxony Anhalt regions, whether it’s Holstein or the lower Harz region – everything is rotating and blinking, the further north you go, the worse it is.”
Büscher forgot to mention “whooshing” and “shredding”.
Germany’s march into environmental insanity
Yet, in the eyes of the wind industry, Büscher laments, “The wind industrialization of Germany is not only without alternative, it is an aesthetic benefit. The industry truly believes the entire country needs to by planted with turbines from border to border.”
As incredible as it may be that a country managed to get its citizens to march into the murderous folly of Nazism some 85 years ago, a similar phenomenon is happening today on the environmental front: there’s now the mad march into environmental murder. The collective German conscience still truly believes it’s all going to rescue the planet from a climate calamity which a group of (false) prophets foresee arriving in the year 2100. Environmental purity awaits!
Back on the eerie path of self-destruction


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of course wind industrialization cannot be compared to Nazism on the scales of murder and destruction, but the path and development stages of the two insanities are the eerily similar. Both proceeded as follows: 1) visions and promises of purification, 2) the mad, blind rush into the project 3) critics silenced, 4) signs of mounting failure ignored, 5) denial, retreat to the bunkers and 6) in evitable self-destruction. In both cases it’s all brought on by intoxicated leaders, one-sided media apparatus, crony industries, dogmatist institutions and unscrupulous banks.
Once madness goes collective, it becomes very difficult to stop.
When Germany gets an obsession in its head, the only thing that is left to stop it is the act of letting it play out and until it destroys itself. We may soon be witnessing the Rise and Fall of The Environmental Reich. Currently we find ourselves somewhere between phases 4 and 5.
Fortunately there are shimmers of hope that leading politicians are beginning to understand, as the newly formed Merkel-led government seems to be in no hurry to keep promoting big wind at any cost.
Former federal minister: “high price to pay”
Not only is the environmental price mentioned by Büscher excruciatingly high, but so is the financial price of the Energiewende, this according to former German Transportation Minister, Peter Ramsauer, now Chairman of the Committee for Economic Cooperation and Development.
In a recent interview with the Passauer Neuen Presse (PNP), Ramsauer slammed Germany’s rapid march into the Energiewende, telling the Bavarian online daily: “We have a high price to pay for the Energiewende.”
“Object lesson to other countries”
Ramsauer warned against the hasty shutdown of half of the country’s nuclear power plants in the wake of the Fukushima disaster back in 2011, “but no one wanted to listen”.
The PNP writes that it was “a fundamental mistake to believe that it would be possible to adequately replace the nuclear power”. Ramsauer told the PNP:
Germany is paying a high price for it and offers an object lesson to other countries.”
The PNP summarizes: “There’s no going back. The deadline for shutting down the remaining nuclear reactors is 2022. Ramsauer says that Germany will have to get used to high electricity prices and dependency on Russian gas. The folly will then be complete.”
Share this...FacebookTwitter "
"When Donald Trump recently announced tariffs on steel and aluminium imports he was condemned by proponents of free trade across the world. His critics said the US president had not understood how protectionist policies would spell disaster for the world economy. Fair enough. But this is the same Trump whose decision to withdraw from the Paris climate agreement also met with massive disapproval.  Trump is simultaneously chided for refusing to cut emissions, and for promoting a trade policy that reduces the causes of such emissions. Both sets of critics may be right on their own terms, but the contradiction between the two reproaches exposes big problems in the mainstream modern worldview. Is it really reasonable to advocate for both more trade and greater concern for the environment? For centuries world trade has increased not only environmental degradation, but also global inequality. The expanding ecological footprints of affluent people are unjust as well as unsustainable. The concepts developed in wealthier nations to celebrate “growth” and “progress” obscure the net transfers of labour time and natural resources between richer and poorer parts of the world.  For instance, the household of an average American couple with one child has the equivalent of an invisible servant working full time for it outside the nation’s borders, while the average Japanese household with one child uses three hectares of land overseas. Yet such material asymmetry appears to be a side issue for mainstream economists, who continue to assert the overall benefits of free trade. This same ignorance is particularly apparent in the fight against climate change. Most environmentalists and researchers put their faith in new technologies for harnessing the sun and wind, and hope that politicians can be persuaded to act. But solar panels and wind farms are not merely products of human ingenuity that have been revealed to us by nature. Nor are they magical keys to limitless energy.  Renewable energy technologies emerged in this specific human society – inequality, globalisation and all – and their very feasibility is dependent on world market prices. Like other modern technologies they depend on high domestic purchasing power combined with cheap Asian labour, Brazilian land, or Congolese cobalt.  Almost 50 years ago the ecological economist Nicholas Georgescu-Roegen warned that the notion that solar power could replace fossil energy was an illusion, because it would require such enormous volumes of materials to harness the requisite amounts of diffuse sunlight to satisfy a modern high-tech society. Some of these materials are rare and expensive and degrade the environment. Moreover, the United Nations Environmental Programme recently warned that the world is heading for ecological disaster unless we use less resources per dollar of economic growth.  The Czech-Canadian energy researcher Vaclav Smil has found that switching to renewable energy would use up vast amounts of land, reversing the land-saving benefits of the Industrial Revolution. Meanwhile the money to invest in solar is still ultimately generated from cheap labour and cheap land. The fact that solar panels have recently become less expensive is partly because they are increasingly being manufactured by low-wage labour in Asia.  When viewed this way it is perhaps no wonder that renewable energy has not even begun to replace fossil energy, and has only been added to the still-increasing use of oil, coal and gas. Solar power still only accounts for about 1% of global energy use. It has hardly made a dent on the global use of energy for electricity, industry, or transports. And this cannot be blamed on the oil lobby, as is illustrated by the case of Cuba. Nearly all of the island’s electricity still derives from fossil fuels. There is obviously something problematic about shifting to solar power that goes beyond corporate obstruction. To explain it in terms of a lack of capital or in terms of the vast land requirements are two sides of the same coin.  So here is the impasse of modern civilization: the free trade promoted by most economists and politicians continues to drive a substantial part of the greenhouse gas emissions that they want to reduce, and yet the sustainable technologies they propose to cut emissions are in themselves dependent on economic growth, international trade, and the use of more and more natural resources.  So how to break this impasse? Economists could start by recognising that the economy is not insulated from nature, just as engineering is not insulated from world society. Global challenges of sustainability, justice and resilience all demand much more integrated thinking. This will involve confronting conventional ideologies of technological progress and free trade. Rather than nervously safeguarding world trade with its escalating greenhouse gas emissions, we have every reason to reconsider what might be perceived as true human progress and quality of life. Instead of economic policies maximising economic growth and resource use, humankind needs to develop an economy that is aligned with the constraints of our fragile biosphere – and a science of engineering that takes account of global inequalities."
nan
nan
"
Share this...FacebookTwitterA new paper by renowned Swedish sea level expert Prof. Axel Mörmer published in the International Journal of Earth & Environmental Sciences dumps lots of cold water on the premise that today’s sea level rise is caused by man and is unusual.
Mörner’s paper looks back at the last 500 years of sea level rise and shows that natural variables are the major drivers, and not man-made CO2-driven global warming.
Previously no study in the Fiji Islands had been devoted to the sea level changes of the last 500 years and so no serious prediction can be made. What was needed was a good understanding of the sea level changes today and in the past centuries. Mörner’s study helps to fill that gap and to answer questions concerning today’s sea level rise.
The Swedish scientist summarizes in the paper’s abstract that there is a total absence of data supporting the notion of a present sea level rise; on the contrary all available facts indicate present sea level stability.

Source: Mörner, Int J Earth Environ Sci 2017, 2: 137, https://doi.org/10.15344/2456-351X/2017/137
Sea level changes over the past 500 years at Ysawa Islands, Fiji, show that sea level was +70 cm high in the 16th and 17th centuries, -50 cm low in the 18th century and that stability (with some oscillations) prevailed in the 19th, 20th and early 21st centuries.
This, Mörner writes, is almost identical to the sea level change documented in the Maldives, Bangladesh and Goa (India), and thus would point to a mutual driving force.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The pattern is the same at other locations:

Source: Mörner, Int J Earth Environ Sci 2017, 2: 137, https://doi.org/10.15344/2456-351X/2017/137
The paper also states that the recorded sea level changes are anti-correlated with the major changes in climate during the last 600 years. Therefore, Mörner concludes that glacial eustasy cannot be the driving force.
The explanation behind the sea level changes, Mörner believes, seems to be rotational eustasy with speeding-up phases during Grand Solar Minima forcing ocean water masses to the equatorial region, and slowing-down phases during Grand Solar Maxima forcing ocean waster massed from the equator towards the poles.
The paper summarizes:
This means there are no traces of a present rise in sea level; on the contrary: full stability.”
About the author:
Nils-Axel (”Niklas”) Mörner took his Ph.D. in Quaternary Geology at Stockholm University in 1969. Head of the institute of Paleogeophysics & Geodynamics (P&G) at Stockholm University from 1991 up to his retirement in 2005. He has written many hundreds of research papers and several books. He has presented more than 500 papers at major international conferences. He has undertaking field studies in 59 different countries. The P&G institute became an international center for global sea level change, paleoclimate, paleoseismics, neotectonics, paleomagnetism, Earth rotation, planetary-solar-terrestrial interaction, etc.  Among his books; Earth Rheology, Isostasy and Eustasy (Wiley, 1984), Climate Change on a Yearly to Millennial Basis (Reidel, 1984), Paleoseismicity of Sweden: a novel paradigm (P&G-print, 2003), The Greatest Lie Ever Told (P&G-print, 2007), The Tsunami Threat: Research & Technology (InTech, 2011), Geochronology: Methods and Case Studies (InTech, 2014), Planetary Influence on the Sun and the Earth, and a Modern Book-Burning (Nova, 2015).
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHere’s something you don’t witness very often…German national public radio telling listeners that natural factors are behind observed changes in something related to climate.
I can’t tell you how many times I’ve heard the German media claim storms are linked to our disdainful energy gluttony. So it comes as quite a shock when you hear something about climate that doesn’t conform to Potsdam Institute dogmatism.
At their Die kalte Sonne site here, Dr. Sebastian Lüning and Prof. Fritz Vahrenholt bring up an example of how German DLF national radio. I’ve translated the German text:
=====================================
Hurricanes are developing more quickly today than 30 years ago due to the Atlantic ocean cycle
A team of researchers at the US Department of Energy and the Pacific Northwest National Laboratory recently made an exciting discovery: Apparently hurricanes are developing more quickly today than they did 30 years ago. Earlier it took longer, but now maximum strength is reached sooner.
The scientists have found the culprit – drum roll – no, it’s not the wanton activity of mankind, rather it’s the Atlantic AMO ocean cycle, which fluctuates with a period of 60 years. During the course of the AMO cycle, hurricanes change accordingly.
Here’s the press release from May 9, 2018:

Powerful hurricanes strengthen faster now than 30 years ago
The storms intensify more rapidly today due largely to a natural climate phenomenon
Hurricanes that intensify rapidly — a characteristic of almost all powerful hurricanes — do so more strongly and quickly now than they did 30 years ago, according to a study published recently in Geophysical Research Letters, a journal of the American Geophysical Union. While many factors are at play, the chief driver is a natural phenomenon that affects the temperature of the waters in the Atlantic where hurricanes are powering up, according to scientists at the U.S. Department of Energy’s Pacific Northwest National Laboratory and the National Oceanic and Atmospheric Administration. They found that a climate cycle known as the Atlantic Multidecadal Oscillation or AMO is central to the increasing intensification of hurricanes, broadly affecting conditions like sea temperature that are known to influence hurricanes.
Stronger hurricanes in a day’s time
Last year’s lineup of powerful storms — Harvey, Irma, Jose and Maria — spurred the scientists to take a close look at the rapid intensification process. This occurs when the maximum wind speed in a hurricane goes up by at least 25 knots (28.8 miles per hour) within a 24-hour period. It’s a rite of passage for nearly all major hurricanes, including the big four of 2017. The team, comprised of Karthik Balaguru and Ruby Leung of PNNL and Greg Foltz of NOAA, analyzed 30 years’ worth of satellite hurricane data encompassing 1986 through 2015. Information came from NOAA’s National Hurricane Center and the U.S. Navy’s Joint Typhoon Warning Center. Consistent with other studies, the scientists did not find that rapid intensification is happening more often nowadays.
But the scientists also looked closely at just how much the storms are strengthening. They found a sizeable jump in the strength of fast-growing storms — the storms are getting more powerful more quickly within a 24-hour period than they were 30 years ago. The team found that the average boost in wind speed during a 24-hour intensification event is about 13 mph more than it was 30 years ago — on average about 3.8 knots (4.3 mph) for each of the three decades studied. Several factors play a role when a hurricane gains more power rapidly, including the temperature of the surface of the ocean, humidity, characteristics of the clouds, the heat content in the ocean, and the direction of the wind at the surface compared to miles above. Among the biggest factors affecting the increase in magnitude in the last 30 years, according to the team’s analysis:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




–The amount of heat available in the uppermost layer of the ocean, known as the ocean heat content. The warmer the upper ocean, the more powerful a hurricane can become.
–Wind shear: The less the vertical wind shear — the difference in the direction and force of the winds at the surface compared to several miles into the air — the more powerful the hurricane can become.The influence of the Atlantic Multidecadal Oscillation
The team found that the biggest factor explaining the increasingly rapid intensification is the AMO. The result comes in part from analyses using 16 separate climate models to isolate the impact from global warming. “This was a surprise, that the AMO seems to be a bigger influence in rapid intensification than other factors, including overall warming,” said Balaguru, the first author of the paper.
The AMO governs how the temperature of the waters in the North Atlantic cycles between warmer and cooler, with each period typically lasting a decade or more. The cycling occurs for reasons scientists don’t completely understand, but it has broad effects on the environment. For example, it plays a big part in determining the heat content of the oceans, an important factor powering hurricanes. The AMO has generally been “positive” — causing warmer waters — since the late 1990s.
Balaguru noted that while rapid intensification historically has occurred more often in the western Atlantic, that’s not where the team found the increasing strength of the last 30 years. Rather, the phenomenon is strengthening more in the central and eastern Atlantic, especially to the east of the islands of the Lesser Antilles, which includes the Virgin Islands and Saint Kitts. That’s the same area where the AMO creates warmer waters and boosts ocean heat content, in the central and eastern Atlantic. That’s exactly the alley where hurricanes Irma, Jose and Maria powered up rapidly last year. It’s a proving ground of sorts where many of the most powerful hurricanes strengthen dramatically. Balaguru notes that teasing out the effects of the AMO from broader effects of global warming was beyond the scope of the current study but is a focus for scientists.”
Even the IPCC-trumpeting Deutschlandfunk (DLF) found this worth reporting. On May 9, 2018, listeners indeed heard on the daily program “Forschung Aktuell” (Current Research) the following points:
1) Despite climate change, hurricanes have not become more frequent (which totally contradicts the usual DLF claims on this subject).
2) The current faster strengthening of hurricanes has NOTHING to do with anthropogenic global warming (AGW), but rather it depends on the AMO phase.
3) The causes of the AMO cycles are unknown and have nothing to do with AGW.
Yet, it is a pity that these revolutionary climate-realist claims (by DLF standards) were presented in just a very short report and that the inconvenient facts were not reported on in greater detail…you can listen to this small DLF revolution here (starting at 1:33).
==================================
Don’t hold your breath thinking this is a new media awakening happening in Germany. Expect Stefan Rahmstorf of the alarmist Potsdam Vatican to order the science illiterate DLF editors to be led deep down somewhere in the catacombs, and be made to recant the heresy.
Share this...FacebookTwitter "
"The New South Wales police investigation into energy minister Angus Taylor over the use of doctored documents to attack Sydney’s lord mayor has been referred to the Australian federal police. The minister greeted the news by claiming NSW police had confirmed to his solicitors they had “concluded their investigation”.  “The minister welcomes that fact,” his spokeswoman said. “This supports his repeated previous statements that neither he, nor any member of his office, altered the document in question. “Of course minister Taylor will cooperate with the AFP and any enquiries they wish to make, although he fully expects they will conclude that this matter is baseless.” The matter was referred to the federal police on December 20, but has only just been made public. A statement from the NSW police confirmed the matter had been handed over in a two line press release. “Following an investigation by the State Crime Command’s Financial Crimes Squad, the matter has been referred to the Australian Federal Police. “No further comment is available.” Sky News reported the NSW police have handed over the investigation because of jurisdictional issues, arguing any crime, if there was one, occurred in Canberra, not Sydney. Sydney’s lord mayor, Clover Moore, declares a climate emergency, which is endorsed by the council. 29 September: The energy and emissions reduction minister, Angus Taylor, writes to Clover Moore, claiming the council’s annual report “shows your council spent $1.7m on international travel and $14.2m on domestic travel” in 2017-18. The Daily Telegraph publishes a story on page three and online accusing Moore of hypocrisy over the council’s emissions. The story quotes a letter from Taylor which says the council spent more than $15m on domestic and international travel. Moore disputes the Telegraph story and asks the reporter to provide evidence of this claim. The reporter provides a page from the council’s annual report. The page contains two figures purporting to show the council spent “$14.2” in expenses on interstate travel and “$1.7” on overseas visits.  Moore checks the annual report: on page 14 it shows councillors spent $4,206.32 on interstate travel and $1,727.77 on overseas visits. Moore angrily disputes the story with Taylor via Twitter. Moore writes to Taylor asking him to “ correct a stark error in your letter” saying the $15m figure was grossly inaccurate. Moore lodges complaint with the Press Council. The AFP confirmed the investigation had been referred. “The AFP can confirm it received a referral from New South Wales Police on December 20, 2019, in relation to the alleged doctoring of a document,” it said in a statement. “While this matter is being evaluated, it would be inappropriate to comment any further.” The Labor leader, Anthony Albanese, said the opposition would continue to pursue the matter when parliament resumed in February. “One of the reports indicates that the AFP have been investigating for some time that it hasn’t been announced today,” he told the ABC. “We need to know exactly what the circumstances are. And we’ll continue to scrutinise that. And scrutinise as well the level of cooperation that is there with any inquiry to make sure that happens. This is pretty clear. There’s a document, it came from somewhere. The evidence will be there, in terms of computer trails of where it came from.” The NSW police launched an investigation into the origins of an altered document which was used to politically attack the Sydney lord mayor, Clover Moore, after Labor contacted the police to ask if an investigation was warranted. A strike force investigation into the issue was launched, leading Scott Morrison to personally contact NSW police commissioner Mick Fuller, by phone, leading to allegations, which were strongly denied, he had influenced the investigation. At the time, Fuller said he believed the matter would be wrapped up in a week. “To be honest with you, I actually don’t feel as though the allegations themselves are serious, in terms of the things that I would normally stand up and talk about the types of crimes,” he said. “But at the end of the day they’re public figures, and at the end of the day I’m assuming that the public and the media would expect that we take all matters seriously against public figures.” Exclusive: Angus Taylor investigation referred by NSW cops to Australian Federal police with NSW cops arguing that if an offence occurred, it occurred in Canberra, in the AFP's jurisdiction"
"A pine marten has been spotted in England recently, the first in more than 100 years. The reemergence of Britain’s second-rarest mammal, a cat-sized relative of badgers and weasels, is a great story in itself. But it may have another upside, as pine martens could be bad news for one of the UK’s least popular animals: the invasive grey squirrel. Unlike pine martens, grey squirrels are not native to Britain. These North American “aliens” were first introduced in the 1870s and soon made themselves at home. In the UK they are considered an invasive species – their “bark-stripping” harms the growth of new woodlands and has a big economic cost.  Grey squirrels’ success has also been to the detriment of the native red squirrel. Greys do not kill reds directly, but they do spread squirrel pox, a virus that causes distinctive ulcers on the reds’ eyes and nose, leading to death within a week. Grey squirrels themselves are unaffected – they’ve developed immunity.  Things are looking pretty dire for the UK’s red squirrels. Competition, disease and habitat loss mean that, if current grey squirrel control efforts were to stop, red squirrels would become extinct in Britain. I’m interested in how pine martens fit into this struggle. Habitat loss, hunting for fur and predator control by game keepers meant they became practically extinct in England and Wales. However in Scotland and Ireland they are making a comeback – and where they are returning, grey squirrels are disappearing. The impact in Ireland has been particularly notable. A four-year study I published in 2014 found pine marten recovery in the Irish midlands was linked to such a significant decline in grey squirrel numbers that the once beleaguered red squirrel population was able to recolonise its former range, including woodlands which had been dominated by greys for more than 30 years.   The study provided the first evidence for what foresters and gamekeepers had been saying for years – where pine martens had returned to healthy numbers, grey squirrels had all but disappeared. But in areas with few or no pine martens, grey squirrels persisted at “invasive” levels.   Red squirrels on the other hand have coexisted with pine martens throughout much of Europe for tens of thousands of years. The two species evolved together. While pine martens will very occasionally eat red squirrels, they don’t seem to have a negative impact on population numbers. In fact, in the Irish study, the areas that red squirrels had recolonised naturally were exclusively those with healthy pine marten populations. We do know that more pine martens in an area means fewer grey squirrels, but we don’t yet know if this is down to direct predation. It does happen though: the first evidence of a pine marten preying on the American grey squirrel was also recorded in Ireland in 2013, and we are now looking for evidence of this in the Scottish borders too.  Grey squirrels are larger and less agile than red squirrels and typically spend more of their time on the ground, making them an easier prey. However, having a healthy native predator around could also affect grey squirrels in various other ways: they might simply learn to avoid known pine marten areas, or they might spend less time on the ground foraging, leading to reduced fitness. Grey squirrels might even be suffering physiological effects such as stress-induced reproductive problems. Ultimately we need to determine whether the pine marten could act as a natural biological control for the grey squirrel in Britain and Ireland. That’s why I’m now looking at Scotland, where there have been reports of grey squirrel declines after pine marten recolonisation since the early 2000s. I want to know if the two processes are linked.  There are several subtle but potentially important differences between pine marten populations in the two countries that I’ll need to take into account – Scottish pine martens can feast on field voles, for instance, a rich food source that isn’t found in Ireland. Mass reintroduction of pine martens may be implausible but the creatures are moving south through Scotland and are literally just a few miles from the English border, so the process of natural recolonisation in England is almost underway. The recent sightings in Shropshire may even mean the remnant Welsh population is spreading into England too. It is important to remember pine martens are very slow breeders however, and it will take the recovering population quite some time to reach levels healthy enough to potentially impact on grey squirrel populations.  Predators are a vital part of a healthy ecosystem and predator prey interactions have an important function. What’s happening in Ireland and potentially Britain with squirrels and pine martens is a great example of how restoring natural predators can reduce the damage caused by invasive species. We are currently living in an unnaturally predator-poor environment, and it’s possible this has allowed some introduced species to reach “invasive” levels, which has ultimately wreaked havoc on our ecosystem.  Interaction between pine martens and squirrels is fascinating from a scientific point of view and we still have lots to learn. But you don’t have to be an ecologist to appreciate the value of promoting one of Britain’s most beautiful native species in order to preserve another."
"
Share this...FacebookTwitterKenneth set to show in just minutes what a sham all the sea level rise alarmism really is. (Now busy setting the troll filter on high!)

NASA photo – public domain
Stay tuned! 🙂
Share this...FacebookTwitter "
"The world’s leading scientists, politicians and even the pope are agreed that the world is warming thanks to human activities. Yet despite this, extreme cold weather still happens. The unusually chilly UK winter of 2009/10, for instance, led to understandable scepticism from some commentators who argued that this “global cooling” conflicted with predictions of warmer winters. There’s no contradiction – the world simply doesn’t work like that. In fact, even after another 85 years of global warming there will still be a small chance of a 2009/10-style winter – a 0.6% chance, to be precise. That’s according to new research by the UK’s Met Office which assesses how likely these sorts of unusual seasons will be in future. The study, published in the journal Nature Climate Change, also says the chances of the sort of historically hot summer that we might currently see only once every two decades will rise to 90% by 2100. What we would today consider a wetter-than-average summer will, by 2100, be relatively rare – just a 20% chance. Importantly, these projections are expressed as probabilities. Climate is most commonly defined by the long-term average of the weather, typically temperature and rain or snowfall, taken over a 30-year time period. This way long-term trends, such as anthropogenic climate change, don’t get lost in the “noise” over much shorter periods. But it’s equally important to assess how variable the weather is. When predicting the long-term effect of climate change it’s necessary to figure out not just what the “background” climate will look like, but also how common freak events, seasons or even years might be. An “average temperature rise of 2℃” isn’t much use to someone working in disaster management, for instance – they need to know whether to prepare for frequent heatwaves, floods or snow. The Met Office uses a climate model known as HadCM3 to make long-term predictions. In this study, several hundred slightly different versions of the model were run to help represent uncertainty in the atmosphere and ocean climate system, for instance in winds and temperatures. The models were also run backwards in time and were assigned degrees of reliability according to their ability to simulate recent climate changes.  The results give a sense of short-term unpredictability of the weather in any given year, rather than merely highlighting what a “typical” year might look like – something not helped by the usual portrayal of projected temperature trends as a smooth line. The cold 2009/10 winter is a good example of year-to-year variability that bucks the trend. The same confusion can happen in shorter timeframes – in the US, Senator Jim Inhofe brought a snowball into congress after it snowed last February to “prove” the world wasn’t getting warmer.  Freak events or even entire seasons don’t invalidate the kind of thorough projections described above. Climate scientists define the norm as within the 2.5-97.5 percentile range, yet these are just probabilities – and 5% of the time anomalies outside that range are expected to occur. The record-breaking winter of 1962/63, the UK’s coldest of the 20th century, or the wet winter of 2013/14 that caused major flooding, simply represent seasons that will only crop up a tiny fraction of the time.  Even a series of seemingly unusual events doesn’t necessarily mean the “background” has changed. England and Wales had a series of wet summers from 2007 through to 2012, which were part of a rising summer rainfall trend after several years of relative “drought” conditions in the 1990s –- a rise that actually goes against long-term predictions of drier summers.  But on the chart below we can see these wet summers are still within the upper shaded grey region (the 2.5-97.5 percentiles) of the probabilistic projections and we can therefore attribute these summers to short-term variability, or noise, in the climate system.  Climate models don’t always capture the amount of year-to-year variability we actually observe in reality, so sometimes they need rescaling. And this variability itself may change over longer timescales, for example the North Atlantic Oscillation (a seesaw of atmospheric surface pressure across the North Atlantic) has become much more variable in winter over the past 50-100 years.  Models are also rather poor at capturing fluctuations in the all-important jet stream which is crucial for simulating precipitation and temperature changes over the British Isles.  Nevertheless, despite some remaining provisos, climate models are an indispensable tool for making predictions, and this new Hadley Centre study is a thorough new benchmark for all those concerned with the impact of climate change in Britain."
"The world is about to experience one of the biggest housing booms in history over the coming decades. In the UK, the housing crisis is a recurring news story, with a lack of affordable housing and problems with the quality of the housing stock. However, these problems pale in comparison with the scale of the housing crisis facing much of the world. The UN estimates the world’s population will grow by an additional two billion people by 2050, with most growth from developing countries in Africa and Asia. Hundreds of millions of additional homes will be needed in these regions over the coming decades. At the same time, another problem looms: our need to reduce our global greenhouse emissions by 60% in order to keep average global warming below 2°C. Read more: The world needs to build more than two billion new homes over the next 80 years  Much of modern housing is built with two materials: fired bricks and concrete. These have served well in terms of their strength, durability and ease of use. Their major drawback lies in their environmental impact – in particular, the carbon emissions associated with their production. Both materials need high temperatures in order to bring about the chemical changes that enable them to gain strength. This is typically over 1,000°C for firing bricks, and 1,450°C for the production of cement. Cement production alone accounts for between 5-10% of global greenhouse gas emissions. Given the scale of reductions in greenhouse gas emissions required, and the sheer number of homes that will need to be built, it is clear that we need new construction materials. These construction materials must provide comparable benefits to concrete and brick – but with an acceptably low impact on the environment, and they will need to be affordable and socially desirable in the developing countries where most house-building will occur. There won’t be one single material that replaces fired brick and concrete. Instead, there’s likely to be a range of new construction materials and building technologies, each suited for different regions to reflect their local resources, climate and culture. One promising candidate material we have been investigating in our ongoing research into stronger and more sustainable bricks for the future, are geopolymer-stabilised soil materials (GSSM). A geopolymer is a hard and durable substance similar to cement, made out of chains of aluminium, silicon, alkali metal and oxygen atoms. Geopolymer-based materials are a young family of human-made materials, first demonstrated in 1940. They can be made in many ways using different starting ingredients, but so far have mostly used industrial waste, such as ash from coal power stations. Some geopolymer materials have already been used in construction since the 1970s, but usually in small volumes for specialist applications. There is untapped potential in using soil as the starting ingredient, which is found in large quantities over much of the planet. Geopolymer-stabilised soil materials are made in a simple process. Soil (taken from beneath the valuable surface layer) is mixed with an alkaline activating solution, which contains chemicals similar to those found in household cleaning products. This activating solution dissolves the clay minerals in soil into their constituent atoms. The resulting mix has a play-dough like consistency, and can be shaped in moulds. The moulded bricks are heated at 80-100°C. During firing, the dissolved atoms rearrange to form a geopolymer. Once fired, the strong geopolymer that was formed in the process acts to stabilise the remaining soil, forming the final brick. The potential benefits of GSSM are the low temperatures required to heat the brick, and the abundance of the primary starting ingredient – soil. Depending on the exact soil, chemical recipe and heating process used, these bricks could have half the carbon emissions of concrete, and a quarter of the amount produced by conventional fired bricks. GSSM would not be used in high-strength applications like high-rise buildings, but it has the potential to be a very good replacement for concrete in low and mid-rise housing, which is how much of the new housing in developing countries is being built. These materials are still yet to be ready commercially. Although bricks have been made this way using individual soils, there is still work to be done around designing a “chemical recipe book” for different types of soils, as its composition varies between regions, and can even vary within one field. Although soil itself is abundant, the alkaline-activating solutions currently used require industrial chemicals. While these are available in bulk quantities and are relatively inexpensive, more research is required into identifying more abundant locally available sources for the activating solution, such as plant ashes from agricultural waste. Aside from technical challenges, there will also be a battle of persuasion. Given that much of the world has moved away from traditional earth-based housing in recent years, there needs to be a culture of confidence – rather than conservatism – about using innovative materials for homes. The stakes are high – housing supply and climate change are two of the biggest issues of our time. These more sustainable bricks may well be a piece of the puzzle."
"The prime minister, Scott Morrison, has promised an aid package for areas ravaged by Australia’s continuing bushfire crisis and says he will consider a royal commission into the deadly blazes, which have burned vast areas on the east coast. Speaking during a press conference the prime minister also said there had been a breakdown in communications with New South Wales authorities that left the Rural Fire Service and their Australian Defence Force liaison unaware army reservists had been called up to help with the fire effort.  Morrison, who has been heavily criticised for taking a holiday in Hawaii during the crisis and was heckled by locals in Cobargo after forcing a handshake on an unwilling woman, also defended his leadership during the emergency. He said he and the treasurer, Josh Frydenberg, would announce a financial aid package for farmers, small businesses and “others who engage in the rebuilding effort” on Monday, after it was approved by cabinet, and ruled out a bushfire levy to fund it. “We will be committing everything that is needed and more as it is required,” he said. He said he would consider a royal commission into the fires “in concert with states and territories”. “There are matters referred to about planning and building regulations and where people are allowed to build residences and in what circumstances and the land clearing arrangements, of course hazard reduction has been a constant refrain as I have been on the ground,” he said. “I also acknowledge the drought conditions can make that very difficult on occasion but we also know there have been many occasions where the hazard reduction has been actively resisted and that is something that we will have to learn from as well.” This was an apparent reference to the idea, which has been described as a conspiracy theory, that green activism has stopped the practice of burning off during the winter. Morrison said he would not be “distracted” by criticism of his leadership during the crisis and the community expected him to concentrate on the needs of communities hurt by the blazes. “That is very much my focus is and that’s where it will continue to be, working closely with the states and territories, working closely with my ministers and the agencies, the defence forces, the recovery agency, to be led by Mr Colvin and ensuring they have the support they need,” he said. However, he said there had been “a breakdown in communications” that meant the NSW Rural Fire Service commissioner, Shane Fitzsimmons, wasn’t told about the reserves being called out. “And so there has been a subsequent conversation between myself and the premier and the minister of NSW and we have addressed any of those issues that arose from that,” he said. Morrison has previously portrayed the response to the bushfires as primarily a state responsibility, saying as recently as Thursday that he did not want state and federal governments “to be tripping over each other in order to somehow outbid each other in the response”. The defence minister, Linda Reynolds, also admitted to traveling overseas during the crisis, initially telling reporters she had been on holidays over Christmas, without saying where she had been. She said that she “spent time with my family over Christmas but throughout that time I can assure you that I have been regularly on the phone with the prime minister, with [emergency management] minister Littleproud constantly”. Asked directly if she was in Bali she said: “Yes I was.” Defence force personnel have played a key supporting role in the firefighting effort, providing transport, logistics and other help to emergency services since early September. Over the weekend, navy ships evacuated more than 1,000 people from Mallacoota, in East Gippsland, and yesterday the federal government called up 3,000 army reservists to bolster the effort. Does climate change cause bushfires? The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  What is the evidence on rising temperatures?  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. What other effects do carbon emissions have? Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  So is climate change making everything dryer?  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. What do recent weather patterns show? The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Is arson a factor in this year's extreme bushfires? Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. Morrison claimed there was “no dispute in this country about the issue of climate change, globally, and its effect on global weather patterns, and that includes how that impacts in Australia”. “The government has always made this connection and that has never been in dispute,” he said. However, overnight the government backbencher Craig Kelly told the BBC there was “no link” between climate change and the fires. In November, the deputy prime minister, Michael McCormack, described people who drew a link between the fires and global heating as “inner-city raving lunatics”."
"In early December, delegates representing Young Liberal branches across the state voted overwhelmingly in approval of a motion recognising the reality of climate change and the need for action. The NSW Liberal party’s youth wing recognises this a particularly important issue facing our generation, as our generation will have to face the risks brought about by climate change.  It is the duty of government to be awake to the challenges of the future. As the great conservative thinker Edmund Burke recognised, current generations hold the present in trust for the future. And climate change is a significant risk that will affect the future of my generation. The climate change debate in Australia has unfortunately become a bit of a poisoned well. While Boris Johnson’s Conservatives went to the UK election with a commitment to achieve net zero emissions by 2050, any talk of climate change can spell the death of a Liberal leader in Australia. It has become divisive and transformed into a tribal marker between those who are “woke” and those who are “anti-PC”. It shouldn’t be this way. It should not be a matter of conservative versus progressive. It should be a matter of science and economics. Liberals should not surrender the debate to those on the left who would use this challenge to push an anti-market, big-government agenda that would do great damage to our economy and hurt those who can least afford it. As the prime minister has rightly said, we do not need to choose between a strong economy and action on climate change. We don’t need to choose between low electricity prices, a decent standard of living for all and ensuring a liveable planet in the future. The science and the economics tells us we can mitigate the effects of climate change through more efficient energy use and allowing greater investment in more efficient renewable energy generation. It is already the case that renewables are on track to become cheaper and more efficient than traditional fossil fuel sources. This is why calls for the government to directly own and operate coal-fired power stations is misguided. Not only is it illiberal – calling for the socialisation of power – but it makes zero economic sense for the government to be using taxpayer money to make a long-term investment in current coal technology that will be less efficient and more expensive than other alternatives in the future. The government shouldn’t pick winners – whether solar, wind, hydro, nuclear or coal. It should create the framework within which there is investment certainty for the private sector to develop and invest in the most efficient technologies. In short, we need policy settings that will provide the investment certainty we’ve been lacking in order to allow the market to work. Government should ensure that Australia is well placed to take advantages of innovation in the clean energy space. Good policy will ensure we keep energy costs low, using the most efficient technology available. Government should incentivise the energy sector and private sector generally to find innovative ways to reduce their emissions without heavy-handed taxes, extravagant subsidies and heavy-handed bans on mining that will have disastrous economic consequences and hurt low and middle-income Australians. Government can do this through tax break incentives and asset write offs, as well as smart public investment in research and development. Let coal, nuclear, wind, solar, gas, pumped hydro, hydrogen and other technologies compete to be the cheapest and most efficient. If we have the right policy settings that encourage innovation and provide certainty to the market, we will see the market picking cheaper, lower emissions technology. It’s time to stop turning climate change into a climate culture war and time to start pursuing good policy which will ensure we have low energy costs and use the most efficient, low-emissions technology available. Our future depends on it. Chaneg Torres is the president of the NSW Young Liberals"
"Trend-spotters may have declared the car is dead for 20-somethings in central London or Paris but among the rest of humanity sales of the ubiquitous gas-guzzler continue to climb. It seems however environmental we may wish to be, owning a set of wheels is just too convenient to give up. But maybe not for long. A more radical solution to tackling climate change is proposed in a paper in the journal Nature Climate Change. As the industry inches towards self-driving cars isn’t it time to consider trading in the family wagon and simply hail a lift? The argument is seductively simple. We can’t do without cars. No transport alternative offers the same flexibility, personalised comfort and sense of control. But automobiles are spectacularly inefficient in terms of environmental and personal cost. Cars depreciate rapidly, are left idle most of their life-time, eat through fuel while seats are often left empty and they demand no end of real-estate to be parked on. If any field is overdue a wave of disruption it’s personal car ownership. Car-sharing schemes such as BMW’s DriveNow and on-demand services such as Uber give us a glimpse of the benefits cloud computing can bring to fleet management. Autonomous vehicles (AVs) however, could transform this field. Autonomous taxis would be more expensive than a normal car, but with no driver the running costs would be substantially lower. Operators would therefore be likely to run large fleets and would keep their cars on the road for as long as possible – meaning that in areas of high population a vehicle would never be too far away. These vehicles would be different. Just as electric cars have increased interior space by doing away with the engine, dispensing with the steering wheel, gear stick and other controls would free up room for extra people and their belongings. This also links to the potential for what the paper’s authors, two Berkeley scientists, call “right-sizing”. If a wide range of vehicles are in constant circulation, people will be able to order whatever suits them: a people carrier to take the kids to school, then a sporty coupé for the commute to work. This would mean more choice for the individual, but with a knock-on benefit for the environment through reducing the number of large fuel-hungry vehicles with only one occupant. It points towards another expected benefit of autonomous taxis. If fleet operators want cheap and easy re-fuelling (to lower those overheads) then fully-electric vehicles make sense. Whereas single car-owning individuals are likely to be concerned about vehicle range, taxi firms smooth out journey and fuelling requirements over an entire fleet, enabling them always to take advantage of the cheapest electricity. Autonomous vehicles could revolutionise driving. Sensors linked to sophisticated electronic control units would enable cars to anticipate and to respond to risk faster than a human driver. Breaking distances could be reduced, enabling convoys of cars to reduce aerodynamic drag and use their power more efficiently. A great vision for the future, one might observe, but isn’t this all a bit sci-fi? Well at this stage, the answer is probably “yes”. Vehicle automation has of course been with us for years and is actively used in high-risk fields such as bomb disposal, mining or submersibles, not to mention the release into the world of robot vacuum cleaners. But these are relatively controlled environments to operate within, and a world away from the complexity of fast-paced, complex city streets. As it stands there are substantial limitations on autonomous vehicles. Cars are reliant on processing, in advance, extensively mapped environments – making any spontaneous deviations from a route difficult. Snow, heavy rain or bright light can cause havoc with sensors, and “reading” visual symbols such as a police officer signalling would be beyond most current processors. Add to this, the cyber-security risks inherent in all connected vehicles and it becomes plain just how much work developers have to do. Even if these challenges can be overcome in any realistic time-frame the usual drawback of car-sharing services would still apply to autonomous vehicles. Being able to hail a taxi when you want it, never mind in the form one desires it, requires a huge fleet to be operating in densely populated area. We’d also need advanced GPS mapping and tracking systems and large out-of-town charging bays – because although more autonomous taxis would mean less parking needed overall, the fleet as a whole would likely seek cheaper overnight charging at a similar time. Yet what was once science fiction has a tendency to move into the mainstream. Carmakers such as BMW and Tesla have been acquiring driverless technology, as have tech giants Google and Apple – not to mention the car-and-driver-for-hire firm Uber. Indeed one doesn’t need to be an early adopter to see driver assistance and connected technologies becoming commonplace. Crossing the hurdle to fully autonomous vehicles will be a major challenge, but once it has been credibly achieved (and credibility is everything), a whole new market may be awaiting the pioneers. Ultimately this is the point. From San Francisco to Moscow, from Seoul to mighty Coventry (where the UK’s largest driverless car test is taking place), there is a digital infrastructure being laid down and growing expectations among the younger generation of ever more connectivity. Autonomous taxis may not be appropriate everywhere or for everyone, but they do have the potential to capture a large slice of interest in urban centres and, because the capital costs are borne by the transport operators not individuals, there is lots of potential for the business models to spread quickly.  By 2030 our cities, not to mention our cars, may start looking very different. It might be worth thinking about grassing over your driveway."
"Extreme storms and rising sea levels will threaten the existence of coastal cities worldwide, unless preventative action is undertaken. With population growth and sea-level rise set to continue, research has estimated that by 2050, we can expect more than US$1 trillion worth of damages per year to be incurred by 136 of the world’s largest cities, if there is no attempt to adapt. The game changer came in 2005, when we saw one of the most active hurricane seasons in US history. Hurricane Katrina, the fifth hurricane of that season, resulted in nearly 1,600 deaths. Almost half of these fatalities occurred in New Orleans: 80% of the city was flooded, at a cost of US$40 billion. When the water subsided, so did the population: ten years on, the city that used to house 500,000 is now home to only 300,000 people.  There are a number of ways to go about changing cities to account for rising sea levels: we can raise coastal defences, build houses on stilts, or simply move cities and their populations away from the coast. Which of these strategies works best was one of many questions set out in Climate Change: A Risk Assessment – a new report led by Sir David King and the Foreign and Commonwealth Office. Globally, sea levels have been remarkably stable since civilisation started to develop several thousand years ago. During the 20th century, sea levels rose about 17cm, at an average rate of 1.8mm per year. Over the past few decades, that rate has doubled to more than 3mm per year. This trend is expected to continue and accelerate. According to the latest Intergovernmental Panel on Climate Change report, the sea level is projected to rise up to 1m by 2100. If the large ice sheets of Greenland and Antarctica melted, even higher rises are considered possible, albeit highly uncertain. Importantly, if carbon emissions are stabilised, or even decrease, the sea level will continue to rise for many centuries, as the deep ocean slowly warms and the large ice sheets reach a new equilibrium. Simply put, sea-level rise is here to stay. It is likely to lead to greater flooding, salinisation (the build up of salt in surface and groundwater) and erosion in coastal areas, affecting millions of people worldwide and costing billions of dollars of damage. The high costs of economic damage and loss of life are becoming less acceptable in a world where extreme weather events can be accurately forecast and coastal protection is possible. In many parts of the world, damages and loss of life remain high, as seen during Typhoon Haiyan, which hit the Philippines in 2013. Preparing coastal cities for extreme events and adapting them to cope with sea-level rise remains challenging: King’s report highlights the engineering, financial and socio-political limits of the adaptation challenge.  But cities are starting to embrace these challenges. For example, last year, Boston put forward the bold, novel idea of becoming an American Venice – a city full of canals to hold water as sea levels rise. New York has considered building a barrier to keep water out, in light of the fact that, with a 1m rise in the sea level, a 1-in-100 year event (that is, a severe storm one would expect to occur once every 100 years) could become 200 times more likely to occur.  London has also developed a range of flexible options that would protect the Thames Estuary against up to 5m of sea-level rise. These include raising defences, implementing flood storage and constructing a new and bigger Thames Barrier further downstream. In developing countries, few cities are preparing for sea-level rise, despite the awareness that this is a long-term hazard. Developing cities also frequently have rapid population growth. In Shanghai and Kolkata more than 400,000 people live less than 2m above the present-day sea level. A rise of 1m will increase the frequency of a current 1-in-100 year event by 40 times in Shanghai, and about 1,000 times in Kolkata. Local ground subsidence is another factor to worry about. This involves the sinking of the land relative to the sea due to natural and sometimes human processes (such as groundwater withdrawal). Local ground subsidence will worsen conditions in about a quarter of coastal cities – namely, those built on susceptible deltaic soils (those at the mouth of a river). Small islands and their cities are also under serious threat from sea-level rise as they are low-lying, remote and dispersed in their territories, and often have limited financial resources. Far from being a green, spacious island, Malé – the capital of the Maldives – is one of the world’s most densely populated cities. Building protective structures is one way of reducing the impacts of extreme events: Malé is surrounded by a sea wall and giant tetrapods (a four-pronged concentrate structure about 2m high). But a lack of space limits future coastal protection.  To overcome this, a new island has been constructed, Hulhumalé, with sea-level rise also in mind. The solution to sea-level rise is simply to build upwards: The island was raised to 2m above present day sea level to protect against storms. This buys time, but moving into the late 21st or early 22nd century this may not be enough. Other Maldivian islands are following suit, with the Safer Islands programme selectively raising parts of islands. This may help the parts of the country, but clearly much more work is required to ensure the long-term prospects of this fragile island nation. Ultimately, these case studies show us that there’s no one-size-fits-all approach to adapting cities to rising sea levels. Rather, the best bet for cities to adapt against rising sea levels is to dare to be different. Both engineering design, government authorities and social attitudes must acknowledge that change needs to occur, if we’re to avoid disaster."
"A hot summer brings out the sunglasses, ice cream and bare feet. Unfortunately it also brings out the flying, biting pests. The UK has 7,000 species of flies, including midges, horse flies and the ones with arguably the worst reputation, mosquitoes. The most common mosquito species in the UK is Culex pipiens. Few people are actually bitten by it, since it mainly feeds on birds, but this year there are likely to be more C. pipiens and other mosquito species than usual. This is down to seasonal conditions. Pregnant female mosquitoes hibernate and the mild winter will have resulted in greater survival – and more eggs. A wet May was ideal for the aquatic larvae. A hot summer means more fly activity and more people outside to be bitten. So the cycle continues.  Mosquitoes have needle-like mouth parts that pierce flesh so they can suck blood. They also secrete anticoagulants that prevent clotted blood blocking their mouth parts, and a local anaesthetic so you can’t feel the bite. But these bites aren’t just annoying – they’re potentially deadly. Mosquito saliva can be a vehicle for transmission of diseases such as malaria, caused by a tiny protozoan organism called Plasmodium. In 2013, between 124m and 283m people contracted the disease and an estimated 584,000 people died from it. As well as malaria, mosquitoes can transmit viruses including dengue fever, yellow fever, West Nile virus and chikungunya. Luckily for the UK, the species that carries most of these diseases, Aedes egypti, doesn’t live here, but it is increasing its range. It recolonised Madeira in 2004-2005 and there are concerns that it could be transported to western European countries. There are very few recent cases of malaria transmission in the UK, although there is evidence for the disease’s presence from the 14th to the 17th centuries. Shakespeare even mentioned it in The Tempest. Malaria in the UK virtually died out by the end of the 19th century due to a combination of marsh drainage, use of quinine and better sanitation.  Aside from this, the absence of malaria in Western Europe is most likely due to its climate. Plasmodium needs a sustained high temperature to complete its reproduction in the mosquito. The lack of the species mainly involved in transmission of the disease is also crucial.    However, the UK’s freedom from dangerous mosquitoes could be set to change. Firstly, with increased globalisation and faster transport, non-native species could be introduced in sufficient numbers to establish a breeding population. Most insects are strongly “r-selected”, meaning they have evolved to produce large numbers of offspring but live relatively short lives. That means once a species has established itself in a location it can increase very rapidly. For example, Harlequin ladybirds spread over most of southern Britain in just 10 years, with a much slower breeding rate than mosquitoes. Secondly, temperatures are predicted to increase due to climate change. A review published in The Lancet Infectious Diseases concludes that warmer conditions and more rainfall could provide the right conditions for disease carrying mosquitoes to arrive in the UK.  Existing species such as the common Culex pipiens, could spread West Nile Virus here. Another virus-carrying species, Culex modestus has already established colonies in the Thames estuary. A further problem is the Asian tiger mosquito (Aedes albopictus), which spreads dengue fever and chikungunya. Both can be serious illnesses and have no effective treatment. This mosquito’s spread, especially in the United States, has been exacerbated by the international trade in used tyres, whose colour and structure provide ideal incubation pools for the species’ aquatic larvae. Predictions based on a 2oC temperature rise – the commonly agreed limit before which “dangerous” climate change will kick in – could extend Tiger mosquitoes’ activity season by a month and its range by up to 30% by 2030. In the past 10 years insect-borne diseases have spread within Europe, including Greece (malaria) West Nile virus (Eastern Europe), Italy and France (chikungunya). This temperature rise could lead to outbreaks of chikungunya in south-eastern England by the second half of the century. So as the climate warms, mosquitoes in the UK may no longer be just a pest that gets worse during the occasional heatwave. They may become a widespread, constant and dangerous health threat."
"A magnitude 5.5 earthquake shook the industrial city of Pohang in South Korea on 15 November 2017, injuring almost 100 people and damaging thousands of buildings at a cost of millions of US dollars. Six months on, two academic papers have suggested that fracking was probably the cause of this earthquake. A local geothermal energy project had been injecting highly pressurised water into two, four kilometre-deep boreholes for almost two years. This process, known as hydraulic fracturing or fracking, creates or enhances fractures in rock to harness the heat stored there. Once the network of fractures connected the two boreholes, the plan was to pump water into one, circulate it through the granite rock, absorbing heat, then extract it from the other borehole and use the heat to generate electricity. Afterwards, the cooled water would be reinjected to begin the process over again. The proposed cause-and-effect connections now identified make the Pohang earthquake by far the largest recorded for which fracking is the likely cause. The previous record holder linked to geothermal development – of magnitude 3.4 – occurred a decade ago in the Swiss city of Basel. Ten days after the Pohang earthquake, the South Korean government suspended the geothermal plant’s operations and ordered an investigation into a possible link, which is still ongoing. The first of the papers reports a collaboration between researchers from Zurich, Switzerland, Potsdam, Germany, and myself. We used public domain data from seismographs (instruments for recording ground motion caused by earthquakes) and remote-sensing satellites to determine the location and position of the geological fault that slipped in the earthquake. Both types of data indicate the rupture of a fault running southwest to northeast and dipping steeply to the northwest. Recorded by satellites, the rock above this fault moved upward, lifting the Earth’s surface by four centimetres. This analysis indicates that the fault slipped over a depth range of three to six kilometres, encompassing the depth of the injection and passing within hundreds of metres of the boreholes. This points strongly to a connection between the high-pressure fluid injection and the earthquake.  The second paper, by Korean colleagues, reports the locations of the many aftershocks of the Pohang earthquake, more accurately defining its fault plane. As shown in the cross-section below, their study indicates the fault passing between the bases of the two boreholes where water was injected, cutting across one borehole at a depth of around three to four kilometres. The results from the two papers are consistent with each other, despite the different types of data used, providing strong confirmation. Preliminary surveys for the geothermal project over a decade ago identified a fault (its position at a depth of four kilometres is shown in orange on the image below) with essentially the same position and alignment as that which slipped in the November 2017 earthquake (its position, also at a depth of four kilometres, is shown in red for comparison). Taking into account the uncertainties in each of the analyses, this comparison indicates that these surveys revealed the fault that is now known to have slipped in the November 2017 earthquake.  Further surveys taken before drilling started led the developers to revise their plans to focus on a WNW-ESE fracture in the granite beneath the site. The boreholes, shown in the cross-section above, were designed for flow in this direction. During drilling, fluid (or drilling “mud”) leaked from one borehole at roughly the depth where the fault cuts across the well bore, with fault gouge – crushed rock debris produced by the movement of the rocks on either side of a fault – indicating a fault there before any injection had taken place. More than 20 years ago, a set of criteria was devised for assessing whether earthquakes are caused by high pressure fluid injection. The Pohang earthquake meets most of these criteria. Notably, it occurred within hundreds of metres of the injection, and at the same depth. Also, during much of the injection, the pressure was high enough for standard calculations to predict the slipping of the fault, if water at this pressure reached this fault. However, the two-month interval between the end of injection in September 2017 and the earthquake in November provides a potential argument against any cause-and-effect connection. A possible explanation for this delay is that once water entered the fault it began to dissolve the granite, gradually weakening the fault so it eventually failed and slipped. It seems entirely plausible at this stage to believe that the earthquake was caused by the injection, and to examine the implications. One reason this seismicity is significant is the disproportion between the size of the main shock and the volume of water injected. A theory has been developed for determining the “worst case scenario” earthquake feasible for a given volume of fluid injection. The overall volume injected at Pohang was roughly 12,000 cubic metres, whereas this theory requires around 1,000 times more volume to cause an earthquake as large as magnitude 5.5. This suggests that the theory needs improving, possibly to incorporate the injection pressure as well as the injected volume. In the meantime, designs for future geothermal fracking projects might require independent expert assessment, as is already the case for projects involving fracking for shale gas in the UK. At Pohang, this could have highlighted the potential implications of a fault cutting across the site, leading to recommendations such as limiting the injection pressure, which could have lessened the force of the earthquake. Elsewhere in the world, successful deep geothermal projects have been designed to incorporate circulation of water along faults, requiring high pressure injection to create or enhance fractures. Pohang illustrates the need for accurate information on the geometry of faults on which project designs can be based.  Still, the disproportion between the small volume of water injected at Pohang and the size of the November 2017 earthquake may give geothermal fracking developers worldwide pause for thought. It may well be the game changer for their industry."
"Muslims have a religious duty to take action against climate change, according to a declaration released by a major group of Islamic scholars, faith leaders and politicians from 20 countries. The Islamic Declaration on Global Climate Change, launched in Istanbul, is aimed at the world’s 1.6 billion Muslims and suggests mosques and Islamic schools should immediately take action.  In using religious authority to call for stronger climate change policies at the UN summit in Paris this December, the Islamic declaration follows a similar intervention by the Pope earlier in the year.  There is a solid religious case for this declaration. Muslims around the world take the Qur’an and the prophetic tradition (sunna) as the main two authoritative sources of the Islamic legal system (Sharia). You won’t find any direct references to carbon budgets or biodiversity in the sacred scriptures of course – the global environmental crisis is far too recent. However there is an environmental framework inherently embedded within the traditional principles of Islam, and it is possible to extend these principles to consider contemporary changes. Traditionally there are five major obligations for all Muslims: proclamation in the oneness of Allah, prayer, fasting, pilgrimage and alms-giving (charity towards the poor). Each can help the environment. The concept of oneness may be extended to the unity of creation – the idea that there is one planet for all humanity to share. Thus Islam teaches an inter-connectedness between the environment and human beings.  Prayer is about seeking guidance from Allah. Similarly, the environment has a purpose and forms another kind of revelation, which may be seen as a source of guidance for humanity. Fasting is performed for Allah but recently Muslim faith-activists have fasted for the planet. For example representatives from Wisdom in Nature, an ecological activist group in Britain, would fast so they could contemplate the human impact on the environment.  Also, during pilgrimage Muslims must be considerate to animals and vegetation in designated areas. Finally, the process of alms-giving indicates Muslims are thoughtful and effectively share resources. This means there is already an Islamic ethic for sustainability, particularly equity within and between generations. Climate change affects us all, and by taking action, mosques can make themselves vital and accessible parts of civil society. In Western states, action will present an opportunity to build bridges between Muslims and non-Muslims, emphasising the importance of mosques in the public sphere.  Of course, such action depends on the people responsible for running each mosque.  Generally each follows a particular way of Islamic thinking and the leadership may be reluctant to take on climate change, particularly when other issues, such as the conflicts in Syria and Palestine/Israel dominate current affairs.   Even so, mosques will need input from environmental NGOs to improve their understanding of the Islamic perspective on climate change. And, as climate change tends to be of interest to younger people, mosques will need to keep young people on board, many of whom now feel alienated from the Islamic establishment. It will be vital to encourage climate change action through the various Islamic schools across the world. These children could have a big impact, as it’s a very new field – academics and Muslim scholars are only just catching up with the ways Islam can be applied to today’s climate problems. Whether young Muslims choose to join campaigns, become scientists or simply decide to lead more sustainable lifestyles, they will help develop the idea of Islamic environmentalism and what it could be.  However, there are great global disparities with regard to the importance of Islamic schooling. In Indonesia, for example, it is particularly significant and climate education would make a huge difference. On the other hand, the operation of Islamic schools is more difficult in the West, where Muslim children tend to assimilate into mainstream state-owned schools.  There is certainly an environmental ethic in the Islamic faith, but those behind the declaration need to consider the challenges facing Muslims, mosques and Islamic schools – it’s easy enough to have sustainable principles, but putting them into practice is much harder."
"It may not seem like one of life’s great mysteries, but a quick internet search reveals that people from across the world – London to Hong Kong, Cape Town to Buenos Aires – are asking this same question: for all the pigeons out there in our cities, where are all the dead ones? Alas they’re not pondering the presence of pigeon heaven, but rather, where are all the bodies?  Pigeons are as ubiquitous in the world’s cities as bad traffic, buskers, and late-night takeaways. London alone is estimated to contain more than a million pigeons, inhabiting the many parks and gardens that crisscross its 1,000 square miles. Given these vast numbers – and the fact that an urban pigeon seldom lives for more than three or four years – it’s a wonder why they are not strewn across city streets. There are several possible reasons for this. First, pigeons are just one part of a wide array of creatures to have adopted our cities as their home. Foxes, rats, gulls, crows and ravens all do a wonderful job of cleaning up any carrion they come across, including deceased pigeons. These species perform inestimable services to the urban ecosystem, reducing human exposure to rotting matter and helping cut the transmission of infectious diseases. Alongside these native janitors, domestic cats are equally happy to take care of a dead or injured pigeon. It is estimated that there are half a million cats living in London alone  –  roughly two pigeons per cat – and if you’re “lucky” they might bring one home as a present. Whether a resident moggy or some other carnivore, this network of surreptitious street cleaners will usually whisk away any pigeon corpses long before they’re seen by human eyes. Most pigeons, however, don’t simply drop dead on the ground. To understand where pigeons themselves are likely to go when feeling vulnerable or unwell, we need to delve into their origins. The pigeons we see in cities are domestic pigeons who have undergone some serious “rewilding”. They were originally bred as homing pigeons, trained birds who relayed important messages over large distances long before telephones. These pigeons even won prestigious medals in both world wars. Going back further, the original homing pigeons were bred centuries ago from wild rock doves, a species which inhabits sea cliffs and coastal caves. Cities, with their high-rise buildings and elevated ledges, provide ideal nest sites for feral pigeons, and create an environment reminiscent of their ancestral homes. This background means that, when sick or injured, pigeons instinctively retreat to dark, remote places – ventilation systems, attics, building ledges – hoping to remain out of reach and unnoticed by predators. The predators don’t see them, but neither do we: often when pigeons expire, they are in hiding. But what actually causes a pigeon to die? As they get older, pigeons become more susceptible to disease, and often become slower to react to oncoming predators. It is well-established that when a predator attacks a flock of birds, slower individuals can become isolated from the group, making them easy prey. Dying of old age is not a luxury afforded to most pigeons: as soon as they shows signs of slowness or sickness, many are snapped up by peregrine falcons, sparrowhawks, or other predators. One slightly macabre alternative that occurs in big cities, involves the netting that often hangs around buildings. Birds can easily fly into it and become entangled: not just old or sick pigeons, but any bird unfortunate enough not to notice it. Netting is usually high above the ground, so after some fruitless struggling dead pigeons usually hang there, away from the scavengers below.  Whether snatched midair by birds of prey, entangled by man made obstacles or alone in a remote corner of a skyscraper’s roof garden, there are many ways that pigeons pass on from this world. But they all take place within an internal urban ecosystem, that, for the most part, is hidden from our sight."
"If we have put too much CO2 into the air, wouldn’t it make sense to find ways to remove it again? Well, yes: it would. But sadly it isn’t likely to be easy or cheap and, according to new research, it isn’t an adequate “solution” to the problems of climate change.  The possible “carbon removal” techniques are very diverse. They include growing trees on land or algae in the sea and capturing and burying some of the carbon they have taken from the atmosphere. There are also engineered solutions that “scrub” CO2 directly from the air, using chemical absorbents, and then recover, purify, compress and liquefy it, so that it can be buried deep underground. That sounds difficult and expensive, and at the moment, it is. Both the UK Royal Society and the US National Research Council point out that doing it on a large enough scale to make a real difference would be hard. Nevertheless, a joint communiqué from UK learned societies recently argued that to limit global warming to 2℃ we are likely to need CO2 removal (CDR) rates in the latter part of this century that will exceed emissions at that time (“net negative emissions”). That will only be possible if we can deploy CDR technologies. A new paper in Nature Communications shows just how big the required rates of removal actually are. Even under the IPCC’s most optimistic scenario of future CO2 emission levels (RCP2.6), in order to keep temperature rises below 2℃ we would have to remove from the atmosphere at least a few billion tons of carbon per year and maybe ten billion or more – depending on how well conventional mitigation goes.  We currently emit around eight billion tonnes of carbon per year, so the scale of the enterprise is massive: it’s comparable to the present global scale of mining and burning fossil fuels. Carbon removal could potentially help to reduce problems such as ocean acidification. So a second paper in Nature Climate Change is also discouraging because it shows that even massive and sustained carbon removal at rates of five billion tonnes a year or more would not be enough to restore anything like pre-industrial conditions in the oceans, if mitigation efforts were to be relaxed. Does all this mean that carbon removal is a blind alley, and that further research is a waste of time (and money)? Well, no. But it is nothing like a magic bullet: this latest research should serve to prevent any unrealistic expectations that we could find a “solution” to climate change, or that carbon removal is any sort of alternative to reducing emissions.  Maintaining and increasing our efforts to reduce emissions is still the crucial top priority. But if we can develop removal methods that are safe and affordable, and that can be scaled up to remove a few billion tonnes per year, that would be useful even now, as it could augment those efforts to reduce CO2 emissions (which is not proving to be easy either). In the longer term, once we have eliminated all the “easily” fixed sources of CO2 emissions, by generating more electricity from renewable sources and capturing carbon from power plants, we shall still be left with several intractable sources, including aviation and agriculture, that are exceedingly hard to abate.  It is then that we shall really need CO2 removal, to take from the air what cannot easily be prevented from reaching it. And beyond that, should we eventually decide that the level of CO2 in the air at which we have stabilised is too high for comfort, and should be reduced, carbon removal will be the only way to achieve that. The low-tech biologically based removal methods are all going to be limited in their scale, not least by potential side-effects in the oceans and conflicts over alternative uses for any land required. However several groups are working on promising methods for direct (physical and/or chemical) capture from the air, trying to reduce the energy, water and materials demands – and of course the costs – to acceptable levels.  In the longer term someone may find a suitable catalyst to accelerate the natural geochemical weathering processes that already remove CO2 from the air (but much too slowly to cope with man-made emissions). That would solve the CO2 disposal problem too, especially if we can avoid mining billions of tons of minerals to use as absorbent. But it’s likely to take several decades to get from the lab to industrial-scale deployment – and none of these technologies will be deployed in practice until we have established a price on carbon emissions that makes them commercially worthwhile.  Carbon removal is not a magic bullet, but it is still a vitally important technology that we shall almost certainly need eventually. We should be researching it steadily and seriously, because it is going to take time and a lot of effort to develop methods that are safe and affordable and can be deployed on a massive scale.  So we should continue to research removal, not as a possible quick fix, but as a vital tool for the end game. It’s a massive scientific and engineering challenge that really needs the sort of concerted effort that was devoted to going to the moon or building the Large Hadron Collider. And in my opinion it would be far more worthwhile."
"Most people would never think of London as a forest. Yet there are actually more trees in London than people. And now, new work by researchers at University College London shows that pockets of this urban jungle store as much carbon per hectare as tropical rainforests. More than half of the world’s population lives in cities, and urban trees are critical to human health and well-being. Trees provide shade, mitigate floods, absorb carbon dioxide (CO₂), filter air pollution and provide habitats for birds, mammals and other plants. The ecosystem services provided by London’s trees – that is, the benefits residents gain from the environment’s natural processes – were recently valued at £130m a year.  This may equate to less than £20 a year per tree, but the real value may be much higher, given how hard it is to quantify the wider benefits of trees and how long they live. The cost of replacing a large, mature tree is many tens of thousands of pounds, and replacing it with one or more small saplings means you won’t see the equivalent net benefit for many decades after. Trees absorb CO₂ during photosynthesis, which is then metabolised and turned into organic matter that makes up nearly half of their overall mass. Urban trees are particularly effective at absorbing CO₂, because they are located so close to sources such as fossil fuel-burning transport and industrial activity.  This carbon storage potential is an extremely important aspect of their value, but is very hard to quantify. A 120-year-old London plane tree can be 30 metres tall and weigh 40 tonnes or more, and some of the carbon in its tissues will have originated from Victorian coal fires.  Measuring the height of a tall tree is difficult, because it’s rarely clear exactly where the topmost point is; estimating its mass is even harder. Typically, tree mass is estimated by comparing the diameter of the trunk or the height of the tree to the mass of similar trees (ideally the same species), which have been cut down and weighed in the past. This process relies on the assumption that trees of a certain species have a clear size-to-mass ratio.  But a fascinating property of trees is how variable they can be, depending on their environment. So inferring the mass of urban trees from their non-urban counterparts introduces large uncertainties. The UCL team use a combination of cutting-edge ground-based and airborne laser scanning techniques, to measure the biomass of urban trees much more accurately. Lidar (which stands for light detection and ranging) sends out hundreds of thousands of pulses of laser light every second and measures the time taken for reflected energy to return from objects up to hundreds of metres away.  When mounted on a tripod on a city street, lidar builds up a millimetre accurate 3D picture of everything it “sees”, including trees. The team are using lidar methods, which they pioneered to measure some of the world’s largest trees, and applying them to trees in the university’s local London Borough of Camden.   Point cloud of Russell Square by kungphil on Sketchfab  The UCL team used publicly available airborne lidar data collected by the UK Environment Agency, in conjunction with their ground measurements, to estimate biomass of all the 85,000 trees across Camden. These lidar measurements help to quantify the differences between urban and non-urban trees, allowing scientists to come up with a formula predicting the difference in size-to-mass ratio, and thus measuring the mass of urban trees more accurately.  The findings show that Camden has a median carbon density of around 50 tonnes of carbon per hectare (t/ha), rising to 380 t/ha in spots such as Hampstead Heath and Highgate Cemetery – that’s equivalent to values seen in temperate and tropical rainforests. Camden also has a high carbon density, compared to other cities in Europe and elsewhere. For example, Barcelona and Berlin have mean carbon densities of 7.3 and 11.2 t/ha respectively; major cities in the US have values of 7.7 t/ha and in China the equivalent figure is 21.3 t/ha. Trees matter, to all of us. Recent protests in Sheffield, Cardiff, London and elsewhere, over policies of tree management and removal show how strongly people feel about the trees in their neighbourhood. Finding ways to value trees more effectively is critical to building more sustainable and liveable cities. Measuring trees in new ways also helps us to see them from a new perspective. Some of these trees have incredible stories to tell. Just one example is an ash, tucked away in the grounds of St. Pancras Old Church, one of London’s (and indeed Britain’s) oldest Christian churches.  The tree has an extraordinary arrangement of gravestones around its roots, placed there when the railway was built from St Pancras in the mid-19th century. The job of rehousing the headstones was apparently given to a young Thomas Hardy, working as a railway clerk before going on to achieve literary fame. The UCL team’s 3D lidar data are helping monitor the state of this “Hardy Ash” tree in its dotage. This is just one of the ways new science is helping tell the stories of old trees.   Hardy Tree (Camden, UK) and gravestones by kungphil on Sketchfab "
nan
"
Share this...FacebookTwitterGerman skeptic and weather expert ‘Schneefan’ here writes how climate activist Mark C. Serreze recently announced this year’s sea ice extent was at the smallest all-time area. But since then Arctic temperatures have plummeted and sea ice area has grown to over 14 million square kilometers:
At the sea ice portal, the development is clearly shown.

Chart: University of Bremen
On March 103 2018 sea ice extent in the Arctic reached 14.55 million km² and so the end of Arctic sea ice growth had in fact not been reached.
The plunge in the mean temperature north of 80°N to -25°C can be seen in the plot by the DMI, and so a growth in sea ice was expected.

After an increase to about -10°C in February (due to a weather pattern) the average temperature above 80°N latitude has since fallen to -25°C. Source: DMI.
Naturally the German mainstream media such as ARD television pounced on the news and set off the climate catastrophe alarms, and thus ended up reporting totally falsely again on the real sea ice development in the Arctic: ARD: heat wave in Arctic.
A heat wave at a mean temperature of -10°C?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Below is what the ARD fake “heat wave” really looks like in the Arctic for the entire 2017/18 winter, shown by a plot of the NOAA reanalysis using measured and computed mean temperatures:

The NOAA reanalysis shows the mean 2m-temperatures for the northern hemisphere in the winter of 2017/18. Source: NOAA reanalysis
Perhaps the editors at ARD should have been more careful in checking where this faulty Arctic information came from before coming out with such climate-alarmist fake news.
A check in the Internet shows that the ARD report quoted Mark C. Serreze, a known climate activist, IPCC author and the person who coined the term Arctic sea ice “death spiral”.
He worked earlier together with the now embarrassed Peter Wadhams, who over the past years falsely forecast  the disappearance of Arctic sea ice on multiple occasions. What unfortunately has escaped the media, such as ARD, is the fact that the Arctic ice cap at the start of March 2018 is much thicker than it was 10 years ago, see the alternating charts that follow:

Impressive growth: sea ice volume (smaller chart, black curve) is greater in March 2018 than the two previous years and is near the average level (gray area) of the past years.
As everyone is aware, the multiple-year ice melts more slowly than the thinner one-year ice – and so we sill see how many Wadhams (1 Wadham = 1 million km²) will be left in September 2018.
This leads us to conclude that there is nothing left of the absurd, Al Gore envisioned, ice-free Arctic fantasies which were suppose to come true already in 2016.
Share this...FacebookTwitter "
"Sulphur will be cut drastically from global shipping transport fuels in 2020, in a move that should reduce some forms of air pollution, and may help towards tackling the climate emergency – but which could also lead to a rise in the price of flights. From 1 January 2020, ships will only be allowed to use fuel oil with a very low sulphur content, under rules brought in by the International Maritime Organisation. This cut in sulphur content has been more than a decade in the planning, and almost all shipping around the world is expected to comply, or face penalties.  “Member states, the shipping industry and fuel oil suppliers have been working for the past three years to prepare for this major change – I am confident that the benefits will soon be felt and that implementation will be smooth,” said Kitack Lim, the secretary general of the IMO. “This [is a] hugely important change which will have significant positive benefits for human health and the environment.” The new regulations are aimed at cleaning up sulphur emissions, which can cause acid rain and other forms of air pollution, rather than tackling the climate emergency. However, the dirty forms of fuel that contain high levels of sulphur are usually higher carbon too, and the costs of cleaning up sulphur may spur shipping companies to become more efficient in their fuel use, which would cut greenhouse gas emissions directly. Moving to cleaner fuels could add substantially to costs, from an estimated $400 (£303) a tonne for fuel oil today to as much as $600 a tonne, according to the International Chamber of Shipping. Higher shipping costs may be absorbed throughout the manufacturing and transport supply chains. The cost impact may also spread beyond shipping, according to the energy analyst firm Wood Mackenzie. “Knock-on effects from the cap on sulphur emissions in marine bunker fuel could even wind up giving you a more expensive plane ticket in 2020,” the company said. The IMO estimates that the new limit – of 0.5% sulphur content compared with the previous limit of 3.5%, enforced under the international convention for the prevention of pollution from ships – will cut sulphur oxide emissions from ships by 77%, an annual reduction of about 8.5m tonnes. Fuel oil for shipping has long been one of the dirtiest forms of fossil fuel, made up of the sort of low-value and cheap crude oil that is unsuitable or expensive to refine into high-grade products such as petrol for cars, or kerosene for planes. Ship engines have been designed to cope with such low-grade fuel, and the emissions they belch out as a result mostly happen far from land, making the accompanying pollution less visible and, for many decades, largely ignored by governments. But the damaging effects of the pollution have grown as globalisation has led to a massive increase in shipping transport. Shipping consumed about 3.8m tonnes of fuel oil a day in 2017, according to Wood Mackenzie, equivalent to half of global fuel oil demand. Carbon from shipping makes up about 3% of global total carbon emissions, but is expected to rise to 17% by mid-century. Fuel oil with a high sulphur content produces sulphur oxides, which can cause acid rain and particulate pollution. Alternatives to low-grade, high-sulphur fuel oil are increasingly available, albeit at a higher price. Liquefied natural gas is still a fossil fuel, but much cleaner, and infrastructure allowing its use is becoming more widespread. Biofuels are also being explored as an alternative – one enterprising cruise company is using fish guts for its fuel – and there are high hopes for harnessing hydrogen fuels in the form of ammonia for ship engines. Vessels can also be fitted with “scrubbers” to remove sulphur, though some of this is then released into the sea as effluent. Ports have also become increasingly concerned at the pollutants from cargo and passenger ships, and some operate zones where sulphur content is even more drastically reduced. Shipping is subject to complex international regulation, overseen by the IMO, the London-based UN body. However, for historic reasons, it has been excluded from calculations of international greenhouse gas emissions, and thus exempted from governments’ obligations under UN climate agreements, including the landmark Paris accord of 2015. That has meant shipping companies have felt less pressure to cut carbon, and progress on all forms of shipping pollution has been slow and often tortuous. The new sulphur regulations were first enacted by the IMO in 2008, after years of debate, but had to be re-confirmed in 2016 before finally coming into force on Wednesday. Climate change campaigners want to see much faster adoption of regulations to cut greenhouse gas emissions from shipping. The next major public meeting will be an IMO conference in London in late March and early April, where countries will come under pressure to lay out a clear plan on cutting carbon from the sector, ahead of a major UN climate conference in Glasgow in November. The IMO has a longterm aim to halve carbon from shipping by 2050, but few concrete plans to achieve it. An increasing number of countries, including the UK, are aiming for net zero carbon emissions by 2050, in line with scientific warnings on the urgency of the climate emergency. The ICS has also proposed a $2 a tonne levy on shipping fuels that would pay into a fund for research and development on zero-carbon forms of shipping, which will be explained to member states at the spring IMO conference, but could take years to come into force."
"There is something special and awe-inspiring about watching new land form. This is what is now happening in Hawai’i as its Kīlauea volcano erupts. Lava is reaching the ocean and building land while producing spectacular plumes of steam. These eruptions are hugely important for the creation of new land. But they are also dangerous. Where the lava meets the ocean, corrosive acid mist is produced and glass particles are shattered and flung into the air. Volcanic explosions can also hurl lava blocks hundreds of metres and produce waves of scalding hot water.  At Kīlauea, lava is erupting from a line of vents on the volcano’s flanks, and is moving downslope to the edge of the island, where it enters the ocean. This is a process that has been witnessed many times at Hawai’i and other volcanic islands. And it is through many thousands of such eruptions that volcanic islands like Hawai’i form. The new lava being added to Hawai’i by this latest Kīlauea eruption replaces older land that is being lost by erosion, and so prolongs the island’s lifespan. In contrast, older islands to the north-west have no active volcanoes, so they are being eroded by the ocean and will eventually disappear beneath the waves. The opposite is happening to the south-east of Hawai’i, where an underwater volcano (Lōʻihi Seamount) is building the foundations of what will eventually become the next volcanic island in this area. The lava erupting from the current Kīlauea vents has a temperature of roughly 1150 degrees °C, and has a journey of between 4.5km and 5.5km to reach the ocean. As this lava moves swiftly in channels, it loses little heat and so it can enter the ocean at a temperature of over 1000 degrees°C. We are witnessing one of the most spectacular sights in nature - billowing white plumes of steam (technically water droplets) as hot lava boils seawater. Although these billowing steam clouds appear harmless, they are dangerous because they contain small glass shards (fragmented lava) and acid mist (from seawater). This acid mist known as “laze” (lava haze) can be hot and corrosive. If anyone goes to near it, they can experience breathing difficulties and irritation of their eyes and skin. Apart from the laze, the entry of lava into the ocean is usually a gentle process, and when steam is free to expand and move away, there are no violent steam-driven explosions. But a hidden danger lurks beneath the ocean. The lava entering the sea breaks up into blobs (known as pillows), angular blocks, and smaller fragments of glass that form a steep slope beneath the water. This is called a lava delta.  A newly formed lava delta is an unstable beast, and it can collapse without warning. This can trap water within the hot rock, leading to violent steam-driven explosions that can hurl metre-sized blocks up to 250 metres. Explosions occur because when the water turns to steam it suddenly expands to around 1,700 times its original volume. Waves of scalding water can also injure people who are too close. People have died and been seriously injured during lava delta collapses  So, the ocean entry points where lava and seawater meet are doubly dangerous, and anyone in the area should pay careful attention to official advice on staying away from them. Once lava deltas have cooled and become stable they represent new land. Studies have revealed that lava deltas have distinctive features, and this has enabled volcanologists to recognise lava deltas in older rocks. Remarkable examples of lava deltas have been discovered near the top of extinct volcanoes (called tuyas) in Iceland and Antarctica. These deltas can only form in water and the only plausible source of this water in this case is melted ice. This means that these volcanoes had melted water-filled holes up to 1.5km deep in ice sheets, which is an astonishing feat. In fact, these lava deltas are the only remaining evidence of long-vanished ice sheets.  It is a privilege to see these incredible scenes of lava meeting the ocean. The ongoing eruptions form part of the natural process that enables beautiful volcano islands like Hawai'i to exist. But the creation of new land here can also bring danger to those who get too close, whether it be collapsing lava deltas or acid mist."
"Smart cities are often discussed as being the key to future urban living. The increase in capacity for more complex information can help solve human and environmental problems by saving energy and regulating traffic flow. A study has now highlighted the potential of adapting the concept of “smart” for national parks. Historically, outdoor recreation gained its popularity because of its juxtaposition to urbanisation. Motivations included adventure, simplicity and immersion in “wilderness” – away from human progress. In many cases this is still true. We are often told that greater exposure to green space and natural environments benefits health and well-being.  But how can the so-called “smart” tech improve our relaxing countryside experience? The challenge lies in integrating technology into outdoor recreation while retaining these crucial elements of the experience. Here are some simple smart options for the future ramblers. The Lake District example suggests that sensors on bins can alert the national park authorities when they are full, which reduces the problem of litter and helps conserve the landscape. Research has shown that these kinds of messages work.   It is also suggested that “smart” car parks will transmit information to motorists when car parks are full. This can reduce carbon emissions by reducing trips to multiple car parks. However, encouraging more travel by public transport and non-motorised modes of transport reduces carbon emissions more effectively. This alternative should be given priority. Planners and managers of national parks have long seen the need to reduce visitor car use. Aside from decreasing carbon emissions, visual pollution from large numbers of cars in natural areas is a long-standing problem. It takes away from the “natural” and “simple” aesthetics which are so important in attracting visitors. There is considerable need to encourage car sharing, especially because the infrastructure in rural areas is less resilient to large numbers of cars. Academics are increasingly pointing to the power of new data sharing and smart capability to solve this through measures such as car-sharing apps and better planning for integrated travel.  People who travel to, from and within national parks can do so sustainably with greater confidence if they have reliable information on public transport – as well as walking and cycling options – at their fingertips. Research on walking tourism in natural settings highlights the growing use of mobile technology as a navigational aid. The internet is increasingly used both to showcase and research walks in national parks. But practitioners urge caution for more adventurous forms of recreation.  Interviews with national park staff revealed that in particular, mountain rescue services can be stretched when hillwalkers rely too much on technology. Navigating solely with a mobile phone or GPS cannot substitute map-reading skills when faced with difficulties. Walking tourists differ in their preference for heavily managed walking routes. Some look for relative simplicity of “wild” surroundings, isolation and solitude. Others prefer abundant directional signs, information and flat, well managed paths. These differences point to an important dichotomy as technology permeates more of the previously “untouched” areas of the world. Technology is redefining how we engage with the natural environment. Sport England’s research on UK outdoor activity acknowledges a need for connectivity even in the most remote natural areas and particularly for younger participants. Rapidly improving mobile technology and information capacity epitomise the fast society many live in.    There are clear benefits to integrating smart technology into rural and natural areas. Tourist in particular are a key focal point because of the capability for improving sustainability in national parks. But the wider implications surrounding this development still need to be considered. National parks should continue to cater for all preferences and preserve some “smart-free” elements, enhancing the experience of those seeking adventure and wilderness. People should also rethink their relationship with nature. If smart technology can help the environment, preserve biodiversity and protect sensitive areas, then it should be considered as an antidote to past human negative effects."
"The energy powering Volgograd’s World Cup stadium floodlights is generated by the nearby Volgograd HydroElectric Station. This is the largest hydro power plant in Europe, and a dam which has played a pivotal role in driving sturgeon – the source of the iconic Russian delicacy, black caviar – to the brink of extinction. The 725m long and 44m high concrete giant sits about 20km outside the city centre and dissects Europe’s longest and most powerful river, the Volga. Construction began in the 1950s, as part of post-war industrialisation initiatives known as the “Great Construction Projects of Communism”. This in a city which during World War II – when it was known as Stalingrad – was the site of one of the bloodiest battles in history. The dam was completed in 1961 and today produces around 12 billion KW-hours of energy a year. The station was groundbreaking in both scale and energetic output. For a few years, it may have been the single largest power plant in the world. But despite the benefits to the climate of “clean” hydro-powered energy, the Volgograd station has been particularly damaging for the sturgeon species that attempt to migrate from the Caspian Sea to reproduce in the upper reaches of the Volga. Sturgeon, affectionately referred to as “Tsar Fish” are perhaps more critically endangered than any other group of species on the planet. There are 27 species in all, of which four are found in the Volga: Russian sturgeon, sterlet, stellate, and the beluga which is famous for producing the world’s finest caviar. These fish are often described as “living fossils”. They’ve been around since dinosaurs walked the earth 150m years ago, and individual fish can live for more than a century. Sturgeon have attained a cultural and historical significance in Russia and are a source of national pride. But socioeconomic change in Russia has been disastrous for these fish. Their rivers have been polluted, fragmented and dammed and this – along with overfishing and poaching for caviar – has caused populations in the Volga to plummet by 90% since 1970.  A slow reproductive cycle means numbers cannot recover quickly. Females do not carry eggs annually, they take many years to reach sexual maturity and, of the 250,000 - 400,000 eggs they can release at one time, only two or three fish will survive. As the last of eight hydroelectric works in the Volga-Kama cascade of dams, the Volgograd Hydroelectric station is the first barrier sturgeon migrating upstream from the Caspian Sea will encounter. In theory sturgeon can pass the dam thanks to a hydraulic fish-lift in the original design. However, it is not clear whether the lift is still operational and, even if it is, its benefits have been counteracted by further dams built upstream. Even if fish do manage to cross the dam, the return journey can prove fatal, as it often requires passing through turbines that can weigh as much as a 747 aeroplane.  The Volgograd Hydroelectric station not only blocks sturgeon migration, but alters the natural flow and temperature of the river. Sturgeon are very sensitive and rely upon signals such as flow speed and temperature to determine when and where to reproduce. Therefore, the dam is said to have directly reduced the spawning grounds of sturgeon from 3600 hectares to only 430 hectares. For beluga sturgeon in particular, 90% of their natural spawning grounds have disappeared as a result of the Volgograd dam. It is undeniable that the Volgograd station has played a part in the demise of the Russian caviar industry. Due to rapidly declining wild sturgeon populations, Russia banned commercial sturgeon fishing and black caviar exports in 2002. Now, Russia allows just 9 tonnes of the delicacy to be sold on the domestic market annually, produced by a few government-regulated fish farms. These farms cannot come close to producing enough caviar to meet Russian, let alone worldwide, demand. As a result an illegal trade meets the shortfall, with reports suggesting that 250 tonnes of illegal caviar are produced each year. Unsurprisingly then, almost all migrating spawners are poached below the Volgograd dam, and a particular hotspot is Russia’s so-called “Caviar Capital”, Astrakhan, around 400km downstream from Volgograd. There, illegal poaching of sturgeon and trade in caviar is said to be rampant – and beluga caviar fetches up to $10,000/kg. This has devastating ecological impacts –  when sturgeon are removed at this point in the river the fish have not had the chance to reproduce. The situation looks bleak. Despite Russia releasing 50m or more sturgeon raised in hatcheries, there is sparse evidence that restocking is successful. In fact, despite such releases there has been an overall decline over the past decade. And it seems counter-intuitive to release millions of juvenile sturgeon when the Volgograd dam still prevents their migration and spawning – and given that downstream poaching is rife. Greater enforcement against poaching would be a good start, along with assertive efforts to help fish move along their natural rivers (something similar has helped shortnose sturgeon in the US). So, for football fans visiting Volgograd for the World Cup the best way to help sturgeon is to avoid the lure of purchasing any black caviar as souvenirs. But, if you are that way inclined, make sure to stick to customs regulations and try your utmost to ensure the caviar is from reputable farmed sources."
"
Share this...FacebookTwitterPaleoclimate data still spotty and incomplete, leaving climate models vague, uncalibrated and filled with uncertainty
Paleo-climatological data, used for the reconstruction of past climate from proxy records such as ice cores, tree rings, sediment cores etc., have not had adequate geographical coverage.

Lake Tanganyika, Tanzania, where a sediment core was extracted. Credit: Andreas31,  CC BY-SA 3.0.
For example although the Medieval Climate Anomaly has also been well documented in other parts of the world, there has been little data when it comes to the Arabian Peninsula and the African continent, which comprise about one quarter of Earth’s land surface.
Too often scientists reconstructing past climates have been overly eager in drawing far-reaching conclusions based only a few datasets, and attempted to adventurously apply them to neighboring regions or even globally.
Climate Anomaly in Africa and Arabia as well
Now a new paper published on Eos.org here by Lüning et al. attempts to fill this data chasm. Their publication “correlated and synthesized the findings of 44 published paleotemperature case studies” from across the Afro-Arabian region and mapped the resulting trends of the Medieval anomaly’s central period of about 1000 to 1200 CE.
The paper is titled: “Warming and Cooling: The Medieval Climate Anomaly in Africa and Arabia”.
“Uncalibrated” and “vague” models
“Enormous data gaps exist,” wrote Lüning in an e-mail. “A high impact study program is needed to close these gaps. Paleoclimate information is essential to validate climate models, which otherwise are not calibrated and remain vague.”
According to their findings, paleotemperature reconstructions from these published case studies show “the Afro-Arabian region experienced climate perturbations, including an extended period of anomalously warm conditions, during medieval times. Because this Medieval Climate Anomaly represents the closest analogue to modern warming, it defines a crucial baseline by which modern postindustrial climate trends can be compared.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Lüning wrote NTZ:
This work is based on a large numbers of valuable paleoclimate studies. We have mosaiced these data points together and found that the Medieval Climate Anomaly was characterized by warming in most of Africa and Arabia, therefore justifying the term “Medieval Warm Period” for the African land area. An exception was the southern Levant where its got cooler.”
In a nutshell, the findings suggest the majority of onshore Afro-Arabian sites also experienced warming during the Medieval Climate Anomaly, and thus the warming was not just a phenomenon confined to the North Atlantic and Europe, as some scientists have tried to suggest.
Outstanding resource
Lüning has spent years researching and compiling paleoclimate data, and as a result has produced a “Climate Reconstruction Map of the Medieval Warm Period“.
Link to solar and oceanic cycles
In some of the records in the newly published study the researchers found that cold spikes corresponded with periods of low decreased solar activity or declining ocean cycles. thus suggesting that solar forcing and changing ocean circulation are the most likely causes of medieval era climate change.
Lüning added:
Climate patterns never cover the whole globe, therefore it is important to first map out trends and understand the pattern distribution. Otherwise the meaning of purely mathematically stacked data series remains unclear.”
This study represents a step toward globally characterizing the Medieval Climate Anomaly, an improved understanding of which will help scientists refine global climate models and improve hind-casting.
Read more at Research Spotlight.
Share this...FacebookTwitter "
"Conservationists tend to spend their time worrying about protecting forests, catching poachers or keeping carbon out of the atmosphere. But all these things (and more) are driven by humans. Given that it’s easier and cheaper to reduce the human birth rate than it is to address these other issues, why aren’t conservationists more concerned about keeping our population down? After all, it is estimated that more than three-quarters of the world’s ice-free land has been modified by people. We are already overstepping the planet’s boundaries and our actions are causing climate change and the sixth mass extinction.  By 2050 human population growth alone will threaten a further 14% of the planet’s species; this is on top of the 52% decline in numbers of mammals, birds, reptiles, amphibians and fish over the past four decades. Only 13 years ago, we were 6 billion; just seven years later, we hit 7 billion and by 2100 we could be as many as 12.3 billion people. Shockingly, with each child a woman has, her carbon emissions legacy is increased six-fold. It cannot be denied that our size, density and growth rate all increase wildlife extinctions.  But all is not lost. Fertility rates decline the longer a girl spends in school. By simply providing better female education, the overall population in 2050 could be 1 billion less than current projections. This is because women who are empowered through education have fewer children, as well as having them later in life and therefore have the resources to provide them with better care. Along with this, one in five women – 800m worldwide – have an unmet need for modern contraception; in developing countries this can be as high as 60%.   We aren’t suggesting any evil population control schemes here – it’s about providing resources to girls who want an education and women who want access to family planning. The benefits can be seen relatively quickly: between 1960-2000, contraceptive use by married women in developing nations increased from 10% to 60%, reducing the average number of children per woman from six to three.  However, we still pay surprisingly little attention to what this all means for the world’s wildlife.  A small but growing number of organisations are beginning to integrate wildlife conservation with family planning. Blue Ventures, a marine conservation organisation in Madagascar, has trained local women to provide contraception in rural villages close to protected areas. In three years, the project reduced its own ecological footprint by 267 global hectares purely by providing access to family planning. A slightly different approach was taken by The Center for Biological Diversity in the US. On World Population Day last year the organisation distributed 40,000 condoms wrapped in packaging depicting endangered species with catchy slogans such as “Don’t go bare … Panthers are rare”. It is unclear whether this had any effect on human behaviour, but the emphasis on bringing the issue to a developed country with a high consumption rate is commendable, given the typical focus on stemming population growth only in developing countries. A more holistic approach combines family planning and other healthcare services with alternative livelihood options – this has been implemented in some key high biodiversity areas that have an unmet need for contraception and healthcare. One programme in Nepal led to an increased use of condoms and reduced wood fuel usage equivalent to saving nearly 9,000 trees annually.  There is an increasing gap between donations and demand for contraception. Filling the unmet need for family planning across developing countries would cost US$8.1 billion annually; finding this amount of money will clearly be challenging. Furthermore, contraceptive use and female access to education are affected by strong cultural and religious problems. We cannot simply advocate for more access to family planning and education without addressing barriers to access. Population growth doesn’t seem to be a major concern for conservationists but it should be. Researchers should investigate the effects of human population interventions on wildlife, while conservationists could form alliances with other sectors of society, such as reproductive choice and womens’ rights groups. As environmental organisations often integrate educational aspects into their programs, it would not be difficult to direct further educational materials towards women and girls. We now have evidence to show the links between human population size, growth and density on the environment, but we need to increase our research efforts on how contraception and female education policies affect biodiversity. Conservation scientists cannot dismiss the effects of overconsumption on the natural world, but we also cannot disregard the effect our sheer population size and growth have on the planet.  Addressing human population growth may be a relatively fast and cheap remedy for wildlife loss, which can help reduce consumption and brings us closer to achieving true sustainability. The sooner we start to pull the brakes, the easier it will be to eventually come to a stop. So what are we waiting for?"
"
Share this...FacebookTwitterVolcano Agung in Bali is showing worrisome signs of a major eruption, writes German climate blogger Schneefan here. The highest level of activity with multiple tremor episodes were just recorded. You can monitor Agung via live cam and live seismogram.
The 3000-mter tall Agung has been at the highest warning level 4 since September 21.
Schneefan writes that the lava rise has started and that “an eruption can be expected at any time“.
So far some 140,000 people have been evacuated from the area of hazard, which extends up to 12 km from the volcano. Schneefan writes:
Yesterday ground activity by far exceeded the previous high level. Quakes have become more frequent and stronger, which indicates a stronger magma flow (see green in the histogram). Since October 13 there has been for the first time a “nonharmonic trembling (tremor), which can be seen in red at the top of the last two bars of the histogram.”



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The colors of the columns in the bar chart from bottom to top stand for perceptible earthquakes (blue) low eartnhquakes (green) surface quakes (orange . Just recently red appeared, signifying non harmonic tremors.  The seismogram below shows what are at times longer period quakes: meaning magma is violently flowing in the volcano. Source: https://magma.vsi.esdm.go.id/.
Since yesterday the seismogram for AGUNG has been showing powerful rumbling (red).

The seismogram of AGUNG shows powerful tremors (level RED). The seismogram is updated every 3 minutes: Source: Seismogramm
Because Agung is located near the equator, a major eruption with ash flying up into the stratosphere would have short-term climatic impacts that could last a few years.
Agung last erupted in 1963 with an explosivity index of VEI 5, sending a plume of ash some 25 km into the atmosphere before leading to a cooling of 0.5°C. The eruption of Pinatubo in the Philippines in 1991 led to a global cooling of 0.5°C.
 
 
Share this...FacebookTwitter "
"“The Darfur conflict began as an ecological crisis”, wrote the then-UN secretary general Ban Ki-Moon back in 2007, about an ongoing war which arose, he said, “at least in part from climate change”. Since then the idea that climate change has caused and will cause human conflict and mass migrations has become more and more accepted – just look at the claimed effects of droughts in Syria and Ethiopia. The media has even started using terms such as “climate refugees” and “environmental migrants” to describe people fleeing their homes from these climate-driven conflicts. But it isn’t clear whether there is much evidence for this link between climate change and conflict –  there certainly seems to be no consensus within the academic literature. In our recent paper, my student Erin Owain and I decided to test the climate-conflict hypothesis, using East Africa as our focus. The region is already very hot and very poor, making it especially vulnerable to climate change (in fact neighbouring Chad is by some measures the single most vulnerable country in the world).  As the planet warms, East Africa’s seasonal rains are expected to become much more unpredictable. This is a particular problem as recent economic development has been concentrated in agriculture, a highly climate-sensitive sector that accounts for more than half of the entire economy in countries like Ethiopia or Sudan. One study led by the European Commission found that declining rainfall over the past century may have reduced GDP across Africa by 15-40% compared with the rest of the developing world.  East Africa also has a long history of conflict and human displacement, which persists in some countries to this day, such as the civil wars in Sudan and Somalia. The UN Refugee Agency reports there were more than 20m displaced people in Africa in 2016 – a third of the world’s total. The World Bank predicts this could rise up to 86m by 2050 due to climate change.  To test the climate-conflict hypothesis, Erin and I therefore focused on the ten main countries in East Africa. We used a new database that records major episodes of political violence and number of total displaced people for the past 50 years for each of the ten countries. We then statistically compared these records both at a country and a regional level with the appropriate climatic, economic and political indicators. We found that climate variations such as regional drought and global temperature did not significantly impact the level of regional conflict or the number of total displaced people. The major driving forces on conflict were rapid population growth, reduced or negative economic growth and instability of political regimes. Numbers of total displaced people were linked to rapid population growth and low or stagnating economic growth.  The evidence from East Africa is that no single factor can fully explain conflict and the displacement of people. Instead, conflict seems to be linked primarily to long-term population growth, short-term economic recessions and extreme political instability. Halvard Buhaug, a professor at the Peace Research Institute Oslo, looked at the same questions in 2015 and his study reached much the same conclusion: sociopolitical factors were more important than climate change. Things were different for “refugees”, however – those displaced people who were forced to cross borders between countries. Refugee numbers were related to the usual demographic and socio-economic factors. But in contrast to total displaced people and occurrence of conflict, variations in refugee numbers were found to be related significantly to the incidence of severe regional droughts. And these droughts can in turn be linked to a long-term drying trend ascribed to anthropogenic climate change. However, it is important to consider the counterfactual: had there been slower population growth, stronger economies and more stable political regimes, would these droughts still have led to more refugees? That’s beyond the scope of our study, which may not be a definitive test of the links between climate change and conflict. But the occurrence of peaks in both conflict and displaced people in the 1980s and 1990s across East Africa suggest that decolonisation and the end of the Cold War could have been key issues.  Nonetheless, while conflict has decreased across the region since the end of the Cold War, the number of displaced people remains high. We argue that with good stable governance there is no reason why climate change should lead to greater conflict or displacement of people, despite the World Bank’s dire predictions. Water provides one reason to be optimistic. The UN reports that, over the past 50 years, there have been 150 international water resource treaties signed compared to 37 disputes that involved violence.  What our study suggests is the failure of political systems is the primary cause of conflict and displacement of large numbers of people. We also demonstrate that within socially and geopolitically fragile systems, climate change may potentially exacerbate the situation particularly with regards to enforced migration."
"
Share this...FacebookTwitterGame over for green energies and CO2 reductions? 
Journalist Daniel Wetzel of German national daily Die Welt here presents a devastating commentary on Paris Accord and Germany’s so far “illusionary” CO2 reductions targets. The German failure is a signal that could have significant global consequences.
Wetzel not only calls the targets illusionary, he also believes the existing Energiewende (transition to green energies) is “at an end”.
2030 target completely illusionary
According to Wetzel, “The Energiewende and climate change are not among the priorities of the government” and that Germany reaching its self-imposed targets is achievable only if “everyone were forced to switch off every boiler, oven, motor. Completely illusionary.”
Lots of talk, no action
While German political leaders like to continue pretending they are taking real action to combat climate change, the reality is that the German government has been rolling back subsidies for green energies such as wind and sun over the past years. And many localities have made the permitting of wind parks far more stringent.
The days of unfettered support for green energies are over.

Germany’s CO2 equivalent greenhouse gas emissions in millions of tonnes (Source: UBA Federal Office of Environment)
The result: Germany has not reduced its CO2 equivalent emissions for close to a decade (see chart above). Wetzel writes that everyone agrees that it is unrealistic to think that Germany will somehow make the sudden downward trend turn.
German Energiewende “at the end”
The Die Welt journalist adds: “Practically all renowned environmental analysts and government experts have already determined that the German Energiewende structurally has reached the end and that a system change is needed.”
Humans burning fuel one million years
In his commentary Wetzel also reminds that humans have been using fire for some one million years, and that it cannot be expected that they will just stop doing so during the course of one single generation.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Wetzel compares the Energiewende to the Apollo space program, which put man on the moon after 12 years and 120 billion dollars of investment. He notes that compared to the great transformation which Energiewende would entail, this was peanuts.
He warns that implementing the Energiewende could cost the country a back-breaking three trillion euros (without mentioning the impact on climate would be negligible) and doubts the public would ever accept such a large-scale, draconian transformation of society.
German credibility takes a blow
Wetzel also comments the Germany’s failure to make the 2020 targets has tarnished the country’s image as a leader in climate protection, and adds: “The coalition agreement and the German Federal Budget for 2018 robbed all remaining credibility.”
In short Germany’s is not serious about reducing CO2.
Environmental groups and the Potsdam Institute, for example, are fuming, yet keep insisting it’s still not too late and achieving the target is still possible. But Wetzel injects sobriety and realism: “In the meantime we know that Germany will not only fail resoundingly to meet its self-imposed 2020 targets, but also those of the EU itself.”
With Germany as Europe’s largest economy, and regarded as a role model for all things green, the country’s failure would send a devastating message to the rest of the continent and the world: The Green Revolution was mostly a dream and was in fact never attainable. If tech-savvy Germany can’t do it, who can?
“No reality basis”
Yet, German officials continue to insist they can meet the 2030 target! But Die Welt’s Wetzel notes that doing so would mean Germany cutting it’s CO2 equivalent emissions by some 40%, or 350 million tonnes, within the next 12 years. That would mean radical and painful transformations. Recall that Germany has not managed to emissions at over the past decade (see chart above). Wetzel asks: “How credible is this target?”
Die Welt’s Wetzel summarizes:
The feasibility rhetoric of policymakers as a rule has no reality basis.”
Finally, he reminds that Germany going it alone will never work, and that climate protection has to be “organized internationally – or not at all”.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHere at NTZ we are glad to see that German weather and climate blogger Schneefan is back from his hiatus and this week he presents a couple of interesting posts, here and here.
Cold to grip Europe for rest of month
First he writes that the latest weather models and patterns are now pointing to an extended winter this year for Europe. For example the 14-day forecast for Hanover shows frosty conditions ahead.
Also a recent run by the European weather model shows severe cold potentially gripping Europe in 9 days:

February 1°C cooler than normal 
So far Germany has seen the first half of February come in almost 1°C cooler than normal, making it among the coldest in years, says meteorologist Dominik Jung of www.wetter.net.
Earlier projection was entirely wrong
According weather experts, March is also expected to be wintry and could be one of the coldest in years. That’s quite a turnaround given that the earlier CFS forecast made back in mid-January which showed blow torch temperatures cooking Europe in February:

The US National Weather Service (CFS) 2m surface temperature forecast made on 18 January, 2018, showed very warm conditions for February. That forecast has since been drastically revised.
La Nina dragging temperatures down
So why have we been hearing about so many harsh winter conditions all over the northern hemisphere this winter?
One reason likely has something to do with the fact that the globe has been cooling off substantially since the last El Nino ended in 2016. Currently we are now experiencing La Niña conditions:

Chart shows the weekly deviations from the mean for the El Niño-region 3.4 from May 2016 until the beginning of February 2018. Currently we find ourselves in the cool La Nina phase. Source: KNMI
Furthermore, the National Weather Service continues to show La Nina conditions persisting through most of 2018, which means a greater likelihood of a further cooling of the globe’s surface:

Over the past 2 years global surface temperatures as measured by satellites show steady cooling. Source: WoodForTrees.org.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Another longer term culprit suspected of being behind the cooling is the current solar cycle number 24, which has been abnormally weak over the entire current decade. The Cycle 24 was the weakest in 200 years. Low solar activity has been shown to lead to cooling surface temperatures.

Screenshot shows the number of spotless days on the sun up to February 14, 2018. Source: www.spaceweather.com/
Oceans cooling over past 3 years
Another sign that bodes especially ill for continued surface cooling is that the world’s oceans have been cooling. 71% of the earth’s surface is covered by water, which means these temperature changes will have significant impacts on the globe’s climate.
In January 2018 Ron Clutz reported at Science Matters on an unexpected phenomenon: the cooling of the global oceans over the past 3 years:
The chart below shows SST monthly anomalies as reported in HadSST3 starting in 2015 through December 2017.”


After a bump in October the downward temperature trend has strengthened. As will be shown in the analysis below, 0.4C has been the average global anomaly since 1995 and December has now gone lower to 0.325C.  NH dropped  sharply along with the Tropics.  SH held steady erasing the Oct. bump.  All parts of the ocean are clearly lower than at any time in the past 3 years.
For Reference:
Global SSTs are the lowest since 3/2013
NH SSTs are the lowest since 3/2014
SH SSTs are the lowest since 1/2012
Tropics SSTs are the lowest since 3/2012
[…]
The oceans are driving the warming this century.  SSTs took a step up with the 1998 El Nino and have stayed there with help from the North Atlantic, and more recently the Pacific northern “Blob.”  The ocean surfaces are releasing a lot of energy, warming the air, but eventually will have a cooling effect.  The decline after 1937 was rapid by comparison, so one wonders: How long can the oceans keep this up?”
Read the entire post at Science Matters.
“Bad times” for global warming alarmists
Ron also looks at the AMO ocean cycle. Schneefan summarizes it all in a nutshell:
Everything points to an imminent tipping of the AMO cycle. That’s going to pull global temperatures downward. For Rahmstorf and Co. bad times are starting.”
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDuring 2017, there were 150 graphs from 122 scientific papers published in peer-reviewed journals indicating modern temperatures are not unprecedented, unusual, or hockey-stick-shaped — nor do they fall outside the range of natural variability.  We are a little over 4 months into the new publication year and already 81 graphs from 62 scientific papers undermine claims that modern era warming is climatically unusual.



Zheng et al., 2018
“In this study we present a detailed GDGT data set covering the last 13,000 years from a peat sequence in the Changbai Mountain in NE China. The brGDGT-based temperature reconstruction from Gushantun peat indicates that mean annual air temperatures in NE China during the early Holocene were 5–7°C higher than today.  Furthermore, MAAT records from the Chinese Loess Plateau also suggested temperature maxima 7–9°C higher than modern during the early Holocene (Peterse et al., 2014; Gao et al., 2012; Jia et al., 2013). Consequently, we consider the temperatures obtained using the global peat calibration to be representative of climate in (NE) China. … The highest temperatures occurred between ca. 8 and 6.8 kyr BP, with occasional annual mean temperatures >8.0 ± 4.7°C, compared to the modern-day MAAT of ∼3°C.”


Anderson et al., 2018
“We estimate that air temperatures were 2.5–3.0 °C higher during the Holocene Thermal Maximum than the local 1960–1990 average. … Between 1700 and 1925 CE temperatures were likely 0.6–0.8 °C lower than the 1950–2015 reference temperature.“


Harning et al., 2018
“Iceland’s terrestrial HTM [Holocene Thermal Maximum] has previously been constrained to ~7.9 to 5.5 ka based on qualitative lake sediment proxies (Larsen et al., 2012; Geirsdottir et al., 2013), likely in association with progressive strengthening and warming of the Irminger Current (Castaneda et al., 2004; Smith et al., 2005; Olafsdottir et al., 2010). Numerical modeling experiments for Drangajokull suggest that peak air temperatures were 2.5 – 3°C warmer at this time relative to the 1961-1990 CE average (Anderson et al., 2018). … During the Little Ice Age (LIA, 1250-1850 CE), the Vestfirðir region entered the lowest multi centennial spring/summer temperature anomalies of the last 9 ka. Based on recent numerical  modeling simulations, this anomaly is estimated to be 0.6-0.8°C below the 1950-2015 average on Vestfirðir (Anderson et al., 2018).”



Grieman et al., 2018
“[C]limate anomalies appear to be reflected in the Tunu VA [vanillic acid] record, with elevated VA [vanillic acid] during the warm periods and lower levels during the colder periods. The data suggest a positive correlation between North American fire and hemispheric mean temperature. This relationship could be due to climate-driven changes in temperature or precipitation on burning extent, frequency, or location, as well as to changes in atmospheric transport patterns. …  … [E]levated VA [vanillic acid] early in the record [Roman Warm Period] and around the MCA [Medieval Climate Anomaly]. It also emphasizes the decreasing trend from 1200-1900 CE [Little Ice Age] and the increase during the 20th century [Current Warm Period].”



Thornalley et al., 2018


Maley et al., 2018


Polovodova Asteman et al., 2018
“The record demonstrates a warming during the Roman Warm Period (~350 BCE – 450 CE), variable bottom water temperatures during the Dark Ages (~450 – 850 CE), positive bottom water temperature anomalies during the Viking Age/Medieval Climate Anomaly (~850 – 1350 CE) and a long-term cooling with distinct multidecadal variability during the Little Ice Age (~1350 – 1850 CE). The fjord BWT [bottom water temperatures] record also picks up the contemporary warming of the 20th century, which does not stand out in the 2500-year perspective and is of the same magnitude as the Roman Warm Period and the Medieval Climate Anomaly.”



Wündsch et al., 2018


McGowan et al., 2018
“Our reconstructed Tmax [temperature maximum] for these warmer conditions peaks around 1390 CE at + 0.8 °C above the 1961–90 mean, similar to the peak Tmax during the RWP [Roman Warm Period]. These results are aligned with the findings that show the period from 1150 to 1350 CE to be the warmest pre-industrial chronzone of the past 1000 yrs for southeast Australia.”


Wu et al., 2018


Hanna et al., 2018
“Reconstructed temperatures are generally coolest between 300 and 800 CE (Tavg = 2.24 ± 0.98°C), displaying three temperature minima centered at 410 CE (1.34 ± 0.72°C), 545 CE (1.91 ± 0.69°C), and 705 CE (1.49 ± 0.69°C). Temperatures then rapidly increased, reaching the warmest interval (800–1000 CE) in the approximately 1700-year record. During this interval, average temperatures were 3.31 ± 0.65°C, with a maximum temperature of 3.98°C.”


Li et al., 2018
“There are also other studies that suggest that the recent climate warming over the southeastern TP actually began in the 1820s (Shi et al., 2015). However, a few reconstructions from the west and northwest parts of Sichuan or from the southeastern TP indicate that there were no obvious increase of temperature during the past decades (Li et al., 2015b; Zhu et al., 2016).”
 


Qin et al., 2018     


Allen et al., 2018
“The longest sustained period of relatively high temperatures in the reconstructions is the post 1950 CE period although there are clearly individual years much earlier that were warmer than any in the post-1950 period.”


Li et al, 2018   (North China)


Levy et al., 2018
“The three historical moraine crests indicate that there were at least three ice-margin stillstands or advances during historical time. Summer temperature records from North lake (Axford et al. 2013) and Lake N3 (Thomas et al. 2016) broadly register cooling in the past 200 years in western Greenland, which likely influenced the advance to the historical moraines.”


Perner et al., 2018
“From c. 1.5 ka BP onwards, we record a prominent subsurface cooling and continued occurrence of fresh and sea‐ice loaded surface waters at the study site.”



Zhang et al., 2018



Guo et al., 2018


Belle et al., 2018


Lemmen and Lacourse, 2018
“The early Holocene was marked by relatively stable temperatures that exceeded modern by ~2 to 3°C. Inferred temperatures generally decrease through the remainder of the Holocene.”




Oppedal et al., 2018


Blundell et al., 2018     


Badino et al., 2018     
“Between ca. 8.4-4 ka cal BP [8,400 to 4,000 years before present], our site [Italian Alps] experienced a mean TJuly of ca. 12.4 °C, i.e. 3.1 °C warmer than today [9.3 °C]. … Between 7400 and 3600 yrs cal BP, an higher-than-today forest line position persisted under favorable growing conditions (i.e. TJuly at ca. 12 °C).”



Song et al., 2018
“[A] general warm to cold climate trend from the mid-Holocene to the present, which can be divided into two different stages: a warmer stage between 6842 and 1297 cal yr BP and a colder stage from 1297 cal yr BP to the present.”


Blarquez et al., 2018


Perner et al., 2018
“[W]e find evidence of distinct late Holocene millennial-scale phases of enhanced El Niño/La Niña development, which appear synchronous with northern hemispheric climatic variability. … Phases of dominant El Niño-like states occur parallel to North Atlantic cold phases: the ‘2800 years BP cooling event’, the ‘Dark Ages’ and the ‘Little Ice Age’, whereas the ‘Roman Warm Period’ and the ‘Medieval Climate Anomaly’ parallel periods of a predominant La Niña-like state.”


 Magyari et al., 2018
“…its climatic tolerance limits were used to infer July mean temperatures exceeding modern values by 2.8°C at this time [8200-6700 cal yr BP] (Magyari et al., 2012).”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->






Mikis, 2018


Kelley et al., 2018
“Historical and remote-sensing records indicate that Nordenskiöld Gletscher has been stable or advancing since AD 1950 (Weidick, 1968, 1994).”


Papadomanolaki et al., 2018  (Baltic Sea)
“A large fraction of the Baltic Proper became hypoxic again between 1.4 and 0.7 ka BP, during the Medieval Climate Anomaly (MCA), when mean air temperatures were 0.9–1.4 °C higher than temperatures recorded in the period 1961–1990 (e.g. Mann et al., 2009; Jilbert and Slomp, 2013).”

Leonard et al., 2018  (Great Barrier Reef, Australia)
“Coral derived sea surface temperature (SST-Sr/Ca) reconstructions demonstrate conditions ∼1 ◦C warmer than present at ∼6200 (recalibrated 14C) and 4700 yr BP, with a suggested increase in salinity range (δ18O) associated with amplified seasonal flood events, suggestive of La Niña (Gagan et al., 1998; Roche et al., 2014).”

Suvorov and Kitov, 2018 (Eastern Sayan, Siberia)
“The authors examined the variability of activity of modern glaciation and variation of natural conditions of the periglacial zone on climate and on dendrochronological data. Results of larch and Siberian stone pine growth data were revealed at the higher border of forest communities. …  It is believed that the temperature could be 3.5 °C warmer at the Holocene optimum than at the present time (Vaganov and Shiyatov 2005). … Since 2000, there has been growth of trees instability associated with a decrease in average monthly summer temperatures. …  Since the beginning of 2000, decrease in summer temperatures was marked.”

Lozhkin et al., 2018 (East Siberia)
“The postglacial occurrence of relatively warm/dry and warm/wet intervals is consistent with results of a regional climate‐model simulation that indicates warmer than present temperatures and decreased effective moisture at 11 000 cal. a BP and persistence of warm conditions but with greater moisture and longer growing season at 6000 cal. a BP.”

Xu et al., 2018 (Northern South China Sea)
“This study provides evidence that thermal coral bleaching events have occurred in the warmer mid-Holocene (where maximum monthly summer SST was 2 °C higher than at present) in Hainan island.  … [C]oral bleaching events under high SST conditions have already occurred in the mid-Holocene and are by no means a new ecological phenomenon of current global warming.”

20th/21st Centuries Non-Warming

Lansner and Pepke Pedersen, 2018
“In locations best sheltered and protected against ocean air influence, the vast majority of thermometers worldwide trends show temperatures in recent decades rather similar to the 1920–1950 period. This indicates that the present-day atmosphere and heat balance over the Earth cannot warm areas – typically valleys – worldwide in good shelter from ocean trends notably more than the atmosphere could in the 1920–1950 period. … [T]he lack of warming in the OAS temperature trends after 1950 should be considered when evaluating the climatic effects of changes in the Earth’s atmospheric trace amounts of greenhouse gasses as well as variations in solar conditions.”













Partridge et al., 2018
“We present a novel approach to characterize the spatiotemporal evolution of regional cooling across the eastern U.S. (commonly called the U.S. warming hole), by defining a spatially explicit boundary around the region of most persistent cooling. The warming hole emerges after a regime shift in 1958 where annual maximum (Tmax) and minimum (Tmin) temperatures decreased by 0.46°C and 0.83°C respectively.”



Payomrat et al., 2018
“During the third segment (1870–2001), the maximum temperature pattern seemed to be constant compared to the changing rate (+0.004 °C/decade). … The short fourth segment, which occurred from 2002 to 2013, showed a deceasing trend at a rate of -0.12 °C/decade.”


Mikkelsen et al., 2018


Westergaard-Nielsen et al., 2018
“Here we quantify trends in satellite-derived land surface temperatures and modelled air temperatures, validated against observations, across the entire ice-free Greenland. … Warming trends observed from 1986–2016 across the ice-free Greenland is mainly related to warming in the 1990’s. The most recent and detailed trends based on MODIS (2001–2015) shows contrasting trends across Greenland, and if any general trend it is mostly a cooling. The MODIS dataset provides a unique detailed picture of spatiotemporally distributed changes during the last 15 years. … Figure 3 shows that on an annual basis, less than 36% of the ice-free Greenland has experienced a significant trend and, if any, a cooling is observed during the last 15 years (<0.15 °C change per year).”


Smeed et al., 2018


 Ahn et al., 2018


Eck, 2018     
“[A] majority (12/14) of the regions within the SAM [Southern Appalachian Mountains] have experienced a long-term decline in mean winter temperatures since 1910.  Even after removing the highly anomalous 2009-2010 winter season, which was more than two standard deviations away from the long-term mean, the cooling of mean winter temperatures is still evident. … Higher winter temperatures dominated the early 20th century in the SAM [Southern Appalachian Mountains] with nine of the ten warmest winter seasons on record in the region having occurred before 1960. The 1931-1932 winter season, the warmest on record, averaged 8.0°C for DJF [December-February], nearly 4.7°C higher than the 1987-2017 normal mean winter temperature of 3.3°C. … Despite the 2016-2017 winter season finishing with the highest mean temperatures (5.7ºC) observed in the SAM [Southern Appalachian Mountains]  since 1956-1957, there have been several years of anomalous negative temperature anomalies, with the 2009-2010 (0.3ºC) and 2010-2011 (1.2ºC) winter seasons finishing as two of the coldest on record for all regions.”


Yi, 2018


Nicolle et al., 2018     



Purich et al., 2018     
“Observed Southern Ocean changes over recent decades include a surface freshening (Durack and Wijffels 2010; Durack et al. 2012; de Lavergne et al. 2014), surface cooling (Fan et al. 2014; Marshall et al. 2014; Armour et al. 2016; Purich et al. 2016a) and circumpolar increase in Antarctic sea ice (Cavalieri and Parkinson 2008; Comiso and Nishio 2008; Parkinson and Cavalieri 2012).  …  Our results suggest that recent multi-decadal trends in large-scale surface salinity over the Southern Ocean have played a role in the observed surface cooling seen in this region. … The majority of CMIP5 models do not simulate a surface cooling and increase in sea ice, as seen in observations.”


Palmer et al., 2018


Clem et al., 2018
“This study finds recent (post-1979) surface cooling of East Antarctica during austral autumn to also be tied to tropical forcing, namely, an increase in La Niña events. … The South Atlantic anticyclone is associated with cold air advection, weakened northerlies, and increased sea ice concentrations across the western East Antarctic coast, which has increased the rate of cooling at Novolazarevskaya and Syowa stations after 1979. This enhanced cooling over western East Antarctica is tied more broadly to a zonally asymmetric temperature trend pattern across East Antarctica during autumn that is consistent with a tropically forced Rossby wave rather than a SAM pattern; the positive SAM pattern is associated with ubiquitous cooling across East Antarctica.”


Kim et al., 2018     
“Recent surface cooling in the Yellow and East China Seas and the associated North Pacific climate regime shift … The Yellow and East China Seas (YECS) are widely believed to have experienced robust, basin-scale warming over the last few decades. However, the warming reached a peak in the late 1990s, followed by a significant cooling trend.  … The most striking evolution pattern is that a robust warming trend at a rate of +0.40°C per decade reached a peak in the late 1990s, and then it turned downward at a rate of  −0.36°C per decade. The positive and then negative trends are estimated throughout the YECS for the periods 1982−1997.”


Shu et al., 2018
“The link between boreal winter cooling over the midlatitudes of Asia and the Barents Oscillation (BO) since the late 1980s is discussed in this study, based on five datasets. Results indicate that there is a large-scale boreal winter cooling during 1990–2015 over the Asian midlatitudes, and that it is a part of the decadal oscillations of long-term surface air temperature (SAT) anomalies.”


Mallory et al., 2018


Jones et al., 2018


Burger et al., 2018
“Previous studies have identified spatial and temporal trends in temperature and precipitation in Chile over recent decades. Temperature rose significantly during the mid to late 20th century in coastal locations between 18 to 33 °S (Rosenblüth et al., 1997), but then started to decrease, with a cooling trend up to -0.20ºC decade-1 dominating over the past 20-30 years (Falvey and Garreaud, 2009).”
 

Hrbáček et al., 2018
“Active layer monitoring in Antarctica: an overview of results from 2006 to 2015 … Air temperatures showed significant regional differences within the study areas. In the western Antarctic Peninsula region, Vestfold Hills and northern Victoria Land, a slight air temperature cooling was detected, while at other sites in Victoria Land and East Antarctica the air temperature was more irregular, showing no strong overall trend of warming or cooling during the study period (Figure 2). The Antarctic Peninsula region has been reported as the most rapidly warming part of Antarctica (e.g. Turner et al., 2005, 2014), but cooling has been reported since 2000 (Turner et al., 2016). Relatively stable air temperature conditions during the past 20 years were reported in Victoria Land (Guglielmin & Cannone, 2012).”


Ramesh and Soni, 2018
“The present paper reviews the progress of India’s scientific research in polar meteorology. The analysis of 25 years meteorological data collected at Maitri station for the period 1991–2015 is presented in the paper. The observed trend in the temperature data of 19 Antarctic stations obtained from READER project for the period 1991–2015 has also been examined. The 25 years long term temperature record shows cooling over Maitri station. The Maitri station showed cooling of 0.054 °C per year between 1991 and 2015, with similar pronounced seasonal trends. The nearby Russian station Novolazarevskaya also showed a cooling trend of 0.032 °C per year. … The temperature trend in average temperature of 19 Antarctica stations is also examined to ascertain the extent of cooling or warming trend (Supplementary Table_S1). The majority of stations in East Antarctica close to the coast show cooling or no significant trend. … Turner et al. (2016) using stacked temperature record found a significant cooling trend for the Antarctic Peninsula for the period 1999–2014.”


Gennaretti et al., 2018
 

Liu et al., 2018


Tang et al., 2018
“The study of Antarctic precipitation has attracted a lot of attention recently. The reliability of climate models in simulating Antarctic precipitation, however, is still debatable. This work assess the precipitation and surface air temperature (SAT) of Antarctica (90°S to 60°S) using 49 Coupled Model Intercomparison Project phase 5 (CMIP5) global climate models”


 Cerrone and Fusco, 2018  (Antarctica)
“Compelling evidence indicates that the large increase in the SH sea ice, recorded over recent years, arises from the impact of climate modes and their long-term trends. The examination of variability ranging from seasonal to interdecadal scales, and of trends within the climate patterns and total Antarctic sea ice concentration (SIC) for the 32-yr period (1982–2013), is the key focus of this paper. The results herein indicate that a progressive cooling has affected the year-to-year climate of the sub-Antarctic since the 1990s.”
Fernandoy et al., 2018  (Antarctic Peninsula)
“As shown by firn core analysis, the near-surface temperature in the northern-most portion of the Antarctic Peninsula shows a decreasing trend (−0.33°C year−1) between 2008 and 2014 [-1.98°C].”
Vignon et al., 2018  (Antarctica)
“The near‐surface Antarctic atmosphere experienced significant changes during the last decades (Steig et al., 2009; Turner et al., 2006). In particular, the near‐surface air over the Western part of Antarctica exhibits one of the major warming over the globe (Bromwich et al., 2013a), with heating rates larger than 0.5 K per decade at some places. Despite a significant warming in the end of the 20th century, the Antarctic Peninsula has been slightly cooling since 1998, reflecting the high natural variability of the climate in this region (Turner et al., 2016). East Antarctica has experienced a slight cooling trend (Nicolas & Bromwich, 2014; Smith & Polvani, 2017) particularly marked during autumn.”
Lei et al.,2018 (N, NE, SE China)
“The authors analyzed the observed winter surface air temperature in eastern mainland China during the recent global warming hiatus period through 1998-2013. The results suggest a substantial cooling trend of about -1.0°/decade in Eastern China, Northeast China and Southeast China.”
Share this...FacebookTwitter "
"British dairy farmers are once again protesting over the low prices on offer for their milk. They worry that too many producers are going bust, and that long-term milk supplies are at risk. Supermarkets are usually cast as the villains in this piece and this time it is no different. Farming unions are meeting Morrisons to ask for a fairer deal – and protesters in Stafford even took two cows into an Asda branch to help make their point. However it is too simplistic to blame the supermarkets – the real problem is global. Too much low-value milk is being produced around the world. Over the past decade, UK milk production averaged 14 billion litres per year, of which around 500m litres are exported. Just 139m litres are imported. Milk made in the UK tends to stay in the UK.  Nonetheless the number of dairy farmers continues to decline, from 40,000 at the start of the 1990s to 14,159 in 2013. This is alarming to some, but it shouldn’t be. For long-term security of milk supplies, it doesn’t really matter how many dairy farmers pack up production. The cows often move to another farm and it is easy enough to step up production through more intensive feeding and selective breeding. After all, even though the total number of cows in the UK has halved since the 1970s, production has remained steady thanks to the fact average yields have doubled. Farmers are literally squeezing more out of each cow.  Half of domestic milk production has to be diverted from the more lucrative liquid market into cheese, yoghurt, ice cream, butter and other manufactured products.  This is partly because people drink a lot less milk these days; from five pints per week in the 1960s to around three pints today. Consumption is down 8.1% in the past ten years alone. Any industry would struggle in such circumstances. This supply and demand problem is replicated across the world – and there is currently a massive oversupply of manufactured milk products on world markets due largely to increased production in China, India, Brazil and New Zealand (where they are dealing with similar issues). This surplus, combined with a collapse in global demand especially in China, has depressed prices. A Russian import ban in retaliation for EU action over Ukraine has also hit prices. Russia used to buy 27% of the EU’s cheese exports and 19% of its butter. The Global Dairy Trade auction, the industry’s main dairy commodities index, hit a 13-year low in August 2015. The GDT has now lost 64% of its value since a record high in February 2014.  The amount paid to farmers – the UK’s farm gate price – has declined sharply since early 2014 to just 23.66p per litre. When it costs farmers around 30p to produce each litre, it’s easy to see why they are annoyed. The major milk processors have to balance their operations across the various markets they sell in and, as a consequence, pay dairy farmers an average price. Farmers will not get, and should not expect to get, the supermarket price for liquid milk. Some supermarkets – Tesco, Marks & Spencer, Sainsbury’s and Waitrose – have agreed direct contracts with dairy farmers that allow them to recover their production costs, but these only involve a small number of farms. Retail supermarket prices for liquid milk are much higher than farm gate, at typically 55-60p per pint (£1.30 or so per litre), but there is no evidence that milk is being used as a “loss leader”. Four pints for 89p at Asda is probably as low as they can get, but the price spread is understandable, appropriate and market-justified; we can’t just hold supermarkets alone responsible. If there is a villain in this piece, it is the world market. With too much supply and not enough demand, farmers have two options. Those near big cities can opt out of the globalised milk market through establishing farmer co-operatives to supply just the local area where they can possibly charge higher prices. Or they can seek high-value, niche markets such as yoghurts, farm-produced ice cream and organic milk. One other way of dealing with supply-demand imbalances would be to bring back dairy quotas, at least at lower levels. The EU introduced quotas in 1984 to control milk production and eradicate butter mountains but they were abolished in April this year.  The problem currently facing the British dairy industry is that it is easy to produce milk in the UK’s green, wet and pleasant land, but it is very difficult to find profitable markets for 14 billion litres of the stuff. Until dairy farmers resolve this overproduction dilemma, many will continue to go out of business.  Uneconomic dairy farms, like uneconomic coal mines, must close down and the adjustment process is harsh and painful for farmers and miners alike. In today’s highly globalised world a more humane outcome is unlikely."
"
Share this...FacebookTwitterThe online German business daily Handelsblatt here writes that European wind energy company Siemens Gamesa will eliminate 6000 jobs.
That means the German-Spanish company will shed more than a fifth of it 26,000 workers. This is the latest bad news slamming the green energy industry in Germany and Europe. Over the years Germany has seen almost every major solar panel and equipment manufacturer become insolvent. Spain too has been hit hard by renewable energy insolvencies.
Once ballyhooed as the sector for the future, German solar and and wind energy industry has taken huge hits. The country’s last remaining major solar manufacturer, Bonn-based Solarworld, earlier this year announced it would file for bankruptcy. Solarworld’s demise was the last of a spectacular series of solar manufacturer bankruptcies that swept across Germany over the past years, with names like Solon, Solar Millenium and Q-Cells going under.
Now the bloodbath is expanding to the wind industry, a branch of green energy that looked far more feasible in Germany than solar energy did.
The announcement by Siemens-Gamesa coincides with the COP 23 Bonn climate conference now taking place, which is calling for more wind and sun energy at a time the industry is collapsing at full speed in Germany.
According to Siemens-Gamesa Board Chairman Markus Tacke: “Our business result is still not at the level where we would like to see it.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Last year Spanish Gamesa and German Siemens combined their wind power operations to form one of the world’s largest producers of wind turbines.
Handelsblatt writes the Siemens daughter company was reacting to “changing market conditions” and that the move will impact 6 countries.
The company also expects to see its revenue for the coming fiscal year fall to 9 billion euros from almost 11 billion.
The Handelsblatt also reports that “no improvement is foreseen in the new fiscal year“.
Other links in English:
– www.expatica.com/de/news/country-news/Germany-layoffs
– http://uk.businessinsider.com/r-siemens-gamesa-to-6000-jobs=T
 
Share this...FacebookTwitter "
"Wouldn’t it be great if scientists could make their minds up? One minute they’re telling us our planet is warming up due to human activity and we run the risk of potentially devastating environmental change.  Next, they’re warning that the Earth is heading for a mini ice age in the next 15 years. The latter headline has its roots in a recent press release from the UK’s National Astronomy Meeting that reported on a study suggesting the sun is heading towards a period of very low output.   Fluctuations in solar activity are not a new discovery. The 11-year variation in the number of dark sunspots on the solar surface was discovered more than 150 years ago. We now understand that these spots are symptoms of increased magnetic activity and occur during periods when explosive outbursts of energy and material such as solar flares and coronal mass ejections are more frequent. The scientists behind the new research have modelled the rhythmic variations in solar activity over recent decades and predict that a deep low is due between 2030 and 2040.  Specifically, the press release suggests that this dip in activity could mark a return to quiet solar conditions not seen for more than 350 years. How is this astronomy story related to an impending ice age? The period of low solar activity in the 17th century, known as the Maunder minimum, lasted about 70 years and roughly coincided with the “Little Ice Age”, a era characterised by an abnormally high number of harsh winters across the UK and Europe.  As almost all newspaper stories have reported, during several particularly cold winters the Thames froze, enabling frost fairs to be held on the ice. Given the apparently strong link between low solar activity and the Little Ice Age reported in the press, it’s understandable that the prospect of a return to Maunder minimum conditions has stimulated a lot of interest.   If this link between variations in solar activity and changes in the Earth’s climate seems obvious, that’s because it is. When the amount of energy emitted by the sun changes, it has an affect on our climate.   But the real issue is just how strong this influence is compared to other factors. The total solar irradiance, a measure of the power produced by the sun in the form of electromagnetic radiation, varies by only about 0.1% over the course of the 11-year solar cycle. Climate scientists have understood this effect for some time and it is already built into the computer models that are used to try and forecast our climate. But there are still some uncertainties. Changes in the ultraviolet portion of the Sun’s output over a solar cycle can be much greater and can deposit energy in the stratosphere – at altitudes above 10km. How this energy influences our weather and climate in the lower atmosphere is still not clear, but there is growing evidence that during periods of low solar activity, atmospheric “blocking” events are more prevalent. These blocking episodes comprise extensive and almost stationary anti-cyclones in the eastern Atlantic that can last for several weeks, hindering the flow of the jet stream and leading to colder winters in the UK and Europe. The good news is that if the sun is heading towards Maunder minimum conditions, the likelihood of which varies greatly in different studies, then a new ice age is not inevitable. During the Little Ice Age, the atmospheric blocking effect probably played a role, but so did increased global volcanic activity that ejected gas and ash in the atmosphere, reflecting solar radiation back into space.   So we have to be careful associating the Maunder minimum with the Little Ice Age. A look at the data shows that the Little Ice Age began a long time (certainly more than a century) before the start of the Maunder minimum – and continued long after it ended. In any case, the Little Ice Age wasn’t really an ice age. Although cold winters in Europe were unusually common, it doesn’t seem to have been a global phenomenon. Research suggests it was a regional phenomenon and that the colder winters in Europe would have been accompanied by warmer ones elsewhere. So what about global climate change?  If solar activity is falling, and that has a cooling influence over the UK and Europe, isn’t that a good thing? Unfortunately not. The overwhelming consensus among the world’s climate scientists is that the influence of solar variability on the climate is dwarfed by the impact of increased levels of carbon dioxide in the atmosphere.  Most calculations suggest that a new “grand solar minimum” in activity would have a cooling effect that would temporarily offset just a few year’s worth of the warming due to the emission of carbon dioxide by humans. We may well be heading towards a period of low solar activity, but a new mini ice age seems very unlikely at this point."
"
Share this...FacebookTwitterDo Supernova Events Cause
Extreme Climate Changes?
“Global warming will not be reduced by reducing man made CO2 emissions”

In recent years, mass die-offs of large animals – like the sudden deaths of 211,000 endangered antelopes within a matter of weeks – have been described as “mysterious” and remain largely unexplained.
Determining the cause of the retreat to ice ages and the abrupt warmings that spawned the interglacial periods has remained controversial for many decades.
William Sokeland, a heat transfer expert and thermal engineer from the University of Florida, has published a paper in the Journal of Earth Science and Engineering that proposes rapid ice melt events and ice age terminations, extreme weather events leading to mass die-offs, and even modern global warming can be traced to (or at least correlate well with) supernova impact events.
The perspectives and conclusions of researchers who claim to have found strong correlations that could explain such wide-ranging geological phenomena as the causes of glacials/interglacials, modern temperatures, and mysterious large animal die-offs should at least be considered…while maintaining a healthy level of skepticism, of course.
Discovery – if that is potentially what is occurring here – is worth a look.

Sokeland, 2017
Scientists generally state that debris from supernova does not impact our planet.  They have no concept that incoming particles from exploding stars are focused by our sun’s gravity and the magnetic fields of the sun and earth.
[M]any harmful effects are possible in the Supernova and Nova Impact Theory, SNIT, including extreme changes of the climate.
Supernova Impacts and Solar Activity, Global Warming Correlation
The scattering of solar energy due to the small particles of supernova debris is also reflected in TSI data as shown in Fig. 3. The timing of impact for supernova debris streams allows the identification of the times and duration time periods for supernova debris streams impacting our planet. Fig. 3 indicates the duration of a single supernova debris stream flowing past our planet is at least 50 years and at times more than 100 years.

Fig. 3 shows an excellent correspondence between sunspot minimums, irradiance depressions, and supernova impact times. The six smaller dips in TSI generated by nova WZ Sagittae in the red portion of the TSI curve of Fig. 3 beginning with the Dalton minimum indicate we have been impacted by six different debris streams from the nova. The last one was in the 1965 to 1970 time region and it is the debris stream of Nova WZ Sagittae that started our current global warming episode near 1966.
Supernova Impacts and Ice Ages, Ice Sheet Melts, and Warm Period Correlations
Incoming supernova debris streams cause warming and melting ice caps that produce increased sea levels.  The increase in sea levels that correlates with supernova impact times is shown in Table 5. 
Termination of the last ice age results due to melting of numerous supernova impacts that correlate time of impact by changing sea level and geothermal energy released for 2,800 years from the exit crater of Dr. J. Kennet’s nano-diamond meteor theory and part of the process involves Dr. O’Keefe’s tektite theory. Correlation of Dr. Frezzotti’s ice melt Antarctica data with supernova impact times over the past 800 years establishes the Global Warming model in conjunction with the November 2016 Antarctic sea ice melt.
Supernova 393 debris impacted earth near 857 AD and started the Medieval Warming Period. When the warm part of the supernova oscillation or cycle stopped and the cooling occurred, the Little Ice Age began near 1250 AD. Supernova 393 also caused the decline of the Mayan Empire near 900 AD. Supernova 393 is proposed to have caused a gamma ray attack upon earth 1,200 years ago.
Two supernovas, G299 and G296.7-0.9, impacted the earth to produce first the Roman warming period shown in Fig. 4 with the normal cooling and then a third unknown supernova created some warming with a lot of cooling dropping temperature to a minimum near 1,100 years ago (900 AD). This cold period produced the Dark Ages. Then SN 393 occurred causing more warming than cooling, but the end result was the Little Ice Age. The Dark Ages and the Little Ice Age were very disastrous periods of time for our planet’s human populations. It should be concluded that the increase in CO2 caused by supernovas 1006 and 1054 that is currently being observed is a boon to mankind and will protect us from the coming cold phase that will be caused by these currently impacting supernovas. 
Consider the Minoan Warming of Fig. 4. The incoming carbon from supernova G29.6+0.1 causes the warm up as shown by the increased Greenland ice core temperatures.
Supernova Impacts and Timings of Megafauna Extinctions and Civilization Collapses
Noted megafauna extinctions in the past 50,000 years are correlated with the times when the debris of supernova explosions impact earth. The time of extinction should be near the time of impact of a supernova debris stream. The time of impact is derived from the time the light of the supernova explosion was seen on earth by adding a correction for the fact that the debris from the explosion moves slower than the speed of light and is shown in the second equation. The severity of the extinction will depend on the distance of the supernova from our planet, the type supernova that indicates the power of the explosion and the surroundings of the supernova when it explodes.  In general, most major disturbances of earth’s biosphere can be attributed to the explosion of supernovas.
Due to the scattering of light for small particles, the sunspots will tend to disappear when a hollow sphere of small particles enters our solar system between the sun and the earth. Other signs of the presence of the small particles are the increase of animal die offs for birds, bees, and fish and a decrease in TSI (total solar irradiance).
Recent outstanding examples of animal and human die offs due to the incoming debris were the Saiga antelope in Asia in May of 2014 and people dying in  India in May of 2015-2016.  These die offs were caused by SN 1006 and would have been called megafauna extinctions if the populations were restricted to small island land areas.  The deaths of the destructive hollow spheres for supernovas 1054 and 1006 will be minimal in the beginning but will increase in intensity as the years of higher particle mass and densities are approached.  Since these supernovas are over 7,000 light-years away from our planet, the effects should not be as severe as the extinctions listed in Table 2 that were due to supernova remnants that were closer to our planet.
Supernova G32.0-4.9 impact time of 4,530 ya corresponds to the fall of Egypt’s fourth dynasty in 2494 BC. It is reported that Ancient Europeans vanished 4,500 years ago. Could supernova debris actually destroy the structure of an empire and change DNA in Europe? An impact time of 4,210 years ago matches the 4.2 Kiloyear Event.
Supernova W50 with an impact time of 17,600 ya and a declination +4 appears to have caused rapid melting of the Patagonian ice sheet 17,500 years ago and corresponds to the last glacial maximum of 18,000 years ago.
Supernova G31.9+0.0 impact time of 8,092 ya produces another correlated woolly mammoth extinction event at Lake Hill on St Paul Island in the Bering Sea 7,600 years ago. The climate change produced by this supernova caused these mammoth to die due to lack of fresh water or drought.
Supernova W51C provides the impact time of 8,130 years ago and this date coincides with the end of the 8.2 Kiloyear Event.
The W50 meteor at 12,800 ya matches the beginning of deglaciation in Antarctica 12,500 years ago. Supernova Vela has a range of impact times shown in Table 1 and Fig. 7 suggests the change of temperature date of 11,700 years ago should be used. Vela’s thermal impact in the northern hemisphere was large because it is the second closest supernova to our planet.
Supernova G82.2+5.3 in Table 3 with an impact time of 5,903 ya produced the 5.9 Kiloyear Event and it is so close in time to the Piora Oscillation that the two different events due to different supernovas are often considered the same event 
The SNIT [Supernova and Nova Impact Theory] Model vs. Climate Models
Any model that claims to know the energy source for global warming must predict the past effects like Antarctic melts in Fig. 3a. Then the model can be successfully used to predict global warming effects in the future. If the proposed model cannot predict past global warming events from previously recorded independent data, the model is useless.
The SNIT model shows unusual and distinct conditions for beginning and ending ice ages. To start an ice age, a close supernova explosion like SN Monogem Ring must produce an extreme amount of iron on earth’s surface. To end an ice age a meteor from a supernova explosion must penetrate earth’s mantle and release geothermal energy over a long period of time to melt the ice.
Applying Occam’s razor, supernova debris impact is the simplest method that explains all these extinction and biosphere disturbance events because the only assumption is all debris streams travel at the same velocity from the remnant to our planet.
What Can We Do?
The debris streams of supernovas 1006 and 1054 have already began to destroy life on earth. When President Obama received a rough draft of this work, he issued an executive order to NASA stating, “Space weather has the potential to simultaneously affect and disrupt health and safety across entire continents”.
Since supernovas 1054 and 1006 are currently incoming, the planet’s average temperatures should continue to increase, global warming. Global warming will not be reduced by reducing man made CO2 emissions and in reality the only defense is to move to a cooler hemisphere, harvest CO2 from the atmosphere, or stop the incoming particles.  
Share this...FacebookTwitter "
"Plastic pollution in the oceans has got a lot of attention lately, seemingly triggered by the BBC’s Blue Planet II and its haunting image of a pilot whale grieving her dead calf. But water pollution isn’t just a problem in the sea – local waters are suffering too, often from pollutants found in common household products.   Even in very low amounts, some medications, hygiene products and pesticides may cause aquatic organisms to change their behaviour or find their homes are no longer habitable. The issue has been on the authorities’ radar for some time, not least the European Union which introduced the first watchlist of emerging pollutants in 2013. But you can help too, with a few simple changes to your everyday habits. Everyday tasks such as washing your hands or brushing your teeth could mean you are unwittingly polluting a river. Hand gel or toothpaste may contain anti-bacterial agents such as triclosan, which mimics the hormone estrogen in animals and can inhibit 
their reproductive systems and ability to swim.  Once in the water, triclosan sticks to the soil on the riverbed where it will be consumed by all the tiny creatures that call the river home. It then accumulates as it is passed on through the food chain, meaning larger predators are worst affected.  Due to these concerns, some companies have began removing triclosan from their products, but in the meantime you can get ahead of the game and chose not to use products that contain it. Metaldehyde is the active ingredient found in many slug pellets. It is of course toxic – that’s why it’s used to kill slugs. But the pellets are washed into drains and ditches, and from there they wind their way into river systems, affecting animals much larger than slugs or snails. And just like triclosan, metaldehyde is passed on up the food chain to predators like hedgehogs or birds.  Metaldehyde is a particular problem when it gets into waterways. In your garden it usually breaks down within a few days. But when it enters the water system, the chemical is much more stable which slows down the degradation, allowing it to hang around in the environment, where it and can then be consumed by aquatic life.  Alternatives to metaldehyde include copper strips, said to deter slugs, or parasitic nematode worms which naturally kill slugs and snails. Or you could simply encourage predators such as hedgehogs and frogs into your garden.  Microplastics are small fragments of plastics less than 5mm in size. There has been a lot of talk about the problems caused when these fragments are ingested by fish or other aquatic animals, but less attention has been paid to the plastics leaching toxic materials as they break down into their original components. These toxic products have been linked to neurological, fertility and immune health problems. Large items such as plastic packaging or tyres are often the source, as they break down into smaller and smaller fragments over time until they are tiny bits of microplastics floating through the water.  But synthetic clothing is another significant source. Every time you wash synthetic fibres, small parts of the material will fragment away and wash into the water system. So the next time you go to wash the fleece that you only wore for ten minutes a couple of weekends ago, think: is this really necessary? Pharmaceutical products are another cause for concern as even in very small quantities they may be considered toxic. Medicines and drugs such as painkillers, antidepressants and contraceptives all end up in waterways as they pass through the human body unaffected and are flushed down the toilet. These drugs can affect the natural reproductive cycle, behaviour and growth of many fish species.  We’re not suggesting people should stop taking their medication. But you can help by making sure old pills are disposed of properly. Many people flush them down the loo, or chuck them in the bin when really they should be returned to a pharmacy where they can be properly disposed of.  It’s too easy to associate river pollution with large factories and heavy industry but we too play a part by just going about our everyday lives. Such simple small changes could make a real difference to the water quality of your local river and it’s so easy to do."
"
Share this...FacebookTwitterUsing a comparison, Japanese skeptic blogger Kirye at KiryeNet drives home how “the real Arctic sea ice volume is much higher than in 2008.”

Source of images: DMI: http://ocean.dmi.dk
Using images and data from the Danish Meteorological Institute (DMI), Kirye put together and posted a comparator showing the immense March/early April sea ice volume increase the Arctic has seen since 2008. It totally defies the panicky claims of a “melting” Arctic, she tweeted.
You can see the animation comparator Kirye put together in action here at Twitter.

Arctic sea ice volume surges a whopping 3000 cubic kilometers since March 1st. Chart: DMI.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Kirye comments that although we have not once seen alarmists’ climate predictions come true, they continue to threaten us with sea ice doom.
Amid rapidly growing Arctic sea ice volume, they continue to cling to the claim it’s melting. That’s irrational.
Media hyperbole
Yesterday Anthony Watts posted here on the Arctic, remarking that the media claims of earlier this year of an unprecedented Arctic warmth had much more to do with hyperbole than with reality. Lately the Arctic has been a generous source of fake news from the global mainstream media giants, all claiming something that is not real, or making something that’s happened many times before look “unprecedented”.
Warm 12°C temperature spikes more than 70 times!
Back in January, 2016, I wrote here how “the Washington Post screamed bloody murder that the North Pole was in meltdown as temperatures at that singular location rose some “50 degrees above normal”, making it sound like this event had been an unprecedented phenomenon.
For that post I had gone back and examined DMI data Arctic temperatures above 80°N latitude going back some 58 years. Here’s what I found:
And examining all the years since 1958 we see that a temperature spike of some 12°K or more in a matter of a few days (during the November to March deep winter period) occurred more than 70 times! And over 100 times for spikes of 10°K and more.”
Once again, hat-tip: KiyreNet.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOver the recent days we’ve been hearing about record snowfall in Montana, record low temperatures in Minnesota and Ontario, New York City “blowing away” a 103-year old record, vicious cold gripping  Lebanon, PA. Moreover, Arctic sea ice and Greenland ice have surprised climatologists with a comeback over the past year.
UPDATE: And now the UK braces for 3 weeks of unusual cold.
What is more, the NOAA has just officially announced the return of the La Niña, after earlier this year an El Niño had been forecast instead.
“Gangbusters cold” in the works
And lately we’ve been hearing a number of meteorologists warning of a harsh coming winter for both Europe and USA.
At his Friday Daily Update, veteran meteorologist Joe Bastardi warns of a “fierce cold” being on the table for this winter for the United States. Bastardi says he does not believe the US climate models at all, and instead could even be as harsh of the notoriously cold 2013 winter.

Bastardi believes the month of December will in fact turn out to be the opposite of what was projected by the US weather models, which foresaw a warm December. See his latest Saturday Summary at Weatherbell.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In his Friday Daily Update Bastardi thinks the cold will be “gangbusters” and that there is a real chance for a “December to remember”. All in all, the Pennsylvania State University meteorologist sees an excellent ski season in the works for the USA.
-40°C for Christmas In Central Europe?
Over in Europe, there have been a growing number of warnings of a “winter of the century“. For example the German nachrichten.de here reports Christmas temperatures of -40°C!
Naturally such forecasts need to be viewed with great skepticism as they are highly speculative. Long-term prediction methods may indicate which way a winter is tending, but I wouldn’t believe any forecast 6 to 8 weeks out.
Nachrichten.de cites information from the Augsburg Meteorological Institute and the Hamburg Weather Warning station. The Federal Office for Weather Observations advises citizens “to prepare for an ice cold Christmas.”“Snowmageddon”?
The express.co.uk warns of a “SNOWMAGEDDON”, thanks to a La Niña bringing a “Big Freeze”. The express.so.uk  writes that “winter weather 2017 is set to be the harshest for years“.
For the winter of 2017/2018 the PDO will bring a winter of deadly blizzards and killer freezes as a perfect storm of catastrophic weather systems gather. Analytical meteorologist Tyler Sotock suggested America was facing Snowmaggedon. And spokesman for another YouTube weather channel Hurricane and Winter Tracker warned of chaos.”
While wild speculation swirls, one thing seems certain: just as Joe Bastardi shows at his Saturday Summary, the US forecasts of a warm November/December period across Canada and the US East are looking more flawed than ever.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterQuartz.com here presents an interesting chart which tells us the green energy revolution of the past 30 years has resulted in practically nothing. It’s been a flop. Fossil fuels remain as wildly popular as ever.

Global fossil fuel use as a share of total energy has risen since James Hansen’s 1988 testimony. Chart: Quartz.com.
In the 1970s the big worry was that fossil fuels would soon run out, and so we should “use them wisely”. But in the 1980s the risk changed to one of an overheating planet, and so we should not use them at all.
Higher than 1988, when James Hansen testified
We can all recall a sweating James Hansen’s 1988 stage-crafted testimony before Congress, warning that increasing atmospheric CO2 concentrations would lead to spiraling global warming. And unless action was taken urgently, the ice caps would soon melt and the earth would sizzle.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Countries as a result mobilized 100s of billions of dollars to eliminate the use of these “dangerous, climate-killing” fossil fuels.
Today for all that money you’d think that tremendous progress in reducing fossil fuels would be the result. You couldn’t be more wrong.
The sad truth is that after hundreds of billions spent, and trillions committed, fossil fuels’ share of total energy consumption globally has in fact risen since Hansen’s doomsday testimony, see the figure above!
Bringing the dead back to life
What may be a surprise to many is that whenever we burn fossil fuels, which originate primarily from ancient plants that died and were naturally sequestered in the earth as “fossils” eons ago, we are in fact taking this once live carbon and recirculating it back into the current, living ecosystem. The result: More carbon-based life is getting produced today. The planet is greening. Now the earth is teeming with more life than it has seen in millions of years. That’s fundamental science.
So if you want the system to have more life, just add carbon to it. One way is to add old carbon (by burning fossil fuels) that’s been locked away in the ground.
On the other hand if you want to limit life, then remove the carbon from the eco-system. Funny how the alarmists claim to be worried about life being under threat on earth, yet are striving to remove its very source.
Share this...FacebookTwitter "
"Russell Crowe and Cate Blanchett have used their Golden Globes speeches to highlight Australia’s bushfire crisis, and the link between climate change and worsening bushfires. The unprecedented fire season has so far burned through 8.4 million hectares across Australia, destroyed thousands of homes and killed 23 people. On Sunday night, Australian actors thanked volunteer firefighters and drew attention to the “climate disaster” that has made the country’s fire season longer and more intense. Crowe won best actor in a limited series or motion picture made for TV for his performance as Roger Ailes in The Loudest Voice. But the Australian actor, whose home in Nana Glen in New South Wales was hit by fires in November, did not attend the ceremony because he was with his family preparing for the latest bushfires. Actor Jennifer Aniston, who was presenting the award, read out Crowe’s statement on his behalf. “Make no mistake the tragedy unfolding in Australia is climate-change based,” Crowe said. “We need to act based on science, move our global workforce to renewable energy and respect our planet for the unique and amazing place it is.” Blanchett, who was presenting an award, used her speech to give thanks to volunteer firefighters, and draw attention to the global “climate disaster”. “There are a lot of Australians in the room tonight,” she said. “I know we are all very grateful for the call-outs to our fellow compatriots who are suffering under the bushfires, so thank you. “I wanted to do a special call-out to the volunteer firefighters who have been at the centre of battling the climate disaster that is facing Australia. “And of course, when one country faces a climate disaster, we all face a climate disaster, so we’re in it together, so thank you very much.” ""I want to do a special callout to the volunteer firefighters who have been at the centre of battling the climate disaster in Australia. And of course when one country faces a climate disaster we all face a climate disaster. We're in it together."" #GoldenGlobes pic.twitter.com/uiumjV4XpT Other actors and presenters, including Patricia Arquette, Phoebe Waller-Bridge and Ellen DeGeneres, also spoke about the Australian bushfires, and urged viewers to donate. Arquette won best supporting actress in a series, limited series or TV movie for her role in Hulu series The Act. “I am so happy to be here and celebrate this, but also I know that … we see a country on the brink of war, the United States of America … and the continent of Australia on fire,” she said. “While I love my kids so much, I beg of us all to give them a better world. For our kids and their kids, we have to vote in 2020 and beg and plead for everyone we know to vote in 2020.” Before the ceremony, fellow Australians Margot Robbie and Nicole Kidman also used social media to encourage people to donate to Australian firefighters.  @redcrossau @wireswildliferescue @salvosau @nswrfs @cfavic A post shared by @ margotrobbie on Jan 5, 2020 at 1:08pm PST  Our family’s support, thoughts and prayers are with everyone affected by the fires all over Australia. We are donating $500,000 to the Rural Fire Services who are all doing and giving so much right now. A post shared by  Nicole Kidman (@nicolekidman) on Jan 4, 2020 at 3:10pm PST"
nan
"The New South Wales division of the Young Liberals’ push for a change in the government’s climate policy was spurred on by a membership which understands the risk it faces if no action is taken, its president has said. The NSW branch passed a motion at the Young Liberal council in early December calling for a “practical, market-based means” for Australia to cut emissions by 30% of Kyoto levels by 2030, and provide energy market certainty.  The motion puts them at odds with the Morrison government, which has stubbornly insisted no change is needed to Australia’s emissions reduction strategy, even as the nation struggles through an unprecedented and deadly bushfire crisis, but is in line with the views of the NSW environment minister, Matt Kean, who has demanded more action. The NSW Young Liberal president, Chaneg Torres, said his branch was just doing its job. “The Young Liberals exist within the party to reflect the views of young people, to our MPs, to our wider party, and I think from our point of view, climate change is a particularly important issue for our generation because it concerns the actions of today affecting the lives and quality of life of my generation, but also generations to come,” he said. “And we certainly understand that it is a risk that we will particularly face, my generation, if nothing is done in the present.” Torres did not go so far as to criticise the Morrison government, which he said had its “heart in the right place”. “And the prime minister has said very clearly that his government and the Liberal party generally accepts the reality of climate change and accepts the need for action,” he said. “But this is an area of policy that has been fraught for many years. I think that, in many ways, the policy well has been poisoned and it has been a very divisive issue in our community. “And I think that in many ways the government has to deal with that fact. “From our point of view, as a young liberal movement, we are reflecting the views of our generation and providing encouragement to the government to take the issue seriously and do what they can to ensure they is a stable and certain policy environment where investment can be made in renewable energy.” Part of the plan endorsed by the division includes taxation write-offs for green assets, a nationwide solar scheme, and repealing the state and federal ban on nuclear energy so it can be considered for baseload power, as well as increased investment in hydro. The Queensland Young LNP also passed motions calling for nuclear energy to be explored as part of a push to lower emissions at its last conference, but did not go as far as the NSW branch in calling for an emissions reduction blueprint. The NSW branch will send a brief to its state and federal MPs, which are in power at both levels. But Torres said he understood the difficulties, with MPs such as Craig Kelly – an influential backbencher in the party room – still denying the need for any action. “Well, as you know, the Liberal party is a very broad church, but I would say that the youth wing of the party, the vast majority of us believe in the need to take action on climate change and actually I’d say the majority of our membership generally in the Liberal party believe that things need to be done to address climate change,” he said. “I think the vast majority of Liberals would agree with what the prime minister said that we don’t need to choose between a strong economy and action on climate change. “Now we can have a debate about what particular measures we need to take, but I think that the vast majority would agree that climate change is real and the action needs to be taken.” The NSW division, like the state environment minister, Matt Kean, who made similar comments, has received some pushback from within the party and its supporters for its stance, which Torres would not comment on. But it did receive support from other Young Liberals who said things needed to change. “NSW are being proactive about what real Young Liberals stand for and the other states need to grow a spine,” one senior member said. “Federally, the Young Libs have a policy vacuum and we haven’t got anything to move away from the previous president’s controversial positions on the ABC, gay marriage and Muslim immigration.”"
"Rabbi Dr Barbara Borts calls Paul the Apostle “a troublesome character” (Letters, 28 December). He was a man of his time, but before you write him off read chapter 13 of his first letter to Corinthians. He says, among other things: “Love is patient; love is kind; love is not envious or boastful or arrogant or rude. It does not insist on its own way; it is not irritable or resentful; it does not rejoice in wrongdoing, but rejoices in the truth. It bears all things, hopes all things, endures all things … Pursue love.”Val SpougeBraintree, Essex • Your editorial (3 January) rightly lambasts the Australian government for its attitude to climate change but on the back page of the same edition you print a full-page advert for high-carbon budget airfares under the banner “The world has never been closer”. Substitute “closer” with “hotter” and we might have the truth of it. Tutting on the sidelines is no longer good enough. The Guardian is in the game too and needs to up it.Richard HawksworthHorwich, Bolton  • Cherry Weston (Letters, 27 December) gets to the heart of Labour’s communications crisis. The time-tested answer is few syllables and even fewer words: Keep it simple, stupid.Val SeddonYork • I don’t know whether flowering daffodils in Stoke Gabriel is a record (Letters, 3 January), but I think that hot cross buns on sale on 26 December in Epsom may be.Jane GarrattEpsom, Surrey • I haven’t seen any daffodils yet but my climbing rose has been flowering since the end of December and I live in Yorkshire.Pam WellsAddingham, West Yorkshire • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"
Share this...FacebookTwitterA few days ago I posted on how brutal cold and snow were gripping the northern hemisphere from every direction. Prominently featured was the western Pacific country of Japan.
“Amazingly cold”
Not only had it been cold and brutal in January, but the story was the same already back in mid December as well, as Japanese skeptic blogger Kirye here tweeted:

Image cropped from Twitter
On December 10th, the Japanese blogger tweeted here:
This year early December in Japan amazingly cold throughout the country. In particular, the daily mean temperature in Sapporo city has been well below 1981-2010 average.”
Coldest October in 46 years
Already back in October Kirye had been tweeting of Japan getting started on the very cold side as she noted how Tokyo had seen its coldest October in almost 50 years:
October 2017 in Tokyo the mean monthly temperature was 16.8℃, the coldest October since 1971.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In fact the cold in Japan started even well before October, 2017. Already in September Japan had had a colder than normal month when Kirye informed me that “Japan’s temperature anomaly for September, 2017, was -0.22C,” and then added she expected the coming winter to be “very interesting”.
And interesting it’s been.
Unusual cold and snow persisting in February 
And now that we find ourselves well into February, there are still no signs of the brutal winter conditions letting up in Japan, let alone of the famous cherry blossoms making their debut any time soon.
According to the English language Asahi Shimbun here on February 8, seven people had been killed and public roads and services had been crippled “as record snowfall kept traffic at a standstill across much of the Hokuriku region of northwestern Japan.”
The Asahi Shimbun added:
Authorities said that 1,400 or vehicles got stuck from Feb. 6, creating a line that stretched about 20 kilometers.”
At her blogsite, Kirye reports here that so far this year 59 stations (out of 928 stations) in Japan marked an all-time temperature low in 2018. In Ohtake in Hokkaido Prefecture, for example, the mercury plummeted to – 24.9°C.
The Mainichi here reported of “cold air” and “heavy snow” along the Sea of Japan coast and added: “The city of Fukui in the region had over 130 centimeters of snow for the first time in 37 years.”
Share this...FacebookTwitter "
"What if there were a way to suck carbon dioxide right out of the air and turn it into useful products? It might seem fantastic but scientists have actually proved it’s possible. One of the challenges with making it a viable process, however, is manufacturing products that are valuable enough to cover the high costs of extracting the carbon dioxide. Some excellent new research has raised the possibility of a breakthrough in this area by using CO2 directly captured from the air to produce a type of graphene, the two-dimensional form of carbon often described as a “wonder material”. But reported claims that this amounts to producing “diamonds from the sky” are somewhat misleading. There is already a significant market for CO2 and products made from it, most obviously fertiliser and fuels. This process of treating the gas as a feedstock rather than a waste product is known as carbon dioxide utilisation (CDU) and usually starts by capturing CO2 from industrial flue gases – exhaust from furnaces or fuel-powered generators. In 2012, carbon dioxide utilisation accounted for 180 megatonnes of CO2 that would otherwise have gone into the atmosphere, and this has been forecast to rise to 256 megatonnes in 2016. But total global greenhouse gas emissions are around 35 gigatonnes and rising, meaning other emission-reduction strategies such as energy efficiency and renewable power currently play a much larger role. Sucking CO2 directly from the air can be a trickier process compared to more concentrated sources such as flue gases. As CO2 represents just 0.04% of the atmosphere, you have to treat very large amounts of air just to produce even modest quantities of the gas. However, several companies have managed to design chemical processes that are more efficient than those used in flue gas capture and produce enough industrial CO2 to be economically viable. The new research from George Washington University in the US demonstrates a way to directly capture CO2 from the air and turn it into carbon nanofibres, powered just by a few volts of solar electricity and solar heat. These smaller-than-microscopic structures are effectively irregular cylinders made from layers of graphene and can be used to strengthen materials for building aircraft, wind turbines and even sports equipment. The researchers claim their new manufacturing method is much cheaper than existing techniques and so could open up new uses for the nanofibres. If they can scale the process up to the level of mass production and keep it cost-effective, this would really help grow the market. But a robust life-cycle analysis is needed to substantiate these claims. If the technique does prove to be a cheap way of mass-producing carbon nanofibres, could it become so widely used that it significantly reduces atmospheric CO2 levels, a claim attributed to the authors (but not in the peer-reviewed paper)? Current production of carbon nanofibres is around 500 tonnes a year and predicted to increase to around 10,000 tonnes a year. But this is still far from the hundreds of millions of tonnes that would be needed to make a meaningful contribution to greenhouse gas levels. It is difficult to see at this time how a market for carbon nanofibres could develop to these levels. One exception might be if the price of nanofibres became so low we could start using it to cost-effectively strengthen building materials. This would also provide an interesting addition to current techniques that turn CO2 and other chemicals and waste products into stable solids. Developing viable direct air capture techniques is a major research challenge that will hopefully one day have far-reaching impact. It could help make CO2 a resource that is available anywhere in the world where a capture unit can be installed. Carbon nanofibres will play a part in the portfolio of technologies that make up carbon dioxide utilisation, again an area that is growing in application and promise. Sadly it’s most unlikely this interesting breakthrough will be scaled up to limit climate change in the way that has been claimed. We believe that due to the size of the potential market, carbon nanofibres alone will not be able to make a significant impact on CO2 mitigation. However, many transformative technologies and materials start with a niche area before moving into more mainstream uses. And perhaps this could be the case for direct air capture nanofibres. After all, change has to start somewhere."
"Dozens of people have been killed, and with many more missing, after Volcán de Fuego (Fuego) in Guatemala erupted on June 3 2018.   In recent years, Fuego has regularly ejected small gas and ash eruptions, which hold little risk to surrounding populations. But Fuego also has a reputation for producing larger explosive eruptions. These larger eruptions have two main primary hazards – falling ash and bombs (collectively known as tephra), and pyroclastic flows. Of these two, pyroclastic flows are the big killers, and are responsible for the deaths from the  latest eruption. So, just what are these flows and why are they such killers? And what can people do to avoid them? Footage taken from a road bridge over a dry valley from on June 3 at Fuego shows what appears to be a soft and billowing ash cloud gently flowing down the volcano. It looks innocuous. Spectators and officials watch mesmerised, but then the cloud moves into the valley and heads directly towards the bridge. Unease spreads and soon alarms sound before people rush away just in time. Footage shows the ash cloud quickly pass under and then over the bridge. These spectators escaped death by seconds, as this benign-looking ash cloud is the notorious killer that is a pyroclastic flow. Pyroclastic flows (also known as pyroclastic density currents) contain a hellish combination of hot rock fragments (pyroclasts), superheated air, and volcanic gases. You can expect temperatures of 100-600°C and they can travel fast – very fast on steep slopes. Speeds generally range between 70-200mph, but they have been recorded reaching 450mph. As they are heavier (denser) than air, they tend to be funnelled into valleys. But their higher density also gives them momentum, so they can travel up the sides of valleys, and even over mountains. The worst place to be when a pyroclastic flow is on the move is in a valley, which is why the spectators at the road bridge were lucky to escape.  What surprised many volcanologists (including us) is that people were actually standing and taking pictures while watching this billowing cloud descend. It is certainly a hypnotic and beautiful phenomenon to observe, but any volcanic cloud moving even vaguely in your direction is a clear sign to flee. This suggests that further education of people living in and around Fuego of its volcanic hazards would not only be helpful, it would save lives. It is rare that eruptions from Fuego produce such large pyroclastic flows that travel so far. This leaves the authorities in an impossible situation. Because if you create exclusion zones based on worst-case scenarios, then decades if not centuries may pass without a worst-case eruption. And all that time people will be grumbling about good and fertile land being inaccessible without good reason. One notorious example of a pyroclastic flow happening elsewhere was the eruption of Mount Pelée on the island of Martinique on May 8 1902. Pyroclastic flows destroyed the town of Saint-Pierre and killed an estimated 30,000 people. Only a handful survived, one of whom was a prisoner in a jail cell. This was the largest loss of life from a pyroclastic flows in the past two centuries. One of the most famous historical examples of the devastation and loss of life caused by pyroclastic flows is what happened at Pompeii and Herculaneum when Vesuvius erupted in 79AD. An important lesson from this eruption is the fickleness of human memory. Because Vesuvius had been dormant for at least 700 years, it wasn’t recognised as a potential threat.  Volcanologists know from their studies that the frequency of large eruptions at a specific volcano may be one every few centuries or every few thousand years. But on a human time scale these numbers lose impact because there may be no aged relatives around who remember past eruptions, and so a complacent sense of “all is well” pervades.  Many communities living around volcanoes have other more immediate concerns, including other natural hazards. It’s an unresolved paradox. The eruption of Vesuvius produced a number of pyroclastic flows which led to the deaths of at least 1,400 people, and the burial of the settlements by volcanic material. Recent excavations revealed evidence of a new type of death from this eruption – a person being struck by a large block, possibly carried by one of the pyroclastic flows.  It is too early to tell what will happen next with Fuego. But given its recent history, this eruption is an unusually large and extreme event. Fortunately, these tend to be infrequent. It is sincerely hoped that there will be no sudden repeat of the large and far-travelled pyroclastic flows that took so many lives. Whatever happens, there will be a huge amount of work to do in rebuilding communities and working through collective grief."
"The fire situation in eastern Australia continues to rapidly escalate. At this stage we cannot predict when this will come to an end, but with losses of lives and property mounting on the south coast of NSW, eastern Victoria, South Australia, southwestern WA and Tasmania, we now have a nationally significant catastrophe that affects city and country alike. The magnitude of these fires alone (about 5 million hectares and rapidly rising), apart from their human and environmental consequences, simply shows us that we now confront a new, more flammable world: a coupling of people, ecosystems and fire that is now irrevocably transformed. As a society we should admit that our current policy, operational, knowledge-gathering and research capacity is inadequate to deal with such a new, fiery world. How do Australians in our most populated regions live in a future defined by heat, drought, powerful wind and lightning storms that lead to inevitable uncontrolled fires? The answers must lie in the intersection of the resilience of human communities, our experience and love of place, with the natural environment. Yet understanding this intersection demands resolving the various contribution of climate change, land management and community preparation and resilience. How many lives, properties, threatened species, ecosystems and their services, did our current management and response capacity actually save? What was the return on human and financial investment in fire preparation and emergency? Are existing administrative arrangements in firefighting and emergency management appropriate? What is the right balance between community/individual responsibility vs. centralised command and control? What is the role and sustainable capacity of volunteer fire management? What can Indigenous fire knowledge bring to bear in stem these blazes? How can biodiversity and ecosystem services, like water and carbon storage, be protected? Comprehensive answers to these questions are not simple to acquire, because of the interlocking nature of the process involved. But to effectively adapt to the challenges the future compels us to deal with them on a scale never attempted before. The temptation to make sense of this unprecedented crisis, and identify a way forward, is to repeat the same model of inquiry that has played out over the last century. Every major bushfire season in Australia has been followed by inquiries and their recommendations, that have led, haltingly, to improved capacity to fight and co-exist with fire threats. However there are a plethora of recommendations that have failed to gain traction in the complex administrative and political ecosystems that reinforce the status quo. For instance, while the Stretton Royal Commission, that followed the 1939 fires in Victoria, established the foundations of modern, organised fire management capacity, its successor, the 2009 Victorian Bushfires Royal Commission has had a lesser impact. Critical recommendations concerning planning, land use and management lie discarded after failure of implementation. As with most inquiries, the feasibility, cost-effectiveness and capacity of governments and citizens to implement its recommendations were beyond both the scope, resources and timeframe of the last Royal Commission. The legacy of a State-centric approach to land, fire and emergency management, so often viewed as a strength by many abroad is now a conspicuous weakness when it comes to the challenges of a world transformed. In sum, all the old bets are off. While laudable initiatives such as the post season summit proposed by ex-fire chiefs or politicians have been proposed, these will be inadequate to derive viable solutions until basic questions about the bushfire crisis are thoroughly explored and answered. The scope and scale of the ongoing national catastrophe requires a significant and non-partisan investments in national capacity to research, investigate, understand and innovate to meet the challenges ahead. There are no short-term fixes or answers to be had in response to the challenge. Setting out to protecting old orthodoxies, and thereby eschewing more innovative solutions, will lead us down a well-trodden path that is incapable of comprehending and adapting to our new circumstances. There needs to be robust and evidence-based debate, encouragement for trying new approaches, and fostering diversity of opinion, outlook and experience. There is an urgent need to develop a nationally co-ordinated, but not centrally controlled, approach to resolving the key questions posed above. This initiative should fully harness the intellectual capacity of our management, research and training institutions, focusing their immense technological capacity for analysis of the fire, human, climate and environment nexus. Without such an approach unprecedented amounts of information yielded by the events of 2019/20 will evaporate, the hard lessons will be skipped, and the vulnerability to another fire crisis will remain. Simply stated, as a nation we are being transformed by drought, heat and fire, to adapt Australians must transform our understanding of these fundamentals, in order to plan, cope and live in a more flammable world. Professor Ross Bradstock is the director at the Centre for Environmental Risk Management of Bushfires at the University of Wollongong. David Bowman is the professor of pyrogeography and fire science, and the director of the Fire Centre Research Hub at the University of Tasmania "
"In a year when the all-important UN climate change summit  will take place in Paris and the UN’s sustainable development goals will be finalised, one would have thought the government of one of the world’s most powerful nations might seize the moment to put forward progressive environmental policies. Unfortunately the summer budget introduced by UK chancellor, George Osborne,  has failed to do just that. Nothing announced this week will put the UK into a leadership position at the negotiations later this year – and it becomes increasingly clear things will not change for the next five years of Conservative government. It’s hard to remember now but the beginning of David Cameron’s Tory leadership was marked by an increasing sense of urgency over environmental issues. He visited Arctic glaciers to see the effects of climate change – and famously hugged the local huskies. On taking office he pronounced the coalition would be “the greenest government ever”. But the new, fully Conservative, government has signalled a rollback of green policies. In his summer budget, Osborne promised continued tax breaks and subsidies for North Sea oil and gas – which, understandably, delights the industry. This comes on top of vast existing direct or hidden subsidies to the UK fossil fuel industry which in 2012/13 amounted to almost £2 billion, according to a Friends of the Earth study. Osborne’s budget continued the onslaught on renewable energy, as he announced the removal of the climate change levy (CCL) exemption for renewables, which might cost the green energy industry up to £1 billion by 2020/21. This comes after the Queen’s Speech announcement to end subsidies for onshore wind projects. Not to forget the chancellor’s new commitment to road building, financed by a new system of green car taxes. New roads will not help reduce the significant share  of the UK’s private transport system in carbon emissions and will increase the pressure on UK cities’ air quality, which is among the worst in Europe. The government has also ditched its pledge to ensure all new homes were zero carbon. In a year when climate change should be on top of the political agenda these policy U-turns are, at best, counter-productive. At worst, making fossil fuels more competitive is nothing but reckless and extremely shortsighted. The chancellor is clearly responding to conservative forces in his party who have called for the removal of green subsidies for a long time. But he’s also listening hard to the needs of oil and gas majors such as Shell and BP which have far more access to Whitehall  than the renewable energy industry. It is not that George Osborne and David Cameron are climate change deniers. Far from it. Along with the big fossil fuel companies – even ExxonMobil – they know about the risks climate change poses to economies and the entire planet. But what the big oil and gas majors are extremely good at is lobbying to keep the existing fossil fuel-driven status quo in place for as long as possible. Shell plans to still expand fossil fuel production until at least 2050 . The G7 doesn’t want to phase out fossil fuels until 2100 . Every sensible climate scientist knows that these are unsustainable projections, posing a real risk to the planet’s biosphere. Yet, at the same time, most major fossil fuel companies have already factored in some form of carbon tax, as ExxonMobil recently admitted. On the one hand it seems oil and gas majors want to prolong the good times (and big profits) they’ve enjoyed. On the other hand, they know perfectly well that this cannot go on forever (and have taken measures accordingly). What’s missing is political leadership. While – back in 2010 – Cameron made political hay out of his commitment to “green up” his government, the first Conservative budget for almost 20 years signals a return to the unsustainable ways of the past. The UK’s environmental and other progressive forces need to unite to pressure the government to honour its legal climate change commitments and follow the Pope’s moral leadership in tackling the big environmental and social issues of our time."
"
Share this...FacebookTwitterA new research institute in Switzerland set to rock the climate science boat…will investigate natural causes of climate change. Director calls claims CO2 the main driver and a pollutant “absurd”.

Swiss institute director and climate scientist Hans-Joachim Dammschneider says natural factors in large part behind recent climate change. Photo credit: IFHGK
The Swiss Basler Zeitung (BZ) reported on April 13, 2018, that a new research institute opened at Lake Aegeri in Switzerland last year: the Institute for Hydrography, Geo-ecology and Climate Sciences (IFHGK), which will focus on the natural causes of climate change.
Contrary to the other government-funded institutes, the IFHGK focusses on the natural causes of climate change: the Institute for Geo-ecology and Climate Sciences wishes to show that CO2 is not necessarily the main driver behind global warming and thus goes against the alleged broad consensus among mainstream researchers, the Baseler Zeitung writes.
A real climate scientist
The new institute, founded at the start of 2017, is located in Oberägeri, Switzerland is directed by Hans-Joachim Dammschneider. who according to the BZ explained:
Unlike many others who speak on the subject of global warming, I’m actually a climate scientist.”
The institute consists of scientists who work on a volunteer basis and operates on a shoestring. Decisive in the founding of the institute was Dr. Hans-Joachim Dammschneider’s encounter with Dr. Sebastian Lüning, who together with Prof. Fritz Vahrenholt wrote the Spiegel bestseller “Die kalte Sonne“,  which upset German mainstream climate science. Lüning also runs the Die kalte Sonne climate site, where he posts daily.
Looking at climate science with “calm, common sense and reason”
Dr. Lüning, a geologist, long ago concluded that the mainstream climate scientists have navigated themselves into a dead end. Dammschneider told the BZ that the institute will look at the issues with “calm, common sense and understanding.”
Dammschneider, who considers himself a climate realist, says that it is absurd that CO2 has been designated a pollutant and that the substance is mainly to blame for es climate change. Dr. Dammschneider is a leading German expert in the field of geography, climate research, oceanography and geology.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The BZ reports that already Dammschneider has published some papers in their own publication series and that he specializes in the field of periodic temperatures changes of the oceans, which have a direct impact on the atmosphere. He told the BZ:
The atmospheric temperatures tend to correspond with the oscillations of the oceans and are subject to a comparable pattern.”
Today’s warmth “not unique”
The German-born researcher believes it is essential to record these changes and to see if the climate changes are normal, or if they only have existed since man started burning fossil fuels.
His research and the findings of Sebastian Lüning for the North African region show that during the period of the year 1000 to 1200 A.D. it was similarly as warm as it is today. The BZ writes:
The works of Lüning and his team show that during this period very optimal climate conditions predominated. They also indicate that today’s warm period is not unique.”
The fear to dissent
On the future success of the institute, the BZ writes that Dammschnei­der is aware that it’s going to be a long and difficult road, saying that “young climate scientists as a rule cannot afford to question asserted truths if they do not want to endanger their careers. Thus the new climate science skeptic institute will have to rely on support from independent scientists and retired professors who are free to speak without the fear of harsh consequences.
Funding needed
The BZ writes that the institute is working to gain public attention, but is in need of funding. However: “business sponsors look promising, and so it hopes to employ some workers,” the BZ reports.
Concerning the widespread alarmism over man-made climate change, Dammschneider told the BZ:
Sooner or later they will have to soften the positions they’ve held so far.”
Read the entire story in German at the Basler Zeitung
Share this...FacebookTwitter "
"The targets set in the Paris Agreement on climate change are ambitious but necessary. Failure to meet them will lead to widespread drought, disease and desperation in some of the world’s poorest regions. Under such conditions mass migration by stranded climate refugees is almost inevitable. Yet if richer nations are to be serious in their commitment to the Paris target, then they must begin to account for the carbon emissions contained within products they import. Heavy industry and the constant demand for consumer goods are key contributors to climate change. In fact, 30% of global greenhouse gas emissions are produced through the process of converting metal ores and fossil fuels into the cars, washing machines and electronic devices that help prop up the economy and make life a little more comfortable. As one might expect, the wealthier parts of the world with their higher purchasing power do more than their fair share of consuming and polluting. For every item bought or sold there is a rise in GDP, and with each 1% increase in GDP there is a corresponding 0.5 to 0.7% rise in carbon emissions. The growing demand for day-to-day conveniences exacerbates this problem. For metal ores alone, the extraction rate more than doubled between 1980 and 2008, and it shows no sign of slowing. Every time you buy a new car, for instance, you effectively mine 3-7g of “platinum group metals” to coat the catalytic converter. The six elements in the platinum group have the greatest environmental impact of all metals, and producing just one kilo requires the emission of thousands of kilos of CO₂.  That car also consumes one tonne of steel and you can add to that some aluminium, a whole host of plastics and, in the case of electric cars, rare earth elements.  Often, no one is held accountable for the carbon emissions connected to these materials, because they are produced in countries where “dirty” industry is still politically acceptable or seen as the only way to escape poverty. In fact, of the carbon emissions that European consumers are personally responsible for, around 22% are allocated elsewhere under conventional carbon accounting practices. For consumers in the US, the figure is around 15%. Carbon emissions from the exhaust pipe tell only part of the story. To get a full sense of the carbon footprint of a car, you have to consider those emissions that go into producing the raw materials and digging a hole in the ground twice – once to extract the metals contained in the car, once to dump them when they can no longer be recycled.  Buying a new car and dumping the old one might be justifiable if the change was made because the new vehicle is more fuel efficient, but it is certainly not when it’s a question of personal taste or corporate-level planned obsolescence. The same is true for any number of high tech items, including smartphones that run on software that renders them unusable in the medium term. The environmental consequences of replacing a smartphone, in terms of carbon emissions alone, are considerable. Apple found that 83% of the carbon dioxide associated with the iPhone X was directly linked to manufacture, shipping and recycling. With these kinds of figures, it is hard to argue a sustainable case for upgrades – regardless of how many solar panels Apple sticks on the roof of its offices. Governments of richer countries that import products but not their emissions must stop pointing the finger at China or other manufacturing or mining giants and start taking responsibility. This means going further than they have been willing to go so far, and implementing sustainable material strategies that address a product’s entire lifecycle from mining to manufacturing, use, and eventually to disposal.  On an individual level people must vote with their money. It’s time to leave behind the laggards who hide the cost of the carbon contained within their products and who design them to fail in order to put profits before people and the environment."
nan
nan
"
Share this...FacebookTwitterA new study shows that the Antarctic Ice Shelf has been thickening – in times of global warming. Logic: Global warming cannot be the driving factor for Antarctic ice shelf mass, let alone CO2.
=====================================
West Antarctic Ice Shelf: El Nino takes, La Nina gives
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P. Gosselin)

Antarctic ice shelf. NASA photo (Chris Larsen) – public domain.
Whenever a chunk of ice breaks off the Antarctic ice shelf, the guilty party is immediately found by the media: man. He did it with his SUVs, his planes and his container ships.
But what makes everything all the more surprising is a recent press release from Scripps Institution of Oceanography from 8 January 2018. Weather phenomenon El Niño is causing the ice shelf in the Amundsen Sea to melt from underneath.
Additional snowfall is unable to compensate for the loss. During a La Niña phase the opposite occurs: The ice sheet grows. It’s natural variability at work. Gradually we are beginning to understand how it works. The press release follows:
New Study Reveals Strong El Niño Events Cause Large Changes in Antarctic Ice Shelves
Oscillations of water temperature in the tropical Pacific Ocean can induce rapid melting of Antarctic ice shelves. A new study published Jan. 8 in the journal Nature Geoscience [Paolo et al. 2018] reveals that strong El Niño events can cause significant ice loss in some Antarctic ice shelves while the opposite may occur during strong La Niña events.
El Niño and La Niña are two distinct phases of the El Niño/Southern Oscillation (ENSO), a naturally occurring phenomenon characterized by how water temperatures in the tropical Pacific periodically oscillate between warmer than average during El Niños and cooler during La Niñas. The research, funded by NASA and the NASA Earth and Space Science Fellowship, provides new insights into how Antarctic ice shelves respond to variability in global ocean and atmospheric conditions.
The study was led by Fernando Paolo while a PhD graduate student and postdoc at Scripps Institution of Oceanography at the University of California San Diego. Paolo is now a postdoctoral scholar at NASA’s Jet Propulsion Laboratory. Paolo and his colleagues, including Scripps glaciologist Helen Fricker, discovered that a strong El Niño event causes ice shelves in the Amundsen Sea sector of West Antarctica to gain mass at the surface and melt from below at the same time, losing up to five times more ice from basal melting than they gain from increased snowfall. The study used satellite observations of the height of the ice shelves from 1994 to 2017.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




‘We’ve described for the first time the effect of El Niño/Southern Oscillation on the West Antarctic ice shelves,’ Paolo said. ‘There have been some idealized studies using models, and even some indirect observations off the ice shelves, suggesting that El Niño might significantly affect some of these shelves, but we had no actual ice-shelf observations. Now we have presented a record of 23 years of satellite data on the West Antarctic ice shelves, confirming not only that ENSO affects them at a yearly basis, but also showing how.’
The opposing effects of El Niño on ice shelves – adding mass from snowfall but taking it away through basal melt – were at first difficult to untangle from the satellite data. ‘The satellites measure the height of the ice shelves, not the mass, and what we saw at first is that during strong El Niños the height of the ice shelves actually increased,’ Paolo said. ‘I was expecting to see an overall reduction in height as a consequence of mass loss, but it turns out that height increases.’
After further analysis of the data, the scientists found that although a strong El Niño changes wind patterns in West Antarctica in a way that promotes flow of warm ocean waters towards the ice shelves to increase melting from below, it also increases snowfall particularly along the Amundsen Sea sector. The team then needed to determine the contribution of the two effects. Is the atmosphere adding more mass than the ocean is taking away or is it the other way around?
‘We found out that the ocean ends up winning in terms of mass. Changes in mass, rather than height, control how the ice shelves and associated glaciers flow into the ocean,’ Paolo said.  While mass loss by basal melting exceeds mass gain from snowfall during strong El Niño events, the opposite appears to be true during La Niña events. Over the entire 23-year observation period, the ice shelves in the Amundsen Sea sector of Antarctica had their height reduced by 20 centimeters (8 inches) a year, for a total of 5 meters (16 feet), mostly due to ocean melting. The intense 1997-98 El Niño increased the height of these ice shelves by more than 25 centimeters (10 inches). However, the much lighter snow contains far less water than solid ice does. When the researchers took density of snow into account, they found that ice shelves lost about five times more ice by submarine melting than they gained from new surface snowpack.
‘Many people look at this ice-shelf data and will fit a straight line to the data, but we’re looking at all the wiggles that go into that linear fit, and trying to understand the processes causing them,’ said Fricker, who was Paolo’s PhD adviser at the time the study was conceived. ‘These longer satellite records are allowing us to study processes that are driving changes in the ice shelves, improving our understanding on how the grounded ice will change,’ Fricker said.
‘The ice shelf response to ENSO climate variability can be used as a guide to how longer-term changes in global climate might affect ice shelves around Antarctica,’  said co-author Laurie Padman, an oceanographer with Earth & Space Research, a nonprofit research company based in Seattle. ‘The new data set will allow us to check if our ocean models can correctly represent changes in the flow of warm water under ice shelves,’ he added. Melting of the ice shelves doesn’t directly affect sea level rise, because they’re already floating. What matters for sea-level rise is the addition of ice from land into the ocean, however it’s the ice shelves that hold off the flow of grounded ice toward the ocean. Understanding what’s causing the changes in the ice shelves ‘puts us a little bit closer to knowing what’s going to happen to the grounded ice, which is what will ultimately affect sea-level rise,’ Fricker said. ‘The holy grail of all of this work is improving sea-level rise projections,’ she added.”
Meanwhile the Potsdam Institute (PIK) continues to work from the alarmism playbook.
Surprisingly there is also a finding from the Ross ice shelf. There researchers carried out a bore sample through the ice. Amazingly the ice  appears to be growing instead of melting for the time being. National Geographic reports on the unexpected result:
Deep Bore Into Antarctica Finds Freezing Ice, Not Melting as Expected
[…] The surprises began almost as soon as a camera was lowered into the first borehole, around December 1. The undersides of ice shelves are usually smooth due to gradual melting. But as the camera passed through the bottom of the hole, it showed the underside of the ice adorned with a glittering layer of flat ice crystals—like a jumble of snowflakes—evidence that in this particular place, sea water is actually freezing onto the base of the ice instead of melting it.”
Read the entire article at National Geographic.
Share this...FacebookTwitter "
nan
"Navy ships and army aircraft have been dispatched to help fight devastating bushfires on Australia’s south-east coast that are feared to have killed at least 17 people, amid a spiralling debate over the government’s stance on the climate emergency. Thousands of people have fled apocalyptic scenes, abandoning their homes and huddling on beaches to escape raging columns of flame and smoke that have plunged whole towns into darkness and destroyed more than 4m hectares of land. Thousands of firefighters were still battling more than 100 blazes in New South Wales (NSW) state and nearly 40 in Victoria on Wednesday, with new fires being sparked daily by hot and windy conditions and, more recently, dry lightning strikes created by the fires themselves. At the end of Australia’s hottest-ever decade, Canberra, the capital, was blanketed in a cloud of dense smoke that left its air quality more than 21 times the hazardous rating. The haze drifted more than 1,200 miles (2,000km) to the South Island of New Zealand, where it turned the daytime sky orange. Fanned by soaring temperatures, strong winds and a terrible three-year drought, huge blazes have ravaged a tinder-dry landscape, causing immense destruction: since November, more than 900 homes have been lost in NSW alone. With three months of the summer still to go, the early and devastating start to the country’s fire season has led authorities to rate it the worst on record and prompted urgent questions about whether the conservative government of the prime minister, Scott Morrison, has taken enough action on global heating. Polls show a large majority of Australians see the climate emergency as an urgent threat and want tougher government action, but Morrison has focused instead on the nation’s response to the bushfire crisis and defending Australian business, while other government officials have publicly disparaged climate activists. In his New Year’s Eve address to the nation, Morrison did not make any connection between the bushfires and global heating, suggesting that while they were a terrible ordeal, Australians had faced similar trials throughout history. Past generations had “also faced natural disasters, floods, fires, global conflicts, disease and drought” and overcome them, the prime minister said in a video message. “That is the spirit of Australians, that is the spirit that is on display, that is a spirit that we can celebrate as Australians.” On Wednesday, he acknowledged at a reception that the fires were “a time of great challenge for Australia”, but deflected debate about the underlying cause of the fires, concentrating again on the nation’s resilience. Fire experts and scientists have described the scale and impact of this year’s fires as unprecedented and said that greenhouse gas emissions, while they do not cause fires, play a proven role in raising temperatures and creating the exceptionally dry conditions that make the risk of fire extreme or catastrophic. Although slightly cooler conditions on New Year’s Day gave the country a moment to take stock of the devastation, conditions were set to deteriorate again over the weekend, said the NSW state premier, Gladys Berejiklian. Dangerous fire conditions were forecast to return to eastern Victoria and NSW on Saturday, with temperatures again likely to reach the mid-40s. “We are assuming that weather conditions will be at least as bad as what they were yesterday,” Berejiklian said. “All of us have to brace ourselves.” While most of the destruction occurred on Tuesday, the ferocity of the fires meant many people were unable to find out basic information until New Year’s Day. Electricity and communications lines were cut for extended periods in many areas. Roads in and out of towns remained closed. Officials in NSW and Victoria said on Wednesday another five people were confirmed dead, and another man was presumed to have been killed. Scores more were missing and the death toll was likely to continue to rise, they said. Three bodies were found on Wednesday at Lake Conjola on the south coast of NSW, bringing the death toll in the state to 15. About 4,000 people in the coastal Victoria town of Mallacoota fled to the shore as winds pushed a fire toward their homes under a sky turned dark by smoke and turned red by flames. Dozens of homes burned before winds changed direction late on Tuesday, sparing the rest of the town. Mark Tregellas, a resident who spent the night on a boat ramp, said only a late shift in the wind direction spared lives. “The fire just continued to grow and then the black started to descend,” he said. “I couldn’t see the hand in front of my face, and then it started to glow red and we knew the fire was coming. “Ash started to fall from the air and then the embers started to come down. At that point, people started to bring their kids and families into the water. Thankfully, the wind changed and the fire moved away.” The Victoria state premier, Daniel Andrews, said four people remained missing after a massive blaze ripped through Gippsland, a rural region about 310 miles (500km) east of Melbourne. Mick Roberts from East Gippsland had been unaccounted for since Monday and was found dead in his home on Wednesday. “He’s not missing any more … sorry but his body has been found in his house,” his niece, Leah Parsons, said on social media. At Malua Bay, on the NSW south coast, survivors spoke of how 1,000 people spent the night on the beach. “Everyone was on the beach, just covered in ash and smoke,” Al Baxter, a retired rugby union international, told Guardian Australia. “There was a strange calmness. People were as close to the water’s edge as they could be. People were literally just lying on the beach trying to keep out of the smoke and ash.” Criticism of the Morrison government’s climate stance has intensified as the fires have raged. Australia is the world’s largest exporter of coal and liquefied natural gas, but the prime minister, who won a surprise election victory in May, last month rejected calls to downsize Australia’s lucrative coal industry. His government has pledged to cut greenhouse gas emissions by 26-28% by 2030, a modest figure compared with the centre-left opposition Labor party’s pledge of 45%. The leader of the minor Australian Greens party, Richard Di Natale, demanded a royal commission, the nation’s highest form of inquiry, on the crisis. “If he (Morrison) refuses to do so, we will be moving for a parliamentary commission of inquiry with royal commission-like powers as soon as parliament returns,” Di Natale said in a statement. Australia’s armed forces, including helicopters, fixed-wing aircraft and naval vessels, were being deployed to help fight the fires, bring water, food and fuel to towns where supplies were depleted and roads cut off, as well as evacuate residents. Victoria’s emergency management commissioner, Andrew Crisp, said the 176-metre HMAS Choules, due to arrive by Friday, may be used to evacuate many of those stranded in Mallacoota, although its capacity of 1,000 people would not be enough on its own to handle everyone who needed to get out. “It doesn’t have the current capacity for everyone at Mallacoota,” Crisp said. “We are exploring all our options … and certainly to look at evacuating some people from Mallacoota by sea is an option we’re seriously considering.” Besides the deployment of HMAS Choules, Australia’s defence force said it had been providing support to bushfire efforts in all states except Tasmania since 8 November and was dispatching Taipan, Black Hawk and Chinook helicopters plus two Spartan aircraft to Victoria, where they would help with firefighting efforts and provide humanitarian assistance to isolated communities."
"The US Clean Power Plan puts a national limit on greenhouse gas emissions for the first time. Despite a few critics, environmentalists have on the whole reacted positively. Yet, as societies around the world are already struggling with the effects of climate change, is Obama’s plan ambitious enough? As he acknowledged himself, “there is such a thing as being too late when it comes to climate change”. We suggest precisely that: his plan is too little, given that it has arrived so late. The Clean Power Plan aims for a reduction in greenhouse gas emissions associated with coal, oil, and gas-fired power plants by 32% below 2005 levels by 2030. It focuses on the electricity sector, which is a good thing. Electricity generation from fossil fuels is the largest single industrial source  of CO2 emissions and 31% of the US total. The plan gives US states a flexible deadline of September 2016 to submit plans for emissions reduction. They must comply by 2022. States have a few different policy options: Yet, if they do not draw up a plan for reducing emissions, the EPA will impose emissions trading by default. As with any climate policy, the ambition of an instrument must be judged in terms of the detail and much of that will depend on how the states react. At this point, we take issue with two aspects of the scheme: first, the ambition of the emissions target and industrial coverage; and second, the potential loophole that emissions trading will create. Obama is right to start with the emissions-intensive electricity sector. However, we should remember that energy exports, transport, agriculture, mining and industrial emissions are outside the scope of the scheme. This is a common problem in climate policy. All too often, major “national” economic drivers of greenhouse emissions do not fall into the scope of federal climate policy. For instance in Australia, the booming energy export market has contributed to a major increase in global greenhouse emissions, but only the “fugitive” emissions associated with mining were regulated under the former carbon trading scheme. Then there is the 2005 baseline the US has chosen, while the Kyoto Protocol established a baseline of 1990. Why? Because from 2005 on the US has experienced a shale gas “revolution”, which crowded dirty coal out of the energy market and reduced emissions. While the Clean Power Plan is clearly a step in the right direction, it is a case of too little given the world’s largest economy is acting so late. We need much more ambitious action from the US and other like nations, if we want to have a realistic chance of staying within the globally agreed 2°C, or better 1.5°C of “safe” global warming. The Clean Power Plan is likely to become a patchwork of emissions trading schemes, something not emphasised in the initial proposals. However, the EPA’s January proposal  and the final rule single out emissions trading as a preferred policy option. If a state does not deliver a plan for emissions reduction, then a federal cap-and-trade program becomes the default policy. States are also able to link to existing state-level emissions trading programs, the California cap-and-trade scheme and the Regional Greenhouse Gas Initiative. How this is managed has been a subject of debate, particularly regarding carbon offsets. Thankfully, the use of “carbon offset” credits from sectors outside the electricity sector for compliance purposes is not likely to be possible. The use of carbon offsets would not qualify under the EPA’s definition of the “best system of emissions reduction” for purposes of the Clean Air Act section 111(d) (see Section V pp. 517-520 of the rule). States which include trading schemes like the California cap-and-trade scheme which relies on forest carbon offsets to a significant degree will have to demonstrate that emissions reductions have occurred in the power sector. Some states and carbon trading advocates have been unhappy that carbon offsets cannot be used. It is for the best they have not been successful in the call for “flexibility” through offsetting. The introduction of carbon-offset credits in the Clean Power Plan would have made it similar to the Australian Direct Action Plan. In the Australian scheme, the “safeguard mechanism” to keep emissions to a minimum seems likely to be turned effectively into a baseline-and-credit carbon trading scheme. This means any kind of “cap” the safeguard mechanism could impose on the most heavy emitters will be undermined. While the US Clean Power Plan is an improvement on the Australian situation, we remain concerned about the direction of US climate policy, particularly the likelihood a jumble of emissions trading schemes will be created. In our recent published research, we find carbon trading lacking on a number of counts. There have been numerous problems with carbon trading, including ineffectiveness, weak regulation and implementation, instances of fraud, little or no emissions reduction and major legitimacy issues for governments and the private sector. The Clean Power Plan design makes a complex set of carbon trading arrangements likely, which may in turn replicate the problems of existing carbon trading schemes across the world. Given the increasingly urgent timeline within which we need to act to radically reduce emissions, the incorporation of carbon trading as an option in the Clean Power Plan is half-baked."
"
Share this...FacebookTwitterNew Paper Spurns Anthropogenic CO2 Warming,
Unveils Natural Explanation For Climate Change

University of California (Santa Cruz) Professor W. Jackson Davis (Ph.D.), President of the Environmental Studies Institute, has published a new paper with colleagues in the journal Climate that thoroughly undermines the conceptualization of a dominant role for anthropogenic CO2 in the global warming since 1850.
Davis points out that CO2 and global temperature have been “decoupled” throughout much of geological history, and that the amplification of CO2 concentrations yields increasingly smaller radiative effects, meaning that the higher the CO2 concentration rises, the weaker its influence.
He even suggests that the reason why the anthropogenic global warming (AGW) hypothesis (it has not reached theoretical status) has been popularized is because there are reputed to be no convincing alternative explanations.
But Davis and two other University of California (SC) scientists have proposed a newly-termed alternative explanation for the 0.8°C global temperature change since 1850.  The Antarctic Centennial Oscillation (ACO) has been identified as varying in sync with solar cycles (orbital), and correlates with glacial-interglacial transitions, the 1,500-year abrupt, global-scale temperature changes (Dansgaard-Oeschger cycles), and, as the name suggests, century-scale fluctuations in global temperature.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Consequently, as the authors conclude, properties of the ACO “can explain the current global warming signal”.

Davis et al., 2018
Introduction:
[T]he contemporary global warming increase of ~0.8 °C recorded since 1850 has been attributed widely to anthropogenic emissions of carbon dioxide (CO2) into the atmosphere. Recent research has shown, however, that the concentration of CO2 in the atmosphere has been decoupled from global temperature for the last 425 million years [Davis, 2017] owing to well-established diminishing returns in marginal radiative forcing (ΔRF) as atmospheric CO2 concentration increases. Marginal forcing of temperature from increasing CO2 emissions declined by half from 1850 to 1980, and by nearly two-thirds from 1850 to 1999 [Davis, 2017]. Changes in atmospheric CO2 therefore affect global temperature weakly at most.
The anthropogenic global warming (AGW) hypothesis has been embraced partly because “…there is no convincing alternative explanation…” [USGCRP, 2017] (p. 12).
The ACO provides a possible alternative explanation in the form of a natural climate cycle that arises in Antarctica, propagates northward to influence global temperature, and peaks on a predictable centennial timetable.
Abstract:
We report a previously-unexplored natural temperature cycle recorded in ice cores from Antarctica—the Antarctic Centennial Oscillation (ACO)—that has oscillated for at least the last 226 millennia. Here we document the properties of the ACO and provide an initial assessment of its role in global climate. We analyzed open-source databases of stable isotopes of oxygen and hydrogen as proxies for paleo-temperatures. We find that centennial-scale spectral peaks from temperature-proxy records at Vostok over the last 10,000 years occur at the same frequencies (±2.4%) in three other paleoclimate records from drill sites distributed widely across the East Antarctic Plateau (EAP), and >98% of individual ACOs evaluated at Vostok match 1:1 with homologous cycles at the other three EAP drill sites and conversely.
The period and amplitude of ACOs oscillate in phase with glacial cycles and related surface insolation associated with planetary orbital forces. We conclude that the ACO: encompasses at least the EAP; is the proximate source of D-O oscillations in the Northern Hemisphere; therefore affects global temperature; propagates with increased velocity as temperature increases; doubled in intensity over geologic time; is modulated by global temperature variations associated with planetary orbital cycles; and is the probable paleoclimate precursor of the contemporary Antarctic Oscillation (AAO). Properties of the ACO/AAO are capable of explaining the current global warming signal.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMedia in typhoon-prone Japan ignore new important findings suggesting hurricanes and typhoon intensification speed depends mostly on natural oceanic cycles, and not related to atmospheric CO2. 
Recently I posted on the surprising and science-realistic German DLF national public radio report on how hurricanes are intensifying more quickly today than they did 30 years ago.
Findings by scientists from the US Department of Energy (DOE) and the Pacific Northwest National Laboratory show it all has to do natural oceanic cycles that change every 30 years.

Hurricane intensification speed depends on natural ocean cycles, scientists recently determined. Yet much of the media remained silent over the findings – even cyclone-prone Japan. Photo image: NASA, public domain
Cyclones of great importance for Japan
And now that NTZ is committed to putting some focus on climate news and developments coming from Japan, I naturally wondered how these new, sober findings were received by the media there. After all, Japan is a country that regularly gets hit by typhoons, and so the subject of tropical cyclones and their genesis and future trends ought to be of great public and civic interest. Moreover, better long-term forecasting is crucial and could save lives.
Yet, no real media reporting by Japanese media
So I asked Japanese blogger Kirye of KiryeNet climate blog to do a quick Google search of the Japanese sites for reports on these findings, which she kindly did. Regrettably I’ve got to report that her search did not find a single report from any major Japanese media outlet on these latest findings.
She informed: “There seems to be no link of those articles published in the Japanese version of Google this year.”
Why would the media in a typhoon-prone country like Japan not find the recent finding worthy of reporting? This is somewhat mind-boggling (unless of course the media’s aim there is to just keep its own people in the dark when it comes to climate facts).
Kirye also wrote that the Japanese media generally have remained steadfast in their promotion of climate alarmism, and that skeptics don’t get any coverage in Japan.
Japan typhoons show no link to “global warming”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Not only do the recent DOE findings show that Atlantic hurricane intensification speed depends on natural ocean cycles, and thus these cycles need to be examined in the Pacific as well, but also the climate-science critical Japanese blogger mentioned other good news concerning cyclones earlier at her blog, namely the number of landings in Japan show no link at all to “global warming”.
Citing the Japanese Meteorological Agency, she wrote:
The number of typhoons formed decreased and the number of landings in Japan has been flat since 1951:
And Kirye provided the following chart:









Chart of typhoon non-alarmism. Courtesy of KiryeNet. 
Here she is in fact being somewhat modest because as you will notice the reality is that trend for the number of typhoons forming has been clearly downward, and thus it defies all the earlier predictions of more storms made by the “climate experts”. Things were considerably worse back in the 1960s when the planet was cooler and atmospheric CO2 lower.
In summary it is puzzling that the Japanese media would so recklessly handle the critical topic of typhoons and mislead the public of Japan in the face of glaring facts.
It’s the sort of information management one would come to expect for Japan’s nearby foe, which the world is currently grappling with.
Link to natural cycles provides useful forecasting tool
By having found a strong link between ocean cycles and tropical storm development, a very useful, longer-term forecasting tool emerges – if only it were acknowledged.
Sometimes it takes awhile before truth trumps obstinance.
Share this...FacebookTwitter "
"Flying cars in The Jetsons and Back to the Future, or Star Trek’s spaceships and teleportation, may have captured the imagination decades ago, but most current methods of transport have been around a long time. Railways were being rolled out rapidly from the 1830s, while the commercial breakthroughs in petrol and diesel engines date to 1876 and 1892 respectively. Even the jet engine that made mass aviation possible can be traced back to Frank Whittle’s first patent in 1932. Despite decades of futuristic predictions, modern transport wouldn’t look all that different to someone from the 1950s – certainly not compared to communications or entertainment. So why has there been so little recent innovation in transport? And will the latest batch of proposed driverless cars, levitating trains and electric aircraft actually make a serious breakthrough? In part, there hasn’t been a revolution because existing technologies have been able to evolve. Engines have become more efficient, fuel is higher quality, we have lighter materials, more aerodynamic designs and better brakes that mean vehicles can operate safely closer together. However, eventually there will be a limit to these evolutions. In any event, transport is not just about technology. It is also about people – and people don’t always like change. We may be locked in to current technology, partly due to habit but also due to economics.  We have an extensive transport refuelling system based on petrol and diesel. To convert to electricity or, more fancifully, to hydrogen, will involve substantial re-tooling that will be difficult to finance. In the UK, drivers are used to manual transmissions and may be reluctant to learn how to use more automated systems, just as we would be reluctant to retrain to use a different keyboard even if it were more efficient. We are stuck with what we have – the economics of QWERTY.   Human factors may lead to unintended consequences – one of the ironies of automation is that it can lead to less attention to related tasks. For example adaptive cruise control can make car drivers less aware of hazards. Even with full automation, when we still have trouble making all trains driverless, one might suggest driverless cars are a flight of fancy. Innovative aeroplane designs, such as the blended wing, are stymied by the human requirements for a window seat (NASA has suggested windows could be replaced with real-time video). Fancy new inventions have to be accompanied by a business model and the right infrastructure, or else they’ll just languish as prototypes like the pneumatic transit system demonstrated in New York City in the early 1870s and a forerunner to Elon Musk’s proposed Hyperloop. Take flying cars. Even supposing the technology works, where would they land?  Such a system would only succeed if infrastructure – air traffic control, landing space and so on – was set aside. While flying cars could technically operate from airport to airport, what’s the point? Until there are sufficient numbers to set aside pieces of land or roads for takeoff we won’t achieve any of the benefits. And there won’t be sufficient demand until this land is set aside. Catch 22. When looking at how technology interacts with wider society it’s helpful to think in terms of three different levels: niches, regimes and landscapes.  In transport, there are plenty of niche innovations – battery electric vehicles, hydrogen fuel cells, car clubs – but few become mainstream.  An exception might be hybrid electric vehicles such the Toyota Prius, but even here the underlying technology may be traced back to a patent registered in 1898 (by Ferdinand Porsche, no less).  The problem isn’t coming up with new ideas – it’s changing the bigger picture. At regime level, new transport technologies have faced resistance from vested interests such as oil producers and car makers. And the wider landscape has not always favoured major innovations – especially low oil prices.  With lots of different individual suppliers, transport is also vulnerable to tragedy of the commons-type outcomes and clashes between rival designs and brands. Navigation technologies can only be sold commercially if they benefit the individual consumer. However, if we all have access to such technologies, we can be collectively worse off due to congestion – for the greater good, it would be beneficial if sometimes our SatNav sends us on a longer route, but who is knowingly going to buy something like that?  Electric battery technology might have more rapid adoption if the technology was standardised, permitting automated battery swaps. But standardised to whose technology? Magnetic levitation train adoption is limited by the fact they can’t run on traditional rail lines and have only limited overlap with other maglevs. In short, despite the fuss over disruptive technologies such as Uber, it is unlikely that transport will have a technology paradigm shift until there is a major landscape change. Of course, with volatile oil prices, limited reserves and sensitive geopolitics, such a change could be just round the corner. But for the moment the technology push does not seem to be complemented by a societal pull – people might like to watch sci-fi, but they aren’t yet ready to live it."
"It can be easy to overlook the monstrous scale of the Antarctic ice sheet. Ice, thick enough in many places to bury mountains, covers a continent roughly the size of the US and Mexico combined. If it were all to melt, as it has in the past, global sea levels would rise by 58 metres. While this scenario is unlikely, Antarctica is so massive that just a small fraction of this ice melting would be enough to displace hundreds of millions of people who live by the coast. Low-lying cities face the threat of flooding when extreme weather coincides with high tides. Although typically rare, these events are already increasing in frequency, and will become commonplace as global sea levels increase. Over the coming decades, rising sea levels from melting ice and the expansion of warming oceans will strain societies and economies worldwide. Improving our understanding of how much Antarctica has contributed to sea level rise in the past, and how much it will contribute in the future, is vital to informing our response to climate change. Achieving this is impossible without satellites. Antarctica is too vast, too remote – satellites are our only means of monitoring its behaviour on a continental scale. Satellites launched by the European Space Agency and NASA allow scientists to monitor changes in ice height, ice velocity and ice mass through changes in Earth’s gravity field. Each of these satellites provide an independent way to measure Antarctica’s past contribution to sea level rise. The ice sheet mass balance inter-comparison exercise (IMBIE) is an international effort: a team of 84 polar scientists from 44 organisations, including both of us, working together to provide a single, global record of ice loss from Earth’s polar ice sheets. In our latest assessment, published in Nature, we used 11 different satellite missions to track Antarctica’s sea level contribution since the early 1990s. We have found that since 1992 Antarctica has lost 2,720 billion tonnes of ice, raising global sea levels by 7.6mm. What is most concerning, is that almost half of this ice loss has occurred in the past five years. Antarctica is now causing sea levels to rise at a rate of 0.6mm a year – faster now than at any time in the past 25 years.  Most of this ice loss has come from West Antarctica. In the Amundsen Sea Embayment (named after Roald Amundsen, one of the first explorers to reach the South Pole) warming ocean temperatures have reduced the floating ice shelves which slow the flow of the mighty Pine Island and Thwaites Glaciers, resulting in a rapid acceleration of ice losses. Between 1992 and 2017 we have observed a threefold increase in the rate of ice loss from West Antarctica, from 53 to 159 billion tonnes a year. In the Antarctic Peninsula, the collapse of the Larsen B and Wilkins ice shelves in the 2000s has had similar consequences: an abrupt acceleration in the rate local glaciers drain into the ocean. This new knowledge will help us better predict sea level rise in the future. In 2014 the intergovernmental panel on climate change (IPCC) published its fifth assessment report, which includes modelled projections of Antarctica’s contribution to sea level rise over the century. By mapping our measured sea level contribution on top of these projections, we found that our previous assessment of Antarctic sea level contribution, which measured ice loss until 2012, was tracking the IPCC’s lowest projection. In light of the acceleration in ice loss we have observed over the past five years, we now find sea level rise from Antarctica to be tracking the IPCC’s highest projection. This amounts to an additional 15cm in global sea level rise from Antarctica alone by 2100. We have long suspected that changes in Earth’s climate will affect the polar ice sheets. The rapid increase in Antarctic ice loss and consequent sea level rise we have measured over the past 25 years are a clear indicator of climate change. Limiting global warming to 2℃ by 2100, as set by the Paris Agreement, looks increasingly unlikely. The rate at which ice losses from Antarctica will increase in response to a warming world remains uncertain. It is important, now more than ever, that we continue to use satellites to monitor Antarctica in order to better prepare ourselves for the challenges ahead."
"In 2004, the Guardian correctly predicted that the developed world’s overreliance on meat would be one of the most pressing issues for the survival of our species. “Britons need to say goodbye to burgers and meat,” we wrote, “because the overemphasis on meat in the western diet is one of the things that stifles sustainable food production.” Thankfully, the past decade and a half has seen an unprecedented interest in meat-free diets. In 2004, veganism was seen as a fussy, faddish lifestyle choice. Now, buoyed by blockbuster documentaries such as Cowspiracy and What the Health, and celebrities such as Ellie Goulding and Ariana Grande, it has never been so popular. According to the Vegan Society, 600,000 Britons are vegan. In 2006, this figure was just 150,000.  Demand for vegan food is at unprecedented levels: in 2018, the UK launched more vegan products than any other nation; in 2019, 250,000 people signed up for the Veganuary challenge, pledging to avoid all meat and dairy for the month of January. For those not willing to make the jump to full veganism, one in three Britons had at least reduced their meat consumption in 2018. But there is still a long way to go. The west’s overconsumption of meat and dairy continues to fuel global warming. Livestock is responsible for approximately 14.5% of greenhouse gas emissions; 70% of global deforestation takes place in order to grow animal feed. In 2019, the EAT-Lancet Commission on Food, Planet and Health determined that substantial dietary shifts must take place by 2050. “Global consumption of fruits, vegetables, nuts and legumes will have to double, and consumption of foods such as red meat and sugar will have to be reduced by more than 50%,” the panel of experts judged. By 2050, meat-eating could seem like a throwback, according to some experts. “Our current method of growing crops to feed to animals so we can eat animals is shockingly inefficient,” says Bruce Friedrich of the Good Food Institute, which works to develop alternatives to meat. “By 2050, [almost] all meat will be plant-based, or cultivated.” Plant-based “meat” is already here. The Beyond Burger and the Impossible Burger are widely available, but Friedrich predicts a widening of the products on offer to consumers. “We’ll have plant-based meat that doesn’t exist yet, whether it’s pork chops, steaks, tuna or salmon.” Anna Taylor, the executive director of the Food Foundation, thinks that plant-based substitutes will find their way into processed foods, without consumers being any the wiser. “Plant-based alternatives to animal protein will appear automatically in foods we eat, without consumers having to change their habits.” The real challenge will be persuading consumers to embrace cultivated meat. Also known as lab-grown meat, this is developed from animal or fish cells in the nutrient bath of a “bioreactor”. It is not yet on the market, but at least 40 private companies are working on cultivated-meat alternatives. In the UK, scientists at the University of Bath are growing bacon on blades of grass. The California startup JUST has created chicken nuggets in a bioreactor. Friedrich is optimistic that we will all be eating cultivated meat by 2050. “There won’t be factory farms or abattoirs in 2050,” he predicts. “People will look back at the idea of growing live animals for meat in the same way that we look back at horse-drawn carriages for getting from London to Brussels.” But we won’t entirely stop eating meat from animals reared for slaughter. “There will be some heritage breed farms and slaughterhouses where the animals are treated well,” Friedrich concedes. But it will be a limited market. If this all sounds like science fiction, strap yourselves in for the predictions of the food designer and futurist Chloé Rutzerveld. “We’ll skip all of our current existing foods and switch towards a whole new eating system where we build food with microorganisms,” she says. “Instead of growing crops or raising animals, we’ll use microorganisms such as fungi, bacteria, yeast and microalgae to directly produce the carbs, proteins and fats we need.” This food will again be produced in bioreactors, before being filtered and dried into powders. But we won’t be consigned to a joyless diet of flavourless dust. Rutzerveld claims that 3D-printing technology will be able to replicate the textures and flavours of regular food. “We can make a library of mouth feels and texture at a nanoscale,” she says, “in order to recreate sensations like freshness or juiciness.” Eating microorganisms grown in bioreactors will be transformative. “We’ll be able to make the food-production system so much more efficient, saving the land and water and energy resources.” Not everyone agrees. “Lab-grown meats are a red herring,” says Prof Pete Smith of the University of Aberdeen. “We don’t need them. We can get most of the protein we need from plant-based foods.” He is also doubtful that we will move to other protein sources such as insects. “Wealthy countries are already massively overconsuming protein,” he says. “We don’t need to move to alternate protein sources – if we cut in half the amount of protein we are already eating, we would be at healthier levels.” But even if we are not eating insects directly, they can still be of use in our wider food chain. “Insects could be really important for feeding animals,” says Taylor. “One of the problems we have is that ruminant animals (including cattle, sheep and goats) are often fed on soya, which is grown in deforested parts of the planet. If we could shift to other ways of feeding animals, such as insects, we can reduce deforestation.” Consumers will continue to turn to plant-based alternatives to dairy. Plant-based milks are already big business, and egg substitutes will be next: JUST and Zero Egg have already developed egg substitutes, and we can expect other brands to emerge. By 2050, climate change will dramatically affect what we can eat and drink. Speciality crops such as avocados, coffee and wine grapes – which can only be grown in a very narrow climate range – will be at risk. “A one- or two-degree change in climate could mean make-or-break for some regions growing speciality crops,” says Prof Gregory V Jones, an expert in wine growing at Linfield College’s department of environmental studies. “Wine-growing regions such as Greece, southern Italy, southern Spain and Portugal will all potentially experience issues,” he continues. In turn, though, wine-growing will become possible in places such as Scandinavia and the north of England. “They aren’t world-class producing regions now, but by 2050, if climate change continues, they could be.” If real progress isn’t made to halt global heating, food production in the global south will be imperilled. “Things aren’t looking good for the developing world,” says Smith. “China has a big problem with water – overextraction means that the soil is becoming too salty. India also has a problem with unsustainable groundwater use.” Rice-growing in China and wheat-growing in the northern plains of India could be affected. All of this augurs badly, given that China and India’s population is forecast to rise to a combined 3.2 billion by 2050. “Water stress will become more acute,” Smith warns. “The more the climate warms, the more droughts we will have. This will take place in areas that are already struggling to feed themselves.” In the UK, much imported produce may become prohibitively expensive. “We are very reliant on countries that are vulnerable to climate change and water scarcity,” says Taylor. The grapes and berries we now import from Morocco, Spain, India and South Africa will become scarce. We will have to return to more seasonal patterns of eating – bananas, for example, will no longer be a cheap household staple. Widening inequality will play out on dinner tables across the world. “I’m worried about uneven dietary development,” says Prof Corinna Hawkes of the Centre for Food Policy at City, University of London. “The diets of rich people will get better, and those of poorer people will get worse, and we’ll end up with appalling inequalities in what we’re eating.” She predicts that wealthy people will continue to reduce their meat and processed-food intake, and embrace indigenous food such as sorghum and amaranth. “These foods were once eaten by poor people, but will become resurgent in higher-income groups, as they become more interested in novel foods.” As fast-food companies continue to make inroads into developing nations, junk-food consumption will rise in historically impoverished communities. “These people have inadequate diets already,” says Hawkes, “and they will be adding junk food into their diet, which only contributes to the problem of obesity.” Rising obesity rates are a real cause for concern; half of Britain’s population is predicted to be obese before 2050. Globally, 60% of men and 50% of women will be obese by 2050, if current trends continue. “We need to put into place policies that level the playing field and reduce the abilities of the companies selling junk food to make it so affordable and accessible,” says Hawkes. “Junk food needs to be taken out of the spotlight and nutritious food put centre-stage.” So, what should we be eating in 2050? It’s fairly simple – what we should be eating now. (And what the Guardian predicted in 2004.) “We need to be eating more fruit and vegetables, wholegrains, less junk food, and less meat and dairy,” advises Taylor. “If we do these four things, we stand a chance of reversing some of the diet-related diseases that are currently crippling the NHS, and also bringing down the carbon and biodiversity impacts of our diet.” But whether we’ll be able to resist the siren pull of processed food and cheap meat remains to be seen. Whole grains: 232g a day, 811 calories Tubers or starchy vegetables (ie potatoes, cassava): 50g a day, 39 calories Vegetables: 300g a day, 78 calories Fruits: 200g a day, 126 calories Dairy foods (whole milk or equivalents): 250g a day, 153 calories Combined protein sources (to include beef, lamb, pork, chicken, eggs, or fish): 84g a day, 151 calories Other protein sources (to include legumes or nuts): 125g a day, 575 calories Fats from oil: 51.8g, 450 calories Added sugars: 31g a day, 120 calories Source: the EAT-Lancet Commission"
"
Share this...FacebookTwitterWe’ve heard about all the cold and icy weather reports and results coming from all corners of the planet lately, and so naturally most of us sense that it just doesn’t jive with all the alarmist global warming claims and rhetoric we hear.
Tremendous ice growth
For example over the past winter the Arctic ice cap did see unusually warm surface temperatures, yet Arctic sea ice did not shrink as some would intuitively expect it to do.
The truth is that Arctic sea ice volume has gained close to 2 trillion cubic meters over last year alone, and over 2016 – using the data provided by the Danish Meteorological Institute here.
Obviously Arctic sea ice has a lot more to do with other factors than just surface air temperatures in the region. Clearly other major factors must be at play in causing this huge increase.
Japanese skeptic blogger Kirye provided at Twitter a nice animation showing the recent development from April 20 – to May 10:

From April 20 to May 10, 2018, Arctic sea ice volume has been greatest in the last four years.ʕ´•ᴥ•`ʔσDMI:https://t.co/UKdL3sM4Y8~#気候変動 #温暖化? #地球温暖化？ #ClimateChange pic.twitter.com/ET2d3PJ5HR
— キリエ (@KiryeNet) May 12, 2018

As one can see, there’s about 2000 cubic kilometers (2 trillion cubic meters) more ice volume than there was a year earlier and in 2016.
Enormous amount of energy
To put this in perspective, 2 trillion cubic meters of ice are enough to…
– Provide every single human being on the planet with 250 tonnes of ice!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




– Cover entire United States with more than 20 cm of solid ice.
– Cover 67,000 Manhattans with almost a meter of ice!
– Circle the earth with 1 cubic meter blocks 50,000 times!
– Stack 1 m³ blocks to make a pile high enough to reach the sun…13 times!
Imagine: 250 tonnes of ice for each and every person. I’d have to run through the numbers, but I don’t think the average person could freeze that amount of water with the total power he/she uses in a year. We’re talking some serious energy here.
So where could all the ice have come from when we consider that the Arctic surface air temperatures have been so warm?
And what about all the heat that had to be extracted from the water to form the ice? Where did it all go? Most of it of course go radiated out into space and so is forever gone and lost.
Surface temperatures not decisive
As discussed above, surface atmospheric temperatures of course do not play the only major role when it comes to the Arctic sea ice show. Obviously other very powerful factors play huge roles, such as natural oceanic cycles and weather patterns over all Arctic atmospheric layers.
Complex, poorly-understood oceanic-atmospheric system
And vice versa, it also implies that these factors also play a role during the summertime when the Arctic sees unusual summer time melt. Surface temperature cannot be the one and only explanation here, as global warming alarmists like to insist it is. Here as well the oceans, winds and clouds, to name a few, play crucial roles.
Polar sea ice depends on the entire oceanic-atmospheric polar “weather” system — from seabed to upper stratosphere — which sober scientists have long realized is an extremely complex one and is still very poorly understood.
===================================
Check out latest on Southern Hemisphere here.
Share this...FacebookTwitter "
"Mediterranean egrets balancing on the backs of cows, multicoloured moths the size of a human hand, and impossibly exotic bee-eaters hawking for insects under English skies. All are here as a direct consequence of the climate crisis, which has allowed continental European species to extend their ranges northwards, and then make the leap across the Channel to gain a foothold in southern Britain. Whenever I take a walk along the disused railway line across the Avalon marshes, near my Somerset home, I can’t help noticing these new arrivals. Tall and elegant, great white egrets first arrived here from France just a few years ago; now I encounter them every time I visit. Down the road, at the Somerset Wildlife Trust’s reserve at Catcott Lows, flocks of cattle egrets – the same species we see in wildlife films from Africa – gather to feed, perched appropriately on the backs of cattle. Elsewhere on the marshes, secretive night herons and little bitterns have also bred in recent years.  When I moved to Somerset with my young family just over a dozen years ago, all these species were so rare they would have attracted a crowd of eager twitchers. Today, everyone – including my own teenagers – takes them for granted. Going back to my own childhood, the now ubiquitous little egret – that Persil-white apparition featuring at a wetland near you – was incredibly scarce. When, at the age of 10, I stumbled across one on Brownsea Island in Dorset, it was the highlight of my birding life for many years afterwards. And it’s not just birds. When it comes to unexpected new arrivals, butterfly and moth enthusiasts have enjoyed a bumper year. First came the news in August that an invasion of long-tailed blue butterflies was occurring all the way along the south coast, from Cornwall to Kent. This unusual looking butterfly – which really does have a tiny “tail” protruding from the back of each wing – was turning up in the most unexpected places: Sussex butterfly expert Neil Hulme even found one laying eggs on pea plants in a pub garden. Thanks to Hulme’s guidance, even I managed to catch up with them, in the equally unlikely setting of a patch of waste ground next to Brighton racecourse. Meanwhile, that group of nocturnal activists known as “moth trappers”, of which I am one, have been attracting some real beauties to their light traps. The greatest prize this summer has been the wonderfully named Clifden nonpareil – literally meaning “beyond compare”. One of our largest and most spectacular moths, with a 12cm wingspan and a bright blue flash on its underwings, it was once considered extinct in Britain. Yet this summer, after an absence of many years, the Clifden nonpareil has been turning up in moth traps all over southern England and Wales. Its day-flying counterpart, the hummingbird hawkmoth, has also had a good year, as has one of our most mysterious and sought-after species, the death’s-head hawkmoth. Made famous – or perhaps that should be infamous – by the novel and film The Silence of the Lambs, several death’s-head caterpillars and pupae have been found in potato patches in the Somerset village of Westbury-sub-Mendip. Brought indoors by local naturalists, they were successfully hatched out, the adult moths revealing the sinister skull pattern on the back of the thorax that gives the species its name. When I went to see this extraordinary insect, it emitted its famous “squeak”, which adds to its terrifying reputation. It’s not just these new arrivals that indicate the effects of climate change – or as we now more correctly call it, the climate emergency. Many resident bird species are rising in numbers; as are short-distance migrants such as the blackcap and chiffchaff. These small, neat warblers are now overwintering in Britain (instead of Spain and north Africa), thanks to milder winters, and the consequent wider availability of their insect food. So, in Voltaire’s ironic comment, all is surely for the best, in the best of all possible worlds. For the moment, that may indeed be true. Yet as long ago as 1990, the German ornithologist and migration expert Peter Berthold warned that during the initial warming period many bird species would benefit from “heavenly conditions”. This, he explained, was a kind of honeymoon period in which warmer springs and summers, and milder winters, would allow them to expand their numbers and range. But if the global climate becomes hotter still, with more frequent and extreme weather events such as droughts, storms and floods, reality will inevitably begin to bite, and all but the most adaptable species will start to decline. Their fall might also be hastened by an increase in populations of parasites and diseases, which flourish in warmer climates. Ironically, the long-tailed blue butterfly I watched sunning itself in August cannot survive Britain’s winters – at least not yet. As Neil Hulme explains, it would need a rise in average temperatures of several degrees, enough to banish winter frosts that kill their larvae, to colonise Britain permanently. But if that did happen, we would have reached a climatic tipping point, and probably lose not just much of our wildlife, but even jeopardise our own long-term existence on the planet. Rapid environmental change is likely to hit some creatures harder than others. On a recent edition of the Radio 4 series The Life Scientific, Professor Anne Magurran of St Andrews University talked about what she calls “the shopping mall effect”. She noted that wherever you go in the world nowadays, from London to Los Angeles, Madrid to Melbourne, shopping centres tend to have the same outlets – well-known international brands whose names we all recognise. Likewise, in response to a panoply of environmental pressures, ecosystems are tending to become more homogenous, with a few highly adaptable species beginning to dominate to the exclusion of less successful ones. As Magurran warns, if environmental conditions become more and more extreme, homogenisation will start to occur, and there will inevitably be species loss. However, as Magurran points out, there is still time for us to take action to help safeguard the earth’s biodiversity. At the moment, she says, the dominant signal is change rather than loss. But unless we take swift and decisive action to mitigate climate change, while at the same time preventing habitat destruction, the rate of extinction will start to accelerate. Given that many governments appear to be heading in the opposite direction, with increased deregulation and a weakening of environmental protection back on the agenda, this is a very real concern. If we fail to act, the consequences for Britain’s wildlife are that successful generalists will do well, while specialists will not. When it comes to making predictions, we also need to take into account the unusual nature of the British Isles, which stretch from Shetland, just a few degrees of latitude short of the Arctic Circle, to Scilly, which has its own – almost subtropical – microclimate. So, while we enjoy the short-term benefits of climate change in the south, problems are already beginning to occur at the other end of Britain. On the Cairngorm plateau, our sole example of the arctic-alpine biome, the ptarmigan – the only British bird that turns white in winter – is gradually declining, with just a few thousand pairs remaining. Until the start of the 19th century, the ptarmigan could be found across a wide swathe of northern Britain, south to the Lake District; but today it survives only in the Highlands. Its decline has been caused by a number of factors, including crows attracted by the rubbish left by visitors, which prey on the ptarmigan’s eggs and chicks. But a more long-term factor is the warming climate, which is altering the habitat and food supply of these highly specialised birds. Should the climate emergency continue to worsen, and temperatures keep rising, the ptarmigan – along with other highland specialists such as the dotterel and snow bunting – looks set to disappear as a British bird in my lifetime. I recall many years ago hearing someone talk about saving the planet “for our children, and our children’s children”. At the time, this felt like an abstract notion; even a rather sanctimonious platitude. Now that I have children and step-grandchildren of my own, who may well live to witness the 22nd century, that phrase feels far more relevant and urgent. If the worst predictions for the climate emergency come true, with devastating consequences for human and animal life on this planet, then I do not envy them living that long. So, much as I enjoy seeing cattle egrets on my local patch, or catching up with a new species of butterfly on the south coast, my pleasure is more than ever tinged with concern. My fear is that these pioneering colonists are not something to be celebrated, but a phenomenon to warn us of impending disaster in this new age of extinction. • Stephen Moss is a naturalist and author based in Somerset, where he is president of the Somerset Wildlife Trust; he also runs an MA in travel and nature writing at Bath Spa University"
"Hurricane Irma passed directly over the tiny Caribbean island of Barbuda in September 2017. Irma was the fifth strongest hurricane ever recorded in the Atlantic, and it reached peak intensity just before landfall, when 180mph winds damaged almost every structure on the island, flattening many of them. Just days later, a mere “normal” hurricane, Jose, also passed over Barbuda. By that point, almost all of the island’s 1,600 or so inhabitants and tourists had been evacuated to nearby Antigua. Yet the prime minister of Antigua and Barbuda (the two islands jointly form a sovereign state), Gaston Browne, had an unusual reaction to the catastrophe. His first legislative response was not to set out a reconstruction plan or to provide funds for housing and essential services. Instead, he focused on land reform.   The proposed Barbudan Land Management (Amendment) Act, announced by the prime minister right after the hurricanes and still being debated in parliament, aims to introduce individual property rights on the island for the first time since its colonisation by the English in the 17th century.  Barbuda’s system of communal land ownership has been in place since slavery was abolished in 1834. Citizens do not own the land but have the right to use it after applying to a locally-elected council. As one council member put it: “A cleaner can apply for beachfront property and get it, and so can a doctor. So there’s no great inequality in Barbuda.” Leases are possible with the approval of the cabinet and the consent of the majority of the people, but what matters most is that each Barbudan has a right to a plot for a house, a plot to farm, and a plot for commercial enterprise. This scenario has not avoided tensions, but has been described by many as egalitarian and just. However, the government sees it as an obstacle to foreign investment or loans. The prime minister appears to have swallowed the idea, linked to the influential Peruvian economist Hernando de Soto, that property rights and foreign investment are the key drivers of growth in developing economies. In this worldview, the island’s reconstruction and development can only be achieved with the intervention of the market, but banks and investors won’t be interested without the guarantee of a clearly identified property title.  The government therefore wants to impose a standardised and uniform property regime. It sees land rights, individual ownership and foreign investments as untriggered opportunities that must be offered to residents and foreigners in order to transform Barbudan land into an asset of the global financial market. Forget public money for reconstruction, forget polluting countries’ responsibilities for climate change and forget the people’s right to be compensated for ecological and historical debts. In this view, the future of the island depends on individual debts and wealthy tourists.  All over the world, the idea of awakening sleepy capital through land reform and privatisation has entrenched inequality and concentrated land in fewer and fewer hands – even in places not affected by catastrophes and climate change. So its adoption in a situation like Barbuda raises further concerns. For example, some on the island have accused the government of taking advantage of the hurricanes to implement a shock doctrine that will favour local and international elites. The current system is egalitarian, they claim, while a market for property would inevitably gentrify and separate the community. To pick one high profile example, the backers of a proposed luxury resort – including Hollywood actor Robert De Niro – have been accused of exploiting the post-hurricane chaos to carry out a “land grab”. Other locals have raised the issue of legitimacy: reform is being implemented without proper consultation and while most people are not even living on the island. This may make it harder for them to claim land or even a mere economic reimbursement.  So who is set to benefit? The banks, for a start. Empirical studies from many different countries have demonstrated that land titling does not guarantee access to credit – people may end up “owning” land only to find it is soon repossessed by a bank or public authorities. It seems likely that Barbudans themselves will get squeezed by a growing appetite for their land and deprived of real control. At best, reform will still mean a very unequal distribution of properties, with elites concentrated along the best beaches and “quasi-slums” arising elsewhere. At worst, Barbudan land reform could lead to an island almost entirely owned by banks. The situation is typical of what happens when land is transformed from a common good into private property. Once Barbudan land becomes a global commodity, enjoyed by tourists but mainly by the investors behind the resorts, those investors will demand unconstrained, cheap and formalised access to their land.  And what of informal rights, traditions, mandatory consultations and the link between a people and the island? Some will see them as obstacles to maximising a return on their investments. Unfortunately, these same obstacles are exactly what allowed Barbudans to construct a unique society where access to land for housing and agriculture is a right not a privilege."
"
Share this...FacebookTwitterNew York Times journalist Erica Goode misses a mountain of polar bear research, instead lets herself get swept up by alarmist polar bear activism.
The New York Times recently published an article penned by Erica Goode on the controversial Harvey et al paper, where 14 scientists (sophomorically) attacked polar bear researcher Susan Crockford and climate science skeptics.
If the Harvey publication makes anything clear, it is that its authors are deeply frustrated by the large share of the public who reject their alarmist climate science. But instead of looking at themselves and the mountain of blunders they have made in the past to see what they could improve, the Harvey scientists chose to lash out and blame their woes on mean-spirited “deniers”. The inconvenient reality, however, is that alarmist climate and polar bear science (and journalism) has not been clean, and at times it’s been outright sloppy, deceptive and shrill. That’s the real big reason skeptics have been so successful.
Sloppy biased journalism
So it is no surprise that Erica Goode at the New York Times sided up with the 14 scientists of the Harvey publication to attack the so-called climate “denialists” in her most recent article. Unfortunately Goode made the fatal journalistic error of failing to keep a healthy distance from the alarmist side and as a result was blinded from seeing the glaring mountain of scientific research showing polar bears are in fact doing fine.
As a result Goode’s work couldn’t have been sloppier.
 A mountain of recent scientific publications gets missed
The reality is that there are many polar bear scientists out there who have produced a considerable body of recent scientific findings, which show that the polar bear populations are in reality stable or even thriving. How could Goode have missed it?
Whatever the reasons, it appears to be to a classic case of journalistic negligence.
Had the seasoned New Times journalist done just the minimum of research one expects of even a beginner journalist, she would have discovered, for example, two very recent papers on polar bears published in the journals Ecology and Evolution and Polar Record, and many others. According to expert polar bear scientists (other than Dr. Susan Crockford) there is no evidence to support recent claims polar bears as a species are in grave danger due to climate change and thinning sea ice.
Somehow Goode allowed herself to be talked into the absurd idea that Susan Crockford is the only skeptic polar bear scientist out there, and so did not bother to check for others, so it seems. And the only crises we find are those from dubious computer-modelled 2050 scenarios.
1) York et al 2016
One scientific publication by York et al in 2016 found that given the paleoclimate record of a much warmer (+4 to + 7.5 °C) Arctic, there was much more reduced sea ice thickness and extent in the past relative to today. They concluded: “it seems unlikely that polar bears (as a species) are at risk from anthropogenic global warming.”
The authors wrote in their summary:
Considering both [observations from native populations] and scientific information, we suggest that the current status of Canadian polar bear subpopulations in 2013 was 12 stable/increasing and one declining (Kane Basin).”
We do not find support for the perspective that polar bears within or shared with Canada are currently in any sort of climate crisis.”
Why didn’t Goode contact these scientists and present their results? There are many other scientists who share Crockford’s view.
2) Wong et al 2017
Another published scientific paper by Wong et al., 2017, “Inuit perspectives of polar bear research: lessons for community-based collaborations”, the authors investigated Inuit observations. Here’s an excerpt of their findings:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Wong, a researcher at the Department of Ecology and Evolutionary Biology, University of Toronto, and her team also found in early 80s, and mid 90s: “there were hardly any bears” and “there’s too many polar bears now”.
Also they noted: “Bears foraging for land-based foods have been reported in the literature prior to recent concerns over climate change (Russell 1975; Derocher and others 1993; Gormezano and Rockwell 2013a).” Also: “Observations of bears consuming garbage are not uncommon (Russell 1975; Lunn and Stirling 1985; Gormezano and Rockwell 2013b)”
More fuel for skeptics
One has to wonder if the activist Harvey team of scientists and the New York Times live in an alternative universe. It is precisely that kind of gross omission and one-sidedness that has been fuelling the skeptics over the years.
And there’s much more that they ignored.
Laforest et al., 2018
A publication by Laforest et al titled Traditional Ecological Knowledge of Polar Bears in the Northern Eeyou Marine Region looked at the perception of communities in Quebec on the prevalence of problem polar bears. Results: One-third of participants reported that polar bears will be unaffected by, or even benefit from, longer ice-free periods. A majority of participants indicated that the local polar bear population was stable or increasing.
Moreover they cited the fact that polar bears are capable of hunting seals in open water as a factor contributing to the stable body condition of the bears. and that none of the participants explicitly linked the effects of a warming climate to specific impacts on polar bears.
The publication also states that a recent aerial survey of the Southern Hudson Bay subpopulation found that the abundance of polar bears has remained steady since 1986 (943 bears; SE: 174) (Obbard et al., 2015).
11 more recent papers show bears survive without ice
Not long ago Kenneth Richard reported on almost a dozen papers showing that polar bears easily survived ice-free and far warmer conditions than those seen today or those expected by mid century.
Even more research shows that polar bear population is up 42% since 2004.
Russia “scientists know little or nothing” 
Goode’s non-researched article also mentions that “scientists know little or nothing” about the situation in Russia and other remote areas (and so it’s got to be bad?). If it is unknown, then how can one be either rationally alarmed or relieved about the situation there? Yet, given the positive situation from Canada and Alaska, there is no rational reason to assume all is bad in Russia.
New York Times’ image of bias
So what can we take home from this? Why did Goode ignore so much polar bear research, and why has she unconditionally lapped up everything handed down to her by the alarmist clique? We can only speculate it’s about activism.
Erica Goode and New York Times again shot themselves in the foot on this one and reaffirmed their reputation for bias.
Had Goode resisted getting distracted from the “us” versus “them” narrative and actually dug a little into the actual scientific results –  and the scientists behind them – like honest journalists do, she would not have produced such a piece of journalism.
Share this...FacebookTwitter "
"Just six months into his presidency, Trump announced America’s withdrawal from the Paris climate accord – making the US, the world’s second-largest emitter of greenhouse gases, one of the only countries on earth that does not participate.  Trump has also gutted the Environmental Protection Agency, forced out or muzzled scientists whose research contradicts the administration’s political preferences, and appointed industry insiders to federal agencies. He has rolled back Obama-era emissions standards, massively expanded the federal land and water available for oil and gas drilling, and propped up the dying and environmentally ruinous coal industry. In only three years, Trump has appointed 185 judges – a staggering re-molding of the federal judiciary. (By comparison, Obama appointed 55 appeals judges in eight years.) Even if Trump loses re-election in 2020, the judges he has appointed will remain in office for decades. Rightwing activist groups have seized the opportunity to push new restrictions on abortion rights, including state abortion bans unprecedented since Roe v Wade. Ohio legislators even attempted to pass a bill that would force doctors to re-implant ectopic pregnancies, a dangerous procedure with no medical justification. Many of these radically anti-abortion laws are explicitly designed to test Roe v Wade, and there is a strong chance Roe could come up for review in the US supreme court, where liberal justices are now a minority. Healthcare policy has come under continual challenge, with the Trump administration attempting to dismantle or roll back many parts of the Affordable Care Act, better known as Obamacare. Trump kicked off his presidency by banning immigration from several Muslim countries; he has also separated migrant children from their parents, escalated Ice deportations, and worked to sideline immigration courts. One of Trump’s advisors, Stephen Miller, is suspected to be an outright white nationalist. Thanks in part to aggressive gerrymandering, the Republican party has consolidated control of state legislatures and governorships across the US. These lawmakers’ ultra-conservative agendas are often wildly unrepresentative of their more moderate, and increasingly diverse, populations. The past several years have seen a dangerous rise in voter suppression, a trend the Guardian is documenting in our ongoing special series on voting rights. Under now-governor Brian Kemp, Georgia purged hundreds of thousands of voters from its rolls before the 2018 midterm elections. Wisconsin is currently on the brink of purging more than 200,000 voters – in a state where Trump won by fewer than 23,000 votes. Despite an increasing bipartisan consensus on the need to reform the criminal justice system, the Trump administration has often promoted ineffective, backwards, and punitive criminal justice policies. Under former attorney general Jeff Sessions, the Department of Justice largely abandoned oversight of troubled police departments operating under federal consent decrees. He also worked to undo state-based marijuana legalization and drug law reform. Trump habitually denigrates US allies, the Nato military alliance, and his own intelligence agencies, while praising dictatorships including Russia, China, North Korea and Saudi Arabia. Polls show global opinion of the US, and Trump’s leadership, at de-moralized levels. In December, the Trump administration announced new work requirements that will strip 700,000 Americans of food stamps. We profiled some of the people who will be affected. Secretary of education Betsy DeVos, an ardent critic of public education, has loosened Obama-era restrictions on predatory for-profit colleges. She’s also fought attempts to introduce debt relief for the millions of Americans who owe a collective $1.6tn in student loans. Since Trump took power, the Guardian has been a constant and rigorous watchdog. We commit enormous resources to our investigative and public interest reporting. In all our journalism, we also work to give voices to the human beings affected by Trump’s policies. Next year America will face a momentous choice. The future of the White House, the supreme court, abortion rights, climate policy and a range of other issues are in play – at the same time that misinformation makes rigorous reporting more important than ever. That’s why robust, fact-based and independent reporting is critical: we hope to raise $1.5m to fund our journalism in 2020. With your help, we’ll continue to fight for the progressive values we hold dear – democracy, civility, truth. Please consider making a contribution. And as always, thanks for reading."
nan
nan
"Australia’s bushfire crisis started much earlier than normal in August 2019, with thousands of fires in Queensland and New South Wales. Despite the evidence a claim persists that a major contributing factor of Australia’s devastating fire season – and the deaths, loss of homes and environmental devastation they have caused – is not climate change but a conspiracy by environmentalists to “lock up” national parks and prevent hazard reduction activities such as prescribed burning and clearing of the forest floor. On Saturday the prime minister, Scott Morrison, said after visiting fire grounds: “The most constant issue that has been raised with me has been the issue of managing fuel loads in national parks.” He claimed that people “who say they are seeking those actions on climate change” could also be the same people who “don’t share the same urgency of dealing with hazard reduction”. Prof David Bowman, the director of the fire centre research hub at the University of Tasmania, said: “It’s ridiculous. To frame this as an issue of hazard reduction in national parks is just lazy political rhetoric.” On Sunday Morrison said he wanted to know “what the contribution of issues” around hazard reduction were, but repeated that it had been an issue raised often with him. He also said “without the planning, without the preparations” of state agencies, “I fear what has really been a terrible tragedy would have been far worse.” Hazard reduction is the management of fuel and can be carried out through prescribed burning, also known as controlled burning, and removing trees and vegetation, both dead and alive. Hazard reduction is carried out by fire authorities, national park staff and individual property owners who can apply for permits to clear areas around their buildings. Coordination of activities happens through local bushfire management committees. There are 120 committees in NSW. The claim of a conspiracy by environmentalists to block hazard reduction activities has been roundly rejected by bushfire experts, and experts say it is betrayed by hard data on actual hazard reduction activities in national parks. Prof Ross Bradstock, the director of the centre for environmental risk management of bushfires at the University of Wollongong, has previously told Guardian Australia: “These are very tired and very old conspiracy theories that get a run after most major fires. They’ve been extensively dealt with in many inquiries.” Former fire chiefs who have been calling strongly for action on climate change, and who have been trying to meet Morrison for months, have also been calling for increased funding for hazard reduction. The Australian Greens say they want “an effective and sustainable strategy for fuel-reduction management that will protect biodiversity and moderate the effects of wildfire for the protection of people and assets, developed in consultation with experts, custodians and land managers”. A federal government factsheet on bushfire management outlines how state agencies and people can carry out a range of hazard reduction activities that have been exempted from national environmental law, even if they “have the potential to have a significant impact on nationally protected matters”. In the last full fire season of 2018 and 2019, the National Parks and Wildlife Service in NSW told Guardian Australia it carried out hazard reduction activities across more than 139,000 hectares, slightly above its target. There are two major restricting factors for carrying out prescribed burning. One is the availability of funds and personnel, and the second is the availability of weather windows. The 2018-19 annual report of the NSW Rural Fire Service says: “The ability of the NSW RFS and partner agencies to complete hazard reduction activities is highly weather dependent, with limited windows of opportunity. Prolonged drought conditions in 2018-19 adversely affected the ability of agencies to complete hazard reduction works.” The RFS said 113,130 properties had been subject to hazard reduction activities, which was 76% of its target. The 199,248ha covered was 106% of its target. A former NSW fire and rescue commissioner, Greg Mullins, has written that the hotter and drier conditions, and the higher fire danger ratings, were preventing agencies from carrying out prescribed burning. But as well as climate change narrowing the window to carry out prescribed burning, Mullins said some fires have become so intense they have burned through areas that had been subject to hazard reduction. Mullins has been fighting fires in NSW for months. Speaking to the ABC on Friday, he said he witnessed a fire in Grafton in an area that had burned only two weeks previously, but “the burnt leaves were burning again”. He said: “There has been lots of hazard reductions done over the years – more by national parks than previous years – but the fires have burned through those hazard reduction areas.” Mullins dismissed suggestions that the bushfires were down to “greenies” preventing hazard reduction activities.“This is the blame game. We’ll blame arsonists, we’ll blame greenies,” he said. “When will the penny drop with this government?” The National Parks Association of NSW’s president, Anne Dickson, has also responded to the attacks on environmentalists. In November 2019, she said: “The increasing intensity and frequency of fire is one of the greatest threats to biodiversity and natural landscapes. It may be politically expedient to pretend that conservationists exercise some mythical power over fire legislation and bushfire management committees, but it is not so. “Such wild and simplistic claims avoid the very real and complex challenges of protecting our communities and the healthy environments that support our quality of life.” Bowman said that separate to the “lazy political rhetoric” of blaming environmentalists, there should be an examination of the benefits and limitations of hazard reduction. But he said there was also a reality to consider: “A lot of people are thinking that hazard reduction burning stops fire. It doesn’t, but what it does do is to try and change its behaviour. “But let’s say you embarked on the biggest fire reduction program the world has ever seen. What’s the budget for that? Who will pay for it. Of course there is a place for hazard reduction but if you have massive increases, where does the money come from? The reality is that you can’t treat everything.” The 2019-20 bushfire crisis coincided with Australia’s hottest year on record. On a state level, NSW easily experienced its hottest year, with temperatures 1.95C above the long-term average, beating the previous record year, 2018, by 0.27C. Climate experts have said not all of that heat came from climate change, as two climate systems were also working to push up temperatures and fire danger. Fire authorities are guided on a daily basis on the risk of fires through the Forest Fire Danger Index, a combined measure of temperature, humidity, wind speed and the availability of dry fuel. Spring 2019 had been the worst year on a record going back to 1950 for bushfire risk. A 2017 study of 67 years of FFDI data found a “clear trend toward more dangerous conditions during spring and summer in southern Australia, including increased frequency and magnitude of extremes, as well as indicating an earlier start to the fire season”. A study of Queensland’s historic 2018 bushfire season found the extreme temperatures that coincided with the fires were four time more likely because of human-caused climate. On Sunday Morrison claimed the government had “always made this connection” between climate change and impacts on Australia’s weather. Advice shared with authorities around the country earlier this year from the National Environmental Science Program said: “These trends are very likely to increase into the future, with climate models showing more dangerous weather conditions for bushfires throughout Australia due to increasing greenhouse gas emissions.” There are also fears that large pulses of carbon dioxide emissions from Australia’s bushfires may not be reabsorbed through regrowth of forests as they have in the past. The fire season has seen several reports of bushfire-generated thunderstorms. Guardian Australia has reported that 2019 would likely be a “stand-out” year for storms known as “pyroCBs” that generate their own lightning and influence the atmosphere at heights of up to 15km. A study in 2019 published in the journal Scientific Reports found that adding more greenhouse gases to the atmosphere would create more dangerous conditions favourable to pyroCB events in the future, particularly for the southern parts of Australia."
"
Share this...FacebookTwitterIn the wake of Category 5 hurricanes Irma and Harvey, Dr. Sebastian Lüning and Prof. Fritz Vahrenholt presented an analysis of what’s behind the hurricane activity and literature at their well known Die kalte Sonne climate website. Their hope is to bring the hurricane discussion back to some rationality.
The German media of course have been covering the story quite intensely, and at times hysterically. The general tenor of most statements: Hurricanes are not directly caused by climate change, however their power and destructiveness is increasing due to global warming.
The claim is that warmer oceans are providing the fuel to produce larger hurricanes.
As plausible as the theory may sound, Vahrenholt and Lüning decided to investigate the category 4 and 5 hurricanes and plotted their frequency over the past 100 years:

Fig. 4: The number of category 4 and 5 hurricanes between 1924 and 2016
There were quite a number of hurricanes in the 1930s and 1950s, as well as in the 2000s, but the trend has been sharply downward since 2010, despite the warming, and so considerable doubt swirls surrounding all the claims heard in the media.
No correlation found between man and hurricanes
Vahrenholt and Lüning looked at some scientific literature on hurricanes. For example a 2014 paper by Holland et al attempted to show man’s impact on hurricanes. Unfortunately the authors went back only to 1975, and produced the following plot:

Fig. 5: The dependency of the share of Cat.4-5 storms on modelled temperature rises (ACCI) in different oceans, green represents the Atlantic region, red is for the Indian Ocean, and blue for the Pacific. Source: Fig. 5b from Holland et.al (2014).
Even using the data from the carefully selected 1975 to 2011 period does not produce any significant trend, Vahrenholt and Lüning note. Moreover the two German analysts say Holland relied on too few data points coming from the Indian ocean and falsely applied them to claim a “global” trend.
Using the great number of typhoons in the Pacific for the carefully chosen period yielded absolutely no correlation (R² = 0.03). Vahrenholt and Lüning add:
Adding in earlier data also leads to a collapse in correlation for the Atlantic, as the paper only sees a man-made share first starting in 1960. Here the a carefully selected period was sought out and found.”
Decadal variability in hurricane energy and the literature shows an influence by the AMO. A paper Kevin J.E. Walsh of the University of Melbourne tells us just how difficult it is to get understand hurricane strength:
However, the Atlantic basin is noted for having significant multidecadal variability in TC (Tropic Cyclons, d.A.) activity levels. The basin was characterized by a more active period from the mid-1870s to the late 1890s as well as the mid-1940s to the late 1960s. These periods may have had levels of activity similar to what has been observed since the mid-1990s.”
No evidence of a link
“Using the trends from the 1975…2011 period to infer a powerful anthropogenic impact of the recent powerful Atlantic events in light of what we know, borders on sheer audacity,” Lüning and Vahrenholt write. “Apparently the claimed evident relationship between man-made climate change and strengthening hurricanes is not supported.”
Hurricanes driven by Passat winds
Vahrenholt and Lüning cite a new paper  to explain what impacts the energy of a hurricane. Mark A. Saunders of Great Britain and the USA diligently examined observations going back to 1878 and discovered a factor that describes the energy of a hurricane very well: the strength of the northern Passat winds.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Fig 6: Correlation (r, blue) and its significance (p<0.1 is highly significant, red) on the hurricanes energy (ACE), -solid blue curve – and its number -dotted blue curve – with the Passat winds. Source: Fig. 3a from Saunders et.al (2017).
Driven by temperature differences between regions
A second related factor improves the correlation further: The temperature difference between the Main Developing Region (MDR) located within 10° – 20°N and 85°W – 20°W and the global tropical area within 10°S to 10°N. It is long known that hurricane development during times of El Nino is dampened and during La Nina it is enhanced, thus it has to do much more with natural oceanic variability.
Figure 8 below depicts the difference in sea surface temperature (SST) between the main developing region (MDR) and the tropics using observations ERSSTv5, with 10-year smoothing applied. 

Fig. 8: The black line is not the horizontal axis, rather its is the linear trend! One observes the AMO similar pattern.
Lüning and Vahrenholt also cite literature showing that the Passat winds will not increase with climate change, but rather indicate a decrease in hurricanes.
All science that seriously looks at hurricanes show no worsening of hurricanes being caused by climate change.”
And what about the thermodynamics of greater evaporation leading to more hurricane energy? A report by Friederike Otto of Oxford finds that there are many possible interactions involved in this highly complex weather phenomenon:
Dynamical factors and thermodynamic aspects of climate change can interact in complex ways and there are many examples where the circulation is as important as the thermodynamics.”
Otto also points out that the climate models are far from adequate:
But in practice this requires climate models that are able to reliably simulate the weather systems in questions over and over again to assess the likelihood of its occurrence.”
Some media outlets have responsibly pointed out that the problems and destruction caused by hurricanes have much more to do with people living in hurricane vulnerable areas.
Sea level rise not a real factor in hurricane flooding
The claim that rising sea levels (10 cm since 1960) caused by global warming is major factor in hurricane destruction is also a non-factor in view of the fact that hurricanes generate waves that are 6 meters high!
Big driver: SST difference between MDR and tropics
In summary, the real hurricane driver of hurricanes is the SST difference between the MDR region and the global tropics. The following graph tells us why current hurricane activity is so high.

Fig. 9: The current sea surface temperature difference between the MDR and the global tropical oceans, source: Tropical Tidbits.
People living in hurricane vulnerable areas need to hope that the curve soon returns to zero. Here and with the bPassat winds doe we find the real reasons for the terrible hurricanes.  Every thing else is propaganda on behalf of a “good cause”
 
Share this...FacebookTwitter "
"The night of New Year’s Eve was a kind of slow torture. I spent it in Vincentia on the New South Wales south coast, lying with my seven-year-old son, unable to breathe or sleep as thick brown smoke filled the house. As I listened to my child coughing, calling out from the depths of his nightmares words that sounded like “don’t die”, I prayed for the Princes Highway to reopen. I write this not because I believe for one minute that what I went through compares to anything people further south on Australia’s east coast experienced that night, or will this weekend, but because friends and family who live outside the region – whether in Sydney or other, unaffected parts of the country, such as Perth – asked me to. They simply cannot believe it when I tell them what horrors are unfolding down there. There is a way to understand, I tell them – tune into ABC Illawarra local radio. Then you’ll hear about the power outages, the food, fuel and water shortages, the stories of locals bringing water to families with children and babies trapped in cars fleeing Ulladulla and its surrounds, of supermarket staff at Milton guiding shoppers around darkened stores with head torches to find dried food because the refrigerated goods have all spoiled. But in all likelihood they won’t, so let me tell you what it was like, the final day of 2019, for my family. We were at a village on Jervis Bay, where we had planned to see in 2020 with my brother-in-law and his wife. We knew there was fire around but we didn’t understand the threat – who did? Everything changed at precisely 1.57pm, when the NSW Rural Fire Service emergency bushfire warning text message came through on our mobile phones – “people nth of Ulladulla & in Bay & Basin & Nowra areas – seek shelter as fire arrives”. “Where is Bay & Basin?” asked my sister-in-law, who is from Melbourne. “That’s here, where we are,” my husband said. They were at the shops, buying food for our celebration dinner, a meal we never cooked, as the night was spent formulating a fire plan, getting ready to evacuate, via the sea in kayaks if it came to that. It was hard to keep a level head as our hearts raced, our panic escalated, as ash fell, the sky blackened and night fell hours before actual sunset. Local ABC radio was our lifeline – and surely, after this summer, the federal government will increase the ABC’s funding? – and it was how we learnt the fire had reached Sussex Inlet on the southern side of St Georges Basin (12km away), where we heard that power and telecommunications would be lost during the night, that people in Lake Conjola were jumping into the lake to escape raging fires (30km away). What did we do? We gathered up woollen blankets, towels and bottled water and placed them outside on the deck. We assigned lifejackets (we had enough for the women and child). We opened all the gates, pulled out the kayaks. We filled the bath, readied the hoses. We dressed in the most appropriate clothes we had, inadequate though they were. We bought batteries from the service station for the radio and torches (the local Coles closed an hour after the RFS text message came through). We found Ziploc bags for our mobile phones, in case they fell into the sea. My husband and his brother organised shifts to stay awake throughout the night, to monitor the radio and sky either side of the house for the orange glow that we hoped would never come. Our plan, if it did, was to don lifejackets, take the women, child and dogs to the beach, and then the men would come back for the boats. Meanwhile my son clutched his little bag of crystals and two small soft toys. His uncle and aunt tried to distract him with a card game but, by 8pm, he was holding his hands to his ears, begging us to turn off the radio – the emergency warnings were upsetting him so – and that’s when I took him to bed. About 1am the RFS dropped the threat level to our area and, when in the morning the Live Traffic App informed us the road to Sydney had reopened, we threw our things into the car and fled. I feel guilty being so safe in Sydney, while the locals we left behind stay to defend their houses and their lives. Politics has to change forever as a result of this summer because our country has – there is simply no turning back now. We, on Australia’s east coast (and beyond) have seen what the consequences of not acting on climate change look like and it is terrifying. How many people will have died and lost their houses when this immediate crisis has passed? How many farmers will have lost their livelihoods? How many animals will have perished? How many species will have been brought closer to the brink of extinction? How many children will have lost their innocence? I am haunted by Cormac McCarthy’s The Road, a book I read only a year ago, never imagining how soon I would see photos in the Australian media of the post-apocalyptic world he described, of refugees fleeing blackened landscapes, parents leading their children through the smoke, carrying their belongings in their arms, hoping for rescue on military convoys. We have no use for politicians who continue to pursue agendas that ignore the reality of our warming climate, that place our suffering planet in ever-greater jeopardy. It is time for a new generation of leaders to stand up. • Cynthia Banham is a Sydney-based author and journalist"
"As a conservation professor I believe people need to understand why protecting nature matters to them personally. Appealing to human self-interest has generated support for conservation in Switzerland, for example, where the government protects forests partly because they help prevent landslides and avalanches, or among communities in Botswana which conserve wildlife partly because of the value of trophy hunting. But this understanding risks being obscured by unhelpful arguments over terminology. The story starts in 2005, when the Millennium Ecosystem Assessment was published. This document, the result of five years work by more than 1,300 scientists around the world, demonstrated beyond doubt that global ecosystems were in decline and that this really mattered. Perhaps its most significant legacy was a diagram which presented the ways human wellbeing is influenced by different categories of what it termed “ecosystem services”. For example, maintaining healthy seas is important because of the “provisioning services” provided to fishing communities, while mangrove forests may provide “regulating services” protecting people from coastal storm surges. The ecosystem services idea has since been hugely influential in mainstream politics and there are now degree courses, textbooks and whole journals framed around the concept. Jump forward 13 years and another global scientific effort has produced another conceptual framework. This time it’s IPBES: the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. In a high-profile article senior scientists representing its consortium of 129 countries have replaced the term ecosystem services with what they argue is a new concept: “Nature’s Contributions to People”.  IPBES was one of the global initiatives set up in response to the success of the ecosystem services concept, so for scientists there to reject the term has caused quite a debate. There are two questions to answer: Is the concept of Nature’s Contributions to People substantially new? And, secondly, is it helpful? First, is it new? Proponents of the Nature’s Contributions to People concept argue that while the ecosystem services idea was readily taken up by ecologists and economists, it has failed to engage a range of perspectives from the social sciences and humanities. For them, there is too much emphasis on services which are easy to quantify, such as the value that insects contribute to agriculture through pollination and pest control (US$57 billion each year in the US alone, apparently). This has resulted in some world views being sidelined in policy debates. It is certainly true that a number of South American governments strongly dislike the concept of ecosystem services which they consider commodifies what are better seen as gifts from “mother earth”.  My sister Katherine Jones works in communications for RSPB Scotland. She agrees that while the term “ecosystem services” can be useful in discussions with British policy makers, it has never resonated with the general public. “When talking to ordinary people”, she told me “it is much more effective to appeal to their innate passion for nature than suggesting that nature provides a service, like a utility company or a bank”. However, the editor of the journal Ecosystem Services responded to the IPBES publication with a scathing editorial in which he argues convincingly that far from being new, Nature’s Contributions to People is simply a non-technical explanation of the same thing. He, and others, suggest that in trying to mark clear water between the two, the authors of the latest paper are wilfully ignoring both the large body of work which addresses issues such as commodification, and the success of ecosystem services in generating political interest in the environment. The second question concerns whether it is helpful. Prominent conservation scientist Kent Redford, and colleagues, pre-empted the recent debate when they pointed out some years ago that conservation suffers repeatedly from fads. Concepts or approaches are enthusiastically promoted for a few years then dropped only for a new concept to be introduced – which looks substantially like the old one but with a snappy new name. The risk they highlight is that by regularly rejecting, reinventing and repackaging approaches, the conservation community fails to learn the lessons from the failures of previous approaches as they view the new concept as so completely new that old issues don’t apply.  If Nature’s Contributions to People can help bring more actors to the table and address some of the limitations of ecosystem services, this will be helpful. However, problems and challenges will remain. Take the feeling of wellbeing many people get when they connect with nature, for example. “Cultural services” such as these have often been given less prominence because they are difficult to value – but it is not clear whether framing the issue around Nature’s Contributions will help solve this.  If you have read this far, then you may be wondering why this rather semantic argument should interest you. However, I would suggest that there are few issues more important than communicating to society at large why nature matters. If we need a new concept to keep this point fresh and alive in the minds of politicians and the general public then so be it. But let’s not argue. To paraphrase the classic number sung by Fred Astaire and Ginger Rogers: you say “Ecosystem Services” and I say “Nature’s Contributions to People”. The point made by both is that destroying nature ultimately harms us all."
"‘Good morning. Here is the shipping forecast for midday, 21 June, 2050. Seas will be rough, with violent storms and visibility ranging from poor to very poor for the next 24 hours. The outlook for tomorrow is less fair.” All being well, this could be a weather bulletin released by the Met Office and broadcast by the BBC in the middle of this century. Destructive gales may not sound like good news, but they will be among the least of the world’s problems in the coming era of peak climate turbulence. With social collapse a very real threat in the next 30 years, it will be an achievement in 2050 if there are still institutions to make weather predictions, radio transmitters to share them and seafarers willing to listen to the archaic content. I write this imaginary forecast with an apology to Tim Radford, the former Guardian science editor, who used the same device in 2004 to open a remarkably prescient prediction on the likely impacts of global warming on the world in 2020. Journalists generally hate to go on record about the future. We are trained to report on the very recent past, not gaze into crystal balls. On those occasions when we have to venture ahead of the present, most of us play it safe by avoiding dates that could prove us wrong, or quoting others. Radford allowed himself no such safe distance or equivocation in 2004, which we should remember as a horribly happy year for climate deniers. George W Bush was in the White House, the Kyoto protocol had been recently zombified by the US Congress, the world was distracted by the Iraq war and fossil fuel companies and oil tycoons were pumping millions of dollars into misleading ads and dubious research that aimed to sow doubt about science. Radford looked forward to a point when global warming was no longer so easy to ignore. Applying his expert knowledge of the best science available at the time, he predicted 2020 would be the year when the planet started to feel the heat as something real and urgent. “We’re still waiting for the Earth to start simmering,” he wrote back in that climate-comfortable summer of 2004. “But by 2020 the bubbles will be appearing.” The heat of the climate movement is certainly less latent. In the past year, the world has seen Greta Thunberg’s solo school strikes morph into a global movement of more than six million demonstrators; Extinction Rebellion activists have seized bridges and blocked roads in capital cities; the world has heard ever more alarming warnings from UN scientists, David Attenborough and the UN envoy for climate action, Mark Carney; dozens of national parliaments and city councils have declared climate emergencies; and the issue has risen further to the fore in the current UK general election than any before it. With only weeks to go until 2020, the bubbles of climate anxiety are massing near the surface. Radford’s most precise predictions relate to the science. Writing after the record-breaking UK heat of 2003, he warned such scorching temperatures would become the norm. “Expect summer 2020 to be every bit as oppressive.” How right he was. Since then, the world has sweltered through the 10 hottest years in history. The UK registered a new high of 38.7C this July, which was the planet’s warmest month since measurements began. He also correctly anticipated how much more hostile this would make the climate – with increasingly ferocious storms (for the first time on record, there have been category 5 hurricanes, such as Dorian and Harvey, for four years in a row), intensifying forest fires (consider the devastating blazes in Siberia and the Amazon this year, or California and Lapland in 2018) and massive bleaching of coral reefs (which is happening with growing frequency across most of the world). All of this has come to pass, as have Radford’s specific predictions of worsening floods in Bangladesh, desperate droughts in southern Africa, food shortages in the Sahel and the opening up of the northwest passage due to shrinking sea ice (the huge cruise liner, Crystal Serenity, is among the many ships that have sailed through the Bering Strait in recent years – a route that was once deemed impossible by even the most intrepid explorers). A couple of his predictions were slightly premature (the snows on Kilimanjaro and Mt Kenya have not yet disappeared, though a recent study said they will be gone before future generations get a chance to see them), but overall, Radford’s vision of the world in 2020 was remarkably accurate, which is important because it confirms climate science was reliable even in 2004. It is even more precise today, which is good news in terms of anticipating the risks, but deeply alarming when we consider just how nasty scientists expect the climate to become in our lifetime. Unless emissions are slashed over the next decade, a swarm of wicked problems are heading our way. How wicked? Well, following Radford’s example, let us consider what the world will look like in 2050 if humanity continues to burn oil, gas, coal and forests at the current rate. The difference will be visible from space. By the middle of the 21st century, the globe has changed markedly from the blue marble that humanity first saw in wondrous colour in 1972. The white northern ice-cap vanishes completely each summer, while the southern pole will shrink beyond recognition. The lush green rainforests of the Amazon, Congo and Papua New Guinea are smaller and quite possibly enveloped in smoke. From the subtropics to the mid-latitudes, a grimy-white band of deserts has formed a thickening ring around the northern hemisphere. Coastlines are being reshaped by rising sea levels. Just over 30cm at this stage – well short of the 2 metres that could hit in 2100 – but still enough to swamp unprotected stretches of land from Miami and Guangdong to Lincolnshire and Alexandria. High tides and storm surges periodically blur the boundaries between land and sea, making the roads of megacities resemble the canals of Venice with increasing frequency. On the ground, rising temperatures are changing the world in ways that can no longer be explained only by physics and chemistry. The increasingly hostile weather is straining social relations and disrupting economics, politics and mental health. Generation Greta is middle aged. Their teenage fears of the complete extinction of the human race have not yet come to pass, but the risk of a breakdown of civilisation is higher than at any previous time in history – and rising steadily. They live with a level of anxiety their grandparents could have barely imagined. The world in 2050 is more hostile and less fertile, more crowded and less diverse. Compared with 2019, there are more trees, but fewer forests, more concrete, but less stability. The rich have retreated into air-conditioned sanctums behind ever higher walls. The poor – and what is left of other species – is left exposed to the ever harsher elements. Everyone is affected by rising prices, conflict, stress and depression. This is a doorway into peak climate turbulence. Global heating passed the 1.5C mark a couple of years earlier and is now accelerating towards 3C, or possibly even 4C, by the end of the century. It feels as if the dial on a cooker has been turned from nine o’clock to midnight. Los Angeles, Sydney, Madrid, Lisbon and possibly even Paris endure new highs in excess of 50C. London’s climate resembles Barcelona’s 30 years earlier. Across the world, droughts intensify and extreme heat becomes a fact of life for 1.6bn city dwellers, eight times more than in 2019. For a while, marathons, World Cups and Olympics were moved to the winter to avoid the furnace-like heat in many cities. Now they are not held at all. It is impossible to justify the emissions and the world is no longer in the mood for games. Extreme weather is the overriding concern of all but a tiny elite. It wreaks havoc everywhere, but the greatest misery is felt in poorer countries. Dhaka, Dar es Salaam and other coastal cities are hit almost every year by storm surges and other extreme sea-level incidents that used to occur only once a century. Following the lead set by Jakarta, several capitals have relocated to less-exposed regions. But floods, heatwaves, droughts and fires are increasingly catastrophic. Healthcare systems are struggling to cope. The economic costs cripple poorly prepared financial institutions. Insurance companies refuse to provide cover for natural disasters. Insecurity and desperation sweep through populations. Governments struggle to cope. “By 2050, if we fail to act, many of the most damaging, extreme weather events we have seen in recent years will become commonplace,” warns Michael Mann, the director of the Earth System Science Center at Pennsylvania State University. “In a world where we see continual weather disasters day after day (which is what we’ll have in the absence of concerted action), our societal infrastructure may well fail … We won’t see the extinction of our species, but we could well see societal collapse.” Adding to the anxiety is the erratic temperature of the planet. Instead of rising smoothly it jolts upwards, because tipping points – once the stuff of scientific nightmares – are reached one after another: methane release from permafrost; a die-off of the tiny marine organisms that sequestered billions of tonnes of carbon; the dessication of tropical forests. People have come to realise how interconnected the world’s natural life-support systems are. As one falls, another is triggered – like dominos or the old board game, Mouse Trap. In some cases, they amplify one another. More heat means more forest fires, which dries out more trees, which burn more easily, which releases more carbon, which pushes global temperatures higher, which melts more ice, which exposes more of the Earth to sunlight, which warms the poles, which lowers the temperature gradient with the equator, which slows ocean currents and weather systems, which results in more extreme storms and longer droughts. It is also now clear that positive climate feedbacks are not limited to physics, but stretch to economics, politics and psychology. The Amazon is turning into a savannah because the loss of forest is weakening rainfall, which makes harvests lower, which gives farmers an economic motivation to clear more land to make up for lost production, which means more fires and less rain. On our current course, carbon concentrations in the atmosphere will pass 550 parts per million by midcentury, up from around 400ppm today. Katharine Hayhoe, an atmospheric scientist and director of the Climate Science Center at Texas Tech University, explains how this stacks the odds in favour of disaster. “By 2050, we’d be seeing events that are far more frequent and/or far stronger than we humans have ever experienced before, are occurring both simultaneously and in sequence.” Her greatest concern is that food production and water supply systems could buckle under the strain, with dire humanitarian consequences in areas that are already vulnerable. Generation Greta live with a level of anxiety their grandparents could barely have imagined Hunger will rise, perhaps calamitously. The United Nations’ International Panel on Climate Change expects food production to decline by 2% to 6% in each of the coming decades because of land-degradation, droughts, floods and sea-level rise. The timing could not be worse. By 2050, the global population is projected to rise to 9.7 billion, which is more than two billion more people to feed than today. When crops fail and starvation threatens, people are forced to fight or flee. Between 50 and 700 million people will be driven from their homes by midcentury as a result of soil degradation alone, the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) estimated last year. Fires, floods and droughts will prompt many others to migrate within and across borders. So will the decline of mountain ice, which is a source of meltwater for a quarter of the world’s population. The poorest will be worst affected, though they have the least responsibility for the climate crisis. For the US author and environmentalist, Bill McKibben, this injustice will make the greatest impact in 2050. “Forcing people to move from their homes by the hundreds of millions may do the most to disrupt the world. And, of course, it’s a deep tragedy, because these are precisely the people who have done the least to cause the problem,” he says. In 2050, climate apartheid goes hand-in-hand with increasingly authoritarian politics. Three decades earlier, worried electorates voted in a generation of populist “strongmen” in the hope they could turn back the clock to a more stable world. Instead, their nationalism made a global solution even harder to achieve. They preferred to focus on the immigration consequences of global heating rather than the carbon-capital causes. When voters realised their mistake, it was too late. The thugocracy refused to give up power. They no longer deny the climate crisis; they use it to justify ever-more repressive measures and ever-wilder efforts to find a technological fix. In the past 20 years, nations have tried volcano mimicking, cloud brightening, albedo modification and carbon dioxide removal. Most were expensive and ineffective. Some made weather circulation even less reliable. Powerful countries now threaten rivals not just with nuclear weapons, but with geo-engineering threats to block sunlight or disrupt rainfall patterns. This is not an inevitable future. Unlike Radford’s prediction for 2020, this vision of 2050 factors in human behaviour, which is more volatile and less predictable than the laws of thermodynamics. Many of the horrors above are already baked into the climate, but our response to them – and each other – is not predetermined. When it comes to the science, the dangers can be substantially reduced if humanity shifts decisively away from business-as-usual behaviour over the next decade. When it comes to the psychology and politics, we can make our situation better immediately if we focus on hope in shared solutions, rather than fears of what we will lose as individuals. That means putting faith in institutions, warning one another about risks, and treasuring shared eccentricities and traditions – a bit like the shipping forecast. A storm is certainly brewing. The science is clear on that. The question now is how we face it."
"Snow and glaciers in New Zealand have turned brown after being exposed to dust from the Australian bushfires, with one expert saying the incident could increase glacier melt this season by as much as 30%. On Wednesday many parts of the South Island woke up to an orange haze and red sun, after smoke from the Victorian and New South Wales blazes drifted east on Tuesday night, smothering many parts of the island for most of the day.  On Thursday, pictures taken from the Southern Alps showed the smoke haze carrying particles of dust had tinged snow-capped mountain peaks and glaciers a shade of caramel, with former prime minister Helen Clark expressing concern for the long-lasting environmental impacts on the mountains. “Impact of ash on glaciers is likely to accelerate melting,” Clark tweeted. “How one country’s tragedy has spillover effects.” There are more than 3,000 glaciers in New Zealand and since the 1970s scientists have recorded them shrinking by nearly a third, with current estimates predicting they will disappear entirely by the end of the century. Professor Andrew Mackintosh is head of the school of earth, atmosphere and environment at Monash University, and the former director of the Antarctic Research Centre. He said in nearly two decades of studying glaciers in New Zealand he had never seen such a quantity of dust transported across the Tasman, and the current event had the potential to increase this season’s glacier melt by 20-30%, although Mackintosh stressed this was no more than an estimate. “It is quite common for dust to be transported to New Zealand glaciers, but I would say that the amount of transport right now is pretty phenomenal – I don’t think I’ve ever seen anything like it,” Mackintosh said.“It is concerning to me to see so much material being deposited on the glaciers.” #AUSTRALIANBUSHFIRES pic.twitter.com/7XDjERi71n Mackintosh said the whiteness of snow and ice reflected the sun’s heat, and slowed melting. But when this whiteness was obscured the glacier could melt at a faster rate. The higher glaciers around Mount Cook could likely get more snowfall soon, Mackintosh said, but the lower glaciers may not get another dump till March, and the dust would sit there until then, likely turning pink when algae began to grow. The impacts of the dust event would likely last no longer than a year, Mackintosh said, but if Australia continued to be impacted by extreme wildfires and droughts “it will be one of the factors that is accelerating the demise of glaciers in New Zealand overall”. The recent smoke haze drifting over New Zealand is the fourth such event this summer, the Met Service said, and despite no official health warnings being issued, many with asthma said they were choosing to remain indoors during the unusual conditions. This the view from the top of the Tasman Glacier NZ today - whole South island experiencing bushfire clouds. We can actually smell the burning here in Christchurch. Thinking of you guys. 😢#nswbushfire #AustralianFires #AustraliaBurning pic.twitter.com/iCzOGkou4o The Met Service said most of the smoke remaining over New Zealand would clear by Friday. Early in December travel writer Liz Carlson took pictures of regions of the Southern Alps turning pink following exposure to smoke from Australia early in the bushfire season. In a blog Carlson wrote: “It’s pretty remarkable to see the impact of the fires from so far away.” “Our glaciers don’t need any more battles as they are already truly endangered; it puts the impact of climate change into even more stark reality we can’t ignore.” Hazy sunrises for the North Island today! The main band of smoke has moved north from yesterday, while another band of smoke lingers over the South Island. ^Tahlia pic.twitter.com/eafnnsu89q Residents in Auckland and some parts of the North Island woke to an unusually bright orange sun on Thursday, thought to be a result of the bushfires 2,000km across the Tasman sea. The Ministry of the Environment has been contacted for comment."
"
Share this...FacebookTwitterGreen energy opposition becoming formidable force in Germany
As Germany’s established CDU and SPD “mainstream” parties find themselves imploding, the smaller parties who oppose Germany’s out-of-control Energiewende (transition to green energies) are rapidly becoming a formidable force and making their presence felt in Germany’s national parliament like never before.
For example Germany’s FDP Free Democrats, who refused to forge a coalition government together with CDU/CSU and Green parties, have become increasingly vocal critics of Germany’s green energy scheme.
Politicians ignoring the concerns of its citizens
Last month in her first speech ever in the German Parliament, FDP parliamentarian Sandra Weeser slammed the struggling Energiewende and the latest signals to promote it even further.

In her speech Weeser points out that despite the rapidly growing green energy capacity being installed, the effort to reduce CO2 has failed, and what’s left is an unpredictable power grid that often produces energy when it is not needed (waste energy) and thus costing Germans hundreds of millions annually.
She also accuses the established politicians of ignoring citizens as they ruin Germany’s landscape with wind parks.
Interestingly it is often Green party voters who we find themselves among wind park protesters. In their daily lives these people are recognizing that what is being sold as green electricity in fact has nothing to do with being green. They are rejecting the industrial turbines in forests.”
Weeser then tells that the expansion of the green energies is totally out of proportion with the existing power infrastructure, and that even the most perfect grid will not be able to handle the volatile wind and solar energies.
Electricity “outrageously expensive”
Weeser also dismisses claims by the Green Party that wind energy is “the most inexpensive” on the market, asking them directly: “If that is really true, then why do they need subsidies? Why are we paying 25 billion euros annually for their feed-in?”
Green engineering debacle
Finally she mentions that an array of expert panels have determined that wind energy is not leading to more climate protection, but rather is only making electricity outrageously expensive. In her final comment, Weeser says:
Policymakers should set up the framework conditions, but please leave the engineering to engineers.”
Anti-wind/solar energy AfD soars to 15% in polls


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also Dr. Rainer Kraft of the Germany’s newly minted rightwing AfD party recently demolished the Energiewende in his first speech before Parliament in Berlin:

According to Kraft, the Parliamentary session on renewable energy requested by the Greens is welcome because it exposes their “incapability to comprehend the factual and physical interrelationships” of the subject.
Policy of a fool…eco-socialist economy
Kraft slams the government’s climate-protection approach of spending “15 euros to avoid 1 euro of damage” as apolicy one would expect from “a fool”. Adding: “there just couldn’t be less scientific understanding than that.”
Echoing Donald Trump’s ideas on international treaties, Kraft also sees them as being ruinous to German industry, and that the ultimate target of climate protection is to establish “an eco-socialist centrally-planned economy” and that climate protection is the “instrument” to bring it about.
He then labeled the Greens’ energy policy as “eco-populist voodoo”.
With so much going wrong with the Energiewende, the FDP and AfD today are having an easy time capitalizing politically on the issue and portraying the government and the Greens as inept.
Vocal green energy critics make up 25% of Parliament
According to recent polls, the FDP and AfD now combine to make up a quarter of Germany’s voters. And now that this anti-Energiewende voice is finally being democratically heard in Parliament and viewed by millions on television screens nationally, expect the traditional established parties to continue seeing the unheard of erosion among their disenchanted voter bases. Never has postwar Germany seen a political shift on such a massive scale.
Tipping point
Though 25% may not sound impressive, it is amazing when one considers that only a decade ago there was virtually universal parliamentary support for green energies. Those days are over.
And now as the failure of the Energiewende becomes ever more glaring, reaching the political tipping point on the issue of the Energiewende is just a question of a few more years.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
A new scientific study says surface temperatures in the Northeastern U.S. (Appalachian Mountains) have undergone a significant long-term cooling trend since the early 20th century, complicating the detection of a clear anthropogenic global warming (AGW) signal for the region.
According to Eck (2018), the two coldest Appalachian winters since 1910 were recorded in recent years (2009-’10 and 2010-’11), and 9 of the 10 warmest winters occurred prior to 1960.
In the early 1930s, Appalachian winters were 4.7°C warmer than they have been during the last 30 years (1987-2017).
Several other recently-published papers also reveal a long-term cooling trend not only for the Northeastern U.S. (Eck, 2018), but the Southeastern U.S. (Rogers, 2013; Christy and McNider, 2016), the Central U.S. (Alter et al., 2017), and the Southwestern and Northwestern U.S. (Loisel et al., 2017; Steinman et al., 2016).
In other words, the regions in the continental United States that are less affected by urbanization biases and artificial instrumental heating may not be responding to “global” warming or to the rise in anthropogenic CO2 emissions as climate models have suggested.

Eck, 2018
“[A] majority (12/14) of the regions within the SAM [Southern Appalachian Mountains] have experienced a long-term decline in mean winter temperatures since 1910.   Even after removing the highly anomalous 2009-2010 winter season, which was more than two standard deviations away from the long-term mean, the cooling of mean winter temperatures is still evident.”
“Higher winter temperatures dominated the early 20th century in the SAM [Southern Appalachian Mountains] with nine of the ten warmest winter seasons on record in the region having occurred before 1960.”
“The 1931-1932 winter season, the warmest on record, averaged 8.0°C for DJF [December-February], nearly 4.7°C higher than the 1987-2017 normal mean winter temperature of 3.3°C.”
“Despite the 2016-2017 winter season finishing with the highest mean temperatures (5.7ºC) observed in the SAM [Southern Appalachian Mountains]  since 1956-1957, there have been several years of anomalous negative temperature anomalies, with the 2009-2010 (0.3ºC) and 2010-2011 (1.2ºC) winter seasons finishing as two of the coldest on record for all regions.”



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->







Central U.S. Cooling (-0.35°C) Since 1910
Alter et al., 2017
“In the central United States … observational data indicate that rainfall increased, surface air temperature decreased, and surface humidity increased during the summer over the course of the 20th century concurrently with increases in both agricultural production and global GHG emissions.”
“From 1910- 1949 (pre-agricultural development, pre-DEV) to 1970-2009 (full agricultural development, full-DEV), the central United States experienced large-scale increases in rainfall of up to 35% and decreases in surface air temperature of up to 1°C during the boreal summer months of July and August … which conflicts with expectations from climate change projections for the end of the 21st century (i.e., warming and decreasing rainfall) (Melillo et al., 2014).”
“Thus, it seems that GHG emissions do not contribute greatly to the regional changes in summer climate that have been observed in the central United States.”


Southeastern U.S. Cooling Since 1890s
Rogers, 2013

Christy and McNider, 2016


Long-Term Cooling Trend In The Western U.S.
Loisel et al., 2017

Steinman et al., 2016

Share this...FacebookTwitter "
"You may have missed it among all the talk of minimum wages and welfare cuts, but as part of its summer budget announcements the UK government also abolished the requirement for new homes to be “zero carbon” from April 2016. A commitment in place since 2006 and supported through successive governments, now thrown into a bonfire of supposed “red tape” holding back new building projects and the productivity of the UK economy. It’s an appalling act of policy vandalism. I heard the news on my way back from a big international conference in Paris on Our Common Future under Climate Change, a prelude to the next round of climate negotiations to take place later this year. Listening to speaker after speaker stressing the urgency of climate action – and the attempts by at least some of those present to be optimistic about what might be agreed – I deluded myself into thinking that maybe the carbon question would now be taken seriously, even in the UK with a government demonstrating at every step its now true-blue ideology.  The zero-carbon homes policy was properly ambitious (at least in its original form). It focused on radically reducing the emissions from housing through a combination of energy-efficient building design and use of low or zero-carbon energy generation, such as solar panels. More recently forms of carbon offsetting were allowed as part of the “zero” calculation so that carbon could be mitigated away from the immediate development site. The Conservative government has now scrapped both this allowable solutions policy and the increase in on-site energy efficiency standards, taking away the foundations of zero-carbon compliance.  The meaning of the “zero” had already been diluted, stripped of any sense of entailing new ways of ongoing low-carbon living – and the closely-related Code for Sustainable Homes had been got rid of. But even so, at least the zero-carbon requirement was still in place in some form.   Not now. Not after nine years of intensive collaborative work by the Zero Carbon Hub, set up after the obligation was first put in place to work out exactly what the zero would mean, how it would be calculated and to provide guidance to housing industry on all sorts of detailed aspects of compliance. This is what makes it policy vandalism and a damaging breach of trust that can only undermine attempts at collaborative initiatives in the future.    It’s not just those trying (increasingly desperately) to make the case for action on climate change that have protested, the British Property Federation, the Chartered Institute of Building the UK Green Building Council and some (but not all) other industry bodies have registered their protest at the loss of a long-term commitment to improving the energy and carbon efficiency of new homes.  The government’s argument is that scrapping the zero-carbon obligation will stimulate house building and help reduce house prices. But the evidence is lacking for both claims, with high house prices in particular far more a function of the dysfunctional way that property markets work in the UK and the lack of a proper regional policy to distribute jobs and growth more evenly across the economy. More fundamentally, a decision that can only serve to increase our carbon emissions projected into the future sends absolutely all the wrong signals, including to those governments in the Global South that quite rightly point out our historic and contemporary responsibility for carbon accumulation.   Maybe house builders will still take up some of the innovation and capacity for building in new low-carbon ways that have been developed over the past nine years. But without the regulatory push it is hard to imagine that things will not level out at a lower standard.  There is now an open invitation to build less carbon-efficient houses, for profit-making at the expense of our deep moral responsibility to act now to mitigate future climate impacts. Yes new homes are needed (in some places), but not at this cost, and not in a way that destroys even the limited sense of what “zero carbon” had become."
"With the ongoing bushfire crisis, it is clear that we are in the middle of a historic event that will change the way we manage fire in the Australian environment. As a bushfire protection scientist, I am mostly focused on the practical problems these major events have given rise to, what went wrong or right, how we can solve these and prepare for the future.  My personal perspective is based on working as a fire protection planner in the aftermath of the 2003 Canberra fires and the 2009 Victorian Black Saturday fires. These events resulted in significant changes to how we protect communities, which have been successful in a lot of cases. There are still several months to run in the current fire season, but we can already start to look to the big questions that we will need to answer once the fires are finally out. We fight fires with data as much as water these days. Apps and social media are now an essential part of warning communities and coordinating evacuations. While this is an amazing capability that simply did not exist at the time of the Canberra and Victorian fires, it was clear from tweets by fire protection experts, such as Bianca Nogrady in the Blue Mountains, that our telecommunications system was being pushed to its absolute limit as people tried desperately to keep on top of where the fires were.  The data-hungry nature of modern bushfire management isn’t just in the emergency operations space. Following Black Saturday, I was part of a team that used state-of-the-art fire prediction models to design bushfire shelters for Victorian public schools, as well as enhanced ember prediction models for the design of new suburbs in Canberra. Previous disasters have taught us lessons around designing houses for bushfire that have served us well in the current situation, but I predict that coming out of this current fire emergency we will see another review of building standards in bushfire zones, as well as an increased focus on adapting Indigenous burning prescriptions to create a more open, park-like vegetation structure near our suburban edges. These two broad approaches complement each other – both reducing the hazard and increasing the resilience of housing stock. Building standards will be a good starting point, as we have control over the design of bushfire resilient houses, and there is a well-established testing and verification system. There is almost certainly going to be an adjustment to building standards, particularly for Queensland. New South Wales has recently reviewed the bushfire planning system, and has effectively prohibited building in areas where there is likely to be direct flame contact with houses.  The fire danger index we use to scale the fire models was previously set at 50 (out of 100) for Queensland. We are now regularly seeing conditions in the 80-100 range, something that was previously an extremely rare event in northern areas of Australia. This was predicted in climate change models, with humidity and moisture dropping to levels similar to NSW and Victoria pretty much when they were expected to. It’s time to adapt our new buildings and look seriously at what we can do to strengthen existing stock that was built to an older, gentler climate that no longer exists. The inevitable calls for more broad-scale hazard reduction burning remind me of the saying that for every complex problem there is a solution that is clear, simple, and wrong. It seems so obvious – burn the fuel while conditions are mild and dangerous fire weather has less capacity to harm us. Like most things in life, the reality is more difficult – large scale fuel reduction is one of the most complex land management treatments to organise and implement. Weather conditions and fuel moisture have to be in a “goldilocks zone”, allowing the fire to burn hot enough to consume lots of fuel, but not so severely that control is impossible. Treating a large enough area requires large numbers of staff (paid and volunteer) to control the burn and protect assets. Even when everything goes right there is the further complication of smoke pollution affecting vulnerable groups. Despite this, agencies met or exceeded their hazard reduction targets this year, which highlights how overwhelming the fire weather extremes have been – in a lot of areas the fuel reduction simply didn’t work. As has been reported several times there have been significant losses during the current fires even in areas where hazard reduction burns had been carried out. The structure of these forests still allowed damaging crown fires to develop, even where the low-level fuels had been reduced. Improving the effectiveness of hazard reduction in the places where suburbs and bushland meet is going to be a major focus. I spend a lot of my time working to balance fire protection with maintaining aesthetic and ecological values. The analysis of house loss on Black Saturday recommended changing the structure of forests and woodlands immediately adjacent to houses. This involves significant thinning of trees and shrubs in the areas near houses to create an open, park-like look and feel that can be maintained in the long term. This needs to be combined with resilient house design and intensive fuel reduction in the immediate surrounds of houses (within eight metres) for maximum effect. So we need to look at approaches that allow us to safely and intensively reduce fuels close to the urban fringe and deliver this vegetation structure. This has led us back to some of the original methods used to work with fire and the landscape, perfected by the oldest continual culture on our planet. Our growing respect for Indigenous culture has seen an increased use of traditional knowledge in fire management, and I have worked on recent projects where we were able to embed this approach in management systems. We are increasingly seeing Indigenous fire managers working alongside bushfire assessors, and it has been fascinating to see how they measure the right time to burn based on fuel condition and knowledge of local weather. There are two significant advantages of traditional burning that make it a good fit for property protection. Firstly, it can be implemented safely close to assets with minimal equipment. The second advantage is that it has an ecological end-state as an objective, often aiming to create an open, park-like vegetation structure that has much less potential for damaging crown fires. The obvious and significant disadvantage is that so much knowledge has been lost, coupled with significant increases in permanent (and very valuable) assets in the landscape. It is critical that we take advantage of the elders who are focused on preserving Indigenous burning regimes and adapt these to fit our modern lifestyles. As a minimum every fire manager should read Dark Emu by Bruce Pascoe. The chapters on fire provide a well-researched summary of how Indigenous fire management worked that is accessible to a wide audience. The houses and lives lost hit us hard as fire planners; it is more than just a job for most of us. We are going to honour those people by learning as much as we can from this disaster and continuing to adapt. Cormac Farrell is an environmental scientist specialising in both the vegetation management and building protection aspects of bushfire management "
"Countries with high levels of human well-being are more likely to show increasing forest growth. That’s the finding of a new study by a group of Finnish scientists, published in PLOS ONE. Their work shows that countries exhibiting annual increases in the amount of trees typically score highly on the UN’s Human Development Index (HDI), a scoring system that uses measures of life expectancy, education, and income to assess development status. Meanwhile, countries with a net annual forest loss typically score lower on the HDI. The logical leap of faith here is to think that a remedy for the ongoing loss and degradation of much of the world’s forests would be a massive push for development in deforested countries. But while such a noble undertaking would be desirable in many ways, these apparent environmental links warrant scrutiny. The authors themselves discuss caveats to their findings, and these should not be ignored. For example, switching from net forest loss to net gain may simply involve sourcing things like wooden furniture or paper pulp from abroad, often from poorer nations with weaker environmental policies and safeguards. This process, known as “leakage”, was perhaps best described and documented by the geographer Patrick Meyfroidt and colleagues in 2010. Among other examples, they illustrate leakage by looking at Vietnam, where national increases in forest cover were linked to sharp increases in imported wood, about half of which was illegal. If such processes are occurring, then how far, and for how long, can the buck of exporting environmental impacts be passed? In any case, those recovered forests often aren’t all they seem. Under some definitions they can include plantations of oil palm or rubber – technically “forests”, yet with few of the ecological benefits of the environment they replace. Even the supposedly naturally-recovered forests are rarely, if ever, as biologically diverse and well-functioning as their natural predecessors.  Things can be worsened by forest restoration schemes which may have human, rather than ecological, motives at heart. In Indonesia, for example, I have witnessed forest restoration work in national parks that favoured useful exotics over native forest species. In Tanzania, local NGOs such the as the Tanzania Forest Conservation Group lobby for policies that promote forest conservation over (and in addition to) tree planting, citing both ecological and well-being benefits. The clear message here is that it is far preferable to prevent damage in the first place, than to try and restore former conditions at a later point. Modern concepts of sustainable development are typified by the UN’s 17 Sustainable Development Goals (SDGs), which cover a variety of topics, including matters of well-being, infrastructure and environment. Studies of how these goals interact (whether for better or worse) are important if we are to achieve truly sustainable development. The latest forest cover study uses a composite index to investigate forest trends, which may disguise a more complex picture. Past work has shown that improved education (SDG 4) is commonly associated with reducing deforestation, while the effect of increasing GDP (SDG 8) on forests is far more complicated. The authors use a metric that combines these components (along with life expectancy), which does not explain how they interact. Further complexities involve other areas of development, which have their own effects. For example, in countries with high levels of inequality (SDG 10), development can exacerbate deforestation rates, rather than remedy them. In Brazil, for example, national efforts to raise people’s development status proved more damaging to forests in municipalities with high levels of land inequality than in those where land was more fairly shared. Some work suggests that improvements in gender equality (SDG 5) could have positive outcomes for forests, while forest-degrading activities witnessed during times of conflict suggest that peaceful relations (SDG 16) are also conducive to healthy forests.  On the flipside, achieving global food security (SDG 2), meeting energy needs (SDG 7), and developing sustainable infrastructure (SDG 11) will all require careful planning and monitoring to ensure that their environmental impacts are minimised. Ultimately, this paper gives reasons to feel positive about the inevitable development of humans and the fate of the world’s forests. It implies that, at a certain level of development, forests lost or degraded in the processes of developing will begin to regenerate or repair (whether naturally or with human assistance). I sincerely hope that the Finnish team’s work encourages nations across the world, developed or otherwise, to restore as much forest as they can. Nevertheless, in an age of rapid climate change, biodiversity loss, and human population growth, we need our remaining forests more than ever. The world must find sustainable ways to develop that do not involve destroying what forests are left.  Following in the footsteps of already developed nations, and simply replacing forests at a later point, should not be considered a viable course of action."
"The beach at Muncar on the island of Java was revolting. The 400-yard wide, mile-long stretch of sand was feet deep in foul-smelling sauce sachets, shopping bags, nappies, bottles and bags, plastic clothes and detergent bottles. Bulldozers had cleared away and buried some of the huge mat of plastic and sand two years ago, but every tide since then had washed up more rubbish from the ocean, and every day tonnes more plastic was washed down the rivers from upstream towns and villages. Now it was fouling the fishing boats’ propellers. “We fear for the future,” one elderly woman said. She remembered Muncar only a decade ago as one of the most picturesque towns in Indonesia and a tourist hotspot. “If it carries on like this we will be buried in plastic. We have no choice but to throw plastic into the rivers. Now we are angry. Something must be done,” she said.  That was January 2019. One year later, the beach heaves with plastic but the local government, working with well-funded international advisers, a recycling company and an army of volunteer collectors have worked to stem the tide of plastic reaching its beaches. It will cost millions of dollars and take years but Muncar may soon have its sand back. But this small Indonesian town is the exception. 2019 was, for most of the world, the year the petrochemical industry and giant food, drink and beauty companies locked the world even further into fossil fuels, creating mountains of plastic for communities and future generations to deal with and making it almost too late to keep global temperatures in check. Ultra-cheap shale gas from the decade-long US fracking boom continued to fuel a surge of billion-dollar investments in new cracking plants that separate ethane from gas to produce ethylene, the building block of most plastic. Since 2010 the petrochemical industry has invested about $200bn, and with $100bn more planned to be spent, plastic production is expected to grow 40% by 2030. The implications for countries struggling to cope with climate change and thousands of communities fighting tides of plastic are only now being understood. From having little impact on the climate just 20 years ago, the production and disposal of plastic now uses nearly 14% of all the world’s oil and gas. Plastic production is expected to grow to 20% by 2050 by which time the industry’s climate emissions could rise to 2.75bn tonnes a year and plastic could be driving half of all oil demand growth. Plastic, says the International Energy Agency, could take up to 15% of the remaining annual carbon budget and make the fast-growing industry the equivalent of the world’s fifth largest climate heating country, emitting more than Germany or the UK, twice as much as all African countries and nearly as much as shipping and aviation combined. Even as anger mounted in 2019 against rich countries’ reluctance to act on climate change, it became clear that plastic was big oil’s great hope for expansion, and one of the world’s leading drivers of climate change. Shell’s giant $6bn ethane-cracking plant being built near Pittsburgh, will produce 1.6m tons of plastic a year but is just one of dozens of similar size plants planned for the US, India, China and the Middle East. Despite UN hand-wringing and corporate pledges, demand for plastics grew a further 3.5% in 2019 and up to 16% in much of Asia. Latest figures suggest 359 m tonnes were produced in 2018. Nearly one-third went to single-use packaging and less than 10% was recycled. The rest went to landfills, was burned in incinerators adding to emissions and increasing air pollution, or was left uncollected, with approximately 8m tons making its way to the sea via rivers. But 2019 was also the year the worldwide revolt against plastic pollution translated into political and corporate action; when going plastic-free in Europe became trendy and giant food, drink and health companies were shamed. It was the year governments pledged new laws and when hundreds of potential solutions and initiatives were initiated. Great steps were taken by the public to clean up beaches and seas but in Britain the size of the task was underlined in November when a sperm whale was found on a Scottish beach. An examination of its stomach revealed a 100kg ball of plastic rope, fishing nets, cups, shopping bags, gloves, packing straps, tubing, sachets, bottles and all the waste of the global consumer society. Such beachings were common in 2019, with plastic-filled whales and cetaceans washed up in Wales, the Philippines, Indonesia, Italy and the US. Rattled by public disgust at the sight of choked animals and soiled coastlines, the packaging industry was forced to respond. Food and drink firms including Unilever, Mars, Danone, Pepsico and Coca-Cola pledged to reduce the amount of virgin plastic they used by 2025 and to increase the amount of recycled plastic. The global commitment on plastic, introduced in late 2018 to get corporations to pledge to use less and recycle more, grew to more than 400 of the world’s biggest companies. Together they are responsible for more than 20% of all the plastic packaging produced. The UK supermarkets Tesco, Sainsbury’s, Asda, Morrisons and Waitrose raced to ditch hard-to-recycle “black” plastic from their ranges and to accelerate the amount of recycled material they use. Asda stated that nearly one-third of its plastic packaging would come from recycled sources by the end of 2020 and all should be recyclable by 2025. Waitrose said it had removed 90% of the 2,291 tonnes of black plastic. Some initiatives were eye-catching: Tesco pledged to remove 1bn pieces of plastic from products for sale in UK stores by the end of 2020. Sir David Attenborough detected a cultural change, telling the Glastonbury crowds that the world was changing its habits on plastic. But he may have been wrong. Greenpeace and the Environment Investigations Agency showed that more plastic than ever was put on shop shelves in 2019 and only Waitrose, Tesco and Sainsbury’s of the 10 biggest chains marginally reduced the amounts they used. Plastic water bottle sales, indeed, soared. Academic research, too, confirmed that pollution was worse than ever in 2019 and that the fishing industry was considerably responsible. The Dutch Ocean Cleanup project, working in the Great Pacific garbage patch, found more than half came from discarded plastic nets and rope, fish aggregating devices [FADs], buoys, long lines, crates and floats. French researchers showed how plastic litter at the bottom of the Mediterranean had tripled since 1990 and the Ellen MacArthur Foundation estimated that there would be more plastic than fish in the oceans by 2050 if business was allowed to continue as normal. Wherever researchers looked, they were horrified. One study found an estimated 1.8m pieces of plastic, old tyres and fishing gear on the sea floor of the Bay of Fundy between Nova Scotia and New Brunswick in Canada; the WWF calculated that 570,000 tonnes of plastic went into the Mediterranean each year – the equivalent of 33,800 plastic bottles every minute and plastic was found widely in the food chain and in human bodies. Under pressure from governments but unwilling to produce less, the industry turned in 2019 to bioplastics, which convert the sugar present in plants and crop residues into plastic. Big manufacturers such as BASF, Dow, Huhtamaki, Plantic, Mondi, and Amcor ramped up research into plastic from corn, wheat, potatoes, soyabean and cotton. The market is still small but is expected to grow 20% annually into a $70bn a year industry by 2024. But critics said bioplastics were not the answer. Not only can they take up land needed for food production, but most bioplastics need high temperature industrial composting facilities to break them down. With few local authorities able to handle them, the result is most must go to landfills where they are likely to release methane, a greenhouse gas 23 times more potent than carbon dioxide. Industry attempts to appear green mostly ended in confusion. Products were increasingly classed as compostable, biodegradable, recyclable, reusable or bio-based. But most of these terms meant little. Biodegradable plastic was found to be intact after years at sea; not all compostable materials, it was found, could be composted at home; and recyclable depended on the local waste stream; with few standards and no timescale attached to bioplastics, many were described as false solutions. Faced with heavily polluted coastlines, a hostile media and angry tourists, the EU launched its plastics strategy in 2019 . This aims to ensure all plastic packaging is reusable or recyclable by 2030. It also calls for 90% of all plastic bottles to be recycled by 2025. The new UK government has pledged to ban the export of plastic waste to poor countries, and to introduce a tax on plastic packaging with less than 30% of recycled content from 2022. It also said it would stop by April 2020 the use of 4.7bn plastic straws, the 316m plastic stirrers and the 1.8bn plastic cotton buds used each year. Asian countries, faced with massively polluted coastlines and a swiftly growing plastics market, promised to act. India and Peru planned to eliminate all single-use plastic by 2022 and the Maldives said it would phase out all its non-biodegradable plastic. By the end of 2019, more than 120 countries had banned plastic bags and 60 more countries said they would impose taxes. Many US states banned or said they were planning to phase out plastic bags. The industry fought back. Companies may have promised publicly to stop using certain types of plastic, but their trade bodies lobbied strongly in 2019 against new laws and argued to be allowed to continue to produce more. Industry and supermarket trade groups lobbied against proposed deposit return schemes, bans and new recycling targets. The US industry responded with threatening lawsuits against local authorities and cities who tried to introduce bag bans. Instead of waiting for governments and industry, coalitions of global and local NGOs, international banks, conservation groups and some plastic producers volunteered to clean up rivers and beaches, and help governments collect and recycle waste. The UK-based Common Seas NGO worked with islands and resorts in the Maldives and Greece, and with city authorities in Indonesia to prevent plastic getting to the ocean. Volunteers cleared millions of pieces of plastic from beaches in the Mediterranean, Middle East, Latin America, China and India beaches. Novel ways to collect plastic from rivers and oceans were introduced. The Ocean Cleanup project launched the Interceptor, a barge-like vessel theoretically able to harvest up to 100,000kg of plastic waste a day from heavily polluted rivers. Waternet, which manages Amsterdam’s canals, introduced a “bubble barrier” to catch floating debris. New Naval adapted oil-spill technology to invent a mesh barrier system to collect river plastic; and Mr Trash Wheel scooped rubbish out of the Jones Fall River in Baltimore with waterwheels. By the end of 2019, the war between the petrochemical companies and those who would stem their tides of plastic was fully engaged, but was still being largely won by the petrochemical industry. 2020 is widely seen to be critically important. In June the UN will host an oceans conference in Portugal at which worldwide progress will be assessed, and countries will pledge to prevent plastic pollution. Many proposed government bans should also come into force and hundreds of smaller initiatives to recycle more and reduce pollution should start to grow and make a difference. What is certain is that calls for a reduction in plastic use will grow louder and the industry will resist. But unless ways are found to use less, most of the efforts to stem the flood of plastic entering the environment are likely to prove temporary and insufficient. • A version of this article appears at commonseas.com"
"The Coalition’s history of reluctance to take action on climate change was alive and well in 1998 and 1999, with cabinet records giving a familiar insight into debates still raging 20 years on. As last year’s cabinet papers release illustrated, the government was nervous in the lead-up to the international climate summit in Kyoto at the end of 1997, not wanting to commit to anything it perceived could harm Australia’s interests.  By March 1998, the cabinet was acknowledging the good deal Australia believed it had negotiated. A submission by the foreign affairs minister, Alexander Downer, the minister for the environment, Robert Hill, and the minister for resources and energy, Warwick Parer, reported the Kyoto agreement was “fair and realistic” and met “all of Australia’s primary objectives”. Key to those was Australia’s insistence on differentiated emissions reduction targets for developed countries, “reflecting individual circumstances”. Another “important achievement”, the submission said, was “securing agreement to emissions from the land use change and forestry sector being treated in essentially the same way as those from other anthropogenic sources”. This controversial deal still allows Australia to claim it is meeting its international obligations, even while its emissions are going up, or flatlining at best. Australia negotiated an increase in emissions  limited to 8% above 1990s levels for the period from 2008 to 2012 at Kyoto – one of only three countries not required to make cuts. That would still be a “considerable challenge”, the cabinet heard, although other wealthy countries had pledged more significant cuts. Europe promised to reduce emissions by 8% and Japan and Canada by 6%.The submission argued Australia’s commitment represented “a cut in our projected business-as-usual growth of 30%”, comparable to the average for industrialised countries. It also noted its disappointment there had been no progress on commitments to cut emissions from developing nations. Australia’s role in Kyoto was controversial, with Hill aggressively pursuing a special deal for Australia at a meeting determined to reach consensus. The land-clearing clause meant that because clearing had declined between 1990 and 1997, emissions from burning fossil fuels could rise substantially while overall emissions could be kept to an 8% rise. The cabinet knew its Kyoto deal was internationally resented. Relations with the EU and Pacific nations were strained, the method for calculating emissions from land clearing had yet to be finalised, and “those who consider that Australia secured too good a deal at Kyoto may use this to try to increase our emission reduction task”. The submission notes that in the lead-up to further climate meetings later that year, “some EU member states have a vested interest in maintaining the EU’s hardline green position because it makes good domestic politics”. That strain continues, as do the repercussions of Australia’s deal under the Kyoto protocol. At December’s Madrid climate change conference, Australia was accused of “cheating” and was named by other countries and observers as one of a few nations that had thwarted the aims of the meeting. Australia planned to use carryover credits, an accounting measure linked to the Kyoto protocol, to meet targets it set at the Paris summit four years ago. Australia signed the Kyoto protocol in April 1998, and the cabinet discussed whether to ratify it, which would have made commitments binding. In the end, it deferred any decision, as ratification was “a completely separate and longer-term question ... important issues remain unresolved”. The Howard government never ratified the protocol, arguing it was not in Australia’s interests. Kevin Rudd’s Labor government, elected in 2007, made ratifying Kyoto one of its first priorities."
nan
"The Australian, Rupert Murdoch’s flagship newspaper, has defended itself against criticism it downplayed unprecedented bushfires by failing to put a picture of the disaster on the front page of an edition, even as newspapers across the world featured the harrowing scenes. Many of the world’s leading mastheads featured pictures of the devastation of the Australian bushfires on page one on Thursday. But the Australian’s first edition ran an upbeat picture story about the New Year’s Day picnic races at Hanging Rock.  Sources at the newspaper said the newsroom was short-staffed over the holidays, however it was noted that resources were found to attack the ABC with gusto over its New Year’s Eve concert. Singer Tex Perkins has upset some ABC viewers after his obscene gesture directed at the PM during his Sydney New Year's Eve performance https://t.co/1hngOWcVyn “Our readers have been fully informed across the nation both online and in paper all week,” editor John Lehmann told Guardian Australia. New masthead. pic.twitter.com/ke7lT9yRcm The national broadsheet’s lead story on Thursday was about a secret proposal by police to ban alcohol in Indigenous communities in Western Australia – a story deemed more important than the bushfire report, which said eight people were dead and mass evacuations were underway. Police push for remote booze ban in WA’s north after 13 youth suicides put a spotlight on rampant alcohol abuse, child neglect and violence. This is really bold and opinion is not uniform ⁦@australian⁩ https://t.co/82oboEuqO8 There wasn’t a single photo of the catastrophic bushfires until page 4. Meanwhile here in Australia the Oz WA edition didn’t have a single bushfire pic until page 4. They went with a pictorial coverage of the picnic races at hanging rock in vic. https://t.co/tUVp7MARZl Before readers got to that coverage, they were given an exclusive interview with “rebel marine scientist Peter Ridd” who has challenged reef scientists to test whether or not human actions have caused a collapse in the growth rate of corals on the Great Barrier Reef. The later editions of the paper dropped the racing story and replaced it with photographs of bushfire victims surveying the damage. The Australian is not the only Murdoch-owned newspaper that has been accused of downplaying the bushfires. On New Year’s Eve, Melbourne’s the Herald Sun also relegated the bushfires to page 4, even as thousands of Victorians faced a serious bushfire threat. For the past 62 years our own ""Onion Oracle"" Halwyn Hermann has been predicting rainfall using a humble onion, and this year we all hope he's spot on, because he says rain is a coming https://t.co/y5hG7LZoQf On the same day, Sydney’s Daily Telegraph blamed the Bureau of Meteorology for inaccurate weather predictions, which may have “lulled residents into a false sense of security about conditions”. But it was the Courier Mail’s story about the “Onion Oracle” that had some readers wondering what was going on at News Corp. The Queensland tabloid carried the optimistic news that “Onion Oracle” Halwyn Hermann was predicting rain using an old German tradition. They even compared the Onion Oracle’s predictions to those of the bureau of meteorology. uhhhh yeah sure man pic.twitter.com/WVXI4a2v0v The Australian has been consistent on one front. Throughout the bushfire season it has kept up its coverage of climate denialism. Before Christmas, the Australian attempted to smear Greg Mullins and his Emergency Leaders for Climate Action group as “largely a vehicle for Tim Flannery”. Flannery is a leading environmentalist and chief counsellor at the Climate Council. The former fire and emergency chiefs from multiple states and territories say Australia is unprepared for worsening natural disasters from climate change and governments are putting lives at risk. The Australian says they are a front for Flannery who is an “alarmist” for urging that coal-fired power stations be shut down. On New Year’s Eve, the paper led with another “exclusive” report that pushed the line Australia should not speed up its response to global warming. EXCLUSIVE | Energy Minister @AngusTaylorMP has warned ""top-down"" pressure from the UN to address climate change will fail and better technologies — not tougher ­government imposts — are needed to meet emissions-reduction targets. https://t.co/Ifz1nllItg Climate pressure was “doomed to fail”, wrote environment editor Graham Lloyd about claims by the energy minister, Angus Taylor, who warned that “top-down” pressure from the UN to address climate change would fail. As communities in Victoria and New South Wales faced devastating weather conditions on the weekend the Australian assured its readers that there was nothing unusual going on. A double-page spread about the history of fires in Australia painted the national disaster as a run-of-the-mill crisis: “History of disasters shows there is nothing new about nation’s destructive blazes.” “While there is no doubt these bushfires are bad and may get worse, fuelling more talk of the nation battling an unprecedented fire threat this summer, the blazes that continue to plague the eastern states and Western Australia are nothing new,” the report said. “Climate change or no, these are some of the costs of being in one of the most fire-prone regions of the world. And those costs have been paid since well before Federation.” “Indeed, of the entire list of official inquiries detailed in the AIDR’s bushfire and natural hazards database, just two make recommendations about factoring in climate change as part of the response to future fire risks.”"
"You walk through a park in a city on a warm day, then cross out to a narrow street lined with tall buildings. Suddenly, it feels much hotter. Many people will have experienced this, and climate scientists have a name for it: the urban heat island effect.  Heavily urbanised areas within cities are between 1℃ and 3℃ hotter than other areas. They are contributing to global warming and damaging people’s health, and it’s set to get worse as urbanisation intensifies.  Numerous cities around the world are trying to do something about this problem. But there is a very long way to go. So what is holding us back, and what needs to happen? Urban heat relates to how most cities have been designed. Many rows of tall buildings are organised into blocks which resist any natural breeze. Streets and roofs are clad in dark materials like asphalt and bitumen, which retain more heat than lighter materials and natural surfaces like soil. Natural ground absorbs rain, which is evaporated by the sun’s rays on a warm day and released into the air, cooling everything down. In a city, the rain just runs into the sewer system instead.  Urban areas tend to lack trees. Trees help reduce the air temperature by blocking the sun’s rays, while cutting the levels of pollution by absorbing harmful particles.  Cities are also warmer because they are full of human activity. Everything from transport to industry to energy output makes them hotter than they otherwise would be.   Urban heat has various consequences. Combined with heatwaves and global warming, both of which are also on the rise, these hotspots are producing conditions that kill and hospitalise growing numbers of people. The worst affected are the elderly and other vulnerable groups like the homeless.  The World Health Organisation (WHO) has warned that increased city temperatures lead to more pollutants in the air. These can aggravate respiratory diseases, particularly among children. As cities get bigger, more and more people will be affected by these threats to their health. Higher city temperatures are one reason why we are using more and more air conditioning. One US study found that the urban heat island effect in Florida was responsible for over $400m (£287m) of extra aircon, for example.  Aircon feeds climate change by producing more carbon emissions through the extra electricity demand, creating a vicious circle where it gets hotter because more aircon is required. The increased energy demand means a greater risk of summer blackouts, causing both human discomfort and economic damage.  Hotter city roads and pavements also raise the temperature of storm-water runoff in sewers. This in turn makes rivers and lakes warmer, which can affect fish and other aquatic species in relation to things like feeding and reproduction.  Finally, there are major economic consequences to hotter cities. One paper from last year predicts that all the extra wear and tear caused by the excess heat would amount to between 1% and 10% of lost GDP in thousands of cities around the world.  The solutions to the problem are clear enough: they include using paler more reflective building materials, and wiser urban planning that incentivises more parks, tree planting and other natural open spaces.  When it comes to taking these steps, however, it’s a very mixed picture. Countries and municipal authorities have typically become very good at adopting plans to cut emissions of carbon dioxide and other greenhouse gases. They are not so good at taking steps to adapt to climate change. A study from 2014 found that most European cities had failed to introduce urban heat plans, and the situation looks little better today.  This being the case, city administrations that have gone the extra mile look particularly enlightened – even though they tend to be somewhat sporadic. Melbourne, for instance, has substituted its trademark bluestone pavements in several areas with a permeable version that absorbs rainwater, thereby increasing the amount of evaporation.  New York City’s Cool Roof Initiative has seen thousands of volunteers painting some of the city’s flat bituminous roofs with a reflective polymer material. Lately, Los Angeles has launched an initiative to paint roads white, part of a pledge by city hall to lower the temperature by 3℃ in the next 20 years. Beijing, meanwhile, has been  introducing zoning measures to reduce smog.  Other administrations have been encouraging green roofs – rooftops covered in vegetation: they are a legal requirement for big new developments in Toronto; there are floor area bonuses for developers who include them in Portland, Oregon; and Chicago had a funding scheme for a while. In Swiss cities and regions,  green roofs have been a legal requirement for many buildings for years.  These are all just pockets of activity, however. Many other mayors and city administrations need to start implementing the kinds of bylaws and incentives to adapt to the reality of hotter cities.  The cities of the future can still be green and cool, but only if they move up the agendas of many city halls. The laggards need to follow the example of those that have been leading the way. The reality is that the social, environmental and economic costs of urban heat islands add up to a bill that is too high for humanity to pay."
"People associate wasps with memories of picnic invasions, BBQs under siege, and painful stings. There is a lot more to these much-maligned insects though, and with more than 100,000 different species, their life histories range from the quietly unobtrusive to the bizarre and gruesome. A new study in the Journal of Experimental Biology documents one such disturbing example of wasp larvae that takes control of their unfortunate spider hosts. The Japanese scientists behind the study thought the host-parasite relationship between the wasp Reclinervellus nielseni  (most wasps have only a scientific name) and its orb-weaver spider host Cyclosa argenteoalba could help us understand how parasitic organisms alter their host’s behaviour. The adult wasps lay an egg on the outside of the spider’s body. The wasp larva hatches out and attaches itself to the spider’s abdomen, where it feeds on the fluids within, while the spider goes about its normal life. At a certain point though, the larva causes the spider’s behaviour to change. It’s as though the larva takes control of the spider and forces it to create the perfect environment for the wasp larva to transform (or “pupate”) into an adult. Under normal circumstances, this species of spider spins two different types of web: a “normal orb web” that looks like a typical spider’s web with a spiral of sticky thread that is used for catching prey, and a “resting web” which lacks the sticky spiral that is spun just before the spider moults its old exoskeleton.  But the parasitised orb-weavers spin a web just before the wasp larvae transform into adults and kill the spider. This “cocoon web” looks very similar to the resting web. In fact, the wasp larvae had induced the spiders to build a modified resting web as it would create a safer environment for the larvae to pupate – just as a resting web creates the perfect conditions for the spider to moult.  To test their theory the researchers observed spiders building webs with and without wasps for company, they examined the structures of the webs and tested the strength of the silk fibres within them. The wasp cocoon webs had similar strength and structure as the regular resting web. They even had similar “decorations” of tiny fibrous threads which reflect UV light which may help to prevent other insects and larger animals disturbing the web, thus increasing the larva’s chances of pupating successfully. They also found that the cocoon webs have extra reinforcement to make them stronger, further increasing the likelihood of the wasp’s survival. The spiders are forced to abandon their normal behaviour to create the cocoon web, either by altering their normal orb web or by creating one from scratch. The spiders then sit in the middle of the web motionless, until the larvae kill it.  The scientists suggest that this control over the spider could be caused by the wasp larvae injecting hormones into the spider which mimic hormones that control the spider’s moulting behaviour. In effect, the spiders have been drugged by the wasps into doing their bidding. These “zombie spiders” are particularly spectacular victims of behaviour that is actually fairly common among wasps. The classic picnic-ruiners represent just a small group: the social wasps, those that live in a colony with a queen and workers. But, as with bees, most wasp species are rather different from the ones we normally encounter.  Most wasps are solitary, they are almost all predatory, and many tens of thousands of species are parasites or parasitoids. The difference here is that a true parasite doesn’t usually kill its host or render it sterile (like a flea or tick), whereas a parasitoid always does, and often consumes it too. Almost all adult wasps feed on nectar, but larvae need protein to grow and develop, and so the adults prey on invertebrates like other insects and spiders to feed their young. Some solitary wasps are kleptoparasites – parasites through stealing – like the cuckoo or jewel wasps. These beautiful wasps lay their eggs in the nests of other wasps or bees. The eggs of the parasite hatch before those of the host, so the parasite larvae can eat the provisions that the host parent left for their offspring, before they get the chance – sometimes they also eat the host egg or larvae. Parasitoid wasps lay their eggs on or inside the adults, larvae or eggs of host species. Some of these wasps also inject venom into the host organism to immobilise it, some also inject chemicals that protect their eggs from the host’s immune system. Most parasitoid wasps prey on other insects, but many are also specialised to parasitise spiders. Some parasitoids are even used commercially for agricultural pest control – the tiny Trichogrammatidae  family attack the eggs of nuisance moths, for instance."
"
Share this...FacebookTwitterPIK alarm story fails the test of science: Jet Stream will also meander as usual in the future
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated by P Gosselin)
Almost one year ago the Potsdam PIK Institute put out a press release, which warned of stalling Jet Stream waves. Due to man-made climate warming weather extremes would remain stuck in a position longer. Among the messengers of the alarm were Stefan Rahmstorf and hockey stich fabricator Michael E. Mann.
Next on February 20, 2018 the horror scenario suffered a setback at the University of Missouri. Using model simulations it was determined that the Jet Stream would also meander in the future as well. Climate alarm shut off once more. This is not the first time that Rahmstorf’s extreme claims have been dispelled in short order by his colleagues. See here, here, here, here, and here.
The University of Missouri press release follows:
Weather should remain predictable despite climate change
Simulations of jet stream behavior in a warming climate suggest ranges of forecasts in the mid-century will be similar to those in present day.
According to the Intergovernmental Panel on Climate Change, temperatures are expected to rise between 2.5 and 10 degrees Fahrenheit over the next century. This warming is expected to contribute to rising sea levels and the melting of glaciers and permafrost, as well as other climate-related effects. Now, research from the University of Missouri suggests that even as rising carbon dioxide levels in the atmosphere drive the climate toward warmer temperatures, the weather will remain predictable.
“The jet stream changes character every 10 to 12 days, and we use this pattern to predict the weather,” said Anthony Lupo, professor of atmospheric science in MU’s School of Natural Resources, which is located in the College of Agriculture, Food and Natural Resources. “We were curious about how this would change in a world with higher carbon dioxide levels. We found that in that warmer world, the variability of the jet stream remained the same.”
Lupo and Andrew Jensen, who earned his doctorate at MU, used an existing climate model to simulate jet stream flow in the Northern Hemisphere. The simulation monitored a variable that responds to jet stream flow changes and can indicate global-scale weather instability. Researchers used this variable to determine when the jet stream altered its flow. Since meteorologists can only accurately predict weather within the 10 to 12 days between jet stream flow changes, a shift in this time frame would directly impact weather predictability.
Over the course of a simulated 31 years, their observations indicated the jet stream would change its character about 30 to 35 times per year, a number that is consistent with current jet stream patterns. As the time frame used to predict weather did not change, the researchers concluded that weather would likely remain as predictable in a warmer world as it is today. The results do not address the effects of climate change on the nature or frequency of weather events but instead focus on the range of predictability afforded by the jet stream. In addition, the researchers did not extend the simulation past the mid-century to ensure their data was as accurate as possible. “Climate change will continue to create a lot of ripple effects, but this experiment provides evidence that the range of forecasting will remain the same,” Lupo said.
The study, “The Dynamic Character of Northern Hemisphere Flow Regimes in a Near-Term Climate Change Projection,” was published in Atmosphere. Other researchers involved in the study were Mirseid Akperov of the Russian Academy of Sciences, Igor Mokhov of Lomonosov Moscow State University and Fengpeng Sun of the University of Missouri-Kansas City.”
Share this...FacebookTwitter "
"On May 7 1813, when Argentina was beginning the process of becoming a sovereign country, the first Argentinian law for the promotion of mining was sanctioned. The day has now become a national day of mining. But mining in Argentina is surrounded by a series of controversies that invite us to question this celebratory commemoration. Most notably, resistance to what is known as “open-pit” or “mega” mining is growing. Open-pit mining is a type of large-scale mining that extracts minerals found in low concentrations from the surface of the earth rather than from tunnelling, generating large craters. This method requires large amounts of explosives and water, and the use of chemicals such as cyanide and sulphuric acid for the separation of metals. From Argentina to Colombia and Mexico, open-pit mining has been at the centre of environmental and human rights conflicts in Latin America for decades now. It is often referred to as the archetype of extractivism due to the magnitude of its environmental and human impact and the alliances between governments and transnational capital that lie behind it.  Mining took place in Argentina throughout the 19th and 20th centuries, but was never one of the country’s principle economic activities. Not until 1993, when under the neoliberal government of Carlos Menem, new mining legislation was introduced. This legislation improved the benefits to transnational companies and laid the ground for the beginning of large-scale open-pit mining for metalliferous minerals such as copper, gold and silver.  In 2017, Mauricio Macri’s government signed a new mining deal, with the objective of attracting even more foreign investment. While the government has claimed the agreement aims to improve environmental regulation, academics, lawyers and activists alike have criticised it for its disregard of current environmental laws. Furthermore, the agreement states that mining companies will now be able to have input on the way mining is taught about in schools, a move seen as an attempt to construct a social license for mining through education. Regional governments of mining provinces continue to argue that mega mining brings jobs, money and investment in infrastructure. But inhabitants of mining regions have told me how the jobs brought by this type of mining are few, and mostly limited to the construction phase of projects. The influx of resources promised by mining companies and provincial governments are also few and far from what is promised. As a teacher from Andalgalá, a town in the province of Catamarca told me, they promise paved roads and new facilities, and end up giving away a few footballs. What mining towns are left with instead is environmental wreckage and health problems. In Andalgalá, two decades of mining have led to draught and polluted water sources. The local paediatric hospital reported a 63% increase of respiratory diseases in children in the first four years that the Bajo la Alumbrera mine was in operation. They stopped publishing statistics after that – and requests for further research and statistics on health problems continue to be brushed aside by the authorities.  Meanwhile in the province of San Juan, the Veladero mine (operated by Barrick Gold) has had multiple spillages of cyanide-contaminated water, one of which was has led to criminal charges and a multi-million fine. While the effects of mega mining are grave and often irreversible, the media have only reported on the harmful consequences of this activity on occasions when it was politically strategic to do so. But communities across the country have come together to fight against mining projects that threaten the environment and their way of life.  In Esquel, in the south of the country, a referendum in 2003 resulted in 81% of opposition to mega mining. In Famatina, in the north-west, the threat of mega mining led to a mass uprising in 2012.  The people of Andalgalá meanwhile, where the Bajo la Alumbrera mine has been in operation for two decades, have stopped the opening of Agua Rica, a mining project three times the size of the former, for eight years now. The local assembly organising against mining, Asamblea del Algarrobo, has pursued a number of routes in their fight, from legal challenges to direct action.  Most prominent in Andalgalá is the range of creative actions that have sprung up. An inter-generational group of local women called Las mujeres del silencio (the women of silence) have staged performative protests outside the headquarters of mining companies. A community radio has been created. And a wealth of murals celebrating the right to life and to water – and denouncing the repression of protest – can be found covering the walls of the town. The fight against mega mining is part of a far wider struggle in Argentina and Latin America against the expansion of an extractive economic model. This approach leads to what sociologist Maristella Svampa and environmental lawyer Enrique Viale call maldesarrollo (bad development). Resistance to such practices is not just about pollution, but also about saving (or rebuilding) the social fabric that is torn apart by extractive activities, and establishing the right to self-determination.  In Andalgalá, I am often told that even though the fight against mining is far from over, the cultural battle has been won. The myths of progress associated with mining have been debunked – and the struggle has generated a creative space for thinking about alternative economic and governing models. At the present time, the government and national and international mining companies are pushing to reverse some of the wins. But as transnational companies and the government attempt to intensify extraction, cultural resistance offers a space for imagining alternatives to false and bad developments."
nan
"The sky over Cobargo in New South Wales was still tainted yellow on Thursday afternoon when Australia’s prime minister arrived. For the past month, the country had been ablaze, and the village 240 miles south of Sydney and home to 776 people, had been hit hard. Standing in the crowd, Zoey Salucci McDermott, 20, eyed Scott Morrison coolly. She and her young daughter had lost her home in the fires, so when he extended his hand in greeting, she did not reciprocate. “I’ll only shake your hand if you give more funding to the RFS [Rural Fire Service],” McDermott said, holding back tears. “So many people here have lost their homes. We need more help.” The prime minister turned away. Days before, two separate firefronts had swept across much of southern NSW and eastern Victoria with unprecedented ferocity, wiping entire towns from the map. When the flames arrived in Cobargo, they tore through the main street, roaring like the ocean, incinerating lives and livelihoods. Robert Salway, 63, and his son Patrick, 29, died when they stayed behind to defend their property. Patrick’s wife, Renee, who is pregnant with the couple’s second child, found their bodies. As the flames retreated, a white hot anger smouldered. “You’re not welcome, you fuckwit,” a man yelled after Morrison as he retreated under a barrage of insults. Referring to the affluent Sydney suburb where the prime minister has an official residence, he said: “I don’t see Kirribilli burning after the fireworks.” The scene, caught on video, has become symbolic of a nation at odds with its leadership as it has endured a week of horror in a rolling, months-long national emergency that has yet to reach its climax. The scale of the destruction is hard to overstate. When the Amazon rainforest caught fire last January, 906,000 hectares burned. And last July, 2.6 million hectares were turned to ash across the Siberian steppe. Since the first fires began in Australia in August, more than 5 million hectares have been set aflame, fanned by unusual weather patterns and lower-than-usual humidity that has allowed firefronts to spread rapidly across a bone-dry landscape. The country’s volunteer firefighting forces are exhausted, outgunned and overwhelmed. The fires are behaving in unpredictable ways, spreading at night and even returning to areas that have already burned. Conditions on the ground have seen walls of flame, spot fires coalescing into fire tornados and some firefronts burning so hot they have formed their own weather systems. The ensuing lightning strikes have gone on to ignite yet more fires. The direct death toll stands at 23, with more expected. Some species have been pushed to extinction, and more than half a billion animals are estimated to have been killed. Farmers have reported running out of bullets as they work to end the suffering of half-dead livestock. Of all the states, New South Wales has been hardest-hit, with 3.41 million hectares scorched over the past few months and Sydney, at one point, finding itself encircled by active firezones. Around the same time, fire ripped through the Adelaide Hills and Kangaroo Island in the central state of South Australia. In the Adelaide Hills in South Australia, flames swallowed 25,000 hectares of prime agricultural land, leaving a third of the wine industry in the area “smashed”. The state of Victoria in the southeast, meanwhile, greeted the first week of the new decade in horror as it watched new firefronts bloom in East Gippsland before quickly building into an inferno which blackened 700,000 hectares in days. Wherever the fires rise, their approach has been foreshadowed by falling ash and the dimming of light as blue skies turn either nicotine yellow, glowing orange or, in some cases, blood red. That was the scene in Mallacoota, a tourist town 260 miles east of Melbourne in the middle of a Unesco biosphere. As the rest of the world readied to bid goodbye to 2019, locals spent their New Year’s Eve watching the flames approach. Residents woke to find skies blackened by ash and temperatures rising to 49C at 8am. As daybreak turned to night, the sky grew red as the flames drew near. With the only road out of town soon cut off by fire, 4,000 people were forced to flee towards the beach. When traffic backed up, people abandoned their cars. Soon after, the emergency services ordered people to wade into the sea to escape the blaze. Over the next two days, as images of the destruction began to emerge on social media, an evacuation order was given for some 14,000 sq kms between Nowra in New South Wales and the border of Victoria. Across the state lines, Victorian state premier Daniel Andrews declared an official state of disaster in Gippsland on Thursday night, enacting sweeping powers that have never been used. Weeks after an advertising campaign began in the UK to attract tourists to Australia, tens of thousands of holidaymakers were evacuated from coastal NSW and Victoria, the largest peacetime evacuation in the country’s history, with the navy deploying to rescue those trapped in Mallacoota. Authorities in those states which have so far escaped the devastation have been left to watch on nervously. Peak fire season typically hits with the height of summer, leaving those states so far unaffected to wait their turn – with far fewer resources. With bushfires flaring so early in the year, volunteers have been sequestered from across the country to assist on the east coast. Their absence means states like Western Australia are reliant on ex-volunteers in the event of emergency.Even regions relatively unaffected by the fires have been unable to escape the consequences. Smoke drift has stained glaciers as far away as New Zealand. In Canberra, the nation’s capital, air quality has been so bad that the postal service has stopped delivering mail. Liz Bashford from Doctors for the Environment Australia said the smoke has probably already caused deaths. Preliminary statistics show mortality rates, ambulance callouts and hospital admissions all increasing since the smoke settled over Sydney, but the precise impact won’t be known ubntil at least the end of this month. “Bushfire smoke will increase the incidence of death, but to say it was responsible for a particular death is much harder,” Bashford said. “It’s a bit like smoking cigarettes. If someone dies of respiratory problems and they’re a smoker we can clearly say it contributed to their death, but it’s very hard to put the finger on the only thing that caused their death.” “All this has been predicted for decades by scientists but it’s happening in a far quicker and in more dramatic way this fire season than we expected.” Morrison has shrugged off criticism of his government’s climate change policies. Speaking at the UN in New York last September he insisted Australia would meet its emissions reductions targets “in a canter”. That same month, his government was being warned about the catastrophic fire risk presented by climate change. Former New South Wales fire chief Greg Mullins and 22 other former emergency services chiefs wrote to Morrison outlining the potential crisis and asking for more specialised equipment to deal with hotter and longer bushfire periods. They were ignored. Emergency services have been asking for resources since 2016, when the National Aerial Firefighting Centre asked for a “national large air-tanker fleet” to support firefighting operations but were rebuffed. The consistent refusal to stump up the cash or even engage with a chorus of experts warning about a crisis has defined the conservative government’s policy since it took power under Tony Abbott in 2013. Now, as the country faces one of the worst natural disasters in its history, a short-term, transactional austerity politics has collided with the long arc of climate change in a way that is clearly visible on the ground. Firefighters in some areas have been forced to crowdfund for basic equipment while until recently the federal government remained steadfast in its refusal to back-pay volunteers for their time, even as the prime minister praised their “spirit”. It took sustained public pressure to drag the government to compensate volunteers, with Morrison announcing last Saturday that they would receive up to $6,000. The denial of reality was seemingly reinforced on New Year’s Day, when Morrison held a reception for a professional cricket team at Kirribilli House, the official residence.In a throwback to an earlier scandal when Morrison was photographed on holiday in Hawaii as the first homes were lost in New South Wales, As the country burned, the prime minister was pictured playing backyard cricket.“Whether they’re started by lightning storms or whatever the cause may be, our firefighters and all of those have come behind them to support them, whether they’re volunteering on the front line or behind the scenes in a great volunteer effort, it is something that will happen against the backdrop of this test match,” Morrison said. These words might have been of little comfort to locals huddled together on a beach in Mallacoota against the approaching inferno on Tuesday morning and who were, on Saturday afternoon, again bracing for “another round”. Brendan, a Mallacoota local who asked that his last name not be printed, told the Observer how three branches of his extended family were now living under one roof as the fires had destroyed the homes of friends and family. “We chose to live here. We love this part of the world, and we’re really quite devastated at the destruction that’s taken place,” Brendan said. “We had a fire plan, we went over it at Christmas, all together. We still didn’t know how big the burn would be.” While they weren’t in any imminent danger, Brendan said the firefront had remained active around the town as of noon and an expected change in weather conditions threatened to make the situation unpredictable. A few hours later Brendan posted to social media another image showing the eerie sky over Mallacoota. It carried the ominous caption: “I don’t think we’re getting light back today. We’re relaxed enough that we’re going to make some dinner. I believe the risk that today’s weather posed has passed, for us at least.”"
"
Share this...FacebookTwitter
Graph from Perner et al. (2018) that shows modern-day Arctic sea ice (furthest left navy trend line) is still only slightly lower than during the Little Ice Age (LIA), and much more extensive (more ice) than during the Medieval Climate Anomaly (MCA), Roman Warm Period (RWP), and nearly all of the last 7,000 years.

Song et al., 2018
[A] general warm to cold climate trend from the mid-Holocene to the present, which can be divided into two different stages: a warmer stage between 6842 and 1297 cal yr BP and a colder stage from 1297 cal yr BP to the present.
The general cooling trend may represent a response to decreasing solar insolation; however, the relative dryness or wetness of the climate may have been co-determined by westerlies and the East Asian summer monsoon (EASM). The climate had a teleconnection with the North Atlantic region, resulting from changes in solar activity.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Perner et al., 2018
[W]e find evidence of distinct late Holocene millennial-scale phases of enhanced El Niño/La Niña development, which appear synchronous with northern hemispheric climatic variability. 
Phases of dominant El Niño-like states occur parallel to North Atlantic cold phases: the ‘2800 years BP cooling event’, the ‘Dark Ages’ and the ‘Little Ice Age’, whereas the ‘Roman Warm Period’ and the ‘Medieval Climate Anomaly’ parallel periods of a predominant La Niña-like state. 
Our findings provide further evidence of coherent interhemispheric climatic and oceanic conditions during the mid to late Holocene, suggesting ENSO as a potential mediator.



Share this...FacebookTwitter "
"
Share this...FacebookTwitterFear of natural catastrophes among German citizens has dwindled over the past 10 years. Back in 2007, just on the heels of Al Gore’s An Inconvenient Truth – the peak of the global warming scare – natural catastrophes took second place among the ranking of top fears for Germans.
Today ten years later in 2017 natural catastrophes are not even in the top three according to German ARD public television, which cited a study by R+V Insurances:

Chart source: R + V Versicherungen, via ARD German television screenshot.
Ranking in the top three are 1) terrorism, 2) political extremism and 3) tensions concerning the influx of foreigners.
South German SWR public broadcasting here cites the R + V study and writes that this year 56% of those surveyed said they feared “natural catastrophes”, putting that factor in fourth place in the ranking. A variety of other social and economic issues followed closely behind.
The ranking of fears is strongly linked to what issue happens to be dominating the news cycle at the time surveys are conducted. Coverage of climate and natural disasters comes and goes in cycles, and at times disappears for weeks or months from the German media radar.
Recently the Atlantic hurricane season was the top stories in the news, and so a survey done last week would have reflected a greater fear of natural disasters. But once the hurricane season dies out later this fall and the La Nina-induced fall of global temperatures starts to happen, the media of course will go to other bad news to feature.
Made-up news: Ice-free Arctic!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This year there has not been any record ice melt, and global temperatures have in fact begun to ease off. There really isn’t much left out there to report. And when facts aren’t there, some even make them up. For example just before its prime time 8 p.m. news, meteorologist Karsten Schwanke of flagship ARD German television announced on 15 September 2017 that the northeast and northwest passages of the Arctic were ice-free, which is a flat out lie:

No ice-free Arctic passages this year, according to the National Snow and Ice data Center (NSIDC). See details here.
Little wonder that most Germans harbour irrational fears of climate change.
Fear a function of media coverage, not observation
German fear of climate and natural disasters is not really related to real world observations made by citizens, but largely depends on media coverage. When media cover it, or make it up, they get afraid. When they don’t cover it, the fear disappears.
Obviously there’s risk involved in the media featuring climate and natural disasters constantly, namely people would simply tune it out. So the German media instead focusses only the major natural disasters, always trying not to overdo it but to keep it at a level that keeps the fear alive.
Keeping fear at high levels is a very tough and challenging job, especially when reports of growing doom don’t match real life observations, or when the reports are organized propaganda.
 
Share this...FacebookTwitter "
"Brexit may well be the government’s immediate priority but the most critical task of its first year will be its ability to secure an ambitious and effective deal to tackle the climate emergency. From the hundreds of people forced from their homes by the Yorkshire floods that dominated the early part of the election campaign, to the bushfires sweeping Australia, the evidence of our own eyes shows us that climate crisis isn’t just a threat to future generations – its devastating effects are playing out right now.  Climate-related disasters are already the biggest cause of internal migration – forcing someone from their home every two seconds. Today, 52 million people in 18 countries across Africa are facing crisis levels of hunger because of drought, flash-flooding and other extreme weather events. In Latin America, Guatemalans facing a sixth year of crop-destroying drought are among the many for whom climate change is now an existential threat – put bluntly, it is starving their children. There is currently no greater test of Boris Johnson’s commitment to the ideal of a post-Brexit-global Britain. In November, Glasgow will host the most important international climate talks in years. Can Johnson and his government persuade world leaders to put aside their own short-term economic interests and do what is necessary for people and planet?  The stage is set for the UK to take a lead. Our parliament has declared a “climate emergency”. We are already the first major industrial nation to commit to reaching net-zero carbon emissions by 2050; other nations, cities and companies are beginning to follow suit. But if we’re to maximise this opportunity, we – the British people, business and our government – need to do more. We know that tackling climate crisis will require changes to our high-carbon lifestyles. Oxfam has calculated that carbon emissions produced by the world’s wealthiest 10% are equivalent to those of the poorest half. Today, we publish new research showing that the average Brit will emit more carbon in first two weeks of 2020 than the citizens of seven African nations emit in an entire year. The good news is that there are increasing signs that the public is ready to act. As many as four in five Britons said they are likely to take one of a number of actions this year to reduce their carbon footprint. More than two-thirds (68%) said they were likely to use energy-efficient products or utility providers and 79% of people said they were likely to recycle more. In September, tens of thousands of people took Oxfam’s #SecondHandSeptember pledge not to buy new clothes for a month, saving carbon equivalent to driving round the world 200 times. But people also need help to adapt. Six out of 10 people in Britain want the government to do more to tackle climate change and solutions are available – improving public transport, introducing schemes to make greener homes more affordable or tax incentives to encourage lower-carbon lifestyles. There are no shortcuts – we can’t argue our way out of the climate emergency by using statistical tricks or clever rhetoric. Greta Thunberg has accused the UK of “very creative carbon accounting”, because our official figures exclude ‘imported’ emissions from goods and services we use that are produced outside our borders, such as electronic goods made in Asia or overseas flights. Climate crisis doesn’t end at our coastline: our shared, international targets on climate action are called “global goals” for good reason and government policy needs to account for that fact. We, and other wealthy industrialised nations like us, have been responsible for the majority of emissions over the past century and a half and therefore have a responsibility to support poorer nations in reducing their own emissions and adapting to the brutal impacts of a climate crisis they did little to cause. Expecting that we can simply “offset” our carbon emissions through schemes overseas, such as paying poorer countries to grow forests instead of food, risks pushing people already on the frontline of climate crisis even deeper into hunger and poverty. Wealthy nations have pledged to provide $100bn (£89bn) in climate finance every year by 2020. The UK should use its position as host of this year’s climate talks to push other countries to follow its lead in helping to reach this target and beyond while ensuring that funding doesn’t come from cuts to medicines, schools and other overseas aid commitments. After the disappointing climate talks in Madrid last month, a bold move is needed to inject energy and optimism into the next round. The prime minister should take charge of this himself, committing to attend and inviting other heads of government to do the same. Then putting the policies and investments in place to get us on track to reach net zero well ahead of 2050 could be a game-changer, showing that Britain is serious about net zero and how to achieve it. Not many governments have the chance to set the world on a truly different course. This one does and for the sake of all, whether in Guatemala, Australia or, indeed, Yorkshire, they must grasp it. • Danny Sriskandarajah is chief executive, Oxfam GB"
"
Share this...FacebookTwitterDr. Ryan Maue here reports at Twitter that although the Atlantic hurricane season “is going gangbusters“, the Pacific is in fact seeing “one of quietest Typhoon seasons on record“.
Last month in the media, amid the aftermath of Harvey and Irma, the public heard a long stream of hysterical reports claiming that the tropical storms were sure sure signs of man-made climate change.
Yet, according to Dr. Maue, the globe has seen significantly below average cyclone activity, despite the near record hurricane activity observed in the Atlantic this season.


Chart above shows cyclone activity globally being well below normal in a year awash with media hurricane hysteria. Status: October 10, 2017. Source: http://wx.graphics/tropical/

Though the North Atlantic is running at 240% of normal, the entire northern hemisphere is near normal at 98%. Astonishing is the fact that Southern Hemisphere cyclonic activity is near record-breaking low of 47%.
Globally the figure is mere 86%. This is an embarrassment and highly baffling to the media and climate alarmists, who have recently been giving false impressions of “unprecedented” storm activity this year.
“WHERE have all the cyclones gone?”
Even the Australian news site www.news.com.au here asks: “WHERE have all the cyclones gone?
Scientists are puzzled as to how global warming is having the opposite effect on storms from what is often claimed.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Dr. Maue’s following chart shows that the overall hurricane trend has been downward over the past quarter century:

Figure:  Global Hurricane Frequency (all & major) — 12-month running sums. The top time series is the number of global tropical cyclones that reached at least hurricane-force (maximum lifetime wind speed exceeds 64-knots). The bottom time series is the number of global tropical cyclones that reached major hurricane strength (96-knots+). Adapted from Maue (2011) GRL. Source: http://wx.graphics/tropical/.
Record-low Southern Hemisphere
In his last chart chart at the above website, shown is how the southern hemisphere has been trending down to a near record low.
In fact the abstract of a recent peer-reviewed paper appearing at the Geophysical Research Letters, confirms the trends, writing (emphasis added):
In the pentad since 2006, Northern Hemisphere and global tropical cyclone ACE has decreased dramatically to the lowest levels since the late 1970s. Additionally, the frequency of tropical cyclones has reached a historical low. Here evidence is presented demonstrating that considerable variability in tropical cyclone ACE is associated with the evolution of the character of observed large-scale climate mechanisms including the El Nino Southern Oscillation and Pacific Decadal Oscillation.”
Reckless media neglect
This is information and data that alarmist climate scientists like Dr. Michael E. Mann or media such as the AP’s Seth Borenstein apparently recklessly neglected to examine before making hysterical statements to the public.
 
Share this...FacebookTwitter "
"It has become one of the fastest growing political campaigns in human history, surpassing similar battles against the tobacco industry and the fight against apartheid in South Africa. Its logic is simple: the only way to avoid climate change and dangerous levels of greenhouse gases in the atmosphere is for most fossil fuel reserves to stay in the ground.  Campaigners launched the fossil fuel divestment campaign in the early 2010s. Their argument was that you curb consumption of fossil fuels if you stop investing in the companies involved in extracting and burning them. Create a significant enough stigma, they argued, and this issue will shoot up the political agenda.  In the past five years or so, investment funds, public institutions and individuals have duly divested around US$6.15 trillion (£4.6 trillion) of fossil fuel assets. It has helped that the campaign attracted a number of prestigious institutions early on, including 
the British Medical Association, University College London, University of California, the Church of England and the World Council of Churches (representing more than a half billion Christians globally).  The campaign gained further traction after a London-based think tank argued that fossil fuels were in any case a bad investment because the true costs of environmental damage had not been priced in and that at some point there would be a severe correction.  The battle is far from over, however, as demonstrated by the recent decision of the Church of Scotland not to divest. One of the cornerstones of European faith, whose teachings have helped shape everyone from Robert Burns to Rupert Murdoch, its annual general assembly held an impassioned two-hour debate on whether to remove oil and gas stocks from its £443m investment fund.  The Church of Scotland has form in this regard: it had already divested its coal and tar sands investments two years earlier. Ahead of the latest debate, its official general assembly report summarised the issue as follows: It is deeply uncomfortable for the Church, as a caring organisation concerned about climate justice, to continue to invest in something which causes the very harm it seeks to alleviate.  While we have profited from oil and gas exploration in the past, we now understand that financing the future exploration and production will take us away from fulfilling the Paris Agreement and delay the transition to a low carbon economy.  Yet the approximately 1,000 commissioners attending the General Assembly Hall on the city’s Mound, next to Edinburgh Castle, narrowly disagreed: 47% in favour and 53% against. Coming from a nation which already gets most of its electricity from renewable sources, and whose government has indicated the end is in sight for fossil fuel vehicles on the roads, it was undeniably a disappointment.  Representatives were persuaded that it was better to stay invested and seek to influence better behaviour than to pull out altogether. Reverend Jenny Adams, who had brought the motion in the first place, argued that all the evidence suggests oil and gas companies have little intention of changing quickly enough to satisfy the Paris agreement. She said:  There is a need for climate emissions to peak by 2020 and if we just keep talking, too much time passes and change is not coming fast enough. She is surely right about this. There may be traditional wisdom in engaging with fellow shareholders and board members on matters pertaining to large companies, but the church’s decision looks naïve in relation to this sector.  To give just one example, consider that approximately 94% of shareholders of the oil giant Royal Dutch Shell voted last year and again this year to reject emission targets that would comply with the Paris climate accord, as it was deemed “not in the best interest of the company”. How do you persuade a bloc like that to change its mind? While the Church of Scotland’s decision to sidestep divestment may have been a setback to the movement, there have been recent successes, too. The Church of Ireland committed to divest its fossil fuel assets earlier in May, while an international coalition of Catholic institutions, including the Scottish Catholic International Aid Fund, pledged in April to divest investments totalling £6.6 billion.  Municipal administrations including New York and Paris are also divesting from fossil fuels and shifting their investments towards renewable energy sources – evidence that the global divestment is making an impact on public policy.  This certainly seems prudent, as newly published research suggests that the “carbon bubble” could “burst” in the next two decades as demand for fossil fuel energy falls despite population increases and burgeoning global economic growth.  The study projects that the global fossil energy demand will drop by as much as 40% by 2050. If that comes to fruition, it would mean containing global warming levels to 1.5 °C, which is the aspirational goal of the Paris climate accord.  That would be great news for environmentalists, most especially for those living on the front lines of climate change such as in the Pacific, less so for investors in fossil fuel businesses – Presbyterian or otherwise. It’s a strong signal that this global divestment movement may still be a long way from its peak."
"Fares for long-distance rail travel in Germany have dropped for the first time in 17 years, as climate protection measures aimed at making train travel more attractive came into effect with the new year. Travellers taking trips of more than 50km (31 miles) on Deutsche Bahn’s Intercity Express trains can look forward to fare decreases of 10%.  The company is also cutting prices on special offers and additional services, such as transporting bicycles. The trend in Germany stands in contrast to the situation in the UK, where millions of commuters face a 2.7% rise in ticket prices from 2 January. The cheaper tickets are a result of Deutsche Bahn passing on to customers the government’s cut in value-added tax on rail travel, from 19% to 7%. The UK does not charge VAT on rail fares. The company said it believed the price drop would bring in another 5 million passengers per year. Germany’s main provider of rail services is a private company in which the state is the single shareholder. Plans to sell off up to 49.9% of the company to private providers were abandoned with the onset of the 2007-08 financial crisis. Not all commuters in Germany will get cheaper fares in 2020. Fares for short-distance travel and public transport in regions such as Berlin, Hamburg, Bremen, Brandenburg and the Rhineland are set to increase, the news agency dpa reported this week. Fares for regional trains in the Bonn area are due to rise by 2.5%, while people in Berlin and Brandenburg face a 3.3% increase in the cost of tickets for bus, tram and subway travel. Public transport providers say the fare increases are due to rising wages and higher prices of diesel and electricity and were agreed before the government passed its climate protection measures."
"
Share this...FacebookTwitterMore Wind Turbines,
More Habitat Harm, Loss
Scientists (Krekel and Zerrahn, 2017 ) report that the installation of wind turbines near human populations “exerts significant negative external effects on residential well-being” and a “significant negative and sizable effect on life satisfaction” due to “unpleasant noise emissions” and “negative impacts on landscape aesthetics”.
“We show that the construction of wind turbines close to households exerts significant negative external effects on residential well-being … In fact, beyond unpleasant noise emissions (Bakker et al., 2012; McCunney et al., 2014) and impacts on wildlife (Pearce-Higgins et al., 2012; Schuster et al., 2015), most importantly, wind turbines have been found to have negative impacts on landscape aesthetics (Devine-Wright, 2005; Jobert et al., 2007; Wolsink, 2007). … We show that the construction of a wind turbine within a radius of 4,000 metres has a significant negative and sizeable effect on life satisfaction. For larger radii, no negative externalities can be detected.”
If human well-being and life satisfaction is seriously compromised by the nearby presence of a wind turbine, imagine the physiological effects on birds, bats, and land-dwelling mammals in general.
Six new papers expose the systematic destruction of natural wildlife habitats via the installation of wind turbines.
1. A 20-Fold Loss Of Bat Habitat At Wind Turbine Sites … A ‘Worldwide Phenomenon’

Millon et al., 2018
“Wind turbines impact bat activity, leading to high losses of habitat use … Island bats represent 60% of bat species worldwide and the highest proportion of terrestrial mammals on isolated islands, including numerous endemic and threatened species (Fleming and Racey, 2009). … We present one of the first studies to quantify the indirect impact of wind farms on insectivorous bats in tropical hotspots of biodiversity. Bat activity [New Caledonia, Pacific Islands, which hosts nine species of bat] was compared between wind farm sites and control sites, via ultrasound recordings at stationary points [A bat pass is defined as a single or several echolocation calls during a five second interval.] The activity of bent winged bats (Miniopterus sp.) and wattled bats (Chalinolobus sp.) were both significantly lower at wind turbine sites. The result of the study demonstrates a large effect on bat habitat use at wind turbines sites compared to control sites. Bat activity was 20 times higher at control sites compared to wind turbine sites, which suggests that habitat loss is an important impact to consider in wind farm planning. …  Here, we provide evidence showing that two genera of insectivorous bat species are also threatened by wind farms.  … To our knowledge, this is one of the first studies quantifying the indirect negative impact of wind turbines on bat activity in the tropics. … The lower attractiveness of the foraging habitat under wind turbines, both in a tropical and in a temperate climate, indicates that the indirect impact of wind turbine is a worldwide phenomenon.”
2. A ‘Distinct Physiological Response’ (Stress) Caused by Wind Turbines’ ‘Disturbance Factors’
Lopucki et al., 2018
“Living in habitats affected by wind turbines may result in an increase in corticosterone levels in ground dwelling animals … Environmental changes and disturbance factors caused by wind turbines may act as potential stressors for natural populations of both flying and ground dwelling animal species. The physiological stress response results in release of glucocorticoid hormones. … The common vole showed a distinct physiological response − the individuals living near the wind turbines had a higher level of corticosterone [physiological stress affecting regulation of energy, immune reactions]. … This is the first study suggesting impact of wind farms on physiological stress reactions in wild rodent populations. Such knowledge may be helpful in making environmental decisions when planning the development of wind energy and may contribute to optimization of conservation actions for wildlife.”
3. Wind Farms’ ‘Known Impacts’: Mortality Increase, Habitat Destruction, Enhanced Human Interference, Reduced Breeding Opportunities
Ferrão da Costa et al., 2018
“According to a review by Lovich and Ennen (2013), the construction and operation of wind farms have both potential and known impacts on terrestrial vertebrates, such as: (i) increase in direct mortality due to traffic collisions; (ii) destruction and modification of the habitat, including road development, habitat fragmentation and barriers to gene flow; (iii) noise effects, visual impacts, vibration and shadow flicker effects from turbines; (iv) electromagnetic field generation; (v) macro and microclimate change; (vi) predator attraction; and (vii) increase in fire risks. … Helldin et al. (2012) also highlighted that the development of road networks associated with wind farms could promote increased access for traffic related to recreation, forestry, agriculture and hunting. The consequence, particularly on remote places, is the increase in human presence, affecting large mammals via significant disturbance, habitat loss and habitat fragmentation. These negative effects are expected to be particularly relevant for species that are more sensitive to human presence and activities, such as large carnivores. Large carnivores, such as the wolf, bear, lynx or wolverine, tend to avoid areas that are regularly used by humans and—especially for breeding—show a preference for rugged and undisturbed areas (Theuerkauf et al. 2003; George and Crooks 2006; May et al. 2006; Elfstrom et al. 2008; Sazatornil et al. 2016), which are often chosen for wind power development (Passoni et al. 2017). … Results have shown that the main impact of wind farms on wolves is the induced reduction on breeding site fidelity and reproductive rates. These effects, particularly when breeding sites shift to more unsuitable areas, may imply decreasing survival and pack viability in the short term.”
4. Installation Of Wind Turbines Have ‘Population-Level Effects’ For Rare, Endangered Species
Watson et al., 2018
“The global potential for wind power generation is vast, and the number of installations is increasing rapidly. We review case studies from around the world of the effects on raptors of wind-energy development. Collision mortality, displacement, and habitat loss have the potential to cause population-level effects, especially for species that are rare or endangered.”
5. An ‘Urgent Concern’: ‘Wind Power Has Negative Effects On Proximate Wildlife’ (Collision Fatalities, Habitat Loss)
Naylor, 2018
“While wind energy provides a viable solution for emission reductions, it comes at an environmental cost, particularly for birds. As wind energy grows in popularity, its environmental impacts are becoming more apparent. Recent studies indicate that wind power has negative effects on proximate wildlife. These impacts can be direct—collision fatalities—and indirect—habitat loss (Fargione et al. 2012; Glen et al. 2013). Negative impacts associated with operational wind farms include collision mortalities from towers or transmission lines and barotrauma for bats. Habitat loss and fragmentation, as well as avoidance behavior, are also consequences resulting from wind farm construction and related infrastructure. The potential harm towards protected and migratory bird species are an urgent concern, especially for wind farms located along migratory flyways. In terms of mortality, wind turbines kill an estimated 300,000 to 500,000 birds, annually (Smallwood 2013). The high speed at which the fan wings move and the concentration of turbines create a gauntlet of hazards for birds to fly through. … [T]he height of most wind turbines aligns with the altitude many bird species fly at (Bowden 2015). Birds of prey— raptors—are of particular concern because of their slow reproductive cycles and long lifespans relative to other bird species (Kuvlesky 2007).”
6. Wind Farms Negatively Affect Waterfowl Via Habitat Loss, Disturbance Displacement, Compromised Foraging Opportunities
Lange et al., 2018
“Results from our surface water extractions and aerial surveys suggest that the wind farm has negatively affected redheads through altered hydrology and disturbance displacement. Our surface water extraction analysis provides compelling evidence that the local hydrology has been greatly affected by the construction of the wind farm. … Our results suggest the occurrence of direct habitat loss and disturbance displacement of redheads from the wind farm along the lower Texas coast. Although our study was directed solely toward redheads, it is likely that this wind farm has affected other species that use these wetlands or migrate along the lower Texas coast (Contreras et al. 2017). Studies in Europe investigating the effects on waterfowl by wind turbines have reported similar results, showing that turbines have likely compromised foraging opportunities for waterfowl through disturbance displacement (Larsen and Madsen 2000).”
Share this...FacebookTwitter "
"Millions of people worldwide can’t afford to keep their homes warm, but few realise the heat wasted in our energy system could provide the answer.  We need to do more to prevent valuable energy being lost to the environment as heat. It’s not just draughty buildings – power stations lose a vast amount of heat through their cooling towers or dumped into waterways, equivalent in the UK to a third of final energy use, while UK industry wastes enough heat to warm more than two million households. Storing this heat can even help us manage renewable energy – at lower cost than batteries. A 2013 study by Buro Happold showed that tapping into the waste heat rejected into London’s environment could provide enough warmth for the whole city. What’s needed is a strategy to “join the dots” between waste heat sources and demand for heat using new infrastructure. Early initiatives are currently underway, looking to capture waste heat from the London Underground and from transformers on the power network to heat homes.  In Scandinavia and Eastern Europe, communities often share their heat sources, with customers connected to a “heat network” carrying hot water in well-insulated pipes. Instead of having boilers in individual buildings, they have heat exchangers which pass heat from pipes buried under the street outside to heating systems inside.  For example, in Warsaw individual boilers were replaced with a network during post-war reconstruction leading to big reductions in local air pollution.  In the UK, cities such as Sheffield and Nottingham have pioneered these networks to distribute heat from waste incinerators. Burning off household waste produces a lot of heat, and putting this energy to use helps the cities to tackle fuel poverty and reduce their carbon footprints. Sheffield already has 50km of heat pipes in its city centre, and a new power station fuelled with locally sourced waste wood will generate renewable electricity and also energy to feed into an extended heat network. Industrial processes rarely produce heat at the right time to meet demand, but energy can be stored in these heat networks by using large, well-insulated hot water tanks that can hold the energy for several days. Boreholes deep underground could store heat between whole seasons. After all, energy stored as heat costs far less per unit than electricity stored in batteries. Energy storage would be part of any plan for Sheffield to make use of industry’s wasted heat, but the benefits could extend much wider than the city itself. As increasing amounts of intermittent renewable energy are fed into the national grid, large heat stores for power stations with a heat network allow for flexible electricity outputs. At times of excess electricity production from renewables, this energy could be taken from the grid and stored as heat. Since heating uses up 44% of the UK’s energy, and a similar amount in the US, heat networks with energy storage can play a major role in making national energy systems more efficient and sustainable. Even in warmer climates, there is a growing market for district cooling systems which operate on similar principles. People working in energy policy are only just beginning to think in a more holistic way by considering how best to provide heat and electricity. So much energy is needed for heating that we won’t meet our emissions targets without a joined-up policy. A more efficient energy system, where heat is valued, preserved and put to use, can lower people’s bills while at the same time reducing carbon emissions and air pollution."
nan
"
Share this...FacebookTwitterThe online fuldainfo.de here reports of mores woes in the German offshore wind industry. It’s turning out that offshore wind power is expensive, and often plagued by technical difficulties.

The offshore Trianel Borkum wind park severely hampered by spiraling costs, lower than expected winds. Photo credit: Trianel
Just days ago I wrote here of another recent technical folly suffered by the North Sea Riffgat wind park, where its power transmission underwater cable worked its way out of the seabed to become exposed and thus at risk of becoming ensnarled with anchors or fishing nets.
“Rosy wind projections”
The fuldainfo.de now writes of “high losses” incurred by aanother nearby wind park: the Trianel Windpark operated by Rhönenergie. This is “not surprising” to FDP Free Democrats Party Chairman Mario Klotzsche:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




We brought up our concerns again and again in the committees, and also in public, because the economic precariousness was foreseeable at a very early stage. That’s why the city of Flensburg left the project already in 2014. That’s what we also wish to do so that Rhönenergie can avoid getting saddled with additional burdens.”
Offshore installation costs more than doubled
A major reason for the losses seen by the Trianel Borkum park today, according to Klotzsche, were the “rosy wind projections”, which he reports was also the case for “many other wind projects“. He is quoted in the fuldinfo.de:
Originally 80 turbines with 400 megawatts of capacity should have been installed for 1 billion euros. In the end only 40 turbines and 200 megawatts of capacity were built for 1.1 billion euros.”
Multiple delays
Klotzsche also spoke of “repeated delays” with respect to hook up to the power grid, and even after the park was put into operation.
Moreover, the winds that were projected never materialized. “There was less wind than what was assumed,” according to Klotzsche. “The responsible persons calculated the project using rosy numbers.”
Share this...FacebookTwitter "
"How do we go about designing buildings today for tomorrow’s weather? As the world warms and extreme weather becomes more common, sustainable architecture is likely to mean one major casualty: glass. For decades glass has been everywhere, even in so-called “modern” or “sustainable” architecture such as London’s Gherkin. However in energy terms glass is extremely inefficient – it does little but leak heat on cold winter nights and turn buildings into greenhouses on summer days. For example, the U-value (a measure of how much heat is lost through a given thickness) of triple glazing is around 1.0. However a simple cavity brick wall with a little bit of insulation in it is 0.35 – that is, three times lower – whereas well-insulated wall will have a U-value of just 0.1. So each metre square of glass, even if it is triple glazed, loses ten times as much heat as a wall.  While the climate is changing, so too is the weather. Climate is expressed in terms of long-term averages, whereas the weather is an expression of short-term events – and the weather is predicted to change by much more than our climate. This creates challenges. A 0.5℃ increase in monthly temperature can made a difference to farmers, or the energy used by an air-conditioning system, but a peak temperature of 38℃ or a vicious cold snap can be far more serious. Buildings are designed to handle extremes, not just averages. Architects and building engineers around the world are now having to struggle with this issue, especially since buildings last so long. At Bath we have recently been awarded a grant to look at long-term weather forecasting and how building design will have to change. After all, you can’t move buildings to a better climate. One obvious possibility, for UK designers at least, is that they pick a place where the weather currently is similar to what the Met Office suggests the UK will have in 2100, and simply put up buildings like the ones they have there.  The problem is this ignores the low-carbon agenda. Many hot countries have spent the past 30 years designing buildings similar to those found in more temperate countries, while leaving enough space for monster air-conditioning systems. The air-conditioned skyscrapers in Las Vegas and Dubai, for instance, look just like buildings you might see in London or Boston, despite being built in the middle of a desert.  As an experiment, type “Dubai Buildings” into Google images and take a look at what has been built and, more worryingly, artist’s impressions of what is on the drawing board. You can even see this inefficiency in cultures that one might expect more of, for example the famous energy-guzzling glass towers of Vancouver.  Buildings will have to be simplified. Heating, lighting, energy supply, air con, escalators, IT networks and so on – all these “building services” will have to be stripped right back. Those services which do remain must use almost no energy – and possibly generate the energy they require on site. Cutting back on glass would be an easy win. Windows need to be sized, not glorified, and sized for a purpose: the view, or to provide natural light or air. Windows also need to be shaded. Many would argue that we need to re-invent the window, or the building. We need to build buildings with windows, rather than buildings that are one big window. Maybe we should look to the Mediterranean. People have mainly lived in countries such as Greece, for example, without air-conditioning – and it is true that such heavyweight, thick-walled buildings with small openings are capable of moderating external conditions very well.  However they don’t offer the climate control we are used to, especially if you pack them with people and computers. The people of the Mediterranean also had generations to adapt themselves and their working arrangements to fit with the climate. We don’t have this luxury: the weather is changing too fast. We have yet to invent architecture ready for whatever happens to the climate, but it is clear that we need to take lessons from the past – and from other cultures. We can’t simply air-condition our way through global warming."
"The year 2019 was the hottest on record for Australia with the temperature reaching 1.52C above the long-term average, data from the Bureau of Meteorology confirms. The year that delivered crippling drought, heatwaves, temperature records and devastating bushfires was 0.19C hotter than 2013, the previous record holder. Climate scientists told Guardian Australia that climate change pushed what would have been a hot year into record territory, driving heat extremes and the risk of deadly bushfires. The Bureau of Meteorology data shows the average temperature across the country was 1.52C above the long-term average taken between 1961 and 1990. The second hottest year was 2013, followed by 2005, 2018 and 2017. The data, from the bureau’s long-term ACORN-SAT data, will be used as part of the bureau’s annual climate statement due for release on 9 January. Prof Mark Howden, the director of the ANU Climate Change Institute, said the continued rising levels of greenhouse gases in the atmosphere, caused mainly by burning fossil fuels, was the underlying driver of the heat. He said: “It’s very clear that greenhouse gas emissions are changing the radiation balance of the Earth. Other contributors are minor in comparison.” He said two other climate systems had also played a role in delivering the record hot year. The Indian Ocean Dipole system had drawn moisture away from the centre of the continent, causing extra heat to build there. Another system known as the Southern Annular Mode had also contributed to the heat. The data also shows that 2019 was the hottest year on record for New South Wales, with temperatures 1.95C above the long-term average, beating the previous record year, 2018, by 0.27C. Western Australia also had its hottest year, with temperatures 1.67C above average, beating the previous 2013 heat record by 0.58C. The Northern Territory and South Australia both had their second hottest years, with 2019 coming in fifth hottest for Victoria and sixth hottest for Queensland, according to the data. Tasmania had a relatively cool year, but was still 0.41C above the long-term average. The previous summer of 2018-19 was the hottest on record. The spring of 2019 also delivered the worst bushfire weather since at least 1950, when the Forest Fire Danger Index data began. On Wednesday 18 December, Australia experienced its hottest day on record with an average maximum temperature of 41.9C (107.4F), beating the previous record by 1C that had been set only 24 hours earlier. Dr Sarah Perkins-Kirkpatrick, a climate scientist at the University of New South Wales specialising in extreme events, said 2019 had started hot, with the previous summer being the hottest on record. She said: “The extremes have been seen in lots of heatwaves and, of course, the bushfires, that are a consequence of the very hot and dry conditions.” She said while natural climate cycles had pushed temperatures higher, “climate change has given them a boost”. “2019 would not have been pleasant anyway, but climate change has made it worse. We are focusing now on the bushfires, but the underlying heat has been driving these conditions for much of the year. “Climate change isn’t the outright cause, but it’s an undeniable contributor to this extreme year on all accounts.” A bureau spokesperson said it would provide official comment on the 2019 temperatures in its annual climate statement on January 9 that would include a “comprehensive analysis of the year’s weather events and climate context, including any records of note”."
"The Open Championship has returned to St Andrews, one of the world’s oldest and most prestigious golf courses and one that has been recognised for its commitment to sustainability. Last month’s men’s US Open was held at Chambers Bay in Washington state, a course built on reclaimed land in what was once a sand and gravel pit. This transition from “pit to prince” has also earned Chambers Bay recognition from the environmental organisation Audubon International.  You might think this all reflects a shift in the golf industry and a growing ease among fans and players towards more “natural” – and environmentally-friendly – courses. From the US Open’s first day onwards, however, the playing conditions at Chambers Bay elicited harsh reviews from players and commentators alike. Top pro Henrik Stenson said the greens were “borderline laughable”, tantamount to broccoli and the surface of the moon. Rory McIlroy injected some humour into the conversation: “I don’t think [the greens are] as green as broccoli … I think they’re more like cauliflower.” Having studied the relationship between golf and the environment, we find these two sides to golf’s environmental “story” to be telling. They reflect, respectively, the reasons why some are optimistic about the golf industry’s professed environmental stewardship and why others continue to express concern. Our research on golf and the environment has focused largely on the Canadian and American contexts. To borrow a phrase from University of Michigan professor Andrew Hoffman, what we have found is that, in the postwar years, environmentalism in the golf industry has effectively gone from heresy to dogma. For much of its history, golf was played on quite rugged terrain. The original coastal links-style courses were subject to the natural shape of the land and, as late as the last decades of the 1800s, inland Scottish courses were characterised by their extreme muddiness.  With golf’s “migration” across the Atlantic, however, key figures in the industry aimed to professionalise course design and maintenance by making these tasks into matters of science and precision. For example, in the eyes of Alister MacKenzie – the British-born designer of the Augusta National golf course, where the annual Masters tournament is played – the “modern” golf architect was one versed in disciplines such as chemistry, botany and geology, and one capable of carefully sculpting the land. Judging from industry trade publications, this “modern” inclination lived on into the postwar years. By this point, however, golf architects had the capacity to radically manipulate development sites. At the same time, those responsible for golf course maintenance had potent synthetic chemicals, most famously the pesticide DDT, at their disposal in the task of keeping the golf course (literally) green. The postwar years, and especially the 1960s, were also a time when the environmental movement was afoot. Thus, what we find in these same publications from this time is both passionate advocacy for chemicals such as DDT and rather stern condemnations of environmentalists like Rachel Carson, famed author of the 1962 treatise Silent Spring. In this context, environmentalism was effectively heretical. Fast-forward 20 years though, and things were far less antagonistic. Through investment in research and the implementation of new “best practices” – for example, Integrated Pest Management, which in theory lessens pesticide usage through the adoption of non-chemical means – golf industry representatives could credibly make the claim that they themselves had become true stewards of the Earth. At present, then, pro-environment rhetoric has seemingly become a matter of dogma for key golf industry representatives.  Can golf really claim to be environmentally-friendly?  Certainly those protesting against the new Olympic golf course in Rio de Janeiro and Donald Trump’s development in Scotland have expressed strong and negative opinions about golf’s “friendliness” in those contexts. Indeed, golf still has environmental costs. In California, golf courses have earned criticism – even “drought-shaming” – for their water consumption in the midst of a severe drought.  Golf’s version of environmentalism is one underpinned by the idea of “sustainability”, which itself puts social, environmental, and economic development alongside one another. Yet it is not evident that the first two “lines” of this triple bottom line can always stand up to the third. Donald Trump’s group wanted a new course on the Scottish coast. Local residents and environmental experts worried that this would “freeze” the dynamic coastal sand dune ecosystem, a Site of Special Scientific Interest. With government support, Trump won out in the end. At the same time, the highly manicured course evidently still holds a place of high prominence. We can infer from complaints about Chambers Bay that a worthy golf course is one that is predictable, consistent, and literally green.  One might well say in response that the best players deserve the best conditions. Yet it has long been a concern – even within the golf industry – that the game’s most famous courses can set an unrealistic standard for the industry as a whole. Indeed, this phenomenon has even been given a name: Augusta National Syndrome, a condition whereby golfers come to expect the “perfect” conditions they see each year during broadcasts of the Masters.  Thus, at the same time researchers and environmental groups have expressed concerns over the chemicals that have come to replace the likes of DDT, Augusta National Syndrome is an issue to the extent that it rationalises the heavy use of water and pesticides in golf course maintenance. As golf returns “home” to St Andrews, we would do well to remember that broccoli – even cauliflower – is a long way from extreme muddiness. The standards we have made for golf are relatively new.  However there are alternative visions of environmentalism in the industry that go even further beyond the Audubon-certified way of being “green”. In our work on the greening of golf, we looked at the (admittedly sluggish) emergence of “organic golf”, a style of course management that often involves eschewing synthetic chemicals completely. We do not romanticise organic golf. It has many struggles and problems of its own, not least its occupation of land for a leisure activity not accessible to everyone. But the organic golf practitioners we have spoken with have been open to blemishes here and there, even if they still hoped to provide a challenging and rewarding experience for golfers in the end. Thus, organic golf holds the potential to subtly change the perception of how a “proper” golf course should appear. It is one thing to go from “pit to prince”. It would be another thing entirely to presume the prince’s appearance need not be perfect."
"
Share this...FacebookTwitterVolatile winds, burning generators, failing blades and buckling towers: these are just some of the technical problems plaguing wind power and thus making it a highly undependable and unreliable source of electricity.
Now we hear from NDR German public broadcasting reported of yet another problem plaguing the North Sea Riffgat offshore wind park located off the coast of the island of Borkum: an exposed underwater power transmission line.
Apparently the huge underwater cable delivering the green power from Riffgat to the mainland had been embedded below the seabed, but for some reason last April it somehow worked itself up above the seabed and is now exposed – vulnerable to North Sea maritime traffic.
Today a ship and a crew remain standing guard at the sea surface above the exposed cable in what the NDR calls “probably the most boring job on the planet”.
Riffgat is 15 kilometers northwest of Borkum and just north of the bustling shipping channel in the southern North Sea. Its 30 wind turbines are built over an area of 6 square kilometers and have a total capacity of 113 megawatts.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s not the first time that Riffgat has seen big problems. Between November 2015 and April 2016, transmission troubles kept Riffgat from exporting power.
According to NDR, the Dutch “Faxaborg” patrol ship manned by a crew of four has been floating at the site since April in order to “warn ships of an unusually dangerous area of hazard”.
NDR writes that some 200 meters of the transmission cable became exposed above the seabed one year ago, a condition that has been confirmed by grid-operating company Tennet.
According to NDR, the 50 kilometer long cable was laid with great effort 3 meters below the seabed and that the 200-meter section was washed away by the turbulent North Sea. Tennet says the exposed cable poses no hazard, but the Faxaborg ship was dispatched to stand guard as “a precautionary measure” to make sure “no fishing nets or anchors get caught with the cable”. The cable is only 8 meters below the sea surface at the location.
Guarding the cable is expected to continue indefinitely, at least until summer when the cable can be buried again. But a real solution remains elusive, NDR writes, adding:
Finding the right solution is no easy task. Just how much the entire affair will cost – indirectly to the consumers – was not stated by the Tennet spokesperson. That’s a company secret.”
Share this...FacebookTwitter "
"British populations of butterflies, including some of the most familiar countryside species, will begin disappearing within decades unless we take action. This is the alarming conclusion of new research published in Nature Climate Change by a group of British scientists.  Butterflies are naturally sun-loving creatures, and with the UK sat on the northern edge of many species’ ranges, previous studies have forecast possible benefits to UK populations from a warming climate. However, as the climate changes, extreme weather events including droughts are expected to become more common. Droughts can be a problem for butterflies, especially if they harm the plants upon which caterpillars rely for food. With less food around, populations can crash, and may take several years to recover to pre-drought levels. The new study used models to predict the frequency of droughts like that of 1995 under different scenarios of greenhouse gas emissions, and examined factors affecting the likelihood and speed of recovery for populations of six species of butterflies that experienced population collapses after the 1995 drought.  While droughts as severe as 1995 have previously only occurred as little as once in 200 years, allowing plenty of time for butterfly populations to recover, the study found that they may become far more frequent. If greenhouse gas emissions continue to increase at current rates, they might even occur on average once every 1.29 years (effectively every summer). Under “business as usual” scenarios, the research forecasts the widespread extinction of local colonies of butterflies as soon as 2050. So, what can be done to conserve our butterflies? Here is my simple, three-step guide: Clearly, reducing the impacts of climate change will be important. Delegates from around the globe will meet in Paris later this year for the 2015 UN Climate Change Conference, hoping to reach the first deal on reducing emissions since Kyoto 1992. Under the study’s best case scenario for emissions, 1995-like droughts might occur only every six to seven years, giving butterfly populations much more opportunity to recover in between. Ensuring the availability of suitable habitats for butterflies can also make a big contribution. The researchers found butterfly populations were more likely to persist through droughts and recovered more rapidly if situated in areas with larger, less fragmented patches of semi-natural habitat, such as grassland. Larger areas are likely to contain more abundant and diverse food-plants, helping more species of butterfly, and can also better resist edge effects associated with drought, such as moisture loss from woodland. Highly fragmented habitats have more edge relative to their area, and therefore experience more severe edge effects. Well connected habitats, through which butterflies can easily mingle and locate breeding sites, could add decades on to the survival of certain populations as the climate warms.  While large-scale habitat management programmes, such as the establishment of nature reserves, are an important means to preserve semi-natural habitat, the restoration of connectivity is where butterfly enthusiasts can help at home.  According to Richard Fox from the charity Butterfly Conservation, many drought-prone species can be encouraged to breed in gardens by leaving grass to grow long. “You don’t have to let your prize lawn go to rack and ruin, you can just leave a strip along the fence”, Fox told me me. Depending on how much is left, this could provide breeding habitat for species including the speckled wood, ringlet, meadow brown and large skipper. Meanwhile, other species can be helped by choosing garden flowers with care, or letting them choose themselves. “Large and small white will breed on Nasturtiums and love to nectar on flowers like buddleia and perennial wallflower,” advises Fox, while “green-veined white caterpillars can feed on lots of weeds, so not being too tidy can help”. If you have a garden, why not plant some butterfly-friendly plants of your own?  So while butterfly lovers will be among those waiting with bated breath for the outcome of the Paris summit, they may also be able to help closer to home. Habitat availability will be vital to the survival of butterflies when drought strikes, and by providing such refuges in back gardens anybody can help them survive and flourish."
nan
nan
"
Share this...FacebookTwitter
Not climate change: forest fires in the USA controlled by El Nino, arson and land use changes
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P Gosselin)
Droughts increase the risk of forest fires; that’s logical. However it is false to reflexively assign every forest fire to climate change. There have always been droughts and forest fires. Anyone wishing to shift the blame over to climate change first has to show that the trend has already deviated from the range of natural variability. For many, that is simply too much work.
Thus they prefer to claim something and hope that nobody will bother to fact check the claim. They don’t like climate skeptics because they have the silly habit of carefully examining the facts. They prefer the silent, non-questioning audience who immediately say yes and amen in response to all alarmist claims.
And when the facts indeed do contradict their alarmist claims, they get personal. They attack the occupation of the skeptic, or education, or skin color, or, or, or.
Nowadays we can find a load of facts in the Internet. Example: forest fires in the USA. The size of the areas ravaged by forest fires is provided by a table from the National Interagency Fire Centers. Strangely the data are not offered in graphical form. You are forced to make your own, which is no problem. Most people however simply are left in the dark. Steven Goddard (Tony Heller) shows such a charts at his Real Science blog.


2004 – 2014 burn acreage trend is falling. Chart source: Tony Heller.
One cannot always just pull climate change at of his magic hat every time a forest fire appears. The University of Colorado at Boulder recently calculated that 84% of all forest and bush fires in den USA are caused by humans. Read the press release from February 2017:
Humans have dramatically increased extent, duration of wildfire season
Humans have dramatically increased the spatial and seasonal extent of wildfires across the U.S. in recent decades and ignited more than 840,000 blazes in the spring, fall and winter seasons over a 21-year period, according to new University of Colorado Boulder-led research. After analyzing two decades’ worth of U.S. government agency wildfire records spanning 1992-2012, the researchers found that human-ignited wildfires accounted for 84 percent of all wildfires, tripling the length of the average fire season and accounting for nearly half of the total acreage burned. The findings were published today in the journal Proceedings of the National Academy of Sciences.
“There cannot be a fire without a spark,” said Jennifer Balch, Director of CU Boulder’s Earth Lab and an assistant professor in the Department of Geography and lead author of the new study. “Our results highlight the importance of considering where the ignitions that start wildfires come from, instead of focusing only on the fuel that carries fire or the weather that helps it spread. Thanks to people, the wildfire season is almost year-round.”  The U.S. has experienced some of its largest wildfires on record over the past decade, especially in the western half of the country. The duration and intensity of future wildfire seasons is a point of national concern given the potentially severe impact on agriculture, ecosystems, recreation and other economic sectors, as well as the high cost of extinguishing blazes. The annual cost of fighting wildfires in the U.S. has exceeded $2 billion in recent years.
The CU Boulder researchers used the U.S. Forest Service Fire Program Analysis-Fire Occurrence Database to study records of all wildfires that required a response from a state or federal agency between 1992 and 2012, omitting intentionally set prescribed burns and managed agricultural fires. Human-ignited wildfires accounted for 84 percent of 1.5 million total wildfires studied, with lightning-ignited fires accounting for the rest. In Colorado, 30 percent of wildfires from 1992-2012 were started by people, burning over 1.2 million acres. The fire season length for human-started fires was 50 days longer than the lightning-started fire season (93 days compared to 43 days), a twofold increase. “These findings do not discount the ongoing role of climate change, but instead suggest we should be most concerned about where it overlaps with human impact,” said Balch. “Climate change is making our fields, forests and grasslands drier and hotter for longer periods, creating a greater window of opportunity for human-related ignitions to start wildfires.”
While lightning-driven fires tend to be heavily concentrated in the summer months, human-ignited fires were found to be more evenly distributed across all seasons. Overall, humans added an average of 40,000 wildfires during the spring, fall and winter seasons annually—over 35 times the number of lightning-started fires in those seasons. “We saw significant increases in the numbers of large, human-started fires over time, especially in the spring,” said Bethany Bradley, an associate professor at University of Massachusetts Amherst and co-lead author of the research. “I think that’s interesting, and scary, because it suggests that as spring seasons get warmer and earlier due to climate change, human ignitions are putting us at increasing risk of some of the largest, most damaging wildfires.” “Not all fire is bad, but humans are intentionally and unintentionally adding ignitions to the landscape in areas and seasons when natural ignitions are sparse,” said John Abatzoglou, an associate professor of geography at the University of Idaho and a co-author of the paper. “We can’t easily control how dry fuels get, or lightning, but we do have some control over human started ignitions.”
The most common day for human-started fire by far, however, was July 4, with 7,762 total wildfires started on that day over the course of the 21-year period. The new findings have wide-ranging implications for fire management policy and suggest that human behavior can have dramatic impact on wildfire totals, for good or for ill. “The hopeful news here is that we could, in theory, reduce human-started wildfires in the medium term,” said Balch. “But at the same time, we also need to focus on living more sustainably with fire by shifting the human contribution to ignitions to more controlled, well-managed burns.” Co-authors of the new research include Emily Fusco of the University of Massachusetts Amherst and Adam Mahood and Chelsea Nagy of CU Boulder. The research was funded by the NASA Terrestrial Ecology Program, the Joint Fire Sciences Program and Earth Lab through CU Boulder’s Grand Challenge Initiative.”
In July 2017 the Institute for Basic Science explained that the risk of forest fires on the US Southwest was strongly dependent on the temperature differences between the Pacific and Atlantic Oceans. Ultimately the ocean cycles are the real drivers. Press release (via Science Daily):

Atlantic/Pacific ocean temperature difference fuels US wildfires
New study shows that difference in water temperature between the Pacific and the Atlantic oceans together with global warming impact the risk of drought and wildfire in southwestern North America
An international team of climate researchers from the US, South Korea and the UK has developed a new wildfire and drought prediction model for southwestern North America. Extending far beyond the current seasonal forecast, this study published in the journal Scientific Reports could benefit the economies with a variety of applications in agriculture, water management and forestry.

Over the past 15 years, California and neighboring regions have experienced heightened drought conditions and an increase in wildfire numbers with considerable impacts on human livelihoods, agriculture, and terrestrial ecosystems. This new research shows that in addition to a discernible contribution from natural forcings and human-induced global warming, the large-scale difference between Atlantic and Pacific ocean temperatures plays a fundamental role in causing droughts, and enhancing wildfire risks.
‘Our results document that a combination of processes is at work. Through an ensemble modeling approach, we were able to show that without anthropogenic effects, the droughts in the southwestern United States would have been less severe,’ says co-author Axel Timmermann, Director of the newly founded IBS Center for Climate Physics, within the Institute for Basics Science (IBS), and Distinguished Professor at Pusan National University in South Korea. ‘By prescribing the effects of human-made climate change and observed global ocean temperatures, our model can reproduce the observed shifts in weather patterns and wildfire occurrences.’
The new findings show that a warm Atlantic and a relatively cold Pacific enhance the risk for drought and wildfire in the southwestern US. ‘According to our study, the Atlantic/Pacific temperature difference shows pronounced variations on timescales of more than 5 years. Like swings of a very slow pendulum, this implies that there is predictability in the large-scale atmosphere/ocean system, which we expect will have a substantial societal benefit,’ explains Yoshimitsu Chikamoto, lead author of the study and Assistant Professor at the University of Utah in Logan.
The new drought and wildfire predictability system developed by the authors expands beyond the typical timescale of seasonal climate forecast models, used for instance in El Niño predictions. It was tested with a 10-23 month forecasting time for wildfire and 10-45 for drought. ‘Of course, we cannot predict individual rainstorms in California and their local impacts months or seasons ahead, but we can use our climate computer model to determine whether on average the next year will have drier or wetter soils or more or less wildfires. Our yearly forecasts are far better than chance,’ states Lowell Stott, co-author of the study from the University of Southern California in Los Angeles.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Bringing together observed and simulated measurements on ocean temperatures, atmospheric pressure, water soil and wildfire occurrences, the researchers have a powerful tool in their hands, which they are willing to test in other regions of the world: ‘Using the same climate model configuration, we will also study the soil water and fire risk predictability in other parts of our world, such as the Mediterranean, Australia or parts of Asia,’ concludes Timmermann. ‘Our team is looking forward to developing new applications with stakeholder groups that can benefit from better soil water forecasts or assessments in future fire risk.’
Paper: Yoshimitsu Chikamoto, Axel Timmermann, Matthew J. Widlansky, Magdalena A. Balmaseda, Lowell Stott. Multi-year predictability of climate, drought, and wildfire in southwestern North America. Scientific Reports, 2017; 7 (1) DOI: 10.1038/s41598-017-06869-7″



Ocean cycles (El Nino, La Nina) were also identified by Mason et al. 2017 as the forest fire drivers in the USA:
Effects of climate oscillations on wildland fire potential in the continental United States
The effects of climate oscillations on spatial and temporal variations in wildland fire potential in the continental U.S. are examined from 1979 to 2015 using cyclostationary empirical orthogonal functions (CSEOFs). The CSEOF analysis isolates effects associated with the modulated annual cycle and the El Niño–Southern Oscillation (ENSO). The results show that, in early summer, wildland fire potential is reduced in the southwest during El Niño but is increased in the northwest, with opposite trends for La Niña. In late summer, El Niño is associated with increased wildland fire potential in the southwest. Relative to the mean, the largest impacts of ENSO are observed in the northwest and southeast. Climate impacts on fire potential due to ENSO are found to be most closely associated with variations in relative humidity. The connections established here between fire potential and climate oscillations could result in improved wildland fire risk assessment and resource allocation.”
El Nino also plays a large role in the US Northwest for controlling driving forst fires, according to Barbero et al. 2015:
Seasonal reversal of the influence of El Niño–Southern Oscillation on very large wildfire occurrence in the interior northwestern United States
Satellite-mapped fire perimeters and the multivariate El Niño–Southern Oscillation index were used to examine the impact of concurrent El Niño–Southern Oscillation (ENSO) phase on very large fire (VLF) occurrences over the intermountain northwestern United States (U.S.) from 1984 to 2012. While the warm phase of ENSO promotes drier and warmer than normal conditions across the region during winter and spring that favor widespread fire activity the following summer, a reduction in VLFs was found during the warm phase of ENSO during summer concurrent with the fire season. This paradox is primarily tied to an anomalous upper level trough over the western U.S. and positive anomalies in integrated water vapor that extend over the northwestern U.S. during summers when the warm phase of ENSO is present. Collectively, these features result in widespread increases in precipitation amount during the summer and a curtailment of periods of critically low-fuel moistures that can carry wildfire.”
Overall forest fires in the USA have decreased significantly compared to the previous century (see article by Larry Kummer at Fabius Maximus).
In Colorado no forest fire trend could be found over the past centuries, see the press release from the University of Colorado issued in 2014:
Colorado’s Front Range fire severity today not much different than in past, says CU-Boulder study
The perception that Colorado’s Front Range wildfires are becoming increasingly severe does not hold much water scientifically, according to a massive new study led by the University of Colorado Boulder and Humboldt State University in Arcata, Calif. The study authors, who looked at 1.3 million acres of ponderosa pine and mixed conifer forest from Teller County west of Colorado Springs through Larimer County west and north of Fort Collins, reconstructed the timing and severity of past fires using fire-scarred trees and tree-ring data going back to the 1600s. Only 16 percent of the study area showed a shift from historically low-severity fires to severe, potential crown fires that can jump from treetop to treetop.
The idea that modern fires are larger and more severe as a result of fire suppression that allowed forest fuels to build up in the past century is still prevalent among some, said CU-Boulder geography Professor Thomas Veblen, a study co-author. ‘The key point here is that modern fires in these Front Range forests are not radically different from the fire severity of the region prior to any effects of fire suppression,’ he said. A paper on the subject was published Sept. 24 in the journal PLOS ONE. The study was led by Associate Professor Rosemary Sherriff of Humboldt State University and involved Research Scientist Tania Schoennagel of CU-Boulder’s Institute of Arctic and Alpine Research, CU-Boulder doctoral student Meredith Gartner and Associate Professor Rutherford Platt of Gettysburg College in Gettysburg, Pa. The study was funded by the National Science Foundation.
‘The common assumption is that fires are now more severe and are killing higher percentages of trees,’ said Sherriff, who completed her doctorate at CU-Boulder under Veblen in 2004. ‘Our results show that this is not the case on the Front Range except for the lowest elevation forests and woodlands.’ One important new finding comes from a comparison of nine large fires that have occurred on the Front Range since 2000 — including the 2002 Hayman Fire southwest of Denver, the 2010 Fourmile Canyon Fire west of Boulder and the 2012 High Park Fire west of Fort Collins — with historic fire effects in the region. ‘It’s true that the Colorado Front Range has experienced a number of large fires recently,’ said Schoennagel. ‘While more area has burned recently compared to prior decades – with more homes coming into the line of fire – the severity of recent fires is not unprecedented when we look at fire records going back before the 1900s.’
In addition, tree-ring evidence from the new study shows there were several years on the Front Range since the 1650s when there were very large, severe fires. The authors looked at more than 1,200 fire-scarred tree samples and nearly 8,000 samples of tree ages at 232 forest sample sites from Teller County to Larimer County. The study is one of the largest of its kind ever undertaken in the western United States. The team was especially interested in fire records before about 1920, when effective fire suppression in the West began in earnest. ‘In relatively dry ponderosa pine forests of the West, a common assumption is that fires were relatively frequent and of low severity, and not lethal to most large trees, prior to fuel build-up in the 20th century,’ said Veblen. ‘But our study results showed that about 70 percent of the forest study area experienced a combination of moderate and high-severity fires in which large percentages of the mature trees were killed.’
Along the Front Range, especially at higher elevations, homeowners and fire managers should expect a number of high-severity fires unrelated to any kind of fire suppression and fuel build-up, said Schoennagel. ‘This matters because high-severity fires are dangerous to people, kill more trees and are trickier and more expensive to suppress.” “Severe fires are not new to most forests in this region,’ said Sherriff. ‘What is new is the expanded wildland-urban interface hazard to people and property and the high cost of suppressing fires for society.’ In addition, a warming Colorado climate — 2 degrees Fahrenheit since 1977 — has become a wild card regarding future Front Range fires, according to the team. While fires are dependent on ignition sources and can be dramatically influenced by high winds, the team expects to see a substantial increase in Front Range fire activity in the low and mid-elevations in the coming years as temperatures continue to warm, a result of rising greenhouses gases in Earth’s atmosphere.”
2016 was a bad year of forest fires in California. Al Gore immediately pointed the finger at climate change. But later it was discovered that a series of arsons was behind most of the fires. The house of climate alarm quickly collapsed. Also the University of Arizona found that the fires were promoted by poor land use practices. Press release:

Forest Fires in Sierra Nevada Driven by Past Land Use
Changes in human uses of the land have had a large impact on fire activity in California’s Sierra Nevada since 1600, according to research by a UA researcher and her colleagues.
Forest fire activity in California’s Sierra Nevada since 1600 has been influenced more by how humans used the land than by climate, according to new research led by University of Arizona and Penn State scientists. For the years 1600 to 2015, the team found four periods, each lasting at least 55 years, where the frequency and extent of forest fires clearly differed from the time period before or after. However, the shifts from one fire regime to another did not correspond to changes in temperature or moisture or other climate patterns until temperatures started rising in the 1980s. ‘We were expecting to find climatic drivers,’ said lead co-author Valerie Trouet, a UA associate professor of dendrochronology. ‘We didn’t find them.’
Instead, the team found the fire regimes corresponded to different types of human occupation and use of the land: the pre-settlement period to the Spanish colonial period; the colonial period to the California Gold Rush; the Gold Rush to the Smokey Bear/fire suppression period; and the Smokey Bear/fire suppression era to present. ‘The fire regime shifts we see are linked to the land-use changes that took place at the same time,’ Trouet said. ‘We knew about the Smokey Bear effect — there had been a dramatic shift in the fire regime all over the Western U.S. with fire suppression. We didn’t know about these other earlier regimes,’ she said. ‘It turns out humans — through land-use change — have been influencing and modulating fire for much longer than we anticipated.’
Finding that fire activity and human land use are closely linked means people can affect the severity and frequency of future forest fires through managing the fuel buildup and other land management practices — even in the face of rising temperatures from climate change, she said. The team’s paper, ‘Socio-Ecological Transitions Trigger Fire Regime Shifts and Modulate Fire-Climate Interactions in the Sierra Nevada, USA 1600-2015 CE,’ was scheduled for publication in the online Early Edition of the Proceedings of the National Academy of Sciences this week. Trouet’s co-authors are Alan H. Taylor of Penn State, Carl N. Skinner of the U.S. Forest Service in Redding, California, and Scott L. Stephens of the University of California, Berkeley.
Initially, the researchers set out to find which climate cycles, such as the El Niño/La Niña cycle or the longer Pacific Decadal Oscillation, governed the fire regime in California’s Sierra Nevada. The team combined the fire history recorded in tree rings from 29 sites all along the Sierra Nevada with a 20th-century record of annual area burned. The history spanned the years 1600 to 2015. However, when large shifts in the fire history were compared to past environmental records of temperature and moisture, the patterns didn’t match. Other researchers already had shown that in the Sierra, there was a relationship between forest fire activity and the amount of fuel buildup. Team members wondered whether human activity over the 415-year period had changed the amount of fuel available for fires.
By using a technique called regime shift analysis, the team found four distinct time periods that differed in forest fire activity. The first was 1600 to 1775. After 1775, fire activity doubled. Fire activity dropped to pre-1775 levels starting in 1866. Starting in 1905, fire activity was less frequent than any previous time period. In 1987, fire activity started increasing again. However, the frequency of forest fires did not closely track climatic conditions, particularly after 1860. The researchers reviewed historical documents and other evidence and found the shifting patterns of fire activity most closely followed big changes in human activity in the region. Before the Spanish colonization of California, Native Americans regularly set small forest fires. The result was a mosaic of burned and unburned patches, which reduced the amount of fuel available to fires and limited the spread of any particular fire.
However, once the Spanish arrived in 1769, Native American populations rapidly declined because of disease and other causes. In addition, the Spanish government banned the use of fire. Without regular fires, fuels built up, leading to more and larger fires. The influx of people to California during the Gold Rush that began in 1848 reduced fire activity. The large numbers of livestock brought by the immigrants grazed on the grasses and other plants that would otherwise have been fuel for forest fires. In 1904, the U.S. government established a fire suppression policy on federal lands. After that, fire activity dropped to its lowest level since 1600. Starting in the 1980s, as the climate warms, fire frequency and severity has increased again. Fires now can be ‘bad’ fires because of a century or more of fire suppression, according to lead co-author Taylor, a professor of geography at Penn State. ‘It is important for people to understand that fires in the past were not necessarily the same as they are today,’ Taylor said. ‘They were mostly surface fires. Today we see more canopy-killing fires.’”

Share this...FacebookTwitter "
"Indonesia carried out cloud seeding to try to prevent further rainfall over the capital, Jakarta, and surrounding areas as the death toll reached 47 on Saturday amid flash floods and landslides.  Two small planes had earlier been readied to drop sodium chloride to break up potential rain clouds in the skies above the Sunda Strait, Indonesia’s technology agency said. Tens of thousands of Indonesians were crammed in emergency shelters waiting for floodwaters to recede in and around Jakarta after the massive new year’s flooding, officials said. Monsoon rains and rising rivers submerged a dozen districts in greater Jakarta and caused landslides in the Bogor and Depok districts on the city’s outskirts, as well as in neighbouring Lebak, which buried a dozen people. National Disaster Mitigation Agency spokesman Agus Wibowo said the fatalities included those who had drowned or been electrocuted since rivers broke their banks early on Wednesday after extreme torrential rains throughout new year’s eve. Three elderly people died of hypothermia.It was the worst flooding since 2007, when 80 people were killed over 10 days. “The waters came very fast, suddenly everything in my house was swept away,” said Dian Puspitasari, a mother of two. Four days after the region of 30 million people was struck by flash floods, waters have receded in many middle-class districts but conditions remained grim in narrow riverside alleys where the city’s poor live. Wibowo said about 397,000 people had sought refuge in shelters across the greater metropolitan area. Those returning to their homes found streets covered in mud and debris. Cars that had been parked in driveways were swept away, landing upside down in parks or piled up in narrow alleys. Sidewalks were strewn with sandals, pots and pans and old photographs. Authorities took advantage of the receding waters to clear away mud and remove piles of wet garbage from the streets. Cloud seeding or shooting salt flares into clouds in an attempt to trigger rainfall is often used in Indonesia to put out forest fires during the dry season. Authorities on Thursday used hundreds of pumps to suck water out of residential areas and public infrastructure like railways. President Joko Widodo has blamed delays in flood control infrastructure projects for the disaster. Widodo announced in 2019 that he will move Indonesia’s capital to East Kalimantan province on Borneo island to reduce the burden on Jakarta, which is overpopulated and sinking. Associated Press and Reuters contributed to this report"
"​It’s hard to know what to write in the midst of a national apocalypse.​​ I’m like most Australians, feeling every shade of human emotion each day as we see this living tragedy unfold before us. Each morning comes with a dull thud as we wake to the awful realisation that the new day could still be worse than the last.  The fear on people’s faces is palpable. It reminds me of Black Saturday. There the horror was quick, although the scars, still deep, remain. Remember those survivors today as the fires of recent weeks reignite the living hell of trauma a decade ago. ​​ Then there’s the helplessness we feel as we sit, watch and wait. Often the best we can do is stay glued to the ABC, ring people we know in harm’s way, and then give to charity. ​​ Then there’s pride in our firefighters, the unsung men and women who, without fanfare train week in week out, for years on end. Although no training can prepare you for war like this. They’re our new Anzacs. In the line of fire, literally, before the nation’s eyes. ​​ Of course there’s anger and rage as well. Much of it justified. Some of it not. Our national government will have much to account for once the current crisis has passed. The uncomfortable truth is the government’s response just doesn’t pass the pub test. It’s been evasive, tepid, tone-deaf and, above all, too late. It’s been conducted as an exercise in “issue management”, rather than a substantive response to one of the worst natural disasters in our history, with every shoulder to the wheel. And the Australian public can spot a fraud at a thousand paces. So in the midst of all this, what can be said that’s in any way productive for the future? ​​First, the feds should be taking their marching orders from the state fire chiefs each day. It’s just not good enough when the New South Wales Rural Fire Service chief says he first heard of the call-out of the army reserve through the media. Canberra’s job is to help provide whatever financial and physical resources firefighters and other emergency services need there and then. And on the question of “who’s going to pay”? I remember it well during the Victorian fires. The answer should simply be: “Whatever it takes. Just get it done.”​​ Second, the National Bushfire Recovery Agency announced on Sunday should be underpinned by national legislation, jointly with the states, and empowered to slice through red tape. There should be a national bushfire appeal – the Black Saturday appeal raised more than $400m – and its disbursement should be coordinated by new agency to avoid waste and duplication. The taxable 13-week disaster recovery allowance should immediately be extended to 26 weeks and be exempted from tax, as it was in Victoria. The disaster relief payment of $1,000 has also lagged inflation, meaning its real value has fallen substantially since 2009. Third, provide proper permanent funding for the nation’s bushfire services. The public will not tolerate any longer a discretionary “top-up” by the feds, or the usual buck passing between the various levels of government. Each brigade needs to be confident of their long-term funding. The formula should be hammered out and rigorously implemented. ​​ Fourth, the nation needs the biggest, best national aerial fire-bombing fleet in the world. We are the driest inhabited continent on Earth. It will get worse. Therefore we need a standing capability the likes of which the world hasn’t seen before. Whenever and wherever a fire breaks out, we then have a fighting chance of containing it at its earliest stages. Bill Shorten was right to take this to the last election. ​​It will cost. There will be accusations of waste if it remains idle for a while. Just like we were accused of waste when we co-funded Adelaide’s desalination plant – until everybody realised we needed it. Fifth, what more warning does Australia need on the absolute imperative of accelerated action on climate change? Not enough it seems for Scott Morrison, who says there will be no change in policy whatsoever. Nor for the high priest of climate change denial himself, Tony Abbott. Indeed, in the midst of the fires, there was Abbott on Israeli radio telling a global audience the real problem for Australia was that we had been taken over by a “climate change cult”! Abbott! He who famously proclaimed climate change was “absolute crap”. The political opportunist who stopped my government imposing a carbon price by toppling Malcolm Turnbull and revoking the deal we’d struck to pass it. The partisan thug who played cheap politics, ridiculing Australian efforts with our partners to broker a global deal at Copenhagen to keep temperatures within 2C. ​The political cynic who ran the biggest fear campaign in recent memory against our carbon price in order to become prime minister in 2013. Yup. The very same Abbott who destroyed Turnbull all over again in 2017 over climate policy – this time the national energy guarantee. So why does Abbott do it? The truth is he doesn’t give a damn about policy. Abbott has always been 100% politics. He’s always seen climate as the perfect political wedge against Labor among working families, deploying fear campaigns based on wildly exaggerated projections about jobs and the cost of living. He’s done the same internally, using it to divide and conquer his moderate opponents in the Liberal party. Pretty tawdry when now we see half the country going up in smoke!​​ But here’s the rub. The Abbott denialist cult has taken over the entire Coalition. It continues under Morrison and, when they oust him soon, it will continue under Peter Dutton. It’s become the battle cry of the far right which now runs the entire conservative show in Canberra. ​​ And yes, before the Murdoch media leap to Abbott’s defence, I know he is a firefighter. Good on him. The problem is most of the fire chiefs just don’t agree with him on the impact of climate change. Nor does the CSIRO. Nor does any credible climate scientist in the world. In Australia, as in America, the conservatives’ strategic partner in climate change denial has been the Murdoch media. Rupert Murdoch, feeling the heat of public opinion, claimed recently there were no climate deniers at News Corp! The Murdochs, senior and junior, must believe the Australian people are total fools. Murdoch’s papers remain a command centre for the entire mission of climate policy obstruction. Just look at their climate coverage over the last four federal elections. In the midst of today’s tragedy, we still see the old denialist trope trotted out that Australia, with only 1.3% of global emissions, can have no influence, and shouldn’t bother trying. ​​What pathetic nonsense. If other middle-sized economies did the same – there are about 20 with a similar share of emissions – that’s a quarter of the global total. We cannot work globally to get the biggest emitters – the US, China and India – to act unless we are also acting to clean up our own backyards. ​​ Our national interest dictates that Australia becomes once again a global climate change leader, not a follower. Unless we lead, and convince others to follow, Australia has a bleak future indeed. ​​ ​​​​​​​​​​​ The Australian people are angry. The Liberals and Nationals are now reaping what they have sown for more than a decade – through climate inaction at home, sabotaging climate negotiations abroad and a continuing pathology of poisonous fear mongering. The melancholy truth is the only policies restraining Australian emissions today were taken under our government a decade ago. Principal among these is our legislated mandatory renewable energy target for 20% of electricity supply to be renewables by 2020. The conservatives said it would destroy the economy. It didn’t. As a result, renewables are now 21%, up from 4% when we started.​​ But this is just the foundation stone of what must now become a bigger, bolder plan of national and international action. I stand by what I said a decade ago: climate change represents the greatest economic, environmental and moral challenge of our time. Moral because it’s about what’s right and wrong for the nation. Moral because it’s about intergenerational justice. The Liberals won’t change on climate. Denial is now their DNA. They may start pretending to care. Scotty from marketing is good at that. But we all know it will be paper thin. That’s why this lot have to go. Before it’s too late for us all. Kevin Rudd is a former Australian prime minister"
"On 3 August 2014, residents of Toledo, Ohio, woke to the news that overnight their water supply had become toxic. They were advised not only to avoid drinking the water, but also touching it – no showers, no baths, not even hand-washing. Boiling the water would only increase its toxicity while drinking it could cause “abnormal liver function, diarrhoea, vomiting, nausea, numbness or dizziness”, read a statement from the City of Toledo, warning residents to “seek medical attention if you feel you have been exposed”.  Toledo sits on the shores of Lake Erie, one of North America’s five great lakes. About half a million residents of the city and surrounding area have relied on Lake Erie for water for hundreds of years. After the news broke on 3 August bottled water quickly vanished in concentric circles around the city. Eventually, a state of emergency was called and the national guard arrived with drinking water. Toledo’s water crisis lasted for nearly three days. But the water wasn’t toxic due to an oil spill or high lead levels, as in Flint, Michigan. Toledo’s water was tainted by something altogether different: an algae bloom.Toledo is not alone. According to scientists, algae blooms are becoming more frequent and more toxic worldwide. A 14-month long algae bloom in Florida, known as the “red tide”, only ended earlier this year, after killing more than 100 manatees, 127 dolphins and 589 sea turtles. Hundreds of tonnes of dead fish also washed ashore. In 2018, there were more than 300 reported incidents of toxic or harmful algae blooms around the world. This year about 130 have been listed on an international database, but that number is expected to increase.Recent reports of a new ‘‘red tide’’ emerging in Florida and more dead wildlife have put the tourist and fishing industries on alert, braced for further devastation. The causes of the blooms vary, and in some cases are never known, but in many parts of the world they are being increasingly linked to climate change and industrialised agriculture. Algae includes everything from micro-algae, like microscopic diatoms, to very large algae, such as seaweed and kelp. Algae are not officially a taxonomic group of creatures (they don’t fit into general groups like plants, animals or fungi), but the name is generally used to describe marine or freshwater species that depend on photosynthesis. An algae bloom occurs when a single member of these species – because of certain conditions – suddenly becomes dominant for a time. Algae are vital to our survival. It’s estimated that at least half of the planet’s oxygen comes from these unsung creatures, who produce it through photosynthesis before releasing it into the water. Algae, like land plants, also sequester carbon dioxide; scientists have explored their potential to draw carbon dioxide out of the atmosphere. They have been used as fertiliser, food sources (such as seaweed), and could be a promising source of biofuel in a more sustainable world. However, some algae blooms can also be harmful – even lethal. Harmful algae bloom (HAB), as scientists have come to describe the phenomenon, often manifest by forming a kind of scum over a body of water that can be green, blue, brown or even red. But others are completely invisible. The problem has become increasingly widespread and the impact can be deadly to marine life. Off the eastern coast of the US, a dinoflagellate – a type of marine plankton named Alexandrium catenella – has the potential to make shellfish lethal. Its appearance routinely shuts down fisheries, crippling local economies. And it’s not just in the US: the same species has shut down mussel farms and recreational collecting of shellfish as far away as New Zealand. Other blooms wipe out marine life. In 2015, a bloom of various dinoflagellates off the coast of South Africa led to low-oxygen conditions, known as eutrophication, killing 200 tonnes of rock lobster. Freshwater blooms, like those in Lake Erie made up of cyanobacteria or blue-green algae, have not only shut down local water sources but have also been blamed for the death of dogs that had been swimming in them. It’s difficult to make generalisations about harmful algae blooms since specific species have different causes and impacts. Scientists have identified about 100 toxic bloom species in the oceans. Dozens of potentially harmful species of cyanobacteria are known to affect bodies of fresh water. During most of the past century, harmful algae blooms were rarely headline news, inspiring little scientific study beyond ecological curiosity. That has changed. Algae blooms are notoriously difficult to predict, but a global monitoring group known as HAEDAT is tracking them across the world as they occur. Harmful algae blooms, such as the one that hit Toledo’s water supply in 2014, are becoming more common and more toxic – and scientists say humans are to blame. “There’s no question that the HAB problem is a major global issue, and it is growing,” said Donald Anderson, director of the US National Office for Harmful Algal Blooms and a lab director at the Woods Hole Oceanographic Institute. “We also have more toxins, more toxic species, more areas and resources affected, and higher economic losses.”  The toxic bloom that took over Lake Erie in 2014 was formed by a cyanobacteria known as Microcystis Aeruginosa, for which farming is at least partly to blame. “You have people that still to this day will only use bottled water,” says Dr Timothy Davis, an expert in plankton ecology at Bowling Green University, five years after the water crisis and even after Toledo spent $132 million (£101 million) on improving its water treatment plant to handle the blue-green algae. Lake Erie, the shallowest of North America’s Great Lakes, has seen such events in the past. During the 1950s and 60s algae blooms were common, most likely, say researchers, due to poor domestic and industrial wastewater treatment. “At one point, Lake Erie was considered a dead lake,” Davis says. But by the early 1970s, the “dead lake” was resurrected, due to new regulations from the Clean Water Act and the Great Lakes Water Quality Agreement that capped phosphorus loads into the lake at 11,000 tonnes. Phosphorus provides nutrients to plants and is commonly found in manure and produced for fertiliser. Then in the late-1990s, blooms began to reappear. A cyanobacteria bloom requires two things: nutrients and heat. In the case of Lake Erie, nearby farms have become increasingly reliant on large inputs of synthetic fertiliser. “We went from agriculture that was small farms [and a] variety of crops to larger commercial farms that were harvested for essentially two row crops, corn and soya beans,” says Davis. Today, corn and soya beans are Ohio’s top crops. Employing more fertiliser to feed a global market, the farms’ excess phosphorus and nitrogen, another plant nutrient, washed out during storms and into the river and streams that feed Lake Erie. About 80% of the nutrients running into Lake Erie are from sources around the Maumee River, which in this case means agricultural runoff from the surrounding farmland. “If you have an agricultural system where the farmer can only survive by polluting Lake Erie, then there’s something fundamentally wrong with that system,” says Dr Thomas Bridgeman, director of the Lake Erie Center. Since the 1990s, Lake Erie has seen a bloom every year – and they appear to be lasting longer and getting larger. This year’s bloom in Lake Erie was the fifth largest since 2002 – when monitoring began in earnest. It was 620 square miles at its largest after growing throughout August, before dissipating in September. Meanwhile, climate change has heated up our planet substantially. Nearby Lake Superior, the most northerly of the Great Lakes and the world’s largest, has had its first documented cyanobacteria blooms over the past decade. Before climate change, the lake simply would have been too cold for a long-lasting bloom. It’s now almost a certainty that blooms will continue to appear every summer, say researchers, unless Ohio changes its agricultural practices and the global community finally tackles the climate crisis. “We have to look around and say, ‘Look, what do we grow here?’” says Bridgeman. “We grow corn and soya beans. Where does the corn go? It goes into our gas tanks. Where do the soya beans go? They go to China, they go to hogs. Is that really what we want to be doing with our watershed?” Algal blooms are also becoming more common and severe in many parts of our oceans, harming wildlife and posing potentially dangerous health impacts for local communities. Scientists say the “red tide” that stuck around the Florida coast from 2017 through to this year may now be a semi-normal part of the ecosystem. These blooms are pumping poison into the air, known as brevetoxin, which may be harmful to humans if inhaled. Anyone breathing it in can suffer from uncontrollable coughing and a sore throat. “It doesn’t make for a pleasant day at the beach,” says Malcolm McFarland, a researcher into algae blooms with the Harbor Branch Oceanographic Institute in Fort Pierce, Florida. It may have long-term health implications as well – one study found that brevetoxin attacked the DNA of lungs in rats, but further research is needed to understand the impact on human health. Scientists are less certain about the causes of these red tide marine blooms, but both nutrient runoff and climate change may play a role. “The red tide seems to initiate and peak in the rainy season when runoff from the land is highest, and nutrient inputs to freshwater and coastal water bodies spike,” says McFarland. Meanwhile, on the other side of the north American continent, a different red tide is attacking a different species: California is seeing more sick sea lions taken in by rescue centres; pups and adults are dying. Scientists believe they are suffering from eating fish tainted by Pseudo-nitzschia australis algae. The highly toxic algae are fatal at high doses, both to sea lions and humans. Unlike Florida’s red tides, those in California appear to be a very recent arrival. Until the turn of the millennium, large-scale toxic blooms were rare off the coast of California. Then something changed. “From [2000] forward, we had a very significant bloom every single year with ecosystem impacts in California, and that has never stopped. Not only that, it seemed as though things were getting more and more toxic,” says Clarissa Anderson, the executive director at Southern California Coastal Ocean Observing System, nothing that in less than 20 years of research, scientists have seen toxin numbers multiply by 200 – from 500 to 100,000 nanograms per litre of sea water. Anderson says the current best working theory is that increasing carbon sequestration by the oceans – due to the huge increase in greenhouse gas emissions since the industrial revolution – is behind the sudden regularity of these deadly blooms and an increase in their toxicity. She says the study of these events and their toxins is so new that there may be incidents of illness from eating affected fish or shellfish that are misdiagnosed because these poisons are not on the radar of many health organisations. Europe has had its own experience of deadly algae blooms that now threaten the future of its fisheries. Last year, the Baltic Sea experienced a bloom so large it could have encompassed Manhattan, and it closed beaches from Finland to Poland. Finland has been systematically sampling its area of the Baltic since 1979, giving us a clearer idea of the spread and growth of the problem, and what’s to blame. In that time, blooms have become larger and longer-lasting, creating dead zones and depleting Baltic fisheries. Like the example in Lake Erie, the Baltic bloom is caused by an influx of nutrients from agriculture and warming waters.Scientists are regularly tracking nutrient loads from Finland’s rivers into the sea. Data from 2014 in the HELCOM Pollution Load Compilation database, the best currently available, found that more than three-quarters of the nutrient load coming into the Archipelago Sea is from agriculture. The number is surprisingly similar to the proportion coming from industrialised agriculture in Ohio. The Baltic is a brackish water body, thus supporting blooms typical of both fresh and salt water. But, as in Lake Erie, of real concern are cyanobacteria: several species have been known to produce blooms here. Below the sea’s surface there has been a decline in the more nutritious phytoplankton – and food for fish – and an increase in the potentially toxic species in the more southerly parts of the sea since the early 1980s.Milder winters and increased rainfall pushing more nutrients into the sea, along with higher surface water temperatures – all due to the climate crisis – are also exacerbating factors, say researchers. Blooms usually begin in July and disappear by August or September. But last year a species particularly resistant to coldremained until November. “The ice was blue-green because of the cyanobacteria under it,” says Sirpa Lehtinen, an expert on plankton for the Marine Research Centre with the Finnish Environmental Institute, who adds that scientists are still trying to work out what this all means for the marine ecosystem, and whether fisheries in the Baltic are in serious long-term peril. So how can we solve a problem like algae? The answer, says Davis, will be part-regional, part-global solutions. For Lake Erie, it will require agricultural changes – including regulations to reduce the nutrient load – and tackling the climate crisis. But solutions elsewhere may be different, for example, blooms in developing countries might require better wastewater treatment. The 2014 water crisis in Ohio forced the issue politically which hasn’t happened in many other places. Governor Mike DeWine recently announced an initiative called H2Ohio, which is expected to include hundreds of millions of dollars for Lake Erie and other Ohio water bodies over the next 10 years. However, scientists say this is not enough. “It’s going to take a lot more money and a lot more political will than what’s happening right now,” says Bridgeman. At the Ohio Department of Agriculture,director Dorothy Pelanda said the department was primarily looking at voluntary programmes based on marketing and education for potential solutions such as cover crops and smarter use of fertiliser. In 2014, Ohio passed new regulations on fertiliser use for farms near the lake: such as not spreading before a storm or on frozen fields. “We know from science that there is not one solution to every farm … It’s about education, it’s about being sensitive to what works, what doesn’t work,” she said. She’s hoping to provide increased access for farmers to use high-tech, but often expensive, equipment that can give them a better idea of what parts of their land may need fertiliser and how much. Pelanda said she’s also seen interest in diversifying crops beyond corn and soya beans, to grapes, chestnuts and maple sugar. Asked if voluntary programmes will go far enough, Pelanda says: “That’s our challenge. We have to get beyond. We’re doing these things … but we’re not doing enough of these things. We need to really increase the voluntary adoption of these practices.” Others are more sceptical of voluntary approaches. “We have a long history in this country of a farmer does what he wants on his land. You can choose to take advantage of a programme or something, but you can also choose not to,” says Bridgeman, who believes local and federal governments can no longer afford to ignore the climate emergency. “We need to do something about climate change and we’re either going to be paying for it by reducing greenhouse gases or we’re going to be paying for it by additional treatment of water,” says Bridgeman, adding that most of the blooms around the world have a human element to them. One thing is certain. Algae blooms aren’t going away but are yet another sign – like ocean acidification, vanishing Arctic sea ice, and mass extinction of the Anthropocene – “of an ecosystem that is out of balance,” says McFarland. • This article was amended on 8 January 2020 to remove a reference to “toxic cryptophytes”. Cryptophytes are not toxic. Also, the text was changed to clarify that baltic bloom in Lake Erie is caused by a range of nutrients, not just nitrogen from farming."
nan
"Finding oneself improperly dressed for the weather can have fatal consequences – just ask a white-coated weasel. Animals that live in areas that usually have a lot of snow in the winter often change their coats to match their surroundings. Some weasels have evolved so that in the autumn they moult their brown summer coat and change to a white version. In spring they reverse the process and return to the brown version. A new study published in Nature Scientific Reports suggests that there is a strong relationship between the quantity and duration of snow in a forest in Poland during the winter and the number of weasels wearing white the following winter. Clearly this is not a fashion decision on the part of the local weasel population.  The scientists behind the study looked at two subspecies of the rather charmingly named “least weasel”, Mustela nivalis nivalis, which does change its colour in the winter, and Mustela nivalis vulgaris, which does not. Both species live in Białowieża, an ancient relic of the vast primeval forest that once covered most of Europe, and both compete for similar resources. Yet the authors determined that when there is no snow cover the white-coated weasels were more likely to be eaten by the foxes, wolves and buzzards that see them as prey.  When there is deep snow, weasels of both colours tend to hunt underneath it, which, after all, is where the small rodents are, so the predation rates for both species are reduced. In periods of winter without snow, however, white weasels are much more conspicuous than brown ones. In recent years, because of climate change, there have been more days without snow, and the date when snow disappears in the spring has become progressively earlier. The conclusion of the study is that the proportion of white weasels in the population is influenced by predation rates due to lack of camouflage during the previous winter. The question for the future seems to be how long M. n. nivalis as a subspecies can survive, given the milder, largely snow-free winters that are becoming more frequent as climate change takes hold. One of the important factors in this is likely to be its ability to change the time when it moults its coat, both in the autumn and the spring. Though the weasels are already showing signs of moulting into their brown summer coats earlier in the spring, they have not yet changed the time of their autumn moult. This is thought to be because autumn temperatures have remained relatively stable while springs have been getting warmer. Another problem for the weasel is that, even when it can shed its winter coat earlier, that will not be helpful in the increasing periods during the winter when there is no snow. Since moulting and growing a new coat is a serious business, chopping and changing during the season is not possible. In addition to ambient temperature, moulting and therefore coat colour change is triggered by day length, the change in which affects the weasel’s hormonal state. So starting a new moult is only possible during spring and autumn when the days reach the appropriate length as well as the right temperature. The issue of having the wrong coat for the weather is not confined to weasels. At least 22 species change their coat colour in winter including snowshoe hares, mountain hares and Arctic foxes. The various hare species face similar problems to weasels and arctic foxes, in so far as they are more vulnerable to predation when they are not camouflaged. However, as plant eaters, they don’t have to worry about also being more visible to their prey. For some animals, such as the Arctic fox, the changes in the timing of seasonal events, known as phenological changes, are not the only problems brought on by climate change. Because of the warming climate and the spread of their prey species northwards, red foxes, which are larger, bolder and more robust than their Arctic cousins, have been able to extend their range northwards, too, and are now threatening to out-compete Arctic foxes.  The situation with the weasels is not quite the same, as it is greater visibility to predators that is threatening the white colour morph, rather than direct physical competition with another species. However, unpublished data from the authors of the study suggest that M. n. vulgaris (the one that stays brown) has a higher resting metabolic rate that M. n. nivalis, which means they will require more calories just to stay alive. This may just give the white form of the weasel the edge in the winter when calories are scarce, which may help to redress the balance.  If not then the potential loss of this beautiful and unique subspecies of the least weasel because of anthropogenic climate change will be one more crime to be laid at the door of humanity."
"
Share this...FacebookTwitterMid-Ocean Seismicity Portends Global Cooling

Image Source: Viterito, 2016
Since the peak of the 2016 El Niño warming event, global temperatures have fallen by a little more than 0.3°C.

Image Source: WoodForTrees.org
According to a new paper published in Environment Pollution and Climate Change by Dr. Arthur Viterito, changes in seismic activity from the Earth’s high geothermal flux areas (HGFA) are “a significant predictor of global temperatures (p<0.05) but CO2 is not (p>0.05) (Table 1).”

An Overview Of The HGFA→Climate Link
Last year, Dr. Viterito succinctly explained the processes connecting high geothermal heat flux areas to the climate system.
 Viterito, 2017
“Namely, increased seismic activity in the HGFA (i.e., the mid-ocean’s spreading zones) serves as a proxy indicator of higher geothermal flux in these regions. The HGFA include the Mid-Atlantic Ridge, the East Pacific Rise, the West Chile Rise, the Ridges of the Indian Ocean, and the Ridges of the Antarctic/Southern Ocean.”
“This additional mid-ocean heating causes an acceleration of oceanic overturning and thermobaric convection, resulting in higher ocean temperatures and greater heat transport into the Arctic. This manifests itself as an anomaly known as the “Arctic Amplification,” where the Arctic warms to a much greater degree than the rest of the globe.”
“[J]umps in HGFA seismic activity can amplify an El Niño event, a phenomenon referred to as a SIENA or a Seismically Induced El Niño Amplification.  Accurately predicting two of these amplified El Niños (i.e., the 2015/2016 event plus the1997/1998 episode) is an important outcome of the HGFA seismicity/temperature relationship.”

New Paper: By 2019, Global Temps Will Drop To Mid-1990s Levels
In a new paper, Dr. Viterito has continued using seismic pattern analysis to formulate a very precise near-term temperature prediction: Global temperatures will continue their ongoing descent to about -0.47 °C below the 2016 peak by the year 2019.
Viterito, 2018
“A striking development for this experiment is that 2017 marks the first three-year decline in HGFA seismic activity since 1979 (Figure 2).  Furthermore, the 2017 HGFA seismic count is 49% lower than the study period’s peak frequency in 2014, the year of the last “Super El Niño”. When viewed within the context of the entire time series, the 2017 dropoff mirrors the jump in HGFA seismic activity experienced in 1995, albeit in the opposite direction. The 1995 “tipping point” was significant as global temperatures spiked in lockstep two years later, followed by a 21-year ‘plateau’ in both global temperatures and HGFA seismicity, a.k.a. ‘The Pause’.”

“It is important to note that a two-year lag is factored into the analysis: The 1979 HGFA seismic frequency is paired with the 1981 global temperature, the 1980 HGFA frequency is paired with the 1982 temperature, and so forth, for the entire series.”
“It is reasonable to conclude that this recent “gapping down” may be a tipping point towards cooler global temperatures. Using HGFAseismic frequencies as the sole predictor of global temperatures going forward, there is a 95% probability that global temperatures in 2019 will decline by 0.47°C ± 0.21°C from their 2016 peak. In other words, there is a 95% probability that 2019 temperatures will drop to levels not seen since the mid-1990s.”

Image Source (Top): WoodForTrees
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Swiss online SRF public television site here reports that German power engineering giant Siemens plans to eliminate some 6900 employees, half of them in Germany. Hit will be the conventional power plant and electric drive systems branch.
German energy sector in turmoil
The SRF writes that the power plant branch “is suffering due to the Energiewende“, Germany’s attempted transition to renewable energies. This branch alone will see 6100 job reductions. Turbine plants in Görlitz, Leipzig, Offenbach, Erfurt, Erlangen, Berlin and Mülheim (Ruhr) will be impacted. The announcement just before the start of the Christmas holiday season has angered trade unionists.
The Handelsblatt here reports that some of the impacted engineering workers are “in shock and in tears” over the news. Protests and strikes have been announced.
The news of the jobs cutbacks are the latest in a series of huge jobs reductions seen throughout the German energy sector. Over the past years, power giants such as RWE and Eon have announced the layoffs of thousands of its workers as the German Energiewende has eroded profits in the conventional energy sector and has led to skyrocketing electricity prices for consumers.
The misery has not only hit conventional energy jobs, but also renewable energy sector as well. Due to cheap imports from China and an uncertain investment future in the wind sector, most of Germany’s solar power equipment producers have become insolvent and thousands of jobs have been lost. Recently wind energy equipment producers such as Nordex have announced job cuts as well and the future for wind energy in Germany looks bleak.
As more volatile wind and solar energy capacity have come online, steam and gas turbines have become uneconomical to operate and investments in conventional power plants have taken a hit over the recent years.
The BBC has reported on the jobs cutback by Siemens, but made no made no mention of the Energiewende.
 
Share this...FacebookTwitter "
"We begin a new decade swamped by visions of our planet in peril. Australia is in flames; Greenland and Antarctic ice shelves are crumbling; thousands of species face extinction, and millions of humans are at risk of losing their homes as sea levels rise and deserts spread. At the same time, amounts of carbon dioxide in the atmosphere – the cause of the global heating that threatens to ravage our world – continue to increase unabated. Our future is being threatened in a manner that would have seemed unthinkable only a couple of decades ago.  It may therefore seem odd at this time for scientists to look away from our afflicted world and to take a renewed interest in issues that lie beyond Earth – in robot probes and vehicles that will take human beings beyond our atmosphere to the moon, Mars and the rest of the solar system. Yet this renewed interest in extraterrestrial matters is real as we make clear in our survey of forthcoming space missions. Not since the heyday of the space race to the moon in the 60s has so much space activity been planned, though this time missions will not be dominated by America and Russia but will also involve China, India and Japan as well as private companies such as SpaceX and Boeing, which plan to launch their own manned spaceships. For good measure, the European Space Agency, of which Britain is a major, active participant, has also decided over the past few weeks to step up its commitment to space exploration with a €12.5bn (£10.7bn) package of projects. Some of these missions will have a bearing on our planet’s present plight, of course. For example, Europe is planning a new fleet of satellites that will monitor carbon dioxide emissions at every point on Earth and so create the first global system for tracking key polluters. Many other observation satellites are planned and should help scientists contain the ecological threats of global heating. However, there is another aspect of interplanetary travel that is relevant to the woes afflicting Earth. From space, we get a proper appreciation of our world and its vulnerability. Until we sent probes to Mars and Venus, we thought our nearest planetary siblings could easily support life. Instead, spacecraft showed that Venus is an acid-drenched, scorching vision of hell while neighbouring Mars was found to have lost most its atmosphere aeons ago. Exactly why these worlds went wrong, in terms of their abilities to support life, is not clear but the matter certainly merits further research. The special perspective of our world that is provided by space missions was summed up by the late astronomer Carl Sagan when he described an image of Earth that had been beamed back by the Voyager 1 probe from a distance of 4bn miles in 1990. In that photograph, our world appeared as a tiny, single point of light. “That’s here. That’s home. That’s us,” said Sagan of this pale blue dot. “Our posturing, our imagined self-importance, the delusion that we have some privileged position in the universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.” And that, ultimately, is what space exploration really tells us. It says there is no Planet B and no chance of starting over again if we continue to make a mess of this world. The view from above shows Earth is precious and needs a lot more care and attention than it is getting at present."
"
Share this...FacebookTwitterIt’s safe to say that the only people who still believe the ultra-alarmist scenarios of the Potsdam Institute for Climate Impact Research are the leftist media and green activists. Even the government funders of this institute know they aren’t really true. After all Germany hasn’t cut CO2 emissions in close to 10 years.
Dr. Sebastian Lüning at Die Kalte Sonne exposes the latest dubious attempt by Potsdam scientist, Stefan Rahmstorf, to spread climate fear and to attack on journalist Daniel Wetzel of flagship daily Die Welt, who not long ago dared to question the science.
The method of attack used by Rahmstorf is every time the same:

Smear the dissenting journalist as a con-man.
Insist the science has long been settled (it isn’t).
Float out charts that use statistical trickery to mislead.

===================================
Again and again: Stefan Rahmstorf and his solar trick
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(Translated/edited by P Gosselin)
The Sherlock Holmes of climate sciences, Stefan Rahmstorf, at his climate blog post “Klimawandel XY Ungelöst” at Klimalounge, warned on 29 July 2017 – of climate con-men, fraudsters and hustlers:
The global CO2 increase: the facts and the tricks of con-men
The facts surrounding CO2 rise are clear, unequivocal and agreed on – yet Die Welt again and again gladly recycles old, worn-out climate skeptic myths. Are forests to blame for the CO2 rise?”
Here Stefan Rahmstorf’s rails against an article by Daniel Wetzel “Kurzschluss bei der Energiewende” [The Energiewende shorts out] in Die Welt, where Wetzel dared to question Rahmstorf’s favorite project. The main focus was man’s share of the total CO2 budget, which is a rather dry issue in itself. Also the article looked at the magnitude and its signficance. Depending on its toxicity, even small amounts can have an impact. The same old stuff.
But looking at his Figure 5, Rahmstorf’s seriousness really needs to be called into question. It involves his favorite chart which he regularly presents. Here it is (Fig. 2):


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2.  Chart from Rahmstorf’s Blog posting “The global CO2 rise: the facts and the tricks of the con-men” dated 29 July, 2017.
Rahmstorf*’s text concerning the chart follows:
Curves showing global temperature, CO2-concentration and solar activity. Temperature and CO2 are scaled so that they correspond to the expected CO2-effect on climate (e.g. the best estimation of climate sensitivity). The amplitude of the solar curve is scaled in such a way as to correspond to the observed correlation between solar data and temperature data. (Details are explained here). You can generate this chart here and copy a code there that allows you to install the chart as a widget on your own website (like at my home page) – where here every year it is updated with the latest data. Thanks to Bernd Herd, who programmed it).
First remark: Contrary to Rahmstorf’s claim, there is no “best estimate of climate senstitivity“. The 5th IPCC report intentionally left this value open as no agreement among the report’s authors could be reached. Instead a very broad range of 1.5°C to 4.5°C for a doubling of CO2 was given, which ranges from manageable to catastrophic.
Second remark: The scaling of the solar curve was designed so as to make it impossible to detect a trend. Also the solar curve that was purposely selected is not really representative if one looks at the solar reconstructions of isotopes and cosmic rays. A more scientifically robust version of the chart would look as follows:

Figure 2: Global temperature (GISS), CO2-concentration and solar activity (Steinhilber et al. 2009).
Rahmstorf complains about con-men and tricksters, but completely fails himself when put to the test. Is this person, who gladly speaks at Green Party campaign events, really as credible as he fancies himself to be?
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI lived in Arizona for a few years and I remember flash floods occurring regularly during the rainy summer season. Good to see things are getting less extreme there. Thanks, global warming! -PG
===================================
Researchers surprised: extreme rainfall in Arizona has decreased over the past 50 years despite climate warming
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated and edited by P Gosselin)
Climate change leads to more extreme rainfall events and worse flooding. That’s the common claim that gets made before one looks at the data.
Bit by bit, however, researchers are filling in the data gaps and finding one surprise after another. For example the University of Bristol put out a press release last October on the extreme rainfall trends in Arizona: In summary: Extreme rainfall has decreased over the past half century despite climate warming.
Moreover, total rainfall fortunately has risen. The alarmist general assumption of increased extreme rainfall events has failed to materialize. The press release follows:
Rainfall trends in arid regions buck commonly held climate change theories
The recent intense hurricanes in the Atlantic have sharply focused attention on how climate change can exacerbate extreme weather events. Scientific research suggests that global warming causes heavier rainfall because a hotter atmosphere can hold more moisture and warmer oceans evaporate faster feeding the atmosphere with more moisture. However, this link between climate warming and heavy rainfall has only been examined in particular regions where moisture availability is relatively high. Until now, no research has been undertaken that examines this relationship in dryland regions where short, sharp rainstorms are the dominant source of precipitation and where moisture availability on land is extremely limited.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




To explore the links between climatic warming and rainfall in drylands, scientists from the Universities of Cardiff and Bristol analysed more than 50 years of detailed rainfall data (measured every minute) from a semi-arid drainage basin in south east Arizona exhibiting an upward trend in temperatures during that period. The analysis demonstrated a decline in rainfall intensity, despite an increase in total rainfall over the years. Interestingly, the study shows that there is a long-term decline in heavy rainfall events (greater than 25 mm/h) and an associated increase in the number of smaller storms each delivering less rainfall. This result is contrary to commonly held assumptions about rainfall trends under climate change.
Lead author, Dr Michael Singer from School of Earth and Ocean Sciences at Cardiff University, said: “In drylands, convective (or short, intense) rainfall controls water supply, flood risk and soil moisture but we have had little information on how atmospheric warming will affect the characteristics of such rainstorms, given the limited moisture in these areas.” Co-author, Dr Katerina Michaelides, from the School of Geographical Sciences and Cabot Institute at the University of Bristol, said: “Our findings are consistent with previous research in the Colorado Basin which has revealed a decline in runoff in the upper part of the Basin. “Our work demonstrates that there is a more regional decline in water resources in this dryland region, which may be found in other dryland regions of the world.”
Since trends in convective rainfall are not easily detected in daily rainfall records, or well-simulated by global or regional climate models, the researchers created a new tool to assess the effects of climate change on rainfall patterns and trends in dryland areas. Their new model, STORM, simulates individual rainstorms and their expression over a river basin, and it can represent different classes of climate change over many decades. Drs Singer and Michaelides employ STORM to show that the historical rainfall trends likely resulted in less runoff from this dryland basin, an effect they expect to have occurred at many similar basins in the region. Dr Singer added: “We see this model as a useful tool to simulate climate change in regions and cases where traditional models and methods don’t capture the trends.”
Paper: ‘Deciphering the expression of climate change within the Lower Colorado River basin by stochastic simulation of convective rainfall’ by M. Bliss Singer and K. Michaelides in Environmental Research Letters.”
In May 2015 extensive flooding occurred in Texas and Oklahoma due to heavy rainfall. This was not caused by a hurricane, but rather by an El Nino, which indeed leads to extreme rainfall events, according to a study by Wang et al. 2015.
In another study by Wang et al. 2014, researchers found a flooding pattern in the region of the Missouri River which happened to follow a Pacific Ocean cycle. Knowledge of this link now allows better forecasts to be made and preventive efforts to be taken. Washington University in St. Louis reminded that not everything can be attributed in knee-jerk fashion to climate change. Extreme rain at the end of 2015 in Missouri led to terrible flooding. Part of the blame here was assigned to the changes in building in flood-prone areas of the river which led to an obstruction of the outflow.
In another study the University of Colorado in Denver was able to show that also the state of Colorado is poorly prepared for flooding. Important bridges and infrastructure urgently need to be upgraded. Damage that occurred in a flood in 2013 would have been much less had the structures been strengthened and better taken care of.

Share this...FacebookTwitter "
"Just hours after saying it was “still the plan” to go to India later this month for trade and defence talks, Scott Morrison now says he is “inclined not to proceed” with the visit. Australia’s coal exports were expected to feature heavily on the Indian trip agenda, but shortly after telling media he still planned on attending, depending on the fire conditions, Morrison reversed course and said he now did not believe he would go. “The national security committee is going to hook up in the morning on this,” he said. “I’m inclined not to proceed on that visit. There are issues I need to resolve formally when working through issues of that nature. That is my inclination on that issue. We’ll make a further announcement and arrangement on that accordingly.” Morrison was due to travel to India on 13 January, before heading on to Japan during the five-day trip. Speaking to Melbourne radio 3AW on Friday morning, Morrison said it was an “important meeting”, and that the plan to attend was “still in place.” “But, you know, when you are dealing with these issues, you need to consider the relative merits of the choices,” he said. The prime minister announced he had accepted Indian prime minister Narendra Modi’s invitation to visit back in October, where he was also due to deliver the inaugural Raisina Dialogue address, which aims to discuss the biggest challenges to the global economy. For Australia, one of those challenges is its fractious relationship with China, with coal exports seen as one of the flashpoints. Australia has increasingly looked to India as a potential buttress against any economic fallout with China, with Morrison making strengthening his relationship with Modi one of his priorities since becoming prime minister. The Coalition has also been an unwavering supporter of the Adani Carmichael mine in central Queensland, with minister for resources and northern Australia Matt Canavan a vocal supporter. Canavan attended a lunch with Adani executives  during an official visit to Calcutta in August last year. The government has often pointed to the 1.3% of global carbon emissions Australia is responsible for, when answering criticisms over its climate policies. A report by the Australian Conservation Foundation, released mid last year, found Australia was responsible for at least 5% of global greenhouse gas emissions, if the pollution from the nation’s fossil fuel exports was included. Once the pollution from proposed projects was factored in, which included the Adani mine, that number had the potential to jump to 17% by 2030. Morrison has held firm to his view the government had its climate policy settings right, despite admitting the link between climate change and worsening bushfire seasons. “Let me be clear to the Australian people: our emissions reductions policies will both protect our environment and seek to reduce the risks and hazards that we are seeing today and at the same time, it will seek to ensure the viability of people’s jobs and their livelihoods all around the country,” he said on Thursday. “What we will do is ensure that our policies remain sensible, that they don’t move towards either extreme and stay focused on what Australians need for a vibrant and viable economy, as well as a vibrant and sustainable environment. Getting the balance right is what Australia, I think, has always been able to achieve.” The Labor leader, Anthony Albanese, who has also been supportive of Australia’s coal industry, said the decision of whether to go to India or not was one for the prime minister. “The prime minister will have to weigh up whether it’s appropriate for him to go or not,” he said. Albanese said he believed people were now seeing the warnings scientists had issued about the worsening impacts climate change would wreak against Australia, and wanted action. “One of the reasons why we have seen, I think, some frustration expressed by people on the ground, they don’t want to be told it’s a natural disaster,” he said. “Yes, Australia’s had natural disasters in the past. We haven’t, in my lifetime, had people on beaches waiting to be evacuated in life jackets, sending boats out to sea like it’s a peacetime version of something that we have seen during wartime. We have not seen that. “This is not business as usual. And it requires national leadership and response. This is a national emergency and it’s important that the response be appropriate to the scale of this emergency.”"
"No doubt, you heard the good news. Barack Obama has announced the US is pushing through plans to reduce emissions of greenhouse gases. Rejoice! Rejoice! We’ve got this climate problem licked – hurrah! Hold the champagne – and not just because it’s full of bubbles of carbon dioxide – while we do a reality check. This is a distinctly underwhelming development. Let’s pick apart the spin from the reality. First, the way the story has been told – the US commits to a 32% reduction in emissions of greenhouse gases by 2030. This is being pushed through by tightening the rules governed by the Environmental Protection Agency (EPA) – a federal agency that the US president can instruct, without the need to get past those pesky filibusterers in the dysfunctional, Republican-dominated houses of Congress. The EPA is confident that its rules have a firm legal footing and will be able to withstand the inevitable court challenges.  The effect of the rules will be to clobber the production of electricity using coal. This is certainly “a good thing”. Quite apart from coal’s high carbon-intensity as a means of producing power, it is dirty in other ways – resulting in pollution that is harmful to human health as well as to the environment. With this commitment, the US can enter the climate negotiations in Paris with its head held high and can push for a global deal to head off dangerous climate change. Whoo-hoo! The reality – it’s a development that is both fragile and feeble. Fragile, because unless every occupant of the White House between now and 2030 is a Democrat, it can be unpicked. Remember the bit about the EPA being a federal agency that the US president can instruct? Well, if the president happens to be a climate-denying Republican (the two words are almost, but not completely, interchangeable), he or she could countermand the previous instruction. Of course, the ruling now will inform business decisions and will have a long-lasting effect, regardless of whether the rules are subsequently reversed, but believing that this announcement sets in stone the target of a 32% reduction in emissions requires a degree of optimism that lies somewhere between the heroic and the delusional. It’s feeble too. A 32% reduction by 2030 sounds quite impressive until you realise it is baselined on 2005 figures. By 2013 (the latest data available) it had already fallen by 15% so the rate of improvement required in the next 15 years is actually slower than what has already been achieved. From 2005-2013, emissions fell at a rate of 2.0% per year – to meet the commitment, emissions would need to fall by just 1.3% per year between 2013 and 2030. And the 32% reduction figure only relates to the emissions from power generation, which makes up less than a third of total US emissions. How does the rate of committed reductions compare with what is actually required to achieve temperature rises of less than 2°C above pre-industrial levels? According to trillionthtonne.org – a website that tracks these things – to have a better than 50% chance of avoiding such dangerous climate change would require emissions to decrease by more than 2.6% per year. For the rest of time. Globally. Not that the US should come in for special criticism – the commitments from the EU and from China are similarly insufficient to head off the threat of dangerous climate change. So why hasn’t this been reported? I think what is going on here is partisanship and a well-intentioned desire to boost the prospects of a meaningful deal in Paris. Climate-denying Republicans hate this plan (of course), therefore all good climate realists see it as a triumph. But it is a tiny, tiny step in the right direction and climatically immaterial. Ah yes, you say, but it’s politically important – the world’s hegemonic power has made a commitment, and that creates a foundation upon which greater progress can be made. Let’s not be pessimistic – this could be the start of a global deal. Well, this emperor has no clothes. The pronouncement reminds me of the words of Neville Chamberlain on his return from the Munich Conference in 1938: “I believe it is ‘peace for our time’. Go home and get a nice quiet sleep.”"
nan
"The prime minister, Scott Morrison, has announced at least $2bn for bushfire recovery, as the government steps away from its pledge to deliver a budget surplus amid the ongoing crisis. Warning that the fires would keep burning over the coming months, Morrison said further government funds may yet be forthcoming as the economic toll from the horror fire season continued to rise.  “The fires are still burning, and they will be burning for months to come,” Morrison said. “If further funds are required, further funds will be provided.” He said that across government “significant and massive” financial commitments were being made, with the final cost likely to rival the $5.6bn paid out in disaster recovery assistance over six years following cyclone Yasi and the Brisbane floods. After meeting on Monday morning, cabinet signed off on an initial $2bn for a national bushfire recovery fund to be used to support the rebuilding of community infrastructure and to help affected farmers and businesses. The fund will be overseen by the newly established National Bushfire Recovery Agency, led by the former Australian Federal Police commissioner Andrew Colvin. This would include funding for roads and telecommunications, the restocking and replenishing of livestock for primary producers, mental health support, funding to attract tourists back to the regions and environmental restoration. The $2bn is in addition to disaster recovery payments and allowances for those affected, which have so far totalled more than $100m. The government also announced 20 Service Australia pop-ups would be set up to help people access government disaster payments, while mutual obligation and debt recovery for welfare payments in nominated areas would also be suspended. “Today’s cabinet was one of great resolve; it was one where we stood together and said, ‘whatever it takes, whatever it costs, we will ensure the resilience and future of this country’, and we will do it by investing in the work that needs to be done,” Morrison said. When asked if the economic impact of the fires could jeopardise the $5bn surplus forecast for the 2019-20 year – the first in 12 years and a key election promise – Morrison said the government was not focused on the financial cost. “The surplus is of no focus for me, what matters to me is the human cost and meeting whatever costs we need to meet,” he said. “I can tell you this: being in the position of strength we are in now enables us to give what is one of the most significant, if not most significant, response to a crisis of this kind the country has seen.” Morrison argued that the government was putting the surplus “straight to work” to meet the needs of Australians, without having to put in place extra levies or make cuts. Does climate change cause bushfires? The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  What is the evidence on rising temperatures?  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. What other effects do carbon emissions have? Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  So is climate change making everything dryer?  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. What do recent weather patterns show? The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Is arson a factor in this year's extreme bushfires? Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. The treasurer, Josh Frydenberg, said it was “too early to tell” what the full economic cost of the bushfires would be, but emphasised the government had never seen the surplus as an “end in itself”, despite it being a prominent feature of this year’s budget. Frydenberg said he would be meeting with the Australian Insurance Council, regulators and company chiefs on Tuesday to ensure insurance payments flowed to affected individuals as quickly as possible, with 6,000 payments totalling almost $400m already paid out. The treasurer also said the Australian Taxation Office had agreed to a two-month deferral for the tax obligations of people living in fire-affected areas, and banks were also assisting. “People should not be concerned about their tax affairs at this time,” he said. Earlier, Greg Mullins, who chairs the Emergency Leaders for Climate Change group, welcomed extra resources from the federal government, but said they had been snubbed when a request was first made in April. “It’s great to see things finally moving but I hope in the future this government will learn to listen to people on the front line,” Mullins said. He also criticised the government’s inaction on climate change, saying it was a “load of rubbish” that the Coalition was taking strong action, with its ability to meet the Paris emission reduction targets based on its earlier “weak” Kyoto targets. “I worry for my grandchildren, their grandchildren. If this is how it is now, this is driven by climate change, imagine what future generations are up against.” Morrison was again asked on Monday about the Coalition’s climate change policies, which the former Liberal deputy leader Julie Bishop had criticised for lacking in global leadership. “We should be showing leadership on the issue of climate change. I attended a number of international conferences and countries do look to Australia for direction, for guidance and leadership. And I believe we should be showing leadership on the issue of climate change,” Bishop told Nine’s Today show. Morrison said his focus was on the $2bn package. “The government will continue to work to meet and beat the commitments we have made for emission reductions,” Morrison said, emphasising that Bishop was part of the cabinet that signed off on the Coalition’s policy. The opposition leader, Anthony Albanese, supported the funding for the recovery agency, saying Labor had been calling for greater federal assistance. “It’s good that the government is now making a number of announcements that we have argued for, including the need for a national response, including the economic compensation for volunteer firefighters, including the upgrade of our aerial firefighting capacity, and the increased use of the defence force,” Albanese said."
"
Share this...FacebookTwitterNote: This post will remain an extra day…
=======================================
More than 70 recent scientific publications show that there is absolutely nothing unusual about the magnitude and rapidity of today’s sea level changes. These academically peer-reviewed papers show that sea levels were on average 2 meters higher earlier in the Holocene than they are today.
Before the advent of the industrial revolution in the late 18th to early 19th centuries, carbon dioxide (CO2) concentrations hovered between 260 to 280 parts per million (ppm).

Within the last century, atmospheric CO2 concentrations have risen dramatically.  Just recently they eclipsed 400 ppm.
Scientists like Dr. James Hansen have concluded that pre-industrial CO2 levels were climatically ideal.  Though less optimal, atmospheric CO2 concentrations up to 350 ppm have been characterized as climatically “safe”.  However, CO2 concentrations above 350 ppm are thought to be dangerous to the Earth system.  It is believed that such “high” concentrations could lead to rapid warming, glacier and ice sheet melt, and a harrowing sea level rise of 10 feet within 50 years.
To reach those catastrophic levels (10 feet within 50 years) predicted by proponents of sea level rise alarmism, the current “anthropogenic” change rate of +0.14 of a centimeter per year (since 1958) will need to immediately explode into +6.1 centimeters  per year.  The likelihood of this happening is remote, especially considering Greenland and Antarctica combined only contributed a grand total of 1.54 cm since 1958 (Frederiske et al., 2018).
70+ Papers: Sea Levels 2+ m Higher 9,000-4,000
Years Ago While CO2 Levels Were ‘Safe’ (265 ppm)
(Click the link above or at the right side bar)
• Are Modern ‘Anthropogenic’ Sea Levels Rising At An Unprecedented Rate?  No.
Despite the surge in CO2 concentrations since 1900, the UN’s Intergovernmental Panel on Climate Change (IPCC) has concluded that global sea levels only rose by an average of 1.7 mm/yr during the entire 1901-2010 period, which is a rate of just 0.17 of a meter per century.
During the 1958 to 2014 period, when CO2 emissions rose dramatically, a recent analysis revealed that the rate of sea level rise slowed to between 1.3 mm/yr to 1.5 mm/yr, or just 0.14 of a meter per century.
Frederiske et al.,2018  “Anthropogenic” Global Sea Level Rise Rate (1958-2014): +0.14 of a meter per century
“For the first time, it is shown that for most basins the reconstructed sea level trend and acceleration can be explained by the sum of contributors, as well as a large part of the decadal variability. The global-mean sea level reconstruction shows a trend of 1.5 ± 0.2mm yr−1 over 1958–2014 (1σ), compared to 1.3 ± 0.1 mm yr−1 for the sum of contributors.”
In the past few thousand years, in contrast, sea levels in some regions rose and fell at rates of + or – 0.5 to 1.1 meters per century., which is 4 to 7 times greater than the change since 1958.
Hansen et al., 2016
“Continuous record of Holocene sea-level changes … (4900 years BP to present). … The curve reveals eight centennial sea-level oscillations of 0.5-1.1 m superimposed on the general trend of the RSL [relative sea level] curve.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




•Other regions have also undergone profound sea level oscillations in the last few thousand years that far exceed modern changes.



Image Sources: Bracco et al., 2014   Whitfield et al., 2017    Strachan et al., 2014 
  Hein et al., 2014    Miguel et al., 2018
•Modern changes aren’t even detectable on graphs of long-term sea level trends.
  

Image Sources: Dura et al., 2016   Bradley et al., 2016
 Scheffers et al., 2012  Kane et al., 2017
 • ~15,000 – 11,000 Years Ago, Sea Levels Rose At Rates Of +4 to +6 Meters Per Century
Cronin et al., 2017   Global Sea Level Rise Rate: +4 meters per century (14,500 to 14,000 years ago)
“Rates and patterns of global sea level rise (SLR) following the last glacial maximum (LGM) are known from radiometric ages on coral reefs from Barbados, Tahiti, New Guinea, and the Indian Ocean, as well as sediment records from the Sunda Shelf and elsewhere. … Lambeck et al. (2014) estimate mean global rates during the main deglaciation phase of 16.5 to 8.2 kiloannum (ka) [16,500 to 8,200 years ago] at 12 mm yr−1 [+1.2 meters per century] with more rapid SLR [sea level rise] rates (∼ 40 mm yr−1) [+4 meters per century] during meltwater pulse 1A ∼ 14.5–14.0 ka [14,500 to 14,000 years ago].”
Abdul et al., 2017   Global Sea Level Rise Rate: +4 meters per century (11,450 to 11,100 years ago)
“We find that sea level tracked the climate oscillations remarkably well. Sea-level rise was fast in the early Allerød (25 mm yr-1), but decreased smoothly into the Younger Dryas (7 mm yr-1) when the rate plateaued to <4 mm yr-1here termed a sea-level “slow stand”. No evidence was found indicating a jump in sea level at the beginning of the Younger Dryas as proposed by some researchers. Following the “slow-stand”, the rate of sea-level rise accelerated rapidly, producing the 14 ± 2 m sea-level jump known as MWP-1B; occurred between 11.45 and 11.1 kyr BP with peak sea-level rise reaching 40 mm yr-1 [+4 meters per century].”
Ivanovic et al., 2017  Northern Hemisphere Sea Level Rise Rate: +3.5 to +6.5 meters per century (~14,500 years ago)
“During the Last Glacial Maximum 26–19 thousand years ago (ka), a vast ice sheet stretched over North America [Clark et al., 2009]. In subsequent millennia, as climate warmed and this ice sheet decayed, large volumes of meltwater flooded to the oceans [Tarasov and Peltier, 2006; Wickert, 2016]. This period, known as the “last deglaciation,” included episodes of abrupt climate change, such as the Bølling warming [~14.7–14.5 ka], when Northern Hemisphere temperatures increased by 4–5°C in just a few decades [Lea et al., 2003; Buizert et al., 2014], coinciding with a 12–22 m sea level rise in less than 340 years [3.5 to 6.5 meters per century] (Meltwater Pulse 1a (MWP1a)) [Deschamps et al., 2012].”
Zecchin et al., 2015 Regional Sea Level Rise Rate: +6 meters per century (14,500-11,500 years ago)
“[M]elt-water pulses have punctuated the post-glacial relative sea-level rise with rates up to 60 mm/yr. [6 meters per century] for a few centuries.”
It has become more and more apparent that sea levels rise and fall without any obvious connection to CO2 concentrations.  And if an anthropogenic signal cannot be conspicuously connected to sea level rise (as scientists have noted), then the greatest perceived existential threat promulgated by advocates of dangerous anthropogenic global warming will no longer be worth considering.
70+ Papers: Sea Levels 2+ m Higher 9,000-4,000
Years Ago While CO2 Levels Were ‘Safe’ (265 ppm)
(Click the link above or at the right side bar)
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSchneefan at German weather and climate analysis site wobleibtdieerderwaermung.de here brings us the latest on atmospheric temperatures.
First we note that the middle troposphere (7,500 meters) as measured by NASA has seen recently a sharp cooling off since the start of April:

The chart shows the daily mean temperature at about 7,500 meters altitude, i.e. middle troposphere (400 mb/hPa). Here we see that temperatures have dived (pink curve) and reached a near 17-year low for this time of the year. Source: https://ghrc.nsstc.nasa.gov/amsutemps/
An enlargement shows a comparison to last year, last updated April 9, 2018.

Temperature at 7,500 m altitude have dived steeply since early April. Source: https://ghrc.nsstc.nasa.gov/amsutemps/
Near surface temperatures sharply down
Also the global 2m surface temperature is showing a strong downward trend:

The plotted data have already been adjusted (falsified?) with the NASA/GISS factor, and so may actually be even lower. Source: http://www.karstenhaustein.com/climate.php
Should the cold temperatures persist, April, 2018, could fall below the zero anomaly for the first time since 2012, foremost with the UAH  satellite data. The following UAH chart shows lower tropospheric temperatures (1500) continuing their decline after the warm peak caused by the natural El Niño phenomenon:

Source: UAH Global Temperature Update for March, 2018: +0.24 deg. C


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In March, 2018, lower tropospheric temperatures (1500m) over the oceans (71% of the earth’s surface) also saw a further drop:

Source: climate4you.
More cooling over the coming months
The following chart shows the El Niño Southern Oscillation (ENSO) 3.4 plot, i.e. the ocean surface temperature anomaly of the western equatorial Pacific region, since July 2016.

Source: climexp.knmi.nl/start.cgi
Much of the last 2 years has been in the globe-cooling La Niña phase (blue). And as satellite instrument temperatures tend to lag the El Niño temperature anomalies by some 4 months, further surface cooling is expected to show up in the satellite data over the coming months.
ENSO indicates more cooling ahead
The Southern Oscillation Index (SOI) is an indicator for the development of the easterly trade winds at the equatorial Pacific and thus tells us what’s ahead for the ENSO. Recall that the ENSO has a powerful impact on global surface temperatures. SOI values over +7.0 indicates La Niña conditions 2 months ahead, while an SOI under -7.0 points El Niño conditions.
Currently the following chart shows the SOI is at +13.7, which means the globe-cooling La Niña should continue on two months from now, and thus means cooler satellite measurements showing up 6 months later.

The Southern Oscillation Index (SOI) is currently at +13,7, thus pointing to La Niña conditions 2 months from now. Source: www.bom.gov.au/climate/enso/
Recently some scientists have postulated that the ENSO is impacted by solar activity, which currently is at a low (earth-cooling) level. The next solar cycle is expected to be a weak one, thus boding more cooling and tough winters ahead.
It’s got nothing to do with trace gas CO2.
Share this...FacebookTwitter "
"Once again the summer holidays have arrived and many people will be jetting off to the beach. However, few tourists will notice the tonnes of sand and gravel that weren’t there the previous year. Each spring, an army of bulldozers gets to work fixing storm and tidal damage on many of the world’s most famous beaches. Fresh sand or gravel (a term encompassing everything from tiny pebbles to chunky stones) is placed where necessary, allowing waves and tides to redistribute the material. It can be dumped in the right spots by bulldozers, or shipped in on barges. It can also be pumped in through pipes, as seen in the video below; the mixed-in water is allowed to drain off and the remaining sand is shoved into the right places. All this is partly driven by the influx of visitors and their rocketing expectations of perfect sand. But beaches aren’t just eye candy for tourists – they themselves have an important role in combating erosion through soaking up wave energy before the sea smashes into the cliffs or seawall. So despite the expense, there’s more demand than ever for what’s known as “beach nourishment”.  If this seems artificial, as though something pristine was being unnecessarily tampered with, then don’t forget beaches weren’t always there. In fact most are relatively new, formed during the past 5,000 or so years after the modern sea level was established. However in recent decades rising sea levels and more frequent storms have led to rapid beach erosion. By the 1990s just 10% of the world’s beaches were still growing, while more than 70% were eroding – with the world still warming and sea levels rising, things will only have got worse since then. To combat this, beaches such as Venice’s Lido or a number along Italy’s north-west coast are “re-nourished” each spring to repair the winter damage. Over the years this adds up. One analysis of the famous beach in Nice, south France, found 558,000 cubic metres of gravel was added during the 30 years to 2005 – that’s enough gravel to fill up Big Ben’s clock tower 119 times over. Along the Costa Brava, on Spain’s north-east coast, entirely artificial beaches have been created, sustained by a combination of breakwaters, groynes (structures built out from the shore to prevent the movement of sediment) and a dump of fresh sand each year. Perhaps the most famous example of all is the artificial beach in Barcelona, which has been nourished with sand from offshore over the past two decades. The loss of the sediment here is only partially due to storms, with the normal action of the waves causing slow erosion – the area simply isn’t meant to have a massive sandy beach. In contrast to many other parts of the Mediterranean, the coastline in our home country of Croatia managed to remain more or less natural. Its rocky coastline isn’t ideal for the creation of large sandy beaches, and political upheavals in the 1990s interrupted the growth of tourism. Now the country has once again become a popular tourist destination, and the beauty of its small and well-hidden gravel beaches are a valuable part of its attraction.  Many beaches have therefore been extended or created from scratch, and our current research is focused on the resilience of artificial gravel beaches in Croatia. Their sustainability depends on factors including a continued supply of sediment, and how well they stand up to stormy weather. Understanding the behaviour of natural and artificial beaches in different environmental conditions is crucial for their maintenance. A number of techniques are used to detect beach changes including advanced GPS surveys, laser scanning, or images from fixed cameras and drones  Our work in Croatia, for instance, uses a simple digital camera combined with advanced computing methods to understand the changing shape of the beach. We also study our nearby coastline in Fylde, Lancashire. With our help, both day trips and holidays to the beach should remain a pleasant feature of our lives into the future."
nan
nan
"
Share this...FacebookTwitterIt’s the food, stupid!
Something off topic today, and intended as food for thought. It’s my infrequent post on nutrition. Yes, I also would like my readers to be healthy and happy.
Stress, lifestyle, pollution are over-rated factors
In western societies, when asked why so many people are getting sick with chronic diseases today, and thus die prematurely, too many medical experts like to blame it on stress!, genetics, pollution, lifestyle, or just plain bad luck. So take my pills!
However, these often cited reasons are all too often bogus explanations designed to get you to accept horrendously costly treatments instead of opting for effective prevention. Prevention and cure, after all, are bad for Big Pharma’s bottom line. The big money is in lifetime treatment.
And in western societies, far too little is being done to drive home the point that the junk-food contaminated Western nutrition is the root cause of many chronic diseases and thus our food supply needs to be revamped radically and rapidly. If this were done, much misery and high costs could be spared.
Life-shortening disease often food-related
What a lot physicians won’t tell you is that many of these diseases are in fact nutrition-related and gradually emerge after years of poor quality (low-nutrient) diets. Many common chronic diseases could be prevented, or at least delayed by years, simply by eating high-nutrient foods. The most important thing you can do to live longer and healthier is to eat correctly, and to start doing so immediately.. Your doctor can tell you to reduce stress and to go for walks all he wants, but unless you wean yourself off the junk food, you’ll very likely end up chronically ill and forever connected to the miserable Big Pharma lifeline.
High stress Japan has highest life expectancy!
To illustrate how stress is an over-rated factor in causing chronic illness, one only needs to check out the countries with the highest life expectancies. Many happen to be high stress environments, like Japan and Europe. Of course stress is generally to be avoided, but is it really the big silent killer everyone makes it out to be? Statistics show us the answer is no. The big killer is junk food.

High stress, urbanized Japan No. 1 in life expectancy. Source: WHO. 
Anyone who has ever visited Japan will tell you that most people there live in high-stress, urban environments and work among the longest number of hours annually. Most Japanese in fact do not spend their time meditating in harmonious Zen rock gardens.
Japanese and Mediterranen diets lead to long lives


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Arguably the most important factor to Japan’s excellent collective health is its nutritional culture. Whereas in western culture many people stuff themselves with sugary, processed junk foods, snacks, sodas and sweets and bad oils, the Japanese diet is rich in vegetables, high-nutrient carbs, fish, fermented foods, antioxidant super-foods and green tea to name a few.


American (sick) lives simply dragged out by Big Pharma
But what about USA’s relatively high life expectancy? First, it really isn’t that high, and actually ranks a lowly 31st on the WHO list above!
And although the American diet is among the world’s most notorious, American’s do live about 80 years, and so food can’t be the big factor one might argue. Though Americans live relatively long, we need to keep in mind that huge numbers of them are plagued by chronic disease, and many are simply kept alive in a state of limited health or outright misery for years by Big Pharma. The average American today spends some $10,000 annually on health care alone.
Here’s what some countries spend on health care per capita:

The USA spends the most by far on health care, yet does not even make the Top 30 on the list for life expectancy. On the other hand, high stress, urbanized Japan spends near the OECD average on health care, yet has the highest life expectancy. Italy also sees a good life-expectancy-to-healthcare-spending-ratio, arguably in large part due to its health Mediterranen diet. Chart: OECD Data: Health resources – Health spending.
A longer life at a fraction of the healthcare cost
Also we note that poorer Mediterranean countries such as Greece, Cyprus, Spain and Malta spend a mere fraction on healthcare of what Americans spend, yet live longer. It’s the food that makes the difference.
Big Junk driving Big Pharma
According to statista, the U.S. pharmaceutical market represents over 45 percent of the global pharmaceutical market, valued at around 446 billion U.S. dollars in 2016.
With that kind of spending, the country should be way up the top of the list for life expectancy, but at a lowly 31st place it’s not even close to the top. One reason is because bad American junk food is creating sickness and thus a big demand for health care. Big Junk is feeding Big Pharma.
To live longer and healthier, good food is the key. Eat healthy, and the rest will take care of itself.
Share this...FacebookTwitter "
"It is not just the lack of timely action by Scott Morrison but also his lack of instinct that has become so apparent over the past few weeks. From refusing to listen to urgent pleas by fire chiefs several months ago, to taking an overseas family holiday, to contemplating leaving the country for an international meeting at such a time, and most recently endorsing a social media video slapping himself on the back – it is all wrong.  He also seems unable to accept responsibility for his poor judgment, instead blaming state and federal systems, etc. These are not the traits of a strong leader, more of someone desperate to hold on to his popularity. Scott Morrison was in the right place at the right time against a poor opponent when he received the mantle of commander in chief. Tragically he appears to be the wrong man to lead our country through this most terrible crisis now and into the future.Luella Brookes-InglisGlen Osmond, South Australia • Well said, Katharine Murphy (Some political leaders find their natural authority in a crisis – not Scott Morrison, 2 January). We have seen the achilles heel of Scott Morrision: in the pay of the likes of Rupert Murdoch and the coal industry, unable to escape from the extreme right of the Liberal party, he has proven useless as a leader. Instead of action, he offered his prayers for people affected by the fires. He seems incapable of learning from new facts and sticks doggedly to his mantra: God exists, climate change doesn’t. Well, maybe his god is stuck on the toilet and can’t hear him. Everyone in Australia can see the impact of climate change, and the accounting dodges his government makes to avoid any real action under the Paris agreement. He makes us ashamed to be Australian and must resign.Linda PhillipsNarrogin, Western Australia • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"In her first interview since taking the helm, new director-general of the National Trust Hilary McGrady has said she wants things to get “radical” at the charity, by looking to more urban conservation. McGrady told the BBC:   I want to reach more people, and more people live in urban areas. The days of walking in to one of our beautiful houses and saying a family lived here, that’s not going to do it. We need to think about what’s relevant – why would someone in the middle of Birmingham say that’s interesting? What is it in Birmingham that they would get more value from? The future McGrady sees is one where the trust expands its conservation role into cities by working with organisations including community groups and local authorities. But who is to say that the countryside can’t be just as radical? Surprisingly, McGrady might find an unlikely ally in Downton Abbey character the Dowager Countess of Grantham. In a 2015 episode that saw Downton’s doors open to the public for a fundraiser day, the dowager countess poured scorn: “Roll up! Visit an actual dining room complete with a real life table and chairs!” She would have called for a stiff drink had she learned what was ahead – in 2017 alone, 24.5m visitors paid to tour a National Trust property.  Downton doesn’t show us the National Trust, but plotlines about fading grandeur and financial ruin fill us in on the crises that befell country houses after the First World War. Many a threatened house ended up in National Trust care, including Knole, and Basildon Park – home to some of Downton’s interior sets. Highclere Castle, the “real” Downton Abbey, is still in private hands, but open to the public in summer. But was stockpiling country houses really in the national interest? Back in the 1980s, the academic and writer Patrick Wright railed against preserving country piles. For him, the “heritage industry” was deeply conservative and out of touch. He might be pleased to see Hilary McGrady finally catching up.  However, as a rural researcher, I’m troubled by how McGrady’s idea of “radical” means “urban”. She’s talking Birmingham, not Betws-y-Coed. It’s as if nothing radical could happen in the countryside, and nothing countryside could matter to city-dwellers.  Imagine telling that to the Kinder Scout trespassers. 86 years ago this week, they claimed the right to roam against violent landowner opposition. Many trespassers came from the nearby cities of Manchester and Sheffield. They saw in the countryside not just rights worth fighting for, but a refuge from industrial smoke and graft. Some went to jail for the cause – and are commemorated by the National Trust.  Though the dowager countess would have released the hounds, at least one Downton resident might have joined in on a mass trespass. Kitchen maid Daisy cheered on the Abbey’s open day: “I think all these houses should be open to the public. What gives them the right to keep people out?” Rights and justice have spurred on many a rural radical. The 19th century Luddites are now wrongly remembered as stick-in-the-muds, but their fight was for livelihoods and labour rights. They met out on the moors, where non-conformist preachers – radically outside the stuffy established church – also roused crowds who gathered from miles around. A few centuries earlier, non-conformism led the Diggers to their radical vision for farming common land. Fast forward to our own time and a piece of former commons was reclaimed as the Greenham Common Women’s Peace Camp.  We needn’t just look back through history for radical rural goings-on. Too often, the countryside gets imagined as though it is a kind of surviving past, a bit simple, and more prone to outbreaks of Morris dancing than anything actually interesting. I suspect this is why Hilary McGrady thinks that a more relevant National Trust must be a more urban one. A real radical plan could be quite otherwise.  The countryside is and can be innovative. Through the ROBUST project, I’m joining with colleagues from across Europe to rethink the narrow notion that if cities are economic engines, then the countryside is just a carriage pulled along for the ride. We’re investigating better and more beneficial interconnections between rural and urban – even the middle of Birmingham.  I take a cue from another radical, William Morris. There’s probably some of the Victorian designer’s wallpaper in Downton, and the dowager countess would have approved of his soft furnishings. But he would have found more to talk about downstairs with Daisy. For Morris, utopia wasn’t back in a staid rural past or ahead in a science fiction city: he saw a radical rural future. Perhaps a radical National Trust might be just as visionary."
"Since the dawn of history, human societies have ascribed sacred status to certain places. Areas such as ancestral burial grounds, temples and churchyards have been given protection through taboo and religious belief. As many of these places have been carefully managed for many years an interesting side effect has occurred – the sites often retain more of their natural condition than surrounding areas used for farming or human habitation. As a result, they are often called “sacred natural sites” (SNS).   Today, as many other natural habitats have become degraded, researchers worldwide are increasingly interested in the role of SNS in biodiversity conservation. Most of the world’s belief systems, including Christianity, give places sacred status.  In Mediterranean Europe, for instance, the grounds of churches – with their associated ancient trees – have become important SNS.  One of the best examples is in the mountainous region of Epirus in north-western Greece. In the municipalities of Zagori and Konitsa almost every village has one or more sacred grove. These places have been protected through religious belief systems for hundreds of years.  The groves are either protective forests that lie uphill from the village, or groups of mature trees surrounding outlying churches, monuments or other works of religious art. Activities such as the cutting of trees or livestock grazing have been either prohibited or strictly regulated in these places (and disobeying these prohibitions sometimes led to excommunication). We have recently been studying these Greek SNS as part of our SAGE (SAcred Groves of Epirus) project. Our team wanted to find out, using a rigorous research approach, whether SNS are more biodiverse than other forest areas, and, if so, what lessons conservationists could learn from this.  To do this, our international and multidisciplinary group has recently completed the world’s first replicated systematic investigation into the claims that areas conserved as SNS are more biodiverse for different types of plant and animal. For our recently published study, we selected eight SNS in Epirus that covered a wide range of environmental conditions. Each was closely matched with a nearby non-sacred “control” forest which had been managed conventionally – sometimes through natural regeneration. We then conducted a detailed inventory in each site, of eight different groups of organisms. These ranged from fungi and lichens, through herbaceous and woody plants to nematodes, insects, bats and passerine birds. We found that SNS do indeed have a small but persistent biodiversity advantage. This is expressed in a number of ways, most clearly through the existence of more distinct communities of species among the sacred groves than in the control sites (this phenomenon is known as beta diversity).  The group with the most notably higher biodiversity in the SNS than in control sites were the fungi. These often grow in dead wood or old trees, which usually get removed in conventionally managed forests. Of the species of passerine birds (a group that includes many songbirds) that are designated as having special conservation importance at a European level, we found twice as many species present in the SNS as in the control sites. Because these sacred sites are often quite small it is often said that their conservation benefits are marginal. But we found that the influence of size is relatively weak – even small SNS can play a significant role in biodiversity conservation. But Epirus’s sacred sites are now in peril. The rules that linked belief and conservation that once protected the SNS have become difficult to enforce, due to changing population and land-use. The value of forests which protect from landslides and floods is no longer being recognised. The value of SNS is not just on the land that is sacred itself, these places can act as a nucleus, around which biodiversity can expand. In Epirus, forests have regenerated around many of the sites we studied over the past 70 years – despite humans farming the land. It should be noted that this can increase risks such as fire, as dense young Mediterranean forest is very flammable. Evidently the already well-conserved SNS are of great environmental importance across the world. So the next step is to link these sites into conventional conservation schemes. But it is vital that such strategies are closely aligned with the cultural status of SNS. Local communities are often highly motivated to maintain their sacred sites and associated belief systems but lack the resources to do so. A fully collaborative approach between conservation professionals and local communities could offer a solution that conserves both biodiversity and local cultural values."
"More than 1,000 illegal waste sites spring up across England each year – and there are probably far more that haven’t been recorded. In 2016, the head of the Environment Agency in England called waste the “new narcotics”, saying of waste crime: “it feels to me like drugs felt in the 1980s: the system hadn’t quite woken up to the enormity of what was going on, and was racing to catch up.” This goes far beyond fly-tipping in country lanes. Waste crime can be highly organised criminal enterprises. The UK received the unwelcome accolade of having had Europe’s largest illegal site. A single site in Derry contains more than 1m tonnes of illegally deposited waste – that’s more municipal waste than Northern Ireland produces in a year.  The government has stepped up its fight against waste criminals. The environment secretary, Michael Gove, announced a comprehensive review of how the government should respond to the threat. This review is timely – there is a real need to strengthen the government’s approach.  Waste crime is also a growing problem internationally. Interpol has identified it as one of the fastest growing areas of organised crime, having the potential to rival drug trafficking in terms of scale and profits. It is estimated to cost EU countries a staggering €72 to €90 billion each year, covering landfill tax evasion, losses to responsible waste disposal businesses and clean up costs. There are other costs too: in the UK, the fire service spends £16m every year putting out fires at waste sites. Yet Europe has been exporting larger volumes of its trash abroad. About one-fifth of all the EU’s plastic waste was sent to China in 2016. Now that the Chinese government refuses to accept large volumes of foreign waste, some European countries are facing a deepening waste crisis. This can only increase the amount of waste ending up in illegal sites. 


      Read more:
      China bans foreign waste – but what will happen to the world's recycling?


 Illegal waste sites don’t have adequate safeguards, so they can present a significant threat to public health and the environment. The waste site in Derry is dangerously close to the River Faughan, which supplies the city with drinking water. Rates for some types of cancer – linked to toxic waste dumping in the Naples region of Italy – were found to be up to 80% above the national average. Some European governments have turned a blind eye to waste crime. Detecting waste sites takes time and money, as investigations, prosecutions and clean up don’t come cheap. Other governments have allocated significant resources to tackling the illegal waste problem. The English Environment Agency received an extra £30m for waste enforcement in last year’s budget. Despite the clear efforts of some regulators to tackle waste crime, it is more entrenched than ever. Regulatory bodies rely heavily on tip offs, as it’s challenging to detect illegal waste sites. Waste dumping is highly profitable, so it is organised and deliberate, which means that sites are typically clandestine. Traditional regulatory approaches have been critiqued for not being proactive in identifying illegal waste sites. Satellite technology offers a promising means of solving this problem – that’s why, together with an Earth observation professor and a military intelligence analyst, I co-founded Air and Space Evidence (ASE) – an academic spin-off company originating from University College London. Using satellite data and waste site identification algorithms, we became the first company in the world to offer a space monitoring service to detect illegal waste sites. A space-based system is a game changer, because it makes it possible to monitor whole countries routinely and cheaply. Illegal sites can be detected more quickly, so authorities can intervene earlier. Sites can also be categorised in terms of risk, so regulators can prioritise ground inspections at the most dangerous sites. To date there has been limited uptake of satellite data in the waste sector, but that could soon change. Since April 2018, landfill tax can be levied at illegal sites (it only applied to legal sites before). This means there is now a direct financial incentive in the UK to tackle the issue. And the gains can be substantial. In one 7,000 km² test area in the UK, we identified 207 sites which were classified as suspicious. A countrywide monitoring programme would be expected to identify potential tax penalties in the hundreds of millions of pounds. In the UK, all of this tax goes to HMRC and not to the environmental regulator. By contrast, in Ireland the regulator is incentivised, because it can receive up to 80% of the tax income gained from illegal waste sites. It would certainly make sense for HMRC to work more closely with regulatory agencies to tackle waste crime, and to share some of the spoils. Modern satellites can see objects as small as manhole covers and are already used in the UK to detect agricultural subsidy fraud and oil spills. The government could make use of these technological advancements, to clear up the rising tide of pollution in its own backyard."
nan
"
Share this...FacebookTwitterJosef Kowatsch and Stefan Kämpfe at the European Institute for Climate and Energy (EIKE) here have been looking at temperatures in Central Europe, foremost Germany, over the past 30 years.

 Heavy snow blankets Germany in January, 2018. Photo: Stefan Kämpfe
The German media like to say that Germany has been warming rapidly due to global warming, especially winter. Yet a look at the data tells a different story. Although January, 2018 was a mild one at a mean of 3.8°C, as measured by the German DWD national weather service, the overall January trend is COOLING.
The warm January, 2018, did little to curb Germany’s overall January cooling trend, as data from the DWD show. Kowatsch and Kämpfe have plotted the January data over the past 31 years along with the computed linear trend line:

Figure 1: January mean temperatures for Germany have been cooling over the past 31 years. Chart: Josef Kowatsch, based on data from the DWD.
This winter’s mild Central European weather has been attributed to a series of lows that have pumped in mild air from the Atlantic and kept much of Europe out of the ice box. Also heavy precipitation has been associated with the lows, and higher elevations have seen heavy snowfalls as a result — especially in the mountain regions and Alps.
Trend contradicts the CO2
Looking at different locations in Germany, the East German station of Erfurt/Weimar shows the same January cooling trend despite rising CO2 concentrations.

Figure 2: January temperature trend (blue) over the past 31 years  in Erfurt (316m elevation) compared to CO2 concentrations (green). Chart: Stefan Kämpfe.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This cooling has been occurring despite urbanization and added waste heat. Currently 100 hectares of building and asphalt are being added daily.
Substantial cooling at the high elevations
Getting away from urban areas, Kowatsch and Kämpfe looked at the mean January temperature atop Germany’s highest peak, the Zugspitze, and at Amtsberg in the East German Erzgebirge near Chemnitz.
The following chart shows the 31 year trend for the Zugspitze, some 2960 m above sea level:

Figure 3: January’s mean temperature on the summit of Germany’s highest mountain, Zugspitze, has trended significantly downward over the 31 years: from -8.3°C in 1988 to over -11°C in 2018 (using the linear trend). That’s about 1°C per decade! Chart source: Josef Kowatsch.
Urban heat island likely skewing the real cooling
Next we look at the station Amtsberg at the foot of the Erzgebirge in East Germany. Here over the past 30 years the mean January temperature has fallen modestly and shows no signs at all of any warming.

Figure 4: Rural station Amtsberg (blue curve) near Chemnitz is similar to the trend observed across Germany (red curve). Chart source: Josef Kowatsch.
Kowatsch and Kämpfe write that they believe the urban heat island (UHI) effect has not been adequately accounted for in the DWD data, and thus the cooling may actually be even stronger.
So if you’re looking for warming, you won’t find it in Central Europe — despite all the fake climate news you might be hearing.
Share this...FacebookTwitter "
"﻿From space, the Amazon rainforest resembles a giant dark-green lung veined with blue rivers that is steadily succumbing to the disease-like spread of grey fires, orange roads and square-cut farms. What the satellite images cannot show is how most of the remaining bands of verdant, healthy foliage are defended on the ground by forest dwellers who act as antibodies to drive out malignant invaders. Among the most impressive of these is the Tapajós-Arapiuns Extractive Reserve in the state of Para in northern Brazil, where residents are trying to bolster their economic resistance with a series of new agro-forestry and solar power projects.  The 74 traditional riverine communities who live in the area fought hard to win protected status for the region, which is 110 times the size of Manhattan. Some activists were murdered by loggers and others received death threats until their home was recognised by the state as an extractive reserve in 1988. This gives them the right to live sustainably on the land, free from intrusion by large-scale business interests. Today, however, their region is once again under threat as the government of the far-right president, Jair Bolsonaro, encourages commercial development of the Amazon. This has put greater pressure on the residents to find alternative forms of income so the forest around them is worth more than it would be if they sold up to cattle ranchers or soya farmers. Among the projects that aim to do this is a new solar-powered fruit pulping facility at Surucua village. Run as a cooperative by the largely women-led residents association, it is part-funded by Global Greengrants Fund UK, one of the Guardian and Observer’s climate emergency appeal charities. The mini-factory provides residents with an economic incentive to protect and replant the forest with a wide variety of trees that provide fruit, nuts, oil, medicine and other products they can use and sell. Surucua is idyllic. Pellucid waters lap against deserted beaches of fine white sand, bordered by palms and rubber trees. The village comprises three dirt roads, several dozen wooden homes, a school, a church, a football pitch and clusters of trees of little-known indigenous fruits such as acerola, piquia and tucumã to global favourites such as mango, pineapple and banana. “It’s a food forest,” says Mayá Schwade, a biologist who works with the community. “The basic idea is to avoid monocultures.” The new pulping centre is a small concrete building with solar panels on the roof that was built by a cooperative of 16 local women with technical support and funds from a mix of institutions and charities. Its goal is to protect the forest and raise living standards by pushing village production up the value chain. Instead of quickly selling low-cost produce before it rots, villagers can process, package and freeze fruit, allowing them to wait for the best market price.  With help from Guardian and Observer readers, their next step will be to build a new facility that can turn the fruit into alcohol and cordials, jams and confectionery. “Our economy is very weak so this project is very important for us. We can make enough to feed ourselves and earn a little more to buy things,” Maria Aranjo, one of the founders of the co-op, explains. “It’s also fun to work together for a better life.” Surucua is intended as a model for other communities. If living standards in this remote village can be raised with native fruit forests and renewable power, then people living in similar conditions elsewhere in the Amazon will have hope they too can generate enough money and electricity to buy and use computers, mobile phones, TV sets and refrigerators without sacrificing the natural wealth – fresh water, abundant food, clean air – provided by the forest and river. The village hopes the small-scale project will inoculate it against the more destructive industries that are moving ever closer. “On the borders of our territory we face a lot of pressure from loggers,” says Angelo Sousa, a forest engineer who was born in the village. Pressure is also coming from the authorities, who are weakening rainforest protections. “The municipal government wants to expand production of soya and other monocultures. In the coming 10 years, they plan to allow cultivation of these crops in the [protected] reserves. The politicians are all big landowners. There is a proposal in congress to reduce the area of many reserves, including ours,” Schwade says. Elsewhere, the impact is already apparent. In the year to July, deforestation of supposedly protected areas of the Amazon – the world’s largest terrestrial carbon sink – hit 5,054 sq km, up 39% from the previous 12 months. To fortify forests against this, Brazilians are increasingly focusing on economic protections such as the project at Surucua.  Rather than the destructive extraction of beef, soya or gold, the agro-forestry project at Tapajós-Arapiuns aims to strengthen the forest rather than run it into the ground. The Guardian and Observer climate emergency charity appeal closes at midnight on 12 January. Please donate here."
"Gaza has often been invaded for its water. Every army leaving or entering the Sinai desert, whether Babylonians, Alexander the Great, the Ottomans, or the British, has sought relief there. But today the water of Gaza highlights a toxic situation that is spiralling out of control.  A combination of repeated Israeli attacks and the sealing of its borders by Israel and Egypt, have left the territory unable to process its water or waste. Every drop of water swallowed in Gaza, like every toilet flushed or antibiotic imbibed, returns to the environment in a degraded state.  When a hospital toilet is flushed, for instance, it seeps untreated through the sand into the aquifer. There it joins water laced with pesticides from farms, heavy metals from industry, and salt from the ocean. It is then pumped back up by municipal or private wells, joined with a small fraction of freshwater purchased from Israel, and cycled back into people’s taps. This results in widespread contamination and undrinkable drinking water, about 90% of which exceeds the World Health Organisation (WHO) guidelines for salinity and chloride. Incredibly, conditions are getting worse, thanks to the emergence of “superbugs”. These multi-drug resistant organisms have developed thanks to an over-prescription of antibiotics by doctors desperate to treat the victims of the seemingly endless assaults. The more injury there is, the more chance there is of re-injury. Less regular access to clean water means infections will spread faster, bugs will be stronger, more antibiotics will be prescribed – and the victims will be ever-more weakened. The result is what has been termed a toxic ecology or “biosphere of war”, of which the noxious water cycle is just one part. A biosphere refers to the interaction of all living things with the natural resources that sustain them. The point is that sanctions, blockades and a permanent state of war affects everything that humans might require in order to thrive, as water becomes contaminated, air is polluted, soil loses its fertility and livestock succumb to diseases. People in Gaza who may have evaded bombs or sniper fire have no escape from the biosphere. War surgeons, health anthropologists and water engineers – including ourselves – have observed this situation developing wherever protracted armed conflict or economic sanctions grind on, as with water systems in Basrah and health systems throughout Iraq or Syria. It’s now well past time to clean it up. It’s not as if there is no fresh water nearby to alleviate the situation in Gaza. Just a few hundred metres from the border are Israeli farms that use freshwater pumped from Lake Tiberias (the Sea of Galilee) to grow herbs destined for European supermarkets. As the lake is around 200km to the north and lies 200 metres below sea level, a massive amount of energy is used to pump all that water. The lake water is also fiercely contested by Lebanon, Jordan, Syria and Palestinians in the West Bank, each of which is seeking their legal entitlement of the Jordan River basin.  Meanwhile, Israel desalinates so much seawater these days that its municipalities are turning it down. Excess desalinated water is being used to irrigate crops, and the country’s water authority is even planning to use it to refill Tiberias itself – a bizarre and irrational cycle, considering the lake water continues to be pumped the other direction into the desert. There is now so much manufactured water that some Israeli engineers can declare that “today, no one in Israel experiences water scarcity”.  But the same cannot be said for Palestinians, especially not those in Gaza. People there have resorted to various ingenious filters, boilers, or under-the-sink or neighbourhood-level desalination units to treat their water. But these sources are unregulated, often full of germs, and just another reason children are prescribed antibiotics – thus continuing the pattern of injury and re-injury. Doctors, nurses, and water maintenance crews meanwhile try to do the impossible with the minimal medical equipment at their disposal.  The implications for all those who invest in Gaza’s repeatedly destroyed water and health projects are clear. Providing more ambulances or water tankers – the “truck and chuck” strategy – might work when conflicts are at their most acute, but they are never more than a band aid. Yes, things will get better in the short term, but soon enough Gaza will be onto the next generation of antibiotics, and dealing with teflon-coated superbugs.  Donors must instead design programmes suited to the all-pervasive and incessant biosphere of war. This means training many more doctors and nurses, providing more medicines, and infrastructure support for health and water services. More importantly, donors should build-in political “cover” to protect their investments (if not the local children), perhaps by calling for those who destroy the infrastructure to foot the bill for repairs.  And there is an even bigger message for the rest of us. Our research shows that war is more than simply armies and geopolitics – it extends across entire ecosystems. If the dehumanising ideology behind the conflict was confronted, and if excess water was diverted to people rather than to lakes, then the easily avoidable repeated injuries suffered by people in Gaza would become a thing of the past. Palestinians would soon find their biosphere a whole lot healthier."
"The final day of the year is a good time to look back at the year’s weather and forward to what might happen next. Weather in the UK in 2019 did not match the two major events of the previous year – the “beast from the east” of February and March 2018 and that summer’s World Cup heatwave. We did, however, see the UK’s record high when, on 25 July, the temperature at Cambridge Botanic Garden reached 38.7C (101.7F), narrowly beating the previous high of 38.5C set in Kent in August 2003.  For the first time, however, television and newspaper reports did not treat high temperatures with unalloyed glee. Instead, they rightly regarded this as a clear indication that climate change and global warming – now more properly called the climate emergency and global heating – have begun to take hold. The marker of this new weather is, above all, unpredictability, which makes it harder to keep faith in the old certainties of weather lore. So, as a valedictory farewell to the old year, I leave you with this ancient rhyme: If New Year’s Eve the wind bloweth south, It betokeneth warmth and growth, If west, much milk, and fish in the sea; If north, much cold, and storms there will be; If east, the trees will bear much fruit, If north-east, flee it man and brute."
"
Share this...FacebookTwitter A Disgraceful Chasm Between Real-World
Observations & Climate Science Reporting

Injecting frightening scenarios into climate science reporting  has seemingly become a requisite for publication.
In a new Nature Geoscience editorial, a common scare tactic is utilized by the (unidentified) author so as to grab readers’ attention.
Nature Geoscience, 2018
The East Antarctic ice sheet is currently the largest ice mass on Earth. If it melted in its entirety, global sea levels would rise by more than 50 metres.
Wow.  50 meters.  That would be catastrophic.
But then we read about real-world observations for East Antarctica.  And they don’t even come close to aligning with the catastrophic scenario casually tossed into the editorial.
First of all, East Antarctica is not losing mass and adding to sea levels.  The ice sheet is gaining mass and thus removing water from sea levels. The surface mass gains have been occurring not only since 1800 (Thomas et al., 2017), but for the recent decade (2003-2013) too (Martín-Español et al., 2017).  Even the author of the Nature Geoscience editorial acknowledges this.
Nature Geoscience, 2018
“The East Antarctic ice sheet may be gaining mass in the current, warming climate. The palaeoclimate record shows, however, that it has retreated during previous episodes of prolonged warmth.”
Not only has East Antarctica been gaining mass, the author goes on to say that it would take 100s of thousands to millions of years for Antarctica to even exhibit partial retreat.  So much for the “if it melted in its entirety” warning we read earlier.
“In terms of immediate sea-level rise, it is reassuring that it seems to require prolonged periods of lasting hundreds of thousands to millions of years to induce even partial retreat.”
So if the editorial department at Nature Geoscience realizes that it would take 100s of thousands to millions of years to even witness a partial retreat of the ice sheet, is there any scientific justification for the inclusion of the sea-levels-would-rise-50-meters-if-East-Antarctica-melted commentary?  Since when do imaginary scenarios pass as science?
A ‘Staggering’ 9 Trillion Tons Of Greenland’s Ice Has Been Lost Since 1900!  That’s A Sea Level Contribution Of Less Than 1 Inch


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s frightening to learn that the Greenland Ice Sheet has lost a “staggering” 9 trillion tons of ice since 1900, which is what the Washington Post warned us about in 2015.
It’s not frightening to learn that 9 trillion tons of ice losses actually amounts to less than 1 inch of sea level rise contribution from Greenland meltwater in 115 years.

Since a total sea level rise contribution of 1 inch in 115 years from the Greenland ice sheet isn’t scary, the author of the Washington Post article (Chris Mooney) finds it necessary to offer his readers a macabre thought experiment: What if that additional 1 inch of water sitting atop the world ocean were to be collected somehow and then dumped onto all the United States interstate highways?   Now that would be scary.  It would mean that 1 inch of sea level rise turned into 98 feet of sea levels rise (63 times over) in very same imaginary world where additional sea water is dumped onto U.S. interstate highways.
This is how the modern version of climate science works.
Below are a few more examples of glacier melt and sea level rise observations from recently-published papers casting doubt on the tragic, alarmist, and attention-seeking headlines that are so prevalent today.

1. ‘Pine Island Glacier Is The Largest Current Antarctic Contributor To Sea Level Rise’ – But Has ‘Not Shown Any Clear Trend Over 68 Years’ (1947-2015)
Arndt et al., 2018
“Pine Island Glacier is the largest current Antarctic contributor to sea level rise. Its ice loss has substantially increased over the last 25 years through thinning, acceleration and grounding line retreat. However, the calving line positions of the stabilizing ice shelf did not show any trend within the observational record (last 70 years) until calving in 2015 led to unprecedented retreat and changed alignment of the calving front. … Despite the thinning and flow acceleration of PIG [Pine Island Glacier], and sustained, rapid thinning of the ice shelf over at least the past 25 years the position of the ice front had not shown any clear trend over 68 years of observations prior to 2015 (Bindschadler, 2002;MacGregor et al., 2012;Rignot, 2002).”

2. More Land Area Above Sea Level In 2014 Than In 1971 In The Tropical Pacific
Kench et al., 2018
“We specifically examine spatial differences in island behaviour, of all 101 islands in Tuvalu, over the past four decades (1971–2014), a period in which local sea level has risen at twice the global average (Supplementary Note 2). Surprisingly, we show that all islands have changed and that the dominant mode of change has been island expansion, which has increased the land area of the nation. … Using remotely sensed data, change is analysed over the past four decades, a period when local sea level has risen at twice the global average [<2 mm/yr-1] (~3.90 ± 0.4 mm.yr−1). Results highlight a net increase in land area in Tuvalu of 73.5 ha (2.9%), despite sea-level rise, and land area increase in eight of nine atolls.”


3. More Land Area Above Sea Level In 2015 Than In 1985 For The Entire Globe
Donchyts et al., 2016
“Earth’s surface water change over the past 30 years [1985-2015] … Earth’s surface gained 115,000 km2 of water and 173,000 km2 of land over the past 30 years, including 20,135 km2 of water and 33,700 km2 of land in coastal areas.”
 (press release)
Coastal areas were also analysed, and to the scientists’ surprise, coastlines had gained more land – 33,700 sq km (13,000 sq miles) – than they had been lost to water (20,100 sq km or 7,800 sq miles).
“We expected that the coast would start to retreat due to sea level rise, but the most surprising thing is that the coasts are growing all over the world,” said Dr Baart.  “We were able to create more land than sea level rise was taking.”

4. Greenland And Antarctica Combined Contributed A Total Of 0.59 Of An Inch To Sea Level Rise Between 1958-2014
Frederiske et al.,2018


5. ‘Recent Lack Of Any Detectable Acceleration In The Rate Of Sea-Level Rise’
Parker and Ollier, 2017
“The loud divergence between sea-level reality and climate change theory—the climate models predict an accelerated sea-level rise driven by the anthropogenic CO2 emission—has been also evidenced in other works such as Boretti (2012a, b), Boretti and Watson (2012), Douglas (1992), Douglas and Peltier (2002), Fasullo et al. (2016), Jevrejeva et al. (2006), Holgate (2007), Houston and Dean (2011), Mörner 2010a, b, 2016), Mörner and Parker (2013), Scafetta (2014), Wenzel and Schröter (2010) and Wunsch et al. (2007) reporting on the recent lack of any detectable acceleration in the rate of sea-level rise. The minimum length requirement of 50–60 years to produce a realistic sea-level rate of rise is also discussed in other works such as Baart et al. (2012), Douglas (1995, 1997), Gervais (2016), Jevrejeva et al. (2008), Knudsen et al. (2011), Scafetta (2013a, b), Wenzel and Schröter (2014) and Woodworth (2011).”
“[T]he information from the tide gauges of the USA and the rest of the world when considered globally and over time windows of not less than 80 years […] does not support the notion of rapidly changing mass of ice in Greenland and Antarctica as claimed by Davis and Vinogradova (2017). The sea levels have been oscillating about a nearly perfectly linear trend since the start of the twentieth century with no sign of acceleration. There are only different phases of some oscillations moving from one location to another that do not represent any global acceleration.”
“The global sea-level acceleration is therefore in the order of + 0.002  ± 0.003 mm/year², i.e. + 2 ÷ 3 μm/year², well below the accuracy of the estimation.”

Share this...FacebookTwitter "
"I felt like putting a bullet between the eyes of every panda that wouldn’t screw to save its species
– “Narrator”, Fight Club Though Edward Norton’s unnamed movie character was no conservation biologist he was actually pretty close to the solution. Giant pandas don’t choose not to “screw” out of spite, of course, or because they have no natural urge to reproduce – in the wild they have perfectly good sex drives. However, just as Tyler Durden and co felt they needed to turn to violence in order to find meaning in their lives, so it is becoming apparent that male pandas also need to fight – in order to mate. To save their species, captive pandas may need a fight club of their own. The breeding problems are particularly apparent in Edinburgh, where the zoo has announced once again that its female giant panda Tian Tian has conceived through artificial insemination, but is not yet pregnant. That is, a sperm has entered an egg, which has been placed inside her; but crucially this fertilised egg has not yet implanted. Unfortunately, these attempts at artificial reproduction often don’t work. Some scientists say giant pandas are so hard to breed that they shouldn’t even exist in nature. Females often don’t accept males, and even if a female is interested in a male this sexual interest is not reciprocated.   None of this makes sense. The meaning of life, as all biologists know, is passing your genes on to the future and to do this you need to mate. But we have arrived at our conclusions on panda reproduction based on our observations of zoo-housed giant pandas and not from data on wild individuals. I suspect herein lies the problem – zoos don’t offer the same sexual opportunities as life in the wild. The panda situation reminds me of another species, which despite being held in captivity for thousands of years had only bred once before the 1950s: the cheetah.  Now the captive cheetah population is growing rapidly.  To understand how zoos have solved this problem with cheetahs and where the problem with giant pandas comes from we need to look back in time. The first modern zoo, in London, was founded in the Victorian era, a time when animal collectors had no qualms about going around the world to gather new species by any means necessary. The animals were housed in a monogamous family unit, in accordance with Victorian standards of morality. Even today zoo visitors explain to their children which animal is the mummy or the daddy and any inconvenient other adults are explained away as aunties or uncles. This situation has been reinforced by zoos describing themselves as modern arks, creating the image of animals entering two-by-two. But once scientists started to study animals in the wild it quickly became obvious that monogamy is not the norm. And even in species that are monogamous there is considerable mate choice or, to be more accurate, reproductive partners are chosen by females. Just like humans, most animals chose their reproductive partners according to their strength and good looks, or the resources they control, as these characteristics affect the perpetuation of their genes. Captive female cheetahs finally started breeding once they were offered a range of suitors rather than being housed with just one male. This replicated their behaviour in the wild, where female cheetahs like to give several males the once over before choosing who to breed with.  It may be that the first male that caught the female’s eye will be the one that she will eventually choose, but confirming he is the best available in the breeding pool seems to be crucial. Zoos who imitate this situation are successful at breeding cheetahs. Giant pandas are no longer housed in pairs. Instead, males and females are typically kept in adjacent enclosures and the male is introduced to the female during the one to three days a year when she is fertile. But it is just one male. What if he is ugly? Or maybe the female panda wishes suitors would strut their stuff on a mating catwalk, like cheetahs do.   Perhaps males need to impress females with shows of strength, by beating up or chasing off other males. It could also be that males need some competition to put them in the mood – studies of wild pandas show their testosterone levels are low except at mating time. Female giant pandas advertise their readiness to breed by scent marking around the edges of their territory, leading to the simultaneous arrival of several males who will chase off or beat-up their competition until only the strongest remains. So, the victor gets the opportunity to propagate its genes.   If all of this is correct it suggests zoos wanting to move away from artificial reproduction might be better off instead focusing on panda fight clubs. Alternatively, it may be possible to dupe females into believing certain males are worthy suitors as has been done with pygmy lorises by making them appear to be winners of fights."
"In the spring of 2001, the UK countryside turned into a crematorium. A foul-smelling haze settled over parts of the country as 6m cows, sheep, pigs and goats were slaughtered and their carcasses burned in the fields. This was the result of a bid to control an outbreak of foot-and-mouth disease – a contagious disease of livestock named after the ulcers it causes in the mouth and between the hooves of  farm animals.  The outbreak began on Burnside farm, Northumberland, where uncooked swill – food leftovers – were illegally fed to a barn of pigs. Under the regulation at the time, food wastes had to be cooked to sterilise them and prevent disease-transfer. This was the first step in a series of unfortunate events which led to the costliest animal epidemic the country had ever seen.   Unknown to the farmer, that unprocessed food waste contained imported meat infected with the foot-and-mouth virus. Once it was introduced the virus spread across the country, exacerbated by a slow response from the Ministry for Agriculture, Fisheries and Food (which was later rebranded as DEFRA, in an effort to move on from the debacle). The ministry’s efforts to limit livestock movements were ultimately too little too late. In the aftermath of the outbreak, the government had to do something, and so the use of all food wastes in animal feed was banned (irrespective of heat-treatment), ending the 9,000-year-old practice of swill-feeding. This UK-wide ban was extended to the EU within a year. But 17 years on, a growing body of research is questioning whether the ban on swill was the right response for farmers, pigs, and the planet. The European livestock industry, in many ways, is in a squeeze. The emergence of large-scale commercial livestock farming operations in the world’s rapidly developing economies has increased the international competition for animal feeds, driving up feed prices and shrinking profits. This is compounded by the bad press that surrounds the use of soybeans, the archetypal modern animal feed ingredient, demonised for its role in deforestation in the forests and savannahs of Latin America. Media headlines criticising the livestock sector for its impact on the environment are becoming all too familiar – on a finite planet with a growing population, some argue it is an unacceptable waste of resources to grow grains to feed livestock instead of people. It is in light of these challenges that interest in swill has been rekindled. Legalising heat-treated swill once more would put livestock back at the heart of a sustainable food system, a green, circular economy where livestock play a key role in recycling nutrients and disposing of food waste. Indeed, when compared with turning food waste into biogas or compost, using it as swill is hands-down the option with the lowest environmental impact. And it can be done – in the same year that the UK banned the use of swill, Japan introduced the opposite policy, establishing a modern, regulated system for safely producing swill. Today, countries like Japan and South Korea recycle around 40% of their food waste as animal feed. Farmers use swill not because of its environmental benefits, but because it reduces their feed costs by 40-60%. They have achieved these remarkable results through a science-based approach to swill feeding; swill manufacturers in Japan and South Korea must be registered and their compliance with food safety regulation is monitored. Food wastes are treated for three minutes at 80°C, or 30 minutes at 70°C – both enough to deactivate viruses such as foot-and-mouth. Since the introduction of these regulated systems, no disease outbreaks have been associated with swill feeding in these countries. The potential in Europe is huge. Recent research suggests that if Europe were to recycle food waste as swill on the scale achieved in East Asia, this would reduce the land required to produce pig feed by 20%. This an area the size of Wales, including more than a quarter of a million hectares of Latin American soybeans.  For Europe to mimic the success of these East Asian systems requires first and foremost the development of safe operating systems for the European context, based on the regulated heat-treatment and production of swill.  Importantly, legalising swill would require the right level of enforcement to ensure it is done safely. There should arguably be zero tolerance for people caught bending the rules – in stark contrast with the Ministry for Agriculture, Fisheries and Food’s softly-softly approach, which facilitated the 2001 foot-and-mouth outbreak (a MAFF inspector even visited the farm where the outbreak occurred two weeks beforehand, but didn’t clamp down on the farm’s questionable practices). And when weighing up the risks and benefits of reintroducing swill, we also must not forget the risks of the current “ban”, which is frequently broken – a survey of smallholder UK pig farmers for example found that a quarter of them feed uncooked food wastes to their pigs. All of this would, of course, be irrelevant if farmers weren’t willing to use swill. But a new study conducted at the UK’s largest pig industry trade-fair shows that 75% of pig farmers and other attendees of the event supported its relegalisation, and half of all pig farmers surveyed said they would consider using swill on their farm.  Support was not unanimous – some people were understandably concerned about disease risks and consumer acceptance of swill. But overall, the level of support for swill was similar to other alternative animal feeds, such as much-hyped insects. As the meat industry battles to clean up an image tarnished by environmental impacts, questions over animal welfare, and food safety scandals, swill remains a good news story rarely told. A key question to all in the sector is how livestock products can be repositioned as a sustainable part of people’s diets, reared in ways that respect traditional farming practices, with high food safety.  Well, perhaps where there’s swill, there’s a way."
"I was walking in Budongo forest one day, years ago, when my Ugandan friend Geresomu pointed out a patch of grey soil on the forest floor. “What’s that?” I asked. “The chimps sometimes eat it,” he replied. I promptly forgot about it – it was of no interest at that time. Now, it’s the central focus of my research. So what’s happened in the meantime? Back in the late 1980s, baby chimps from the Budongo Forest were being taken by poachers to Entebbe airport and illegally smuggled to Dubai and other places. I decided to set up a field station to protect the chimps, and spent years looking for funds to do so. In 1990, I founded the Budongo Conservation Field Station, and we hired a team of assistants to help us find and track the chimps. Geresomu was one of our first field assistants, and he’s still with us today. He knows more about chimps than any Westerner.  In the early 90s, Geresomu showed me a raffia palm tree, growing in a swampy area of the forest, which had a carefully made hole in the trunk. He told me that the hole had been made by chimps. “They put in their hands and pull out the dead wood and chew it – then they spit out the wood, but they swallow the juice,” he said. “Why would they do that?” I asked. No one knew.  I lived with that question for years, then one day I read a paper about gorillas, which said that they ate dead wood for its sodium content. I live near Brighton, and the Dean of Science at Brighton University – Andrew Lloyd – runs a lab that measures mineral elements. He offered to analyse the raffia wood. On my next visit to Budongo, I collected some samples and sent them to him. They turned out to be rich in sodium.  Sodium is an essential mineral for all mammals, and many find it easily in their diet. But chimps mainly eat fruit and leaves, which contain little or no sodium. So, they need to supplement their diets. Besides the sodium, there was a cocktail of other minerals present in the raffia samples: iron, manganese, magnesium, aluminium, and others. It seems the dead raffia palms suck water from the swamp forest, which then evaporates, leaving the minerals behind in the decaying pith. We published this discovery in PLoS ONE, in 2009.  Just at the time we published our findings, we found to our dismay that the raffia trees were disappearing fast. Their leaves were being cut down and used by local tobacco farmers to make raffia string, which they used to hang up their leaves in the smoking sheds. The tobacco trade was booming, and soon the last raffia trees were gone. The dead ones that remained were used so much by chimps that soon, all the pith was taken out and there was nothing left. But the chimps turned out to be ingenious. That low level of clay eating Geresomu had told me about so many years ago suddenly became more important. At any one time we have a number of research students at Budongo doing PhDs and Masters degrees. One by one, they reported finding chimps feeding on clay, or using leaf sponges to remove clay-rich water from the waterholes under the trees. They would congregate around these trees, pull the leaves off surrounding undergrowth and chew them a little. Then, they would use the leaves as sponges to soak up the clay-water, chew and suck the leaves in their mouths, and dip them in the water again. This was seen and filmed by Cat Hobaiter, Brittany Fallon and Caroline Mullins of St Andrews University, Anne-Marijke Schel of Utrecht University, and Noemie Lamon of Neuchatel University. I was in the UK and the students wrote me emails describing this new behaviour, and collecting information about it and photographing and videoing it. The field assistants also recorded it when they came across it in the course of their normal work, so their records went into the archives too. Naturally, I could not wait to get out to Budongo myself and see the clay pits, the waterholes, and the clay-eating and drinking behaviour itself. Meanwhile, all I could do was speculate that this might be a new way for the chimps to get sodium and all the other minerals, which they used to source from the raffia palms. In due course, I visited Budongo and came back armed with a set of samples of clay and clay-water, as well as some control samples of forest soil and rainwater for comparison. We ran them in Andrew Lloyd’s Brighton laboratory. They were mineral rich, yes, but disappointingly low in sodium. We discovered that the grey clay was kaolinite – a particularly rich source of aluminium, which people also use to treat upset stomachs. The samples also contained high quantities of iron and all the other minerals that we had found in the raffia pith. So we concluded that the clay was being eaten by our chimps as a mineral supplement, as well as for its digestive properties. And interestingly, when the clay-water was sponged up using leaves, it contained more minerals than when it was taken without the leaves. So the leaves and the clay were interacting with each other. Just how the chimps knew to use the clay as a mineral supplement is still a  mystery. Probably, it’s a habit passed from adult to juvenile: we see the young ones watching as the adults do it, and then doing it themselves. Or perhaps it may be that, back in evolutionary time, the chimps who took the supplements lived longer, healthier lives than those that didn’t, and so the habit got passed on, either through learning or as an inherited tendency. Such questions are hard to answer. But sometimes even the smallest clue – like that patch of grey soil Geresomu showed me all those years ago – can hold the key to a fascinating story."
"
Share this...FacebookTwitterHurricane activity trend declines significantly over the past 65 years
Dr. Sebastian Lüning and Prof. Fritz Vahrenholt posted here yesterday results showing that  hurricane activity has decreased over the past decades, despite all the hysteria we’ve been hearing from the usual suspects.

Data and studies show that Potsdam Institute thinly veiled claims that hurricanes are connected to man-made CO2 are spurious. Photo: Hurricane Isabel, NASA (public domain).
Taking a look at the recent literature, we find a paper authored by Ryan Truchelut and Erica Staehling appearing in the Geophysical Research Letters on December 8, 2017. They looked at the development of American hurricanes based on accumulated cyclone energy (ACE).
CO2 and hurricanes are not connected
The two authors found a statistically significant reduction in hurricane activity over the past 65 years. The relative inactivity of the past years (except for the very active 2017 season) was the most inactive phase of the examined time period. The abstract (emphasis added):
An Energetic Perspective on United States Tropical Cyclone Landfall Droughts
The extremely active 2017 Atlantic hurricane season concluded an extended period of quiescent continental United States tropical cyclone landfall activity that began in 2006, commonly referred to as the landfall drought. We introduce an extended climatology of U.S. tropical cyclone activity based on accumulated cyclone energy (ACE) and use this data set to investigate variability and trends in landfall activity. The drought years between 2006 and 2016 recorded an average value of total annual ACE over the U.S. that was less than 60% of the 1900–2017 average. Scaling this landfall activity metric by basin-wide activity reveals a statistically significant downward trend since 1950, with the percentage of total Atlantic ACE expended over the continental U.S. at a series minimum during the recent drought period.”
As CO2 in the atmosphere continuous to rise unabated, hurricane activity is decreasing. Obviously the two trends have little to do with each other.
Experts advise caution in assigning blame
Looking at the very clear body of facts, it is little wonder that the NOAA (via the Geophysical Fluid Dynamics Laboratory, GFDL) cautioned against linking the greenhouse gases and hurricanes in an official statement:
It is premature to conclude that human activities–and particularly greenhouse gas emissions that cause global warming–have already had a detectable impact on Atlantic hurricane or global tropical cyclone activity. That said, human activities may have already caused changes that are not yet detectable due to the small magnitude of the changes or observational limitations, or are not yet confidently modeled (e.g., aerosol effects on regional climate).”
Shrill in Potsdam
However in Germany, scientists scientists at the Potsdam Institute for Climate Impact Research (PIK) don’t accept the facts for what they are. For them t’s more important to be shrill and to set off alarms. For example in the wake of Hurricane Harvey, PIK scientist Anders Levermann gave a radio interview at Radio Eins on September 1, 2017. He said that when it’s warmer, more water vapour ends up in the air, and thus more rain and a hurricane with a flooding catastrophe in Houston. It’s just that simple, right? Then later in the interview Levermann added a “maybe”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




What Levermann committed here of course was an irresponsible misleading of the listeners. The clear decrease in hurricanes over the past 65 years contradicts his extraordinarily simplistic claims.
Poor urban planning
At the BBC the journalists were more serious with the subject, and did not stay silent regarding the complexity that is involved as it is known that a vast number of factors are at play. In Houston, for example, a blocking pattern was at work and thus led to a heavy rainfall over an urban area for a protracted time period. Anyone who claims that a part of the disaster can be attributed to man is unrealistically simplifying the system and ought to know better.
The answer to the question as to what the problem in Houston was is provided by an article by the BBC:
Climate change did not make people build along a vulnerable coastline so the disaster itself is our choice and is not linked to climate change.”
On ARD German public television, they too have gotten much more careful. TV meteorologist Donald Bäcker flatly dismissed the shot from the hip from Potsdam.
Forecasts thanks to link to AMO
In June, 2017, at the start of the hurricane season, Judith Curry and CFAN published a prognosis for the hurricane season. Here they anticpated an above-average hurricane activity season. They were right. If they are successful next year as well, then we’ll have an important forecasting method that will be of great use for society.
Other prognoses: Hurricanes have a certain development. Satellite photos allow the creation of storms to be tracked. But not all West African hurricane babies make it across the Atlantic and reach America. Tel Aviv University developed a model that allows us to determine which storms pose a risk and which ones will die off. Read the press release here.
Over the mid-term, hurricane activity can be forecast quite well because it is closely coupled to the Atlantic Multidecadfal Osciallation (AMO) ocean cycle, which has a periodicity of about 60 years. Michel de Rougemont reminds us in an essay appearing at WUWT.
Building in flood-prone areas is negligent
In Germany’s leading political daily FAZ of August 31, 2017, Winand von Petersdorff pointed out an important damage factor related to Hurricane Harvey. It was a man-made disaster in the sense that many homes and buildings had been built in classic flood-prone areas.

The enormous costs were foremost caused by the fact that the booming Houston metropolitan area, with its 6.5 million inhabitants, permitted building in grand style in areas where flooding and high water would occur.”

Share this...FacebookTwitter "
"
Share this...FacebookTwitterCould Earth’s Shifting Plates 
Be Driving Modern Climate?

Within the last year, Dr. Arthur Viterito (geography professor) has published multiple scientific papers documenting the significant correlation (r=0.80) between the seismic activity changes in the Earth’s high geothermal flux areas (HGFA) and both El Niño events and global temperatures.
The HGFA/global temperature correlation has been found to be stronger than the correlation for CO2 concentration changes (r=0.74) for recent decades (1979-2016).
Other recent research has provided further support for the significant influence of seismic activity (i.e., there is a very high correlation [r=0.935] between geothermal flux and North Magnetic Dip Pole movement).
These robust and well-documented seismic activity associations have led Dr. Viterito to call for a reconsideration of the paradigm that says variations in atmospheric CO2 concentrations drive changes in global temperatures.

Viterito, 2016



Viterito, 2017
“The Correlation of Seismic Activity and Recent Global Warming (CSARGW) demonstrated that increasing seismic activity in the globe’s high geothermal flux areas (HGFA) is strongly correlated with global temperatures (r=0.785) from 1979-2015. The mechanism driving this correlation is amply documented and well understood by oceanographers and seismologists.”
“Namely, increased seismic activity in the HGFA (i.e., the mid-ocean’s spreading zones) serves as a proxy indicator of higher geothermal flux in these regions. The HGFA include the Mid-Atlantic Ridge, the East Pacific Rise, the West Chile Rise, the Ridges of the Indian Ocean, and the Ridges of the Antarctic/Southern Ocean. This additional mid-ocean heating causes an acceleration of oceanic overturning and thermobaric convection, resulting in higher ocean temperatures and greater heat transport into the Arctic. This manifests itself as an anomaly known as the “Arctic Amplification,” where the Arctic warms to a much greater degree than the rest of the globe.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“As illustrated in CSARGW, jumps in HGFA seismic activity can amplify an El Niño event, a phenomenon referred to as a SIENA or a Seismically Induced El Niño Amplification.  Accurately predicting two of these amplified El Niños (i.e., the 2015/2016 event plus the1997/1998 episode) is an important outcome of the HGFA seismicity/temperature relationship.”

“Applying the same methodology employed in CSARGW, an updated analysis through 2016 adds new knowledge of this important relationship while strengthening support for that study’s conclusions. The correlation between HGFA seismic frequency and global temperatures moved higher with the addition of the 2016 data: the revised correlation now reads 0.814, up from 0.785 for the analysis through 2015. This yields a coefficient of determination of .662, indicating that HGFA [high geothermal flux area] seismicity accounts for roughly two-thirds of the variation in global temperatures since 1979.”

Viterito, 2017
“[T]he idea that increased flux of oceanic geothermal heat (as indicated by increased seismic activity in these areas) can significantly alter temperature counters the hypothesis that increasing carbon dioxide has been the primary driver of recent global temperature change. Despite the general “non-acceptance” of this hypothesis, a recent study by Williams (2016) links a seemingly unrelated geophysical phenomenon to mid-ocean seismicity; thus a new paradigm may be emerging from this important association. Specifically, Williams shows that the speed at which the North Magnetic Dip Pole (NMDP) moves is highly correlated (r=0.935) with mid-ocean seismic activity.”
“More importantly, multiple regression analysis corroborates the findings of the previous studies: mid-ocean seismic activity is significantly correlated (p<0.05) with changing temperatures.”

“However, CO2 concentrations, along with NMDP [North Magnetic Dip Pole] displacements, do not explain a significant percentage of the total variance (p>0.05) when they are included and must be dropped from the analysis. This high degree of multicollinearity is a prominent finding.”
“However, it is important to note that, despite high correlations, CO2 increases cannot be causing an intensification of mid-ocean seismic activity nor can higher CO2 concentrations be driving the acceleration of the NMDP [North Magnetic Dip Pole]. There is simply no plausible mechanism that can be invoked here.”
“Clearly, there is far more in play than is currently accounted for in our understanding of earth’s climate. The shifting of plates, along with the concurrent shifts of earth’s NMDP, should spur the geophysical community to create a new and enduring paradigm that links these phenomena to changing global temperatures.”
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterScience journalist Axel Bojanowski at German flagship, center-left news weekly Spiegel here dismissed a recent study published by Columbia University scientists Wolfram Schlenker and Anouch Missirian, who had claimed climate warming was driving masses of environmental refugees to Europe.
The two scientists claimed in Science to have found a relationship between weather disasters and refugees migrating to Europe.
However, the far-fetched conclusions by the two scientists has since been met with sharp and harsh criticism for its loose use of statistics. The study was financed by the JRC of the European Union. One member of the JRC, Juan-Carlos Ciscar, said it was time for policy makers to act.
Paper gets “crushing assessment” from other scientists
However, Spiegel’s Bojanowski reports that a number of leading experts dismissed the paper’s claims. For example Thomas Bernauer and Vally Koubi the Zurich-based ETH said: “Politicians would be ill-advised to orient themselves based on this study.”
Bojanowski added that other experts SPIEGEL ONLINE asked gave it “a damning verdict“.
“Dumbest use of statistics”
The Spiegel journalist also took jabs at other leading media outlets, such as the Guardian, Reuters and AP, implying they uncritically used the study for hype.
Bojanowski then cited statistics expert William Briggs of Cornell to assess the methodology used by the study:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The dumbest, most idiotic use of statistics that I’ve seen in a long time.”
Tobias Ide of the Georg Eckert Institute also warned against simplifying “the relationship between warmer temperatures and migration“, Bojanowski wrote, and that Jonas Vestby of the Peace Research Institute in Oslo was surprised the paper ever got by peer-review.
Christiane Fröhlich of the University of Hamburg is considering a rebuttal at Science, Spiegel’s Bojanowski wrote.
Confusion
Fröhlich says the Columbia University authors confused environmental refugees with political refugees. In Europe refugee status is given to persons who are politically oppressed and persecuted and not to those moving due to the environment. Critics of the study also called the projections and correlations claimed by Schlenker and Missirian as “highly speculative“.
Fröhlich also told Spiegel the paper “ignored numerous studies” on migration and warming. Briggs added that the two authors based their assumptions on only “15 years and in only one region” and “ignored 6000 years of human history“.
Bojanowski cited Briggs:
Just how important it would have been to include other regions was made clear by Briggs using one provocative question: ‘Why then don’t asylum applications in cool Chile rise after heat waves in the warm neighboring countries?'”
 
Share this...FacebookTwitter "
nan
"A relationship that lasted 93 years has hit the rocks. The UK’s Met Office is not on the shortlist for renewal of the contract it has held since 1922 to provide weather forecasts to the BBC. This is unlikely to affect the visible face of the weather forecast dramatically, but is the underlying change in the data provider a good idea? The Met Office has its origins in a service for mariners, set up in 1854 under the leadership of Robert FitzRoy, who had previously captained HMS Beagle on Darwin’s famous voyage. The organisation developed as part of the Ministry of Defence, with an important role to play in both world wars. Weather prediction helped the 1944 D-Day landings succeed, but equally armies have often waited for bad weather to attack, such as during the Ardennes counter-offensive later that year. For most of the 20th century, the Met Office provided a public service in supplying weather forecasts to the BBC and conducting atmospheric science research. It has acquired an obligation to generate commercial revenue, however, and in 2011 it became part of the Department for Business, Innovation and Skills.  The Met gets its income through providing a range of specialised forecasts, for example on behalf of the aviation and insurance industries, but the BBC contract remains a key part of its work. At a time when government is reducing spending, it is natural that the Met Office should seek to maximise income and the BBC should seek to cut costs, leading to the current schism. The clichéd conversational opener about the state of the weather is invariably followed by the comment that it wasn’t predicted correctly. Yet anecdotal evidence is misleading. The “skill” of a weather forecast is determined by measuring the correlation between a forecast weather map and what actually happened. This can be judged against the skill of a forecast based on statistical summaries of past records, known as climatology.  Since the 1980s, computer-based forecasting has steadily improved at a rate of about one forecast day per decade. This means that a five-day forecast made now is about as skillful as a two-day forecast was in 1985. The World Meteorological Organisation compares all national services and the UK’s forecast quality ranks among the very best, despite the notorious unpredictability of the country’s weather.  This notable improvement is a result of two related factors. First, the data from a vast network of surface and satellite observations is shared and distributed to national services. This is vital: knowing what the whole atmosphere is doing right now is the first step to predicting how it might change. Second, the weather models that project that information forward in time have been improved. This requires supercomputers, which are becoming increasingly accessible. In principle, any meteorological service that runs a global model of the atmosphere could offer a forecast for any region of the world. Global models are needed to forecast more than a few days into the future, but models that focus on a smaller area are also required to account for local factors, such as hills and valleys. For UK forecasts, a large-scale model of the North Atlantic and a still smaller-scale model of the UK and Ireland alone can be embedded into the global picture, each working down to finer scales. Other meteorological services can run similar local-area models focused on the UK, but it is not a trivial task and requires computing resources roughly equivalent to those for the global model. There is also a human aspect. Computer results are rarely presented raw, but are first interpreted by a human forecaster with local knowledge and experience. Different audiences also understand weather information communicated in different ways and we’re still not sure if forecasts are best presented as a single, most-likely case or as a range of probabilities. It is a brave move to change format, and impossible to please everyone. The BBC has an obligation to find the best value. But value should not be confused with price, and cheaper bidders must provide a forecast of comparable quality. Viewers may be reassured by retaining the same presenters and graphical presentation; subtle changes in forecast quality and communication will be more difficult to assess. Then there are strategic issues for the country. The Met Office will inevitably lose income, a net loss to the British economy, as well as public recognition. Since the 19th century, the UK has built world-leading atmospheric science expertise, of vital importance in applications ranging from defence to pollution and climate monitoring. Any erosion of capability in these areas may later be a cause for regret."
"Edgar Allan Poe wrote a brilliantly sinister short story about a man who wakes up in the night and sees a shadow against the curtains that looks just like a knife‑wielding murderer come to kill him in his sleep. The twist: it really is a knife-wielding murderer come to kill him in his sleep. The murderer has a plan for this. He freezes, knowing his victim will convince himself he is simply looking at a shadow, that something this frightening can’t actually be happening, that your worst fears never really come true. Steadily the murderer starts to creep closer, hour after hour, a millimetre at a time. By the end of the story he’s stood over the bed as early morning light floods the room, knife raised, staring into the face of his victim, who’s too far gone by now to break the illusion or admit that death really is standing in his eyeline. I mention this because something similarly frightening and equally hard to grasp is happening in Australia, and happening in plain sight too. Imagine an area three times the size of Yorkshire on fire. This is the scale of the disaster in New South Wales. Homes have been lost. Half a billion animals have been killed. New Zealand’s glaciers have turned brown from the smoke 2,000 miles away. All this in a country experiencing its hottest recorded summer, where the disaster minister “doesn’t have an opinion” on whether man-made climate change really exists, even as the horizon burns behind him. How close does it have to get? How far does that shadow need to creep? How long before we admit that death is now in the room? These are not the observations of a renowned climate activist. They are simply the facts, albeit facts that are still hard to grasp for those of us whose brains work slowly, whose concerns remain – let’s face it – essentially solipsistic. It is perhaps a question of scale, of finding a way to respond that isn’t simply panic or despair. With this in mind here’s another aspect, a detail that has been reported insistently this week in the background of all this macro-destruction and terror. There is now a clear and obvious danger this might start affecting the cricket. For those who think it’s frivolous to be concerned with cricket right now, to worry about disruption to cricket caused by symptoms of the impending apocalypse: stick around. Take, as they say in America, a step back. Because details are important here. Grade and Shield cricket has already been affected. Last month a Big Bash game in Canberra was abandoned due to smoke over the Manuka Oval, a ground that is already hotter than the exposed reactor core at Chernobyl on any given summer’s day. Right now there are fears the third Test between Australia and New Zealand at the SCG might be threatened, with temperatures due to hit 45C on Saturday afternoon and a suggestion play may be halted if the air quality index rises above a reading of 150. “This is a challenge on two metrics: visibility and breathing,” a Cricket Australia spokesperson has said. And he’s right. Those are two very important metrics. At which point it is tempting to write a classic sport-style column about this. The standard form is to declare that sport really must do something, that sport must set an example, carry the message, become a model of sustainability. Presumably it was once credible to talk like this, to suggest professional sport might be some kind of force for collectivism and universal good. It might even have been true. It isn’t true now. Let’s face it, Big Sport simply isn’t concerned with issues like this. There are nice people and nice gestures within it. Various Aussie cricketers have already promised help and donations. Sam Stosur will give $200 to the fire relief fund for every ace she serves this summer (all the while flying around the world on the absurdly carbon-heavy tennis circuit). But in reality sport cares only about growth, profit and consumption. There will be platitudinous noises and pledges. But at the elite level sport is run, funded and used as a reputation-garnish by the world’s greatest carbon-gorgers. There will be no rainbow coalition of Fifa-ICC-IOC junketers holding hands to save the planet, not unless the people of the earth can crowdfund a sufficiently tempting “special payments” fund. So what can we do about this now? Send our love, support and hard currency to one of the relief funds, and of course to the firefighting services charged with containing the revenge of nature. As Nathan Lyon said this week: “We just play a game, the real heroes are the firefighters out there fighting fires” – which is of course arguable from the GOAT. How many firefighters have taken a Test seven-for? How many firefighters have chased down a score on a crumbling Dubai deck in front of 300 Pakistani taxi drivers? But you can and should still give to them here. As for sport, it is still pretty useful as a way of shining some light. For one thing, here we are talking about climate emergency and destruction of the natural world because the Test match might be affected. There are details here you can grasp. That thing you love is in peril. Plus there is something painful and indeed strangely beautiful in the fact cricket is caught up in this, if only because cricket is so vital to the way Australia perceives itself: those dreamy white figures out there in all that pastoral green, a vision of Australia’s own youth and strength and righteousness. For now that great outdoors has become strange, licked by flames and fogged with smoke. The shadow is beginning to clarify. One thing is certain. We will, from here, have a front-row seat."
"Nine of 13 of Africa’s oldest and largest baobab trees have died in the past decade, it has been reported. These trees, aged between 1,100 and 2,500 years, appear to be victims of climate change. Scientists speculate that warming temperatures have either killed the trees directly or have made them weaker and more susceptible to drought, diseases, fire or wind.  Old baobabs are not the only trees which are affected by climatic changes. Ponderosa pine and Pinyon forests in the American West are dying at an increasing rate as the summers get warmer in the region. In Hawaii the famous Ohi’a trees are also dying at faster rates than previously recorded. There are nine species of baobab trees in the world: one in mainland Africa, Adansonia digitata, (the species that can grow to the largest size and to the oldest age), six in Madagascar, and one in Australia. The mainland African baobab was named after the French botanist Michel Adanson, who described the baobab trees in Senegal. The African baobab is a remarkable species. Not only because of it’s size and lifespan but also in the special way it grows multiple fused stems. In the space between these stems (called false cavities) bark grows, which is unique to the baobab.  Since baobabs produce only faint growth rings, the researchers used radiocarbon dating to analyse samples taken from different parts of each tree’s trunk and determined that the oldest (which is now dead) was more that 2,500-years-old.  They also have more than 300 uses. The leaves, rich in iron, can be boiled and eaten like spinach. The seeds can be roasted to make a coffee substitute or pressed to make oil for cooking or cosmetics. The fruit pulp has six times more vitamin C than oranges, making it an important nutritional complement in Africa and in the European, US and Canadian markets.  Locally, fruit pulp is made into juice, jam, or fermented to make beer. The young seedlings have a taproot which can be eaten like a carrot. The flowers are also edible. The roots can be used to make red dye, and the bark to make ropes and baskets.  Baobabs also have medicinal properties, and their hollow trunks can be used to store water. Baobab crowns also provide shade, making them an idea place for a market in many rural villages. And of course, the trade in baobab products provides an income for local communities.  Baobab trees also play a big part in the cultural life of their communities, being at the centre of many African oral stories. They even appear in The Little Prince.  Baobab trees are not only useful to humans, they are key ecosystem elements in the dry African savannas. Importantly, baobab trees keep soil conditions humid, favour nutrient recycling and avoid soil erosion. They also act as an important source of food, water and shelter for a wide range of animals, including birds, lizards, monkeys and even elephants – which can eat their bark to provide some moisture when there is no water nearby. The flowers are pollinated by bats, which travel long distances to feed on their nectar. Numerous insects also live on the baobab tree. Ancient as they are, baobab trees can be cultivated, as some communities in West Africa have done for generations. Some farmers are discouraged by the fact that they can take 15-20 years to fruit – but recent research has shown by grafting the branches of fruiting trees to seedlings they can fruit in five years.  Many “indigenous” trees show great variation in fruit morphological and nutritional properties – and it takes years of research and selection to find the best varieties for cultivation. This process, called domestication, does not refer to genetic engineering, but the selection and cultivation of the best trees of those available in nature. It seems straightforward, but it takes time to find the best trees – meanwhile many of them are dying. The death of these oldest and largest baobab trees is very sad, but hopefully the news will motivate us to protect the world’s remaining large baobabs and start a process of close monitoring of their health. And, hopefully, if scientists are able to perfect the process of identifying the best trees to cultivate, one day they will become as common in our supermarkets as apples or oranges."
"
Share this...FacebookTwitterHey, we just saw something similar from Japan. 
===============================================
On Spitzbergen it was as warm 70 years ago as it is today
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated by P Gosselin)
Newspapers like to write about heat and melt records in the Arctic, which supposedly had never happened before. That really sparks fear among the citizens. However an examination of the facts regularly brings amazing things to light, for example weather records from a German station on Spitzbergen during World War 2 for the period of 1944-1945.
In the journal International Journal of Climatology Rajmund Przybylak and his colleagues evaluated the data. Summary: Back then it was similarly warm as it is today. Abstract:
Air temperature conditions in northern Nordaustlandet (NE Svalbard) at the end of World War II
This article presents the results of an investigation into air temperature conditions in northern Nordaustlandet (NE Svalbard) based on meteorological observations made by German soldiers towards the end of World War II (1944/1945) and 4 months after its end. Traditional analysis using mean monthly data was supplemented by a detailed analysis based on daily data: maximum temperature, minimum temperature and diurnal temperature range. The latter kind of data made it possible to study such aspects of climate as the number of “characteristic days” (i.e., the number of days with temperatures exceeding specified thresholds), day‐to‐day temperature variability, and duration, onset and end dates of thermal seasons. The results from Nordaustlandet for the warmest period of the early 20th century warming period (ETCWP) were compared with temperature conditions both historical (the end part of the Little Ice Age) and contemporary (different sub‐periods taken from the years 1981–2017) to estimate the range of warming during the ETCWP.
Analysis reveals that the expedition year 1944/1945 in Nordaustlandet was, in the majority of months, the warmest of all analysed periods, that is, both historical and contemporary periods. The study period was markedly warmer than 1981–2010 (mean annual −6.5 vs. −8.4 °C) but colder than the periods 2011–2016 (−5.7 °C) and 2014–2017 (−5.8 °C). The majority of mean monthly air temperatures in the ETCWP lies within two standard deviations of the modern 2014–2017 mean. This means that values of air temperature in the study period lie within the range of recent temperature variability. All other thermal characteristics show changes in accordance with expectations associated with general warming of the Arctic (i.e., a decrease in diurnal temperature range and number of cold days, and an increase in number of warm days). The latter days were most common in the ETCWP.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNever mind the severe cold hitting the Super Bowl this year, or scientists lecturing us on global warming while their host resort Davos got buried in snow.
There’s been a lot of cold gripping all over the northern hemisphere this winter – much more than many of us expected. Europe has also joined in on the freeze-fest as the harsh winter spreads across the old continent and even into Africa:

Cold is forecast to keep Europe shivering this week. Image cropped from wetter.online.de.
Cooling globe
One reason for this could be due to the rapidly falling global surface temperatures  as recorded by satellite data. In January the global mean temperature anomaly dropped to +0.26°C, with the tropics (where most of the heat is found) posting a nippy -0.12°C anomaly, according to Dr. Roy Spencer.
Other cooling factors include the current La Nina and possibly the low solar activity playing a role. IceAgeNow here reported last July that solar activity was at its most rapid decline in 9300 years.
Europe
In Northern Europe cold winter are normal, but the recent forecast for the Finnish region of Lapland warned of temperatures down to -40°C. This Finnish website here writes:
Temperatures have been low all winter in Finnish Lapland, but the cold dip expected this week could see record-breaking extremes.”
Snow and cold are also forecast across UK as the Express here reports: “Britain set for FOOT of heavy snow NEXT WEEK in COLDEST freeze for decade.”
Spain, North Africa get frostbitten and snowed on
The wintry conditions will likely impact agriculture and the European food markets. Fruitnet.com here writes: ” The cold snap gripping much of the Spanish peninsula is likely to reduce the availability of vegetables and salads on the European market during the coming weeks.”
The extreme cold has even extended beyond southern Europe and into Africa! For example Southern Morocco saw snow for the first time. And so has the Canary island of Tenerife seen its landscape get blanketed with the white stuff.
Stunning Sahara snow
Also The Mail here reports snowfall in the Algeria – the Sahara Desert. Up to December 2016 it had not snowed there in 37 years. Now according to The Mail it has snowed 4 times since, and it’s the second time this year already. “Locals were stunned to see snow on the sand dunes in the Sahara Desert yesterday.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Records in Japan
Japanese blogger Kirye here tweeted that minimum temperature dropped to -17.2 ℃ on February 2, 2018, in Ikarigaseki, Aomori Prefecture. “It is the coldest daily minimum temperature since records began on November 24, 1976! The previous record low was -16.6 ℃, set on January 18, 2014.”
Kirye also tweeted: “The minimum temperature in Fuchu, Tokyo dropped to minus 8.4 ℃ on January 25, 2018. It is the lowest daily minimum temperature since records began on December 15, 1976. The previous record low was minus 8.2 ℃, set on February 8, 1984.”
Moreover, Japan’s mean temperature anomaly for January 2018 was a chilly -0.22 ℃. Kirye writes that there’s been no warming trend for January from 1986 to 2018.
The English language NTV of Japan writes: “This winter’s harshest cold wave continues in Japan with freezing temperatures in central Tokyo recorded two days in a row for the first time in 55 years.”
Kirye adds: “The minimum temperature dropped to -3.1℃ on January 26 in Tokyo. It is the coldest daily minimum temperature for January 26 since 1965.”
Russian Snowmageddon…minus 67°C
As I already highlighted here earlier, a number of locations across the northern hemisphere are seeing surprising brutal winter conditions. Another example: media outlets have reported widely that Moscow just saw a record snowfall. Also read here.
And in the Siberian region of Yakutia, the temperature fell as low as minus 67 Celsius.
Australia and New Zealand
Even the southern hemisphere has not been spared. The weatherzone.com.au here reports “many towns in south-east Queensland have experienced their coldest February day on record”  and that “Archerfield managed only 21°C and Coolangatta on the Gold Coast 21.6°C”.
According to a local meteorologist: “These are the coldest February days that we’ve ever experienced in those places and some of those records date back quite some time.”
Finally Ice Age Now here writes that Tasmania even recorded a “summer blizzard.”
That’s a lot of winter, snow and cold for a planet that is supposedly warming rapidly.
 
Share this...FacebookTwitter "
"The BBC is under attack again, but not from its usual right-wing opponents.  This time the charge comes from those concerned about the amount of unchallenged air time the BBC gives to climate change sceptics.  The current controversy centres around an episode of the Radio 4 programme What’s the point of …, presented by Daily Mail columnist Quentin Letts, in which the work of the UK’s national weather service the Met Office is subjected to unsubstantiated criticism. Leading the attack is one of the BBC’s former environment correspondents, Richard Black. He has blasted the editors of the programme for allowing Letts to play fast and loose with the BBC editorial guidelines.  Black contends that the material in the programme, contrary to the guidelines, was not “well sourced and based on sound evidence”.  The programme is worth a close listen as it raises important questions about the presentation on air of minority views on climate change, the ubiquitous presence of non-specialist opinion in the British media, and its possible effect on audiences. The programme’s sub-title is “expensive liability or essential?”, but much of it is an attack on the Met Office over its mainstream position on climate change.  One of the first witnesses Letts introduces to attack the Met Office is Piers Corbyn, a well-known sceptic of mainstream climate science. His scientific credentials to speak on the issue are never established. A little later comes Graham Stringer, a Labour MP, who casts doubt on the link between climate change and the 2013/2014 UK flooding:  …the chief scientific officer [at the Met Office] said that this was undoubtedly due to climate change, but most of the scientists even in the Met Office looked askance at that, because there’s no scientific evidence whatsoever that rain was related to climate change. However, it turns out there is evidence to link the two, according to Dr Friederike Otto from the Environmental Change Institute at Oxford University. “I would say that Stringer is wrong”, she tells me. “We do have scientific evidence that the likelihood of these kinds of floods occurring has increased.” She and her colleagues have studied the UK floods as part of their wider research on individual extreme weather events becoming more (or less) likely as a result of climate change.   Stringer is followed by Conservative MP and self-described “luke-warmist” Peter Lilley. He is allowed to put the case for the so-called “climate pause” since 1998 without any challenge. More significantly, there is no mention that both Stringer and Lilley sit on the board of trustees of the sceptic campaigning organisation, the Global Warming Policy Foundation.  Omitting the interests of interviewees in this way does not give the listener enough context to understand their views.  You can argue that in the name of pluralism it’s desirable to have minority views on air, but they must be clearly labelled and fairly challenged. In this instance, Letts laughs along with Stringer and Lilley; only the Met Office representative is confronted. As the host of a “personal view” programme, Quentin Letts may enjoy more editorial latitude than most. But the BBC editorial guidelines are clear, stating that authored pieces, “particularly when dealing with controversial subjects, should be clearly signposted to audiences in advance”.  I may have missed it, but I did not hear Letts’ programme presented as such. Such pieces should also “retain a respect for factual accuracy”, and “fairly represent opposing viewpoints when appropriate”.  Letts could have kept the wit in his text and still have been true to the guidelines.    The wider picture in the UK media’s coverage of climate change is that in recent years it has often been non-specialist opinion that gets disproportionate time or space.  A recent study of the presence of sceptical voices in the UK print media concluded that such voices were more likely to be included in pieces written by in-house non-specialist columnists than by environment editors or correspondents. It would be worrying if the BBC was going down a similar path of giving exaggerated space to non-specialists.  Despite the recent revolution in the way people, and particularly younger age groups, consume news, the BBC is still a very well used and trusted source. Research shows that the promotion or presence of uncertainty in media reporting of climate science can act as an obstacle to public understanding and lead to disengagement, so it is critically important that the BBC provides proper context when covering such an important issue.  In one of his final comments, Letts describes the Met Office as following a “politically risky intervention on climate change said by some fellow scientists to be plain wrong”. Which scientists say this, and why weren’t they invited onto the programme? Much of the BBC’s coverage that relies on the expertise of its correspondents and editorial guidelines is first class, but problems arise when handing over airtime to others to make assertions like this without due scrutiny. Maybe it’s time for the BBC editors to dust off their handbooks."
"
Share this...FacebookTwitterGovernments promote biofuels as renewable, carbon-neutral resources that serve to reduce CO2 emissions.  Meanwhile, scientists have determined that biomass burning generates more CO2 emissions per kWh than burning coal does, and the projected rapid growth in biofuel use will only serve to ‘increase atmospheric CO2 for at least a century’. 

Sterman et al., 2018
“[G]overnments around the world are promoting biomass to reduce their greenhouse gas (GHG) emissions. The European Union declared biofuels to be carbon-neutral to help meet its goal of 20% renewable energy by 2020, triggering a surge in use of wood for heat and electricity (European Commission 2003, Leturcq 2014, Stupak et al 2007). … But do biofuels actually reduce GHG emissions?”
“[A]lthough wood has approximately the same carbon intensity as coal (0.027 vs. 0.025 tC GJ−1 of primary energy […]), combustion efficiency of wood and wood pellets is lower (Netherlands Enterprise Agency; IEA 2016). Estimates also suggest higher processing losses in the wood supply chain (Roder et al 2015). Consequently, wood-fired power plants generate more CO2 per kWh than coal. Burning wood instead of coal therefore creates a carbon debt—an immediate increase in atmospheric CO2 compared to fossil energy—that can be repaid over time only as—and if— NPP [net primary production] rises above the flux of carbon from biomass and soils to the atmosphere on the harvested lands.”
“Growth in wood supply causes steady growth in atmospheric CO2 because more CO2 is added to the atmosphere every year in initial carbon debt than is paid back by regrowth, worsening global warming and climate change. The qualitative result that growth in bioenergy raises atmospheric CO2 does not depend on the parameters: as long as bioenergy generates an initial carbon debt, increasing harvests mean more is ‘borrowed’ every year than is paid back. More precisely, atmospheric CO2 rises as long as NPP [net primary production] remains below the initial carbon debt incurred each year plus the fluxes of carbon from biomass and soils to the atmosphere.”
“[P]rojected growth in wood harvest for bioenergy would increase atmospheric CO2 for at least a century because new carbon debt continuously exceeds NPP.”
“[C]ontrary to the policies of the EU and other nations, biomass used to displace fossil fuels injects CO2 into the atmosphere at the point of combustion and during harvest, processing and transport. Reductions in atmospheric CO2 come only later, and only if the harvested land is allowed to regrow.”

Fanous and Moomaw, 2018
“These nations fail to recognize the intensity of CO2 emissions linked to the burning of biomass. The chemical energy stored in wood is converted into heat or electricity by way of combustion and is sometimes used for combined heat and power cogeneration. At the point of combustion, biomass emits more carbon per unit of heat than most fossil fuels. Due to the inefficiencies of biomass energy, bioenergy power plants emit approximately 65 percent more CO2 per MWH than modern coal plants, and approximately 285 percent more than natural gas combined cycle plants.”
“Furthermore, the Intergovernmental Panel on Climate Change (IPCC) states that combustion of biomass generates gross greenhouse gas (GHG) emissions roughly equivalent to the combustion of fossil fuels. In the case of forest timber turned into wood pellets for bioenergy use, the IPCC further indicates that the process produces higher CO2 emissions than fossil fuels for decades to centuries.”
 

Share this...FacebookTwitter "
nan
"Around the world, cities endeavour to cut greenhouse gas emissions, while adapting to the threats – and opportunities – presented by climate change. It’s no easy task, but the first step is to make a plan outlining how to meet the targets set out in the Paris Agreement, and help limit the world’s mean temperature rise to less than two degrees Celsius above pre-industrial levels.  About 74% of Europe’s population lives in cities, and urban settlements account for 60-80% of carbon emissions – so it makes sense to plan at an urban level. Working to meet carbon reduction targets can also reduce local pollution and increase energy efficiency – which benefits both businesses and residents.  But it’s just as important for cities to adapt to climate change – even if the human race were to cut emissions entirely, we would still be facing the extreme effects of climate change for decades to come, because of the increased carbon input that has already taken place since the industrial revolution.  In the most comprehensive survey to date, we collaborated with 30 researchers across Europe to investigate the availability and content of local climate plans for 885 European cities, across all 28 EU member states. The inventory provides a big-picture overview of where EU cities stand, in terms of mitigating and adapting to climate change.  The good news is that 66% of EU cities have a mitigation or adaptation plan in place. The top countries were Poland – where 97% of cities have mitigation plans – Germany (81%), Ireland (80%), Finland (78%) and Sweden (77%). In Finland, 78% of cities also had a plan for adapting to climate change.  But only a minority of EU countries – including Denmark, France, Slovakia and the UK – have made it compulsory for cities to develop local climate plans. In these countries, cities are nearly twice as likely to have a mitigation plan and five times as likely to have an adaptation plan. Throughout the rest of the EU, it is mainly large cities that have local climate plans.  There were some shortcomings worth noting: 33% of EU cities (that’s 288 cities) have no standalone climate plans whatsoever – including Athens (Greece), Salzburg (Austria), and Palma de Mallorca (Spain). And not one city in Bulgaria or Hungary has a standalone climate plan. Only 16% of cities – that’s a total of 144 – have joined-up mitigation and adaptation plans, and most of these were in France and the UK – though cities such as Brussels (Belgium), Helsinki (Finland) and Bonn (Germany) had joined-up plans as well.  Some cities have made climate initiatives a common feature in planning activities, often aiming for broader environmental goals, such as resilience and sustainability. Some of these forward-looking cities – Rotterdam and Gouda in the Netherlands, for example – may not have standalone climate change mitigation or adaptation plans, per se. Instead, climate issues are integrated into broader development strategies, as also seen in Norwich, Swansea, Plymouth and Doncaster in the UK. Plans for mitigating the effects of climate change are generally straightforward: they look at ways to increase efficiency, transition to clean energy and improve heating, insulation and transport. In doing so, they are likely to result in financial savings or health benefits for the municipality, and the public. For example, more low-emission vehicles on the road doesn’t just mean less carbon emissions – it also means better air quality for the city’s residents.  Adapting to climate change is not always so simple. Each area will need to adapt in different ways. Some adaptations – such as flood defences – can require huge investment to build, and only rarely prove their effectiveness. Yet there are plans and measures that cities can take, to both mitigate the threats from climate change and adapt to the changes that are already coming.  One way for cities to become more resilient to climate change is to integrate infrastructures for energy, transport, water and food, and allow them to combine their resources. A sensors become more commonplace across European cities, it’s easier to monitor the impacts of local plans to reduce emissions and stay on top of extreme weather. The University of Newcastle in the UK is home to the Urban Observatory, which provides one of the largest open-source digital urban sensing networks in the world. Across the board, cities need to improve the way they manage water at the surface and below ground. Installing more green features in city centres or strategic locations can help urban areas adapt to heatwaves, extreme rainfall and droughts all at once. To find out what works and what doesn’t, it’s essential for cities to network and share knowledge, to create and improve on their local climate plans.  There is simply too much at stake for the world’s cities to go their separate ways when it comes to climate change. We have found that international climate networks make a big difference to countries and cities, as they develop and implement their climate plans. For instance, 333 EU cities of our sample are signatories of the Covenant of Mayors and through that are given support and encouragement as they engage in climate change planning and action. Our study shows that cities are taking climate change threats seriously, but there is clearly more work to be done. It is a near certainty that if cities do not plan and act now to address climate change, they could find themselves in a far more precarious position in the future.  While there is plenty that cities can do, national governments must still take the lead – providing legal and regulatory frameworks and guidance. Our study has demonstrated that this is one of the most effective ways to make sure that cities – and their citizens – are well prepared for the threats and opportunities that climate change will bring."
"
Share this...FacebookTwitterThe extreme German leftists Die Linke (The Left) Party in Germany issued a press release blasting Angela Merkel’s decision not to personally attend the “One Planet Summit” in Paris.
Apparently climate change is not an issue important enough for the German chancellor to devote her time to.
Merkel “a total no-show”
German Leftist Party climate and energy politician Lorenz Gösta Beutin said:
Climate-politically Merkel is a total no show: In Paris the head of the German government could have sent a powerful signal in support of implementing the UN Climate Treaty, and against the unspeakable anti-climate protection course of President Donald Trump, who announced the USA’s withdrawal from the Climate Accord and requested renegotiations. Instead the federal government sent Environment Minister Barbara Hendricks, who had to explain that Germany would resoundingly miss its self-stated climate targets.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




International “faux pas”
The “One Planet Summit” in Paris was held by France, the United Nations and the World bank. Reutin commented further:
That Angela Merkel did not attend the climate conference – where over 50 state and government heads from all around the world wanted to push ahead the historic Paris climate protection accord from two years ago – is a disgrace par excellence. After an embarrassing appearance at the UN climate conference in Bonn, where the chancellor only delivered empty words instead of concrete measures, the self-proclaimed climate chancellor demonstrated a faux pas on the international stage.”
The neoliberal belief held by French President Emmanuel Macron that the free market and private capital would put the brakes on global warming and remove the damage to climate change caused by man and nature is faulty. Capitalism functions only through unbridled growth and the profit of a few. It is the cause, not the solution, of humanity-problem climate change.
The LINKE (LEFTISTS) demand the introduction of a financial transaction tax, whose revenue would in part be allocated to southern countries as climate change support funds. Also the industrialized countries must meet their obligations and pay 100 billion dollars annually  into the Green Climate Fund beginning in 2020, and do so without offsetting already existing development aid funds.“
 
Share this...FacebookTwitter "
nan
"Cars are one of the biggest threats to the planet. The transport sector accounts for more than 60% of global oil consumption and about a quarter of energy-related carbon emissions, and it’s seen as harder to decarbonise than other parts of the economy. Typical forecasts of future world vehicle ownership point to substantial increases, particularly in the developing economies. But the problem of transport-related greenhouse gases may be less than generally thought. There is emerging evidence that individual car use, as measured by the average annual distance travelled, has ceased to grow in most of the developed economies, starting well before the recent recession. In some countries, it may already be declining, a phenomenon known as “peak car”.  A number of factors could could contribute to this trend. Suggestions have included a decline in younger people holding driver’s licences, changes to company car taxation and technological constraints that stop us travelling faster on roads. And it may be we have simply sufficient daily travel to meet our needs. There has also been a shift away from car use in urban areas. This could be particularly important in a world where future population growth will be mainly urban and densely populated cities are seen as a driver for economic growth. For example, over the past 20 years the population of London has been growing and incomes have been rising, but car use has held steady at about 10m trips a day. This is mainly because the city has not increased road capacity but instead has invested in public transport. Most importantly, rail offers speedy and reliable travel for work journeys compared with the car on congested roads. This gets business and professional people out of their cars, which makes the city a less congested and more agreeable place to be. With a growing population but static car use, London has seen a marked decline in the share of journeys by car, from 50% of all trips in 1990 to 37% currently. With continued population growth projected and more investment in rail planned, the share of trips by car could fall to 27% by mid-century. There is every reason to suppose that London will continue to thrive as car use declines – and perhaps because car use declines. This decrease in car use from 1990 was preceded by a 40-year period of growth from 1950, the result of rising incomes leading to increased car ownership – and, at the same time, a falling population as people left an overcrowded damaged city for new towns, garden cities and greener surroundings. So we see a marked peak in car use at around 1990, the time when the population of London was at a minimum, which was when attitudes to city living began to change. This phenomenon of peak car in big cities is not unique to London, although this is the city for which we have the best data. There is evidence for something similar happening in Birmingham, Manchester and other British cities as well as those in other developed countries. The shift in economies from manufacturing to services is an important driver, as is the growth of higher education located in city centres, attracting young people for whom the car is not part of their lifestyle. If car use has really peaked, both in the sense of national per capita figures and the share of trips in cities, it should help mitigate greenhouse gas emissions from transport. I have estimated that these changes in behaviour, taken together with expected developments of low-emission vehicles, could by 2050 reduce UK surface transport greenhouse gas emissions by 60% compared with 1990. This falls short of the overall target of an 80% reduction, but is a good deal better than conventional projections. Peak car is not just an emerging phenomenon to be investigated. It is a helpful trend to be encouraged, to achieve both successful, sustainable cities and national reduction of transport greenhouse gas emissions."
"
Share this...FacebookTwitter“We now seriously need a Schellnhuber timeout. […] We do hope the new PIK leadership will correct the extreme direction the institute is currently on and rapidly puts an end to the flow of climate-alarmist press releases.“
At their Die kalte Sonne site here, Dr. Sebastian Lüning and Prof. Fritz Vahrenholt comment on Prof. Hans-Joachim Schellnhuber’s stepping down as director of the alarmist Potsdam Institute for Climate Impact research (PIK).

Prof. Hans-Joachim Schellnhuber stepping down as head of the Potsdam Institute (PIK). Photo: PIK.
Schellnhuber is often worshipped by the fringe-element climate alarmists as a sort of Climate Pope, whose every uttered word is to be regarded as infallible.  Now he may be paying the price for his entrenched, radical positions on climate change.
========================================
From Die kalte Sonne here, by Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(Translated by P Gosselin)
Hans-Joachim Schellnhuber is stepping down later this year in autumn and consequently will relinquish his position as director at the Potsdam PIK Climate Institute. The successors have already been named. Over the past years Schellnhuber had increasingly become a burden for the institute. Parts of the German political spectrum had already requested his removal from the “Scientific Advisory Council for Environmental Change” (WBGU) for representing a direction leading to a green dictatorship. In the end it was likely a direct decision by the Chancellor that saved him. Recently there had been a mysterious spree publications by Schellnhuber appearing in the journal of the National Academy of Science. The suspected secret is that he was allowed to choose his peer reviewers himself as a member of the society.
Parts of the press also reported what many already suspected. Spiegel on Hans-Joachim Schellnhuber: “One is getting the impression that he has become more activist than physicist”. Here his role as a ghostwriter for the Pope fits very well as he basically put all the words of his choice into the Pope’s mouth. He almost totally ignored all the uncertainties climate science is afflicted by. His aim: The unconditional destruction of the fossil fuel industry. And as a direct advisor to the chancellor, he pushed this message to the highest levels. History over the coming years and decades will be decided by Schellnhuber’s role in the climate debate and the overly hasty rush into renewable energies. We now seriously need a Schellnhuber timeout. We do hope the new PIK leadership will correct the extreme direction the institute is currently on and rapidly puts an end to the flow of climate-alarmist press releases. What really is now needed is a balanced and rational presentation of the results, without the constant pressure of having to do missionary work.”
Share this...FacebookTwitter "
"Imagine studying animals without seeing them. Does that sound ludicrous? To people like us, who first got interested in biology because we love animals and enjoy studying them, yes, it sounds like a poor deal. Yet, if you think about what forensic investigators do when they seek DNA evidence at a crime scene, or what doctors do when they detect a pathogen in a patient’s blood, it is exactly that: they detect life forms without seeing them. DNA is life’s blue print. It is present in virtually every organism on Earth, and we usually study it by extracting it from a piece of tissue or a blood sample. But DNA, really, is everywhere: animals shed it constantly, when they scratch themselves, when they release urine, eggs, saliva, excrement and, of course, when they die. Every environment, from your bed to the deepest recesses of the oceans, is full of “biological dust”, mostly cellular material, which contains the DNA of the organisms that left it behind. This, we call “environmental DNA”, or eDNA. Assisted by increasingly fast, accurate and affordable technology, scientists have begun, in recent years, to sequence this trace DNA from many environments. And this “micro” approach has even proved to be useful to scientists investigating environments as vast as the oceans. Many marine animals are large, rare, elusive and highly mobile. Sharks are an obvious example: in the oceans they make up a small proportion of the biomass, most of them are pretty difficult to catch, and they have been in conflict with humans since we started venturing at sea. With a few exceptions, they avoid us, and because of us many have become threatened with extinction.  This is why we thought it would be interesting to see if, just by sampling a few bottles of ocean water (and the DNA fragments therein), we could rapidly map shark presence and distribution, without engaging in wild chases or employing time and resource-intensive shark fishing methods. We were happy to find out that, indeed, this was possible, and that different species could be detected in different geographical regions, although the areas that had been more affected by humans would show scant presence of sharks. But the true measure of the efficiency of this eDNA approach to shark monitoring would only be revealed when contrasted against established, tried-and-tested methodologies, such as scuba-diving visual censuses or baited underwater camera recordings.  This was the focus of our most recent study, conducted with colleagues based in the South Pacific archipelago of New Caledonia, France, Australia and the US, and now published in the journal Science Advances. The results were very exciting: 22 water samples collected over a few weeks detected more sharks than hundreds of baited underwater camera observations over two years, and thousands of scuba dives over a period of decades. Nearly half of the species detected through environmental DNA could not be found at all using traditional methods. And while eDNA could detect the presence of some sharks in about 90% of the samples, underwater cameras could only manage just over 50%, and scuba diving around 15%.  Interestingly, eDNA outperformed the other methods in both pristine and impacted areas. A range of shark species were detected even in busy, noisy and depleted areas, where they were thought to be extirpated. This suggests some “dark diversity” may still be present, in the form of remnant individuals and groups requiring protection. Similarly, eDNA can help by revealing the appearance of newly established, alien species that are expanding their range. All of this is good news for everyone, and this is why. Given the speed and efficiency of eDNA sampling, a much larger portion of the sea can be screened, in a shorter time, to gather an overview of the patterns of diversity across large areas and habitats, along various environmental gradients, and at different times. Potentially, we could rapidly build maps of species diversity and use them to create predictive models and identify the factors that influence diversity, while methods are being developed to improve the quantitative aspect of eDNA detection, also in other charismatic species. All of it will be of great help to those who must devise plans to protect crucial habitats and ecosystems.  Environmental DNA science is still rapidly developing. The databases that we use to match the unknown sequences retrieved from the sea must be enriched with new DNA references of many existing species – every multi-species eDNA study to date has detected large amounts of sequences that could not be matched against any reference. A significant proportion of these belong to organisms that are yet to be described by scientists.  The “DNA probes” currently available will have to become longer, as short sequences may sometimes fail to distinguish closely related species. For instance, the blacktip shark shared some identical sequences with the grey reef shark along the DNA stretch used in our study. Nevertheless, all the initial indications suggest that this approach can get us a step closer to understanding and better managing the largest ecosystem on Earth."
"
Share this...FacebookTwitterUpdated: The Shrinking 
CO2 Climate Sensitivity

A recently highlighted paper published by atmospheric scientists Scafetta et al., (2017) featured a graph (above) documenting post-2000 trends in the published estimates of the Earth’s climate sensitivity to a doubling of CO2 concentrations (from 280 parts per million to 560 ppm).
The trajectory for the published estimates of transient climate response (TCR, the average temperature response centered around the time of CO2 doubling) and equilibrium climate sensitivity (ECS, the temperature response upon reaching an equilibrium state after doubling) are shown to be declining from an average of about 3°C earlier in the century to below 2°C and edging towards 1°C for the more recent years.
This visual evidence would appear to indicate that past climate model determinations of very high climate sensitivity (4°C, 5°C, 6°C and up) have increasingly been determined to be in error.  The anthropogenic influence on the Earth’s surface temperature has likely been significantly exaggerated.

Scafetta et al., 2017   “Since 2000 there has been a systematic tendency to find lower climate sensitivity values. The most recent studies suggest a transient climate response (TCR) of about 1.0 °C, an ECS less than 2.0 °C and an effective climate sensitivity (EfCS) in the neighborhood of 1.0 °C.”
“Thus, all evidences suggest that the IPCC GCMs at least increase twofold or even triple the real anthropogenic warming. The GHG theory might even require a deep re-examination.”

An Update On The Gradually Declining Climate Sensitivity


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The graph shown in Scafetta et al. (2017) ends in 2014, which means that papers published in the last 3 years are not included.   Also, there were several other published climate sensitivity papers from the last decade that were excluded from the analysis, possibly because they did not include and/or specify TCR and/or ECS estimates in isolation, but instead just used a generic doubled-CO2 climate sensitivity value (shown in purple here).
Below is a new, updated graph that (1) includes some of the previously unidentified papers and (2) adds the 10 – 12 climate sensitivity papers published in the last 3 years.  Notice, again, that the trend found in published papers has continued downwards, gradually heading towards zero.  The reference list for the over 20 additional papers used for the updated analysis is also included below.
For a more comprehensive list of over 60 papers with very low (<1°C) climate sensitivity estimates, see here.



Smirnov, 2017 (~0.4°C)
It is shown that infrared emission of the atmosphere is determined mostly by atmospheric water. One can separate the flux of outgoing infrared radiation of the atmosphere from that towards the Earth. The fluxes due to rotation-vibration transitions of atmospheric   CO2  molecules are evaluated. Doubling of the concentration of  CO2 molecules in the atmosphere that is expected over 130 years leads to an increase of the average Earth temperature by (0.4±0.2) K mostly due to the flux towards the Earth if other atmospheric parameters are not varied.

Smirnov, 2016
[W]e take into account that CO2 molecules give a small contribution to the heat Earth balance and, therefore, one can use the altitude distribution of the temperature for the standard atmosphere model [1], and a variation of the CO2 concentration does not influence this distribution.  …  [I]njection of CO2 molecules into the atmosphere leads to a decrease of the outgoing radiation flux that causes a decrease of the average Earth temperature. But this decrease is below 0.1K that is the accuracy of determination of this value.  Thus, the presence of carbon dioxide in the atmosphere decreases the outgoing atmospheric radiative flux that leads to a decrease of the Earth temperature by approximately (1.8 ± 0.1) K. The change of the average temperature at the double of the concentration of atmospheric CO2 molecules is determined by the transition at 667cm−1 only and is lower than 0.1K.
In particular, doubling of the concentration of CO2 molecules compared to the contemporary content increases the global Earth temperature by ΔT = 0.4 ± 0.2K. … From this we have that the average temperature variation ΔT = 0.8 ◦C from 1880 up to now according to NASA data may be attained by the variation of the water concentration by 200ppm or Δu/u ≈ 0.07, Δu = 0.2. Note that according to formula (2) the variation of an accumulated concentration of CO2 molecules from 1959 (from 316ppm up to 402ppm) leads to the temperature variation ΔT = 0.15°C. One can see that the absorption of a water molecule in infrared spectrum is stronger than that of the CO2 molecule because of their structures, and the injection of water molecules in the atmosphere influences its heat balance more strongly than the injection of CO2 molecules.



Reinhart, 2017 (<0.24°C)
Our results permit to conclude that CO2 is a very weak greenhouse gas and cannot be accepted as the main driver of climate change. … The assumption of a constant temperature and black body radiation definitely violates reality and even the principles of thermodynamics. … [W]e conclude that the temperature increases predicted by the IPCC AR5 lack robust scientific justification. … A doubling [to 800 ppm] of the present level of CO2 [400 ppm] results in  [temperature change] < 0.24 K. … [T]he scientific community must look for causes of climate change that can be solidly based on physics and chemistry. … The observed temperature increase since pre-industrial times is close to an order of magnitude higher than that attributable to CO2.

Abbot and Marohasy, 2017  (0.6°C equilibrium)
The largest deviation between the ANN [artificial neural network] projections and measured temperatures for six geographically distinct regions was approximately 0.2 °C, and from this an Equilibrium Climate Sensitivity (ECS) of approximately 0.6 °C [for a doubling of CO2 from 280 ppm to 560 ppm plus feedbacks] was estimated. This is considerably less than estimates from the General Circulation Models (GCMs) used by the Intergovernmental Panel on Climate Change (IPCC), and similar to estimates from spectroscopic methods.
The proxy measurements suggest New Zealand’s climate has fluctuated within a band of approximately 2°C since at least 900 AD, as shown in Figure 2. The warming of nearly 1°C since 1940 falls within this band. The discrepancy between the orange and blue lines in recent decades as shown in Figure 3, suggests that the anthropogenic contribution to this warming could be in the order of approximately 0.2°C. [80% of the warming since 1940 may be due natural factors].

 Harde, 2016 (0.7°C equilibrium)
Including solar and cloud effects as well as all relevant feedback processes our simulations give an equilibrium climate sensitivity of CS = 0.7 °C (temperature increase at doubled CO2) and a solar sensitivity of SS = 0.17 °C (at 0.1 % increase of the total solar irradiance). Then CO2 contributes 40 % and the Sun 60 % to global warming over the last century.

Bates, 2016  (~1°C)
Estimates of 2xCO2 equilibrium climate sensitivity (EqCS) derive from running global climate models (GCMs) to equilibrium. Estimates of effective climate sensitivity (EfCS) are the corresponding quantities obtained using transient GCM output or observations. The EfCS approach uses an accompanying energy balance model (EBM), the zero-dimensional model (ZDM) being standard. GCM values of EqCS and EfCS vary widely [IPCC range: (1.5, 4.5)°C] and have failed to converge over the past 35 years. Recently, attempts have been made to refine the EfCS approach by using two-zone (tropical/extratropical) EBMs. When applied using satellite radiation data, these give low and tightly-constrained EfCS values, in the neighbourhood of 1°C. … The central conclusion of this study is that to disregard the low values of effective climate sensitivity (≈1°C) given by observations on the grounds that they do not agree with the larger values of equilibrium, or effective, climate sensitivity given by GCMs, while the GCMs themselves do not properly represent the observed value of the tropical radiative response coefficient, is a standpoint that needs to be reconsidered.

Evans, 2016 (<0.5°C equilibrium)
The conventional basic climate model applies “basic physics” to climate, estimating sensitivity to CO2. However, it has two serious architectural errors. It only allows feedbacks in response to surface warming, so it omits the driver-specific feedbacks. It treats extra-absorbed sunlight, which heats the surface and increases outgoing long-wave radiation (OLR), the same as extra CO2, which reduces OLR from carbon dioxide in the upper atmosphere but does not increase the total OLR. The rerouting feedback is proposed. An increasing CO2 concentration warms the upper troposphere, heating the water vapor emissions layer and some cloud tops, which emit more OLR and descend to lower and warmer altitudes. This feedback resolves the nonobservation of the “hotspot.” An alternative model is developed, whose architecture fixes the errors. By summing the (surface) warmings due to climate drivers, rather than their forcings, it allows driver-specific forcings and allows a separate CO2 response (the conventional model applies the same response, the solar response, to all forcings). It also applies a radiation balance, estimating OLR from properties of the emission layers. Fitting the climate data to the alternative model, we find that the equilibrium climate sensitivity is most likely less than 0.5°C, increasing CO2 most likely caused less than 20% of the global warming from the 1970s, and the CO2 response is less than one-third as strong as the solar response. The conventional model overestimates the potency of CO2 because it applies the strong solar response instead of the weak CO2response to the CO2 forcing.

Gervais, 2016 [full]  (<0.6°C transient)
Conclusion: Dangerous anthropogenic warming is questioned (i) upon recognition of the large amplitude of the natural 60–year cyclic component and (ii) upon revision downwards of the transient climate response consistent with latest tendencies shown in Fig. 1, here found to be at most 0.6 °C once the natural component has been removed, consistent with latest infrared studies (Harde, 2014). Anthropogenic warming well below the potentially dangerous range were reported in older and recent studies (Idso, 1998; Miskolczi, 2007; Paltridge et al., 2009; Gerlich and Tscheuschner, 2009; Lindzen and Choi, 2009, 2011; Spencer and Braswell, 2010; Clark, 2010; Kramm and Dlugi, 2011; Lewis and Curry, 2014; Skeie et al., 2014; Lewis, 2015; Volokin and ReLlez, 2015). On inspection of a risk of anthropogenic warming thus toned down, a change of paradigm which highlights a benefit for mankind related to the increase of plant feeding and crops yields by enhanced CO2 photosynthesis is suggested.

Marvel et al., 2016 (1.8°C transient, 3.0°C equilibrium)
Assuming that all forcings have the same transient efficacy as greenhouse gases, and following a previous study, the best estimate (median) for TCR is 1.3°C. However, scaling each forcing by our estimates of transient efficacy (determined from either iRF or ERF), we obtain a best estimate for TCR of 1.8°C. This scaling simultaneously considers both forcing and ocean heat uptake efficacy. Other estimates of TCR which differ slightly due to choices of base period and uncertainty estimates and the aerosol forcing used, are similarly revised upward when using calculated efficacies.  We apply the same reasoning to estimates of ECS. Using an estimate4 of the rate of recent heat uptake Q = 0.65 ± 0.27 W m-2, we find, assuming all equilibrium efficacies are unity, a best estimate of ECS = 2.0°C, comparable to the previous result of 1.9°C.  However, as with TCR, accounting for differences in equilibrium forcing efficacy revises the estimate upward; our new best estimate (using efficacies derived from the iRF) is 2.9°C. If efficacies are instead calculated from the ERF, the best estimate of ECS is 3.0°C. As for TCR, alternate estimates of ECS are revised upward when efficacies are taken into account.

Soon, Connolly, and Connolly, 2015 [full] (0.44°C)
Nonetheless, let us ignore the negative relationship with greenhouse gas (GHG) radiative forcing, and assume the carbon dioxide (CO2) relationship is valid. If atmospheric carbon dioxide concentrations have risen by ~110 ppmv since 1881 (i.e., 290→400 ppmv), this would imply that carbon dioxide (CO2) is responsible for a warming of at most 0.0011 × 110 = 0.12°C over the 1881-2014 period, where 0.0011 is the slope of the line in Figure 29(a). We can use this relationship to calculate the so-called “climate sensitivity” to carbon dioxide, i.e., the temperature response to a doubling of atmospheric carbon dioxide. According to this model, if atmospheric carbon dioxide concentrations were to increase by ~400 ppmv, this would contribute to at most 0.0011 × 400 = 0.44°C warming. That is, the climate sensitivity to atmospheric carbon dioxide is at most 0.44°C.

Lewis and Curry, 2015 (1.33°C  transient, 1.64°C  equilibrium)
Energy budget estimates of equilibrium climate sensitivity (ECS) and transient climate response (TCR) are derived using the comprehensive 1750–2011 time series and the uncertainty ranges for forcing components provided in the Intergovernmental Panel on Climate Change Fifth Assessment Working Group I Report, along with its estimates of heat accumulation in the climate system. The resulting estimates are less dependent on global climate models and allow more realistically for forcing uncertainties than similar estimates based on forcings diagnosed from simulations by such models. Base and final periods are selected that have well matched volcanic activity and influence from internal variability. Using 1859–1882 for the base period and 1995–2011 for the final period, thus avoiding major volcanic activity, median estimates are derived for ECS of 1.64 K and for TCR of 1.33 K.

Johansson et al., 2015 (2.5°C  equilibrium)
A key uncertainty in projecting future climate change is the magnitude of equilibrium climate sensitivity (ECS), that is, the eventual increase in global annual average surface temperature in response to a doubling of atmospheric CO2 concentration. The lower bound of the likely range for ECS given in the IPCC Fifth Assessment Report was revised downwards to 1.5 °C, from 2 °C in its previous report, mainly as an effect of considering observations over the warming hiatus—the period of slowdown of global average temperature increase since the early 2000s. Here we analyse how estimates of ECS change as observations accumulate over time and estimate the contribution of potential causes to the hiatus. We find that including observations over the hiatus reduces the most likely value for ECS from 2.8 °C to 2.5 °C, but that the lower bound of the 90% range remains stable around 2 °C. We also find that the hiatus is primarily attributable to El Niño/Southern Oscillation-related variability and reduced solar forcing.

Kissin, 2015 (~0.6°C)
[A] doubling the CO2 concentration in the Earth’s atmosphere would lead to an increase of the surface temperature by about +0.5 to 0.7 °C, hardly an effect calling for immediate drastic changes in the planet’s energy policies. An increase in the absolute air humidity caused by doubling the CO2 concentration and the resulting decrease of the outgoing IR flux would produce a relatively small additional effect due to a strong overlap of IR spectral bands of CO2 and H2O, the two compounds primarily responsible for the greenhouse properties of the atmosphere.

Kimoto, 2015  [full] (~0.16°C)
The central dogma is critically evaluated in the anthropogenic global warming (AGW) theory of the IPCC, claiming the Planck response is 1.2K when CO2 is doubled. The first basis of it is one dimensional model studies with the fixed lapse rate assumption of 6.5K/km. It is failed from the lack of the parameter sensitivity analysis of the lapse rate for CO2 doubling. The second basis is the Planck response calculation by Cess in 1976 having a mathematical error. Therefore, the AGW theory is collapsed along with the canonical climate sensitivity of 3K utilizing the radiative forcing of 3.7W/m2 for CO2 doubling. The surface climate sensitivity is 0.14 – 0.17 K in this study with the surface radiative forcing of 1.1 W/m2.

Ollila, 2014 (~0.6°C equilibrium)
According to this study the commonly applied radiative forcing (RF) value of 3.7 Wm-2 for CO2 concentration of 560 ppm includes water feedback. The same value without water feedback is 2.16 Wm-2 which is 41.6 % smaller. Spectral analyses show that the contribution of CO2 in the greenhouse (GH) phenomenon is about 11 % and water’s strength in the present climate in comparison to CO2 is 15.2. The author has analyzed the value of the climate sensitivity (CS) and the climate sensitivity parameter (l) using three different calculation bases. These methods include energy balance calculations, infrared radiation absorption in the atmosphere, and the changes in outgoing longwave radiation at the top of the atmosphere. According to the analyzed results, the equilibrium CS (ECS) is at maximum 0.6 °C and the best estimate of l is 0.268 K/(Wm-2 ) without any feedback mechanisms.

Loehle, 2014  (1.1°C  transient, 2.0°C  equilibrium)
Estimated sensitivity is 1.093 °C (transient) and 1.99 °C (equilibrium).  Empirical study sensitivity estimates fall below those based on GCMs.

Skeie et al., 2014  (1.8°C  equilibrium)
Equilibrium climate sensitivity (ECS) is constrained based on observed near-surface temperature change, changes in ocean heat content (OHC) and detailed radiative forcing (RF) time series from pre-industrial times to 2010 for all main anthropogenic and natural forcing mechanism. The RF time series are linked to the observations of OHC and temperature change through an energy balance model (EBM) and a stochastic model, using a Bayesian approach to estimate the ECS and other unknown parameters from the data. For the net anthropogenic RF the posterior mean in 2010 is 2.0 Wm−2, with a 90% credible interval (C.I.) of 1.3 to 2.8 Wm−2, excluding present-day total aerosol effects (direct + indirect) stronger than −1.7 Wm−2. The posterior mean of the ECS is 1.8 °C, with 90% C.I. ranging from 0.9 to 3.2 °C, which is tighter than most previously published estimates.

Scafetta, 2013 (1.5°C)
A quasi 60-year natural oscillation simultaneously explains the 1850–1880, 1910–1940 and 1970–2000 warming periods, the 1880–1910 and 1940–1970 cooling periods and the post 2000 GST plateau. This hypothesis implies that about 50% of the ~ 0.5 °C global surface warming observed from 1970 to 2000 was due to natural oscillations of the climate system, not to anthropogenic forcing as modeled by the CMIP3 and CMIP5 GCMs. Consequently, the climate sensitivity to CO2 doubling should be reduced by half, for example from the 2.0–4.5 °C range (as claimed by the IPCC, 2007) to 1.0–2.3 °C with a likely median of ~ 1.5 °C instead of ~ 3.0 °C.

Asten, 2012 (1.1°C)
Climate sensitivity estimated from the latter is 1.1 ± 0.4 °C (66% confidence) compared with the IPCC central value of 3 °C. The post Eocene-Oligocene transition (33.4 Ma) value of 1.1 °C obtained here is lower than those published from Holocene and Pleistocene glaciation-related temperature data (800 Kya to present) but is of similar order to sensitivity estimates published from satellite observations of tropospheric and sea-surface temperature variations. The value of 1.1 °C is grossly different from estimates up to 9 °C published from paleo-temperature studies of Pliocene (3 to 4 Mya) age sediments. 

Lindzen and Choi, 2011 (0.7°C)
As a result, the climate sensitivity for a doubling of CO2 is estimated to be 0.7K (with the confidence interval 0.5K – 1.3K at 99% levels). This observational result shows that model sensitivities indicated by the IPCC AR4 are likely greater than the possibilities estimated from the observations.

Florides and Christodoulides, 2009 (~0.02°C)
A very recent development on the greenhouse phenomenon is a validated adiabatic model, based on laws of physics, forecasting a maximum temperature-increase of 0.01–0.03 °C for a value doubling the present concentration of atmospheric CO2. 

Gray, 2009 (~0.4°C)
CO2 increases without positive water vapor feedback could only have been responsible for about  0.1 – 0.2 °C of the 0.6-0.7°C global mean surface temperature warming that has been observed since the early 20th  century.  Assuming a doubling of CO2 by the late 21st  century (assuming no  positive water vapor feedback), we should likely expect to see no more than about 0.3-0.5°C global surface warming and certainly not the 2-5°C warming that has been projected by the GCMs [global circulation models].

Chylek et al., 2007 (~0.39°C)
Consequently, both increasing atmospheric concentration of greenhouse gases and decreasing loading of atmospheric aerosols are major contributors to the top-of atmosphere radiative forcing. We find that the climate sensitivity is reduced by at least a factor of 2 when direct and indirect effects of decreasing aerosols are included, compared to the case where the radiative forcing is ascribed only to increases in atmospheric concentrations of carbon dioxide. We find the empirical climate sensitivity to be between 0.29 and 0.48 K/Wm-2 when aerosol direct and indirect radiative forcing is included.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSometimes you have to wonder which are the biggest fraud: Germany’s claim that its cars are clean, or its claim of being a leader in climate protection. Both, it turns out, are very fake and even downright frauds.
While German Chancellor Angela Merkel and German activists like going around and scolding Donald Trump for his “irresponsible” stance on “greenhouse” gas emissions, it is coming to light that Germany’s climate posturing is indeed a total swindle.
While USA’s greenhouse gas emissions have declined impressively over the past decade, Germany’s have gone nowhere.
Flat for 9 years
And now Cleanenergywire.org here reports that Germany again this year (2017) will again fail to reduce its CO2 equivalent emissions for the 9th year running. Ironically one of the reasons cited for this year by Cleanenergy.org is “cold weather” (again).

Germany’s CO2 equivalent CO2 greenhouse gas emissions in metric tonnes since 2009. This year (2017) CO2 equivalent emissions are expected to be slightly over those seen in 2016. Data taken from German Ministry for environment.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The reality is that Trump and America has nothing to learn from the Green-preaching Germans, except on how to deceive and mislead the public. Cleanenergy.org writes that the higher energy demand is “triggered by economic growth and colder weather“.
Cleanenergy.org cites the AG Energiebilanzen (AGEB), which said in a press release: “From January until September, energy demand was 1.9 percent higher than in the same period last year.” And thus the AG Energiebilanzen expects “energy-related CO2 emissions will rise slightly in 2017”.
“A disaster”
Merkel’s glaring failure, however, did not prevent her from taking a swipe at Trump, the Handelsblatt reports. Unfortunately for Merkel there is no disguising Germany’s failure to meet its own imposed targets. The Environment Ministry says the 2017 emissions figures are “a disaster for Germany’s international reputation as a climate leader.”
CO2 emissions reduction is pie-in the sky, another hoax
Whatever gets decided in Bonn will be pure meaningless self-deception. The fact remains that China, India and the rest of the developing world are going to continue boosting their fossil fuel energy consumption and CO2 emissions are going to keep rising for quite awhile. The recent OPEC report makes that very clear.
Some advice for Merkel: Forget the CO2 reductions. Cutting Germany’s puny 2% global share would theoretically lead to a temperature reduction of 1 or 2 hundredths of a degree Celsius, meaning some 100 trillion euros per °C. It’s pure economic insanity. Take the idea and discard it quickly into the dustbin for good.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe abysmal track record of computer models in simulating climate trends has increasingly been highlighted in the scientific literature.  Recently published papers indicate that in some cases climate models actually get it right zero percent of the time (Luo et al., 2018; Hanna et al., 2018), or that hydrological models are off by a factor of 8 and 4 of 5 simulate trends opposite to real-world observations (Scanlon et al., 2018).  
Even the model-based assumption that positive water vapor feedback accompanies and amplifies CO2-forced temperature change is not supported by observations, with CO2 climate sensitivity overestimated by 200% (Ollila, 2018).  Simply put, climate modeling is increasingly being recognized in the scientific literature as lacking scientific merit.

ZERO Of The 126 Models Reproduce Recent Pacific Ocean Cooling
Luo et al., 2018
“Over the recent three decades sea surface temperate (SST) in the eastern equatorial Pacific has decreased, which helps reduce the rate of global warming. However, most CMIP5 model simulations with historical radiative forcing do not reproduce this Pacific La Niña-like cooling. Based on the assumption of “perfect” models, previous studies have suggested that errors in simulated internal climate variations and/or external radiative forcing may cause the discrepancy between the multi-model simulations and the observation…. Based on the total 126 realizations of the 38 CMIP5 model Historical simulations, the results show that none of the 126 model historical realizations reproduce the intensity of the observed eastern Pacific [1981-2010] cooling  and only one simulation produces a weak cooling (−0.007 °C per decade).”


ZERO Of 36 Models Capture Greenland’s Recent Blocking/Climatological Changes

Hanna et al., 2018
Recent changes in summer Greenland blocking captured by none of the CMIP5 models
“Recent studies note a signiﬁcant increase in high-pressure blocking over the Greenland region (Greenland Blocking Index, GBI) in summer since the 1990s. … We ﬁnd that the recent summer GBI increase lies well outside the range of modelled past reconstructions (Historical scenario) and future GBI projections (RCP4.5 and RCP8.5). The models consistently project a future decrease in GBI (linked to an increase in NAO), which highlights a likely key deﬁciency of current climate models if the recently-observed circulation changes continue to persist. Given well-established connections between atmospheric pressure over the Greenland region and air temperature and precipitation extremes downstream, e.g. over Northwest Europe, this brings into question the accuracy of simulated North Atlantic jet stream changes and resulting climatological anomalies […] as well as of future projections of GrIS mass balance produced using global and regional climate models.”

IPCC’s CO2 Climate Forcing Values 200% ‘Too Sensitive’, Water Vapor Feedback ‘Does Not Exist’
Ollila, 2018
“The temperature effects of the water and CO2 are based on spectral analysis calculations, which show that water is 11.8 times stronger a GH gas than CO2 in the present climate. … There are essential features in the long-term trends of temperature and TPW [total precipitable water], which are calculated and depicted as mean values 11 years running. The temperature has increased about 0.4°C since 1979 and has now paused at this level. The long-term trend of TPW [total precipitable water] effects shows that it has slightly decreased during the temperature-increasing period from 1979 to 2000. This means that the absolute water amount in the atmosphere does not follow the temperature increase, but is practically constant, reacting only very slightly to the long-term trends of temperature changes. The assumption that relative humidity is constant and that it amplifies the GH gas changes over the longer periods by doubling the warming effects finds no grounds based on the behavior of the TWP [total precipitable water] trend. The positive water feedback exists only during the short-term ENSO events (≤4 years).”
“The validity of the IPCC model can be tested against the observed temperature. It turns out that the IPCC-calculated temperature increase for 2016 is 1.27°C, which is 49 per cent higher than the observed 0.85°C. This validity test means that the IPCC climate forcing model using the radiative forcing value of CO2 is too sensitive for CO2 increase, and the CS [climate sensitivity] parameter, including the positive water feedback doubling the GH gas effects, does not exist.”
“The CO2 emissions from 2000 onward represent about one-third of the total emissions since 1750, but the temperature has not increased, and it has paused at the present level. This is worthy proof that the IPCC’s climate model has overestimated human-induced causes and has probably underestimated natural causes like the sun’s activity changes, considering the historical temperatures during the past 2000 years.”
“The RF [radiative forcing] value for the CO2 concentration of 560 ppm is 2.16 Wm−2 according to equation (3), which is 42 per cent smaller than 3.7 Wm−2 used by the IPCC. The same study of Ollila (2014) shows that the CS [climate sensitivity] parameter λ is 0.27 K/(Wm−2), which means that there is no water feedback. Using this λ value, equation (3) gives a TCS [transient climate sensitivity] value of 0.6°C only. This same result is also reported by Harde (2014) using the spectral analysis method. …There are both theoretical- and measurement-based studies showing results that can be explained only by the fact that there is no positive water feedback. This result reduces the CS [climate sensitivity] by 50 per cent. Some research studies show that the RF [radiative forcing] value of carbon dioxide is considerably smaller than the commonly used RF value, according to the equation of Myhre et al. (1998). Because of these two causes, the critical studies show a TCS [transient climate sensitivity] of about 0.6°C instead of 1.9°C by the IPCC, a 200 per cent difference.”

Observations Have ‘Factor Of Two’ Less Warming Than Modeled Projections
Christy et al., 2018
“All datasets produce high correlations of anomalies versus independent observations from radiosondes (balloons), but differ somewhat in the metric of most interest, the linear trend beginning in 1979. The trend is an indicator of the response of the climate system to rising greenhouse gas concentrations and other forcings, and so is critical to understanding the climate. The satellite results indicate a range of near-global (+0.07 to +0.13°C decade−1) […] trends (1979–2016), and suggestions are presented to account for these differences. We show evidence that MSUs on National Oceanic and Atmospheric Administration’s satellites (NOAA-12 and −14, 1990–2001+) contain spurious warming, especially noticeable in three of the four satellite datasets.”
“Comparisons with radiosonde datasets independently adjusted for inhomogeneities and Reanalyses suggest the actual tropical (20°S-20°N) trend is +0.10 ± 0.03°C decade−1. This tropical result is over a factor of two less than the trend projected from the average of the IPCC climate model simulations for this same period (+0.27°C decade−1). … Because the model trends are on average highly significantly more positive and with a pattern in which their warmest feature appears in the latent-heat release region of the atmosphere, we would hypothesize that a misrepresentation of the basic model physics of the tropical hydrologic cycle (i.e. water vapour, precipitation physics and cloud feedbacks) is a likely candidate.”

Climate Models Are Conceptual And We Don’t Understand The Mechanisms 
Collins et al., 2018
“Here there is a dynamical gap in our understanding. While we have conceptual models of how weather systems form and can predict their evolution over days to weeks, we do not have theories that can adequately explain the reasons for an extreme cold or warm, or wet or dry, winter at continental scales. More importantly, we do not have the ability to credibly predict such states. Likewise, we can build and run complex models of the Earth system, but we do not have adequate enough understanding of the processes and mechanisms to be able to quantitatively evaluate the predictions and projections they produce, or to understand why different models give different answers. … The global warming ‘hiatus’ provides an example of a climate event potentially related to inter-basin teleconnections. While decadal climate variations are expected, the magnitude of the recent event was unforeseen. A decadal period of intensified trade winds in the Pacific and cooler sea surface temperatures (SSTs) has been identified as a leading candidate mechanism for the global slowdown in warming.”


Hydrological Modeling Off By A Factor Of 8, With 4 Of 5 M0dels Yielding Opposite Trends Vs. Observations
Scanlon et al., 2018
“The models underestimate the large decadal (2002–2014) trends in water storage relative to GRACE satellites, both decreasing trends related to human intervention and climate and increasing trends related primarily to climate variations. The poor agreement between models and GRACE underscores the challenges remaining for global models to capture human or climate impacts on global water storage trends. … Increasing TWSA [total water storage anomalies] trends are found primarily in nonirrigated basins, mostly in humid regions, and may be related to climate variations. Models also underestimate median GRACE increasing trends (1.6–2.1 km3/y) by up to a factor of ∼8 in GHWRMs [global hydrological and water resource models] (0.3–0.6 km3/y). Underestimation of GRACE-derived TWSA increasing trends is much greater for LSMs [global land surface models], with four of the five LSMs [global land surface models] yielding opposite trends (i.e., median negative rather than positive trends).”
“Increasing GRACE trends are also found in surrounding basins, with most models yielding negative trends. Models greatly underestimate the increasing trends in Africa, particularly in southern Africa. .. TWSA trends from GRACE in northeast Asia are generally increasing, but many models show decreasing trends, particularly in the Yenisei.  … Subtracting the modeled human intervention contribution from the total land water storage contribution from GRACE results in an estimated climate-driven contribution of −0.44 to −0.38 mm/y. Therefore, the magnitude of the estimated climate contribution to GMSL [global mean sea level] is twice that of the human contribution and opposite in sign. While many previous studies emphasize the large contribution of human intervention to GMSL [global mean sea level], it has been more than counteracted by climate-driven storage increase on land over the past decade.”
“GRACE-positive TWSA trends (71 km3/y) contribute negatively (−0.2 mm/y) to GMSL, slowing the rate of rise of GMSL, whereas models contribute positively to GMSL, increasing the rate of rise of GMSL“

Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently I wrote about 7 signs showing that the earth has been cooling and likely will continue to cool.
To back this up, Kenneth Richards commented in a reply that this year has seen 7 new peer-reviewed papers that show us that the earth’s surface temperature at the poles and elsewhere has been cooling since about a decade. What’s worrisome is that the southern hemisphere surface is mostly ocean.
Eastern North Atlantic cooling since 2010
The first paper is Gladyshev et al., 2017  which states in its abstract that there’s been “a sharp and stable freshening and cooling of SPMWs [Subpolar Mode Water] in the eastern part of the North Atlantic since 2010 . In the years 2010–2016, the mean temperature of the SPMW [Subpolar Mode Water] core in the Rockall Trough dropped by -0.73°C (-0.12°C/yr); in the Iceland Basin it dropped by -2.12°C (-0.35°C/yr), and salinity decreased by 0.12 psu (0.02 psu/yr) and 0.23 psu (0.04 psu/yr), respectively.”
Subpolar North Atlantic trend reversal in 2005
In another paper, Piecuch et al., 2017,  the authors notes that subpolar North Atlantic (SPNA) is subject to strong decadal variability and found that in 2004–2005 the SPNA decadal upper ocean and sea-surface temperature trends reversed from warming during 1994–2004 to cooling over 2005–2015.
Southern Ocean now cooling
On the other side of the planet at the South Pole the story is pretty much the same. A study this year by Kusahara et al., 2017 showed that in contrast to a strong decrease in Arctic sea ice extent, the overall Antarctic sea ice extent has modestly increased since 1979. The paper’s abstract adds:
Concomitant with this positive trend in Antarctic sea ice, sea surface temperatures (SSTs) over the Southern Ocean south of approximately 45°S have cooled over this period.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Remaining at the South Pole, a new paper by Turney et al., 2017 here states that the Southern Ocean, which occupies a massive 14% of the world’s surface, plays a fundamental role in ocean and atmosphere circulation — and thus climate — and found that it has produced a cooling trend since 1979.
Cooling since 1999
Also Oliva et al., 2017 points out that a recent analysis by Turner et al., 2016 has shown that “the regionally stacked temperature record for the last three decades has shifted from a warming trend of 0.32 °C/decade during 1979–1997 to a cooling trend of − 0.47 °C/decade during 1999–2014“. Oliva et al tell us that “this recent cooling has already impacted the cryosphere in the northern AP [Antarctic Peninsula], including slow-down of glacier recession, a shift to surface mass gains of the peripheral glacier and layer of permafrost in northern AP islands“.
Fernandoy et al., 2017 here also points out:
The firn stable isotope composition reveals that the near–surface temperature at the Antarctic Peninsula shows a decreasing trend (−0.33 °C y−1) between 2008 and 2014.”
“No evidence” of snow decline
Finally, moving on land to the Tibetan Plateau, a recent paper appearing in Nature by Wang et al. 2017 shows there’s been “no evidence of widespread decline of snow cover on the Tibetan Plateau over 2000–2015“.
That’s 7 fresh papers telling us that large, important areas of the earth’s surface have stopped warming and begun cooling. Time is running out for the global warming hoaxsters.
Note: Recently desparate climate warming trolls have been appearing in force (trying to keep their sham alive). Serious comments are welcome, but trolling comments will be deleted.
Share this...FacebookTwitter "
"Greta Thunberg’s father has opened up about how activism helped his daughter out of depression but still worries about how she will deal with the impact of her international fame. Speaking to the BBC to mark his daughter’s guest-editing slot on the Today programme, Svante Thunberg revealed he thought it was a “bad idea” for Greta to stage the school strike that catapulted her into the public eye. The programme also featured a discussion between Greta Thunberg and the veteran naturalist Sir David Attenborough, in which the latter praises the teenager for raising awareness of the climate crisis. She had “achieved things that many of us who have been working on it for 20-odd years have failed to achieve – that is you have aroused the world”, said Sir David, adding that she was the main reason climate was discussed during the British election campaign. Svante Thunberg reveals how activism had changed the outlook of the teenager, who suffered from depression for “three or four years” before she began her school strike protest outside the Swedish parliament. She was now “very happy”, he said. “She stopped talking ... she stopped going to school,” he said of her illness, adding that it was the the “ultimate nightmare for a parent” when Greta began refusing to eat. Svante Thunberg, an actor, said he and his wife, the opera singer Malena Ernman, scaled back their professional lives to spend more time with Greta in order to help her overcome her depression. He became vegan and his wife stopped travelling to concerts by plane. He said Greta became energised about green issues as the family began talking more about environmental issues. He accompanied her on her tour of the United States and visit to the Madrid climate crisis this year “I did all these things, I knew they were the right thing to do ... but I didn’t do it to save the climate, I did it to save my child,” Svante Thunberg said. “I have two daughters and to be honest they are all that matters to me. I just want them to be happy. “You think she’s not ordinary now because she’s special, and she’s very famous, and all these things. But to me she’s now an ordinary child – she can do all the things like other people can,” he said. “She dances around, she laughs a lot, we have a lot of fun – and she’s in a very good place.” He was concerned about the negative comments his daughter attracted in the media and online and “all the hate that that generates”. But his daughter dealt with it “incredibly well”. “Quite frankly, I don’t know how she does it, but she laughs most of the time. She finds it hilarious.”"
"Seafood consumption is both a love and a necessity for hundreds of millions of people worldwide. And its supply is a key part of maintaining food security for the whole planet. But during a time of rapid population growth and increasing demand, stocks of wild fish and invertebrates (such as mussels and prawns) are declining.  The problem is that policies and plans designed to make sure there are enough fish and invertebrates almost exclusively target fishing activity. But we also need to protect the critical habitats that are essential for the sustainability of these stocks and fisheries. Most species that are fished require more than a single habitat to live and thrive. Atlantic cod (Gadus morhua), for example, spends its adult life shoaling in deep water where it lives, feeds and spawns. But juveniles require more stable habitat such as seagrass meadows. So, if we want to manage fish and invertebrate stocks for sustainability reasons, it is essential to protect the supporting habitats of targeted species.  Seagrass meadows are just one of these critical habitats. These large areas of marine flowering plants are abundant in shallow seas on all continents except Antarctica. They support biodiversity and in turn the productivity of the worlds fisheries. As seagrass meadows occur from the intertidal – the area exposed by the daily ebb of the tide – to about a depth of 60 metres in clear waters, they are an easily exploitable fishing habitat.  Though it is clear that seagrasses are a vital part of ocean ecosystems, until now, there has been no information on the role that meadows play in supporting the productivity of world fisheries. But we have now published the first quantitative global evidence on the significant roles that seagrasses play. Nursery grounds in seagrass meadows are a safer, less exposed, environment for eggs to be laid and young animals to find food and protection from predators as they grow. The very fact that they are there means that there are places for commercial fish stocks such as tiger prawns, conch, Atlantic cod and white spotted spinefoot to be caught by global fisheries. In fact, a fifth of the world’s most landed fish – including Atlantic cod and Walleye pollock – benefit from the persistence of extensive seagrass meadows. But it is not just large-scale fishing industries that benefit from the presence of seagrass meadows. As they are an easily accessible fishing ground, small scale artisanal and subsistence fisheries around the world also use them.  Seagrass is also essential for communities that take part in gleaning – fishing for invertebrates such as sea cucumbers in water that is shallow enough to walk in. This is often done by women and children, and provides a source of essential protein and income for some of the most vulnerable people in tropical coastal communities.  It is a common and increasingly visible activity, but it is not usually included in fishery statistics and rarely considered in resource management strategies. And the benefits of seagrasses don’t only lie in the meadows themselves, their presence supports nearby fishing areas, as well as deep water habitats. They do this by creating expansive areas rich in fauna, from which there are vast quantities of living material, organic matter and associated animal biomass that supports other fisheries. Seagrasses also promote the health of connected habitats (like coral reefs), and have the capacity to support whole food webs in deep sea fisheries. The coastal distribution of seagrass means that it is vulnerable to a multitude of threats from both land and sea. These include land runoff, coastal development, boat damage and trawling. On a global scale, seagrass is rapidly declining, and when seagrass is lost associated fisheries and their stocks are likely to become compromised with profound and negative economic consequences.  The importance of seagrass meadows for fisheries productivity and hence food security is not reflected by the policies currently in place. These are urgently needed to continue enjoying the benefits that healthy and productive seagrass meadows provide.  Fisheries management must be broadened from just targeting fishing activity to also targeting the habitats on which fisheries depend. Awareness of the role of seagrass in global fisheries production – and, so, food security – must be central to any policy, and major manageable threats to seagrass, such as declining water quality, must be dealt with.  Seagrass can be a resilient and supportive habitat – but only if we take action to continue to enjoy the benefits it provides."
"
Share this...FacebookTwitterCO2 emissions exert no detectable effect on Arctic, Antarctic temperatures. The Arctic region is no warmer in recent decades than it was some 80 years ago, or before CO2 emissions began rising significantly.

Graph Source: Mikkelsen et al., 2018
According to the IPCC, the Arctic and Antarctic regions warm more than the rest of the globe — a phenomenon branded as polar amplification.
Further, it is conclusively stated (with “high confidence”) that this enhanced polar warming occurs largely in response to increases in atmospheric CO2 concentration.

Image Source: IPCC AR5 
A 2015 Scientific Paper Affirms CO2 Forcing Is ‘Weak’ To Negligible At The Poles
In late 2015, four climate scientists published a groundbreaking paper (Schmithüsen et al.,[2015]) in the highly-regarded Geophysical Research Letters scholarly journal.
Although obligatorily insisting their research did not undermine the main tenets of anthropogenic global warming (AGW) theory at one turn, the authors nonetheless landed a devastating blow to the conceptualization of a CO2-amplified polar climate – and thus to the narrative that says the ice sheets and sea ice are melting primarily due to increases in anthropogenic CO2 emissions.
Schmithüsen and colleagues reached the conclusion that CO2-forcing is “rather small” and even “weak“ at the poles.  They found the planet’s tiniest warming signal from CO2 occurs for central Antarctica; they characterized the CO2-forcing for the Arctic region as “comparatively weak”.    For example, quadrupling CO2 concentrations over the Antarctic Plateau is poised to yield a net radiative forcing value of just 1 W m-2.
The authors even assert that increasing CO2 concentrations causes atmospheric cooling in some areas above the Antarctic continent.  They characterize this as a “negative greenhouse effect” due to the “increased long-wave energy loss to space, which cools the Earth-atmosphere system”.
Key points from the paper are highlighted below.

Schmithüsen et al., (2015)
 

 

Warming From Increased CO2 Is ‘Comparatively Weak‘ For The Arctic Region


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





‘Polar Amplification’ From Increased CO2 Not Detectable For Antarctica
Consistent with the conceptualization that “polar amplification” from increasing human CO2 emissions has gone unrealized, the temperature records for the Antarctic continent do not suggest warming has occurred in recent decades.

Graph Sources: Climate4you, Miles et al., 2013, Turner et al., 2016
Increasing CO2 Emissions Has Exerted No Detectable Effect On The Arctic Region
Consistent with the conceptualization that “polar amplification” from increasing CO2 has gone unrealized, the temperature records for the Arctic region also do not suggest a discernible net warming has occurred in response to the rapid increase in anthropogenic CO2 emissions since the mid-1940s.  The Arctic region is no warmer in recent decades than it was ~80 years ago, or before CO2 emissions began rising significantly.  This would support the conclusion that CO2 emissions increases have exerted no detectable effect on the Arctic region’s temperatures.

Graph Source: Hanhijärvi et al., 2013

Graph Source:  Hanna et al., 2011
‘Weak’ To Negligible CO2 Forcing At The Poles Lands A Devastating Blow To AGW Alarm
If the warming effect from increasing CO2 concentrations is only “weak” to negligible for both the Antarctic and Arctic regions, then the justification to endorse the most alarming tenets of the anthropogenic global warming conceptualization may be thoroughly undermined.
For example:
1. The decline in Arctic sea ice since the late 1970s may no longer be predominantly attributed to CO2-induced Arctic warming.
2. Mass ice losses for both the Antarctic and Greenland ice sheets in the modern era may no longer be predominantly attributed to CO2-induced polar warming.
3. The net ice melt contribution to sea level rise from the Antarctic and Greenland ice sheets in the modern era may no longer be predominantly attributed to CO2-induced polar warming.
4. The post-1980s temperature warming for the Arctic region (that has significantly affected the overall global warming trend) may no longer be predominantly attributed to CO2-induced Arctic warming.
In sum, affirming the Schmithüsen et al.,(2015) analysis leaves little room for continued insistence that rising CO2 emissions are a profound and existential planetary threat.

Update: A just-published paper, Flanner et al., 2018, cites the negative CO2 greenhouse effect conceptualization introduced by Schmithüsen et al.,(2015).  At no time do the authors challenge the relatively quite weak radiative forcing values (~1 W m-2) for the CO2 greenhouse effect in the polar regions as depicted in the colorized graph above.  Instead of challenging these very small CO2-forcing values for polar regions, the authors only challenge the less consequential concept of whether or not a cooling would occur at the poles in response to increases in greenhouse gases (GHGs) in general, and not CO2 in particular.   It would appear the weak CO2 forcing (W m-2) values for the polar regions as determined by Schmithüsen et al., (2015) are accepted by mainstream climate science.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterJapanese skeptic blogger Kirye posted here at Twitter the latest news on Arctic sea ice volume, which earlier this spring took a sudden and unexpected jump upwards – adding some 2 trillion cubic meters.
What follows is the latest chart from the Danish Meteorological Institute (DMI):

Source: DMI.
As the chart shows, Arctic sea ice volume hasn’t really budged that much since it peaked back inApril.
Less than 4% below the mean
And when one looks at the chart closely, it is seen that the mean Arctic se ice volume for this time of year is just under 25,000 cubic kilometers. Currently we see that volume is the same as it was in 2014, at some 24,000 cubic kilometers.
The deviation from the mean is less than 1000 cubic kilometers, i.e. less than 4%. That means sea ice volume is well within the range of natural variability.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Predictions of an ice-free Arctic go back decades
Two days ago the Deplorable Science Blog here reminded us: “Sixty years ago, the New York Times predicted ships would be sailing over the North Pole ‘within the lifetime of our children.’”
The false prophets
In the same article Steve Goddard brings up a 2008 article by the AP’s Seth Borenstein, who quoted climate alarmist, Massachusetts Senator Ed Markey, who then called James Hansen a “climate prophet”.
Both Al Gore and former NASA GISS director James Hansen warned –“echoing work by other scientists” — that the Arctic would be ice-free in the summer by now!
So, add two more names to False Prophets Hall of Fame.
Skeptics right
Ironically the real “prophet” turns out to be Oklahoma Senator, global warming skeptic Sen. James Inhofe, who in 2008 dismissed all the predictions as media doom, and said that Americans weren’t buying it.
Ten years later Senator Inhofe turns out to be right.
Share this...FacebookTwitter "
nan
"Thousands of people fled to the lake and ocean in Mallacoota, as bushfires hit the Gippsland town on Tuesday. The out-of-control fire reached the town in the morning and about 4,000 people fled to the coastline, with Country Fire Authority members working to protect them. The town had not been told to evacuate on Sunday when the rest of East Gippsland was, and authorities decided it was too dangerous to move them on Monday. People reported hearing gas bottles explode as the fire front reached the town, and the sound of sirens telling people to get in the water. By 1.30pm the fire had reached the water’s edge. A local man, Graham, told ABC Gippsland he could see fire in the centre of the town, and 20m high flames on the outskirts where he believed homes were alight. Fire appears at the #Mallacoota water front. Via Snapchat #VictoriaFires pic.twitter.com/mu521uwVnG “We saw a big burst of very big flames in Shady Gully,” he said. “As I speak to you I’m looking across Coull’s Inlet and there are big flames … and they would be impacting houses. That’s not good at all.” People in Mallacoota posted in community social media groups estimates of about 20 houses lost, with the school, bowling club and golf club also hit. Guardian Australia has not been able to confirm these reports. Hundreds more evacuees sheltered in the community centre. “There are a lot of people at the waterfront jetty, in the lake, on the sand spit between the lake and the ocean, and there are people on a sandbar, and some on boats,” Charles Livingstone told Guardian Australia from the community centre. He said there were at least 350 people in the community centre, many with children and pets. He, his wife and their 18-month-old baby were at the jetty on Monday night but moved to the community centre to avoid the heavy smoke. “The CFA advised yesterday they would protect the waterfront jetty and the hard stands that go along the lower lake here, just in front of where we are. They were saying we’ll protect you down there if the worst comes to worst,” he said. “I’m sure the CFA will do what they said, but the relief centre to us seemed like to best option. They’re pretty busy and we haven’t had an update in a while.” 10:30am update from Dad at the wharf in Mallacoota - “fire front not far away” #Mallacoota #bushfirecrisis pic.twitter.com/MvgeiZqujM Livingstone said he was barely thinking about their holiday house to the south of town. “We’ll be happy to get out of here ourselves,” he said. “It’s mayhem out there, it’s armageddon … The other issue is how the hell we’re all going to get out of here – there’s one road in and one road out.” The fire, which hit the town on Tuesday, started on Sunday in Wingan. Livingstone said there had been “confusion”, with roads closing and reopening, and so he and his family had not left. The Victorian premier, Daniel Andrews, said the decision had been made on Monday afternoon that the safest option for people in Mallacoota was to stay there. “At the community level and regional level [authorities] had to work through what their options were and undertake a risk assessment of that,” Andrews said. “We decided it would be unsafe to move them back along the Princes Highway.” Livingstone said the temperature dropped from above 40C to about 20C later in the morning, and people in the water were getting cold. Community radio in #Mallacoota: ""The power is cut. We are isolated. It's a holocaust, basically. Some have been sent to the hall, some to the lake, others have to stay in their homes. There's a lot of thunder. The fires are creating their own weather."" While speaking to Guardian Australia Livingstone said he heard an extraordinarily loud boom outside – Mallacoota does not have gas lines and many people have gas bottles that are likely exploding in the heat. Matt Manning heard it too. He spoke to Guardian Australia from his boat in the lake at the back of Goodwin Sands, about 3km from the centre of town. “It was a big explosion but we don’t know what it was,” he said. “Up until an hour or so ago it was pitch black, you couldn’t see 10 feet in front of you. It was just insane. [Now] it looks like it’s 8.30 in the evening.” Manning, who has been coming to Mallacoota for 20 years, said there were about 30 boats out where he was, most of them locals. He and his wife, their two friends, and his dog had been camping at the foreshore camp park. He packed up the boat on Monday morning, and late last night they all moved to the waterfront to sit out the fire. At about 3am they decided to get in the boat. “We’re pretty safe here, hopefully we don’t get any hot embers,” he said. There’s a lot of debris and ash but so far there’s no hot embers. Winds are about 30-40 knots coming from the south.” Francesca Winterson is in a building on the main street of #Mallacoota and describes the wind, darkness and falling embers as fires burn about 500 metres away.She says it's too late to leave and fire crews are on hand to offer as much protection as they can. pic.twitter.com/6Tjfb4nyUR “We are completely isolated,” community radio broadcaster Francesca Winterson told ABC Breakfast on Tuesday morning. “We’ve been broadcasting for 48 hours without a break and we’re all very tired,” she said. “Now we are here in the station and I’m just watching my town burn.” This picture just in from family boarding boat in #Mallacoota #MallacootaFires approx time of photo 9:45am pic.twitter.com/WJEQScDp9f The township was under one of eight emergency warnings in the East Gippsland area. More than 200,000 hectares have burned, including about 80,000 just in the last 24 hours. Four people were unaccounted for in Victoria on Tuesday morning, and three more in NSW. Fire authorities said there have been “significant losses” of property, but it was too early to confirm numbers. Sister in a BRIGHT ORANGE work suit blending in with the #Mallacoota sky pic.twitter.com/SfK93GhbUU"
"
Share this...FacebookTwitterCO2 as the major climate driver looks shakier than ever.
Scientists confirm clouds and their changes have a huge impact on the earth’s surface temperature…

Anna Possner’s research shows clouds and their changes have a real impact on earth’s surface temperature, a Goethe University press release confirms. Photo source: annapossner.com, Carnegie Science.
According to Germany’s Goethe University, Carnegie Institution for Science climatologist Anna Possner’s research on layered clouds in the lower atmosphere shows that clouds “act as a semi-transparent parasol” and “reflect a significant portion of incoming sunlight” and “have a cooling effect on Earth’s surface.” …and that cloud changes “can result in significant changes to Earth’s surface temperature”.

Clouds like a semi-transparent parasol
Released at: Wed, 09 May 2018 13:40:00 +0200 (024)
FRANKFURT. Following the Paris Climate Agreement, Germany and France created the program “Make Our Planet Great Again,“ to promote climate change research. One of 13 researchers selected by an expert jury of the German Academic Exchange Service (DAAD) is coming from the USA to the Goethe University in a few months.
The climate change researcher Dr. Anna Possner is leaving the renowned Carnegie Institution for Science in Stanford and will join the Department for Atmospheric and Environmental Sciences at the Goethe University. Thanks to a one million euro grant, she will start her own research group in Frankfurt. This group will cooperate with the Frankfurt Institute for Advanced Studies (FIAS), where it will also be located.
Anna Possner’s research focuses on layered clouds in the lowest kilometres of the atmosphere, which act as a semi-transparent parasol for Earth’s surface. They reflect a significant portion of incoming sunlight, but only marginally affect Earth’s heat emission. They thus have a cooling effect on Earth’s surface. Any sheet of low-level cloud may span hundreds of kilometres and all together they span around one fifth of Earth’s oceans. Changes in their areal extent or reflective properties can result in significant changes to Earth’s surface temperature.
In some regions of the globe, the mid-latitudes and the Arctic, these clouds consist not only of water drops, but may contain a mixture of ice particles and water drops. The proportion of water drops to ice crystals affects the clouds’ reflective properties. “While we have hypotheses about how the radiative properties may be affected within a single cloud,” Anna Possner explains, “we are limited in our understanding of how the presence of ice crystals impacts the areal coverage and reflective properties on the scale of an entire cloud field.” She will use satellite retrievals and sophisticated numerical models to help answer this question.
Since completing her doctoral dissertation at the ETH Zurich, Anna Possner, who was born in Jena, has studied the impact of particles on the reflective properties of clouds. During this time she focused in particular on low-lying clouds over the oceans, where she quantified and evaluated the impact of ship emissions on clouds. During her postdoc years at the ETH Zurich and the Carnegie Institution for Science in Stanford, she extended her analyses to include mixed-phase clouds.
The German-French program “Make Our Planet Great Again“ seeks to support the creation of solid facts as a basis for political decisions in the fields “climate change”, “earth system research” and “energy transformation”. Of the 13 scientists selected for Germany, seven are in the US, two were most recently working in Great Britain and one each is in Switzerland, Canada, South Korea and Australia. They were selected during a two-stage process out of approximately 300 applications.
Further Information: Prof. Joachim Curtius, Department for Atmospheric and Environmental Sciences, Faculty for Geosciences / Geography, Riedberg Campus, Tel.: +49 (0) 798-42058, curtius@iau.uni-frankfurt.de.
Share this...FacebookTwitter "
"Talking to a fourth-generation grazier west of Townsville a few years ago, Prof Stephen Williams says he “made the mistake” of mentioning climate change. “He said it was bullshit, but we kept talking,” the James Cook University ecologist says. Later the grazier admitted the property was much more difficult to manage than in his great-grandfather’s time because “the weather has gone to shit”. That chat with the grazier, Williams says, is one example of a “social barrier” that gets in the way of Australians taking action on climate change. “One side of his personality denied climate change was real, yet he fully recognised the climate had changed.” Analysis from Australia’s now defunded National Climate Change Adaptation Research Facility has ranked research priorities based on their urgency, cost-effectiveness and technical feasibility. The conclusion is that research into social barriers should be given the highest priority to help save the country’s ecosystems from climate change impacts. Williams was the co-ordinator of the facility’s natural ecosystem network, covering climate impacts on land, in freshwater systems and in the marine environment. The analysis, published in the journal Global Change Biology, drew on eight years of consultations with about 2,000 scientists and stakeholders, at more than 50 workshops. “They all recognised that the only way we were going to get anything important done was to get the whole of society on board,” says Williams, the lead author of the analysis. “Social barriers came out as the most highly ranked question to answer. There are all sorts of social barriers to us adapting to climate change. “Some are purely the psychological make-up of people that don’t want to acknowledge a problem, some of it is a government more interested in elections every three years, and some is the vested interests of industry that undoubtedly confuse the issues. They are all barriers. “We are a group of physical scientists and biologists,” Williams says, “but we came up with this reasoning that we need to do some social research. “We’ve had highest temperatures ever, and the longest and most intense heatwaves geographically, and we’ve had floods happening at the same time as the biggest drought, and have seen rainforests burning. “What’s the social resistance against something that is so in your face, yet people still want to stick their heads in the sand and deny it? It is the social barriers that are stopping us and it is incredibly frustrating.” Adaptation is going to be critical to Australia. Mitigation has to be global, but adaptation can be local Climate adaptation looks at ways to reduce the impacts of climate change that are unavoidable, and is distinct from mitigation, which focuses on reducing emissions. “Climate change mitigation is about avoiding the unmanageable,” says Dr Alistair Hobday, a research director at CSIRO Oceans and Atmosphere and a co-author of the analysis. “Adaptation is about managing the unavoidable. Adaptation is going to be critical to Australia. Mitigation has to be global, but adaptation can be local.” Hobday says that as well as understanding why some might see climate change as a lower priority, another barrier is public resistance to translocating species that are not able to adapt or move as temperatures and rainfall change. “These are situations where people resist doing something despite the evidence that the animal’s world is changing,” he says. The only example of a project like that in Australia is a pilot study that has moved 35 captive-bred western swamp turtles to new sites in south-west Western Australia. Prof Lesley Hughes, of Macquarie University, and also an author on the analysis, says there has been resistance to “interventions” like translocation, but this needs to change. Despite 30 years of research looking at how climate change would affect habitats, she says, very little work has been done to protect species. Other priorities identified by the analysis are to make environmental and planning laws more proactive, and to improve understanding of the effects of extreme events on ecosystems. NCCARF was launched in 2008 with $47m of federal government funding over five years, and then a further $8.8m to 2017. A final $300,000 of funding ran out in June 2018, and the facility is now vastly scaled-down and operating through Griffith University. Williams says it is “frustrating” that NCCARF has not received continued funding when it has built a foundation of research that could be exploited. “We had got to the stage where in the next five or 10 years we were really going to start to achieve things, and that’s when it got chopped.”"
"In the late 1960s a patch of land to the east of Amsterdam was reclaimed from the sea for industry. Following the 1973 oil crisis this plan was abandoned and flocks of geese moved in. As the geese grazed the land they created changing mosaics of vegetation and a rich and unique environment spontaneously developed. Dutch ecologists saw this and were inspired by the potential for animal grazing to restore a thriving “self-willed” ecosystem. They proposed a unique experiment to recreate the mix of large herbivores that inhabited the region after the last ice age, around 8,000 years ago, and to let natural forces, rather than human management, decide what environment they would create. The Oostvaardersplassen nature reserve, or OVP, was established on the site in 1986. Founder herds of konick horses, heck cattle and deer were introduced and the reserve became an iconic example of “rewilding”. Recently, however, this 55 square kilometre reserve became front-page news in the Netherlands after images of starving animals spread outrage across social media. Plans to cull 3,000 weakened animals led to protests and prompted activists to throw bales of hay over the fence. Rangers and ecologists associated with the project have even faced death threats. Yet hungry animals at winter’s end is a natural situation. Allowing nature to take its course means that animal numbers will fluctuate, and following a series of mild winters the reserves’s populations of konik horses, heck cattle and deer were unusually high. In a hard winter, like the most recent, the grass stops growing and many animals will starve and die.  Nonetheless, some have suggested the experiment has failed, and that OVP simply creates animal suffering. But the starving animals and public outcry is a failure of politics rather than a failure of rewilding itself. All rewilding projects operate on the principle that herbivore populations should fluctuate as nature intended. This is important because it leads to varied vegetation with lots of different species. As a “first generation” rewilding experiment, the OVP left its animals to fend entirely for themselves.  Elsewhere in the Netherlands, ecologists quickly modified the concept into something more inclusive and entrepreneurial that takes animal welfare into account. This is why second generation rewilding projects, such as those at Gelderse Poort, the Border Meuse and Kempen Broek are less controversial and hence less well known.  These reserves are developing the category of “kept wild”, where herds behave like wild animals and perform the same ecological role, but are also managed by humans to some degree. In these projects, the condition of each animal is assessed at the end of each winter. Those that would suffer and die from starvation or predation if left to nature are fed until they regain their condition in the spring. At this point they are removed to new rewilded areas or harvested and sold as “wild meat”. This approach has created flourishing rewilding areas where visitors can feel the tingle of unease that comes from being in the proximity of large free-living animals. In the Netherlands there are now dozens of such areas along the coast and rivers. Social enterprises have emerged to manage the wilded herds on these second generation projects. Some are developing breeds better suited to living in rewilded landscapes and with public access. Traits include a more docile temperament, smaller udders to reduce injury, or larger horns for defence against wolves and feral dogs.  For rewilding purists, there is a trade-off: carcasses are removed, even though they are key to restoring natural scavengers – anything from vultures to carrion beetles – and the ecological processes they encourage. However, contemporary attitudes to processes of death, decay and decomposition are mostly negative. In places with lots of visitors, people find rewilding principles easier to accept if there are no dead animals around. In April this year, a local government committee advised that the number of large herbivores on the OVP should be “reset” and actively managed at sustainable levels. But, if adopted, the park’s natural cycle of grazing-induced ebbs and flows in different species at different times would be constrained, and the experimental principle lost. The OVP was previously part of a progressive vision – the OostvaardersWold – to create a natural corridor linking it with the Veluwe, a national park to the south. This would have created the conditions for animals to move with the seasons and for predators such as wolves to establish themselves. Although the Dutch state had acquired most of the land in the corridor, the policy was abandoned in 2010 following a change of government and a new minister who thought it a waste to convert good agricultural land to nature. The corridor idea should be reignited. Politicians are wary of trying things again, but ideas are emerging to present a new strategy that integrates “kept wild” approaches. This is important because in the longer term even an expanded system may not avert population booms and the starvation events that follow. Studies from Africa show that it is the availability of food, not the presence of predators, that limit populations of larger herbivores (which are too big for Europe’s lynx and foxes to tackle anyway). Opening up the corridor even in a limited way would enable hungry animals to leave the OVP. They would migrate in social groups and decisions could be made on what to do with each group – a sort of “tap” in the system. The main expense would be building an ecoduct over the A6, a large motorway that runs north from Amsterdam around the edge of the park. The land for the corridor is already bought and the Netherlands has plans to build another 20 ecoducts. Society and rewilding have both moved on since the Oostvaardersplassen was created, but the OVP “experiment” has not been able to do so. It’s time to change this – after all, rewilding should be about the future, not the past."
"
Share this...FacebookTwitterTornadoes have become less frequent since 2010: Pacific ocean cycles control storm frequency
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P Gosselin)
We haven’t heard much about tornadoes lately. For a while they were the favorites among climate activists. When did the love affair end? Here we cast a look at the official NOAA tornado statistics:

Fig. 1: Cumulative curve showing the number of tornadoes. Chart: NOAA.
Here we see 2017 was (fortunately) only average. The tornado trend over the past 60 years below shows the comparisons clearly. From 2005-2010 we saw an increased frequency of tornadoes in the USA, but they’ve since become less frequent. That’s bad news for the purveyors of catastrophe stories.

Fig. 2: Number of tornadoes in the USA since 1950. Source: NOAA.
With respect to the dangers of tornadoes, Hannes Stein asked in 2013 at German daily Welt, why Americans do not build build more stable structures, for example homes made of stone instead of wood:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Tornado damage is terrible – so why don’t Americans build better homes? That’s what Europeans ask, and thus prove their ignorance and arrogance.”
By the way, one finds an excellent display of global winds at ventusky.com.
So why does tornado activity fluctuate so much over the course of decades? Scientists at the University of Missouri found the answer: Tornadoes are influenced by the Pacific Decadal Oscillation (PDO), as explained in a press release dated October 10, 2013:
Pacific Ocean Temperature Influences Tornado Activity in U.S., MU Study Finds
Meteorologists often use information about warm and cold fronts to determine whether a tornado will occur in a particular area. Now, a University of Missouri researcher has found that the temperature of the Pacific Ocean could help scientists predict the type and location of tornado activity in the U.S.
Laurel McCoy, an atmospheric science graduate student at the MU School of Natural Resources, and Tony Lupo, professor and chair of atmospheric science in the College of Agriculture, Food and Natural Resources, surveyed 56,457 tornado-like events from 1950 to 2011. They found that when surface sea temperatures were warmer than average, the U.S. experienced 20.3 percent more tornados that were rated EF-2 to EF-5 on the Enhanced Fuijta (EF) scale. (The EF scale rates the strength of tornados based on the damage they cause. The scale has six category rankings from zero to five.). McCoy and Lupo found that the tornados that occurred when surface sea temperatures were above average were usually located to the west and north of tornado alley, an area in the Midwestern part of the U.S. that experiences more tornados than any other area. McCoy also found that when sea surface temperatures were cooler, more tornadoes tracked from southern states, like Alabama, into Tennessee, Illinois and Indiana.
“Differences in sea temperatures influence the route of the jet stream as it passes over the Pacific and, eventually, to the United States,” McCoy said. “Tornado-producing storms usually are triggered by, and will follow, the jet stream. This helps explain why we found a rise in the number of tornados and a change in their location when sea temperatures fluctuated.” In the study, McCoy and Lupo examined the relationship between tornadoes and a climate phenomenon called the Pacific Decadal Oscillation (PDO). PDO phases, which were discovered in the mid-1990s, are long-term temperature trends that can last up to 30 years. […]. “In the warm phase, which lasted from 1977 to 1999, the west Pacific Ocean became cool and the wedge in the east was warm.”
Also the El Ninos and the La Ninas (ENSO) impact tornadoes, as documented by Lepore et al. 2017:
ENSO-based probabilistic forecasts of March–May U.S. tornado and hail activity
Extended logistic regression is used to predict March–May severe convective storm (SCS) activity based on the preceding December–February (DJF) El Niño–Southern Oscillation (ENSO) state. The spatially resolved probabilistic forecasts are verified against U.S. tornado counts, hail events, and two environmental indices for severe convection. The cross-validated skill is positive for roughly a quarter of the U.S. Overall, indices are predicted with more skill than are storm reports, and hail events are predicted with more skill than tornado counts. Skill is higher in the cool phase of ENSO (La Niña like) when overall SCS activity is higher. SCS forecasts based on the predicted DJF ENSO state from coupled dynamical models initialized in October of the previous year extend the lead time with only a modest reduction in skill compared to forecasts based on the observed DJF ENSO state.”
There are also tornadoes in Germany from time to time. However, there has been no discernable trend over the past 15 years as shown by he Figure 7 in the DWD report.
Share this...FacebookTwitter "
"The furore over Cecil the lion clearly demonstrates that the public are passionate about conserving wildlife – wherever it is. Yet conservation spending in richer nations is still trapped in a parochial “home first” mindset. Given most plants and animals, and particularly endangered species, are found in poorer countries where money goes further, why are we worrying about hedgehogs, squirrels or wild boar? Last year, around £571m of public sector funding was spent on UK biodiversity. However only £60m was earmarked for international biodiversity, barely 10% of the total budget. From a global perspective, the UK contains only a tiny proportion of the world’s biodiversity. In terms of diversity of species, it ranks 89th in the world, and that’s despite the country’s wildlife being recorded far better than most.  If we are really serious about conserving biodiversity then perhaps those two budgets should be reversed. We’re likely to get far greater “biodiversity for our buck” on every pound spent abroad, especially if we focus on the world’s poorest developing countries, many of which are in global biodiversity hotspots. The government’s flagship Darwin Initiative has been striving to achieve exactly this. Since 1992 it has received just £113m, but supported 943 projects in 159 countries, and its achievements speak for themselves. Projects have ranged from supporting the only national herbarium in Papua New Guinea, developing a conservation action plan for the mammals of Tanzania and conservation for critically endangered species such as the spoon-billed sandpiper and sociable lapwing. Those projects averaged only £200,000 apiece. In contrast, our domestic concerns couldn’t appear more mundane. Take, for example, the iconic red squirrel, that bastion of British conservation. In a rearguard action to slow their decline, we are spending at least £1.2m from both public and private sources on their conservation. The government recently approved a scheme to fund population control of invading grey squirrels at £100 per hectare over five years, with a cost likely to run into the millions. In 2012 we even helicoptered five reds to Tresco on the Isles of Scilly to establish a new population (three died shortly afterwards).  Reds are listed as “least concern” by the main conservation body, IUCN. Their population is widespread and non-threatened in Europe. Meanwhile, in southern Africa, critically endangered black rhinos are being airlifted to establish insurance populations against the very real threat of extinction, and the Northern white rhino will shortly become extinct because we acted too late. In an ideal world maintaining our native squirrel would be a justifiably worthwhile thing to do. But in the face of global biodiversity loss, does the colour of our squirrels really matter? If there is even a tiny chance that our limited biodiversity funding could be better spent, then this is a debate that must be had; but only if we are clear that biodiversity and conservation is our aim, rather than locally emotive projects. Public spending is one way of understanding the priority that is given to different issues in a democracy. These figures suggest an inward-looking nation, at precisely at a time when we need to propel biodiversity issues to the world stage if we hope to encourage meaningful commitments at the Paris climate talks later this year. The outcome will have far reaching repercussions for global biodiversity and for us all. But it need not be a black and white choice; perhaps we could invest internationally and reap the benefits at home. Perhaps protection of critical habitats abroad might actually improve UK biodiversity. Birds such as nightingales, cuckoos and turtle dove all migrate from Africa, but numbers have dropped 73% since the late 80s, probably due to habitat loss in both the UK and sub-Saharan Africa. Could it be more cost-effective to conserve migratory species in other parts of their range? Would we then see larger populations returning to British shores? More importantly, would the public support it? There is yet another opportunity for compromise in the form of the UK’s Overseas Territories; these small islands such as Bermuda, the Turks and Caicos or Saint Helena represent proverbial jewels in the country’s biodiversity crown. Just 7,000 square miles (excluding British Antarctic Territory) supports an estimated 90% of UK biodiversity. This includes around 180 endemic plants (there are just 12 on the UK mainland) and 517 globally threatened species. The territories are home to 23 endemic birds, with at least 14 having already become extinct, largely due to humans.  The UK spends around £3m per year on environmental protection in these territories, but this still falls far short of the estimated £16m needed to meet international biodiversity commitments. Finally, to give us some perspective, it’s worth remembering that in 2013/14 the £571m spent on biodiversity amounted to just £3 for every £10,000 of GDP. So all things considered, we really can have our cake and eat it. We could support biodiversity at home while increasing spending abroad and barely cause a ripple in GDP. Our investment to date is but a drop in the ocean."
"Climate change is one of the great security challenges of the 21st century. As the world warms, conflicts over water, food or energy will become more common and many people will be forced from their homes. Scientists, think-tanks, NGOs, militaries and even the White House (albeit under President Obama) all agree that climate change threatens human safety and well-being. Yet the organisation charged with global security has remained relatively silent. The UN Security Council, responsible for maintaining international peace and security, is comprised of 15 countries. Five seats are reserved for permanent members with veto powers (China, France, Russia, the UK and the US) while the other ten members are elected to represent their region (“Africa”, “Asia-Pacific” etc) for two year terms.  Together, this semi-rotating group of 15 takes binding decisions for all 193 UN members. This alone makes the Security Council a very powerful institution, but combined with its capacity to sanction, and intervene in the affairs of states it has an influence far exceeding that of any other international body. It is, in many respects, the executive of the international system. For this reason the council has considered contemporary security challenges such as international terrorism, nuclear weapon proliferation, and transnational crime. Positive results include an international crackdown on the financing of terrorism, the sharing of information to tackle various criminal problems, stronger border controls for nuclear materials, and the global mobilisation of experts to address a health epidemic.  The fact the Security Council has helped combat these varied and largely unrelated challenges shows its potential to do good. Yet these interventions also pose the critical question of why it has yet to engage climate change in any meaningful way. Article 41 sanctions would be available to the council in the event of states not meeting their Paris Agreement obligations. Economic sanctions could also be placed upon corporations, that currently operate with relatively little international scrutiny. What the council brings is an ability to coerce – something that is currently lacking throughout international climate law. The council hasn’t entirely ignored climate change, of course. In 2007 the first open debate on the matter took place, though this was based on the unofficial proviso that no binding output would follow. Similar discussions were held in 2011 and 2013 but again stark divides among the members prevented any meaningful outputs. What this represents is a lack of unity over whether climate change really belongs on the agenda. While most states now agree climate change is a priority – as exhibited by the success of the Paris conference in 2015 – there is no consensus on what role, if any, the Security Council should play. From one perspective, countries like New Zealand and Germany view climate change as a security issue of immense proportions and worthy of the council’s attention. On the other hand, states such as China and South Africa argue that if the council engages with climate change it will undermine the sovereignty of states, fracturing the international system. These positions are entrenched, reflecting vastly opposing ideologies in relation to both climate change and international relations, thus precluding any meaningful intervention. Yet this does not necessarily mean that the Security Council is frozen indefinitely. The council has a history of taking tentative steps when moving into new territory, and climate change will not be an exception. In 2011 a statement made by then-president of the Security Council (a position that rotates between member states each month) loosely linked climate change and traditional security challenges. In 2017, the council unanimously adopted Resolution 2349, which hinted that climate change had contributed to conflict and instability around Lake Chad and the wider Sahel region. And in January 2018 a second presidential statement twice referenced climate change in the context of instability in the Sahel region. These statements fall short of finding climate change an explicit security threat, but do they show the council is steadily becoming more comfortable with the subject. And without that degree of comfort we would likely not have seen the passing of Resolution 2408 on March 27, 2018. This resolution, again adopted unanimously, extended the mandate of the UN mission in Somalia for another year and became the latest council resolution to include reference to climate change. The language remains speculative and the council is careful to only recall its 2011 statement instead of making a bolder standalone declaration on climate security. However, inclusion of the expression “grave concern” in regard to the drought and famine engulfing Somalia is proof that the council is experiencing a change of perspective. It is beginning to make discursive links between environmental realities and security, using the language often reserved for terrorism or nuclear weapon proliferation. The resolution fails to indict climate change as the cause of these problems yet it is nonetheless progress. After years of dispute council members are starting to agree on the inclusion of the words “climate change” in a resolution – a big step forward for the world’s most powerful but politically polarised body. So where are we? The Security Council has access to the tools the world so desperately needs to enforce state and private action on climate change, and although it is taking its time there is some advancement. That does not mean climate change is about to be recognised as a security concern in its own right, but each step taken is valuable and the council is certainly on the right path to identifying climate change as the security threat it so clearly is."
"The army has been called in to help firefighters deal with a huge wildfire on Saddleworth Moor, Greater Manchester, where residents have been forced to evacuate. Wildfires are also blazing across Northern California while the issue of bushfires in Australia calls for constant vigilance from the emergency services there. These fires are becoming more common and one of the reasons for this is climate change. Warmer temperatures in the summer and associated drier conditions desiccate plant materials and create more vegetation litter, providing more fuel for these fires. Several studies have linked the increase of wildfires with climate change in various parts of the world, such as North America and Southern Europe. For example, a study in California from 2004 found that the warmer and windier weather (brought about by an atmosphere with higher levels of CO2) produced fires that burned more intensely and spread faster in most locations. Despite enhanced firefighting efforts, the number of escaped fires (those exceeding initial containment limits) increased by 51% in the south San Francisco Bay area, 125% in the Sierra Nevada. It has also been demonstrated that increases in rainfall during winter and spring – which are also known consequences of climate change – provide more favourable conditions for plant growth and therefore more potential fuel for the fires later in the summer. Even though climate change increases the vulnerability of dry environments to wildfires, a source of ignition is still required. In the UK, it can be natural (such as bolts of lightning) or caused by man either deliberately or accidentally. Various studies have shown that the number of recreational visits to “risky” sites, such as the English Peak District, increase the occurrence of wildfire. Human activities have shaped heathlands and moorlands in the UK over the centuries, keeping them open and slowing down the natural succession towards more closed forest habitats. Despite the human impact on their origin, moorlands represent important ecosystems for numerous endangered species including reptiles, insects and birds.  But historic poor management has caused a lot of damage in moorland habitats. The introduction of non-native species for the moor, such as Rhododendron or planted conifers, has affected biodiversity. Overgrazing and drainage has increased the risks of erosion and flooding by reducing vegetation cover and limiting the ability for the soil to absorb precipitations. This, in turn, as lead to an increase in aridity of the habitat – which is the perfect environment for wildfires. Nowadays, most of the UK’s moorlands are associated with red grouse shooting and are managed in relation to that activity. Procedures include rotational burning and control of predators. Some of these processes are controversial with some environmentalists claiming it can turn the moorland into a  “monoculture” of low heather which can be highly susceptible to wildfires. But the evidence on this is not clear and a report by the RSPB found little proof of the negative effect of grouse moor management on biodiversity, flooding and wildfires. Landscapes and their plant and animal communities are not fixed in time. They are under the influence of dynamic processes that can be recurrent (such as marine tides and seasonal flooding) or catastrophic (volcanic eruptions or storms). Fire – whether natural or man-made – is an important factor that will drive the structure and wildlife composition of ecosystems. Some areas, such as the Mediterranean region or the African savannah, have been shaped by fire for thousands of years. Plants and animals have evolved to cope with the periodic perturbations due to it. For example, some seeds can only germinate after they have been burnt.  There are even some plants and animals that are contributing in the propagation of wildfires. In Australia, some raptor birds have been observed picking up burning sticks and dropping them in unburned areas to force potential prey out of their burrows. Despite its destructive power, fire is an important ecological process that can benefit several endangered species by maintaining their habitat. It is an important tool in the management and preservation of heathlands and moorlands in the UK when used appropriately and in a controlled way. But climate change and human activities increase the vulnerability of those habitats to uncontrolled wildfires and higher population densities near these areas will potentially put more people and houses at risk. In addition to the global battle against climate change, appropriate management procedures are necessary to maintain those habitats and ensure the risks of uncontrolled fires are minimised and the potential spread of them reduced."
nan
