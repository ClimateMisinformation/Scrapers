"

Notice all the horrendous news about our environment? That’s a sure sign that the UN is about to throw another mega‐​gabfest where global leaders will shake their heads and shake down the U.S. for monies that Congress will wisely refuse to fork over.



Two weekends from now, the UN is holding its “Rio+20 Earth Summit,” the largest meeting in the history of an organization that pretty much does nothing but stage meetings. The 1992 Rio Summit produced the Framework Convention on Climate Change, which was the basis for the completely failed Kyoto Protocol on global warming. It also spawned Agenda 21, a document which outlined in great detail its plans to punish and pillage producer nations and transmit their wealth to the world’s great kleptocracies.



Rio 1992 was also the basis for 19 annual “Conferences of the Parties” to the Framework Convention, all of which succeeded in doing exactly nothing measurable about climate change. The most famous of these, after Kyoto in 1997, was in Copenhagen in December, 2009.



President Obama flew there, fresh with an “Endangerment Finding” from carbon dioxide hot off of his EPA’s presses. Because it was obvious that the Senate wouldn’t touch cap‐​and‐​trade, he needed something credible in order to goad the world into a new treaty to replace the dead Kyoto agreement. Despite being treated pretty roughly by Brazil, South Africa, China and India, he declared victory — with no specifics — as the meeting drew to a close. Obama couldn’t answer many questions, though, as he had to hightail it back to Washington to beat the first of that winter’s three blizzards. He didn’t, and the image of Air Force One landing in a blinding snowstorm will forever be the icon of the Copenhagen fiasco.



The great “success” of Copenhagen was an agreement that all the participants would submit plans detailing how they would reduce their dreaded greenhouse gas emissions in six weeks. Two weeks before that deadline, Yvo de Boer, Executive Secretary of the Framework Convention, announced that, never mind, we didn’t mean it, we don’t need your silly plans, and then he resigned.



Rio+20 is intended to go beyond all this. Failure is not an option, it is guaranteed.



While the agenda has yet to be finalized at this late date, it’s more of the same hand‐​wringing gloom and doom followed by more of the same outstretched hands. Not surprisingly, the same fault lines that have continually plagued the UN’s are emerging. Poor nations want our money. Europe agrees with this but they’re fresh out. Our Congress wants to be re‐​elected and won’t cooperate. India and China plead for special treatment.



But the list is longer than ever. In addition to climate change, we now have to remediate biodiversity loss, poverty, acid oceans (no such thing), poverty, “unsustainable consumption” (honest!), poverty, the right to food, poverty, and the “right to an adequate standard of living.” If much of this sounds like the wish list of your indolent teenager, that’s about right.



My academic pals are doing their level best to flog for the UN. Just this week, and, according to the _Christian Science Monitor_ , “timed for the Rio meeting,” _Nature_ published a remarkable screed by a team of twenty scientists forecasting the end of the world as we know it (literally) caused largely by increasing human population.



(Hint: a policy‐​driven piece authored by more than ten people, accompanied by a breathless press release, and published before a UN summit is known as a “petition.”)



If this sounds anything like the Club of Rome’s sophomoric 1972 “Limits to Growth,” it is. That forecast of the end of the world as we knew it by 2000 obviously failed, using the advanced methodology of the day (harmonic analysis and multiple regressions). The new paper by Anthony Barnosky uses a “fold bifurcation with hysteresis.” That’s impressive to all the UN delegates, most of whom avoided math and science in order to boss around mathematicians and scientists.



Actually, it really means a lagged discontinuous function, something you can find in honors Algebra II.



The 1972 and 2012 ends‐​of‐​the‐​world are simply the same shtick with the same tactics and objectives, namely abuse of authority to give authority to a global bureaucracy. Between then and now there have been literally dozens of such silly screeds. They obviously didn’t or won’t work, just like the 1992 Earth Summit and Rio+20.



If these people were serious about greenhouse gases and hot air, they would meet online. But they are not, not after 20 consecutive failures.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Today we plot the Japan Meteorological Agency (JMA) data for Northern Europe for the month of August, 2020.
We have selected this region because it is the home of 17-year old climate alarmist/activist Greta Thunberg, who thinks the planet is heating up rapidly and so we’re all doomed.
We plot the data for the stations for which the JMA has sufficient data going back over 2 decades. First we plot the August data for Sweden, Greta’s home country:

Data: JMA
Five of the 6 stations plotted show a cooling trend. So it’s a mystery how Greta thinks her country is warming up. The data suggest that summers have been shortening a bit. Over the course of Greta’s life, she has yet to see warming in August.
Next we examine Norway, Greta’s western neighbor:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data: JMA
Here we see 6 of the 11 stations have seen an August cooling trend over the past quarter century. The colder stations have warmed somewhat, while the warmer ones have cooled. Overall, no warming to speak of, really.
The story is similar in Finland (further from the Atlantic), but here the colder stations have cooled, while the warmer stations have warmed slightly – but statistically insignificantly:

Data: JMA
Finally we plot the data for the emerald island, Ireland, next to the tempestuous Atlantic:

Data: JMA
Four of the six stations in Ireland have been cooling in August. Those warming have done so insignificantly. Overall the emerald island has been cooling during the month of August since 1983!


		jQuery(document).ready(function(){
			jQuery('#dd_0f4f1e38f8790e798fa8564c196424a6').on('change', function() {
			  jQuery('#amount_0f4f1e38f8790e798fa8564c196424a6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
There is a lot of intense interest out there in watching TS Noel to see if this disorganized system turns into a Hurricane when it hits the warmer waters of the Gulf Stream. Click on the image for an animated version.

Then, place your bets.
The image is from my company, IntelliWeather, and updates every half hour.
UPDATE: Noel did in fact become a hurricane, and now has 80 mph sustained winds, but it appears to be headed out to sea, and into cooler waters where it will likely dissipate.
from NHC advisory 22: …HURRICANE NOEL EXPECTED TO BECOME A LARGE AND POWERFUL EXTRATROPICAL STORM OVER THE OPEN ATLANTIC ON FRIDAY… NOEL IS MOVING TOWARD THE NORTH-NORTHEAST NEAR 20 MPH…32 KM/HR…AND THIS MOTION IS EXPECTED TO CONTINUE DURING THE NEXT 24 HOURS. ON THIS TRACK…NOEL WILL CONTINUE TO MOVE AWAY FROM THE BAHAMAS.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2e89e8d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When Senate Majority Leader‐​to‐​be Tom Daschle asked Jim Jeffords what he wanted in return for switching political sides, the junior senator from the Green Mountain State asked to be chairman of the Environment and Public Works Committee.



That’s the only take‐​home prize Jeffords gets for his defection, so it must be important. And telling.



One can imagine, after this promise, what happened at Jeffords’ meeting with Messrs. Bush and Cheney: “OK, Senator, what does it take to keep you with us?”



Jeffords: “Change your position on the Kyoto Protocol.”



Cheney: “Hit the road, Jim.”



The truth is that Kyoto and global warming will be the focus of Jeffords’ Environment Committee. Expect him to parade witness after witness, on the hottest summer days, decrying Bush & Co. for being “out of touch” with the world on this issue. Robert Watson, head of the United Nations’ Intergovernmental Panel on Climate Change, which bills itself as the “consensus of scientists,” is sure to get top billing.



When he appears, Watson will trot out the U.N.’s new “Third Assessment Report” on climate change, a compendium of more than 1,000 pages that’s due to hit the streets in a couple of months. With this report, Jeffords will show us how loony the current Administration is about planetary heating.



Unlike the U.N.’s first and second Assessments, published in 1990 and 1996, this one purports to tell world leaders what to do about climate change. Here’s what the “consensus of scientists” prescribes (it can be found on page 12 of the “Policymakers Summary” of the new report):



“emissions/​carbon/​energy taxes, tradable or non‐​tradable permits [indirect taxes], subsidies [which require taxes], deposit/​refund systems [taxes and regulations], technology or performance standards [regulations by fiat], product bans [let’s outlaw coal!], voluntary agreements [ha!], government spending and investment [taxes], and support for research and development [paid for by taxes].”



Who is “out of touch” here? Each of these general policy prescriptions requires confiscation of individual wealth to cool the planet or adapt to warming. Apparently, the “consensus of scientists” is that people and markets are too stupid to do this on their own. Did it ever occur to the United Nations that if global warming is as terrible and costly as it thinks it is, there will be a substantial market for technologies and products that would reduce that cost? The U.N. thinks we would rather sit around and fry, unless we are taxed into mending our evil ways.



In fact, there’s good scientific evidence that we adapt well to hot days. Some of today’s most in‐​demand technology, in fact, is designed to prevent death from heat stroke, an affliction that was more common decades ago. The technology is called “air conditioning.”



Are we quietly adapting or passively frying? My University of Virginia colleague Robert Davis and I recently looked at heat‐​related mortality data from major American cities for the last 40 years. At first, we found what everyone seems to know: In some cities, mainly older ones, daily death rates skyrocket on exceedingly hot days. Therefore, the United Nations tells us, if we heat things up more, “several thousand” more people will die every summer in North America because of global warming. The truth is that cities have been heating up by themselves, without global warming, for hundreds of years, compromising our ability to measure the earth’s true temperature.



But when we looked at the trends in heat‐​related deaths, we found that in most cities, the deaths occurred early on–in the 1960s and 1970s. By the time we get to the 1990s, we have engineered heat‐​related deaths out of most cities, with electrically driven air conditioning. In fact, the last big urban die‐​off, in Chicago in July 1995, occurred largely because there was a power failure. Our results have been presented at several peer‐​screened professional conferences and written up many times, all to favorable review.



We don’t expect Jeffords to invite us before his Committee, because we’re “out of touch,” too. Our results show that free markets, not taxation, create the capital that people use to invest in technologies that shield them from the vagaries of our naturally hostile environment. And there’s the problem: When it comes to the environment, Jeffords, the Kyoto Protocol, and the United Nations believe more in taxation and coercion than they do in free markets and free will.
"
"
NOTE: Earlier today I posted a paper from Joe D’Aleo on how he has found strong correlations between the oceans multidecadal oscillations, PDO and AMO, and surface temperature, followed by finding no strong correlation between CO2 and surface temperatures. See that article here:
Warming Trend: PDO And Solar Correlate Better Than CO2
Now within hours of that, Roy Spencer of the National Space Science and Technology Center at University of Alabama, Huntsville,  sends me and others this paper where he postulates that the ocean may be the main driver of CO2. 
In the flurry of emails that followed, Joe D’Aleo provided this graph of CO2 variations correlated by El Nino/La Nina /Volcanic event years which is relevant to the discussion. Additionally for my laymen readers, a graph of CO2 solubility in water versus temperature is also relevant and both are shown below:
   
Click for full size images
Additionally, I’d like to point out that former California State Climatologist Jim Goodridge posted a short essay on this blog, Atmospheric Carbon Dioxide Variation, that postulated something similar. 
UPDATE: This from Roy on Monday 1/28/08 see new post on C12 to C13 ratio here
I want to (1) clarify the major point of my post, and (2) report some new (C13/C12 isotope) results:
1.  The interannual relationship between SST and dCO2/dt is more than enough to explain the long term increase in CO2 since 1958.  I’m not claiming that ALL of the Mauna Loa increase is all natural…some of it HAS to be anthropogenic…. but this evidence suggests that SST-related effects could be a big part of the CO2 increase.
2.  NEW RESULTS: I’ve been analyzing the C13/C12 ratio data from Mauna Loa.  Just as others have found, the decrease in that ratio with time (over the 1990-2005 period anyway) is almost exactly what is expected from the depleted C13 source of fossil fuels.  But guess what? If you detrend the data, then the annual cycle and interannual variability shows the EXACT SAME SIGNATURE.  So, how can decreasing C13/C12 ratio be the signal of HUMAN emissions, when the NATURAL emissions have the same signal???
-Roy
Here is Roy Spencer’s essay, without any editing or commentary:

Atmospheric CO2 Increases:
Could the Ocean, Rather Than Mankind, Be the Reason?
by
Roy W. Spencer
1/25/2008


            This is probably the most provocative hypothesis I have ever (and will ever) advance:  The long-term increases in carbon dioxide concentration that have been observed at Mauna Loa since 1958 could be driven more than by the ocean than by mankind’s burning of fossil fuels.
            Most, if not all, experts in the global carbon cycle will at this point think I am totally off my rocker.  Not being an expert in the global carbon cycle, I am admittedly sticking my neck out here.  But, at a minimum, the results I will show make for a fascinating story – even if my hypothesis is wrong.  While the evidence I will show is admittedly empirical, I believe that a physically based case can be made to support it.
            But first, some acknowledgements. Even though I have been playing with the CO2 and global temperature data for about a year, it was the persistent queries from a Canadian engineer, Allan MacRae, who made me recently revisit this issue in more detail.  Also, the writings of Tom V. Segalstad, a Norwegian geochemist, were also a source of information and ideas about the carbon cycle.

            First, let’s start with what everyone knows: that atmospheric carbon dioxide concentrations, and global-averaged surface temperature, have risen since the Mauna Loa CO2 record began.  These are illustrated in the next two figures.

 

Both are on the increase, an empirical observation that is qualitatively consistent with the “consensus” view that increasing anthropogenic CO2 emissions are causing the warming.  Note also that they both have a “bend” in them that looks similar, which might also lead one to speculate that there is a physical connection between them.
Now, let’s ask: “What is the empirical evidence that CO2 is driving surface temperature, and not the other way around?”  If we ask that question, then we are no longer trying to explain the change in temperature with time (a heat budget issue), but instead we are dealing with what is causing the change in CO2 concentration with time (a carbon budget issue).  The distinction is important.  In mathematical terms, we need to analyze the sources and sinks contributing to dCO2/dt, not dT/dt.
So, let us look at the yearly CO2 input into the atmosphere based upon the Mauna Loa record, that is, the change in CO2 concentration with time (Fig. 3).

Here I have expressed the Mauna Loa CO2 concentration changes in million metric tons of carbon (mmtC) per year so that they can be compared to the human emissions, also shown in the graph.
Now, compare the surface temperature variations in Fig. 2 with the Mauna Loa-derived carbon emissions in Fig. 3.  They look pretty similar, don’t they?  In fact, the CO2 changes look a lot more like the temperature changes than the human emissions do.  The large interannual fluctuations in Mauna Loa-derived CO2 “emissions” roughly coincide with El Nino and La Nina events, which are also periods of globally-averaged warmth and coolness, respectively.  I’ll address the lag between them soon. 
Of some additional interest is the 1992 event.  In that case, cooling from Mt. Pinatubo has caused the surface cooling, and it coincides in a dip in the CO2 change rate at Mauna Loa.
These results beg the question: are surface temperature variations a surrogate for changes in CO2 sources and/or sinks?
First, let’s look at the strength of the trends in temperature and CO2-inferred “emissions”.  If we compare the slopes of the regression lines in Figs. 2 and 3, we get an increase of about 4300 mmt of carbon at Mauna Loa for every degree C. of surface warming.  Please remember that ratio (4,300 mmtC/deg. C), because we are now going to look at the same relationship for the interannual variability seen in Figs. 2 and 3.
In Fig. 4 I have detrended the time series in Figs. 2 and 3, and plotted the residuals against each other.  We see that the interannual temperature-versus-Mauna Loa-inferred emissions relationship has a regression slope of about 5,100 mmtC/deg. C. 
There is little evidence of any time lag between the two time series, give or take a couple of months.

So, what does this all show?  A comparison of the two slope relationships (5100 mmtC/yr for interannual variability, versus 4,700 mmtC/yr for the trends) shows, at least empirically, that whatever mechanism is causing El Nino and La Nina to modulate CO2 concentrations in the atmosphere is more than strong enough to explain the long-term increase in CO2 concentration at Mauna Loa.  So, at least based upon this empirical evidence, invoking mankind’s CO2 emissions is not even necessary. (I will address how this might happen physically, below).
In fact, if we look at several different temperature averaging areas (global, N. H. land, N.H. ocean, N.H. land + ocean, and S.H. ocean), the highest correlation occurs for the Southern Hemisphere ocean , and with a larger regression slope of 7,100 mmtC/deg. C.  This suggests that the oceans, rather than land, could be the main driver of the interannual fluctuations in CO2 emissions that are being picked up at Mauna Loa — especially the Southern Ocean.
Now, here’s where I’m really going to stick my neck out — into the mysterious discipline of the global carbon cycle.  My postulated physical explanation will involve both fast and slow processes of exchange of CO2 between the atmosphere and the surface. 
The evidence for rapid exchange of CO2 between the ocean and atmosphere comes from the fact that current carbon cycle flux estimates show that the annual CO2 exchange between surface and atmosphere amounts to 20% to 30% of the total amount in the atmosphere.  This means that most of the carbon in the atmosphere is recycled through the surface every five years or so.  From Segalstad’s writings, the rate of exchange could even be faster than this.  For instance, how do we know what the turbulent fluxes in and out of the wind-driven ocean are?  How would one measure such a thing locally, let alone globally?
Now, this globally averaged situation is made up of some regions emitting more CO2 than they absorb, and some regions absorbing more than they emit.  What if there is a region where there has been a long-term change in the net carbon flux that is at least as big as the human source? 
After all, the human source represents only 3% (or less) the size of the natural fluxes in and out of the surface.  This means that we would need to know the natural upward and downward fluxes to much better than 3% to say that humans are responsible for the current upward trend in atmospheric CO2.  Are measurements of the global carbon fluxes much better than 3% in accuracy??  I doubt it.
So, one possibility would be a long-term change in the El Nino / La Nina cycle, which would include fluctuations in the ocean upwelling areas off the west coasts of the continents.  Since these areas represent semi-direct connections to deep-ocean carbon storage, this could be one possible source of the extra carbon (or, maybe I should say a decreasing sink for atmospheric carbon?).   
Let’s say the oceans are producing an extra 1 unit of CO2, mankind is producing 1 unit, and nature is absorbing an extra 1.5 units.  Then we get the situation we have today, with CO2 rising at about 50% the rate of human emissions.
If nothing else, Fig. 3 illustrates how large the natural interannual changes in CO2 are compared to the human emissions.  In Fig. 5 we see that the yearly-average CO2 increase at Mauna Loa ends up being anywhere from 0% of the human source, to 130%.  
It seems to me that this is proof that natural net flux imbalances are at least as big as the human source.

Could the long-term increase in El Nino conditions observed in recent decades (and whatever change in the carbon budget of the ocean that entails) be more responsible for increasing CO2 concentrations than mankind?  At this point, I think that question is a valid one.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1663853',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The 11th biennial fiscal report card on the governors comes at a time when those leaders have struggled with a sluggish recovery, resulting budget deficits, unemployment, and other economic problems in their states. Many reform‐​minded governors elected in 2010 have championed tax reforms and spending restraint to get their states back on track. Other governors have expanded government with old‐​fashioned tax‐​andspend policies. In **“Fiscal Policy Report Card on America’s Governors: 2012”** (White Paper), Chris Edwards, director of tax policy studies at the Cato Institute, graded all of the states’ governors and awarded four of them A’s — Sam Brownback of Kansas, Rick Scott of Florida, Paul LePage of Maine, and Tom Corbett of Pennsylvania. Five governors were awarded F’s — Pat Quinn of Illinois, Dan Malloy of Connecticut, Mark Dayton of Minnesota, Neil Abercrombie of Hawaii, and Chris Gregoire of Washington. Edwards offers short analyses of each governor’s performance in addition to a letter grade. Many states are facing major fiscal problems in coming years. Rising debt and growing health and pension costs threaten tax increases down the road. At the same time, intense global economic competition makes it imperative that states improve their investment climates. To that end, some governors are pursuing broadbased tax reforms, such as cutting income tax rates and reducing property taxes on businesses. This report discusses those trends and examines the fiscal policy actions of each governor in an attempt to make their records more transparent.





**Amtrak’s Insatiable Appetite for Federal Funds**  
When Congress created Amtrak in 1970, passenger‐​rail advocates hoped that it would become an efficient and attractive mode of travel. But, in **“Stopping the Runaway Train: The Case for Privatizing Amtrak”** (Policy Analysis no. 712), Cato senior fellow Randal O’Toole shows that more than 40 years of operations have disappointed. Amtrak has become the highest‐​cost mode of intercity travel and remains an insignificant player in the nation’s transportation system. Nationally, average Amtrak fares are more than twice as much, per passenger mile, as airfares. Despite these high fares, perpassenger‐ mile subsidies to Amtrak are nearly 9 times as much as subsidies to airlines, and more than 20 times as much as those to driving. When fares and subsidies are combined, its costs per passenger mile are nearly four times as great as airline costs. “A close look at the data reveal that Amtrak has failed for two primary reasons,” O’Toole notes. First, passenger trains simply aren’t competitive in most markets. Second, government control of Amtrak has saddled it with numerous inefficiencies, including unsustainably expensive labor contracts. “No amount of reform will overcome the fundamental problem that, so long as Amtrak is politically funded, it will extend service to politically powerful states even if those states provide few riders,” O’Toole continues. The only real solution, he writes, is privatization. “Simple justice to Amtrak’s competitors as well as to taxpayers demands an end to those subsidies,” he concludes.



 **The Transparency Problem**  
“President Obama has committed to making his administration the most open and transparent in history,” the White​house​.gov website declared just minutes after the new president took office on January 20, 2009. Yet, it is now clear that government transparency has not improved materially since the beginning of President Obama’s administration. According to Jim Harper, Cato’s director of information policy studies, in **“Grading the Government’s Data Publication Practices”** (Policy Analysis no. 711), this is not due to lack of interest or effort. “Along with meeting political forces greater than his promises,” Harper writes, “the Obama transparency tailspin was a product of failure to apprehend what transparency is and how it is produced.” Starting from a low transparency baseline, this administration made extravagant promises and put significant effort into the project of government transparency. It has not been a success. House Republicans, who manage a far smaller segment of the government, started from a higher transparency baseline, made modest promises, and have taken limited steps to execute on those promises. President Obama lags behind House Republicans, but both have a long way to go. The solution, Harper argues, is to tackle the low‐​hanging fruit first. Establishing an authoritative list of programs and projects within the basic units of government, for instance, “is like creating a language, a simple but important language computers can use to assist Americans in their oversight of the federal government.” Harper goes on to lay out a variety of good data publication practices, which he insists can help to produce a more accountable, efficient, and responsive government.



 **The Race for Protection**  
The world is awash in trade‐​distorting subsidies. According to Scott Lincicome, a former employee of Cato’s Center for Trade Policy Studies and now an international trade attorney at White & Case, governments have adopted massive “stimulus” packages since the financial crisis of 2008 — handouts that have included taxpayer subsidies for various industries including agriculture, alternative energy, and automobiles. In **“Countervailing Calamity: How to Stop the Global Subsidies Race”** (Policy Analysis no. 710), Lincicome argues that these subsidies have, in turn, “distorted global markets, bred cronyism, and undermined free trade.” They have also encouraged copycat subsidization, which spawned an increase in litigation at the World Trade Organization and led to the frequent imposition of protectionist duties. Trade reform is badly needed. “Unfortunately,” Lincicome writes, “the U.S. government has little credibility on this issue: it is one of the world’s largest subsidizers, funneling billions of dollars annually to chosen industries, causing economic uncertainty, and creating breeding grounds for corruption.” Yet, ironically, with 59 currently active or pending national countervailing duty (CVD) measures affecting over $11 billion of imports, the U.S. government is also one of the most frequent users of antisubsidy disciplines. The only answer here, according to Lincicome, is true reform. By curtailing federal subsidies to favored industries and by changing CVD procedures to ensure that they serve the rule of international trade law — rather than protectionist objectives — “the U.S. government can reduce market distortions, restore some faith in free markets, and lead national and international subsidy reform initiatives,” he concludes.
"
"
Share this...FacebookTwitterIt didn’t receive much a attention in 2015, but a comprehensive Nature journal study of 0-2000 A.D. global sea surface temperatures shows 1) climate changes occurred more than twice as fast during the Little Ice Age (LIA) than since 1800, 2) the entire first millennium was >1 standard deviation (s.d. unit) warmer than today, and 3) 1800-2000 ocean changes amounted to just 0.08 of a s.d. unit per century.

Adapted Image Source: McGregor et al., 2015
There are several reasons to question the presentation of data in  McGregor et al., 2015. – a global-scale reconstruction of sea surface temperatures.
The myriad authors decided not to clearly depict actual temperature changes in their reconstruction, preferring instead to “reimagine” temperatures as standard deviation units.
The graphical presentation of “standardized SST [sea surface temperature] s.d. units” abruptly and curiously stops in 1900. This unexplained truncation was used despite mentioning in the body of the paper that the 1900-2000 period had a “statistically significant” warming trend of (just) 0.08 s.d. units/century – half of the century-scale changes during 1200-1400 and 1400-1600 (0.17 and 0.18 s.d. units/century, respectively). Perhaps the insignificance of the post-1900 uptick wasn’t considered helpful to the AGW (anthropogenic global warming) narrative.
The graphs depicting no remarkable modern global ocean temperature changes (shown below), such as the ones with flatline trends from the 1860s to 2000, are buried in the supplemental information for the paper, making the data and graphs less accessible. One would think that the lack of any remarkable or anomalous global temperature changes occurring during modern times would deserve some scientific attention.
Finally, this study shows the Roman Warm Period and Medieval Warm Period were globally warmer than today at the ocean surface (with some location and timing differences). It also affirms the LIA was “globally coherent.” The authors even identify the mechanism for “robust” LIA cooling: “high frequency explosive volcanism” with centennial-scale impacts.
At least the latter point made its way into the paper’s abstract…rather than hidden or buried.

Image Source: McGregor et al., 2015 and supplementary data for the paper
Share this...FacebookTwitter "
"

The _Washington Post_ has an article today on the battle over the Keystone XL pipeline. There is a sense of urgency on both sides as the decision on the project is expected to be fast approaching.   
  
  
The _Post_ features arguments from pipeline proponents that the project will provide an economic boost to the state of Nebraska, and from pipeline opponents that the oil carried though it will lead to more carbon dioxide emissions than previously thought, thus upping the impact on global warming and climate change.   
  
  
But the numbers being tossed about don’t tell the whole story.   
  
  
First, a look at the new economic claims. An analysis from the Consumer Energy Alliance concludes that during the two year construction phase of the pipeline, the economic activity in Nebraska will increase by a bit more than $400 million per year—generating directly or indirectly, about 5,500 new jobs. Sounds impressive, but this boost is short‐​lived. After that, for the next 15 years, the economic input drops down to about $67 million/​yr, supporting about 300 jobs. A net positive, but not as much as many proponents claim.   
  
  
The climate claims are even less significant. In its new report, Oil Change International asserts that the current estimates of the well‐​to‐​wheel (WTW) carbon dioxide emissions from oil extracted from the Alberta tar sands have been underestimated. They claim that the State Department failed to fully include carbon dioxide emissions from the burning of the petroleum coke that is produced as a side product of producing oil from the tar sands. This “petcoke” can be burned like coal, and in fact, is cheaper and more energy dense than coal, so it is often preferable. According to Oil Change International, including the petcoke in the calculation would increase the WTW carbon dioxide emissions by about 13 percent.   
  
  
There are several things wrong with the Oil Change International analysis. First is that the State Department actually did include a considerable discussion of the influence of the treatment of petcoke in its assessment. It concluded, just like Oil Change International, that if the petcoke is burned, it increases the total wells‐​to‐​wheels carbon dioxide emissions of Canadian tar sands oil by the same 13 percent. But what the State Department points out, and which Oil Change International plays down, is that the burning of petcoke to produce energy by and large displaces the use of coal for the same purpose. So instead of the total emissions, what is important is the _incremental_ carbon dioxide emissions produced from using petcoke instead of coal. And when that number is used, the WTW emissions increase by less than 1 percent—which is why the State Department concluded that the fate of the petcoke really wasn’t all that significant in the overall WTW emissions calculation.   
  
  
But whether consideration of petcoke increases the WTW carbon dioxide emissions of the tar sands oil by 1%, 13%, or any number in between, really doesn’t matter anyway in terms of its impact of global warming. For as I have shown previously, the global warming potential of the Keystone XL pipeline oil is only about 0.00001°C/yr. Increase that by 13% and you basically get the same environmentally insignificant number. In fact, you’d have to increase it _by several orders of magnitude_ before it is even worth paying attention to.   
  
  
The war over the pipeline will probably rage on until (and even after) a decision is reached in a couple of months. Hopefully, emotion will play a role secondary to facts.
"
"

How many hurricanes do you think will hit the East Coast of the United States in 2015? Will the Arctic ice sheet disappear next year? How fast will the U.S. economy grow? What will the level of the Dow Jones stock index be at the end of 2015? Which team will win the World Series?



Go back and look at predictions made by the experts, and then look at what really happened. The climate alarmists 15 or so years ago were forecasting catastrophic events by this time. Yet sea levels have not been rising any faster than they have been for centuries. The major climate models were projecting steady rises in global warming each year, yet average temperatures have not risen for 17 years. Al Gore and his alarmist crowd told us that the Arctic would be free of sea ice during the summer by now and that we would be having more and stronger tornadoes and hurricanes. The Arctic sea ice is still with us, and few ships dare sail there. Many tornado and hurricane records have been broken — not because there were more — but because there have been fewer. Florida has gone a record nine straight seasons without a significant hurricane.



None of the above disproves climate change, but it should caution those who have made many rash predictions. The economist‐​philosopher F.A. Hayek warned about “the limits of knowledge” and the “fatal conceit” exhibited by so many “experts.” The communists and socialists claimed that they could allocate resources and income better than markets. These false claims ultimately destroyed the lives of tens of millions and caused untold human misery. Despite the never‐​ending failures of socialist and other collectivist schemes (such as Obamacare), colleges, governments and the media are still filled with smug — but ignorant or uncaring — individuals (think Jonathan Gruber) who still think they are smarter than markets, and thus have the self‐​appointed right to control your life.



Economists have little to crow about when it comes to forecasting. Most of them missed calling the Great Recession. The Federal Reserve, which employs hundreds of economists, many from the best schools, kept predicting 4 percent‐​plus economic growth each year, after the recession bottomed in 2009. In fact, actual growth has been about half of what they predicted — but perhaps 2015 will be the year of 4 percent growth. Too many of my fellow economists, including many of those in the administration, get things wrong, in part, because they still use Keynesian economic models that treat increases in government spending as a positive rather than a negative, among other errors.



To make an accurate forecast, one needs to know what the Fed will do in regard to monetary policy and what Congress and the administration will do in terms of taxing, spending and regulation. One also needs to know what the economic policies of other countries will be — since the United States is not an island unto itself — and when wars and tsunamis will occur. The impossibility of knowing all of this does not mean that it is not useful to attempt to forecast, but merely that it is not scientific in the way that we can precisely predict the boiling point of water at a specific atmospheric pressure.



Forecasters need to have a good understanding of the major variables that might greatly affect their reasoning. The safest starting point for most forecasts is what happened in the previous period. Tomorrow’s weather is likely to be similar to today’s. One might begin an economic forecast by assuming next year is likely to look much like this year, then alter the forecast based on assumed changes in policy. For instance, if you assume the Republican‐​led Congress is likely to reduce government spending as a share of gross domestic product, and if you believe, as many of us do, that government spending (at the current high levels) is a drag on growth, then it is sensible to boost the growth estimate a bit — other things being equal. This same exercise needs to be repeated with each major variable — taxes, regulations, monetary actions, changes in oil prices — and what Russia’s Vladimir Putin might do next.
"
"
Share this...FacebookTwitterThe online German Der Spiegel reported yesterday here that the EU Commission wants to accelerate cuts in CO2 emissions, but industry and government officials are saying “no!”. With economies gripped by hardship and overall growing public scepticism (see here), calls for even more draconian measures to curb CO2 emissions are ringing hollow.
German Minister of Economics Rainer Bruederle says it’s time to take a break from efforts to protect the climate:
It accomplishes nothing for environmental protection when Europe goes it alone and jobs are sent to other regions of the world.
Werner Schnappauf, Director of the German Association of Industry adds:
As long as there is no international and legally binding climate protection treaty,  industry rejects increasing the climate reduction target from 20% to 30%. There are only disadvantages for both the climate and economy if Europe rushes and goes it alone.
      The EU wants to ratchet up the target from 20% to 30% less CO2 emissions than 1990 by 2020. Other leading German government officials think they can both appease the climate gods by making human sacrifices at the Altar of Climate, and at the same time boost the economy. German Minister of the Environment Norbert Roettgen and other EU environment ministers have said they want to go ahead and require the 30% target, with or without an international treaty.
It’s good for the environment and also for the incentive to innovate, from which the German industry would greatly profit.
      According to the EU Commission, a CO2 reduction of 30% by 2020 would lead to a 0.54% drop in GDP. Can’t these people think of ways to make our lives easier for a change?
Share this...FacebookTwitter "
"
Share this...FacebookTwitter UPDATED LIST:   !! CLICK HERE !!
What follows is only the first orignal list of climate gates, click above for the newest list.
Climate science has produced an entire warehouse full of scandals. In fact the scandals are in such big supply that you can now get them at the House of Climate Science for a dime a dozen. No other science has produced such a huge inventory.
It reminds me of a scene in the cult film Dusk till Dawn by Quentin Tarantino where an unseemly character named Chet makes a famous sales pitch outside a certain locale. I simply substituted the obscene term, for what some consider to be the world’s oldest known commodity, with the different “gates” we’ve seen in climate science.  Here’s the new sales pitch:
Come on in gate lovers!! Here at Sleazy Science we’re slashing gates in half! Give us an offer on our best selection of gates! This is a gate blowup! All right, we got drought-gate, Gore-gate, Sting-gate, Himalaya-gate, ursus-gate, we got Amazon-gate, China-gate, Russia-gate, Dutch-gate, Toad-gate
We got data-deletion-gate, we got Overpecker-gate, Hansen-gate, we got Inconvenient-gate, Africa-gate, boot-gate, Finland-gate, we even got Freedom of Information-gate, Greenpeace-gate, Yamal-gate. Come on you want gates, come on in gate lovers! If we don’t got it you don’t want it! Come on in gate lovers!  

(Warning! Should you get the idea to watch the original Chet speech in Dusk Till Dawn, then first clear the kids and ladies out of the room.)
Anyway, that said, here’s my abridged list of gates in climate science for your future reference. There’s something there for everyone! We now got:
1. Acceleration-gate
2. Africa-gate
3. AIT-gate
4. Amazon-gate
5. Antarctic sea-gate
6. Bangladesh-gate
7. Boot-cleaning manual-gate
8. China-gate and here
9. Climate Camp-gate
10. Climate-gate
11. CRU data deletion-gate
12. Dog-ate it-gate
13. Discernable influence-gate
14. Drought-gate
15. EPA-gate  h/t Climate Depot
16. Five-star WWF-gate
17. Finland-gate
18. Flooded house-gate
19. FOI-gate
20. Fungus-gate
21. Gatekeeping-gate
22. GISS Metar-gate
23. Gore private jet-gate
24. Greenpeace-gate
25. Hansen 1930s hot-gate
26. Hansen stagecraft-gate
27. Himalaya-gate and here
28. Hockey-stick-gate and here and here WCR
29. Hollywood hypocrites-gate and Dave Matthews
30. Hurricane-gate
31. Jesus Paper-gate
32. Kilimanjaro-gate
33. Malaria-gate and here (new!)
34. Meat-gate h/t reader Catalina
35. Mega-mansion-gate
36. Met Office computer-gate
37. NASA/NCDC bad data-gate and here
38. New Zealand-gate
39. NOAA adjustment-gate and here, and here
40. NOAA/GISS data selection-gate and here
41. NYT alarmism-gate and here
42. Overpeck get rid of MWP-gate
43. Oxbourgh-gate and here (bishop hill)
44. Pachauri-gate and here and here
45. Peer-review-gate 1
46. Peer-review-gate 2
47. Persecute and execute-gate
48. Polar bear-gate and here
49. Porn(soft)-gate
50. Rahmstorf smoothing-gate and here and here
51. Revelle-gate
52. Russia-gate and and here video
53. Solar-gate  Spain solar-gate
54. Sting-gate
55. Student dissertation-gate
56. Surface stations-gate and here
57. Toad-gate
58. UNEP-gate
59. UN natural disasters-gate
60. Ursus-gate
61. Windmill-gate and here
62. Wikipedia William Connelly-gate h/t to rechauffementmediatique.org
63. Yamal-gate 
      Come on in gate lovers we got lots of gates coming in!  We even got gates for a penny! Gates for only a penny! Come on in!
Hopefully this list will be helpful for the observers of climate science.
Share this...FacebookTwitter "
"

 **Presented by Daniel T. Griswold at the James and Margaret Tseng Loe Chinese Studies Center Conference, St. Vincent College, PA, on November 6, 2002.**



Let me confess up front that I am not a China expert. But one cannot talk about international trade and globalization for even a few minutes without addressing China. We are all students of China now. Today China has become one of the world’s major trading nations, and it is destined to grow more influential in the years ahead. 



My remarks today will address four aspects of the topic of China and international trade, what we might call “Chairman Dan’s Four Theses”: The Re‐​emergence of China as a Trading Nation; U.S.-China Commercial Relations Today; Answering the Critics of Normal Trade Relations; and Tilling the Soil for Human Rights.



If we were to travel back six or seven centuries, we would enter a world where China was the most advanced economy on earth and the most and dynamic force in Asian trade. China organized a professional Navy in 1232, with treadmill‐​operated paddle‐​wheelers and catapults that launched heavy stones. 



Marco Polo testified to the vigor of China’s international trade during his visits in the late 13th century. The commercial city of Hangzhou had 1 million residents by then, including a merchant class and uprooted refugees. The city embraced relative freedom, change, and travel, and was open to Arab and Hindu learning. The citizens of Hangzhou had a saying: “Vegetables from the east, water from the west, wood from the south, and rice from the north.” 



In those days, Chinese plied the Indian Ocean with fleets of ocean going merchant junks, 100 feet long and 25 feet wide, carrying 120 tons of cargo and 60 crew. Those ships visited Indonesia, Ceylon, and the west coast of India. By the 13th century, the Chinese had developed dry docks and gunpowder bombs–300 years before those were seen in the West. 



Beginning under Emperor Zhu Di, the Chinese launched seven official naval expeditions between 1405 and 1431 to Indonesia, India, Arabia, and East Africa. The expeditions were lead by the eunuch officer Zheng He. These “Treasure ships” were the largest in the world, 400 feet long by 160 feet wide (vs. 85 feet long for the Santa Maria). The ships were multi‐​decked, with nine masts and sails of red silk traversed with laths of bamboo for more durability and precise steering. Each ship carried hundreds of sailors and had 15 or more watertight compartments, and 60 cabins. At 7,800 tons of displacement, they were the largest ships in the world until those of the British Navy after 1800. In all, China built 250 such ships as part of a major shipbuilding program that would have been unimaginable in Europe at the time. 



The Treasure Ships were sent on huge trade missions. The first, in 1405, consisted of a fleet of 317 ships with 28,000 Chinese. What a sight that must have been! On a mission to Hormuz, in the Persian Gulf, Chinese traded porcelains and silks in exchange for sapphires, rubies, oriental topaz, pearls, coral beads, amber, woolens, and carpets, along with lions, leopards, and Arabian horses. But these were not market‐​opening missions, but more diplomatic in nature, a showing of the flag. No attempt was made to establish bases for trade or military objectives. The missions were very costly for the Chinese government and not profitable in a commercial sense. 



The bureaucratic mandarins, who detested commerce, soon prevailed over the rival eunuchs. At its peak in early 1400s, the great Ming navy consisted of 3,500 ships, but the number fell in half by 1440, and rapidly diminished after that. With the death of Zhu Zhanji in 1435, the new emperor recalled the fleets. In 1477, one of leading eunuchs called for writings of Zheng He to stimulate interest in naval expeditions, but the vice president of the ministry of war ordered them destroyed, calling them “deceitful exaggerations of bizarre things far removed from the testimony of people’s eyes and ears.” By 1500 it was a capital crime to build a ship with more than two masts. In 1525 coastal authorities were enjoined to destroy all ocean‐​going vessels, and in 1551 it was declared a crime to set sail in a multi‐​masted ship. 



In 1400, China was in every way superior to West: in technology, living standards, and global influence. But the country became enveloped in a smug self‐​sufficiency, cultural and economic inwardness, a closed and centralized political system, and an anti‐​commercial culture. In the 15th century, China turned its back on the world economy. It even abandoned naval defenses. Its highly educated elite was uninterested in Western technology and military potential. A British mission in 1793 brought 600 cases of presents, including chronometers, telescopes, a planetarium, chemical and metal products. Chinese officials rebuffed the foreigners, asserting that “there is nothing we lack‐​we have never set much store on strange or ingenious objects, nor do we want any more of your country’s manufactures.” 



So for more than 500 years, from the 15th to the 20th century, China’s economy slipped further behind the rest of the world. As late as 1820, the gross domestic product of China was still 30 percent higher than the total GDP of Western Europe and its settlements, but it was only one‐​twelfth the size by 1950. The Chinese economy was not open in the 19th century despite trade treaties and Western encroachment. Its trade was conducted in self‐​contained trade zones with little impact on the rest of China. The share of exports in China’s GDP was only 1.2 percent in 1913 at the height of pre‐​war globalization in the West. The Taiping Rebellion in the mid‐​19th century and World War II, Civil War, and communist convulsions in 20th devastated China’s economy. 



The economic reforms that began in late 1970s reversed 500 years of history. China’s trade with the rest of the world has grown from only $20 billion at the beginning of the reforms to more than $500 billion in 2001. China is now the world’s sixth largest exporter of goods and also the world’s sixth largest importer. In the past decade, China has reduced its average tariff from 43 percent to 15 percent, and those barriers will fall further as it implements the agreement it signed to join to World Trade Organization. So much for China being a closed economy! 



I believe the re‐​emergence of China as a trading nation is one of the most important and far‐​reaching developments in the last, oh, half millennium or so. After 500 years on the sidelines, China has rejoined the global economy.



Since China began to unilaterally open its market, the people of China and the United States have enjoyed a growing and mutually beneficial trade relationship. From practically nothing in 1980, two‐​way U.S.-China trade grew to more than $120 billion in 2001. China today is America’s fourth largest trading partner. In 2001, Americans imported $102 billion worth of goods from China while we exported $19 billion‐​leaving a bilateral trade deficit with China of $83 billion. 



Since 1980, the United States has allowed Chinese products to enter the U.S. market at the same tariff rates applied to our other trading partners. But the extension of so‐​called normal trade relations to China was always conditioned on the president granting a waiver to the Jackson‐​Vanik amendment (a relic of the Cold War that conditions trade with communist countries on their emigration policies). Each year congressional opponents of trade with China would try in vain to override the waiver, and in 2000 Congress made normal trade relations permanent to clear the way for China’s entry into the World Trade Organization. 



So what in the world do we buy from China? It’s a running joke with my kids that we cannot go to the store without buying something–clothing, toys, household goods–made in China. Three‐​quarters of what Americans import from China are toys and other miscellaneous manufactured goods: footwear‐​1 billion pairs of shoes a year‐​furniture, lighting fixtures, office machines, household electronics, electrical appliances, and clothing. Wal‐​Mart alone will import an estimated $12 billion worth of goods from China in 2002. Those goods mean lower prices, more choice, and more real income for American families. 



On a much smaller scale, China buys American‐​made aircraft, telecommunications equipment, scientific instruments, oil seeds and fruits, electrical machinery and appliances, data processing machines, and fertilizers. 



Why do we run such a large bilateral trade deficit with China? We are the world’s number one consumer society and China has become the world’s workshop for consumer goods, so it should be no surprise that we have become China’s best customer. On the other hand, we are the world’s leading high‐​end manufacturer, while China remains a relatively poor country. In sum, we are more willing and able to buy what the people of China make than they are willing or able to buy what we make. 



Despite warnings, the United States is not dangerously “dependent” on trade with China. Our imports to and exports from China remain a small fraction of our total trade. If anything, China is more dependent on trade with the United States than vice versa. Both our imports and exports with China are less than 10 percent of our total trade, but 38 percent of China’s exports go to the United States. If our trade relations were disrupted, by an outbreak of protectionism or a hot or cold war, both countries would suffer economically but China would suffer more. 



And despite the warning that U.S. factories will soon lock up and move to China, American investment in the mainland remains modest. At the end of 2001, American companies owned $7 billion worth of direct manufacturing investment in China. That is less than 2 percent of the total stock of U.S. manufacturing FDI abroad, and far less than the $35 billion in manufacturing investment American companies own in the tiny Netherlands, population 15 million. Annual outflows of manufacturing investment to China remain a tiny fraction of what American companies invest domestically in the U.S economy, and what the rest of the world invests in China. 



Criticism of U.S. trade with China takes two basic forms: that our trade with China, and by this the critics invariably mean what we import from China, threatens our national security, and that it threatens our economy. 



Let’s examine the national security argument first. The most legitimate concern about trade and national security is what we export to China. The U.S. government wields extensive powers to block exports to China of sensitive military and so‐​called dual‐​use technology‐​and the government should use that power when necessary. We should not be selling cutting‐​edge military technology to China that could then be sold to our enemies or turned against us in any way. But that is not what really bothers the critics of trade with China. What they object to are imports from China. They believe in a simple, what I would call a simplistic, formula that says: When we buy goods from China, China becomes richer, and the richer China becomes, the more it can fund its military to threaten American security. 



That was the conclusion this summer of the U.S.-China Security Review Commission. The commission was established by Congress in 2000 when it approved permanent normal trade relations. In its first annual report, the commission warns that, through our trade and investment ties with China, “we are strengthening a country that could challenge us economically, politically, and militarily.” 



“If China becomes rich but not free,” the commission warns, “the United States may face a wealthy, powerful nation that could be hostile toward our democratic values, to us, and in direct competition with us for influence in Asia and beyond.” 



The commission’s national security critique is fundamentally flawed, for at least two major reasons. First, while trade with the United States has been important for China’s development, it has not been the most important factor. Far more important has been China’s own internal liberalization, starting with its farm sector in the late 1970s, and then expanding to the privatization of its state‐​owned sector, repeal of price controls, and the unilateral opening of its economy to foreign competition. If the U.S. market were far less open to Chinese goods than it actually is, China would still have grown rapidly in the last 20 years, although not quite as rapidly as it actually has. 



Second, even if it were possible, through changes in U.S. trade policy, to put the brakes on China’s economic growth, would we even want to? From a humanitarian point of view, a dramatic slowdown in China’s growth would cause hardship for hundreds of millions of families and condemn millions of children to lives of perpetual poverty without hope for further education and upward mobility. And from a foreign policy point of view, a still‐​poor, stagnant, and frustrated China may be more unstable and hostile to American interests than a China that is advancing economically. In fact, a policy of disengagement from China could be self‐​fulfilling, creating the very enemy its proponents claim to be protecting us from. In sum, it would be cynical and foolish to stake our national security on a policy designed to keep 1 billion people isolated and poor. 



The other major criticism of trade with China is that is threatens America’s economy. Here the critics believe in an equally simplistic formula that says: Every widget we import from China means one less widget we make ourselves, which means a weaker U.S. economy and a potentially dangerous dependence on foreign widgets. And here too the argument against trade with China is fundamentally flawed. 



First, the types of goods we import from China are not important for the U.S. military. Recall the list of top imports from China: toys, shoes, clothing, office machines, household appliances and household electronics. American soldiers may be buying those goods at the local Wal‐​Mart or PX, but they are not being procured by the Pentagon. The China Security Commission warns that the U.S. steel industry may be jeopardized by Chinese imports, but the Commerce Department has already investigated the national security impact of steel imports and found no connection. 



Second, imports from China do not weaken the U.S. economy, cause unemployment, or threaten our industrial base. Imports strengthen our economy by raising real wages for families, providing lower‐​cost inputs for business, and spurring innovation and higher productivity through competition. Like technology, trade does cause certain industries to decline, thus eliminating some jobs, but it also creates new opportunities for wealth and job creation. In an economy with a reasonably flexible labor market, jobs eliminated by technology and trade will be fully offset by the creation of new jobs. 



A blatant example of overblown rhetoric about the trade deficit and jobs occurred on the eve of the vote on permanent normal trade relations in May 2000, during a segment of the NewsHour with Jim Lehrer on PBS. In summing up why the House should reject permanent normal trade relations with China in a vote the next day, AFL-CIO executive Richard Trumka asserted:



No one is saying isolate China. That’s the smoke screen they blow out because they don’t have the facts. Look, we have a $70 billion trade deficit with China. The U.S. International Trade Commission came out with a study yesterday [Monday, May 22] saying, if you give them permanent NTR status, two things will happen: We’ll lose one million jobs, and the trade deficit will increase.



Trumka’s sweeping claim offers a textbook example of how opponents of trade liberalization abuse trade deficit figures to serve their agenda. In fact, the U.S. International Trade Commission had issued no such study that week on trade with China. The commission’s most recent study on the impact of China PNTR had been released in August 1999, almost a year earlier, and it contained no estimate of job gains or losses. 



The actual source of the figure of one million jobs lost was a paper released the week before by the Economic Policy Institute, a union‐​aligned, non‐​profit organization. The EPI had used numbers from the 1999 USITC study to extrapolate an estimate of future bilateral trade deficits with China. It then crunched the hypothetical trade deficit numbers to estimate a total loss of almost 900,000 jobs during the next decade if Congress were to approve PNTR with China. But the EPI estimate of job losses was based on three whoppingly false assumptions. 



One serious error of the EPI study was to misapply the USITC’s estimates for the growth in China trade. The USITC study only offered a one‐​year, static estimate of the impact of Chinese tariff liberalization on the U.S. trade deficit. The ITC study didn’t even attempt to estimate the number of American jobs that would be created or eliminated by the further opening of the Chinese market. 



The EPI’s second crucial error was then to assume that rising imports from China automatically mean lost jobs in the U.S. economy. But rising imports need not and typically do not translate into a net loss of jobs. In fact, the growth of real goods imports and manufacturing output tend to be positively correlated. That is, as manufacturing output rises in the United States so too do imports of goods, adjusted for price changes. As with so many other economic indicators, the same economic expansion that spurs manufacturing output also attracts more imports and enlarges the trade deficit. 



Trade critics such as EPI wrongly assume that every import from China displaces domestic production, eliminating jobs in the economy. In reality, much of what we import from China, such as toys, shoes, and clothing, substitutes for imports from other low‐​wage producers. Another sizeable portion of our imports consists of intermediate inputs, which are then assembled into U.S.-made products by American manufacturers. That helps to explain why there is no correlation between rising manufacturing imports from China and falling manufacturing output. 



A third critical error of the EPI study was to consider the bilateral trade balance with China in isolation. While a change in trade policy can affect a particular bilateral deficit, the increased bilateral deficit tends to be offset by changes in other bilateral balances. The ITC study confirms this. The USITC estimated that China’s lower tariffs would cause America’s overall trade deficit to shrink slightly. Although America’s bilateral deficit with China would increase within the USITC’s limited model, our trade balance with other countries would “improve” enough to more than offset the increased deficit with China. The USITC estimated that America’s total exports would growth by $1.9 billion while imports would grow by $1.1 billion, decreasing the overall U.S. trade deficit by $0.8 billion. If you believe EPI’s own faulty methodology, the smaller overall U.S. trade deficit caused by China’s lower tariffs should lead to an increase in U.S. jobs, not a decrease. 



Trade with China is about more than jobs and incomes. Around the world, trade and the development it has spurred have created a more hospitable climate for civil and political freedoms. The economic openness of globalization allows citizens greater access to technology and ideas through fax machines, satellite dishes, mobile telephones, Internet access, and face‐​to‐​face meetings with people from other countries. Rising incomes and economic freedom help to nurture a more educated and politically aware middle class. People who are economically free over time come to want and expect to exercise political and civil liberty as well. Catholic social thinker Michael Novak identified this as the “Wedge Theory”:



Capitalist practices, runs the theory, bring contact with the ideas and practices of the free societies, generate the economic growth that gives political confidence to a rising middle class, and raise up successful business leaders who come to represent a political alternative to military or party leaders. In short, capitalist firms wedge a democratic camel’s nose under the authoritarian tent.



The interplay of economic openness and political and civil freedom is admittedly complex, and the question of causation remains unsettled, but the two phenomena are clearly linked in the real world. In the past 25 years, as an expanding share of the world has turned away from centralized economic controls and toward a more open global market, political and civil freedoms have also spread. Since 1975, the share of the world’s governments classified by Freedom House as democracies has risen sharply, especially since the late 1980s when globalization began to gather steam. Many of those new democracies are low‐ and middle‐​income countries that have simultaneously liberalized and opened their economies. 



When we compare countries according to their economic openness and their degree of political and civil freedom, the connection becomes even more evident. People who live in countries relatively open to international trade and investment are far more likely to enjoy full political and civil liberties than those who live in countries that are relatively closed. Among the top two quintiles of nations ranked according to their economic openness, 90 percent are rated “Free” by Freedom House and not a single one is rated “Not Free.” In the bottom quintile of openness (i.e. those with the most closed economies), fewer than 20 percent are rated “Free” and more than half are rated “Not Free.” In other words, countries that maintain a relatively open economy are more than four times more likely to be free of political and civil oppression than countries that remain closed. 



Recent decades have witnessed dramatic examples of how economic freedom and openness till the soil for civil and political reform. Twenty years ago, both South Korea and Taiwan were military dictatorships without free elections or full civil liberties. Today, thanks in part to economic growth and globalization, both are thriving democracies where citizens enjoy the full range of civil liberties and where opposition parties have won elections against long‐​time ruling parties. In Mexico, more than a decade of economic and trade reforms helped lay the foundation for the historic July 2, 2000, election of the opposition candidate Vicente Fox, ending 71 years of one‐​party rule by the PRI. Internal economic reforms and the North American Free Trade Agreement helped to undermine the dominance of the PRI over Mexican political life. Alejandro Junco, publisher of the opposition newspaper Reforma, noted after the PRI’s historic defeat, “As the years have passed, and with international mechanisms like NAFTA, the government doesn’t control the newsprint, they don’t have the monopoly on telecommunications, there’s a consciousness among citizens that the president can’t control everybody.” 



While genuine political reform has been absent so far in China, and dissent is still brutally suppressed, economic reform and globalization give reason to hope for political reforms. After two decades of reform and rapid growth, an expanding middle class is experiencing for the first time the independence of home ownership, travel abroad, and cooperation with others in economic enterprise free of government control. The number of telephone lines, mobile phones, and Internet users has risen exponentially in the past decade. Tens of thousands of Chinese students are studying abroad each year. 



China’s economic reforms have opened the door for religious witnessing. More than 100 Western missionary organizations are active in China. Those organizations have distributed millions of Chinese language Bibles in China. Thousands of Christian workers who are tent‐​making as English teachers and in other occupations are able to minister to the growing body of believers in China. All this would have been unthinkable 25 years ago when China was still isolated from the global economy. 



All this must be good news for individual freedom in China, and a growing problem for the government. A recent study by the Chinese Communist Party’s influential Central Organization Department noted with concern that “as the economic standing of the affluent stratum has increased, so too has its desire for greater political standing.” The study concluded that such a development would have a “profound impact on social and political life” in China. 



Globalization and economic development do not guarantee political reform in China or anywhere else, but the track record of economic engagement is far more promising than the failed record of sanctions and economic isolation. Four decades of an almost total U.S. embargo against Cuba have yet to soften Fidel Castro’s totalitarian rule. Sanctions against Burma (a.k.a. Myanmar) have only worsened the condition of the very people we are trying to help without bringing any progress toward democracy and freedom. The folly of imposing trade sanctions in the name of promoting human rights abroad is that it deprives people in the target countries of the technological tools and economic opportunities that can help to free them from tyranny. 



For the past two decades, globalization, human rights and democracy have been marching forward together, haltingly, not always and everywhere in step, but in a way that unmistakably shows they are interconnected. By encouraging more trade and market liberalization in China, we not only help to raise growth rates and incomes, promote higher standards, and feed, clothe and house the poor; we also spread political and civil freedoms. 



President Bush, in The National Security Strategy of the United States of America document released in 2002, wrote that, “Chinese leaders are discovering that economic freedom is the only source of national wealth. In time, they will find that social and political freedom is the only source of national greatness.” Opponents of trade with China see the rising incomes and falling poverty of hundreds of millions of people as a threat to our security and well‐​being. Instead, we should see China’s rising prosperity as an immediate blessing for mankind. And we should understand that trade offers the best hope that China will one day join the community of nations that are free and democratic just as it now seeks to join those that are open and prosperous.
"
"
Share this...FacebookTwitterA new international agreement to replace the Kyoto environmental protocol will not be signed in 2010, the Russian presidential advisor on climate change has said. Read more:
http://en.rian.ru/Environment/20100426/158743966.html
On April 16, Russian President Medvedev said:
All countries, including developed and developing economies, should reach an agreement, or, if we do not agree on this [the common terms of carbon emissions reduction], Russia will not prolong its participation in the Kyoto agreement – you cannot have it both ways.
http://en.beta.rian.ru/Environment/20100416/158607110.html
Share this...FacebookTwitter "
"

Foreign policy has been a contentious issue for libertarians since September 11, 2001. There have been countless harangues in Washington bars and policy salons over the past five years about libertarianism and the Iraq War, and the topic has been so divisive for libertarians that even Rose and Milton Friedman disagreed. She was in favor and he against, with Rose noting later: “This is the first thing to come along in our lives, of the deep things, that we don’t agree on.We have disagreed on little things … but big issues, this is the first one!”



Why has the war — and post‐​9/​11 foreign policy generally — been so controversial for libertarians? And now, more than six years after 9/11 and more than five years into the war in Iraq, what can libertarian insights tell us about how we got here and what to do next?



To try to answer these questions, we should begin with some libertarian starting points about government and then review the debate over the Iraq war and foreign policy more generally in the wake of 9/11.



Then we can consider where to go from here, and what a counterterrorism policy that paid heed to libertarian insights would look like.



 **Government: Dangerous at Home, Beneficent Abroad?**



Nation‐​states are self‐​interested collective organizations, both at home and abroad. As public choice economists tell us, the first interests the state looks after are the state’s — not the people’s. Quite often, the state’s interests are served by war.



War historically has been the most effective generator of big government. As Bruce D. Porter observed in his book _War and the Rise of the State_ , the nonmilitary sectors of the federal government grew at a faster pace during World War II than they did under the New Deal. War creates the perfect climate for the collectivist mentality, as well as ready‐​made occasions and arguments for expanding the power of the national state.



In the international arena, it is important to note that security — the first‐​order concern of any state — is ultimately contingent on a state’s ability to defend itself. Decisions about national policies are based on how threatening a state views the international environment. Overall, security is scarce, and history tells us that states are competitive and leery of any state that grows too powerful and/​or throws its weight around. The concentration of military power in the hands of one actor in the international system can cause fear, particularly if that state appears intent on overturning the existing balance. It was for that reason that Thomas Jefferson wrote in 1815 of his desire that nations “which are overgrown may not advance beyond safe measures of power, [and] that a salutary balance may be ever maintained among nations.”



In recent years the United States has upset the world’s balance. Countries assess threats on the basis of capabilities and intentions, and the U.S. government at present appears to have enough of both to alarm other governments. Washington spends roughly as much on its military as does the rest of the world combined, and political leaders in both parties argue that we need a military significantly bigger. At the same time, in addition to the attack on Iraq, American leaders have begun to openly discuss their intentions of unraveling the international order. During a June 2007 speech to the Economic Club of New York, Secretary of State Condoleezza Rice argued that “America has always been, and will always be, not a status quo power, but a revolutionary power.” Thus we should not be surprised when we encounter fear and distrust from Berlin to Beijing.



What is most peculiar about this state of affairs is that the United States sits unchallenged atop the international order, with an unparalleled ability to shape it and with any potential peer competitor several decades away. This state of affairs is hugely beneficial to us; imperfect though it is, the United States should be working to _preserve_ , not overturn, the existing international order. But some observers, including a few libertarians, seem to have concluded that the threat from terrorism is so great that the United States must embark on radical social engineering projects abroad to combat it.



 **What Changed after 9/11 — and What Didn’t**



Despite the preeminent position of the United States in the international order, many American political leaders and thinkers — including some libertarians — embraced aggressively interventionist foreign policies after 9/11. The threat of international terrorism, primarily from al Qaeda, was broadened to include the nation‐​state of Iraq. President George W. Bush argued that an effective strategy for fighting terrorism must include regime change in Iraq in order to transform the social and political culture of the Middle East.



Most libertarians questioned those moves. Some embraced them.



Perhaps the most prominent libertarian to advance these ideas has been Randy Barnett, a nonresident senior fellow of the Cato Institute and professor of law at Georgetown University. Barnett published an op‐​ed in the _Wall Street Journal_ in July 2007 criticizing noninterventionist libertarians for failing to understand that “libertarian first principles … tell us little about what constitutes appropriate and effective self‐​defense after an attack.” He argued that libertarians can and should think of the attack against Iraq as appropriate self‐​defense in response to 9/11. Further, he argued that libertarians should favor “a strategy of fomenting democratic regimes in the Middle East.”



Such radical government programs could only be endorsed by a libertarian _in extremis_. But there was never reason to believe Iraq was either responsible for 9/11 or plotting the next one. The Iraqi government was not involved in 9/11, and attacking it devoted scarce resources to the wrong target. The appropriate response to the newly prominent threat of nonstate terrorism was to concern ourselves more with nonstate terrorist groups, which do not have return addresses and frequently cannot be deterred. To lump in states — whose relations with each other were largely unchanged by 9/11 — with such groups is to confuse different types of problems.



Barnett himself wrote in his 1998 book _The Structure of Liberty_ that libertarian conceptions of self defense are limited to _imminent_ attacks, a limitation that Barnett deemed “well‐​founded … because of the enormous knowledge problem that would be confronted if we were to permit selfdefense actions prior to a threat becoming imminent.” Barnett warned readers further that “every erroneous and unjust use of violence threatens to induce resentment, bitterness and the desire on the part of those against whom violence is used to rectify this injustice by responding violently, thereby setting off a cascade of violence.”



One could apply those insights to the war in Iraq. The U.S. government attacked Saddam’s regime in the absence of any imminent threat, and it seems that we indeed induced a significant amount of resentment, bitterness, and desire for vengeance by starting the war. (The debate over whether the intelligence supporting the case for war resulted from governmental incompetence or malfeasance — and neither explanation should confound a libertarian — is irrelevant.)



In his _Wall Street Journal_ article, Barnett admits supporting the war even though he believed that it would go poorly. He concedes that “to a libertarian, any effort at nation building seems to be just another form of central planning which, however well‐​motivated, is fraught with unintended consequences and the danger of blowback” and that he is “disappointed, though hardly shocked, that the war was so badly executed.” A critic of the decision to go to war might then ask why one should support a war you expect to go badly. And given that the objective of the war was a massive social engineering project unprecedented in scope — the destruction and reformation of a regional order — how could libertarians have envisioned it going any other way than poorly?



Indeed, how is it simultaneously possible to oppose government involvement in education or health care on the grounds of the inherent lack of necessary knowledge, but believe that the federal government could invade Iraq and then unravel and reweave the fabric of a thousands‐​year‐​old society whose language we do not speak and whose tribal and confessional allegiances we do not understand? Following the insights of thinkers such as F. A. Hayek, libertarians are deeply skeptical that governments could collect and sort enough data to plan government health care or education effectively. Surely those difficulties are compounded when the goals are even more ambitious and the policies are conducted in foreign countries wracked by sectarian conflicts.



The _Atlantic’s_ Matthew Yglesias observed the debate among libertarians over the war and judged that “the notion that anything even remotely resembling libertarianism could underwrite an effort to conscript huge quantities of resources from the American public and deploy them in an attempt to wholly remake the social and political order in a foreign country is too absurd to merit a rebuttal… . It’s coercion, it’s planning, it’s every non‐​libertarian thing under the sun.”



The policies that libertarian hawks have supported have cost more than half a trillion dollars and four thousand American lives — greater than cost of the 9/11 attacks themselves. (Libertarians also should not ignore the violations of individual rights that occurred in the form of the hundred thousand or so Iraqis who perished as a result of our political science experiment in their country.) Government power, unchecked by prudence or other constraints, can do great harm not only to foreign targets, but also to the very citizens that the government is charged with protecting. To craft an effective response to the terrorist threat, it is necessary to dispassionately assess the nature and scope of the threat.



 **Getting Threat Assessment and Response Right**



The very real problem of terrorism can be handled without massive nation‐​building projects in the Middle East. In fact, the biggest successes in fighting terrorism since 9/11 have been achieved through cooperation with foreign intelligence services and police agencies. Precious few meaningful victories against terrorism, by contrast, can be ascribed to the government’s tinkering with Iraq.



My colleague Benjamin Friedman observes that even in 2001, the flu killed more than 10 times as many Americans as did terrorism. Certainly past performance is no guarantee of future results, and one can conceive of improbable scenarios that would radically expand the destructive capacity of terrorists (their acquisition of a nuclear weapon, say). But to date, the government’s nation‐​building‐​as‐​counterterrorism approach has been more destructive and wasteful than terrorism itself and has done little to diminish the problem. In fact, there is ample evidence that terrorists realize that the best way to inflict harm on America is to trick us into responding in ways that harm ourselves.



Osama bin Laden boasted in 2004 that it is “easy for us to provoke and bait this administration.” Describing his desire to “bleed America to the point of bankruptcy,” bin Laden remarked, “All that we have to do is to send two mujahedeen to the furthest point east to raise a piece of cloth on which is written ‘al Qaeda,’ in order to make generals race there to cause America to suffer human, economic and political losses.”



Instead of allowing ourselves to be goaded into self‐​destructive responses, we should review our diagnosis, our prescription, and our prognosis. In pursuing an accurate diagnosis, we must confront a painful truth that study after study has revealed: U.S. foreign policy plays a significant role in public opinion in the Islamic world — and as a result, represents a big part of our terrorism problem. As a 2006 Government Accountability Office report noted, “U.S. foreign policy is the major root cause behind anti‐​American sentiments among Muslim populations and … this point needs to be better researched, absorbed, and acted upon by government officials.”



The Pentagon’s Defense Science Board was less diplomatic, writing in 2004 that “Muslims do not hate our freedom, but rather, they hate our policies.” Bin Laden himself argued in 2004 that “contrary to what Bush says and claims — that we hate freedom — let him tell us then, why did we not attack Sweden?”



Of course, not every terrorist is motivated by rage at U.S. foreign policy. There are clearly a small number of terrorists who carry out murders for other reasons. It should go without saying that the only viable policy approach toward committed terrorists — no matter their motivation — is to pursue them and capture or kill them in cooperation with foreign intelligence services and, in some cases, with the limited use of American military power. But our strategy should not be solely reactive. There are a vast number of people who may be receptive to bin Ladenism but aren’t yet convinced they should join him. And by far the most effective recruiting tool in al Qaeda’s arsenal is the notion — alarmingly widely accepted in the Muslim world — that America’s actions prove we are out to destroy Islam.



Accordingly, to treat the problem we need to focus more on the question of how we can better affect the marginal terrorist recruit. What makes him or her more or less likely to join the cause? Wouldn’t removing bin Laden’s best recruiting tool be helpful? The other side of the coin is that al Qaeda’s remarkable barbarity has been a public relations disaster in the Islamic world. Very few people — far fewer than support relatively liberal governance — express any desire to be governed by people like al Qaeda. Shibley Telhami, one of the leading pollsters of the Islamic world, testified to Congress in 2005 that al Qaeda’s support in the Arab world stems disproportionately from its opposition to U.S. foreign policy. Of the Arabs in Telhami’s poll expressing support for any of al Qaeda’s aims, only 6 percent supported the group’s objective of creating a Taliban‐​style state.



Al Qaeda can’t sell an affirmative agenda; what it can sell is opposition to U.S. foreign policy. A smart approach to counterterrorism would recognize that fact and avoid providing bin Laden and his comrades with opportunities to pose as the defenders of Islam against a hostile, colossal, anti‐​Islam United States.



Now for the prognosis. It is time to take a deep breath and recognize the strength of our system. Liberal capitalism is the best means for organizing human activity. It provides for the most flourishing, it provides for the most technological innovation, and it has the strength to endure through time. During the Cold War, alarmists warned constantly about the durability of the Soviet system, insisting that it was, in many ways, stronger than our own. They were proved fantastically wrong when the sclerotic Soviet state collapsed in a shambles in 1991. To respond to the band of fanatics we face today with hysteria does not befit a great nation of our size and vitality.



Hollow though it was, Soviet communism was a far more dangerous force than Islamic terrorism. The system that withstood the challenge of communism can similarly survive the threat from Islamic terrorists. As mentioned above, the style of governance that al Qaeda and its affiliates can offer to Muslims around the world is exceedingly unpopular. Earlier in the Bush administration, citizens of Arab countries held surprisingly favorable views of American freedom and the American people, although those figures have declined substantially. What becomes clear from the data, however, is the overwhelmingly negative view of U.S. foreign policy in the Islamic world. Putting our best face forward and emphasizing the positive features of the United States will go a long way to repairing our poor position in the world. As George F. Kennan wrote in his 1993 memoir, the United States must “never lose sight of the principle that the greatest service this country could render to the rest of the world would be to put its own house in order and to make of American civilization an example of decency, humanity and societal success from which others could derive whatever they might find useful to their own purposes.” 



We have lost sight of this principle. But in the months and years to come, we should refocus and take solace in the fact that certain important and basic truths remain unchanged. Our system is strong; bin Laden’s is weak. We are wealthy; al Qaeda is poor. We have greatly influenced the structure of the world order; they can only affect it by provoking reaction. The best thing to do now is to jealously guard our strength, not squander it; to keep and hold our quiet confidence, not panic; and to pursue this new breed of enemy with the prudence and wisdom of a mature nation.



The political scientist Hans Morgenthau wrote in _Politics among Nations_ that “throughout the nation’s history, the national destiny of the United States has been understood in antimilitaristic, libertarian terms.” This fact is linked to the rugged individualism of the American founding and the kernel of libertarianism that lies at the heart of the nation even today. Those who would jettison the antimilitarism would also jettison the libertarianism, compounding the tragedy.



Before his death in 2006, Milton Friedman lamented that his life’s project of limiting government power was “being greatly threatened, unfortunately, by this notion that the U.S. has a mission to promote democracy around the world,” pointing out: “War is a friend of the state… . In time of war, government will take powers and do things that it would not ordinarily do.” It is for precisely that reason that libertarians, more than anyone, should not be friends of war.



 _Justin Logan is associate director of foreign policy studies at the Cato Institute._
"
"
Share this...FacebookTwitterScientists continue to publish papers revealing no unusual climate trends for the last several centuries in many regions of the world.
Despite the 135 ppm increase in CO2 concentration (275 ppm to 410 ppm) since the 1700s, a new 250-year temperature (precipitation) reconstruction (Peng et al., 2020) shows there has been no net warming in Central Asia since 1766. Two other reconstructions from this region also show no warming trend in recent centuries.

Image Source: Peng et al., 2020
Earlier this year we highlighted a new study that indicated France was up to 7°C warmer than today about 7800 years ago after cooling by 3°C in the last 200 years.
Another new study (Esper et al., 2020) suggests there has been no net warming in Spain since 1350 A.D.
The years that spanned 1474-1606 A.D. scored 7 of the 10 warmest years in the record. In contrast, there has been only 1 warmest year (1961) and 4 of the 10 coldest years since 1880.
The 2 warmest 30-year (climate) periods occurred in the decades surrounding the ~1530s and ~1820s.
The authors record a “striking” and abrupt (within decades) 1°C warming trend during the late 1700s to early 1800s that exceeds any temperature change in the modern record.

Image Source: Esper et al., 2020
Share this...FacebookTwitter "
"
Dr. Roger Pielke Sr. has started up his Climate Science blog again, sans comments, as an informational source only. This is some very good news. See his post below.
November 27, 2007: Climate Science Is Relaunching As An Information Source
As a result of very positive encouragement from many Climate Science readers, I have decided to relaunch the website. The format will be different than in the past, however, in that comments will not be permitted. The posting of information will not be on a schedule, but when new information on a climate science issue is available that is otherwise not very visible, or has been misrepresented in the media.
The presentation of climate science in the media, unfortunately, remains biased, as has been documented numerous times on Climate Science. Thus, I have decided to reenter this mechanism of providing information. While comments will not be permitted on the website, guest presentations will be invited when there is value in providing this source of information.
Climate Science will thus provide a source of information on climate that, hopefully, will be useful to others, as part of a much needed effort to provide a balanced view of climate science.
Thanks to those who have found my website of value and take the time to read it!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea298c767',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea35292fa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Got a person on your christmas list that is fully deserving but can’t find that lump of coal at K-Mart at the last minute? Thanks to the good folks at Free Carbon Offsets, you too can join the ranks of the carbon purified. Just visit: www.freecarbonoffsets.com  and you can print your own Carbon Offset Certificate suitable for a stocking stuffer for the most deserving person on your Christmas list.

Merry Christmas Everyone! 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1e71ffe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’m taking a few days off blogging for friends and family. Seeing the pandemonium that has gripped the streets of my town while people scurry about trying to pull off that last minute shopping, and seeing my in-laws already getting into a small accident (even though they tried to avoid the traffic glut) and seeing local cycling enthusiast Ed McLaughlin get into a serious biking accident reminds me to remind you to drive, bike, and walk safely this holiday.
Watch for ice, it’s cold out there, don’t slip like the polar bear!
When I return I’ll have an update on my Stevenson Screen paint experiment.
In the meantime I wish each and every one of you a joyous Christmas holiday and I want to thank all of you for the help and support this year on this blog and in the www.surfacetstations.org project.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1cb16ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 A new paper published  by the Astronomical Society of Australia titled:
Does a Spin–Orbit Coupling Between the Sun and the Jovian Planets Govern the  Solar Cycle?
contains a warning about earthly climate change not immediately obvious from the abstract:

Based on our claim that changes in the Sun’s equatorial rotation rate are  synchronized with changes in the Sun’s orbital motion about the barycentre, we  propose that the mean period for the Sun’s meridional flow is set by a Synodic  resonance between the flow period (~22.3 yr), the overall 178.7-yr repetition  period for the solar orbital motion, and the 19.86-yr synodic period of Jupiter  and Saturn.
According to an interview with Andrew Bolt, of the Australian Newspaper, Herald Sun, Ian Wilson, one of the authors explained:
It supports the contention that the level of activity on the Sun will  significantly diminish sometime in the next decade and remain low for about 20 –  30 years. On each occasion that the Sun has done this in the past the World’s  mean temperature has dropped by ~ 1 – 2 C. 
###
Hmmm, I’m not sold on this idea. This is a lot like what Dr. Theodor Landscheidt proposes. I have a little bit of trouble understanding how the “mass at a distance” gravitational effects of Jupiter and Saturn could have much effect on the solar dynamo.

I’m sure both my readers, and Dr. Leif Svalgaard, who regularly monitors this blog, will have something to add to provide additional insight. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e396d70',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In my recent op-ed for _The Hill_ examining the Obama administration’s estimation of the social cost of carbon (SCC)—a measure of how much future damage is purportedly going to be caused by each ton of carbon dioxide that is emitted through human activities—I identified two major problems with their measure.   
  
First, the administration’s SCC was based on an estimate of _global_ rather than domestic damages from anthropogenic climate change—an odd scope for a measure designed to be incorporated in the cost/benefit analysis of U.S. rules and regulations governing domestic activities (such as the energy efficiency of microwave ovens sold in the United States). In fact, Office and Management and Budget (OMB) guidelines state that   




Your analysis should focus on benefits and costs that accrue to citizens and residents of the United States. Where you choose to evaluate a regulation that is likely to have effects beyond the borders of the United States, these effects should be reported separately.



Instead of “reporting separately,” the administration’s SCC embodies “effects beyond the borders of the United States.”   
  
Second, the administration recently revised (upwards) its initial calculation of the SCC. In doing so, it included updates to its underlying economic/climate-change/damage models, but it did not include any updates to the characteristics of the equilibrium climate sensitivity used by the models. Since the equilibrium climate sensitivity is the _key factor_ in how much climate change will result from a given amount of anthropogenic carbon dioxide emissions, and since there is mounting scientific evidence that the equilibrium climate sensitivity is better constrained and lower than that used in the initial analysis, there is no defensible reason why the new science was not included in the administration’s revised SCC calculation.   
  
So that’s two strikes against it.   




What I didn’t go into in my op-ed, because it is a rather complicated topic, is the choice of discount rate used in the administration’s SCC analysis. The discount rate, generally put, reflects how much you are willing to pay now to avert future damages. The lower the discount rate, the more costly (in today’s dollars) future damages become. The same OMB guidelines mentioned above also cover the selection of the discount rate to use in cost/benefit analysis. The OMB guidance is that as a default an analysis should use a 7 percent discount rate as the base case, and to show the sensitivity of the results to the discount rate assumption, the analysis also should include the results of using a 3 percent discount rate.   
  
The administration ignored that guideline as well. Instead it opted to determine the SCC using discount rates of 2.5, 3, and 5 percent, and didn't include results for a 7 percent rate—results that would have indicated a _substantially_ reduced cost of future damages.   
  
With now three strikes against it, the administration’s determination of the social cost of carbon should be tossed out.   
  
The door to doing so has just been opened slightly with the announcement that the Department of Energy is opening for public comment a Petition for Reconsideration of its use of the administration’s newly figured and newly increased SCC in its above-mentioned microwave oven energy efficiency rule. We’ll see what becomes of that.   
  
In the meantime, here's an example of how the SCC is currently being used and abused in the justification of new regulations. Economist Robert Murphy (who recently testified to Congress as to the problems with the administration’s SCC methodology) posted this gem from the Environmental Protection Agency discussions of a proposed new rule regulating discharges from steam electric power plants (and how the rule may impact carbon dioxide emissions from the plants):   






Murphy comments:   




As the above table shows, EPA is being a dutiful federal agency, following Executive Branch guidelines on how to calculate costs and benefits—it reports its findings using both a 3 percent and a 7 percent discount rate. Yet as the footnote explains, when reporting the benefits of reducing CO2 emissions, the EPA actually _can’t_ use a 7 percent discount rate, because an estimate of the SCC (social cost of carbon) for a 7 percent rate is “not available.” _Why_ is it not available? Because the [administration’s] Working Group explicitly ignored the OMB guidelines, and only reported the figures for 3 percent and 5 percent.   
  
We thus have an absurd situation, in which EPA and other regulatory agencies will be following the rules and calculating benefits and costs at both the 3 percent and 7 percent discount rates. Yet, when they express the “social benefits” of reducing greenhouse gas emissions at the 7 percent rate, they are actually going to plug in the wrong number, and explain in a footnote why they are doing so. To repeat, this is important, because the “right” number would show that there are virtually _no_ “social benefits” from reducing greenhouse gas emissions.   
  
As I have explained elsewhere, there are far more problems to the Obama Administration’s computer-model-case against carbon, than just the choice of discount rate. Yet the knots into which the federal government has tied itself, in order to avoid revealing the truth about the actual economic literature, is quite revealing—not to mention hilarious.



The administration’s SCC is a devious tool designed to justify more and more expensive rules and regulations impacting virtually every aspects of our lives, and it is developed by violating federal guidelines and ignoring the best science.   
  
All around bad news.


"
"
While I was on my week long road trip to survey weather stations and visit the National Climatic Data Center in Asheville, NC last week, I encountered lots of signs. Restaurant signs, road signs, signs from above, you name it. I think I must have passed 100 Bojangles restaurants and/or road signs. Bojangles is a popular southern chicken and biscuits restaurant. One though, really got my attention.

But please read on for the real “mother of all signs” I encountered.

Then there was Cracker Barrel and Waffle House…

Cracker barrel has an interesting marketing slogan on their supply trucks:

Yeah, I drove some of those….
On good advice from my readers, I avoided every one of these I saw:

This place is the southern version of Bob’s Big Boy and Frisch’s, good eats and a great breakfast bar. I didn’t try the wine though.

There were some other restaurant signs that I didn’t quite understand…

And there were some signs that I often wished I had for use when moderating this blog:

Then there were some signs that really spoke to the mission I was on:

And then there were others that I encountered that didn’t have a hint of southern hospitality at all…

I saw a lot of these at gas pumps, and given how I feel about biofuels, I drove to the next stations where I didn’t have to burn food to finish my trip:

I had mentioned that after surveying too many weather stations at sewage treatment plants that I needed a long shower, but when I saw this while I briefly toyed with the idea, I just didn’t see how it would change anything. I’d just be trading one smell for another.

There was one sign though that left a lasting impression on me, and it requires just a little bit of explanation.
When I was driving in Northwestern North Carolina, I went through many small towns and country roads that had small churches, I must have passed 200 during my trip. One thing I noticed is that pastors in these towns tend to try to out do each other with sermon topics on their front signs. I’d drive into a little town, and I’d see one sign advertising salvation, the next would have salvation plus breakfast, the next might have salvation, confession, a quote from the Bible, and a spaghetti feed, while the fourth might just have a zinger that would put all the others to shame.
This was one of those:

Now a caveat, the mountain road I was traveling on when I saw this had no shoulder and there was a semi truck right behind me. I looked for a place to pull over and turn around, and didn’t see one coming up, I couldn’t even pull over to let the truck pass. Five miles later I was still stuck and gave up on the idea.
Today I Google image searched to see if I could find an image where I could recreate the message, and found this neat web site called the Church Sign Generator, the output of which you see above. The sign and message was real, and didn’t look much different from this example except the top had something about Sunday’s service which I didn’t include because I didn’t get a good look at it.
We should all take this message home with us and live it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9cc70b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman libertarian site Die Freie Welt reports that German Extinction Rebellion activists protested against short-haul flights at several German airports earlier this week.
Lübeck / Munich
In Lübeck, 10 people tried to glue themselves to the runway with superglue, but the police were able to prevent the action. In Munich, people chained themselves to luggage trolleys.Even political figures took part, for example Lorenz Gösta Beutin, climate policy spokesman of Die Linke (The Left)  accompanied the protests in Lübeck, reports Freie Welt. “100 to 150 additional demonstrators protested outside the airport building.”
Düsseldorf
In Dusseldorf, one person even managed to get through security and onto a plane for Hamburg. The protester claimed to be a climate saver and shouted: “Please stop this plane. I am not prepared to fly. I want to get off. I am not prepared to sit down again.”
Many annoyed passengers reacted angrily. One passenger shouted, “Hey you arse—, sit back down”.  The activist claimed to represent everyone and that the issue was not about him.
“The earth is getting hot”, he declared, “Our mother nature is going to hell.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to witnesses, the activist tried to continue reading his speech, which he had written down, and stammered about “the end of the dear world”. The pilot was forced to to return the aircraft to the parking position so that the passenger could get off.
Berlin
In Berlin another activist made her way onto a plane and demanded passengers “disembark and stay on the ground” in order to keep domestic German flights from heating up the planet. “We are all threatened by an ecological collapse.”

BREAKING: Flugzeug in #Berlin #Tegel durch #ExtinctionRebellion Aktivisti gestört!!!Die sich täglich verschärfenden #Klimakrise weiter durch innerdeutsche Flüge anzuheizen ist Wahnsinn – Zeit auszusteigen und am Boden zu bleiben!#LuftBlock #ActNow #RebelForLife #stayGrounded pic.twitter.com/fevcjIAlse
— Extinction Rebellion Berlin 🌍 (@XRBerlin) August 17, 2020

The Freie Welt refuted the claims made by the Düsseldorf protestor, writing:

The figures actually say something completely different: domestic flights within Germany are responsible for just 0.3 percent of Germany’s CO2 emissions. In contrast, car traffic alone accounts for 20.8 percent. But you can imagine how motorists will react if activists stick themselves to the autobahn with super glue.”


		jQuery(document).ready(function(){
			jQuery('#dd_8a0034c297c24f880c76cad0fcfcb63f').on('change', function() {
			  jQuery('#amount_8a0034c297c24f880c76cad0fcfcb63f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"
Share this...FacebookTwitterAs recently as 2000 to 1000 years ago, spanning the Roman to Medieval Warm Periods, East Antarctica was 5-6°C warmer than it is today. The consequent ice melt resulted in >60 meters higher water levels in East Antarctica’s lakes.
East Antarctica has been rapidly cooling in recent decades, with magnitudes reaching -0.7°C to -2.0°C per decade since the mid-1980s (Obryk et al., 2020).

Image Source: Obryk et al., 2020
A new study (Myers et al., 2020) reports that until about 15,000 years ago and throughout the Last Glacial Maximum, East Antarctica was 4-9°C colder than it is today.
Antarctica then abruptly warmed 15°C within centuries. From 12,000 to 6,000 years before present, East Antarctica was about 5°C warmer than it is today.

Image Source: Myers et al., 2020
And then as recently as 2,000 to 1,000 years ago, East Antarctica was so warm (~6°C warmer than present) that its lakes were filled with 60 to 80 meters more meltwater than exists in lake basins today.
“Resistivity data suggests that active permafrost formation has been ongoing since the onset of lake drainage, and that as recently as 1,000 – 1,500 yr BP, lake levels were over 60 m higher than present. This coincides with a warmer than modern paleoclimate throughout the Holocene inferred by the nearby Taylor Dome ice core record. …  Stable isotope records from Taylor Dome (located roughly 100 km west of the MDVs) indicate mean annual air temperatures ca. 4-9 °C lower than modern during the LGM (Steig et al., 2000).”
“Between 12,000–6,000 yr BP, Taylor Dome ice core record indicates that regional temperatures were up to 5 °C warmer than modern conditions (Fig. 2) (Steig et al., 2000).”
“Permafrost age calculations indicate late Holocene lake level high-stands (up to ~81 masl, 63 m higher than modern Lake Fryxell) roughly 1.5 to 1 ka BP that would have filled both Lake Fryxell and Lake Hoare basins (Fig. 3b). …  Taylor Dome ice core records show a highly variable Holocene, with short lived peaks up to + 6 °C above modern temperatures between 1-2 ka BP (Steig et al., 2000).”
“Lake levels were higher potentially during and after the LGM when an ice dam blocked the mouth of TV, allowing for lake levels to increase by over 280 m compared to modern level. Taylor Dome ice core records indicate an abrupt warming of >15 °C from 15 – 12 ka BP, (Steig et al., 2000), which may have coincided with the maximum lake level of GLW.”
“Short lived changes in temperature such as a 6 °C increase in the late Holocene could have resulted in anywhere between 60 to 80 m of lake level rise and subsequent drawdown.”
This substantial regional warmth can also be verified by the 1,000-year-old elephant seal remains that document a time when Antarctica was sea ice free 2,400 kilometers south of where sea ice free conditions occur today (Koch et al., 2019). Elephant seals require sea ice free conditions to breed, and the same locations where they used to breed during the Medieval Warm Period are today buried in sea ice.

Image Source: Koch et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_9e221820cc7def01ad3e9985c5250b19').on('change', function() {
			  jQuery('#amount_9e221820cc7def01ad3e9985c5250b19').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




The figure below is a portion of a screen capture from the “Heat-Related Deaths” section of the EPA’s new “Climate Change Indicators” website. It is labeled “Deaths Classified as ‘Heat-Related’ in the United States, 1979–2010.”   
  
We don’t know anyone who could look at this chart and not be left with the strong impression that heat-related deaths in the United States are on the rise—apparently confirming the president’s concern about climate change and underscoring his desire to do something about it.   






  
  
But notice the asterisk at the bottom of the box. Here's the text associated with it:   




Between 1998 and 1999, the World Health Organization revised the international codes used to classify causes of death. As a result, data from earlier than 1999 cannot easily be compared with data from 1999 and later.



In other words, you shouldn’t plot pre- and post-1999 data on the same chart because the data are not comparable, lest you mislead the uninitiated reader. The EPA ignores its own warning and instead plots the two sets of not-easily-compared data side by side on the same chart, ensuring that they are compared!   
  
Such an analysis would probably grade out as an F in an undergraduate paper, but perhaps the EPA is suffering from a bit of “Noble Cause Corruption.” After all, they are trying to save us from certain death.   
  
The proper way to view the EPA chart is to put your hand over the data points on the right-hand side of the chart (1999 and onwards) and then over the data points on the left-hand side of the chart (pre-1999 data). In doing so, you’ll see that during both periods the rate of heat-related mortality does not rise.   
  
For those who want a clearer image of the truth when it comes to the effect of global warming on trends in heat-related mortality across the United States, see the figure below, taken from a brand new study by Jennifer Bobb from the Harvard School for Public Health and colleagues. The graph shows the number of heat-related deaths (for every thousand overall deaths) that result from the daily temperature being 10°F above normal, from 1987 to 2005. The trend is strongly downward—in other words, fewer deaths are associated with heat.   






_Temporal trends, from 1987 to 2005, in the excess number of deaths (per 1,000 deaths) attributable to each 10°F increase in the same day’s summer temperature, nationally in the United States (excerpted from Bobb et al., 2014)._   
  
Or as Bobb and colleagues put it:   




This study provides strong evidence that acute (e.g., same-day) heat-related mortality risk has declined over time in the US, even in more recent years. This evidence complements findings from US studies using earlier data from the 1960s through mid-1990s on community-specific mortality rates, as well as European studies that found temporal declines in heat-related mortality risk, and supports the hypothesis that the population is continually adapting to heat. [citations removed]



Not only is the risk from extreme heat declining, but so too are the actual numbers of people dying from extreme heat both in the United States and abroad (when properly standardized for demographic changes and population growth).   
  
This is opposite from what the EPA chart leads you to believe.   
  
The only way for the EPA to be so out of touch with the prevailing science is to be so on purpose.   
  
We can’t help but to think that purpose will be revealed today.   
  
**Reference:**   
  
Bobb, J.F., R.D. Peng, M.L. Bell, and F. Dominici, 2014. Heat-related mortality and adaptation in the United States, _Environmental Health Perspectives_ , http://dx.doi.org/10.1289/ehp.1307392


"
"

Back in December, Senate Democrats, with President Obama’s backing, attempted to prohibit anyone on the federal government’s terrorism watchlist from purchasing a firearm.   
  
  
At the time, I criticized the proposal for its lack of process and its inevitable inefficacy at reducing gun crime or terrorism.   
  
  
Yesterday, Senate Democrats launched a filibuster in order to push for the resurrection of the failed “No‐​Guns List.”   
  
  
The substance of their plan has not changed, and my earlier criticism still stands:   




How does a person prove they are not a terrorist? It’s virtually impossible. A no‐​flyer doesn’t receive the evidence against them or a hearing before being placed on the list. They are not allowed to confront their accuser. Even getting the government to acknowledge that a person is on the list may require lengthy and expensive litigation. A person on the no‐​fly list may not even know they are on the list until they’re refused service at the airport. A person on the broader terror watch list has no means of finding out. The system is devoid of anything resembling due process, a flaw _The New York Times_ condemned as being intolerable in a free and democratic society and over which the American Civil Liberties Union is currently suing the Obama administration. The no‐​fly listing procedure has already been declared unconstitutional by at least one federal judge.   
  
  
Including too many people on the list is inevitable. Nobody wants to explain, after a terrorist attack, why the attacker wasn’t in the database. And that overly inclusive quality has manifested itself in absurd ways already. Just a few examples of no‐​fly denials: the late Democratic Massachusetts Sen. Ted Kennedy, congressman and civil rights hero John Lewis, dozens of people named Robert Johnson, members of the U.S. military and federal air marshals.   
  
  
The potential for false positives and mistaken identities is not just accepted as collateral damage by these no‐​gun list proposals; it is the entire point. Anyone who has actually been convicted or is currently charged with terrorism‐​related crimes is already prohibited from purchasing a firearm under federal law. The people adversely affected by this proposal will inevitably be people against whom the government lacks sufficient evidence to charge.   
  
  
The fact that a person hasn’t been adjudicated as dangerous doesn’t preclude them from committing violence, of course. But just how much discretion should the president have in abolishing constitutional rights without charge or trial?



What _has_ changed is the political climate in the interim.   
  
  
The No‐​Guns List appears to have picked up some powerful allies on the right. Presumptive Republican presidential nominee Donald Trump has expressed support for the idea, and is apparently lobbying the National Rifle Association to come along with him.   
  
  
The GOP and the NRA are generally regarded as the two primary bulwarks against misguided gun control proposals. Adding their weight to this particular gun control proposal would bolster its legislative prospects immensely.   
  
  
Even if, as some supporters have urged, the law requires hearings before a watchlisted person can be denied the right to bear arms, important questions remain. What exactly does the state need to prove in order to take someone’s 2nd Amendment rights away? What is the burden of proof? Will judges allow the use of secret evidence, citing state secrecy concerns for refusing to disclose it? Will the individual be entitled to legal representation? Can he call and cross‐​examine witnesses? Can he appeal the ruling? Can he publicly discuss his case?   
  
  
And those are just the legal concerns. There are also pragmatic issues. What information does the FBI convey to the gun seller when someone on the list is denied? Is the gun seller told that he’s got a terror suspect standing in his store? What if the person actually is an aspiring terrorist under government surveillance? Doesn’t this process inevitably tip him off? Would finding out that he’s on the government’s radar only encourage an aspiring terrorist to act quicker? Would it compromise legitimate surveillance operations?   
  
  
The Boston bombers didn’t need guns. Nor did Timothy McVeigh or the 9/11 hijackers. Giving terror suspects a sure‐​fire way to figure out whether they’re being surveilled seems like a large price to pay for what may be a non‐​existent benefit.   
  
  
Omar Mateen passed background checks. He passed training requirements. He had access to weapons as a security guard. He wasn’t even on the terrorism watchlist. Nothing in this proposal, and nothing in any of the other gun control proposals this tragedy has spawned, would have kept firearms out of Omar Mateen’s hands. The only way his rampage could have been prevented was for someone to kill him first. Unfortunately, laws that deny even sober people the right to carry weapons in establishments that serve alcohol meant that the law‐​abiding victims were sitting ducks.   
  
  
Knee jerk reactions to horrible tragedies have proven to be a poor basis for good public policy. We have institutions like due process precisely for times when emotions threaten to overrun safeguards that are just as important for protecting the innocent as the guilty.   
  
  
It’s hard to imagine a graver violation of the spirit of the 2nd Amendment than a law allowing the President to declare anyone an enemy of the state without so much as a charge and subsequently bar them from exercising their 2nd Amendment rights. But Republicans, lured from their stalwart support of gun rights by fears of terrorism, and Democrats, lured from their stalwart support of civil rights by their zeal for gun control, combined with an election cycle that has been defined by appeals to fear may be creating a perfect storm and a severe threat to liberty.   
  
  
P.S. Two tweets this morning from sitting Congressmen highlight the divide.   
  
  
Democratic Senator and gun control advocate Joe Manchin doesn’t inspire confidence when he says things like “due process is killing us.”   




> .@Sen_JoeManchin: Due process is what's killing us right now https://t.co/OTf9LnxHXZ
> 
> — Morning Joe (@Morning_Joe) June 16, 2016



.@Sen_JoeManchin: Due process is what's killing us right now https://t.co/OTf9LnxHXZ



Luckily, not everyone in Congress agrees.   




> Amazing that U.S. senators would filibuster in favor of using secret lists, like some authoritarian regime, to deny rights w/o due process.
> 
> — Justin Amash (@justinamash) June 16, 2016



Amazing that U.S. senators would filibuster in favor of using secret lists, like some authoritarian regime, to deny rights w/o due process.
"
"
Share this...FacebookTwitterYou would think that with all the added wind and solar energy in Germany, along with all the conventional power plants on standby, all totaling up to huge unneeded capacity, there would be no need to import any power at all. Well, think again.

Photo: P. Gosselin
The German epochtimes.de here reports that German imports of electricity in fact: “rose by 43.3 percent to 25.7 billion kilowatt hours in the first half of 2020 compared with the first half of 2019.”
The epochtimes.de explains further:
One reason for this was the declining share of domestic feed-in from base-load-capable, mostly conventionally operated power plants, which mainly use coal, nuclear energy and natural gas. As a result, electricity was imported to cover the demand for electricity, especially when there was no wind or darkness. The main import country for electricity was France with 8.7 billion kilowatt hours.
Overall, however, more electricity was still exported from Germany.”
What the article does not mention, however, is the reason for the rise in export from Germany. On windy and sunshine-plenty days, Germany produces more electricity than needed, and so is forced to dump the excess power into neighboring foreign markets – often at negative prices. The negative prices, in combination with the mandatory feed-in tariffs and excess production capacity, all means higher costs for consumers.
Little wonder that at close to 35 US cents per kwh, Germany’s electricity prices are among the highest in the world.


		jQuery(document).ready(function(){
			jQuery('#dd_f32389d0d82421273a7d3eb7eaea0392').on('change', function() {
			  jQuery('#amount_f32389d0d82421273a7d3eb7eaea0392').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
In our last episode, we looked at a COOP station on a roof of a fire station operated by the NWS in San Diego. Moving north, we have another COOP station operated by the San Francisco/Monterey Weather Service office that is also on a rooftop. You can see the MMTS sensor in the photo provided.
This station, COOP number 04-1838 is in Cloverdale, CA is just a few feet away from a chimney flue and an exhaust stack of a diesel generator. Note also the rain gauge placement. This station, while not a USHCN station, is part of the “A” network, which does report climate for NCDC.

Photo from NWS SFO/Monterey
For those unfamiliar at spotting NOAA issued MMTS temperature sensors, its the post on the leftmost portion of the lower roof, directly beneath the satellite dish.
To see other stations, try my blogs “weather_stations” link under the categories at right. To see stations in your state see www.surfacestations.org and click on the online image gallery link. You may even wish to signup to help survey a station in your area.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea288d583',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
‘Die Welt’ science editor Axel Bojanowski (right) comments “This is how climate change is forged”. He had 4 leading scientists confirm the above hockey stick chart was dubious. Image: Die Welt , Source:: DeWikiMan/commons.wikimedia.org/CC BY-SA 4.0; Bojanowski/private
ZDF weather moderator Özden Terli under fire: Did he deceive viewers on historical climate with sleight of hand?
By Die kalte Sonne
(German text translated/edited by P. Gosselin)
Disinformation at ZDF German public broadcasting?
ZDF meteorologist Özden Terli has been a topic at this blog several times. Now ‘Die Welt’ science editor Axel Bojanowski  has taken a closer look at a weather report, particularly a chart that Terli presented July 24, 2020.
The article is behind a paywall, but is worth the money. The centerpiece is the chart from the Potsdam Institute for Climate Impact Research (PIK). It depicts a hockey stick temperature curve that is supposed to convey drama. Excerpt from the article in Die Welt:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The weather report of the “Heute Journal” on ZDF on July 24th presented something that was supposed to be sensational. Moderator Özden Terli presented a graph that allegedly showed the course of the global average temperature since the Ice Age. At first ‘the warming was very slow,’ Terli explained, ‘and then it was stable for a long time’. Suddenly, however, since around 1900, the temperature “jumped up”.
The graph showed an almost vertical red line, the peak of which far exceeded all temperatures since the Ice Age. “This jump is quite enormous,” Terli said.
Millions of viewers were shown that the present day would be warmer than all the rest of human civilization. The discovery could only be a scientific sensation, or a hoax.”
Bojanowski interviewed several experts on the subject and their reactions speak volumes because the graph cleverly mixed two things together, namely smoothed averages from the past with measured current values. Fluctuations in the past can thus no longer be seen. The experts’ verdict was unanimous: it is dubious. Bojanowski analyzes:
The graph was a sleight of hand: the steep red line at the end was not comparable to the data in the previous period. It showed annually measured average temperatures on Earth since the end of the 19th century. However, no such precise records exist for earlier times. Most of the time in the history of civilization can only be shown with average values.
Often there is only one temperature value for hundreds of years, or data has been “smoothed”, i.e. only its average value is shown – short-term warming or cooling is not shown. In order to make the period from industrialization to the present day (the steep red line) comparable with the data for the rest of the time, it should therefore only be shown as a dot showing the average temperature from 1900 to the present day – the red line would only be an inconspicuous dot.”
And because Bojanowski has been there before, the subject of drought can be brought up again. In a weather report Terli attributed this to changes in the climate system. But without going into other sources. Perhaps the weatherman would then have noticed that annual precipitation in Germany has increased over the past 120 years. … It’s astonishing that the head of ZDF Wetter, Katka Horneffer, allows Özden Terli to include such inserts in the weather report again and again.
Share this...FacebookTwitter "
"

A plurality of likely voters now say they disagree with Vice President Gore on what is clearly his innermost, core belief. As a result, Washington insiders have advised Bush to capitulate to Gore. 



Last Earth Day, Gore re‐​released his book “Earth in the Balance,” which declares that fighting global warming should be the “central organizing principal for civilization,” and that the price of energy should be increased. Gore says he “wouldn’t change a thing” about the original (1993) edition. 



In order to create the legal framework for his program of global salvation, he told ABC’s This Week that he would “build support” for the Kyoto Protocol on global warming, which coerces dramatic reductions in energy use through higher prices, before submitting it to the Senate for ratification.



According to a recent Zogby/​Reuters poll, so far he has failed. By a margin of 46 percent to 42 percent, people support Bush’s position over Gore’s. Specifically, in the second debate, Bush said, “I’ll tell you one thing I am not going to do. I’m not going to let the United States carry the burden for cleaning up the world’s air, like the Kyoto treaty would have done.” 



Not only did Bush carry the day, it looks as if Kyoto causes heartburn for a lot of Democrats. Of people identifying themselves as Democrats, 68 percent agreed with Gore. That means one‐​third of the party faithful either agreed with Bush or had no opinion, while only 15 percent of Republicans favored Gore’s position. 



Sensing defeat, Gore recently retreated from his “Earth in the Balance” position on energy taxes, which are the fastest (and most economically disruptive) way to discourage fuel use. In the second debate, he said “I’m not in favor of energy taxes.”



So, why hasn’t Bush belled Gore’s cat? Not only does Bush enjoy popular support, he has caught Gore in another clear misstatement. The answer is that the Bush campaign itself is conflicted about climate change. Tucked away in his energy policy white paper released last month is a statement about limiting carbon dioxide emissions, the main greenhouse warming gas. That position potentially puts him to the left of Gore on global warming. 



In many ways, Bush is handling environmental policy a lot like his father did. In 1992, President Bush went to Rio de Janeiro to sign the original U.N. global warming treaty, against the advice of many but riding a crest of popularity. Five months later he was beaten by Clinton and Gore, the latter of whom heckled him in Rio. 



Why do Bushes do this? While Gore governs, campaigns, lives and breaths confrontation, Bushes make compromises and “bring people together.” They’re “kindler and gentler” and “compassionate.” 



In Washington, the people you “bring together” are involved in Washington’s primary industry: government. And so when either Bush sought Washington advice on climate change, he ran into people who can always be depended on to recommend federal programs to “do something.” For global warming, this means placing some type of restriction on greenhouse gas emissions. “Nothing” is not an acceptable answer in governmentville.



There are neither term limits nor elections for lobbyists, and they will do anything to stay here. The food is good (they’re not paying), the wine is cheap (D.C. has the lowest liquor taxes around), and the power is even more intoxicating. Keeping those perks means defining anything as a problem, requiring a solution from the federal government. 



Thus it was the electric utility lobby that encouraged Bush to include emissions restrictions in his environmental proposals. But the electric utility lobby will pursue its regulatory interests regardless of who is elected. With the prospect of a Bush victory, the lobby just made up a set of proposals, in order to have a reason for existence come inauguration time. 



Do they care what voter opinion is? Look at the Zogby poll and decide for yourself. Have they rewarded Bush for his reluctance to go along with Kyoto? No, because they’d be out of a job. Instead, they advise Bush to capitulate on his opponent’s most heartfelt opinion, in spite of evidence that Bush articulates the most popular position. They fear that Bush might actually win and make them irrelevant.
"
"

“War is God’s way of teaching Americans geography,” the 19th century American writer Ambrose Bierce sagely observed, and the war in Iraq is no exception. After weeks of intense coverage, one fact is plain: The people of Iraq should be among the richest in the world. 



Iraq has been cursed by brutal politics, but it has been abundantly blessed by geography. Two great rivers bound a fertile plain in a subtropical climate. It possesses a seaport and ready access to major markets. As the repository for many of the world’s greatest archeological sites, it should be a tourist Mecca. And, of course, the country sits on top of the world’s second largest known oil reserves. 



Among his many crimes, Saddam Hussein squandered and stifled the potential wealth of his country by his warmongering and official thievery. The economic policy of his Baathist Party was a kind of thuggish socialism: government control of prices and industry, ubiquitous rationing, arbitrary confiscation of private wealth, and little trade other than arms and oil. 



Saddam’s misrule has left Iraq by far the poorest country in the Persian Gulf region. Its per capita gross domestic product is the equivalent of $2,500 a year, far lower than the per capita GDP of Qatar ($21,200), Kuwait ($15,000), Saudi Arabia ($10,600) or even Iran ($7,000). Iraq’s imports are a fraction of what they were in the 1980s, when it citizens were buying almost $1 billion worth of U.S. farm products a year. 



America’s well‐​earned victory on the battlefield will be in jeopardy if the people of Iraq cannot enjoy the material fruits of their new‐​found freedom. To safeguard our investment of blood and treasure, the United States and Great Britain should ensure that any new Iraqi government protects the economic as well as the political and civil freedom of its citizens. 



An essential part of any plan to establish freedom in Iraq should be a commitment to a free market and the institutions that support it, including a commitment to free trade. Iraqis must enjoy a secure right to property, a stable currency, decontrolled prices, the rule of law and contract, and the freedom to engage in business, at home and through international trade. 



Post‐​war reconstruction of Europe provides a model, although not in a way most people believe. Yes, Marshall Plan aid played a role in reviving Western Europe, but the real story was the continent’s turn toward markets and free trade. In June 1948, Germany’s Ludwig Erhard abruptly repealed price controls and issued a new currency. Tax and tariff cuts soon followed. As one historian noted: “The spirit of the country changed overnight. The gray, hungry, dead‐​looking figures wandering about the streets in their everlasting search for food came to life.” Iraq needs an Arab Ludwig Erhard. 



Study after study has confirmed that nations relatively open to trade grow faster and achieve higher incomes than those that are relatively closed. The model for Iraq’s new leaders should be Ireland, Chile, the tigers of East Asia, and other countries that have achieved sustained growth through expanding trade—not the stagnant and increasingly isolated countries of the Arab Middle East. 



With the Saddam regime now history, UN sanctions should be lifted immediately. If France and Russia insist on keeping sanctions in place to protect the UN’s bureaucratic control over Iraqi oil, the United States should ignore the sanctions and unilaterally allow Americans to trade with the people of Iraq. U.S. markets should be opened to goods made by Iraqis, especially import‐​sensitive textiles, apparel, and farm products. Beyond stimulating growth, trade with Iraq would bring humanitarian relief, cement ties between our two countries, and send a positive signal that Iraq is open to foreign investment. 



Much has been written about the need for political reform in the Arab world, but it also desperately needs economic reform. The Arab world’s share of global trade and foreign investment has been declining in the last two decades. Outside of oil, Arab countries export little that the rest of the world is willing to buy. With a few exceptions, barriers to trade and foreign investment remain high. There are more McDonald’s franchises in the tiny Netherlands than in all the Arab world. 



A vibrant Iraqi economy would give hope to a new generation of Arabs to reclaim their rightful place in the world of trade, science and ideas. An educated, hopeful middle class would in turn create more fertile soil for limited and representative government. But if a post‐​Saddam Iraq fails to prosper, its people will grow frustrated and blame the market and the West for their troubles, creating fertile soil for terrorism. 



The technology, dynamism, and openness of our own market economy helped us win this war; if spread to Iraq, those same market forces can help us win the peace.
"
"
Share this...FacebookTwitterWow! Arctic sea ice is still at levels we had back in mid-February. It’s at the highest level for this date in nine years. http://www.ijis.iarc.uaf.edu/en/home/seaice_extent.htm
Normally there’s about a 1 million sq. km decrease from mid Feb to late April. This year – nada!
Share this...FacebookTwitter "
"

In February 2012 Gen. Martin Dempsey, the chairman of the Joint Chiefs of Staff, declared, “I can’t impress upon you [enough] that in my personal military judgment, formed over 38 years, we are living in the most dangerous time in my lifetime, right now.”



One year later, he upped the ante: “I will personally attest to the fact that [the world is] more dangerous than it has ever been.” But General Dempsey is hardly alone. Dire warnings about our uniquely dangerous world are ubiquitous. Director of National Intelligence James Clapper testified in early 2014 that he had “not experienced a time when we’ve been beset by more crises and threats around the globe.”



Members of Congress agree. Sen. John McCain (R-AZ), born before World War II, explained in July 2014 that the world is “in greater turmoil than at any time in my lifetime.”



Is it? Do we actually live in a uniquely dangerous world? And, if we do not, why do we believe that we do?



In his magisterial study of the decline in violence worldwide, Harvard’s Steven Pinker posits that “we may be living in the most peaceable era in our species’ existence,” even as he concedes that most people don’t believe it.



If our perceptions aren’t entirely accurate, if the world isn’t, in fact, more dangerous than a decade ago, or a century ago, we could blame our 24/7 media. After all, reporters don’t write about the planes that land safely; the 11 o’clock news never leads with the murder that didn’t happen. Likewise, the stories about the personal information not stolen by identity thieves, the wars that aren’t fought, and the trade and commerce that flows uninterrupted, are rarely told. Moreover, we lack perspective. There is little focus on the threats that no longer threaten. Few talk about the dangers no longer looming. It is rare, even, to find people putting today’s threats in context with the recent past. Or the distant past. Few even bother to ponder the question.



However, is not an easy task. From wars between states to wars within them, from crime and terrorism to climate change and cyber‐​mischief, we are beset by a seemingly endless array of threats and dangers. How canone compare them to past threats, especially given that the judgments of what should or should not frighten us are inherently subjective?



It remains true that the only existential threat to the United States comes from a prospective thermonuclear war — the stuff of countless novels and Hollywood films during the dark days of the Cold War. Who is to say that this event, which has never occurred, is, or should be, more frightening than the very real acts of violence that do take place every day? And does society benefit if our fears of very low probability, high‐​impact events (e.g., global thermonuclear war) were merely supplanted by fears of slightly higher probability, low‐​impact ones? Some might say that it is better to be safe than sorry. That we should worry about all potential threats. By this logic, it is better to fear things that aren’t real than to take too lightly those that are.



Perhaps the tendency to take seriously even seemingly modest dangers has been programmed into our DNA, a product of thousands of years of natural selection. Our distant ancestors who correctly perceived a fourlegged creature charging at them from a distance to be a dangerous predator had time to either flee or defend themselves, and thus lived to procreate. By contrast, their threatdeflating neighbors, who believed the approaching beast to be harmless, realized their error too late and were mauled to death.



But while we have learned to take threats seriously, we are also taught to differentiate the real from the imaginary. Fallacious claims of impending danger will erode one’s credibility, to the point that the congenital fearmonger is no longer taken seriously. The parable warns of the dangers of crying “wolf” when there are no wolves, but it doesn’t teach us to stay silent when we see one. In the parable, the wolf eventually does come, and the dishonest boy is eaten. The moral of the story is not that all dangers are inflated, but rather that the phony ones should not be.



In truth, we should be on the lookout for both kinds of errors. The business world punishes both the imprudent optimist as well as the too‐​gloomy pessimist. The financial analyst who rated all tech startups as “strong buys” in 2000 or the housing speculator who bought multiple condominiums in Miami in 2007 could rightly be cast as too optimistic. On the other hand, extreme risk aversion can blind us to possibilities. And excessive fear can be harmful to both our physical health and emotional well‐​being. The National Institute of Mental Health explains that “excessive, irrational fear and dread” are key symptoms for one of several anxiety disorders, which according to one estimate, afflict 18 percent of Americans.



 **FEAR IS THE HEALTH OF THE STATE**  
But there is a political harm as well. Individual liberty is often threatened during periods of heightened fear and anxiety, a fact that informed the very structure of the U.S. government. James Madison, in making the case for restraining the new government’s war‐​making powers, warned the delegates to the Constitutional Convention in Philadelphia: “The means of defence against foreign danger, have been always the instruments of tyranny at home.”



He went on: “Among the Romans it was a standing maxim to excite a war, whenever a revolt was apprehended. Throughout all Europe, the armies kept up under the pretext of defending, have enslaved the people.” A decade later, Madison returned to this theme in a letter to Thomas Jefferson. Madison knew that there was already some demand for a standing military, and that a few would use fear of foreign threats to whip up public sentiment in favor of a more powerful state. Indeed, Madison postulated “a universal truth that the loss of liberty at home is to be charged to provisions against danger real or pretended from abroad.”



Others since then have stumbled upon similar ideas about popular notions of threats, and of how the fear of threats has been used to grow the power of government. For example, the noted writer, social critic and satirist H.L. Mencken declared “the whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



Madison and Mencken’s warnings remain relevant today. Recall how in November 2008 incoming Obama chief of staff Rahm Emanuel called for swift government action to deal with what he said was an urgent threat. “You don’t ever want a crisis to go to waste,” Emanuel explained in an interview, “it’s an opportunity to do important things that you would otherwise avoid.”



While Emanuel was talking about an economic crisis, an increasingly powerful state can be used in many different ways, regardless of whether it was precipitated by fears of foreign or domestic threats. The same sorts of powers that allowed the Justice Department to go after suspected terrorists allowed the IRS to harass suspected tea partiers.



 **NEW TECHNOLOGIES, NEW FEARS**  
Because inaccurate or misleading characterizations of threats pave the way for the growth of government, it is crucial to understand their true nature.



Thus, is the world more dangerous than ever before? In a word, no. Americans, especially, enjoy a measure of security that our ancestors would envy, and that our contemporaries do envy.



This is not to say that there are no dangers in the world today, as the residents of Tel Aviv and Gaza City will attest. Nor can we say that circumstances will not change for the worse in the future. No one would have predicted that a single act of violence in late June 1914 would precipitate a series of events that culminated in the First World War. Could the 21st‐​century successors to Gavrilo Princip deploy cyber‐​weapons to wound, or even kill, their chosen targets? Could they do more than simply kill a single head of state, but do grievous harm to millions? Or could their actions, as Princip’s did, lead to a major war, which in the nuclear era would result in the deaths of hundreds of millions?



The possibilities cannot be ruled out. But, for now, they are only that: possibilities, and unlikely ones at that. Since their inception, nuclear weapons have been the one true weapon of mass destruction. And following the 9/11 attacks, many believed that they would inevitably fall into the hands of terrorists or other nonstate actors inclined to use them. Still others worry that more nation‐​states will acquire them. The fear of proliferation is not new. In either case caution is warranted, but excessive fear is not. Few countries have ever seriously aspired to possess such weapons, and many of those who did eventually gave up. In a few cases, countries actually turned over their weapons entirely. In fact, for nearly every country in the world, nuclear weapons are more trouble than they are worth.



Terrorists and nonstate actors have, so far at least, come to a similar conclusion. Contrary to the apocalyptic predictions immediately after 9/11, al Qaeda and others have relied exclusively on conventional weapons–chiefly bombs and bullets–to terrorize their victims. They seem to be heeding the advice found in a memo on an al Qaeda laptop seized in Pakistan in 2004: “Make use of that which is available … rather than waste valuable time becoming despondent over that which is not within your reach.”



 **NATION-STATES VS. NONSTATES**  
What of the more traditional threats posed by states? While Vladimir Putin seems to be trying to restore Russia to its place at the top of the enemies list, China is the one country with sufficient size and potential wealth to directly challenge the United States in the future. But it is premature, to say the least, to assume that a war between China and the United States is inevitable. To be sure, U.S. treaty commitments to some of China’s neighbors risk drawing the United States into vexing territorial disputes, and China has been developing military capabilities that could significantly raise the costs for the United States if it chose to back its allies’ claims by force. For now, however, all parties have many reasons to try to resolve these claims peacefully — including the fact that China is the leading trading partner throughout the region. Similarly, the United States and China have many reasons to work together to address other problems beyond the Asia Pacific region.



Many of those common dangers emanate from nonstate and substate actors, from terrorists and insurgents to revolutionaries and rebels. And while these threats are real, they pale in comparison to what states used to do to one another on a regular basis.



Terrorism is, in fact, far less dangerous than widely believed. Consider, for example, that a total of 19 Americans have been killed in four separate terrorist incidents carried out by Islamist extremists on American soil since 9/11. For reference, 50 people were killed in just three separate incidents during a 14- month span in 2012 and 2013 (Aurora, Colorado; Newtown, Connecticut; and the Washington Navy Yard). Excluding U.S. military personnel, fewer Americans have been killed by terrorism globally since 2002 than have died from allergic reactions to peanuts (an average of 50–100 per year).



Although a relatively small number of people are killed or injured by terrorism every year, many people worry that new technology will allow nonstate actors to inflict harm in other ways, say, for example, by attacking the Internet, or company or individual computers connected to it. States, too, are known to have used so‐​called cyberweapons, or have aspired to do so. Thus, numerous U.S. officials have warned that cyberattacks are the single greatest threat to national security. Here again, however, some skepticism is in order. Hackers and criminals are adept at exploiting the vulnerabilities in computer networks, but it is extremely difficult to carry out a major attack with far‐​reaching consequences.



Even an attack that managed, somehow, to crash the banking system completely would be unlikely to undermine confidence in the wider economy, let alone trigger a recession. As the RAND Corporation’s Martin Libicki points out, NASDAQ’s three‐​hour shutdown on August 22, 2013, didn’t spark a wave of panic selling. “It would require data corruption (e.g., depositors’ accounts being zeroed out) rather than a temporary disruption,” Libicki explains, “before an attack would likely cause depositors to question whether their deposits are safe.”



The greater threat may come from measures taken to prevent attacks if they significantly impede legitimate transactions in cyberspace. Similarly, poorly conceived or badly executed policies after the fact might cause more harm than the original incident that precipitated them. The attribution problem compounds the risk. The difficulty in tracking possible cyberattacks to their true source, and in ensuring that the punishment or retaliation is directed at the perpetrators, places a high premium on measured, targeted responses. Maintaining that standard will help ensure a safer world.



Another factor that can explain why the world is becoming less, rather than more, dangerous, is evolving social norms. Harvard’s Pinker notes a perceptible shift in public attitudes toward violence, and documents an associated decline in violent crimes of all types. The rate of such crimes, including murder, rape, and assault, are at or near all‐​time lows. Within the United States, for example, the homicide rate (homicides per 100,000 residents) fell by nearly half (49 percent) in the 20‐​year period from 1992 to 2011.



 **SUPPOSED THREATS TO GLOBAL STABILITY AND ECONOMIC PROSPERITY**  
Still others worry not so much about physical security, but rather about our prosperity and way of life. They fear that a war could cripple the international economy, or that the mere threat of war could disrupt global trade and commerce, including the world’s oil supplies. This concern, at least today, is the main justification for the U.S. military’s forward presence around the world, a posture oriented around stopping possible threats before they materialize.



But the patterns of global trade are far more resilient than the pessimists envision. War between major trading partners is highly unlikely, and, even if it were to occur, trade flows between nonbelligerents would not be disrupted, or not for very long. Indeed, Eugene Gholz shows that when countries shift resources to the purchase of military goods, the result is similar to other consumption binges. During wartime, neutral nations may well benefit economically, as they become a safe haven for investment diverted from warzones, and as they are able to buy certain goods at low cost.



While war itself has many horrific effects, the costs that Americans pay to stop all wars are unlikely to be outweighed by the benefits. There are other risks associated with maintaining a forward military posture. Current U.S. strategy encourages other countries to free‐​ride on the security guarantees provided by the U.S. military, imposing an unnecessary — and ultimately counterproductive — burden, on U.S. taxpayers. Exaggerated fears of distant conflicts could even prompt the United States to fight wars that pose no direct threat to U.S. security and to spend too much on the military, which, in turn, weakens the overall U.S. economy.



 **PUTTING TODAY’S THREATS IN PERSPECTIVE**  
Although the world will never be free from dangers, we should aspire to understand them clearly. Maintaining perspective isn’t easy when we are bombarded with images of fighting from Eastern Ukraine or Gaza. But, in many instances, we are today merely seeing what has always existed beyond our field of vision. Tragic, even horrifying, stories of human suffering do not portend that we are living in a more dangerous world. In most respects, we are living longer, better lives. Our chances of suffering a violent or premature death are very low, and still declining. And our prosperity and broader wellbeing are protected by a dynamic and resilient international economy, and by the spread of powerful ideas that have reduced poverty and disease.



A better understanding of what actually threatens us will help us tame our tendency to overreact. An honest assessment of the threat environment — problems that lurk today and on the horizon — will allow us to redirect some of the money that goes to the Pentagon and military contractors back to the taxpayers and private entrepreneurs. And, recalling Madison and Mencken’s warnings about how and why states exaggerate threats to grow their power, a more accurate assessment of the world’s dangers will ultimately help us to preserve our liberty.
"
"

A couple of comments have mentioned the global “turn off your lights” night. Lubos Motl at the Reference Frame has a suggestion
Earth Hour: turn your lights on at 8 p.m.
Tonight, at 8 p.m. local time, you should turn on all the light bulbs you have for 60 minutes (it will only cost you 3 cents per light bulb in average for the whole hour) to fight global obscurantism. You should look how many lights are on around. Every light bulb you see will be a sign of the audacity of hope, as Jeremiah Wright would say.”15 years ago, I would have done this. Now, I plan to turn all my lights on as my silent form of protest against the likes of Gore and his Enron like carbon credit scheme. I’m going to “Watts Up” my house!
If you want to learn about the event, here is the web page:
http://www2.earthhourus.org/
Of course if you are simply interested in saving money and using less electricity (something I’m for, especially here in California since the state has hamstrung itself for future power generation) then get one of these:

I have several. They work great. And, buying one via this link sends some help back to me for keeping my www.surfacestations.org effort running.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea021377a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"





There will be a story featuring Al Gore and his climate views on CBS 60 minutes this weekend.  Normally I don’t pay much heed to this program, but Gore is publicly calling those who question the science “…almost like the ones who still believe that the moon landing was staged in a movie lot in Arizona and those who believe the world is flat…”.
To me, a person who has at one time been fully engaged in the belief that CO2 was indeed the root cause of the global warming problem, I find Gore’s statements insulting. In 1990 after hearing what James Hansen and others had to say, I helped to arrange a national education campaign for TV meteorologists nationwide (ironically with CBS’s help) on the value of planting trees to combat the CO2 issue. I later changed my thinking when I learned more about the science involved and found it to be lacking.

I’ve never made a call to action on media reporting before on this blog, but this cannot go unchallenged.
The press release from CBS on the upcoming story on Gore is below. You can visit the CBS website here and post comments:
http://www.cbsnews.com/stories/2008/03/27/60minutes/main3974389.shtml
See the video clip here
But let’s also let the producer, Richard Bonin,  know (via their communications contact) what you think about it, as I did when Scott Pelley aired a whole hour long special telling us Antarctica was melting. They did no follow up.

Kevin Tedesco KEV@cbsnews.com
Director, CBS News Communications (”60 Minutes”)



That email is listed on the CBS website, so it is fair to send comments to it. In fact, here is a contact list they have on their website where you can comment about this story. I feel it is important to respond and to spread the word to others. While I have not seen the video segment, let us hope that it has some semblance of balance, because the press release certainly does not.






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea031f862',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Since 9/11, the U.S. government has poured a breathtaking amount of resources into investigating suspected terrorism operations within the United States. A key component of these investigations is known as “ghost‐​chasing” — the thousands of leads and tips investigated daily, and classified as “threats,” despite the fact that only one in 10,000 fails to be false.



These efforts are often criticized on the basis of civil liberties abuses. But, convinced that terrorism is an “existential” threat, many people are perfectly comfortable overlooking these abuses. In their new book, _Chasing Ghosts: The Policing of Terrorism_ , Cato’s John Mueller and Mark G. Stewart of the University of Newcastle, Australia, take aim at the very premises of U.S. counterterrorism operations. Is terrorism truly a significant threat? Are most would‐​be terrorists actually skilled enough to pull off an attack? Is it true that we can “never be safe enough”? Mueller and Stewart examine the methods of the FBI, National Security Agency, the Department of Homeland Security, and local policing agencies, revealing the government’s exaggerated claims about the “threats” they divert. The question, they write, is not whether any real terrorists exist — but whether the chase is worth the cost.



 **You Might Be a “Lukewarmer” If …**  
When it comes to global warming, most people think there are two camps: “alarmist” or “denier” being their respective pejoratives. Either you acknowledge the existence of manmade climate change and consider it a dire global threat, or you deny it exists at all. But there’s a third group: the “lukewarmers.” As Cato scholars Pat Michaels and Paul C. Knappenberger write in their new ebook, _Lukewarming: The New Climate Science that Changes Everything_ , “Lukewarmers believe the evidence of some human‐​caused climate change is compelling, but it is hardly the alarming amount predicted by models.”



Lukewarmers are skeptical that government pacts, like those sought at the 2015 United Nations Climate Change Conference in Paris, will do much to temper climate change’s effects. They also tend to question the incentive structure of climate science, where scientists are vying for millions of dollars of government funding — meaning that any proposal that global warming’s effects have been overforecast “threatens to derail everyone else’s gravy train.” This, they argue, has brought about “a systemic distortion in the direction of alarmism.” Lukewarmingtells a different story — one that ends with optimism. “Lukewarmers know,” they write, “that economic development is the key in adaptation to the vagaries of weather and climate, even climate change induced by people.”



 **Property Rights after _Kelo_**  
When Cato published the first edition of _Cornerstone of Liberty: Property Rights in 21st Century America_ , the infamous Supreme Court case of _Kelo v. New London_ had only recently been decided, declaring that the government can seize private property by eminent domain under a broad definition of “public use.” In the decade since, by one estimate, the government has taken over a million homes from their owners. Cato adjunct scholar Timothy Sandefur of the Pacific Legal Foundation and his wife Christina, vice president for policy at the Goldwater Institute, set out to revise the book for its second edition — but, as they write, “So much has happened in the years after _Kelo_ that what started as a simple update to this book became a complete renovation.”



As in the first edition, the Sandefurs narrate the heartrending stories of Americans forced from their homes, explaining along the way how property rights became eroded. But this updated edition also contains a wealth of new material on the ever‐​changing threats to property owners. The Sandefurs conclude by examining the backlash from _Kelo_ and suggesting a new path forward. As Washington Post columnist George Will wrote, “Not since Babe Ruth and Lou Gehrig has there been a one‐​two punch quite like Timothy and Christina Sandefur. Both lawyers. Both authors. Both helping shape the country.”



 **Paving the Path to Growth**  
“If you could wave a magic wand and make one or two policy or institutional changes to brighten the U.S. economy’s long‐​term growth prospects, what would you change and why?” Brink Lindsey, Cato’s vice president for research, posed this question to 51 prominent economists and policy experts for his ebook _Reviving Economic Growth_.



Their ensuing essays constitute a “brainstorming” session from an eclectic group of contributors, featuring libertarian, progressive, and conservative perspectives. “By bringing together thinkers one doesn’t often see in the same publication,” writes Lindsey, “my hope is to encourage fresh thinking about the daunting challenges facing the U.S. economy — and, with luck, to uncover surprising areas of agreement that can pave the way to constructive change.”



In a second ebook, _Understanding the Growth Slowdown_ , Lindsey and his contributors dive yet again into the pressing questions surrounding the disappointing performance of the U.S. economy in recent years. Lindsey asks whether this could be more than a temporary trend, but rather the “new normal” — and if so, why. “The U.S. economy is a phenomenon of mind‐​boggling complexity,” Lindsey observes. These collected essays don’t aim to provide all the answers, but to provoke new ideas — without which an economic revival will certainly not be possible.
"
"
Share this...FacebookTwitterPia Heinemann reports in Die Welt today, Ocean Acidification Does Not Lead To Species Die-Off, on a new study appearing in the latest edition of Science. The study contradicts the assumption that ocean acidification leads to species die-off, surprising scientists.Abstract in Science here
Manmade emissions of CO2 are thought to be partly absorbed by the oceans, which in turn would acidify and pose a huge threat to calcareous organisms like corals and plankton. This is the horror story that has been widely circulating in the media for the last couple of years, and with ever-growing alarmism, at a time the dangers of global warming are turning out to be wildly exaggerated.
Italian and Swiss scientists have found answers by looking at 120 million year old sediment deposits. The team directed by Elisabetti Erba of the University of Milan describes new findings in the latest issue of Science.
It is not unusual for CO2 concentrations in the atmosphere to surge after large volcanic eruptions. This has happened often in the past.
They examined microscopic fossils and nannoplankton from a time period just after large volcanic eruptions 120 million years ago, when the air’s CO2 content rose to about twice today’s level. Their studies contradicted their expectations. Die Welt writes:
Contrary to what was expected, no large-scale die-offs occurred among the organisms when acidification increased. The species simply adapted: They formed smaller shells and remained small.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They endured the changes far better than first thought.
Heinemann writes that the study also delivered yet another surprise:
Apparently, the oceans acidify with a delay. After the volcanoes erupted and the surface water pH value began to sink, it took 25,000 to 30,000 years longer for the CO2 effect to reach the sea bottom.
These new findings deal a massive blow to those hoping to exploit ocean acidification as the next disaster scenario to replace the discredited catastrophic AGW story. Expect the MSM to bury or spin the story.
Update/Note: Keep in mind that the plankton and coral studied were from 120 million years ago, meaning the species has since survived climate extremes and changes that were off the charts when compared to today’s mild natural changes. They’ve handled much colder and much warmer conditions with widely varying ocean chemistry.
Update 2: The Alfred Wegener Institute in Germany [Read here] is planning years of research on acidification, costing millions of euros, to study a bogus non-problem. They’ve teamed up with neutral Greenpeace, and so you can be sure they’ll come up with “catastrophic” findings and demand more money for reasearch. Whatever it takes to bilk the taxpayer out of money.
Share this...FacebookTwitter "
"
The survey project continues to move forward, even in these cold and snowy winter months. I’m pleased to announce that we have just passed the 500 mark for surveyed stations. Now with 41.1% of the network surveyed comprising 502 stations surveyed so far, that leaves 719 to go out of 1221 stations nationwide.
Some stations have recently become catalysts for larger investigations, such as the station in Lampasas TX, done by Julie K. Stacy which has brought out questions from a number of other bloggers. This prompted a review of stations previously surveyed, such as Cedarville, CA, which then prompted a larger investigation in the satellite city nightlights methodology used by NASA GISS. A whole new avenue of exploration has now opened up not just for US stations, but worldwide thanks to new features of Google Earth.
You never know where curiosity and serendipity will lead you. Thanks to Atmoz for starting the ball rolling. I also want to thank Barry Wise and Gary Boden, our early volunteers, whose help on this project has been indispensable.
Recently, this project got a significant endorsement from Dr. Roger Pielke of the University of Colorado in Boulder in his weblog. I and all the volunteers appreciate the recognition.
Here is the latest breakdown of USHCN stations that have been surveyed, and their site quality ratings:


We could really use some help this spring and summer in the following states:
Kansas, Nebraska, Arkansas, Alabama, Illinois, Idaho, Kentucky, Tennessee, Missouri, Mississippi, North Dakota,  South Dakota, Oklahoma, Texas.
If you think that you can help with this project by surveying a station near you, please visit the www.surfacestations.org website and sign up. We’ll provide instructions and help on locating stations in need of surveying.
You may also wish to consider signing up for the national flower and foiliage survey to help track climate change which is prominently mentioned on Dr. Roger Pielke’s weblog.  You can double your fun!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea10c3fc4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
September mean temperature in Sweden has not been warming like alarmists said it certainly would.
Looking at data from the Japan Meteorological Agency (JMA) for the 6 stations with data going back 22 years, we see that 3 of 6 stations have seen cooling September mean temperature over the past 22 years:

Data: JMA.
Europe on the path to societal/energy suicide?
On another note FORBES reported on Prof. Fritz Vahrenholt, the leading German climate science skeptic who I had reported on recently. Here’s an excerpt:
Let us move on to our second piece of evidence, this time from the other side of the “climate emergency” aisle.  Professor Fritz Vahrenholt is a giant among environmental circles in Germany. (The country is well known as the world’s leading champion for all things environmental and for pushing Europe to “net zero emissions by 2050”.) Prof. Vahrenholt holds a doctorate in chemistry and started his professional career at the Federal Environmental Agency in Berlin (responsible for the chemical industry) before joining the Hessian Ministry of the Environment. From 1984 until 1990 he served as state secretary for environment, from 1991 till 1997 as minister for energy and environment in the state of Hamburg.
One day before the publication of the Boston Review article on October 5th, Prof Vahrenholt stated baldly in a German TV interview that climate science was “politicized”, “exaggerated”, and filled with “fantasy” and “fairy tales”. He pronounced that “The [Paris] Accord is already dead. Putin says it’s nonsense. […] The Americans are out. The Chinese don’t have to do anything. It’s all concentrated on a handful of European countries. The European Commission in massively on it. And I predict that they will reach the targets only if they destroy the European industries.” He lambasted Germany as a country “in denial when it comes to the broader global debate taking place on climate science”. He went on to characterize Europe’s recent push for even stricter emissions reduction targets to madness akin to Soviet central planning that is doomed to fail spectacularly.”

Read entire FORBES article here.



		jQuery(document).ready(function(){
			jQuery('#dd_f520153282479aa8bdd5967e8ebb475b').on('change', function() {
			  jQuery('#amount_f520153282479aa8bdd5967e8ebb475b').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Thomas Friedman of the _New York Times_ has a column today provocatively titled “Why I Am Pro‐​Life.” Of course he doesn’t mean that he wants the government to protect life in utero. Instead he turns to a standard Democratic theme: How can you say you’re “pro‐​life” and oppose welfare, environmental regulation, and every other government program? Friedman doesn’t miss a beat: “common‐​sense gun control…the Environmental Protection Agency, which ensures clean air and clean water, prevents childhood asthma, preserves biodiversity and combats climate change that could disrupt every life on the planet.… programs like Head Start that provide basic education, health and nutrition for the most disadvantaged children.…”   
  
  
But then he takes it a breathtaking step further: 



the most “pro‐​life” politician in America is New York City Mayor Michael Bloomberg. While he supports a woman’s right to choose, he has also used his position to promote a whole set of policies that enhance everyone’s quality of life — from his ban on smoking in bars and city parks to reduce cancer, to his ban on the sale in New York City of giant sugary drinks to combat obesity and diabetes, to his requirement for posting calorie counts on menus in chain restaurants, to his push to reinstate the expired federal ban on assault weapons and other forms of common‐​sense gun control, to his support for early childhood education, to his support for mitigating disruptive climate change.



Thomas Friedman’s vision of “pro‐​life” policies is, in every case, a network of bans and mandates forcing us to live our lives in ways that are pleasing to him and Mayor Bloomberg. No “life, liberty, and the pursuit of happiness” for him. No, his pro‐​life vision is Ira Levin’s dystopia in _This Perfect Day_ , a world in which the state takes care of our every need.   
  
  
When Hayek, in his essay “Why I Am Not a Conservative,” wrote about “the party of life,” he described it as “the party that favors free growth and spontaneous evolution.” Not Tom Friedman’s party! And certainly also not the party that seeks to ban drugs, gay marriage, and the discussion of evolution in science class. In her book _The Future and Its Enemies_, Virginia Postrel wrote at length about “the party of life,” and she didn’t have in mind Friedman’s crabbed view of a government that “protects life” by snuffing out liberty.   
  
  
Some years ago I wrote a column titled “Pro‐​Life,” and I too had the Hayekian, not the Bloomberg‐​Friedman, view of life and liberty in mind. But long before that, as usual, Alexis de Tocqueville, in “What Sort of Despotism Democratic Nations Have to Fear,” warned us that one day Thomas Friedman and Michael Bloomberg would come for our liberties: 



Above this race of men stands an immense and tutelary power, which takes upon itself alone to secure their gratifications and to watch over their fate. That power is absolute, minute, regular, provident, and mild. It would be like the authority of a parent if, like that authority, its object was to prepare men for manhood; but it seeks, on the contrary, to keep them in perpetual childhood: it is well content that the people should rejoice, provided they think of nothing but rejoicing. For their happiness such a government willingly labors, but it chooses to be the sole agent and the only arbiter of that happiness; it provides for their security, foresees and supplies their necessities, facilitates their pleasures, manages their principal concerns, directs their industry, regulates the descent of property, and subdivides their inheritances: what remains, but to spare them all the care of thinking and all the trouble of living?   
  
  
Thus it every day renders the exercise of the free agency of man less useful and less frequent; it circumscribes the will within a narrower range and gradually robs a man of all the uses of himself. The principle of equality has prepared men for these things;it has predisposed men to endure them and often to look on them as benefits.   
  
  
After having thus successively taken each member of the community in its powerful grasp and fashioned him at will, the supreme power then extends its arm over the whole community. It covers the surface of society with a network of small complicated rules, minute and uniform, through which the most original minds and the most energetic characters cannot penetrate, to rise above the crowd. The will of man is not shattered, but softened, bent, and guided; men are seldom forced by it to act, but they are constantly restrained from acting. Such a power does not destroy, but it prevents existence; it does not tyrannize, but it compresses, enervates, extinguishes, and stupefies a people, till each nation is reduced to nothing better than a flock of timid and industrious animals, of which the government is the shepherd.
"
"
Sky note:
 After sunset on Sunday, Dec. 23rd, the full Moon and Mars will rise in the east less than 2 degrees apart. They’ll be two brightest objects in the evening sky, as Mars is very near it’s closest approach (opposition) to Earth, which happened just a couple of days ago. 
It will look something like this:

Note the image is not to scale. Mars is bigger than it will actually appear.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1dacdc0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Note: This is my analysis of a new paper by Joe D’Aleo, I’ve tried to simplify and explain certain terms where possible so that  it can reach the broadest audience of readers. You can read the entire paper here.
Joe D’Aleo, an AMS Certified Consulting Meteorologist, one of the founders of The Weather Channel and who operates the website ICECAP took it upon himself to do an analysis of the newly released USHCN2 surface temperature data set and compare it against measured trends of CO2, Pacific Decadal Oscillation, and Solar Irradiance. to see which one matched better.
It’s a simple experiment; compare the trends by running an R2 correlation on the different data sets. The result is a coefficient of determination that tells you how well the trend curves match. When the correlation is 1.0, you have a perfect match between two curves. The lower the number, the lower the trend correlation.



Understanding R2 correlation



R2 Coefficient
Match between data trends


1.0
Perfect


.90
Good


.50
Fair


.25
Poor


 0 or negative
no match at all


If CO2 is the main driver of climate change this last century, it stands to reason that the trend of surface temperatures would follow the trend of CO2, and thus the R2 correlation between the two trends would be high. Since NCDC has recently released the new USHCN2 data set for surface temperatures, which promises improved detection and removal of false trends introduced by change points in the data, such as station moves, it seemed like an opportune time to test the correlation.
At the same time,  R2 correlation tests were run on other possible drivers of climate; Pacific Decadal Oscillation (PDO), Atlantic Multidecadal Oscillation (AMO), and Total Solar Irradiance (TSI).
First lets look at the surface temperature record. Here we see the familiar plot of temperature over the last century as it has been plotted by NASA GISS:
 
The temperature trend is unmistakeably upwards, and the change over the last century is about +0.8°C. 
Now lets look at the familiar carbon dioxide graph, known as the Keeling Curve, which plots atmospheric CO2 concentration measure at the Mauna Loa Observatory:


CDIAC (Carbon Dioxide Information Analysis Center – Oak Ridge National Lab) also has a data set for this that includes CO2 data back to the last century (1895) extracted from ice core samples.  That CO2 data set was plotted against the new USHCN2 surface temperature data as shown below:


A comparison of the 11year running mean of the USHCN version 2 annual mean temperatures with the running mean of CO2 from CDIAC. An r-squared of 0.44 was found.
The results were striking to say the least. An R2 correlation of only 0.44 was determined, placing it between fair and poor in the fit between the two data sets.

Now lets look at other potential drivers of climate,  TSI and PDO.
Scafetta and West (2007) have suggested that the total solar irradiance (TSI) is a good proxy for the total solar effect which may be responsible for at least 50% of the warming since 1900. To test it, again the same R2 correlation was run on the two data sets.

In this case, the correlation of TSI to the surface temperature record is better than with CO2, producing an R2 correlation of 0.57 which is between fair and good.
Finally. Joe ran the R2 correlation test on PDO, the Pacfic Decadal Oscillation. He writes:
We know both the Pacific and Atlantic undergo multidecadal cycles the order of 50 to 70 years. In the Pacific this cycle is called the Pacific Decadal Oscillation. A warm Pacific (positive PDO Index) as we found from 1922 to 1947 and again 1977 to 1997 has been found to be accompanied by more El Ninos, while a cool Pacific more La Ninas (in both cases a frequency difference of close to a factor of 2). Since El Ninos have been shown to lead to global warming and La Ninas global cooling, this should have an affect on annual mean temperature trends in North America.
This PDO and TSI to surface temperature connection has also been pointed out in previous post I made here, for former California State Climatologist, Jim Goodridge. PDO affects the USA more than the Atlantic cycle (AMO) because we have prevailing westerly wind flow.
Here is how Joe did the data correlation:

Since the warm modes of the PDO and AMO both favor warming and their cold modes cooling, I though the sum of the two may provide a useful index of ocean induced warming for the hemisphere (and US). I standardized the two data bases and summed them and correlated with the USHCN data, again using a 11 point smoothing as with the CO2 and TSI. 
This was the jackpot correlation with the highest value of r-squared (0.83!!!). 


An R2 correlation of 0.83 would be considered “good”. This indicates that PDO and our surface temperature is more closely tied together than Co2 to surface temperature by almost a factor of 2.
But he didn’t stop there. He also looked at the last decade where it has been commonly opined that the Top 11 Warmest Years On Record Have All Been In Last 13 Years to see how well the correlation was in the last decade:

Since temperatures have stabilized in the last decade, we looked at the correlation of the CO2 with HCSN data. Greenhouse theory and models predict an accelerated warming with the increasing carbon dioxide. 


Instead, a negative correlation between USHCN and CO2 was found in the last decade with an R or Pearson Coefficient of -0.14, yielding an r-squared of 0.02. 


According to CO2 theory, we should see long term rise of mean temperatures, and while there may be yearly patterns of weather that diminish the effect of the short term, one would expect to see some sort of correlation over a decade. But it appears that with an R2 correlation of only 0.02, there isn’t any match over the past ten years.
As another test, this analysis was also done on Britain’s Hadley Climate Research Unit (CRU) data and MSU’s (John Christy) satellite temperature data:

To ensure that was not just an artifact of the United States data, we did a similar correlation of the CO2 with the CRU global and MSU lower tropospheric monthlies over the same period. We found a similar non existent correlation of just 0.02 for CRU and 0.01 for the MSU over troposphere. 


 So with R2 correlations of .01 and .02 what this shows is that the rising CO2 trend does not match the satellite data either.
Here are the different test correlations in a summary table:

And his conclusion:

Clearly the US annual temperatures over the last century have correlated far better with cycles in the sun and oceans than carbon dioxide. The correlation with carbon dioxide seems to have vanished or even reversed in the last decade. 
Given the recent cooling of the Pacific and Atlantic and rapid decline in solar activity, we might anticipate given these correlations, temperatures to accelerate downwards shortly. 

While this isn’t a “smoking gun” it is as close as anything I’ve seen. Time will give us the qualified answer as we have expectations of a lower Solar Cycle 24 and changes in the Pacific now happening.
References: 
US Temperatures and Climate Factors since 1895 , Joeseph D’Aleo, 2008
Persistence in California Weather Patterns,  Jim Goodridge, 2007
Phenomenological reconstructions of the solar signature in the Northern Hemisphere surface temperature records since 1600  Scafetta and West, 2007
The USHCN Version 2 Serial Monthly Dataset, National Climatic Data Center, 2007


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea177569c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Click image for a live interactive view of the National Climatic Data Center in Asheville, NC
Today started off terrible. I slipped in the bathtub last night at the hotel, and strained a back muscle and was so sore that just getting dressed and into the car was a chore. As a result, I was late getting to NCDC this morning. I’ve been popping Aleves today. Fortunately, they had slack built in so the day got started cheerfully with a review of the new Climate Reference Network with the principal scientists. It was a super meeting and I took many notes, I’ll have much to share later.
Next came a briefing on “Climate Science” from Tom Peterson, but I’m afraid I stole his thunder a little bit when I announced that I had already seen his presentation, which included an analysis of the Marysville USHCN Station.  See the powerpoint he presented here:aapg-san-antonio-peterson
Then came a personal tour of the Asheville CRN station by Dr. Bruce Baker. In addition to taking visible light photos, I also took matching IR photos from many angles. Bruce and his team were quite impressed with the IR camera I use, and he says he plans to buy a couple in use for siting surveys. He also plans to post the IR photos I took today on the CRN site to show how well the design and siting is free of IR influences.

I’ll have much more on all of this but I still have 8 more stations to survey plus an unexpected customer detour service call Friday to WDNN-TV in Dalton, GA which has some trouble with our weather display system there. So stay tuned for more details on the visit and questions that were asked and answered.
But the big news came with Dr. Baker providing me with a press release (new today) to post here for you all to see. CRN is getting completed and USHCN modernization is starting:
NOAA today announced it will install the last nine of the 114 stations as part of its new, high-tech climate monitoring network. The stations track national average changes in temperature and precipitation trends. The U.S. Climate Reference Network (CRN) is on schedule to activate these final stations by the end of the summer.
NOAA also is modernizing 1,000 stations in the Historical Climatology Network (HCN), a regional system of ground-based observing sites that collect climate, weather and water measurements. NOAA’s goal is to have both networks work in tandem to feed consistently accurate, high-quality data to scientists studying climate trends.
See the full press release here:
 press_release_042408_climatereferencenetwork
What this means: No more adjusted data, the raw data from CRN and from HCN-M is the real data and will be pristine, assuming the network is maintained. No more torturous gyrations of FILNET, SHAP, and TOBS. The downside is that a track record needs to be built up, the older data is also going to be revised with USHCN2 algorithms soon, and I’ll touch on that later.
One thing that Debra Braun said to me today in the meeting hit home: “our funding had been cut for the last two years, and we were unable to move forward until this year”. This made me think that perhaps some of the focus the surfacestations.org project brought to illuminating the deplorable condition of the network may have helped a little bit in convincing some legislators that it was time to get serious about allocating funding to complete the CRN and fix the USHCN. A little public embarrassment of the USHCN provided by all of us that have contributed to surfacestations.org may have helped. I’d sure like to think so.
I want to extend my heartfelt thanks to Dr. Baker, Debra Braun, Grant Goodge, and the entire CRN science team, plus Jeff Arnfield, and Steven Del Greco for answering all my questions and taking such careful time with me. Additionally I wish to thank Dr. Karl, and Assistant Director Sharon LeDuc for hearing my concerns  and offering ideas.
Everyone there at NCDC made me feel welcome and appreciated.
Most importantly, I want to thank you, my loyal readers and volunteers, because without your help, the trip and presentation I made would not be possible.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fd87aba',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"
Share this...FacebookTwitterGünther Aigner released a German video with the title “Die Alpengletscher im Klimawandel: Status quo“ (The Alps glaciers in climate change: status quo).
Hat-tip: Die kalte Sonne
Today global warming alarmists insist blaming climate change on man-made CO2 emissions. Yet, everywhere we look it’s difficult to find any correlation between CO2 and warming. Pre-industrial history shows that changes in CO2 in fact followed temperature changes.
Today we look at some climate charts of the European upper Ostalpen to look for hints what may be behind the warming since the late 20th century. We know glaciers there have been receding over the recent decades.
First is a mean temperature chart of the region for the May to September period going back 133 years:

Chart cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
Plotted are data from the Austrian ZAMG and the Swiss MeteoSchwiez, 5-year smoothed (green) and the linear trend (black). Clearly there’s been a long-term warming., but the vast share of the warming occurred since the late 1970s, after a 30-year period of cooling (since the early 1940s).
What could have happened since the early 1980s?
Of course CO2 emissions rose since 1980, and summer temperatures high in the Ostalpen rose. But is there something else?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One thing that could cause summertime temperatures to rise and glaciers to melt is sunshine. So Aigner plotted the number of sunshine hours for the May-September period for each year and produced the following chart:

Source: Cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
As the above chart shows, the sun has shined considerably more over the recent decades than it used to in the 1970s, or late 19th century. Today the region sees about a whopping 200 hours more sunshine than 120 years ago! More sunshine would mean more warmth.
The plot of sunshine hours indeed looks awfully similar to the plot depicting mean temperatures.
Aigner in the video then superimposed to two plots going back 50 years. Here’s how they compare:

Number of sunshine hours (orange) compared to the mean temperature in degrees Celsius (red). Each curve is 10-year smoothed. Source: Cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
The region receives over 100 hours of sunshine more today on average than it did 40 years ago. It shouldn’t be a surprise that the Alps have warmed and glaciers receded.
The real question is why is the region less cloudy today?


		jQuery(document).ready(function(){
			jQuery('#dd_cf0de1bdc5883a7b54f7c33f21051906').on('change', function() {
			  jQuery('#amount_cf0de1bdc5883a7b54f7c33f21051906').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEarly in 2011, NTZ readers and I entered a climate bet with Rob Honeycutt and climate warming dogmatist Dana Nuccitelli. The bet, which I dubbed the Honeycutt Climate Bet for Charity, was whether globally the 2011-2020 decade would be warmer or cooler than the previous 2001-2010 decade.
Myself and and a number of NTZ followers that bet the 2011-2020 decade would see no warming, or even cooling. Conversely Messieurs Honeycutt and Nuccitelli claimed global warming would continue, due to manmade greenhouse gas emissions, of course. To decide the winner of the bet, it was agreed that the RSS and UAH datasets would be used.
Now we are at the end of 2020, and the data suffices to declare a winner. The following chart is the latest UAH from Dr. Roy Spencer:

Chart: Dr. Roy Spencer.
As we can see, before 2016 the global mean temperature had been cooling since 1998, thus establishing what came to be known as “the hiatus”. The early half of the 2011-2020 decade had been running a bit cooler than the previous decade.
But then came the monster 2015/16 El Nino, a natural event occurring at the equatorial Pacific and it saved the warmists from losing the bet:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




That ENSO event was the strongest in memory, and thus drove global surface temperatures to a new level. As we can see from the UAH chart above, the overall current decade indeed did become warmer than the previous 2001-2010 decade. I haven’t crunched the numbers, but I think no one will dispute it.
Congratulations to the warmists on winning the bet, all thanks to the natural factors that coolists keep arguing in favor of.
Of course, the bet winners will boast and insist it’s all due to human activity. But it isn’t. Such a claim is Dominion-voting-machine science. The one deciding factor was the powerful mid-decade El Nino event, which is natural and has little to do with people burning fossil fuels.
Personally the results do not change my skeptic view of CO2’s role in climate at all. In fact the results only reinforce my view because it’s crystal clear that the 2016- 2020 warming was due to the El Nino, a natural factor, and not CO2.
200 euros for SOS Kinderdorf e.V. 
Those participating in the bet of course must honor their bets and pay the pledged amounts to a charity helping needy children. I myself will be paying to SOS Kinderdorf e.V., a Germany-based charity set up to aid needy kids, 200 euros (ca. 230 USD). I’ll be posting proof of payment in the days ahead, as soon as the bank statement comes in).
I’ll also be sending an e-mail to the other bet participants and asking them to pay up. Hopefully they are all still with us.
I don’t ever lose my climate bets. But as you can see, there’s always a first time.


		jQuery(document).ready(function(){
			jQuery('#dd_ec8e161b5077dbcad2b1a7613e27689f').on('change', function() {
			  jQuery('#amount_ec8e161b5077dbcad2b1a7613e27689f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitter

Climate alarmism dissenters getting increasingly vocal.
Yesterday I reported on how science editor Axel Bojanowski at German national daily DIE WELT had written a commentary on the deceptive use of a faulty climate hockey stick by ZDF German broadcasting.
Naturally whenever anything of the sort happens here in Germany, attack dog Stefan Rahmstorf of the alarmist Potsdam Institute rushes out to discredit the dissenter and defend the beloved but flawed chart.
Rahmstorf attacks DIE WELT’s Axel Bojanowski
And Rahmstorf again made the mistake of attacking the messenger of the news (Axel Bojanowski). It was probably the only option left for the ever haughty Rahmstorf, because the diligent DIE WELT editor based his stinging hockey stick commentary on statements made by four experts. Rahmstorf likely had no desire going after the four distinguished colleagues. So it was probably easier and safer for him to just take shots at DIE WELT and editor Bojanowski.
What follows is part of the Twitter exchange between Rahmstorf and Bojanowski, English translation follows:


Hallo Herr Rahmstorf, schade, dass Sie einen Fehler nicht zugeben können, sondern einen drauf setzen müssen.
Ich zitiere in meinem Artikel vier Experten mit Kommentaren zu Ihrer nicht in der Fachliteratur publizierten Kurve, alle fällen ein höchst kritisches Urteil⬇️    1/🧵 pic.twitter.com/pjd2Xl83Zf
— Axel Bojanowski (@Axel_Bojanowski) August 7, 2020


Rahmstorf writes to Bojanowski (see above):
Needless to say – the hockey stick curve is well confirmed by hundreds of scientists after more than two decades of further research – also by the latest data shown above. Its authors have received many awards.
And I’m going swimming now!

Bojanowski reply (in English):

Hello Mr. Rahmstorf, it is a pity you cannot admit a mistake, but even have to put one on top.
In my article, I quote four experts with comments on your curve, which has not been published in the technical literature, all of whom make a highly critical judgment.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Mann jumps in
Even Michael Mann, the creator of the false hockey stick, jumped in as well in typical hothead fashion, offering to “fix” Bojanowaski’s Twitter account:



Like Rahmstorf, Mann too avoided criticizing the four scientists underpinning Bojanowski’s comment, instead resorted to throwing insults and name-calling.
Kachelmann: “potsdumb unscientific nonsense of Rahmstorf”
Moreover, warmist (but non-alarmist) high-profile Swiss meteorologist Jörg Kachelmann got into the fracas, taking a hard shot as well, but at the alarmist camp, tweeting (English below):


Ein #Thread von @Axel_Bojanowski zum potsdämlich-unwissenschaftlichen Unsinn von @rahmstorf und dessen Pressesprecher @terliwetter, der den Sender @zdf als Outlet für die regelmässige Absonderung von Stuss missbraucht.
Wie lange noch? Keine journalistischen Standards mehr @zdf? https://t.co/eqyvz01pXc
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) August 8, 2020

Kachelmann’s tweet above in English:
A #thread from @Axel_Bojanowski on the pots-dumb unscientific nonsense of @Rahmstorf
and his press spokesman @terliwetter, who misuse broadcaster @zdf as an outlet for the regular secretion of bullshit.
How much longer? No more journalistic standards @zdf?”
In summary, there seems to be some progress being made on how science gets conducted in Germany. Increasingly dissenters are seeing victories and the public is growing weary of all the arrogance from certain scientists – especially in the fields of climate, COVID-19 and energy.
But it remains to be seen whether or not this long overdue trend gathers steam.


		jQuery(document).ready(function(){
			jQuery('#dd_fff92674c5c3098e9bc708715a5b22b9').on('change', function() {
			  jQuery('#amount_fff92674c5c3098e9bc708715a5b22b9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"
The Dr. Roger Pielke Sr. weblog today includes a letter from Dr. Joanne Simpson, recently retired.  He calls her “among the most preeminent scientists of the last 100 years”. It seems that she really spoke her mind on the subject of climate models and the problems of the changing measurement environment around climate monitoring stations.
The full letter is here on that weblog.
Excerpt: 

Since I am no longer affiliated with any organization nor receive any funding, I can speak quite frankly. […] The main basis of the claim that man’s release of greenhouse gases is the cause of the warming is based almost entirely upon climate models. 
We all know the frailty of models concerning the air-surface system. We only need to watch the weather forecasts. […] The term “global warming” itself is very vague. Where and what scales of response are measurable? One distinguished scientist has shown that many aspects of climate change are regional, some of the most harmful caused by changes in human land use. 
No one seems to have properly factored in population growth and land use, particularly in tropical and coastal areas. 
[…] But as a scientist I remain skeptical. I decided to keep quiet in this controversy until I had a positive contribution to make. […] Both sides (of climate debate) are now hurling personal epithets at each other, a very bad development in Earth sciences. 

I agree, enough of this sniping. 
Witness the cordial exchange I have with Atmoz, a graduate student at the University of Arizona in Tucson. We see things differently, each of us has made some good analyses and each of us has made some mistakes, but we don’t insult each other over it.
Though I do wish he and others would remove the cloaks of anonymity. Science has never been advanced by an anonymous person, there’s always a real person with a name at the center of discovery and progress.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0e96a5c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe  green dream, with all its scenic beauty and nature conservation, has arrived in northern Germany. But now that green dream faces more obstacles. 

All is not so wunderbar when it comes to Germany’s wind power outlook.
Germany’s Renewable Energy Sources Act, passed in 2000, was intended to ensure the generation of “green” electricity. Operators of wind turbines were guaranteed subsidies for a period of twenty years  – with the hopes the technology would develop to such an extent that it would operate economically without subsidies.
20 years later, the wind turbines are still not competitive reports trendsderzukunft.de here.
Plagued by high costs
The first problem with the old turbines? The costs. They require comparatively frequent maintenance. “This drives up the costs, which is why operation is not economical in many cases.” reports trendsderzukunft.de. “A study has shown that at an electricity price of 3.375 cents euro per kilowatt-hour, only 23 percent of the old plants can be operated without subsidies.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Feed-in requirement running out
The second problem: “The Feed-in priority” law which forced power grid operators to purchase wind electricity. “However, it is unclear whether this regulation will continue to apply despite the expiration of subsidies,” says trendsderzukunft.de. “This question will probably have to be cleared up by the courts in the end. However, many operators will probably not wait for this and prefer to shut down the old wind turbines instead.”
Sites will have to be abandoned
Trendderzukunft.de. adds: “By 2025, there is a risk of losing 2,300 to 2,400 megawatts of capacity every year.”
Moreover, legal hurdles prevent repowering, which involves “replacing several old turbines with one new and larger one.” The problem, reports trendsderzukunft.de  is that at many existing locations “there is a height limit for wind turbines. The installation of the latest generation of wind turbines is therefore not possible there. However, smaller turbines are no longer available on the market. In many cases, therefore, the sites simply have to be abandoned.”
“Imminent catastrophe”
For that reason, around 1,000 of 1691 wind turbine sites are affected in Lower Saxony alone and are currently not available for so-called repowering. “Lower Saxony’s Minister of Energy and Environment, Olaf Lies (SPD), speaks of an imminent ‘catastrophe’ for wind power in Germany,” writes trendsderzukunft.de.


		jQuery(document).ready(function(){
			jQuery('#dd_4233aa9cba44a662eec750787b2ca3fe').on('change', function() {
			  jQuery('#amount_4233aa9cba44a662eec750787b2ca3fe').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

As this year draws to a close, I think back about what I’ve accomplished on this blog in the last year, and it occurs to me that I have a lot of people to thank. It truly has been a team effort in a lot of ways, with many people contributing from many different angles to help make the work I’m doing a possibility.
First and foremost, I’d like to thank Steve Thompson of Assemblyman Rick Keene’s office. It was his mention in an email to me “that Russ Steele and I ought to get together” that started me on the path to study climate change from the data gathering aspect. Of course Russ and I had similar ideas, but we just didn’t know about each other, and knowing that there’s somebody else nearby that thought like I did whom I could converse with, was really a boost. Of course on the political front, I should also thank the local activist group “Esplande League”, because if they hadn’t worked so hard to keep me from being re-elected to the local school board, I never would have had the time to pursue this research.
I owe Russ Steele a lot, not only for the many stations he’s surveyed, and for the encouragement and support, but also for introducing my work to many people, including Steve McIntyre of Climate Audit. It wasn’t until Steve took notice that things really began to take off. Steve has been most gracious in helping to promote my work and for offering me the ability to co-author on his blog.
And there are many others, I can think of the many volunteers on surfacestations.org that have contributed many ideas, data sorting and spreadsheet macros that saved me time and effort, and made the project’s data analysis better. Gary Boden, Chris Dunn, Joel McDade, John Goetz, Barry Wise, and Eric Gamberg have all made significant contributions to the project via surveyed stations and or improvements to the survey process and analysis.
Super surveyor Don Kostuch, has been traveling the country and surveys new stations every week. He is leader of the station surveyors not only in terms of quantity, but of quality too. His surveys are always carefully done. 15 year old Kristen Byrnes and her dad have surveyed almost all of New England single handedly.
One volunteer, Arthur Edelstein, I owe a great deal to because he did some significant data capture and collation that I wouldn’t have been able to do myself in the fraction of time that he did it in.
I owe Dr. Roger Pielke Sr. a debt of gratitude for his faith in my work and his encouragement, along with his assistant, Dallas Staley, who has pulled many an obscure request for data or publications out of nowhere, even after hours.
Then there’s all the other blogs and newspaper authors out there that have promoted what I’m doing.  Joe D’Aleo of ICECAP comes to mind, and does Barry Hearn and Steve Milloy of JS for publishing my “How not to measure temperature” series, and Kate from Small Dead Animals for being a regular traffic driver.  There’s Evan Jones, who is my most prolific and enthusiast commenter, along with regulars George M., Papertiger, Larry Sheldon, and Stan Needham. Let’s not forget Steven Mosher and Jeez, for putting up with my silly rants at dinner with Mac at AGU. Jeez also footed the dinner bill, and so deserves double thanks.
Local blogger Lon Glazner deserves a nod for blogging some early support and for some mental stimulus on thermometers that got me fired up last spring.
In the newspaper realm, Ryan Olson of the local Chico Enterprise Record, not only for the stories he’s done, but for putting up with my complaints about Moveable Type and helping me migrate to WordPress where I’ve been able to make a better product.  I thank Bill Steigerwald of the Pittsburgh Tribune whose article launched me into national attention. And finally, Evan, who did a really balanced and fair article even though I feared the worst.
Then there’s the 300 plus volunteers for www.surfacestations.org Thank you each and every one.
I owe you all a debt of gratitude. Thank you. If I’ve missed anyone, don’t be shy about speaking up.
There’s a few that deserved coal this year, but I’ll leave them nameless.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1ae9c6f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

**March 1:** Mexico’s peso crisis is only the latest example of how misguided monetary policy can wreak havoc on economiclife and disrupt society. At a Book Forum, editors James A. Dorn and Roberto Salinas‐​León discussed their new book, _Money and Markets in the Americas_. The book provides a framework for thinking about how to end the monetary chaosthat has plagued Latin America and how to energize the market‐​liberal order that is now emerging in the Americas. SteveHanke, professor of applied economics at Johns Hopkins University; Sir Alan Walters, former economic adviser to MargaretThatcher; and Ed Hudgins, Cato’s director of regulatory studies, provided comments. 



**March 7:** At a Capitol Hill Policy Forum, “It’s Time to Tear Up the Tax Code: The National Retail Sales Tax,“cosponsored with the National Taxpayers Union and Citizens for an Alternative Tax System, Cato’s director of fiscal policystudies Stephen Moore led a discussion on completely replacing the tax code with a retail sales tax. The panel included Rep.Dan Schaefer (R‐​Colo.), Rep. Billy Tauzin (R‐​La.), David Keating of the National Taxpayers Union, David Burton of theArgus Group, and Vic Krohn of Citizens for an Alternative Tax System. 



**March 8:** A debate over whether regulations, like legislation, should have to be signed by the president before they becomelaw was the centerpiece of a Cato Policy Forum on “Ending Regulation As We Know It: Legislative Deregulation inthe Dock.” Professor Marci Hamilton of Cardozo Law School discussed the highly controversial constitutional questionssurrounding legislative delegation, and Professor David Schoenbrod of New York Law School, author of Power withoutResponsibility: How Congress Abuses the People through Delegation, addressed the policy ramifications of executive‐​branchlawmaking. Nadine Strossen, president of the American Civil Liberties Union, explained how civil liberties are curtailed by theabrogation of legislative responsibility. David Hawkins, senior attorney at the National Resources Defense Council, spokeabout how ending the delegation of lawmaking power to the executive branch will affect regulatory practice; and Rep. J. D.Hayworth (R‐​Ariz.) described legislation he has sponsored to require all lawmaking regulations to be affirmatively adopted byCongress and signed into law by the president before they take legal effect. 



**March 18:** Cato hosted a Capitol Hill Policy Briefing on the question, “Is the Immigration Bill in the NationalInterest?” A panel featuring Ben Wattenberg of the American Enterprise Institute, Scott Hoffman of Americans for TaxReform, and Stuart Anderson and Stephen Moore of the Cato Institute addressed the economic, demographic, and politicalimplications of the pending legislation. 



**April 3:** A Cato Book Forum celebrated publication of _Oil, Gas, and Government: The U.S. Experience_. Author RobertL. Bradley Jr., president of the Institute for Energy Research, debunked the “market failure” arguments for oil and gasregulation, including those based on a theory of natural monopoly, predatory pricing, and national security. 



**April 9:** A Policy Forum, “The New Prohibition? Freedom and Tobacco under Siege by the FDA,” asked whether theFood and Drug Administration is acting outside its statutory authority and threatening free speech. Sam Kazman of theCompetitive Enterprise Institute, Larry Pilot of McKenna & Cuneo, L.L.P., Jack Calfee of the American Enterprise Institute,and Matthew Myers of the Coalition on Smoking or Health debated the issue. 



**April 9:** The conflict over Taiwan is the latest in a series of rough spots in the U.S.-Chinese relationship. At a Policy Forumentitled “Tensions in the China Sea,” Ted Galen Carpenter, Cato’s vice president for defense and foreign policy studies;James Przystup, director of the Asian Studies Center at the Heritage Foundation; and Selig Harrison, senior associate at theCarnegie Endowment for International Peace, discussed the steps that Washington could take to avoid a showdown overTaiwan. 



**April 10:** The Honorable Vojt_​ch Cepl, a justice on the Czech Constitutional Court and currently E. L. WiegandDistinguished Visiting Professor of Democratization at Georgetown University, discussed the legal foundations of civil societyat a Roundtable Luncheon with Cato policy staff. Justice Cepl attributed the Czech Republic’s successful transition fromcommunism to capitalism to the combination of a tradition of civil society in Bohemia and Moravia and a carefully implementedpolicy of legal “lustration”: a clean legal break with the collectivist past. 



**April 11:** The inclusion of medical savings accounts (MSAs) in Medicare reform proposals has sparked an intense debateover “adverse selection”–the notion that MSAs will appeal only to the young and healthy, leaving traditional Medicare to servethe elderly and sick. Proponents of MSAs maintain that they will appeal to everybody because they minimize theout‐​of‐​pocket costs of both the healthy and the sick. At a Policy Forum entitled “Medical Savings Accounts and AdverseSelection,” Leonard Burman, tax analyst at the Congressional Budget Office; Edwin Hustead, chair of the MSA WorkGroup of the American Academy of Actuaries; and Peter Ferrara, general counsel and chief economist at Americans for TaxReform, debated the issue. 



**April 17:** Many Americans anticipate that the Federal Communications Commission will be doing less under theTelecommunications Act of 1996. The Clinton administration, however, has proposed increasing the FCC’s budget by almost$47 million. In light of those opposing expectations, Cato organized “The Future of the FCC,” a Policy Forum that askedwhat the administration’s policy bodes for proposals to phase out the FCC altogether. Discussants included Gregory Simon,chief domestic policy adviser to Vice President Al Gore; James Gattuso, vice president of policy research at Citizens for aSound Economy; and Kenneth Robinson, attorney at law. 



**April 17:** While the political tide of regulatory and statutory reform appears to have been stemmed for the time being by theenvironmental lobby, many people still criticize the centralized command‐​and‐​control regulatory structure of mostenvironmental laws. At a Policy Forum entitled “The Politics and Policy of Environmental Protection: Beyond the 104thCongress,” Kenneth Chilton, executive director of the Center for the Study of American Business, and Debra Knopman,director of the Center for Innovation and the Environment at the Progressive Foundation, discussed both what should be doneto reform the status quo and how to do it in the current political climate. 



**April 18:** As the Senate prepares to consider congressional term limits, Sen. John Ashcroft (R‐​Mo.) has launched anunprecedented online petition drive to highlight popular support for that constitutional change. At a Policy Forum entitled“Emerging Technologies and the Fight for Congressional Term Limits,” Senator Ashcroft discussed the role that termlimitation and emerging technologies can play in promoting participation in the democratic process. Introductory remarks wereby Paul Jacob, executive director of U.S. Term Limits. 



**April 23:** Recent Supreme Court opinions and the California Civil Rights Initiative have catapulted affirmative action to centerstage as we enter the political season. Not to be outdone, Congress itself will revisit its 30‐​year‐​old policy of groupentitlements this year as it debates H.R. 2128, the Equal Opportunity Act of 1995, a bill to end preferences in federalprograms, contracting, and employment. At a Capitol Hill Policy Forum entitled “An End to Preferential Treatment?“Cato’s director of constitutional studies Roger Pilon moderated a critical discussion of affirmative action. Panelists includedClint Bolick of the Institute for Justice, Faye Anderson Douglass of the Policy Institute, Linda Chavez of the Center for EqualOpportunity, Rep. Charles Canady (R‐​Fla.), and Rep. Tom Campbell (R‐​Calif.). 



**April 25:** Cato hosted a Roundtable Luncheon with Hisahiko Okazaki, former Japanese ambassador to Saudi Arabia andThailand. Okazaki spoke about U.S.-Japanese relations and East Asian security issues. 



**April 26:** The Institute hosted a City Seminar in New York City that featured a keynote address by John Stossel,investigative reporter for ABC’s 20/20. Other speakers included José Piñera, president of the Center for Pension Reform andcochairman of Cato’s Project on Social Security Privatization, and Cato’s president Edward H. Crane, director of fiscal policystudies Stephen Moore, and director of telecommunications and technologies studies Lawrence Gasman. 
"
"

NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
Part II
By Basil Copeland and Anthony Watts
 
 
In Part I, we presented evidence of a noticeable periodicity in globally averaged temperatures when filtered with Hodrick-Prescott smoothing. Using a default value of lambda of 100, we saw a bidecadal pattern in the rate of change in the smoothed temperature series that appears closely related to 22 year Hale solar cycles. There was also evidence of a longer climate cycle of ~66 years, or three Hale solar cycles, corresponding to slightly higher peaks of cycles 11 to 17 and 17 to 23 shown in Figure 4B. But how much of this is attributable to value of lambda (λ). Here is where lambda (λ) is used in the Hodrick-Prescott filter equation:

The first term of the equation is the sum of the squared deviations dt = yt − τt which penalizes the cyclical component. The second term is a multiple λ of the sum of the squares of the trend component’s second differences. This second term penalizes variations in the growth rate of the trend component. The larger the value of λ, the higher is the penalty. 
For the layman reader, this equation is much like a tunable bandpass filter used in radio communications, where lambda (λ) is the tuning knob used to determine the what band of frequencies are passed and which are excluded. The low frequency component of the HadCRUT surface data (the multidecadal trend) looks almost like a DC signal with a complex AC wave superimposed on it. Tuning the waves with a period we wish to see is the basis for use of this filter in this excercise.
Given an appropriately chosen, positive value of λ, the low frequency trend component will minimize. This can be seen in Figure 2 presented in part I, where the value of lambda was set to 100. 

Figure 2 – click for a larger image
A lower value of lambda would result in much less smoothing. To test the sensitivity of the findings reported in Part I, we refiltered with a lambda of 7. The results are shown in Figures 3 and 4.

Figure 3 – click for a larger image 
As expected, the smoothed trend line, represented by the blue line in the upper panel of Figure 3, is no longer as smooth as the trend in the upper panel of Figure 1 from Part I. And when we look at the first differences of the less smoothed trend line, shown in Figure 4, they too are no longer as smooth as in Figure 2 from Part I. Nevertheless, in Figure 4, the correlation to the 22 year Hale cycle peaks is still there, and we can now see the 11 year Schwabe cycle as well. 


Figure 4 – click for a larger image 
The strong degree of correspondence between the solar cycle peaks and the peak rate of change in the smoothed temperature trend from HadCRUT surface temperature data is seen in Figure 5. 

Figure 5 – click for a larger image
The pattern in Figure 4, while not as eye-catching, perhaps, as the pattern in Figure 2 is still quite revealing. There is a notable tendency for amplitude of the peak rate of change to alternate between even and odd numbered solar cycles, being higher with the odd numbered solar cycles, and lower in even numbered cycles. This is consistent with a known feature of the Hale cycle in which the 22 year cycle is composed of alternating 11 year phases, referred to as parallel and antiparallel phases, with transitions occurring near solar peaks. 
Even cycles lead to an open heliosphere where GCR reaches the earth more easily. Mavromichalaki, et. al. (1997), and Orgutsov, et al. (2003) contend that during solar cycles with positive polarity, the GCR flux is doubled. This strongly implicates Galactic Cosmic Ray (GCR) flux in modulating global temperature trends. The lower peak amplitudes for even solar cycles and the higher peak amplitudes for odd solar cycles shown in Figure 4 appears to directly confirm the kind of influence on terrestrial climate postulated by Svensmark in Influence of Cosmic Rays on Earth’s Climate (1998)From the pattern indicated in Figure 4, the implication is that the “warming” of the late 20th century was not so much warming as it was less cooling than in each preceding solar cycle, perhaps relating to the rise in geomagnetic activity. 
It is thus notable that at the end of the chart, the rate of change after the peak associated with solar cycle 23 is already in the negative range, and is below the troughs of the preceding two solar cycles. Again, it is purely speculative at this point, but the implication is that the underlying rate of change in globally averaged temperature trends is moderating, and that the core rate of change has turned negative.It is important to understand that the smoothed series, and the implied rates of change from the first differences, in figures 2 and 4, even if they could be projected, are not indications of what the global temperature trend will be. 
There is a cyclical component to the change in global temperature that will impose itself over the underlying trend. The cyclical component is probably dominated by terrestrial dynamics, while the smoothed series seems to be evidence of a solar connection. So it is possible for the underlying trend to be declining, or even negative, while actual global temperature increases because of positive cyclical factors. But by design, there is no trend in the cyclical component, so that over time, if the trends indicated in Figures 2 and 4 hold, global warming will moderate, and we may be entering a phase of global cooling.
Some are probably wondering which view of the historical correspondence between globally averaged temperatures and solar cycles is the “correct” one: Figure 2 or 4? 
Such a question misconstrues the role of lambda in filtering the data. Here lambda is somewhat like the magnification factor “X” in a telescope or microscope. A low lambda (less smoothing) allows us to “focus in” on the data, and see something we might miss with a high lambda (more smoothing). A high lambda, precisely because it filters out more, is like a macroscopic view which by filtering out lower level patterns in the data, reveals larger, longer lived processes more clearly. Both approaches yield valuable insights. In Figure 2, we don’t see the influence of the Schwabe cycle, just the Hale cycle. In Figure 4, were it not for what we see in Figure 2, we’d probably miss some similarities between solar cycles 15, 16, and 17 and solar cycles 21, 22, and 23.In either case, we are seeing strong evidence of a solar imprint in the globally averaged temperature trend, when filtered to remove short term periodicities, and then differenced to reveal secular trends in the rate of change in the underlying long term tend in globally averaged temperatures. 
At one level we see clear evidence of bidecadal oscillations associated with the Hale cycle, and which appear to corroborate the role of GCR’s in modulating terrestrial climate. At the other, in figure 4B, we see a longer periodicity on the order of 60 to 70 years, correspondingly closely to three bidecadal oscillations. If this longer pattern holds, we have just come out of the peak of the longer cycle, and can expect globally average temperature trends to moderate, and increased likelihood of a cooling phase similar that experienced during the mid 20th century. 
In Lockwood and Fröhlich 2007 they state: “Our results show that the observed rapid rise in global mean temperatures seen after 1985 cannot be ascribed to solar variability, whichever of the mechanisms is invoked and no matter how much the solar variation is amplified.” . Yet, as Figure 5 demonstrates, there is a strong correlation between the solar cycle peaks and the peak rate of change in the smoothed surface temperature trend.
The periodicity revealed in the data, along with the strong correlation of solar cycles to HadCRUT surface data, suggests that the rapid increase in globally averaged temperatures in the second half of 20th century was not unusual, but part of a ~66 year climate cycle that has a long history of influencing terrestrial climate. While the longer cycle itself may be strongly influenced by long term oceanic oscillations, it is ultimately related to bidecadal oscillations that have an origin in impact of solar activity on terrestrial climate.
 
UPDATE: We have had about half a dozen people replicate from HadCRUT data the signal shown in figure 4 using FFT and traditional filters, and we thank everyone for doing that. We are currently working on a new approach to the correlations shown in figure 5, which can yield different results using alternate statistical methods. A central issue is how to correctly identify the peak of the solar cycle, and we are looking at that more closely. As it stands now, while the Hodrick-Prescott filtering works well and those results in figures 2,3, and 4 have been replicated by others, but the correlation shown in figure 5 is in question when a Rayleigh method is applied, and thus figure 5 is likely incorrect since it does not hold up under that and other statistical tests. There is also an error in the data point for cycle 11. I thank Tamino for pointing these issues out to us. 
We are continuing to look at different methods of demonstrating a correlation. Please watch for future posts on the subject.
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
References: 
 
Demetrescu, C., and V. Dobrica (2008), Signature of Hale and Gleissberg solar cycles in the geomagnetic activity, Journal of Geophysical Research, 113, A02103, doi:10.1029/2007JA012570.
Hadley Climate Research Unit Temperature (HadCRUT) monthly averaged global temperature data set (description of columns here) 
J. Javaraiah, Indian Institute of Astrophysics, 22 Year Periodicity in the Solar Differential Rotation, Journal of Astrophysics and Astronomy. (2000) 21, 167-170
Katsakina, et al., On periodicities in long term climatic variations near 68° N, 30° E, Advances in Geoscience, August 7, 2007
Kim, Hyeongwoo, Auburn University, “Hodrick-Prescott Filter” March 12, 2004 
M. Lockwood and C. Fröhlich, Recent oppositely directed trends in solar climate forcings and the global mean surface air temperature, Proceedings of the Royal Society of Astronomy doi:10.1098/rspa.2007.1880; 2007, 10th July
Mavromichalaki, et. al. 1997 Simulated effects at neutron monitor energies: evidence for a 22-year cosmic-ray variation, Astronomy and Astrophysics. 330, 764-772 (1998) 
Mavromichalaki H, Belehaki A, Rafios X, et al. Hale-cycle effects in cosmic-ray intensity during the last four cycles ASTROPHYS SPACE SCI 246 (1): 7-14 1997.
Nivaor Rodolfo Rigozo, Solar and climate signal records in tree ring width
from Chile (AD 1587–1994), Planetary and Space Science 55 (2007) 158–164
Ogurtsov, et al., ON THE CONNECTION BETWEEN THE SOLAR CYCLE LENGTH AND TERRESTRIAL CLIMATE, Geophysical Research Abstracts, Vol. 5, 03762, 2003 
Royal Observatory Of Belgium, Solar Influences Data Analysis Center, monthly and monthly smoothed sunspot number. (Description of data here)
Svensmark, Henrik, Danish Metorological Institute, Influence of Cosmic Rays on Earth’s Climate, Physical Review Letters 15th Oct. 98
Wikipedia, Hodrick-Prescott Filter January 20, 2008


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9feb48b0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn my recent post here I wrote about a ZDF story on an Expert Assessment Report, led by Prof. Dr. Kai Konrad of the Max Planck Institute and a team of finance researchers, on Europe’s and Germany’s climate policy. The report is titled:
Climate Policy Between Emissions Prevention and Adaptation
Expert Assessment By The Scientific Advisory Board Of The Federal Ministry of Finance
Note: The report itself is not a product of the Max Planck Institute, as some have mistakenly believed. The lead author is Dr. Kai Konrad of the Max Plank Institute, who is also vice chairman of the Finance Ministry’s Scientific Advisory Board, the actual producer of the assessment report. The members of the Scientific Advisory Board participating in the expert assessment are listed below at the end of this post.
You’ll recall the assessment report was so damning that the Finance Ministry took it down from its website. When you read the following summary and conclusion you’ll see how it completely contradicts the government’s current policy, which is to prevent CO2 emissions and to subsidise alternative energy. This is a finding that was embarrassing for the government.
Note that the authors of the assessment report take the position that CO2 is bad for the climate, i.e. the more CO2 that is produced, the worse the climate will become. They are finance experts after all, and not climate experts – obviously.
I’ve translated the all-important Part 4, Summary and Conclusion (bold print is my emphasis), which is as follows:
4. Summary and Conclusion
Economic and political action on global warming can be categorised under two kinds of measures: 1) measures that aim to slow down global warming (prevention) and 2) measures that aim to react to global warming (adaptation).
With adaptation measures, the beneficiary and the cost-bearer are the same. Decisions concerning many adaptation measures can thus be decided by the private economy. In the cases where this is not possible, the extent of adaptation measures can be handled by the local, regional or national politics.
But when it comes to measures for preventing CO2 emissions, the circle of beneficiary and the cost bearer splits apart. A meaningful reduction in emissions through uncoordinated, single country initiatives cannot be achieved. Effective emissions reduction with respect to global climate protection can be accomplished only through global coordination. In the past, global coordination has proven to be difficult and hardly successful. Despite various international attempts and considerable use of resources on the part of some countries, a worldwide climate policy has not been reached.
The theory of international public good offers an economic explanation as to why the international climate policy has not reached its ambitious goals up to now. That’s why suspicions that the current efforts will not lead to any success are being confirmed.
This assessment yields the following results:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




• The uncoordinated, single-country go-it-alone approach leads to unachievable emissions reductions. Many polluters hardly participate in avoiding emissions. It has to be expected that only the more populated, economically strongest, environmentally aware and climatically threatened countries will make any notable efforts to undertake emissions reductions.
• Efforts by single countries to act as a leader in climate protection and to influence climate policy by imposing emissions reductions on itself can cause other countries to slack off in their own climate-policy efforts rather than intensifying them. As a result, taking a leadership role in climate policy leads to, as a rule, higher costs in that country without assuring any decisive improvement in the global climate.
• Special efforts and leadership initiatives made by individual countries also do not necessarily improve the situation for a global climate agreement, but rather can actually imperil an agreement. Diminishment of remaining benefits arising from worldwide climate agreements make the realisation of an agreement more improbable.
• Also unfavourable are agreements among groups nations of the international community of nations. Such agreements greatly burden  the participating countries economically, and serve to benefit the countries that do not participate. Despite the high costs, the positive climate effects of such group-nation agreements can end up being very small. Moreover, coalitions of nations can actually worsen the chances of an international worldwide climate treaty.
However, in no way do these arguments speak against continuing international negotiations. Effective international climate agreements are urgently needed. The arguments listed above do, however, speak against going it alone nationally, taking a leadership role, in preventing CO2 emissions.
When it comes to implementing measures for adaptation to climate change, there are no problems like those listed above. Measures for adapting to climate change do not have the problems that measures for prevention have. Adapting to climatically related environmental changes do not have the “free-rider” problem, where one incurs the costs and the other reaps the benefits. The circle of beneficiary and cost-bearer are mutual when it comes to adaptation measures. The strategy of adaptation thus offers opportunities for a unilateral, cost-effective national climate policy in a wide variety of impact areas (e.g. against flooding or storm damage). At the same time, such a policy augments the chances of an international emissions limitation.
• The adaptation strategy leads to an immediate climate cost reduction in one’s own country, independent of  international agreements.
• If a country invests in national adaptation measures, it also improves its bargaining strength in negotiations for a climate treaty.
• When all countries take up adaptation strategies, it results in – when compared to an ideal, worldwide combination of both instruments – a strain that in the end favours adaptation instead of prevention. The economic-political result would be worse than the one from a non-existing prosperity-maximizing world government, but better than the result that would arise from foregoing an adaptation strategy.
• Without adaptation measures, more prevention measures would have to be undertaken due to reasons of precaution and in view of the uncertainty of climate impacts from irreversible CO2 emissions. Adaptation buys governments time to more precisely research climate impacts.
The way for some especially motivated industrial countries to use comprehensive unilateral early contributions and subsides for alternative energy is misguided with regards to a timely, binding and adequately scaled climate policy. Even worse, it is to be feared that this policy not only has been and is very expensive for Germany and Europe, but also that it is an obstacle to reaching an effective worldwide climate policy. In view of the fact that emissions reduction is an internationally public good and in view of strategy effects, the Advisory Board recommends options for adaptation to climate change be examined and pursued more vigorously by single countries than in the past. The strategy of adaptation does not only ensure immediate adaptation to climate change, but also increases the chances for an effective international agreement to reducing emissions.
Directory of members of the Scientific Advisory Board at the Federal Ministry of Finance
Prof. Dr. Clemens Fuest (chairman)
Prof. Dr. Kai A. Konrad (vice chairman)
Prof. Dr. Dieter Brümmerhoff
Prof. Dr. Thiess Büttner
Prof. Dr. Werner Ehrlicher
Prof. Dr. Lars P. Feld
Prof. Dr. Lutz Fischer
Prof. Dr. Heinz Grossekettler
Prof. Dr. Günter Hedtkamp
Prof. Dr. Klaus-Dirk Henke
Prof. Dr. Johanna Hey
Prof. Dr. Bernd Friedrich Huber
Prof. Dr. Wolfgang Kitterer
Prof. Dr. Gerold Krause-Junk
Prof. Dr. Alois Oberhauser
Prof. Dr. Rolf Peffekoven
Prof. Dr. Dieter Pohmer
Prof. Dr. Helga Pollak
Prof. Dr. Wolfram F. Richter
Prof. Dr. Ulrich Schreiber
Prof. Dr. Hartmut Söhn
Prof. Dr. Christoph Spengel
Prof. Dr. Klaus Stern
Prof. Dr. Marcel Thum
Prof. Dr. Alfons Weichenrieder
Prof. Dr. Dietmar Wellisch
Prof. Dr. Wolfgang Wiegard
Prof. Dr. Berthold Wigger
Prof. Dr. Horst Zimmermann
Share this...FacebookTwitter "
"
Share this...FacebookTwitter      While world governments bedwet over a fantasized climate catastrophe taking place 100 years out, mankind could be facing a potential catastrophic food shortage. A worthhile read (see link below).
A fungus threatens 20% of the world's food supply.
      The disease is Ug99, a virulent strain of black stem rust fungus (Puccinia graminis), discovered in Uganda in 1999, threatens the world’s wheat supply. Read the scary details here: http://www.wired.com/magazine/2010/02/ff_ug99_fungus/all/1
      Wheat provides 20% of all calories consumed by humans. According to Nobel laureate Norman Borlaug, father of the Green Revolution:
This thing has immense potential for social and human destruction.
      According to wired.com, the fungus attacks the stem of the wheat plant, causing it to wither and die.
Stem rust is the polio of agriculture, a plague that was brought under control nearly half a century ago as part of the celebrated Green Revolution. After years of trial and error, scientists managed to breed wheat that contained genes capable of repelling the assaults of Puccinia graminis, the formal name of the fungus.
But now it’s clear: The triumph didn’t last.
      The new fungus has spread from Africa and into the Middle East. It would only take a  traveller with a single spore on his shirt to transport it to the USA and Canada.
The pathogen makes its presence known to humans through crimson pustules on the plant’s stems and leaves. When those pustules burst, millions of spores flare out in search of fresh hosts.
      It goes to show that nature has a bag full of nasty tricks, and there’s nothing you can do to stop her. All you can do is adapt, hopefully quickly enough. But if you waste your time trying to appease her, and don’t invest your resources wisely in adapting, you’ll get eliminated.
Share this...FacebookTwitter "
"
This thread debates the Miskolczi semi-transparent atmosphere model. 
The link with the easiest introduction to the subject is http://hps.elte.hu/zagoni/Proofs_of_the_Miskolczi_theory.htm


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ea1d30e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Is it possible to address environmental problems, such as pollution, without resorting to the traditional regulatory approach? In the new issue of the _Cato Journal_ , economists Geoffrey Black, D. Allen Dalton, Samia Islam, and Aaron Batteen offer one prominent example of allowing the market to work. By examining the New York City Watershed Memorandum of Agreement (MOA), the authors demonstrate how large‐​scale externalities can be successfully internalized with minimal state intervention.



In 1997, New York City entered into an agreement in which it assigned the area’s watershed communities the property rights to continue developing, despite the fact that some of those activities degraded the city’s drinking water. This assignment placed the burden of water quality on the city. Once responsibility was established, the government opted to buy lands that were contributing to water quality degradation, instead of building a multibillion‐​dollar filtration system. In turn, the residents and landowners upstream were compensated for development restrictions incurred from the agreement. In short, the New York City Watershed MOA is the first of its kind. “The negotiations forged a new method for dealing with externalities showing how … solutions could be facilitated and used in wide‐​reaching economic conflicts,” the authors write.



In September 2012, seven weeks before the presidential election, a Congressional Research Service (CRS) study claimed that there is no evidence that changes in top marginal tax rates have had any impact on U.S. economic growth since World War II. The mainstream media, politicians, and political groups favoring higher taxes on the wealthy widely cited the study as evidence against Mitt Romney’s economic program and in favor of President Obama’s plan to raise top marginal rates. In “Marginal Tax Rates and U.S. Growth,” economists Jason E. Taylor and Jerry L. Taylor revisit the CRS analysis and pinpoint its fatal flaw. “Our results are consistent with what economists have long understood: that a tradeoff exists between income redistribution and economic growth,” they conclude.



Despite pronounced differences in medical financing arrangements, the United States and other countries throughout the Organization for Economic Cooperation and Development (OECD) have witnessed a tremendous growth in health care costs over the last several decades. In “The Medical Care Cost Ratchet,” scholars Andrew Foy, Christopher Sciamanna, Mark Kozak, and Edward J. Filippone explain that health care spending increases over time as new technologies that confer only modest clinical benefits are incorporated into the traditional standard of care. They argue, furthermore, that encouraging individuals to economize on nonemergent health care decisions would help bend the cost curve over time. “Reform efforts,” they conclude, “should focus on rejuvenating market forces that have been systematically suppressed.”



Forecasts of future economic activity underlie any budget revenue projection. However, public choice models of political decisionmaking suggest that government agencies such as the Congressional Budget Office (CBO) and Office of Management and Budget (OMB) face pressures that are likely to result in systematically biased forecasts— whereby, for instance, rosy growth forecasts are rewarded and underforecasted growth penalized. In his article, economist Robert Krol finds that while the CBO, consistent with the private‐​sector forecast, has a downward bias, the OMB estimates indicate a significant upward bias—which is “interpreted to mean executive branch political pressure influences the forecast.” Other contributors include Thomas L. Hogan and William J. Luther on “The Explicit Costs of Government Deposit Insurance,” Paul H. Rubin on “Pathological Altruism and Pathological Regulation,” and Paul Ballonoff with “A Fresh Look at Climate Change.”



The Winter 2014 issue also features reviews of books on the importance of Ayn Rand’s ideas, a theoretical framework for understanding the financial crisis, and the famous bet between Julian Simon and Paul Ehrlich.
"
"

In a recent speech to the Washington‐​based think tank Resources for the Future, EPA Administrator Gina McCarthy promoted the White House’s new Clean Power Plan by: (a) appealing to science and disallowing any debate about it; (b) making statements unsupported by the science; (c) praising the economic analysis behind the plan; and (d) announcing rules that economic analysis says won’t work and will cost too much.



In other words, it was business as usual in the world of climate policy.



She started her speech by saying that scientists are as sure that humans cause climate change as they are that smoking causes cancer, and “we are way past any further discussion or debate.…don’t debate about climate change any longer because it is our moral responsibility to act.”



From there she focused on the harms from extreme weather events, attributing the California drought to carbon dioxide emissions, as well as increased storms, wildfires and floods. She said anthropogenic climate change (i.e. global warming) leads to more extreme heat and, amazingly enough, more extreme cold. And she linked weather‐​related economic threats facing families and small businesses to anthropogenic climate change.



Now whatever you do, don’t question the science. For many years we have been told to rely exclusively on the UN Intergovernmental Panel on Climate Change (IPCC) for official truth on all climate topics. There are, of course, lots of reasons to mistrust the IPCC, including its past blunders, the conduct of its disgraced and discredited chair Rajendra Pachauri, and its clique‐​like report‐​writing process. But for now, let’s play the game and turn to the IPCC.





A growing body of economic analysis over the years has indicated that the models overstate the potential savings from energy efficiency programs.



In 2012 the IPCC published a Special Report on Extreme Weather (SREX), gathering up the available knowledge on storms, droughts, etc, and their possible connection to climate change generally and carbon dioxide emissions specifically.



Contrary to McCarthy’s claim, the SREX singled out the U.S. as a region where “droughts have become less frequent, less intense or shorter.” Worldwide there is only “limited to medium” regional evidence regarding changes in floods because the records are sparse and the effects are confounded with changes in land use and engineering. “Furthermore,” they said, “there is low agreement in this evidence, and thus overall low confidence at the global scale regarding even the sign of these changes.”



Does this sound like the level of confidence associated with the link between smoking and cancer?



Overall the IPCC’s attribution of a causal link between extreme weather and carbon dioxide emissions was the limpest possible: “There is evidence that some extremes have changed as a result of anthropogenic influences, including increases in atmospheric concentrations of greenhouse gases.” But, they went on, there is only low confidence in attribution of tropical cyclone activity to anthropogenic influences, and “Attribution of single extreme events to anthropogenic climate change is challenging” — UN speak for “we’d go further if we could but even we can’t torque the evidence that far.”



They also made it clear that economic vulnerability to weather is a function of a nation’s wealth, adding “Increasing exposure of people and economic assets has been the major cause of long‐​term increases in economic losses from weather‐ and climate‐​related disasters (high confidence). Long‐​term trends in economic disaster losses adjusted for wealth and population increases have not been attributed to climate change, but a role for climate change has not been excluded.”



Do doctors say “trends in lung cancer have not been attributed to cigarette smoking, but a role for tobacco has not been excluded”? Of course not. McCarthy’s invocation of scientific certainty and prohibition on further debate was mere demagoguery.



After boasting about the extensive research and consultation that went into the rule, McCarthy then said that it will reduce household utility costs, a prediction based on engineering studies behind the energy efficiency rules in the Clean Power Plan. But do these programs really save households money?



A growing body of economic analysis over the years has indicated that the models overstate the potential savings from energy efficiency programs. New evidence from a large‐​scale randomized field experiment has confirmed this. Conducted by a team of economists from Berkeley and MIT, the study tracked more than 30,000 households in the federal Weatherization Assistance Program. Participants in the program went through household energy audits using a government‐​approved engineering model to estimate the savings from undertaking a fully subsidized efficiency upgrade.



By comparing before‐ and after‐​data, and comparing against households that did not undergo weatherization, the authors showed that the engineering models were way off, exaggerating the energy savings 2.5-fold, with the result that the renovations cost twice the value of the subsequent energy savings. Even taking account of social and environmental benefits the rate of return on the program was about -9.5 percent annually, in other words the costs greatly exceeded the benefits.



The authors also found that the implicit cost of carbon dioxide emission reductions in the program were about $330 per tonne, roughly ten times the administration’s own estimate of the Social Cost of Carbon. In other words, taking the administration’s science and economics at face value, a major component of their climate plan costs $330 per tonne for emission cuts they themselves value at $38 per tonne.



McCarthy told her audience not to question her science and to respect the research behind their climate policy. If you doubt her analysis, you will definitely find the policy plan misguided. The problem is that, even if you accept her science and economics, it’s still misguided.
"
"

The President on Tuesday signed the continuing resolution that funds the government through September and (gasp) keeps the sequester cuts intact. Now that it appears sequestration isn’t going away (and yet the earth continues to spin merrily on its axis), the focus should be on how this small step might be extended.



Unfortunately, the reaction to Paul Ryan’s relatively modest budget indicates the fight for smaller government will continue to be an uphill battle in the current political climate. The Ryan budget has brought predictable condemnation from the political left. Sen. Harry Reid called it “extreme,” and the New York Times called it “the worst of the Ryan budgets.” The plan’s sin: restraining the growth of federal spending to 3.4 percent instead of 5 percent.





Serious policymakers need to start explaining to the American people how the federal government doing less will do more to enhance their personal and economic well‐​being.



While there are some of us that don’t feel the Ryan budget goes nearly far enough, it was never going to become law as is with a Democratic White House and Senate. But here’s the important question: does sequestration and Ryan’s follow‐​up give proponents of limited government a reason to be optimistic?



Currently, the answer is no.



Sequestration has yet to cause a public revolt and the markets have treated it with indifference throughout. Although the cuts that happened under sequestration are hardly an occasion for a victory lap, they are a small and welcome bit of evidence that government can spend less without society as we know it coming to an end.



Sequestration reduces federal spending by $44 billion this year, which is a relatively small sum considering that total spending will be around $3.5 trillion. The budget deficit alone is projected to be around $850 billion. That means to balance the budget this year, the spending cuts would have to be almost 20 times larger. However, sequestration barely scratches entitlement programs, which dominate the federal budget and are the source of our long‐​term fiscal problems. And because it doesn’t actually terminate any agencies or programs, spending can be restored in the future.



So in the big picture, sequestration hasn’t changed all that much. Federal spending is still on a dangerous upward trajectory. Unfortunately, while there is much talk about the need to reform the welfare state in order to make it more affordable, the underlying desirability of our centralized system of cradle‐​to‐​grave entitlement programs remains virtually unchallenged on Capitol Hill. And while the Pentagon’s bloated budget is being challenged, only a handful of policymakers are questioning the underlying desirability of the United State’s global military footprint.



The size and scope of the federal government needs to be dramatically reduced. Republicans are commonly understood to be in favor of limited government, but their track record suggests otherwise. Federal spending went through the roof under Republican rule in the previous decade. After reclaiming the House in 2010, Republicans positioned themselves as the frugal alternative to the debt‐​happy Obama administration. Unfortunately, the manner in which Republicans handled sequestration indicates that they are still unwilling or incapable of making a principled argument for smaller government.



Ever the defenders of the warfare state, Republicans bemoaned the sequestration cuts made to the Pentagon’s budget. Mirroring the administration’s orchestrated hysteria over cuts to domestic programs, some Republicans even claimed that the cuts would “gut” the military — a specious assertion considering that military spending under sequestration would be higher in real dollars than peak Cold War spending.



So if sequestration doesn’t do a whole lot to shrink the size and scope of government, what about Mr. Ryan’s proposal? In his budget, Ryan calls for ending Obamacare, but that wouldn’t end the federal government’s involvement in health care. Ryan says that higher education subsidies should be capped, but that wouldn’t end the federal government’s involvement in education. How the federal government delivers the goods might change, but a more efficient government isn’t the same as limited government. And if the goal is limited government, as Republicans often claim, then there has to be actual limits on what the government is involved in.



The tough reality is that the average voter is content to spend other people’s money on programs that they benefit from. And every government program is backed by a special interest that will fight tooth‐​and‐​nail to protect their share of Uncle Sam’s loot.



That’s an obviously difficult dynamic to overcome. But if progress is to be made, serious policymakers need to start explaining to the American people how the federal government doing less will do more to enhance their personal and economic well‐​being. That means making the case for limited government. Until that happens, the question of whether or not proponents of limited government should be optimistic will remain no.
"
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
Share this...FacebookTwitter


Pachauri CCX advisory board member

EIKE, the European Institute for Climate and Energy, a sponsor of the 4th ICCC in Chicago, has dug up a some information on the Chicago Climate Exchange CCX, where Maurice Strong and a host of other influential leftists are among its board members.

When it comes to cap & trade, i.e. emissions trading, we’re talking serious money here. According to Dr. Richard Sandor, an economist and CCX architect, cap-and-trade represents a $10 trillion per-year market.
Recognizing the enormous profit potential, Al Gore’s Generation Investment Management (GIM)  purchased a 10 percent stake in CCX and became the company’s fifth largest co-owner. Yet all this coziness shouldn’t surprise anyone.
But taking a closer look at the CCX Advisory Board, we find among the likes of Joe Kennedy, Ed Begly, Thomas Lovejoy – Rajendra Pachauri. It’s known that Pachauri has huge investments in carbon markets, but this is ridiculous. Talk about a scam. Yet another gate to add to the long list.
Conflict of interest? Nahhhh
Update: FYI, CCX Directors here and External Advisory Board members here.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOther blogs have mentioned today a report from the German financial daily, Handelsblatt here, but didn’t provide many details, and so I’ve decided to shine a little more light on the article. It is indeed frightening.
The German government has been generously subsidising renewable energy sources for years now, and it’s going to cost the German consumer a bundle – and soon.
The big price driver is solar energy. Year after year more and more panels are getting installed on German roofs and far surpassing even the most optimistic projections. But that shouldn’t be a surprise because Germany’s Energy Feed in Act (EEG Gesetz) guarantees solar energy system operators a fixed tariff for 20 years, making solar energy systems extremely lucrative for those who have them.
According to the Rhine Westphalia Institute for Business Research (RWI) the net costs of all photovoltaic system installed between 2000 and 2010 add up to a whopping $107 billion, Subsidies for renewable energy are going out of control.
An open letter written by Johannes Lackmann, former director of the German Association of Renewable Energy, caused many to take a closer look. Lackmann warns:
Companies are positioning themselves on the same square as the old industries, who failed to modernise and keep up with the market demands because they came to rely on generous subsidies paid by governments to survive. The EEG Act must not be allowed to be misused as cushion to sleep on.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed the extreme comfort of the EEG subsidies will lead to an additional 9000 MW of solar energy capacity to be mounted on roofs this year alone in Germany. It’s a run-away train that will have serious consequences.
Recall that solar energy generators are guaranteed payment for 20 years. For systems going online this year, an average of almost $0.40/kw-hr is guaranteed by law until 2031. By comparison conventional energy is traded on the EEX  exchange for just over 6 cents. The huge difference is passed on to the German consumer.
According to the RWI the net costs for all photovoltaic systems installed between 2000 and 2010 over their 20 year operating life will add up to $107 billion. According to the Handelsblatt:
This sum is more than one quarter of the entire German annual federal budget. Yet the amount of solar energy as a share of the total energy produced is still puny despite the huge subsidies. It is only 1 percent.
Just the photovoltaic systems installed this year will lead to an additional $33 billion dollars in costs over their 20 year operating life. Electrical energy rates will climb 10% in 2011. Energy giant RWE just announced it will increase rates by 7.3% in August.
Sure Germany has cut back the subsidies – some, but the prices of solar panels and systems have fallen even faster, and so as a result they are even more lucrative. Lackmann thinks reductions in subsidies are long overdue, and the industry will not be doing itself a favour attempting to fight them off.

They don’t provide any incentive for investment in R&D. Leading German solar companies invest less than 2% of their sales turnover in R&D. This is far below what a company like Siemens invests in R&D.

Share this...FacebookTwitter "
"
According to wire reports, temperatures reached their lowest point in 30 years, reaching to -2°C in the capital, Riyadh, and to -6°C in mountainous regions blanketed by snow.  At least 10 people have died in the country as a weather system driven South from Siberia sent temperatures plummeting. Below are some pictures of snow from that region.
  
click for larger images
Apparently its gotten so bad (or they just aren’t prepared to deal with it) that King Saud ordered that government assistance should be given in the affected areas, which witnessed sub-zero temperatures this week.

I had to laugh at the photo above and the caption:  “Saudi Arabians are used to getting stuck in the sand, but snow is a new challenge for many.” It almosts seems Pythonesque.
Meanwhile, many roads were flooded by heavy rains in the nearby country of Dubai, which attracts sun-hungry tourists with its year-round blue skies. Roofs in some luxury hotels and office blocks were leaking water and several schools asked parents to keep their children home on Wednesday. It’s hard to imagine getting a “rain day” in the middle east.

While I’m enjoying pointing out these uncommon phenomena, I’d also point out that even though both the northern and southern hemispheres have both seen some record cold events in the past 6 months, that doesn’t necessarily equate to “climate change”. Still, something seems afoot as we are seeing more and more events like this. Maybe the massive La Niña now stretching across the Pacific ocean has something to do with this.
Oh but wait…there’s more!

Snow was seen yesterday atop Maui’s Mount Haleakala  – see story
Yeah, somethings up.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea124d90b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterH/T: Benny Peiser
This is the final day of the Deutsche Welle’s Global Media Forum, this year’s conference is titled “The Heat is On – Climate Change and the Media”. If any conclusion can be drawn, it is that elite warmists are extremely frustrated. Read here.
Bob Ward:
British journalists don’t know difference between fact and fiction.
Peiser’s GWPF report reads: “But he also concedes that there have been grave mistakes made by researchers”. And Ward called for scientists to handle their findings and knowledge responsibly. Ward goes on to say:
The IPCC is too slow in correcting the faults.
Naomi Oreskes, non-consensus denialist:
The statements from scientists are so greatly disconnected from the media in the USA because the journalists unknowingly and inaccurately repeat what was said.
…so-called climate skeptics are nothing but “contrarians” and can’t be taken seriously because their critique isn’t scientifically based.
Can you hear their teeth gnashing?
Share this...FacebookTwitter "
"
You may recall the previous post where Basil Copeland and I looked at correlations between HadCRUT global temperature anomaly and sunspot numbers. This is similar, but looks at the Pacific Decadal Oscillation (PDO) and uses the same Hodrick-Prescott (HPT) filter as before on the HadCRUT global temperature anomaly data and the PDO Index.

click for a larger image –
NOTE: the purple line is a monthly warming rate, to get decadal values, multiply by 120
This graphic provides some context to what may be happening with the PDO. In the upper panel we’ve plotted the PDO (in red), a smoothed PDO (in light blue), and our analysis of the bidecadal variation in warming rates.
From the PDO data itself, it is just too soon to be able to tell whether the current cool phase is just one of the shorter cycles, or whether it is the beginning of a longer term cycle like we saw back in the 1950’s and 1960’s. It is tempting, when looking at the warming rate cycles, to believe that we’ve just come out of a 60-66 year “Kerr” climate cycle, and are on the cusp of a cool phase like we see for the 1950’s and 1960’s.
But if you look closely at the end of the purple curve for our warming rate cycle, it seems to be about ready to turn back up. Now we do not want to put too much stock in the end values of a series that has been smoothed with HP filtering. So it could still be on a downward trend.
Then, to make it all the more interesting, we have solar cycle 23 lingering on. Considering that also, confidence is higher that we will continue to see a relative respite in the rate of warming and that we’re not likely to see our warming rate cycle jump back to where it was during solar cycles 22-23. But whether we see a full blown interlude between two strong warming trends, like we saw during the 1950’s and 1960’s, remains to be seen.
In other words, as we saw with Easterbrook’s analysis, we can be reasonably confident in projecting at least no further warming for a while. For that to happen, the purple warming rate curve must not only turn back upwards, it must rise into the region of positive values, and continue to rise for several years. If solar cycle 24 turns out to be a weak solar cycle, and there are historical precedents for cycle length suggesting it is likely to be weak, that probably isn’t happening.
I’ll have more on solar cycles 23 and 24 coming up in the next day or so.
So, in summary; probably no net warming for awhile, and maybe a period of extended cooling as in the mid 20th century. It all depends on whether this current PDO shift is a short term or longer term event such as we saw in the mid 20th century.
This is inline with the article in today’s UK Telegraph, saying: 
“Global warming will stop until at least 2015 because of natural variations in the climate, scientists have said. Researchers studying long-term changes in sea temperatures said they now expect a “lull” for up to a decade while natural variations in climate cancel out the increases caused by man-made greenhouse gas emissions. 
The average temperature of the sea around Europe and North America is expected to cool slightly over the decade while the tropical Pacific remains unchanged. This would mean that the 0.3°C global average temperature rise which has been predicted for the next decade by the UN’s Intergovernmental Panel on Climate Change may not happen, according to the paper published in the scientific journal Nature.”
There’s a similar article in Yahoo News.
The paper by Keenlyside et al entitled “Advancing decadal-scale climate prediction in the North Atlantic sector” from the Nature website


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f4de349',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAlso the Danish Meteorological Institute (DMI) projects a sturdy Arctic sea ice extent for this July, meaning no falling summer ice extent trend since 2007! The climate alarms are being muffled. 
Snowfan here gives us the latest on global mean temperature and Arctic sea ice.
After the year’s low in June 2020, with an anomaly of +0.48°C from the 1981-2010 WMO climate mean, the global 2-meter temperatures (black line) depicted below shows the July 16, 2020 analysis and forecast up to July 23.

Source: here
Both the anomalies of the global 2-meter temperatures from the WMO mean (black line) and especially the temperatures on the SH (blue line) continue to fall, with the deviations on the SH repeatedly falling below the zero line. This also pulls the global temperatures (black line) down to near zero in the forecast.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But also the temperatures in the Northern Hemisphere (red line) have had a falling trend since the end of February 2020, having since reached a new annual low. There has been no more global warming since 2016. Note: With 81% of the sea surface, the southern hemisphere has the largest energy storage on earth.
Surprise DMI projection
Arctic sea ice trend growth in July 2020?

Source: DMI
A surprising DMI forecast was issued on July 14, 2020 which projects strong growth of Arctic sea ice areas for July 2020. If this expert forecast is correct, it would mean there’s been a strongly positive summer trend since 2007 – instead of the ridiculous Al Gore complete meltdown.


		jQuery(document).ready(function(){
			jQuery('#dd_9c06936df01f592ca3b87782180bf27a').on('change', function() {
			  jQuery('#amount_9c06936df01f592ca3b87782180bf27a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
 
Given that the sun is so quiet lately (click image – no sunspots) and there is talk of an ebb in its next solar cycle 24, it bears looking into the details of our primary climate driver.
 The National Geographic Channel has a TV special on the sun, sunspots, climate, etc. They interviewed several people involved in that debate. It includes interviews with Judith Lean, Leif Svalgaard, and others.
It will be shown on the National Geographic Channel. It’s titled: Naked Science ‘Solar Force’.
It goes out on Tuesday 30th October 2007 at 9pm ET and again at midnight ET.
It also rebroadcasts on Thursday 1st November 2007 at 10pm ET.
TV listings can be found on http://channel.nationalgeographic.com/channel/ if you are interested.
UPDATE: description from the NGC website –
The suns energy seems to be constant, but this gigantic nuclear reactor is in a continual state of flux. National Geographic Channel (NGC) reveals the latest scientific information that is uncovering the hidden ways that fluctuations in the suns output influence our climate. See how a radical experiment supports the idea that the suns invisible cosmic rays may have a visible impact on our weather, and find out how a new NASA program could shed new light on how solar wind impacts Earth. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2f7f181',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“This is the age of maximal surveillance,” says Bruce Schneier — the so‐​called “security guru” who spoke at Cato’s Second Annual Surveillance Conference in October. Surveillance is now ubiquitous and virtually unescapable for those who wish to enjoy the conveniences of modern life. And while the government downplays the importance of its access to citizen’s metadata, as Schneier observed, this data is about us — everything about us. “Metadata reveals who we are,” he said. “Google knows more about me than I know — because Google remembers better.”



Throughout the day‐​long conference, experts from around the country discussed the perils of national and global surveillance, as well as prospects for encryption and other tools to protect privacy in an ever‐​changing technological landscape. Sen. Patrick Leahy (D-VT), a longtime proponent of surveillance reform in Congress, lauded the passage of the USA Freedom Act, a reform bill first introduced at Cato’s surveillance conference in 2013 and signed into law in 2015. He particularly praised Cato’s role as a consistent champion for privacy rights. “I want to thank the Cato Institute,” he said. “You worked very hard on this — when we had people starting to back away, you helped give them courage.”



 **Immigration from 1965 to 2015**  
Fifty years ago, President Lyndon B. Johnson signed the Immigration Act of 1965 — a defining component of the American legal immigration system. Its passage meant that, after years of discriminatory Progressive Era immigration policies, immigrants from Western Europe no longer had legal preference over immigrants from places like Asia and Eastern and Southern Europe. At the same time, however, the Act introduced new limitations on immigrants from countries like Mexico and Canada. To commemorate the anniversary of this law, Cato hosted a conference, “Fifty Years after Reform: The Successes, Failures, and Lessons from the Immigration Act of 1965.”



Rep. Ruben Gallego (D-AZ), the son of immigrants from Colombia and Mexico, opened the morning by praising immigration as an engine of economic growth. Jim Gilmore, a 2016 Republican presidential candidate and former governor of Virginia, warned that deporting all illegal immigrants would require turning America into a “police state.” And Bill Richardson, the former governor of New Mexico, called for expanding visas for high‐​skilled workers. “While American businesses try to adapt and grow amidst economic and technological revolutions, our outdated immigration policies are holding us back from our full potential,” he said.



 **Preparing for the UN’s Climate Change Conference**  
Just before November’s highly anticipated gathering of world leaders in Paris for the United Nations Climate Change Conference, Cato hosted its own conference: “Preparing for Paris: What to Expect from the U.N.‘s 2015 Climate Change Conference.” Speakers discussed what is at stake in Paris, where leaders will attempt to negotiate a climate change agreement; the potential legal implications of such an agreement; as well as the latest scientific developments in climate science. “There is increasing evidence that the threat from global warming is overstated,” said Judith Curry of the Georgia Institute of Technology.



She denounced the “stifling” of more moderate positions on the effects of climate change, saying that anyone who diverges even slightly from the Intergovernmental Panel on Climate Change consensus is considered a “denier.” Richard Tol of the University of Sussex, whom Cato’s Pat Michaels called the “preeminent environmental economist in the world,” delivered the keynote address, in which he predicted that not much will happen in Paris. “For the last 20 or 25 years, governments have tried to reduce greenhouse gas emissions,” he said — to little success. “We should be dismayed,” he said, that so much money has been wasted on these efforts.



 **Rethinking Monetary Policy**  
This year’s 33rd Annual Monetary Conference, attended by over 200 people, was the first hosted by Cato’s new Center for Monetary and Financial Alternatives — a center dedicated to moving monetary and financial regulatory policies toward a more rules‐​based, free‐​market system. The conference featured distinguished speakers like St. Louis Fed president James Bullard, Richmond Fed president Jeffrey Lacker, and Stanford economist John B. Taylor. Bullard gave the opening address, in which he argued that a stable interest rate peg is “a realistic theoretical possibility.”



Claudio Borio of the Bank for International Settlements, whom _The Economist_ has called “one of the world’s most provocative and interesting monetary economists,” proposed challenging some of the “deeply‐​held beliefs” of monetary policy, including the idea that monetary policy is “neutral,” or that deflations are always disastrous. Rep. Bill Huizenga (R‐​Mich.), who chairs the House Financial Services Subcommittee on Monetary Policy and Trade, discussed his legislation requiring the Fed to adopt an explicit policy rule, among other reforms. “The Fed ultimately must be accountable to the people’s representatives, as well as to the hard‐​working taxpayers themselves,” he said.



 **Evaluating the TTIP**  
“It’s been quite a year for trade policy,” Cato’s Dan Ikenson remarked at the beginning of Cato’s conference, “Will the Transatlantic Trade and Investment Partnership Live Up to Its Promise?” “I think we are about to embark on a robust debate in the United States about the TTIP.” Cato’s conference helped prime for that debate, featuring leading trade experts who analyzed TTIP’s status and geopolitical implications. The day opened with a keynote address from Shawn Donnan of the Financial Times, who predicted that the deal may prove difficult to get through the Obama administration. Despite the fact that negotiations began in 2013, he said, “A lot of the conversations feel like they’re just getting started.” The conference, which was broadcast on C-SPAN, continued with discussions of what is at stake in the negotiations — from GMO regulations, to labor and environmental standards, to intellectual property issues.



Speakers including Michelle Egan, a professor at the American University’s School of International Service, and Swedish economist Fredrik Erixon, succinctly explained some of the complex issues under negotiation, like standards‐​related trade barriers and TTIP’s effect on global trade policy. The participants also wrote essays on crucial aspects of TTIP, all of which are available online at www​.cato​.org.
"
"
Share this...FacebookTwitterYet another Russian scientist believes the Arctic is set for cooling and thus increasing sea ice, this reported in the German version of the Russian online news RIA NOVOSTI (see links below). Scientist Vladimir Sokolov says:
The warming that occurred in the Arctic has swung back to cooling and sea ice that melted over the past years is recovering.
http://de.rian.ru/science/20100427/126088756.html, The English version is here: http://en.rian.ru/russia/20100427/158771845.html
Arctic sea ice reached a historic minimum in 2007 when it shrank to 4.28 million sq km. But the trend now appears to have reversed. According to the weather observation administration Roshydromet, it has grown by a fifth reaching 5.2 million square km in 2009, Sokolov said at the Petersburg Research Insttitute for the Arctic and Antarctic on Tuesday.
Sokolov calls predictions of continued shrinking Arctic ice “incorrect”.
He says the cooling is due to the polar night and the associated missing sunlight, and this as a result will lead to ice formation. Some scientists are warning that politicians and corporations who promise lucrative oil and gas projects in the Arctic may have made dramatic miscalculations. The researchers say that no warming will take place, instead cooling will impact the earth over the next decades.
Share this...FacebookTwitter "
"

The Wall Street Journal is reporting that a bunch of venture capitalists are now backing Norway’s Think electric-car company. Their plan is to bring the company’s Think City car to the U.S. in 2009 and build it here as well.
I drive a 2002 Ford Think electric car, the open frame model. I’m pretty happy with it, at 3 cents a mile, and I’ve put about 300 miles on it around town since buying it 3 weeks ago. It has gotten a lot of attention in my hometown of Chico, and people are constantly asking me how much it cost and where could they get one? The town is blessed with many alternate back routes, so I don’t have to travel the main congested roads.
The U.S. version is expected to travel 110 miles on a single charge and kind of resembles Smart’s ForTwo. The company expects the car to be priced under $25,000. It’s looking for a site in the U.S. to build U.S.-spec models because it’s cheaper to build an entire line here than it is to ship from Europe, thanks to the weak dollar. Maybe Michigan politicians should be making some calls to Oslo.
The Think City is already in production in Europe, and the company is rushing to produce 10,000 units this year for sale there. One of the people behind the VC funding says they could sell 30,000 to 50,000 Think City cars in the U.S. See Norway’s Think to Produce, Sell Small Electric Cars in U.S. (from WSJ.com) 
There is another car that Think has in the pipeline, and it is pretty cool looking, see it below:



Its new concept, called “Ox”, looks to be a much more mainstream vehicle than any of the minicars the company sells overseas.
But the name needs to change, because I don’t want my friends teasing me that I’m driving an “Ox car”. I think they were shooting for some spin on “Oxygen” but missed the mark.
Roughly the size of a Scion xB, the front-wheel-drive Ox MPV will have a 60-kW electric motor and a range of 124 miles on a full charge. It can be charged via a normal household outlet. Charging the car to 80% will take just an hour using a special charger, while a full charge will take 12 hours. 
The company is planning to use either sodium or lithium-ion batteries, and there’s a strip of solar cells running down the center of the roof. The Ox is built on an interchangeable platform, so a coupe body style with a larger motor and batteries or a taxicab configuration could also be manufactured.
Unfortunately, the Ox looks to be a true concept, with no firm date on when we could expect to see it on the road. The other unfortunate part is that Think doesn’t have a presence in the U.S. General Electric recently invested $4 million into Think, though, so don’t give up hope of one day seeing the “Ox” on the street. More photos here.


Bring a production version of the Ox with a different name, though, and I’d expect people to line up.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ec9aba2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
NOTE: Please note that part 2 is now online, please see it here.
I recently plotted all four global temperature metrics (GISS, HadCRUT, UAH, RSS) to illustrate the magnitude of the global temperature drop we’ve seen in the last 12 months. At the end of that post, I mentioned that I’d like to get all 4 metrics plotted side-by-side for comparison, rather than individually.
Of course I have more ideas than time these days to collate such things, but sympathetic reader Earle Williams voluntarily came to my rescue by collating them and providing a nice data set for me in an Excel spreadsheet last night.
The biggest problem of course is what to do with 4 different data sets that have different time spans. The simplest answer, at least for a side by side comparison is to set their time scales to be the same. Satellite Microwave Sounder Unit (MSU) data from the University of Alabama, Huntsville (UAH), and Remote Sensing Systems (RSS) of Santa Rosa, CA only go back to 1979. So the January 1979-January 2008 period is what we’ll concentrate on for this exercise as it very nearly makes up a 30 year climate period. Yes, I know some may call this an arbitrary starting point, but it the only possible point that allows comparison between land-ocean data-sets and the satellite data-sets.
Here is the first graph, the raw anomaly data as it was published this month by all the above listed sources:

Here is the source data file for this plot and subsequent plots.
4metrics_temp_anomalies.txt
I also plotted a magnified view to show the detail of the lat 12 months with notations added to illustrate the depth of the anomaly over the past 12 months.

March 2005 to January 2008, magnified view – click for larger image
I was particularly impressed with the agreement of the 4 metrics during the 1998 El Niño year as well as our current 2008 La Niña year.
I also ran a smoothed plot to eliminate some of the noise and to make the trends a bit more visible. For this I used a 1 year (12 month) average.

Again there is good agreement in 1998 and in 2008. Suggesting that all 4 metrics picked up the ENSO event quite well.
The difference between these metrics is of course the source data, but more importantly, two are measured by satellite (UAH, RSS) and two are land-ocean surface temperature measurements (GISS, HadCRUT). I have been critical of the surface temperature measurements due to the number of non-compliant weather stations I’ve discovered in the United States Historical Climatology Network (USHCN) from my www.surfacestations.org project.
One of the first comments from my last post on the 4 global temperature metrics came from Jeff in Seattle who said:
Seems like GISS is the odd man out and should be discarded as an “adjustment”.
Looking at the difference in the 4 times series graphs, differences were apparent, but I didn’t have time to study it more right then. This post today is my follow up to that examination.
Over on Climate Audit, there’s been quite a bit of discussion about the global representivity of the GISS data-set due to all of the adjustments that seem to have been applied to the data at locations that don’t seem to need any adjustments to compensate for things like urban heat islands. Places like Cedarville, CA and Tingo Maria, Peru both illustrate some of the oddities with the adjustment methodology used by NASA GISS. One of the issues being discussed is the application of city nightlights (used as a measure of urbanization near the station) as a proxy for UHI adjustments to be applied to cities in the USA. Some investigation has suggested that the method may not work as well as one might expect. There’s also been the issue of whether of not stations classified as rural are truly rural.
So with all of this discussion, and with this newly collated data-set handed to me today, it gave me an idea. I had never seen a histogram comparison done on all four data-sets simultaneously.
Doing so would show how well the cool and warm anomalies are distributed within the data. If there is a good balance to the distribution, one would expect that the measurement system is doing a good job of capturing the natural variance. If the distribution of the histogram is skewed significantly in either the negative or positive, it would provide clues into what bias issues might remain in the data.
Of course since we have a rising temperature trend since 1979, I would expect all 4 metrics to be more distributed on the positive side of the histogram as a given. But the real test is how well they match. All four metrics correlate well in the time series graphs above, so I would expect some correlation to be present in the histogram as well. The histograms you see below were created from the raw data from 1979-2008. No smoothing or adjustments of any kind were made to the data. The “unadjusted” data in this source data file were used: 4metrics_temp_anomalies.txt
First we have the satellite data-set from UAH:

University of Alabama, Huntsville (UAH) Microwave Sounder Data 1979-2008 – click for larger image
The UAH data above looks well distributed between cool and warm anomaly. A slight warm bias, but to be expected with the positive trend since 1979.
Next we have the satellite data-set from RSS:

Remote Sensing Systems (RSS) Microwave Sounder Data 1979-2008 – click for larger image
At first I was surprised at the agreement between UAH and RSS in the percentages of warm and cool, but then I realized that these data-sets both came from the same instrument on the spacecraft and the only difference is methodology in preparation by the two groups UAH and RSS. So it makes sense that there would be some agreement in the histograms.
Here we have the land-ocean surface data-set from HadCRUT:

Hadley Climate Research Unit Temperature data 1979-2008 – click for larger image
Here, we see a much more lopsided distribution in the histogram. Part of this has to do with the positive trend, but other things like UHI, microsite issues with weather station placement, and adjustments to the temperature records all figure in.
Finally we have the GISS land-ocean surface data-set:

NASA Goddard Institute for Space Studies data 1979-2008 – click for larger image
I was surprised to learn that only 5% of the GISS data-set was on the cool side of zero, while a whopping 95% was on the warm side. Even with a rising temperature trend, this seems excessive.
When the distribution of data is so lopsided, it suggests that there may be problems with it, especially since there appears to be a 50% greater distribution on the cooler side in the HadCRUT data-set.
Interestingly, like with the satellite data sets that use the same sensor on the spacecraft, both GISS and HadCRUT use many of the same temperature stations around the world. There is quite a bit of data source overlap between the two. But, to see such a difference suggests to me that in this case (unlike the satellite data) differences in preparation lead to significant differences in the final data-set.
It also suggests to me that satellite temperature data is a more representative global temperature metric than manually measured land-ocean temperature data-sets because there is a more unified and homogeneous measurement system, less potential bias, no urban heat island issues, no need of maintaining individual temperature stations, fewer final adjustments, and a much faster acquisition of the data.
One of the things that has been pointed out to me by Joe D’Aleo of ICECAP is that GISS uses a different base period than the other data-sets, The next task is to plot these with data adjusted to the same base period. That should come in a day or two.
UPDATE1: I’ve decided to make this a 3 part series, as additional interest has been generated by commenters in looking at the data in more ways. Stay tuned for parts 2 and 3 and we’ll examine this is more detail.
UPDATE2: I had mentioned that I’d be looking at this in more detail in parts 2, and 3. However it appears many have missed seeing that portion of the original post and are saying that I’ve done an incomplete job of presenting all the information. I would agree for part1, but that is what parts 2 and 3 were to be about.
Since I’m currently unable to spend more time to put parts 2 and 3 together due to travel and other obligations, I’m putting the post back on the shelf (archived) to revisit again later when I can do more work on it, including show plots for adjusted base periods.
The post will be restored then along with the next part so that people have the benefit of seeing plots and histograms done on both ways. In part 3 I’ll summarize 1 and 2.
In the meantime, poster Basil has done some work on this of interest which you can see here.
UPDATE3: Part 2 is now online, please see it here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0ceeee4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Here’s something you don’t see every day; an exploding comet.  In fact the last time this comet did this was in 1892.
Comet 17P/Holmes is now larger than Jupiter. Astronomer Eric Allen of Quebec’s Observatoire du Cégep de Trois-Rivières combined images he captured on three consecutive nights (Oct. 25, 26 and 27) and placed them beside a picture of Jupiter scaled to the same distance as the comet, as shown above. More at www.spaceweather.com
The comet is visible to the naked eye, and looks even better through a telescope. This would be a good excuse to go visit the Chico Community Observatory in upper Bidwell Park Sunday night and have them swing the telescope by for a look. Or if you want to spot it yourself, here is a sky map.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea306c1e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On Oct.10, Tom Daschle (D-SD), Senate Majority Leader and potential presidential candidate, ordered his own Energy and Natural Resources Committee to stop working on a National Energy Policy bill. He did this because a majority of that committee now favors drilling for a modest amount of oil in a tiny area of the remote Arctic National Wildlife Refuge (ANWR). Further, he has substitute‐​legislation in mind that goes beyond ANWR, calling for drastic changes in our energy structure.



Hopefully, Daschle will reconsider his actions on the energy bill. Although ANWR can supply only a small fraction of our energy, it is a political no‐​brainer. Who’s going to vote against domestic oil when we are at war? The emerging energy bill also encouraged the development of technologies to further reduce the already small atmospheric impact of coal combustion, promoting the increased use of this domestically abundant fuel.



But do we need any energy legislation? Coal is cheap and plentiful, and it is regulated via the Clean Air Act Amendments of 1990. If there’s the political will, go ahead and re‐​open those (though expect some opposition here). Drilling in the ANWR doesn’t require 30 pages of text; a few sentences will do.



However logical it might be to “do nothing here,” that’s not the way D.C. works. Instead, Daschle would like to substitute another bill, S.556, for large portions of the current energy legislation. The substitute‐​bill is sponsored by Jim Jeffords (?-VT). Instead of drilling in ANWR and developing cleaner coal technology, the bill looks a lot like the old Kyoto Protocol on global warming, which has wisely been rejected by President Bush because it is 1) expensive, and 2) scientifically indefensible. Jeffords’ bill mandates that we reduce emissions of carbon dioxide from power plants to 1990 levels by January 2007. Our national emissions are currently about 15 percent above 1990 levels. As we continue on our merry economic way, those emissions will go 20 percent above 1990 levels by 2007. To meet this law would therefore require a major energy reduction in a nation at peace, let alone in one at war.



Now, about 55 percent of the nation’s electricity is produced by the combustion of coal. Jeffords’ target could be met if somehow every coal‐​fired turbine was converted to natural gas. But we do not have the infrastructure to move that much gas, and, further, S.556 mandates “policies that would reduce the rate of growth of natural gas consumption” Without coal or natural gas, there’s only one significant source of power production left: nuclear. Does anyone seriously think that the same radical greens that pressured Daschle into all this will allow him to push nuclear power?



It gets worse. The substitute‐​bill also requires that 90 percent of mercury emissions be removed from power generation by 2007. There is only one way to do that: Stop burning coal. If this bill is passed, it will be against the law to use coal to produce appreciable amounts of electricity. That is mandated for a nation that has hundreds of years of coal supply, and depends upon the rest of the world for 60 percent of its oil. That is mandated for a nation that is currently fighting a war using oil‐​powered technology.



So what S.556 scuttles is more than just ANWR drilling, it is our domestic energy hole card. The result of the substitute‐​bill is that the country will become bereft of power. Jeffords’ bill makes energy prohibitively expensive even as it mandates an impossible result. All of this when there’s a war on.



How could such folly evolve? It all goes back to “genus: Extreme environmentalism, species: global warming.” You’d think this critter would at least go into hibernation given the current problems in the world. But instead it is flying stealthily through Congress while the public concentrates on the more important matters at hand.



How much additional global warming “gain” do we get for the Jeffords pain? We ran the United Nations’ own computer model, assuming their dire “storylines” (their word) for global warming and development. The amount of global warming that Jeffords’ bill prevents in the next 50 years is 0.04ºF. No one will be able to measure this against the natural variability of climate.



Do we need omnibus energy legislation when a few sentences will do? Worse, do we want to substitute a bill that will increase energy prices, have no demonstrable effect on climate, and outlaw an inexhaustible domestic source of energy?



If anybody has noticed that we are at war, it must be Sen. Daschle. Maybe it’s time to be a bit more conservative about domestic energy policy. There will be plenty of time to debate things like global warming after we win a victory that is much more assured by domestic energy security at this precarious moment in time.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
It’s well known that the United States suffered severe drought and record high temperatures back in the 1930s, which we know resulted in the famous Dust Bowl and economic hardship across North America.
But now, from a climate point of view, that period has become an embarrassment to the scientists who propose the anthropogenic global warming theory. Atmospheric CO2 concentrations were much lower back then, and so according to their theory,  it should have been cooler than it is today. But it wasn’t.
But instead of questioning CO2’s role in driving global temperature, NASA GISS has decided to just rewrite the historical data so that it fits their flakey theory. This of course is scientific fraud.
Today we examine NASA GISS data for the Wellsboro station in Pennsylvania. First we look at the Version 4 “unadjusted” annual mean temperature data and compare it to the new Version 4 “homogenized” data:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data: NASA GISS
Above we see how NASA scientists simply went back in the old datasets and simply rewrote them so that the hot years of the early 20th century are cooled tremendously – by over two degrees in many years.
Earlier Wellsboro saw a cooling trend. But now since NASA scientists have fiddled with the data, the trend has been forged to fit the AGW theory.
Next is a chart comparing version 3, which starts in 1883 and ends in 2019, to Version 4, which starts in 1882:

Version 4 data source: NASA GISS, Version 3 here.
The earlier Version 3 also showed cooling before NASA rewrote the data and wiped it out. The current Version 4 used to show cooling, but was that too was tampered with and now it shows strong warming.
Many would argue that this is not science, but outright Orwellian scientific fraud.


		jQuery(document).ready(function(){
			jQuery('#dd_03feef225a932a34440c9fc287be9307').on('change', function() {
			  jQuery('#amount_03feef225a932a34440c9fc287be9307').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRudolf Kipp of http://www.science-skeptical.de/blog/deutsche-klimaforscher-starten-neuen-rettungsversuch-fur-ein-globales-klimaabkommen-die-klima-kopfpauschale/002480/  reports on the latest German plan to save the planet: Peak and Trade. This is my summary in English.
Without a doubt the Copenhagen Climate Conference last December was a major flop. A treaty for reducing CO2 emissions could not find the support it needed. The conference was such a disaster that a climate agreement is all but ruled out for 2010 as well. So, what to do?
Policy Shift in Germany
The Potsdamer Institute for Climate Impact Research, which includes alarmists Stefan Rahmstorf and Hans Joachim Schellnhuber, has been the primary advisor for chancellor Angela Merkel, and have arguably lost much clout since the embarrassment that was Copenhagen.
To get things back on track, PIK has come up with a new position paper that outlines a whole new approach that could lead to a breakthrough in climate treaty negotiations. It’s dubbed New Strategies for Reaching the 2°C max Climate Target, a scheme which the PIK calls Peak and Trade. It calls for a so-called per capita climate quota, see here (Abstract written in English). http://www.pik-potsdam.de/research/publications/pikreports/.files/pr116.pdf
The PIK scientists call for allocating a quota of 5 tonnes of CO2 to every inhabitant on earth. Should an earthling exceed the quota, then payment would be made into a fund (a yet to be created World Climate Bank). If the quota is not reached, then money would come out of the fund as a reward.
This of course becomes very attractive for the poorest of countries, and very expensive for industrious nations. For example the average American emits 16.9 tonnes of CO2 – more than three times the quota. Yet, some European countries with high amounts of hydroelectric or nuclear power are well-positioned. For example Sweden and Switzerland emit on average 5.6 tonnes, the French 6.3 tonnes. The financial punishment of Peak & Trade would be mild and thus bearable.
The per capita climate quota is very attractive for developing countries
Before anyone writes this idea off as crazy, take a close look at the numbers and who the potential winners and losers would be. But first, where does the 5.1 tonnes of CO2 per earthling number come from? The authors of the paper simply have taken the projected 2015 CO2 emissions figure of 35 billion tonnes and divided it by the current world population of 6.9 billion. Copenhagen failed in part because of resistance from the developing countries, especially Brazil, China and India. They just didn’t see enough incentives. These countries however are well below the magic 5.1 tonne quota – especially Brazil and India, and so now they have ample reason to enthusiastically support Peak and Trade.  It’s reasonable to think this is the instrument to get them back on board.
The per capita climate quota is also an incentive for population growth (and poverty) 
Poor developing countries now stand to rake in the cash. The more inhabitants and the poorer the standard of living, the more money a country stands to pull in. That could be an irresistable incentive for many countries. The humanitarian problem with this German Meisterwerk of a plan now becomes clear: governments and regimes in poor countries would have no incentive to improve living standards for their citizens, and thus keep CO2 emissions at low levels. The funds earned by falling below the 5.1 tonne quota of course would never be seen by the poor citizens. Most likely these funds would flow into in the hands of corrupt government officials and into anonymous bank accounts in Switzerland.
Readers may think this is some far-fetched idea. It is – and that’s the danger. These things are often hatched by social engineering experimentalists here in Europe and they always seem to grow legs. Don’t underestimate it. Peak and Trade has to be killed.
Share this...FacebookTwitter "
"

Today’s question at “ _Politico_ Arena”:   
  
  
**“Have the greens failed?”**   
  
  
My response:   
  
  
If the greens have failed, it’s not for lack of trying. For years now, in everything from pre‐​school programs to “educational” ads aimed at adults, they’ve been “greenwashing” our brains. In September the _Wall Street Journal_ reported that the EPA was focusing on children: “Partnering with the Parent Teacher Organization, the agency earlier this month launched a cross‐​country tour of 6,000 schools to teach students about climate change and energy efficiency.”   
  
  
Yet for all that effort, the public isn’t buying. As _Politico_ notes this morning: “The Pew Research Center found that by last January, global warming ‘ranked at the bottom of the public’s list of policy priorities for the president and Congress this year.’ ” And “Independent voters and Republicans ranked it last on a list of 20 priorities, while Democrats ranked it 16th.” Meanwhile, “other polling suggests Americans are growing more skeptical of the science behind climate change, with those who blame human activity for global warming — 36 percent — falling 11 percentage points this year, according to Pew.” And that was _before_ “Climategate” came to light.   
  
  
At bottom, the greens face three basic problems. First, by no means is the science of global warming “settled” — if anything, the fraud Climategate surfaced has settled _that_ question. Second, even if global warming were a settled science, the contribution of human activity is anything but certain. And finally, most important, even if the answers to those two questions were clear, the costs — or benefits — of global warming are unknown, _but the costs of the proposals promoted by the greens are astronomical._   
  
  
So how do they respond to all of this? _Politico_ cites Greenpeace executive director Phil Radford: “ ‘Obama’s problem is not his position on the climate issue but, rather, his will,’ says Radford. ‘The question is how much the president will lead.’ Americans have ‘overlearned’ the lessons of Kyoto, where President Bill Clinton agreed to a treaty that he never submitted for ratification because it faced near‐​unanimous rejection in the Senate, Radford said. ‘They’re using that as a reason to hide behind Congress instead of to lead Congress.’ ”   
  
  
There you have it. It’s all a matter of will — indeed, of belief. The president needs simply to will this through, the people (and Congress) be damned. We, the anointed, know what’s right, what needs to be done. Is it any wonder that the greens are failing, at least where the people can still be heard?
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
As urban expansion continues worldwide, it wouldn’t surprise anyone that cities would see a growing number of hot days as asphalt, concrete, steel and automobiles act as heat sinks that absorb the summer sun’s energy, a phenomenon known as the urban heat island effect (UHI).
Indeed this has been the case for many German cities over the past decades. Though he climate in Europe has changed over the past 30 years, that change is likely due to natural cyclic pattern changes.
Tokyo not seeing hotter summer highs
But even with a stronger UHI and supposed climate warming, Japan’s sprawling megalopolis of Tokyo has not seen an increase in the number of hot days (days where the thermometer climb to 30°C or higher), as the following chart clearly depicts:

Chart shows the number of days each year (May 1st to October 31st) where the temperature rose to 30°C or more in Tokyo. Data source: Japan Meteorological Agency (JMA). 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Clearly the number of hot days has little to do with CO2 emissions.
Tokyo October not warming
Also we’ve got the mean temperature data for October 2020 for Tokyo:

Data source: JMA
Above the chart shows that the mean October temperature in the city of Tokyo has not risen in almost 3 decades. In fact it has trended downward a bit, though statistically insignificantly.
So if anyone is claiming Tokyo is getting hotter days and hotter in general, then they either don’t know what they are talking about or they are misleading us.


		jQuery(document).ready(function(){
			jQuery('#dd_a5ebd9bed6ff70015a66e5fd318aaf1e').on('change', function() {
			  jQuery('#amount_a5ebd9bed6ff70015a66e5fd318aaf1e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMelting mountain snow in the Canadian Mackenzie Mountains has uncovered ancient weapons used by early hunters. In the Canadian Mackenzie Mountains scientists have found weapons up to 2400 years old, reports Tom Andrews of the Prince of Wales Northern Heritage Centre in Yellowknife and his colleagues in a press release from the Arctic Institute of North America.
http://www.arctic.ucalgary.ca/main/documents/media_release_pdfs/Melting%20ice%20reveals%20ancient%20artifacts.pdf
 Scientists suspect that hunters followed herds escaping mosquitoes and heat during the hot summers. Caribou was an important food source.
 The results of their findings have been extraordinary. Andrews and his team have found 2400-year-old spear throwing tools, a 1000-year-old ground squirrel snare, and bows and arrows dating back 850 years. Biologists involved in the project are examining caribou dung for plant remains, insect parts, pollen and caribou parasites. It is very likely that snow and ice today still covers more ancient relicts.
 The findings and their dates of origin undercut warmists’ claims that the Medieval Warm Period did not exist, or was localised in Europe, and that today’s warm period is unprecedented. The age of the found artefacts correspond to the Roman Warm Period and the Medieval Warm Period.  Ancient artefacts recovered in the Alps tell the same story.

H/T: http://klimakatastrophe.wordpress.com/
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHow long have we been hearing this kind of talk from alarmists? How often are we told that we have to make lots of sacrifices and give governments unlimited power – otherwise the earth will be destroyed? The answer is: almost everyday.
High energy taxes, loss of freedom, massive regulation, constant monitoring, surveillance over how we do things will be huge inconveniences; but it’s necessary, and so just suck it up.
Can’t you see all the destruction all over the planet? It’s spreading everywhere, and soon it will be at your doorstep, unless of course you suck it up and give them the power they need.
I ask,  just what kind of person does one have to be to heed that kind of advice? Pretty clueless I think.
 Just suck it up, otherwise the planet is going to be destroyed

I know as a climate-blogger I’m going out on a limb with this. But I got a feeling we have not heard the end of this story by any means. Where’s there’s smoke, there’s fire. My eyes and ears are perked.
I can understand the other climate blogs not wanting to touch this with a 10-foot pole, claiming whatever righteous reason. But I think someone has to observe and report on this. I got a hunch, and I#m going to follow it..
I happen to think Al Gore is not an okay person, and so it would not surprise me if this story turns out to be true. I’m not saying the story and allegations are true. Yellow journalism and the such are not what I draw my conclusions on. That’s for the IPCC to use (before asking us to suck it up).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The problem is that Al Gore unwittingly revealed a lot about himself and his movement when he made the film AIT. The film was carefully designed to mislead and manipulate its viewers in a mean kind of way. And it was carefully crafted to demonize anyone who refused to fall in line. It uses bully psychology.
Does AIT show any respect for the very element that is needed for science to progress? The answer is of course: none whatsoever!  There was absolutely zero tolerance for sceptical views. Sceptics were ridiculed, mocked and called flat-earthers, and other nasty things.
There are lots of other things in the film that are troublesome, which speak volumes about the persons involved in making it. Of course some involved were just gullible and innocently believed the rubbish.
And let’s not even discuss how Gore lives his life, his business ventures, and so on. These also speak volumes about what kind of guy he is. To me it’s crystal clear. Gore in the film is a pretty bitter and quite peeved chap and he’s capable of a lot of things. 
But we’ll see how the story turns out soon enough. I’m not going to stop blogging about it until the story ends and the (inconvenient?) truth is out.
And my feeling is that the other climate bloggers will be joining in with the commentary before too long. Like it or not, it’s a climate story. But, my hunches could be wrong.
FULL POLICE REPORT HERE FOR YOUR READING PLEASURE!
Update: By yellow journalism, I don’t mean the Examiner, rather the Nation Enquirer, who broke the story, and whose website for whatever reason I can’t access from Germany.
Other Links:
http://www.kptv.com/news/24102273/detail.html
Fox News video

Share this...FacebookTwitter "
"
Share this...FacebookTwitterI must admit I’m in a bit of shock reading this stuff, especially full police report about Al Gore’s alleged behaviour. I’m not going to draw any conclusion right now. It’s all too stunning, if it’s true.
The other climate blogs have not written a word about it so far. Maybe they want to be extra careful, which is understandable. But then again, Drudge plasters the story as its big headline for the day. Matt Drudge has been in business for years, and surely he’s done his homework. Maybe it’s to encourage other victims to speak out, if there are any.
I read the entire police report and it is shocking – really. Will other women come forward? Is it all a hoax? We’re not talking about Mike Tyson or Kobe Bryant here. We’re talking about the former VP of the USA and the prophet of AGW.
Incredible.
UPDATE; Germany’s top tabloid Bild reports here (German).
Share this...FacebookTwitter "
"

Super Typhoon Haiyan left a path of tremendous devastation as it traversed the Philippines. Virtually all Category 5 hurricanes that make landfall do the same, with the number of casualties modulated by poverty, preparation, and preparedness (which are two different concepts).



While they are not infrequent over the open ocean, Category 5 storms don’t hit land very often. There’s some evidence that hurricanes of this intensity are unstable, and the eyewall structure required to maintain such winds often undergoes a natural change that reduces them to Category 4, or even three.





The best defense against Category 5 storms is resilient infrastructure and preparedness—characteristics surely better achieved through a free‐​market, than global governance.



But some of them do devastate a coastline before this occurs. That includes the current record‐​holding storm for (estimated) maximum wind, 1969 Hurricane Camille, which one of us (Michaels) rode out in an oceanfront laboratory not far from the eastern eyewall. Haiyan may well beat Camille’s records, but we won’t know until damage surveys are completed.



If Category 5 landfalls were more common we’d probably be more cautions as to how and where (and if) we decided to construct coastal communities. It’s a fact that buldings can be hardened to withstand some pretty severe winds. Hong Kong is going to get a Category 5 one day, and the high‐​rise building codes there are extremely stringent. It’s a matter of cost and benefit, like so many other things.



Nowadays, in the aftermath of every weather‐​related disaster, proponents of restricting fossil fuel use in the name of halting climate change are quick to place the blame for the tragedy on human‐​caused climate change (i.e., industrialized nations like the U.S.). The calls to “do something” amplify.



This is happening right now in Warsaw, at the latest (19th) in a long string of U.N.-organized Climate Change Conferences aimed at getting countries to agree to some sort of action aimed at mitigating climate change.



On the conference’s opening day, an envoy form the Philippines, Yeb Sano, gave an emotional address to the delegates in which he vowed to stop eating until something was accomplished.



“I will now commence a voluntary fasting for the climate. This means I will voluntarily refrain from eating food during this (conference) until a meaningful outcome is in sight.”



Adding,



“We can fix this. We can stop this madness. Right now, right here.”



Sano got a tear-filled standing ovation.



While the outpouring of sympathy was certainly deserved, an outpouring of action on climate change is certainly not. A story from the _Associated Press_ covering the events at the conference summed up the science on anthropogenic climate change and tropical cyclones pretty accurately:



Scientists say single weather events cannot conclusively be linked to global warming. Also, the link between man-made warming and hurricane activity is unclear, though rising sea levels are expected to make low-lying nations more vulnerable to storm surges.



In other words, limitations, even strict ones, on anthropogenic emissions of carbon dioxide and other greenhouse gases—the very thing that Sano seeks—will have no detectable (at least based on our current scientific understanding) impact on the characteristics of future tropical cyclones, such as Haiyan, or Sandy, or Katrina, or any other infamous storm. And as for sea level rise, projections are far more lurid than observations.



The hard numbers (from Ryan Maue’s excellent compilation) show that global tropical cyclone activity for the last 40+ years—during the time of decent observations and the time with the greatest potential human impact from greenhouse gas emissions—while showing decadal ups and downs, show little overall change. In fact, global cyclone activity has been below average for the past 5 years.





_Figure 1. Global cyclone activity as measured by the ACE ACE -2.14% (accumulated cyclone energy) index since 1972 (top). The Northern hemisphere ACE index is denoted by the lower (black) points, and the Southern Hemisphere ACE is the area in between the two curves (from Ryan Maue)._



The science on tropical cyclones is complicated and ultimately unclear in terms of the influence of greenhouse gas emissions, but is quite clear when it comes to the influence of demographics and wealth vs. climate change—the former grossly dominates the latter when it comes to future tropical cyclone disasters. So, no matter what this year’s U.N. climate confab does (forecast: nothing significant), it will not result in any meaningful changes to damages from future tropical cyclones.



Category 5 storms like Haiyan, Andrew, and Camille will always pose a threat to coastal communities (and beyond) in tropical cyclone-prone areas of the globe. The best defense against them is resilient infrastructure and preparedness—characteristics surely better achieved through a free-market, than global governance. But no matter what actions are taken, more Category 5 monster storms are coming. When they arrive, the news ought to focus on where they hit, not _that_ they hit.
"
"
Share this...FacebookTwitterThe pitfalls of planned economies…

Power shortages, power surpluses, seas of milk and mountains of butter
By Reinhard Storz
(Translated/ edited by P. Gosselin)
Some of the older generation will still remember the time when the press, radio and television talked about a sea of milk and a mountain of butter. Politicians had meant well and, in order to aid the farmers, decided on adequate prices for milk. This planned economy led to an ever increasing quantity of milk in West Germany. All the talk was about a sea of milk. There were numerous ideas on what to do with the surplus milk. One should give milk to the school children for school breaks etc. Surplus milk was processed into milk powder and butter. The resulting butter was stored in cold stores until there was no room left. Finally, a part of the butter that could not be sold in Germany was sold to the USSR for a fraction of the market price, and another part was ultimately given away to Chile.
Politicians learned back then that this was the wrong way, and so introduced a milk quota more than 30 years ago to promote agriculture. As a result the sea of milk evaporated and the mountain of butter disappeared. Today, due to the planned economy with solar and wind power, we have similar conditions as we did with milk and butter. When the wind blows strongly and the sun shines, coal, gas and nuclear power plants are throttled down. Nevertheless, we still have a surplus of green electricity at times. This cannot be accommodated by consumers in Germany. To get this problem under control one has two options.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




1. You search for customers in neighboring countries. That frequently succeeds. However, they are usually not willing to pay for the surplus electricity. The power is thus sometimes given away, often money has to be paid on top so that the power gets accepted.
2. Wind turbines are turned off if the grid gets overloaded. The operators of the wind parks, however, are still reimbursed for the price of the unproduced electricity. These costs are also passed on to us, the electricity consumers.
But there are also times when the wind does not blow and the sun does not shine. There you could very well use the electricity that was previously available in abundance. A program for even more wind turbines or solar roofs will not help. A tenfold amount of solar surfaces does not supply electricity at night and a tenfold amount of wind turbines has no use during calm periods. In nature, fluctuations are nothing unusual. We remember the story of the Pharaoh and the 7 fat people and the lean years. Surplus food was already stored for bad times thousands of years ago.
To even out the fluctuations in solar and wind power, electricity storage is therefore urgently needed. Politicians to plan it out in such a way that additional wind turbines and solar areas are only approved if they are equipped with appropriate storage capacity. The goal of the Energiewende (transition to green energies) is to replace electricity from coal-fired power plants with electricity from the sun and wind. But it must then also be available around the clock. This is not possible with the sporadically available electricity from wind turbines and solar systems.  This electricity, disparagingly referred to by some people as inferior, fidgety electricity, must be made permanently available by means of electricity storage systems. Only in this way can a transition to green energies succeed.
Therefore, demonstrations should first take place to demand the introduction of sufficient electricity storage. Until this is achieved, there will remain a need for coal-fired power plants. Anyone who wants to abolish coal-fired electricity without a secure and affordable alternative supply, is just sawing off the branch we are all sitting on. But we should not give up hope. Like the politicians who finally got the mountain of butter mountain and the sea of milk under control with the milk quota, they will also bring the necessary power storage facilities on the way as soon as possible.
Pumped storage facilities for such amounts of electricity are likely to be ruled out due to technical and economic reasons. The same is true with regards to compressed air storage, which has a lower efficiency than pumped storage plants. Battery storage is unaffordable at today’s costs. The only option is to produce hydrogen as an energy storage from surplus electricity.


		jQuery(document).ready(function(){
			jQuery('#dd_a4404f563bedf5c83941b0e5d9152640').on('change', function() {
			  jQuery('#amount_a4404f563bedf5c83941b0e5d9152640').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

_Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Global warming buffs have been fond of claiming that the roaring winds of Typhoon Haiyan were the highest ever measured in a landfalling tropical cyclone, and that therefore (?) this is a result of climate change. In reality, it’s unclear whether or not it holds the modern record for the strongest surface wind at landfall.   
  
This won’t be known until there is a thorough examination of its debris field.   
  
The storm of record is 1969 Hurricane Camille, which I rode out in an oceanfront laboratory about 25 miles east of the eye. There’s a variety of evidence arguing that Camille is going to be able to retain her crown.   
  
The lowest pressure in Haiyan was 895 millibars, or 26.42 inches of mercury. To give an idea, the needle on your grandmonther’s dial barometer would have to turn two complete counterclockwise circles to get there. While there have been four storms in the Atlantic in the modern era that have been as strong or a bit stronger, the western Pacific sees one of these approximately every two years or so.   
  
Camille’s lowest pressure was a bit higher, at 905 mb (26.72 inches). At first blush it would therefore seem Haiyan would win the blowhard award hands down, but Hayian had a very large eye around which its winds swirled, while Camille’s was one of the smallest ever measured. At times in its brief life, Camille’s was so small that the hurricane hunter aircraft could not safely complete a 360 degree turn without brushing through the devastating innermost cloud band, something you just don’t want to be near in a turning aircraft. In fact, the last aircraft to get into Camille, which measured 190mph sustained winds, lost an engine in the severe turbulence and fortunately was able to limp home.   
  
Haiyan’s estimated 195mph winds were derived from satellite data, rather than being directly sensed by an aircraft. But winds over the open ocean are always greater than those at landfall because of friction, and the five mph difference between the two storms is physically meaningless. 



The chance that an onshore anemometer (wind-speed and direction sensor) will survive such a storm isn’t very high, so the winds are inferred by scientists and engineers from the texture and distribution of what’s left behind.   
  
Every year, our National Hurricane Center summarizes the Atlantic hurricane season in painstaking detail in article published in the prestigious journal _Monthly Weather Review_. Describing Camille’s destruction, it said:   




Maximum winds near the coastline could not be measured, but from an appraisal of splintering of structures within a few hundred yards of the coast, velocities probably approached 175 k[nots]. 



That’s 201 mph.(Higher winds have been measured on small islands. With Haiyan and Camille, we are talking about storms running into large landmasses, where friction takes place.)   
  
Camille killed 143 along the Gulf Coast, while Haiyan’s toll is currently estimated to be more than 2,500.   
  
The difference, which is more than an order of magnitude, is largely (but not completely) due to poverty. Despite experiencing roughly five landfalling tropical cyclones per year, Philippine infrastructure simply isn’t as sound as it is in wealthier countries. As a grim example, a number of Haiyan’s casualties actually occurred in government-designated shelters that collapsed in the roaring eyewall.   
  
In addition, the transportation infrastructure simply couldn’t handle a mass evacuation. If a similar situation applied to the U.S. Gulf Coast, Camille would have killed thousands at landfall, a fact noted in the Hurricane Center’s report on the 1969 season. Where Haiyan hit in the Philippines, there simply weren’t any roads capable of evacuating the citizens of Tacloban City safely inland, forcing them to ride it out dangerously close to the invading ocean and exposed to winds that pulverized most structures.   
  
So, while we really don’t know which storm had higher winds, we do know that more affluent societies are much less affected by even the strongest storms. As Indur Goklany, (who writes frequently for Cato) has pointed out, if left to develop, the entire world will be much more resilient to climate change than it would be if the ineffective policies to “stop” it slowed economic growth.


"
"

While doom and gloom predictions continue about CO2 induced global warming, saying that it now is the largest driver of climate, overwhelming any influences of the suns variation, there appear to be other things happening. There are forecasts emerging for a wet and cold winter.
Lets review. We have a longer than normal solar minimum occurring, and we have a strong La Niña developing too. We have colder water in the Pacific.
Here is a animated view of the growing La Niña. Watch the animation, note the exapnding La Niña off the west coast of South America, note also the expanding pool of cooler water developing the Gulf of Alaska. This will be a key formation point for cold wet storms.
And there are other signs too. Acorns. Have you noticed this year we have an overabundance of acorns? I was walking in Bidwell Park a couple of weeks ago and the ground was covered with them, and they were still raining down like hailstones. I’ve never seen anything like it. This has been what biologists call a “mast year” for valley oaks.
While this may sound a bit like an “Old Farmers Almanac” moment, but I have a theory for it.
Trees are directly in touch with the sun, more so than other living things in the biosphere. Our “valiant” dendroclimatologists, like Michael Mann, point to tree rings as a proxy for earths climate. That may be true, but I think in addition to “treemometers” they also act as helioproxies too.
In a nutshell (ahem); I think it’s highly likely that trees have evolved survival strategies that are based on detecting changes in the sun’s output. It stands to reason that over the billion plus of years that plant life has been on earth and the millions of solar cycles they’ve been through, that they can detect changes in their primary energy source, the sun, and adapt accordingly. Producing abundant acorns could well be such a survival strategy.
We have strong signs of a solar cycle that is late and well below average, a near record low hurricane season, and a strong La Niña emerging.  Now we have valley oaks producing acorns like there is no tomorrow. Maybe we should heed the trees.
h/t Russ Steele at NCwatch for the animation for forecast links


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea315f833',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

As we approach the end of the year, it is appropriate to give you an update on Cato. Fortunately, Cato is doing very well and has positive momentum on all fronts, although there are always opportunities for improvement.



Commentators on both the left and the right are discussing the “libertarian moment.” While it’s obvious we do not have the current president’s ear, libertarian ideas are being taken very seriously, in part as the result of 37 years of Cato scholarship. There is a rapidly rising libertarian student movement where Cato has played a critical role in training future student leaders and providing the intellectual ammunition for students to challenge their left‐​leaning professors.



The Cato brand has also been rising. As a number of our fellow think tanks have become visibly politically partisan, we have maintained a reputation for objectivity in a politically charged environment. Of course, since we have plenty of disagreements with both Republicans and Democrats, this is relatively simple to do.



Cato’s financial position is strong, with our second year of significant increases in contributions. Thank you! However, our budget is tiny compared to the billions of dollars in resources controlled by the statists. We have many opportunities to use additional funding to productively communicate the libertarian message. As we approach the end of the year, I hope you will put Cato high on your list for financial support so we can maintain and accelerate the libertarian momentum.



The list of Cato’s recent accomplishments is impressive. Largely because of a great effort by Cato’s Michael Cannon, the court battle to undo Obamacare continues, with two recent victories in an uphill fight. Cato’s Constitutional Studies team’s record was 10–1 for amicus briefs filed at the Supreme Court this past session and 15–3 for the previous session. This is the result of over 30 years of effort to persuade the Court to limit Congress to its constitutionally enumerated powers.



Since 1999 Cato has been discussing the danger of police militarization. Finally, the public is becoming aware of this issue and its threat to innocent citizens (bring back Barney Fife). The Herbert A. Stiefel Center for Trade Policy Studies has started an invitational dinner program for business and government leaders to discuss trade liberalization and its many benefits. Participants have included Eric Schmidt, executive chairman of Google; Russ Girling, president and CEO of TransCanada Corporation; Greg Page, executive chairman of Cargill; and former U.S. trade representative Susan Schwab.



Harvard’s Jeff Miron, who has joined Cato as director of economic studies, has commissioned a series of economic briefs from top scholars. The scholars are typically not libertarian, but their research supports libertarian policy positions. The willingness of these high‐​level scholars to publish under the Cato brand is very encouraging.



We also took our ideas into the world’s hot spots this year: we cosponsored a conference in Ukraine with approximately 500 policymakers, businessmen, and journalists, as well as a student conference for freedom in Venezuela.



In the 12 months ending in August, Cato’s scholars were mentioned in 5,622 news stories, published 849 opeds, and made 442 major broadcast appearances. We published approximately 100 academically credible articles; held 30 conferences, 38 policy forums, 41 book forums, and 14 Capitol Hill briefings; and testified 18 times before congressional committees. Over 1,300 students participated in various Cato educational programs and our scholars made many presentations to student organizations such as Students for Liberty, Young Americans for Liberty, and the Federalist Society.



We just launched the Cato Center for Monetary and Financial Alternatives, which is the first serious challenge to the Federal Reserve in its 100‐​year history. The quality of scholars and business leaders who have agreed to participate is extraordinary and reflects the increasing concern by even mainstream economists about both the Fed’s monetary and regulatory policies.



The Cato Center for the Study of Science is growing rapidly to take on the increasingly politicized climatechange movement. Climate change is the religion of the Progressives and as their models have continued to fail they have become more irrational in defending their detached‐​from‐​reality positions.



We have launched a campaign to help independent thinkers truly grasp that big government is failing across the board. We are in the process of hiring two civilliberties scholars to take on the NSA, the IRS, and the many other agencies and policies (including the drug war) that are a serious threat to your individual liberties.



We are a voice of reason in an increasingly politicized and irrational environment. Cato is committed to creating a free and prosperous society based on the principles of individual liberty, free markets, limited government, and peace — a noble endeavor. Your support makes our work possible.
"
"

 **Presented by Alan Reynolds at “Deregulation in the Global Market Place** : **Challenges for Japan and the United States in the 21stCentury** **”  
A Keidanren‐​Cato Institute Symposium, Tokyo, April 6, 1998 .**



American economists have been giving policy advice to Japan since the Shoup tax reform commission of 1949. Even then, the advice was not always helpful. 



U.S. government officials now tell Japan that larger budget deficits are the key to boosting economic growth and stock prices. Yet the same officials claim that the U.S. economy and stock market have benefitted from much _smaller_ budget deficits. 



When looking at tax policy as a separate policy tool, we must get beyond Keynesian “macroeconomics,” and the related bad habit of ascribing magical properties to government borrowing. A few years ago, prominent macroeconomists in the U.S. and Japan were absolutely confident that American budget deficits had pushed interest rates and the dollar up, causing trade deficits. Today, much larger budget deficits in Japan are associated with the lowest interest rates in world history, a falling yen, and rising trade surpluses. 



A theory that can predict anything can also predict nothing. Fiscal macroeconomics is an unsolvable mystery, something best relegated to a museum. Even the dogma that budget deficits “stimulate” demand (nominal GDP) is just another archaic theory with no shred of evidence.(1) Budget deficits are a consequence of slow economic growth, not a cause of rapid growth. Yet the elusive theory of “fiscal stimulus” still allows politicians to claim that endless public works schemes are a free lunch, rather than a dubious debt service burden on taxpayers.(2)



Japan’s nominal GDP growth surged in the late 1980s because the Bank of Japan was buying a lot of securities (see the Appendix: “Money Matters”). Nominal GDP growth slowed after 1991–92 because the monetary base shrank. Growth of aggregate demand depends on monetary policy, not government borrowing. Whether or not supply can keep up with demand, however, _is_ affected by the _micro_ economic structure, including tax rates. 



Any serious look at policy must be based on microeconomics — all the messy but vital structural details about disincentives and distortions. An improvement in tax policy is _not_ measured by how much revenue is lost, but by how much efficiency and prosperity is gained. 



To describe tax policy in macroeconomic terms — solely by its immediate effect on the budget — leads to paradoxical conclusions. Many people suspect that the April 1997 increase in Japan’s value‐​added tax contributed to recession.(3) If that is correct, this “tax increase” will surely result in _less_ tax revenue, not more. A macroeconomist may later observe such a loss of revenue and conclude that tax policy has become “stimulative” — the opposite of what actually happened. 



Conversely, if a reduction in the highest, most damaging tax rates results in slightly more rapid economic growth, then it may also result in _more_ tax revenue. It is the impact on the economy that matters. What happens to the budget deficit mainly depends on what happens to the economy, not the other way around. 



An _efficient_ tax system is one that _raises revenue with the least possible damage to economic progress._ With such a system, tax revenues naturally _grow_ as the economy expands. 



The _marginal_ tax burden in Japan is unusually high, yet average tax revenues are relatively low. Aside from Social Security, revenues have been _falling_ since 1990, when many new taxes were added. High tax _rates_ and weak revenues are symptoms of an _inefficient_ tax system. 



******Figure 1** shows that it is _not_ unusual for countries with high tax rates (such as Japan) to collect very little revenue, even in _static_ terms — as a percentage of GDP, rather than real revenue growth over time. These figures are for the _national_ income tax on individuals (although local taxes are very important in Sweden, Canada, Japan and others). The only countries with high tax rates that collect much revenue are those in which the _lowest_ tax rate is also high, such as the 19% rate in Germany (although even a 25% minimum tax does not work in Turkey). Tax rates above 35–40% never seem to yield much revenue, anywhere. 



As the table indicates, the ratio of tax revenues to GDP can be a misleading measure of the actual burden of taxes. Some taxes may be extremely destructive yet yield very little revenue.(4) Just as “prohibitive” tariffs yield no revenue, extremely high tax rates can also be prohibitive, or nearly so, because they (1) seriously discourage the taxed activity, (2) lead to rampant tax avoidance and evasion, and (3) foster capital flight and a “brain drain.” 



A tax system that suffocates growth of GDP may result in a rising ratio of revenues to GDP even though revenues are growing very slowly in real terms. This is the situation of many Continental European countries. Real tax revenues grow very slowly, but the economy grows even more slowly, so revenues rise as a percentage of stagnant GDP. Yet such governments have little room to increase spending in real terms because revenues have stopped growing in real terms. 



Conversely, revenues may appear quite low relative to GDP and yet be growing very rapidly in real terms, because the tax climate is conducive to rapid growth of real GDP. Hong Kong’s tax revenues ( _excluding_ land sales) grew by 17.8% a year from 1984 to 1996 — nearly three times as fast as the 7.1% pace of U.S. revenue growth. Yet the ratio of taxes to GDP remains low in Hong Kong because real GDP has increased almost as rapidly as real tax receipts. Government consumption in Hong Kong has long increased by 6% a year, in real terms. 



In short, tax policy is _not_ properly described by revenue losses alone, nor even by revenues as a percentage of GDP. We need to look at _marginal_ tax rates on the returns to additional capital, including human capital. 



**The Shoup Mission**



Rather than rely too heavily on economic theory, or on foreign advice, it is often useful for a country to reexamine its own history (and that of its neighbors) to see which policies were followed by prosperity and which were not.()



In the late 1940s, the American Occupation had imposed brutal income tax rates on Japan, as high as 86% on income above Â¥5 million. This was a central part of a severe austerity program, _not_ a plan to promote economic growth. As Edwin Reischauer pointed out at the time, “Steeply graduated income taxes and inheritance taxes have been adopted to prevent in the future the accumulation of … concentrations of wealth.” But taxes designed to punish additions to _income_ must also punish additions to _output_ — economic growth. So, Japan set out to free itself from the oppressive Occupation tax regime. 



In late 1950, following a similar policy coup in Germany, Japan’s highest individual income tax rate was slashed to 55% from 86%. From 1950 to 1974, Japan cut taxes _every year_ (except 1960) often by greatly increasing the income thresholds at which the higher tax rates applied, or by enlarging deductions and exemptions. The taxable income needed to fall into a 60% tax bracket was raised to 3 million yen by 1953, for example, compared with only 300,000 in 1949. The Shoup Commission’s net worth tax was also abolished in 1953. The sting of high tax rates was further neutralized by exemptions for interest income and capital gains, deductions from corporate and individual taxes on dividends, a deduction for earnings, and various other holes in the tax base, legitimate and otherwise.(6)



Some deductions were far from neutral, and therefore less desirable than lower tax rates would have been. Yet the continual tax reductions from 1950 to 1974 accomplished two things. First, they greatly reduced effective marginal tax rates. Second, they moved the system a long way toward what is sometimes called a “consumed income tax” or “expenditure tax” — that is, a system that taxes income only once, regardless of whether the income is saved or devoted to immediate consumption.(7)



American economists were extremely critical of both accomplishments. In 1958, a member of the Shoup Commission, Jerome Cohen, described the tax cuts as “foolhardy from an economic point of view.”(8) In 1964, Martin Bronfenbrenner complained that Japan’s reductions of the highest tax rates amounted to “a retreat from the _equitable_ policies of the American Occupation.”(9)



Tax reduction was considered “foolhardy” precisely _because_ it fostered rapid economic growth. American economists were convinced that Japan would always be plagued with what Bronfenbrenner called “endemic Japanese balance of payments problems.” Failing to distinguish between tax rates (which were falling) and tax revenues (which were soaring), the U.S. critics argued on Keynesian grounds that Japan’s taxes had to be kept high to _suppress_ economic growth. Otherwise, Japan’s trade deficit would supposedly reach “currency crisis” proportions. For the same reason, many U.S. economists were sharply critical of Japan’s deep tariff cuts in the 1960s (e.g., by 1975, the effective tariff on autos fell from 40% to 10%, and the tariff on televisions from 30% to 5%). 



Bronfenbrenner’s remark about Occupation tax rates being more “equitable” was also in the spirit of the times. From about 1938 to 1962, U.S. “public finance” economists were much more concerned about using taxes to prevent people from becoming rich than about any adverse effects on economic growth. An intellectual godfather of the Shoup Commission, Henry Simons, knew perfectly well that “every gain [in] better distribution will be accompanied by some loss in production.”(10) But Simons and his followers assigned a low priority to production, believing that income leveling was the primary goal of tax policy.(11)



In the U.S., the postwar emergence of Keynesian macroeconomics destroyed any lingering anxieties about the impact of high tax rates on economic growth. Early postwar predictions of chronic “underconsumption” and “secular stagnation” meant that _saving was considered a terrible thing_ , and therefore an excellent target for confiscatory taxation. After those dire predictions proved false, slow growth was then said to be something that could easily be fixed by printing more government bonds (called “fiscal policy”) or more money (“monetary policy”). Attention to tax incentives disappeared until the Kennedy Administration, when it was revived largely because of the embarrassing success of falling tax rates and rising income thresholds in Japan and Germany. 



The point of all this history is to emphasize that promoting economic growth in Japan was certainly _not_ the primary goal of the Shoup Commission. Yet the Shoup Commission continues to influence influential Japanese tax specialists who were trained in this early postwar American tradition. 



**1975–87: Bracket Creep and Public Works**



In 1971–73, the U.S. devalued the dollar, pursued an inflationary money policy, and tried to disguise the consequences with price controls. When the dollar goes down, commodities priced in dollars go up. By the fall of 1972, commodity futures prices had already begun to soar, with oil a relative laggard. The end result was the stagflationary swamp of 1974. Japan was an innocent victim. 



Although the event was temporary, it had a lasting impact on Japan’s economic _policies._ Japan imposed a corporate surtax in April of 1972 and stopped cutting individual tax rates after 1974, thus allowing inflation, aging and economic growth to push more and more taxpayers into higher tax brackets. There was a major shift in emphasis away from fostering _private_ investment toward “social overhead capital” and “infrastructure improvement.”(12) In the quaint language that economists used at the time, endless bracket creep would supposedly generate so much revenue that public works spending would be needed to prevent “fiscal drag.” 



**Figure 2** illustrates the main reason why seemingly high marginal tax rates of 40–70% had not done much damage before 1974. The income thresholds for the highest tax brackets were repeatedly increased by huge amounts, much more than enough to keep far ahead of inflation and rising real incomes. 



From 1975 to 1984, by contrast, there were no adjustments of tax thresholds at a time when inflation averaged more than 6% a year. The resulting bracket creep appeared to generate a lot of revenue, but this was partly money illusion. It was not until 1979 that real, inflation‐​adjusted revenue was again as high as it had been in 1974. Revenues did rise as a percentage of GDP, but that would not have happened if real GDP had not slowed. Whatever its long‐​term effect on revenues, bracket creep certainly generated more and more tax distortions and disincentives. 



In 1984, there was a modest increase in the threshold for the 60% rate, but also an increase in the corporate tax rate. In fiscal 1987, the 60% rate was reduced to 55% and the 65–70% rates to 60%. That was temporarily helpful, contributing to a brief spurt of growth in the economy and in tax receipts. 



In April 1989, national tax rates above 50% were ended. But a provision that kept total national and local taxes from exceeding 78% had been eliminated in 1987, leaving combined national and local tax rates as high as 76% even in 1992. The value‐​added tax was also introduced in 1989, raising marginal rates by three percentage points at all income levels. 



**Figure 2** shows that even as recently as 1993, the 40% national tax rate still applied to the same _nominal_ income at which a 42% rate had applied back in 1974. _If tax thresholds had merely been adjusted for consumer price inflation from 1974 to 1993, not for real wage gains, they should have more than doubled in terms of yen_! The latest threshold adjustment finally came close to making up for past inflation with respect to the 50% bracket, but is still inadequate at the 40% rate. All things considered, there has been surprisingly little relief from the highest, most destructive tax rates on individual income, despite appearances to the contrary. 



Japan has been quite unique in this respect. Between 1979 and 1986, many countries had cut their highest income tax rates in half — including the United States, United Kingdom, South Korea, and Singapore. And no country that cut tax rates in half suffered any sustained revenue loss, even as a percentage of GDP. It is not even true that lowering the highest U.S. tax rate from 50% to 28–33% in 1986 was financed by eliminating deductions. Fewer itemized deductions were replaced with a larger standard deduction, but total deductions were not any smaller. Taxable incomes reported by higher‐​income people were “surprisingly” strong after tax rates came down, and labor force participation increased sharply among their spouses. 



**Social Security**



As bracket creep was pushing middle‐​aged Japanese professionals and mid‐​level managers into tax brackets once intended for the very rich, the burden of “contributions” for Social Security pensions also doubled from 6.2% in 1970 to 12.4% in 1986, and to 14.3% more recently.(13) The entire Social Security tax burden, including public health insurance, reached 25.5% of payroll by 1996. Most of this must be added to _marginal_ tax rates on labor income because, unlike the U.S., there is no maximum contribution to eliminate the _marginal_ burden at high incomes. It is the deadly combination of income tax, payroll tax and VAT that matters, at the margin. 



The fact that employees can deduct Social Security taxes from their income tax base is modestly helpful (only employers can deduct payroll taxes in the U.S.). But neutral taxation of savings requires that if contributions are deductible when going into a pension fund, then the funds must be fully taxable when they are later withdrawn. This is the principle behind the original Individual Retirement Account in the U.S. 



If contributions to public or private pensions were _not_ deductible, then income going into a pension fund would have already been taxed before it was saved. In that case, withdrawals from the pensions should _not_ be taxed a second time, when the money is taken out. This is the principle behind the new Roth retirement accounts in the U.S. 



Either of these _neutral_ treatments of saving would be equivalent to a “consumed income” tax, if applied uniformly without restrictions (such labels are misleading, however, since _all_ taxes fall on the individual owners of factors of production). Such a policy had long been advocated by British economists (Mill, Marshall, Pigou, Kaldor), and by Irving Fisher of Yale. It is in marked contrast with the Haig‐​Simons concept of “comprehensive” income taxation that inspired Shoup Commission plans. When older economists, in the Haig‐​Simons tradition, speak of “broadening the tax base,” they often mean defining income in ways that ensure that savings will be taxed several times. Some even refer to corporate depreciation as a “tax preference,” as though the cost of plant and equipment was not a cost at all, but taxable income. Today, younger U.S. specialists in public economics have moved away from these Haig‐​Simons ideas that were so fashionable fifty years ago. Most favor neutral tax treatment of saved income, and prefer immediate expensing of plant and equipment. 



Japan’s system of fully deducting contributions to (public) pensions _but not treating_ pensions as taxable income is not consistent with neutral treatment of savings. To deduct contributions and also exempt withdrawals is _too generous_ toward this specific form of savings (Social Security). Yet Japan’s tax policy after 1989 moved back toward multiple levels of taxes on all other forms of savings — capital gains, dividends and interest income. A policy of tax neutrality for _all_ savings, or at least for all _retirement_ savings, would greatly ease the fiscal strains expected from an aging population. Neutrality requires taxing income from public pensions exactly like any other income (because contributions were deducted), even if pretax benefits have to be increased to sweeten the deal.(14) Applying that same principle to _private_ pensions (deductible going into a pension plan and taxable coming out, or vice‐​versa) would encourage greater use of private retirement plans and thus ease the excessive dependence on Social Security pensions. This would be a start toward partial or full “privatization” of pensions — something that should never be a state _monopoly._



Because incomes generally rise with seniority, tax burdens tend to be extremely heavy at ages 45–57, then fall sharply at older ages due to the dubious exemption of pension income. Since Japan is aging fast, the combination of superhigh taxes on older workers and none on pensions must push many people into early retirement, whether they like it or not. 



**Different Policies; Different Results**



Before 1975, tax policy greatly reduced effective marginal tax rates and eased the multiple taxation of saved income. Economic growth in Japan ( **Figure 3** ) averaged 9.6% a year from 1952 to 1973. 



From 1975 to 1987, “bracket creep” and higher Social Security taxes reversed much of the previous progress on marginal tax rates. Economic growth slowed to 4.3% from 1975 to 1991. 



After 1989, tax policy also _reversed_ much of the previous progress toward neutral treatment of savings. Tax rates on new capital investments increased (at the level of individual investors). Economic growth slowed to 1.2% from 1992 to 1997. To continue blaming this on the “oil shocks” of the 1970s, as many do, is no longer plausible.(15) Oil has been very cheap for more than a dozen years. 



A large increase in the marginal cost of oil can indeed make production of many goods unprofitable. Growth stalls. An increase in the marginal cost of labor or capital can have the same depressing effect. An increase in the marginal cost of government is no different. It reduces the (after‐​tax) return on investments in physical and human capital — the primary source of economic growth. 



Although the dramatic slowdown of economic growth after 1974 coincided with an equally dramatic change in tax and spending policies, that change in economic policies is rarely blamed for the change in economic results. From 1989 to 1992, Japan added more taxes on sales, land, capital gains, dividends and interest. Yet the dramatic deterioration in economic growth after 1990 is rarely blamed, even in part, on those simultaneous changes in tax policy. The sole exception was the increased VAT of April 1997. Even in that case, however, complaints that this particular tax increase hurt the economy are not often translated into the logical conclusion that rolling‐​back such a counterproductive tax increase must likewise _help_ the economy. 



**Figure 4** shows OECD estimates of _average_ effective tax rates on capital and labor in the U.S. and Japan.(16) Before 1985, Japan had much lower tax rates on capital than the U.S. did. Since then, that situation has been reversed — _Japan is now more hostile to capital_. Little wonder that Japan’s domestic investment is weak, and capital flows out. 



It is often said that because Japan’s savings exceed domestic investment, the answer is to use tax policy to reduce savings (adding and increasing the VAT on consumption was therefore an odd choice, even from this strangely contractionary point of view). A much better alternative is to roll back some of the recent tax impediments to domestic investment. 



_Average_ tax rates on labor were still relatively low in Japan over the 1985–95 period as a whole. But the opposite is true of _marginal_ tax rates on highly skilled salaried people, which are extremely high in Japan. As a practical matter, the complaint that salaried people are more heavily taxed than small business owners, farmers and politicians can only be ameliorated by reducing the _marginal_ burden on salaries. 



Rising marginal tax rates on labor and capital are certainly not Japan’s _only_ problem. Monetary policy also made an unsettling shift between extremes between 1988 and 1991. Excessive regulation of finance and commerce is another unnecessary obstacle to economic progress. But the likelihood that changes in the _microeconomic incentives_ of tax policy may account for a large part of Japan’s longer‐​term economic slowdown deserves more serious attention than it has been getting. This is _not_ about tax revenues (which are terribly weak). It is about _incentives._



**“Tax Reform” Took a Wrong Turn**



Some Japanese tax specialists really believe the Shoup Commission designed a system that was “pure” in some theological sense. It does not work in practice, but it looks beautiful in theory. Such students of Shoup missionaries must therefore regard the _annual tax cuts_ of 1950–74 as a horrible mistake — a deviation from the “comprehensive” (yet arbitrary) Haig‐​Simons definition of taxable income. Hiromitsu Ishi, for example, recently urged that _“reform should achieve a return to the Shoup proposals.”_ Indeed, that is exactly the direction that Japan has taken since 1988–89 — heading back toward Occupation austerity policies in the name of “tax reform.” 



The Shoup Commission advocated a value‐​added tax, and urged that income taxes be applied to dividends, interest and capital gains. From 1989 to 1992, “tax reform” in Japan was almost _defined_ as adding a value‐​added tax and adding income taxes on dividends, interest and capital gains. There was also a curious fascination with the _number_ of tax brackets (which is irrelevant), but too little attention among academic economists to the uncompetitive _height_ of marginal tax rates, and to the declining real income thresholds at which they were applied.(17)



 **Figure 4** showed that Japan’s taxes on capital have become much heavier since all these “tax reforms” were adopted. In one respect, this may seem paradoxical. From 1988 to 1990, the statutory tax on (retained) corporate earnings was reduced from 42% to 37.5%, at the national level. But the corporate tax on earnings paid out as dividends was simultaneously increased from 32% to 37.5%. The net effect left the effective corporate tax virtually unchanged.(18) Although the _national_ corporate tax rate of 37.5% does not appear to be much higher than the U.S. rate, local taxes on corporate profits are much higher in Japan. 



The main reason that Japan’s tax on capital increased, however, was the new taxes on interest income and realized capital gains of individual investors. After April 1988, individuals who supplied debt capital to businesses (but not to housing) were subjected to a new 20% withholding tax on interest income. A year later, individuals who supplied equity capital to corporations were also subjected to national and local taxes of 26% on capital gains. Although these taxes are collected from individuals, the increased tax wedge raises the cost of capital to businesses. 



**Punishing Efficient Uses of Land**



The 1992 land tax is another new tax on capital, but one designed to achieve a specific purpose. Ishi describes the land tax as “an attempt to seek an effective policy with respect to reducing land prices.” Unfortunately, the Bank of Japan had the same goal. If inflicting windfall losses on landowners is a legitimate policy objective, then these policies were extremely effective. 



Land had been a very important asset behind Japan’s stocks, so deflating land prices deflated stock prices too. The addition of taxes on stockholder capital gains was also bound to be capitalized in lower stock prices. Land and stock were collateral behind many loans, so the goal of “reducing land prices” continues to have unpleasant repercussions on banks and credit. 



The actual problem is that capital gains taxes on land _sales_ can be _extremely_ high, while property taxes on land _ownership_ are very low, particularly for farms. The net effect is a powerful “lock in” effect that discourages property sales, and discourages efficient use of a valuable resource. Adding another tax on large business land holdings did nothing to fix this problem, because it continued to impose the lowest tax on the least efficient uses of land (urban farmland) while adding a discriminatory tax on the most efficient uses (big business). The real problem continues to be the prohibitive capital gains tax. Henry George made an overly enthusiastic theoretical case for taxing the _appreciation_ of unimproved land. But the only practical way of doing that is to tax _realized_ capital gains land _transactions._ Since any such _transactions_ tax is easily avoided by _not selling_ , changing land ownership toward more efficient uses (a process _facilitated_ by “speculation”) is thwarted by confiscatory taxes on this particular source of capital gain. Capital gains on land _transactions_ should surely not be taxed at a higher rate than gains on financial claims to land (which is largely what many corporate stocks represent). Also, higher capital gains tax rates on assets held for short periods do not make economic sense for _any_ asset, except perhaps as a very crude way of indexing for inflation. In short, the capital gains tax on land is far too high, and the new land tax is much less efficient than raising the broader property tax. 



******High Rates, Low Revenues**



There are usually two obstacles to reducing excessive tax rates. One is the belief that marginal tax rates of 50% or more actually raise substantial revenue. We have indicated before (in **Figure 1** ) that high individual income tax rates are normally associated with relatively _low_ tax revenues, and with slow growth of revenues. We also noted that several countries which slashed their highest tax rates to 28–40% in the 1980s suffered no revenue losses.



Despite our earlier warnings about expressing tax revenues as a percentage of GDP (rather than as real growth over time), **Figure 5** shows that revenues from Japan’s Social Security tax increased from 6% of GDP in 1975 to 10.4% in 1995. In the same period, revenues from all other taxes increased more modestly, from 14.9% to 18.1% of GDP. In fact, revenues were no higher in 1995 than they had been in 1980, despite 15 years of bracket creep, a new VAT, and new taxes on interest income, capital gains, and land. 





**Figure 6** takes a closer look at the nineties. It shows that receipts from individual income taxes declined by 22% from 1990 to 1995. Corporate tax receipts fell even more, thanks to weak profits. 



In the nineties, Social Security has been the _only_ significant source of added revenue. All other sources of tax revenue dropped to 18.1% of GDP in 1994–95, down from 22.2% in 1990. That actually understates the problem, because there was also very little growth of GDP. 



The prolonged downward trend of income tax revenues after 1990 was certainly not due to any “tax cuts” during these years. On the contrary, 1989 is when the biggest effort to _increase_ taxes began. 



Receipts from the VAT have been almost flat and rather small — about 1.5% of GDP. It is a common mistake to regard revenues from a value‐​added tax as a net addition to total revenues. Any variety of sales tax must _reduce sales_ , and therefore reduce personal and corporate incomes that would otherwise have been generated in producing and selling goods and services that are no longer marketable (because the VAT has raised their prices). Whether the increased revenues from VAT exceed the lost revenues from incom ****e taxes is an empirical question.



 **Figure 7** switches to a long‐​run picture of revenues from both income taxes (corporate and individual) and the VAT in real terms, adjusted for inflation. Together, Figures 5 to 7 hammer home a simple point: _Japan’s total tax revenues have declined in real terms ever since the VAT and various investor taxes were introduced._ Without the increase in Social Security tax, which depends on it remaining attractive for employers to employ and for workers to work, the budget would be in much more serious shape than it is. All these ambitious new taxes have been killing revenues. 



If taxes were measured solely by their revenue yield, as macroeconomists tend to do, then Japan’s numerous and painful new taxes would look like a “tax cut.” Such falling revenues from rising taxes might be a run of bad luck. But Canada’s experience suggests otherwise. After Canada added a VAT in 1990, revenues from all taxes virtually stopped growing. Revenues rose by 19% from 1990 to 1995 (only 1.2% if measured in U.S. dollars), compared with a 52% rise from 1985 to 1990, and 59% from 1980 to 1985. 



Japan’s subtraction‐​method VAT (with exemptions for small firms, and for some important services) is easy to evade. Such a tax will never raise much money. Higher rates will just produce more evasion, and that evasion will result in greatly reduced _income and Social Security_ tax receipts. It would be easier and more effective to allow prefectural and municipal governments to adopt simple _retail sales taxes_ , preferably at rates of their own choosing, in exchange for lower local income tax rates. 



When a large package of new taxes is followed by eight years of economic stagnation and falling government revenues, it is time to consider the possibility that some of those tax policies were a mistake. There is nothing wrong with admitting mistakes and then fixing them. 



**Income Leveling**



Aside from revenue anxieties, the other major obstacle to any constructive policy change, in both the U.S. and Japan, is the stubborn notion that tax policy can and should “redistribute income.” 



Taxes do not redistribute income; they just _reduce_ income. If less real income is produced, there is less to distribute. 



High marginal tax rates on capital must make capital more scarce and therefore _more valuable_. Pretax returns to capital rise to adjust for the tax (in a world of mobile capital, after‐​tax returns cannot possibly remain below the global norm). With less capital per worker, however, there will be less real output per worker, and therefore less real income per worker. Labor thus ends up bearing the burden of taxes ostensibly aimed at affluent owners of capital.(19)



A steeply progressive tax on _labor_ income is mainly a tax on the returns to investments in higher education. This tax makes human capital more scarce than otherwise, and therefore more valuable. Salaries of highly skilled people will command a larger premium (or they will emigrate until that happens). Consumers end up paying this tax, because they must pay more for the artificially scarce services of highly skilled professionals and managers, directly or indirectly. 



Distribution tables, which purport to show how various income groups will fare under different tax policies, involve hopelessly static, zero‐​sum, partial equilibrium incidence assumptions.(20) The first step toward meaningful tax reform is to discard these meaningless statistical exercises. 



**Minimal First Steps**



It would be wonderful to see Japan embrace some sort of fundamental tax reform, perhaps borrowing ideas from Hong Kong or Singapore, but this might take more time than the situation can afford. In the meantime, there are many very constructive steps that could be taken immediately. 



1\. The highest individual income tax rate should not exceed 40% at the national level. The threshold for the 30% rate should be raised too. 



2\. The effective corporate tax should also be reduced to no more than 40% (including the enterprise tax) through some combination of lower rates and more rapid depreciation. 



3\. The VAT could be rolled‐​back to 3% for _at least_ two years (announcing a return to the 5% rate would shift buying plans forward in time, risking another slump after the increase took effect). 



4\. The securities transactions tax should be abolished _immediately_. Phasing it out would provide a risky incentive to delay stock transactions until 1999.(21)



There is much more to do, of course, and many possible ways by which to do it. The Japan Research Institute, among others, has offered several worthy suggestions.(22) The essential point is that marginal tax rates on capital and human capital are much too high in Japan, sapping the entrepreneurial vitality of the economy. The highest tax rates do the most damage to the economy in return for the least revenue. There is little risk in being bold, and great danger in being too timid. 



Economic growth requires more and better capital, including human capital. All taxes fall on individual suppliers of labor and capital, including taxes ostensibly levied on corporations or consumption. Even consumption taxes are really production taxes. Taxes on a company’s stockholders, workers and consumers hurt business, and taxes on business hurt stockholders, workers and consumers. Excessive tax rates on capital hurt labor by reducing investment and therefore slowing the growth or real output and income per hour of work. Demoralizing tax rates on labor likewise hurt capital by raising reservation wages, shortening lifetime work hours, and reducing the intensity and quality of work. 



If Japan continues to embrace the tax and spending policies of Continental Europe and Scandanavia, nobody should be surprised if economic performance becomes as disappointing as it has been in those areas. Without more vigorous economic growth, Japan’s future budget problems could become far more difficult. Philosophers are free to debate “equity” all they like. But the serious question to ask about the structure of tax incentives is the question that was at the top of Japan’s list in the 1950s: “ _How will this tax proposal help economic growth_?” An economy that is taxed into oblivion will not help anyone — not the poor, and not even the politicians. 



* * * * *   
**Appendix: Money Matters**  
  




Macroeconomics is concerned with the short‐​term growth of aggregate demand, or _nominal_ GDP. Microeconomics is concerned with longer‐​term incentives to expand aggregate supply, or _real_ GDP. That distinction led to the 1976 phrase “supply‐​side” economics — meaning the application of microeconomics to macroeconomic problems. The “demand side” was not neglected, however, but was properly assigned to the central bank. 



The Bank of Japan pursued a very expansionary monetary policy during the so‐​called “bubble” period, and an extremely restrictive monetary policy since 1991. From 1987 to 1989, reserve money grew by 11.5% a year, and broad money (M2+CDs) by 10.9%. **As Figure 8** shows, this expansive monetary policy financed growth of _nominal_ GDP of more than 7% a year from 1988 through 1990. 



In 1991–92, the monetary base was actually _reduced_ by 2.8% a year, which was quite remarkable. Broad money then grew at only a 1.2% rate. Growth of nominal GDP slowed to 2.8% in 1992 and to less than 1% from 1993 to 1995. Consumer prices have been _falling_ lately, aside from the one‐​time effect of the increased VAT in April 1997. 



The conventional wisdom is that monetary policy is impotent in Japan — merely “pushing on a string” — because nominal interest rates are very low. That is exactly what was said about the Federal Reserve from 1930 to 1933. It was dangerously wrong then, and still is. Interest rates are high in Turkey because the central bank prints too much money. Interest rates are low in Japan for the opposite reason. 



In a deflationary situation, shaky banks naturally want to hold more reserves, and people want to hoard more currency. The central bank has to accommodate those liquidity demands before additional bank reserves and currency can have any “reflationary” effect. If the central bank fails to convert enough securities into cash, through the discount window and open market purchases, then people have to liquidate assets and inventories to get cash. To stop such a deflation, the Bank of Japan merely has to purchases as many domestic or foreign securities as necessary, and discount freely (i.e., without rationing access to the discount window) at a penalty rate. 









**NOTES**



1\. William Niskanen writes of “the residual Keynesian perspective of many older economists, based on a theory — without evidence — that government deficits increase total demand.” — “Myths About the 1980s,” _The Wall Street Journal_ , November 5, 1996. 



2\. In his 1852 critique of Louis Napoleon’s “hot house” economics, even Karl Marx understood that “public works increase the obligations of the people in respect of taxes.” Indeed, debt service now accounts for 22% of Japan’s national budget, despite the lowest interest rates in world history. 



3\. When the United Kingdom first introduced a 10% VAT in April 1973, this was promptly followed by two full years of recession in 1974–75. The VAT was increased to 15% in June 1979, followed by another two years of recession in 1980–81. 



4\. “The problem is that analysts use tax revenue, not tax rates … When households evade a high tax, they drive down tax revenue, and so the high‐​tax experiment is less noticeable in the data.” — William Easterly’s comment in _Brookings Papers on Economic Activity_ , 1995:2, p. 421. 



5\. See Alan Reynolds, “Tax Cuts Will Restore the Tigers’ Roar,” _The Wall Street Journal_ , March 17, 1998. 



Also, Reinhard B. Koester & Roger C. Kormendi, ” Taxation, Aggregate Activity and Economic Growth: Cross‐​Country Evidence on Some Supply‐​Side Hypotheses” _Economic Inquiry_ , July 1989, pp. 367–86. 



6\. “By 1956 the total number of special [tax] measures exceeded fifty because the government was very active in the promotion of economic growth through tax devices.” Keimei Kaizuka, “The Tax System and Economic Development in Japan,” in Richard A Musgrave, Ching‐​huei Chang & John Riew, eds., _Taxation and Economic Development Among Pacific Asian Countries_ , Westview Press, 1994, p.55 



7\. “In 1950, a drastic change was enforced to make the income tax less progressive than it was under the influence of the Shoup proposals.… The treatment of savings or investment income [became] almost the same as that which would be applied to all savings under and expenditure tax.… However, this hybrid developed spontaneously, without any special attempt to avoid the double taxation of savings.” Hiromitsu Ishi, _The Japanese Tax System,_ Clarendon Press, 1993, pp. 86 & 97\. 



8\. Jerome B. Cohen, _Japan’s Postwar Economy_ , Indiana University Press, 1958, p. 107. 



9\. Martin Bronfenbrenner, “Economic Miracles and Japan’s Income‐​Doubling Plan” in William W. Lockwood, ed., _The State and Economic Enterprise in Japan_ , Princeton University Press, 1964, pp. 536n & 551–52. A more perceptive essay by Hugh Patrick of Yale University attributed Japan’s vigorous growth to continual “tax rate cuts,” but even Professor Patrick saw Japan as careening “from one balance of payments crisis to another.” 



10\. Henry C. Simons, selection from _Personal Income Taxation_ (1938) reprinted in H.C. Harlan, ed., _Readings in Economics and Politics_ , Oxford University, 1961, p. 303. Simons’ influential 1948 book, _A Positive Program for Laissez Faire,_ also advocated breaking‐​up large enterprises, which was almost adopted in extreme form by Occupation officials until a properly alarmed U.S. Congress stopped them. 



11\. Referring to Shoup, Vickrey, Pechman and others, Musgrave captured the zeal with which that former generation of tax missionaries spread their gospel: “The comprehensive income tax base thus became the banner of tax reform in the United States, designed to . . provide a global base on which progressive rates could be assessed… This movement … provided the focus of analysis and delight for a generation of tax economists in the United States.” R. A. Musgrave, “A Brief History of Fiscal Doctrine,” in A. Auerbach & M. Feldstein, eds., _Handbook of Public Economics_ , Elsevier Science, 1985, Vol. 1, p. 22. 



12\. Government investment, including “the pump‐​priming function of public finance,” was the main theme of former Prime Minister Kakuei Tanaka, _Building A New Japan_ , The Simul Press, 1972, p. 207. 



13\. Yukio Noguchi, “Tax Reform Debates in Japan,” in Michael J. Boskin & Charles E. McClure, eds., _World Tax Reform_ , International Center for Economic Growth, 1990, p. 114. 



14\. “A scheme that subjects the old to global income taxation would be superior to simply using the consumption tax.” — Maria S. Gochoco, comment on Yukio Noguchi, “Aging of Population, Social Security and Tax Reform,” in Taktoshi Ito & Anne O. Krueger, eds., _The Political Economy of Tax Reform_ , University of Chicago, 1992, p. 232, 



15\. Ishi, _op. cit_., p. 54: “What are the main causes for the sharp rise of fiscal deficits since 1973? … First, there was a conspicuous slowdown of Japanese economic growth cause by the two oil crises.” 



16\. Willi Leibfritz, John Thornton & Alexandra Bibbee, “Taxation and Economic Performance,” OECD Working Paper No. 176, 1997, p. 50. 



17\. Unlike leading groups of academic and think tank economists, who advocated leaving the top _national_ tax rate at 50–60% (while also taxing much more of investment income), Japan’s Committee for Economic Development, a business group, made a relatively bold proposal in January 1986 that the _combined_ national and local income tax rate (then 78%) should not exceed 50%. Even the major trade unions advocated reducing progression. M. Homma, T. Maeda & K. Hashimoto, “Japan,” in Joseph A. Pechman, ed., Comparative Tax Systems, Tax Analysts, 1987, pp. 429–32. 



18\. Toshiaki Tachibanaki & Tatsuya Kikutani, “Japan” in Dale W. Jorgenson & Ralph Landau, eds., _Tax Reform and the Cost of Capital_ , Brookings Institution, 1993, p. 262. 



19\. This modern “general equilibrium” analysis of tax incidence is often associated with Joseph Stiglitz, _Economics of The Public Sector_ , Norton, 1988, pp.430–32. But it was well understood by classical, pre‐​Keynesian economists. Harvard’s Sumner H. Slichter, _Modern Economic Society_ , Henry Holt, 1929, p. 743: “To the extent that a tax on capital retards the increase in the supply of capital, it enables capitalists to obtain a higher return on their funds and falls, therefore on others [consumers and wage earners].” 



20\. David F. Bradford, ed., _Distributional Analysis of Tax Policy_ , American Enterprise Institute, 1995. 



21\. There is no legitimate reason to restrict corporate share repurchases, and therefore no reason for making the easing of such restrictions temporary, as has been proposed. Share repurchases create capital gains, which will generate tax revenue and strengthen loan collateral. If the reason for the restriction on repurchases has been to foster dividend payouts, that would be better accomplished by restoring some relief from double‐​taxation of dividends. 



22\. _Japan Research Quarterly_ , Winter 1997/98. 
"
"
The National Weather Service office in San Diego, CA operates a cooperative observer network of weather stations, as do all NWS offices. The station in Coronado, CA, is particularly interesting since it is located on the roof of the Fire Station there.
Given that the MMTS sensor shown below is only about 2 feet above the tar and pea gravel roof, which is known to be a hot environment during the day, and a source for re-radiated heat at night, you have to wonder: “What were they thinking?”

Photo from NWS San Diego, click photo for larger image
The NOAA’s credit, this station is not part of the USHCN climate station network, but still, of what possible value could an air temperature measurement just 2 feet off the rooftop be to anyone?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2b7ab04',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I mentioned this a couple of posts back, exploding Comet 17P/Holmes is one of the strangest things in the sky ever but it gets stranger…Last night, astrophotographer Alan Friedman of Buffalo, NY, took a close-up picture of the comet’s core. “A strong deconvolution filter followed by multiple passes of unsharp mask and gaussian blur reveals startling new structure in comet 17P/Holmes.” i.e. after processing the image with Photoshop Here’ what popped out:

Linus would be impressed. The structure is due to outgassing from fissures and crevices in the comet’s core, but it is just a little too coincidentally spooky. Of course we’ve always seen things in the sky that mirror our minds, from constellations to UFO’s,  so why not Halloweeen?
Would you like to see the comet masquerading as the great pumpkin?? Look north after sunset for an expanding fuzzball in the constellation Perseus: sky map. Comet Holmes is about as bright as the stars of the Big Dipper, easy to see with the unaided eye and for backyard telescopes. The Chico Observatory in upper Bidwell Park will also be looking at this object.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2d50172',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterCORRECTION: The chart below is misleading. The ARD has not changed its color scheme. The chart for 2009 shows the 3-day forecast, whose color coding according to ARD in fact has not changed.  The 2019 is the forecast chart for the day, and this 1-day chart uses a different color code for temperature. It too has not changed since 2009. The third chart, as already mentioned, is from another drama source: website: wetter.de. Hat-tip: reader Taylor Martin
=======================================
WARNING: Watching too much German public television will make you hysterical, and pretty stupid (if you’re already leaning in that direction)
Sometimes you really wonder: can people be fooled really so easily?
German ARD public television network apparently thinks so, and has since gone off the deep end with the hype, drama and disinformation they add to their reports. I guess they think it’s working.
At Facebook one person illustrated this very well by showing the evolution of weather charts used by ARD German television in 2009, 2019 and this year. If you were to rate numerically the level of redness and fiery imagery and plot it,  you’d get a real hockey stick trend. Check out the forecast chart evolution below:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Hat-tip: Axel Robert Göhring at Facebook
In 2009, a nice hot summer day way forecast using pleasant color scheme of yellow and green. Even the “Samstag” (Saturday) forecast of highs from 19-25°C used the same green color scheme as it did for showing 34-36°C.
But as the years went by, Germans refused to panic enough about warming, and so by 2019 everything charts had to look hot and unbearable – even for forecasts of 20°C, see middle chart. Even temperatures in the 70sF look warm and toasty.
Then last year came doomsday prophet Greta with her inferno-visions and messages of hell. This year, weather forecast charts of highs in the 70s! to upper 80s are exploded in size, and made to look like explosion-like inferno images taken from beyond the gates of Hell. Just a note: I’m not sure if the 2020 chart is from ARD or another station, but you get the idea of what’s going on.
Also read here.


		jQuery(document).ready(function(){
			jQuery('#dd_579b3b87884f076d22b60daba2b95d2e').on('change', function() {
			  jQuery('#amount_579b3b87884f076d22b60daba2b95d2e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

A new report from the International Energy Agency is sparking headlines across the media. “Global carbon dioxide emissions soared to record high in 2012” proclaimed USA Today; The Weather Channel led “Carbon dioxide emissions rose to record high in 2012”; and the Seattle Post‐​Intelligencer added “The world pumped a record amount of carbon dioxide in the atmosphere in 2012.”   
  
  
The figure below (taken from the IEA summary) provides the rest of the story.   
  
  
It shows a breakdown of the change in carbon dioxide emissions from 2011 to 2012 from various regions of the globe.   






  
  
  
Notice that the U.S. is far and away the leader in reducing carbon dioxide (CO2) emissions, while China primarily is responsible for pushing global CO2 emissions higher. In fact, CO2 emissions growth in China more than offsets all the CO2 savings that we have achieved in the U.S.   
  
  
This will happen for the foreseeable future. Domestic actions to reduce carbon dioxide emissions will not produce a decline in the overall atmospheric carbon dioxide concentration. The best we can hope to achieve is to slow the rate of growth of the atmospheric concentration—an effect that we can only achieve until our emissions are reduced to zero. The resulting climate impact is small and transient.   
  
  
And before anyone goes and getting too uppity about the effectiveness of “green” measures in the U.S., the primary reason for the U.S. emissions decline is the result of new technologies _from the fossil fuel industry_ that are leading to cheap coal being displaced by even cheaper natural gas for the generation of electricity. As luck would have it, the chemistry works out that that burning natural gas produces the same amount of energy for only about half of the CO2 emissions that burning coal does.   
  
  
A new report from the U.S. Energy Information Administration estimates that as a result of these new technologies (e.g., hydraulic fracturing and horizontal drilling), globally, the technologically recoverable reserves of natural gas are nearly 50% greater than prior to their development.   
  
  
Currently, the U.S. is the leader in the deployment of these technologies, and the effects are obvious (as seen in the figure above). If and when more countries start to employ such technologies to recover natural gas, perhaps the growth in global carbon dioxide emissions will begin to slow (as compared to current projections).   
  
  
Considering that possibility, along with the new, lower estimates for how sensitive the global average temperature is to carbon dioxide emissions, and the case for alarming climate change (and a carbon tax to try to mitigate it) is fading fast.
"
"
Share this...FacebookTwitterMany news outlets have reported today that humans hunted and killed off the large methane-emitting mammoths 13,000 years ago, thus causing global cooling. This report has been published in Nature by some University of New Mexico scientists. Again man, armed with nothing but spears and arrows, may have been the culprit in significant climate change. Read here for example. Here, the Telegraph reports:
But by 11,500 years ago, around 80% of these big mammals had vanished forever.
Their disappearance, accounting for more than 114 lost species, came within 1,000 years of the arrival of humans in the New World.
So the mammoth killing ended at about 11,500 years ago? Wouldn’t that mean that it should have gotten colder from then on as a result? Let’s take a look at the temperatures over the last 20,000 year or so.

This graphic is taken from: oceanworld.tamu.edu.
At about 11,500 years ago the Ice Age ended! With all those big inefficient herbivores disappearing and the millions of tons of methane along with it, wouldn’t the hypothesis suggest that the Ice Age would have deepened, and not ended? Using AGW logic, less methane means more cooling, which leads to more ice, which then leads to more albedo, more cooling, more ice…you know an irreversible tipping point into a permanent ice age. But the opposite happened!
To me this stinks of more junk science by a journal desperate to rescue a science that’s quickly going the way of the mammoths: to extinction. This will be debunked in a matter of days.
Update: The following graphic from Jeff Masters Weather Underground shows the temperature for the last 100,000 years. Note all the spikes during the period. Why are the Younger Dryas caused by man and all the other dips not? Clearly the graphic shows that climate is always changing, often wildly. Belching mammoths were not the drivers.

Share this...FacebookTwitter "
"
La Nina and Pacific Decadal Oscillation Cool the Pacific 

Click here to view full image (228 kb) 

 “The shift in the PDO can have significant implications for global climate, affecting Pacific and Atlantic hurricane activity, droughts and flooding around the Pacific basin, the productivity of marine ecosystems, and global land temperature patterns. ” – NASA JPL

       
A cool-water anomaly known as La Niña occupied the tropical Pacific Ocean throughout 2007 and early 2008. In April 2008, scientists at NASA’s Jet Propulsion Laboratory announced that while the La Niña was weakening, the Pacific Decadal Oscillation—a larger-scale, slower-cycling ocean pattern—had shifted to its cool phase. 
This image shows the sea surface temperature anomaly in the Pacific Ocean from April 14–21, 2008. The anomaly compares the recent temperatures measured by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) on NASA’s Aqua satellite with an average of data collected by the NOAA Pathfinder satellites from 1985–1997. Places where the Pacific was cooler than normal are blue, places where temperatures were average are white, and places where the ocean was warmer than normal are red.
The cool water anomaly in the center of the image shows the lingering effect of the year-old La Niña. However, the much broader area of cooler-than-average water off the coast of North America from Alaska (top center) to the equator is a classic feature of the cool phase of the Pacific Decadal Oscillation (PDO). The cool waters wrap in a horseshoe shape around a core of warmer-than-average water. (In the warm phase, the pattern is reversed).
See the entire story here:
http://earthobservatory.nasa.gov/Newsroom/NewImages/images.php3?img_id=18012
See the PRESS RELEASE from JPL here:
http://www.jpl.nasa.gov/news/news.cfm?release=2008-066
Look out California agriculture. The wine industry, fruits and nut growers will be hit with a shorter growing season and more threats of frost, among other things.
Recently in Nevada County, much of their grape crop was wiped out. From The Union in Nevada County (h/t Russ Steele)
Nevada County’s agricultural commissioner will seek disaster relief from the state after tens of thousands of dollars worth of crops were ruined from last week’s freezing temperatures.
Orchard trees, wine grapes and pastures were hardest hit, Pylman said. The commissioner is compiling a report of damages that he will send to the state Office of Emergency Services in coming weeks.
“Growers don’t have anything to harvest. That’s a disaster in my mind,” Pylman said.
 
In Paradise, CA, Noble Orchards reports damage to their Apple crop from recent colder weather, as well as reports of issue with vineyards in the Paradise ridge area suffering from frost damage recently.
Here is a short history of PDO phase shifts:
In 1905, PDO switched to a warm phase.
In 1946, PDO switched to a cool phase.
In 1977, PDO switched to a warm phase.
California agriculture has ridden a wave of success on that PDO warm phase since 1977, experiencing unprecedented growth. Now that PDO is shifting to a cooler phase, areas that supported crops during the warm phase may no longer be able to do so.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f7d202f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The _Washington Post_ recently ran a shocking above‐​the‐​fold article warning us of “Escalating Ice Loss Found in Antarctica.” A new paper by Eric Rignot of NASA’s Jet Propulsion Laboratory shows a net loss of ice where most scientists thought the opposite would occur.



The _Post_ went full‐​bore with this one, spreading the article on to an entire interior page. The piece ends by noting that Rajenda Pachauri, head of the United Nations’ Intergovernmental Panel on Climate Change (IPCC), is so concerned that he’s is personally going down to inspect the situation.



He should. Before he even gets to Antarctica, Pachauri is going to see something even more surprising than Rignot’s finding. Despite a warming Southern Ocean, the amount of ice surrounding Antarctica is now at the highest level ever measured for this time of the year, since satellites first began to monitor it almost thirty years ago. This represents a continuation of the record set last winter (our summer).



Thanks to the miracles of modern technology, we can also look at the departure from the average for ice mass in a given month. At present, the coverage of ice surrounding Antarctica is almost exactly two million square miles above where it is historically supposed to be at this time of year. It’s farther above normal than it has ever been for any month in climatologic records. Around now, because it’s summer down there and the ice is headed towards its annual low point, there should be about seven million square miles of it. That means, as data in University of Illinois’ web publication Cryosphere Today shows, that there is nearly 30% more ice down in Antarctica than usual for this time of the year.



All of the IPCC’s models of Antarctica in the 21st century forecast a gain in ice, as a warmer surrounding ocean evaporates more water, which subsequently falls in the form of snow when it hits the continent. It’s simply too cold for rain in Antarctica, and it’ll stay that way for a very long time.



Concerning Antarctica as a whole, the IPCC’s new climate compendium notes “the lack of warming reflected in atmospheric temperatures averaged across the region.” Other studies, such as Peter Doran’s in _Nature_ in 2003, show actual cooling in recent decades. (There is a small area of significant warming in the peninsula that points towards South America, but this is less than 2% of Antarctica’s total land mass.)



There’s brand new evidence, just published in mid‐​January in _Geophysical Research Letters_ , of a striking increase in snowfall over that peninsula. The few snowfall records that are available elsewhere in Antarctica show considerable variation from decade to decade, so discriminating the “signal” of increased snowfall caused by global warming from all the rest of the “noise” may be very difficult indeed.



We see the same problem with hurricanes and global warming. Their strength and numbers vary considerably from year to year. 2005 was the most active year ever measured in the Atlantic Basin, while 2007 was one of the weakest in history. How do you find the fingerprint of global warming amidst such variation?



So it’s not warming up, and the snowfall data are equivocal, yet the continent is experiencing a net loss of ice. How can this be, and is it even important? The current hypothesis is that warmer waters beneath the surface are somehow loosening the ice. That’s plausible, but again, there’s precious little proof of it.



And further, the bottom line is that there is more ice than ever surrounding Antarctica.



One of the tired tropes that reverberate throughout global warming reporting is that inconvenient facts get left out. In this case, it’s blatant. Midway through the _Post_ ’s page‐​long article comes a statement that “these new findings come as the Arctic is losing ice at a dramatic rate.” Wouldn’t that have been an appropriate place to note that, despite a small recent loss of ice from the Antarctic landmass, the ice field surrounding Antarctica is now larger than ever measured?
"
"

A friend from my coffee group sent this about recognizing the signs of a stroke and encouraged me to post it and spread the word. I checked it out to make sure it was not another Internet hoax and I’m happy to report it is valid.
If everyone can remember this simple STR procedure, lives could be saved.
Some background –
During a BBQ, a friend stumbled and took a little fall – she assured everyone that she was fine (they offered to call paramedics) …..she said she had just tripped over a brick because of her new shoes.
They got her cleaned up and got her a new plate of food. While she appeared a bit shaken up, Ingrid went about enjoying herself the rest of the evening.
Ingrid’s husband called later telling everyone that his wife had been  taken to the hospital – (at 6:00 pm Ingrid passed away.) She had suffered a  stroke at the BBQ. Had they known how to identify the signs of a stroke, perhaps Ingrid would be with us today. Some don’t die…. they end up in a  helpless, hopeless condition instead.
A neurologist says that if he can get to a stroke victim within 3 hours he can totally reverse the effects of a stroke… totally . He said the trick was getting a stroke recognized, diagnosed, and then getting the patient medically cared for within 3 hours, which is tough.
RECOGNIZING A STROKE
Remember these ‘3’ steps:  STR. It’s the first three letters of the word STRoke.
Sometimes symptoms of a stroke are difficult to identify. Unfortunately, the lack of situational awareness spells disaster. The stroke victim may suffer severe  brain damage when people nearby fail to recognize the symptoms of a stroke .
Now doctors say a bystander can recognize a stroke by asking three simple
questions:
S * Ask the individual to SMILE.
T * Ask the person to TALK and SPEAK A SIMPLE SENTENCE (Coherently)
       (i.e. It is sunny out today)
R * Ask him or her to RAISE BOTH ARMS.
If he or she has trouble with ANY ONE of these tasks, call 999/911 immediately and describe the symptoms to the dispatcher.
See References: American Stroke Foundation, Stroke Awareness.org
New Sign of a Stroke ——– Stick out Your Tongue
Ask the person to ‘stick’ out his tongue.. If the tongue is ‘crooked’, if it goes to one side or the other , that is also an indication of a stroke.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1962e56',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWind energy investment plummeted in the third quarter of this year in Germany, reports the online IWR here.
“The nine-month figures now available make it clearer that this trend will continue in 2020,” writes the IWR.
Issued construction licenses drop 70 percent in 3 years
Installation of wind energy in Germany peaked in 2017 (see bar chart here), but has since fallen sharply after the German federal government enacted new rules and regulations against their construction.
According to Clean Energy Wire here,
An analysis by energy industry lobby group BDEW found that the falling number of permits issued for onshore wind turbines was the main factor behind the decline, with issued licenses dropping by 70 percent over three years. About 11 GW, roughly 2,000 turbines, were stuck in bureaucratic procedures as of mid-2019.”
In the third quarter of 2020, only 85 turbines with a total capacity of 293 MW were installed. Germany has approximately 30,000 turbines operating.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the first nine months of 2020, only a total of 306 new wind turbines  (1,104 MW) were added.
The IWR forecasts 1200 MW of new onshore wind energy capacity to be added for the full year 2020. In 2019 the figure was slightly lower at 1,078 MW. The slight increase, the IWR reports “is not sufficient to compensate for the decline in offshore wind energy.”
“Overall, it is thereforeexpected that the total increase in new wind power capacity in the current year will again be significantly weaker than in the already weak previous year 2019.”
Green energy expansion “being totaled”, a “farce” 
Green energy lobbyist Volker Quaschning at Twitter sees Germany’s green energy expansion as being “totaled” and the country’s promises of climate protection as “a farce”

Totalschaden mit Ansage: Ausbau der #Windkraft bricht noch weiter ein. So werden alle deutschen #Klimaschutz-Versprechen zur Farce. Lieber @peteraltmaier, @BMWi_Bund: Wann sorgen Sie endlich dafür, dass wenigstens Ihre Ausbauziele eingehalten werden? https://t.co/5JhhFYoUml
— Volker Quaschning (@VQuaschning) October 6, 2020



		jQuery(document).ready(function(){
			jQuery('#dd_a568152ecfe8f688295399cd73262f09').on('change', function() {
			  jQuery('#amount_a568152ecfe8f688295399cd73262f09').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The nation’s 76 million stockholders have “internalized their new role as capitalists,” causing public opinion to favor investor‐​friendly policies over government programs, says Richard Nadler in “The Rise of Worker Capitalism” (Policy Analysis no. 359). Forty‐​three percent of U.S. households own stocks or stock mutual funds, a 126 percent increase in shareholding over the last 15 years. The rate of increase was particularly steep among laborers and farmers (106 percent), householders 34 years old or younger (64 percent), and families with incomes under $25,000 (80.4 percent). As wage earners become owners of capital, Nadler finds, they increasingly favor policies that reduce taxes on savings and distrust government “investments” such as Social Security. “Congress should enact policies that expand worker ownership and financial self‐​sufficiency,” Nadler concludes, pointing out the importance of spreading wealth to even larger segments of the population by expanding individual retirement accounts and 401(k) plans and instituting individually owned Social Security accounts.



 **Tennessee—Still the Volunteer State?**



In “The Case against a Tennessee Income Tax” (Cato Briefing Paper no. 53), Stephen Moore and Richard Vedder argue that insti‐​tuting an income tax in Tennessee would reduce growth and job creation and would be the most economically destructive way to close the state’s budget shortfall. The study was released the day before the state legislature was to begin debating Gov. Don Sundquist’s proposed 3.75 percent state income tax. Tennessee is currently one of only nine states without an income tax. Moore, director of fiscal policy studies at Cato, and Richard Vedder, an economics professor at Ohio University, contend that “Tennessee’s structural deficit problems are a result of a huge growth in state expendi‐​tures, not insufficient revenues.” Of the options available for closing the state budget deficit, estimated to be between $300 million and $500 million, an income tax “would likely be the single most economically harmful. Tennessee derives large economic benefits from not having an income tax, and it should not forfeit those benefits,” the authors conclude.



 **An Agenda for the WTO**



Supporters of free trade should abandon the reciprocity model of negotiations and instead pursue a course of coordinated unilateralism, in which the benefits of open markets at home and abroad are clearly recognized, write the authors of “Seattle and Beyond: A WTO Agenda for the New Millennium” (Trade Policy Analysis no. 8). Brink Lindsey, director of Cato’s Center for Trade Policy Studies; Daniel Griswold, associate director of the center; Mark Groombridge, research fellow; and Aaron Lukas, trade policy analyst, argue that the new WTO round should be seen as a “ ‘bottom‐​up’ process in which countries liberalize, not merely to gain ‘concessions’ from other countries, but primarily to reap the economic rewards of their own liberalization.” Free traders, the authors maintain, “should focus on getting the available gains as quickly as possible and fend off efforts to clog and corrupt the agenda with illiberal initiatives.”



 **Iraqi Threat Overblown**



The U.S. policy of attempting to remove Saddam Hussein from power will be difficult, could be counterproductive, and might throw Iraq into a civil war, argues defense analyst David Isenberg in “Imperial Overreach: Washington’s Dubious Strategy to Overthrow Saddam Hussein” (Policy Analysis no. 360). The author contends that the Iraq Liberation Act of 1998, which states that the United States will aid efforts to overthrow Saddam and promote democracy, is flawed because it does not offer a realistic way of dealing with the Iraqi leader. Isenberg believes that the threat of Saddam is “overblown,” pointing out that Saddam’s army has already been decimated by war and sanctions. “Saddam may be odious, but his regime does not pose a major threat to America’s security.” Isenberg argues that a more realistic policy would be to lift general economic sanctions in exchange for international weapons inspections and to continue a selective embargo on military weaponry.



 **Not‐​So‐​Smart Growth**



The campaign to eliminate urban “sprawl” and replace it with “smart growth” has been financed with federal tax dollars, note the authors of a new Cato study, “Smart Growth at the Federal Trough: EPA’s Financing of the Anti‐​Sprawl Movement” (Policy Analysis no. 361). The federal government, via grants from the Environmental Protection Agency to nonprofit organizations, has been covertly supplying funds and technical support to anti‐​automobile, anti‐​suburb groups. Peter Samuel, editor of _Toll Roads Newsletter_ and a consultant on EPA policies for the George C. Marshall Institute, and Randal O’Toole, executive director of the Thoreau Institute and an adjunct scholar at the Cato Institute, argue that “EPA’s campaign fundamentally subverts not only the Tenth Amendment but the very concept of democracy itself.”



 **Social Security Is Still a Bad Deal**



The current Social Security system would not pay higher rates of return and benefits than a privatized system of personal retire‐​ment accounts, writes Peter J. Ferrara in “Social Security Is Still a Hopelessly Bad Deal for Today’s Workers” (Social Security Paper no. 18). The analysis refutes a recent study by John Mueller for the National Committee to Preserve Social Security and Medicare. Ferrara, chief economist and general counsel with Americans for Tax Reform and senior fellow at Cato, points out that Mueller’s findings are contradicted by a broad range of analysts, institutions, and leaders, including President Clinton, Harvard economics professor Martin Feldstein, the Heritage Foundation, the World Bank, and the 1994–95 Social Security Advisory Council.



 **The Imperial Presidency**



Modern presidents have moved beyond their constitutional duty of seeing “that the Laws be faithfully executed” and have instead been usurping vast lawmaking powers reserved to Congress or the states, argue attorneys William J. Olson and Alan Woll in “Executive Orders and National Emergencies: How Presidents Have Come to ‘Run the Country’ by Usurping Legislative Power” (Policy Analysis no. 358). The authors note that, during the recent presidential scandals, many people called for the investigations to end “so that the president could get back to ‘the business of running the country.’ ” How did we get to a point, the authors ask, “where so many Americans think of government as embodied in the president and then liken him to a man running a business?” The answer rests, in part, “with the growth of presidential rule through executive order and national emergency,” according to the authors. Congress has delegated more and more power to the executive branch, aiding and abetting the expansion of presidential power, the authors note. The courts have acted in just two cases––in 1952 and 1996––to restrain the executive branch. The good news, the authors point out, is that the nation’s governors have just forced President Clinton to rewrite a federalism executive order; and now there are two proposals in Congress that seek to limit presidential lawmaking.



 **Clinton’s Pyrrhic Victory in Kosovo**



The Clinton administration’s policy in Kosovo has habitually failed to meet its objectives and will continue to entangle the United States in multi‐​billion‐​dollar, open‐​ended peacekeeping operations, writes Christopher Layne, a visiting scholar at the Center for International Studies at the University of Southern California. In “Faulty Justifications and Ominous Prospects: NATO’s ‘Victory’ in Kosovo” (Policy Analysis no. 357), Layne writes that the administration “stumbled into war and blundered its way to ‘victory.’ ” Layne says that President Clinton’s claim of victory “rings hollow”: NATO’s intervention not only killed many innocent civilians in Yugoslavia; it also caused serious economic and social disruptions throughout the Balkans and greatly strengthened the position of the extremist Kosovo Liberation Army. Layne warns that the war continues to have negative policy repercussions. “The war with Yugoslavia has had important geopolitical effects that reverberate far beyond the Balkans. Clinton’s Kosovo policy has had portentous consequences for America’s relations with its great‐​power rivals, Russia and China, and its great‐​power allies, the West European nations.”



 **Cut Global Warming Program**



Congress should eliminate funding for a $1.4 billion global warming program, argues Jerry Taylor, Cato’s director of natural resource studies, in “Energy Efficiency: No Silver Bullet for Global Warming” (Policy Analysis no. 356). The Climate Change Technology Initiative, being pushed by the Clinton administration as a way to combat global warming, is a “sham,” and a “repackaging of failed programs” that do nothing to significantly reduce global temperatures, he writes. The program—an amalgam of tax credits, research and development, product labeling and awareness programs, demonstration projects, and subsidies and regulations to increase energy efficiency and the economic attractiveness of renewable energy—is “built on economic ignorance and political symbolism,” Taylor writes.



 **Protocols on Biological Weapons Ineffective**



The protocols proposed for the Biological Toxins and Weapons Convention would do little to stop the spread of bioweapons and could compromise valued U.S. secrets and critical data used for defense against biological weapons, writes Eric R. Taylor of the University of Louisiana at Lafayette in “Strengthening the Biological Weapons Conventions: Illusory Benefits and Nasty Side Effects” (Policy Analysis no. 355). Taylor writes that proposed protocols render inspections “useless” in demonstrating either compliance with or violation of the convention. According to Taylor, U.S. pharmaceutical development, which relies heavily on the very technology that is also critical to bioweapons research and development, would be especially hurt by the new protocols. “The future of the people’s right to be secure in their possessions and personal effects is placed in peril by the Biological Toxins and Weapons Convention protocols,” he writes. “Although an attack with biological weapons on the United States would be dangerous, an assault on U.S. constitutional rights in an effort to strengthen an international convention has little hope of stopping the spread of those weapons.”



 **Repeal the Community Reinvestment Act**



The Community Reinvestment Act should be repealed, writes economist George J. Benston in “The Community Reinvestment Act: Looking for Discrimination That Isn’t There” (Policy Analysis no. 354). Originally intended to deal with “redlining”—the alleged refusal of banks to lend to residents of poorer urban areas inhabited by racial minorities—the two‐​decade‐​old CRA is an expensive way to deal with a problem that may not exist, the study finds. Benston reports that qualified applicants, regardless of their address, do not suffer unwarranted discrimination in lending. “Researchers using the best available data find very little discernible home‐​mortgage lending discrimination based on area, race, sex, or ethnic origin,” writes Benston, the John H. Harland Professor of Finance, Accounting, and Economics at Emory University.



 **Cradle‐​to‐​Grave Taxation**



The federal gift and estate tax, better known as the “death tax,” is clearly a failure from an economic standpoint, but “the biggest problem with the death tax is a moral one,” writes law professor Edward J. McCaffery in “Grave Robbers: The Moral Case against the Death Tax” (Policy Analysis no. 353). He notes that the tax’s economic shortcomings are well‐​known. It “raises barely over 1 percent of total federal tax revenues,” and “for every dollar raised from the tax, roughly another dollar is lost because of avoidance, compliance, administrative, and enforcement costs.” But it is the moral impact that is most objectionable, according to McCaffery. The tax “rewards a ‘die‐​broke’ ethic, whereby the wealthy spend down their wealth on lavish consumption, and discourages economically and socially beneficial intergenerational saving.” McCaffery, a professor in the University of Southern California Law School, finds that the death tax rewards those who don’t work, don’t save, and spend all of their wealth.



 _This article originally appeared in the January/​February 2000 edition of_ Cato Policy Report.
"
"

The security of the United States does not require nearly 1,600 nuclear weapons deployed on a triad of systems — bombers, land‐​based intercontinental ballistic missiles (ICBMs), and submarine‐ launched ballistic missiles (SLBMs) — to deliver them. As Cato’s Benjamin H. Friedman, Christopher A. Preble, and Matt Fay calculate in **“The End of Overkill? Reassessing U.S. Nuclear Weapons Policy”** (White Paper), a smaller arsenal deployed entirely on submarines would save roughly $20 billion annually while deterring attacks on the United States and its allies. The triad grew from the military services’ competition to meet the Soviet threat. The public rationale was based upon the notion of a second strike: a diversity of delivery systems ensured the nuclear arsenal’s survival against a Soviet preemptive attack. The more sophisticated rationale was a first strike: deterring Soviet aggression against European allies required the ability to preemptively destroy their nuclear forces. But, as the authors show, “U.S. power today makes the case for the triad more dubious.” No U.S. adversary has the capability to destroy all U.S. ballistic submarines, let alone all three legs, and there would be time to adjust if that changed. In fact, nuclear weapons are essentially irrelevant in actual U.S. wars, which are against insurgents and weak states without nuclear arsenals. Cases where the success of deterrence hinges on the U.S. capability to destroy enemy nuclear forces are far‐​fetched. “Even hawkish policies do not require a triad,” the authors write. At a time when austerity heightens competition for Pentagon resources, service leaders may see nuclear missions “as red‐​headed step‐​children that take from true sons.” That shift would facilitate major reductions in the nuclear arsenal, the elimination of at least one leg of the triad, and substantial savings.



 **Comparing Welfare to Work**  
In 1995 the Cato Institute published a groundbreaking study estimating the value of the full package of welfare benefits available to a typical recipient in each of the 50 states and the District of Columbia. It found that not only did the value of such benefits greatly exceed the poverty level but, because welfare benefits are tax‐​free, their dollar value was greater than the amount of take‐​home income a worker would receive from an entry‐​level job. Since then, many welfare programs have undergone significant change. In their new analysis, **“The Work versus Welfare Trade‐​Off: 2013”** (White Paper) Michael D. Tanner, senior fellow at the Institute, and Cato research assistant Charles Hughes examine the current system in the same manner. “Welfare benefits continue to outpace the income that most recipients can expect to earn from an entry‐​level job, and the balance between welfare and work may actually have grown worse in recent years,” they write. In fact, the current system provides such a high level of benefits that it acts as a disincentive for work. Welfare currently pays more than a minimumwage job in 35 states, even after accounting for the earned income tax credit, and in 13 states it pays more than $15 per hour. “If Congress and state legislatures are serious about reducing welfare dependence and rewarding work,” the authors conclude, “they should consider strengthening welfare work requirements, removing exemptions, and narrowing the definition of work.” Moreover, states should consider ways to shrink the gap between the value of welfare and work by reducing current benefit levels and tightening eligibility requirements.



 **Subsidizing the Risk of Terrorism**  
The terrorist attacks of September 11, 2001, inflicted enormous losses on the insurance industry and businesses. In the wake of these disruptions, the government enacted the Terrorism Risk Insurance Act of 2002 to create a “temporary” federal backstop against catastrophic losses. In effect, this program subsidized private risk with public funds through a cost‐​sharing program for which the government does not receive any compensation. But as Robert J. Rhee, professor of law at the University of Maryland, writes, “if there was some ambiguity about the program’s need before, there is none now.” In **“The Terrorism Risk Insurance Act: Time to End the Corporate Welfare”** (Policy Analysis no. 736), Rhee argues that terrorism risk is not more severe than other insurable risks such as natural catastrophes. A federal backstop stakes public money to protect the insurance industry, and subsidizes the terrorism risk insurance premiums for commercial policyholders. “The private market is capable of underwriting this risk,” he continues. Yet in response to effective lobbying by the insurance industry and business interests, Congress has twice extended the program. The program is now scheduled to sunset at the end of 2014, 12 years after this supposedly temporary program was instituted. Rhee argues that the program should sunset as scheduled in 2014, thus ending this form of corporate welfare. “After the fears of the unknown have subsided,” he concludes, “[the insurance market] can more rationally assess terrorism risk and price it.”



 **Driving Investment Policy**  
No country has been a stronger magnet for foreign direct investment than the United States. Valued at $3.5 trillion, the U.S. stock of inward foreign direct investment accounted for 17 percent of the world total in 2011, more than triple the share of the next largest destination. In **“Reversing Worrisome Trends: How to Attract and Retain Investment in a Competitive Global Economy”** (Policy Analysis no. 735), Daniel J. Ikenson, director of Cato’s Herbert A. Stiefel Center for Trade Policy Studies, notes that as the world’s largest economy, the United States has been able to attract the investment needed to undergird its position atop the global economic value chain. “But the past is not necessarily prologue,” he argues. Indeed, while the U.S. claim to 17 percent of the world’s stock of foreign direct investment is impressive, the share stood at 39 percent as recently as 1999. To a large extent, this trend reflects the emergence of new, viable destinations for investment resulting from inevitable demographic, economic, and political changes. “However, some of the decline is attributable to a deteriorating U.S. investment climate,” Ikenson writes. That environment conspires to deter inward investment and to encourage companies to offshore operations that could otherwise be performed competitively in the United States. Ikenson concludes that a proper accounting of these policies, followed by implementation of reforms to remedy shortcomings, will be necessary if the United States is going to compete effectively for the investment required to fuel economic growth and higher living standards.



 **Against Military Action in Syria**  
In the midst of growing public wariness about large‐​scale foreign interventions, the Obama administration has decided to arm the Syrian rebels. But according to Erica D. Borghard, a PhD candidate in political science at Columbia University, in **“Arms and Influence in Syria: The Pitfalls of Greater U.S. Involvement”** (Policy Analysis no. 734), those who call for increasing the scope of U.S. aid to the Syrian rebels are wrong on all counts. “There is a high risk that the decision to arm the Syrian rebels will drag the United States into a more extensive involvement later,” she writes — and this is the very scenario that the advocates for intervention claim they are trying to avoid. The unique characteristics of alliances between states and armed nonstate groups — in particular “their informal nature and secrecy about the existence of the alliance or its specific provisions” — create conditions for states to become locked into unpalatable obligations. That seems especially likely in this case. The Obama administration, therefore, should not have decided to arm the Syrian rebels. Looking ahead, Borghard writes, it is important for policymakers to understand the nature of alliances between states and armed nonstate groups even after the Syrian conflict is resolved. “Given that Americans are unwilling to support large‐​scale interventions in far‐​flung reaches of the globe, policymakers looking for military solutions to political problems may conclude that arming proxy groups may be an attractive policy choice,” she concludes. They should instead, however, avoid committing to conflicts that don’t threaten core national security interests.
"
"

Government policies encourage Americans to live in risky places on seacoasts and along flood‐​prone rivers. Disasters happen, governments bail people out, they rebuild in the same places, bad incentives stay in place, further disasters strike and more dollars and lives are lost.   
  
  
The _Washington Post_ reported on recent flooding along the Mississippi River, which I’ve excerpted below.   
  
  
But first, here are some general points about flooding and governments:   




These points are developed further here, here, here, here, here, here, and here.   
  
  
Here is what the _Washington Post_ reported:   




The city [St. Charles, MO] of about 70,000 has long grappled with flooding from the Missouri and Mississippi rivers, as well as rising water from creeks and streams — making it one of the most flood‐​prone regions in the state, with some $18 million in flood insurance claims paid out since 1970 by the Federal Emergency Management Agency.   
  
  
Yet that hasn’t stopped the city from planning a $1.5 billion riverfront development along the Missouri’s banks, 120 acres of upscale shops, restaurants and apartments mostly in the river’s flood plain, an area that has been partly submerged this summer.   
  
  
Several hundred miles to the south, Louisiana is experiencing the fallout from decisions like that one — decades of rampant development behind tall levees that have cut the Mississippi and its many tributaries off from the vast open floodplains the rivers once carved for themselves.   
  
  
Tropical Storm Barry, which made landfall Saturday as a hurricane, is predicted to travel up the Mississippi toward St. Louis, deluging surrounding areas as it goes, with rainfall that will be channeled back toward the Louisiana coast in days to come — the latest potential catastrophe.   
  
  
“The major cause of record, recent flooding is entirely man‐​made — the dramatic constriction of our large rivers by oversized levees, flood plain development and structural narrowing for barge traffic,” said Robert Criss, professor emeritus with the Department of Earth and Planetary Sciences at Washington University in St. Louis.   
  
  
This year’s historic floods throughout the Midwest caused billions of dollars in damages; washed out highways, bridges and dozens of levees; swamped crop lands and cities; sent residents fleeing for their lives; and left a death toll in several states.   
  
  
Decades of development have contributed to the problem. Claims to FEMA’s flood insurance program have increased rapidly in the past two decades and spread beyond coastal regions.   
  
  
… Yet with millions of people living in flood plains and shipping and tourism economies built on these key waterways, there is little political will for change. Environmentalists charge that jurisdictions hungry for tax revenue are continuing to plan risky projects without taking floods’ worsening intensity into account, heedless of the economic and human consequences.   
  
  
“It’s lunacy,” said David Stokes, executive director of the Great Rivers Habitat Alliance. “They’re continuing to build in places where Mother Nature intended water to go. And there’s no end to it.”


"
"
Share this...FacebookTwitterHow is it that a settled science keeps finding things never expected?
For example, the HIAPER Pole-to-Pole Observations (HIPPO) mission was launched in January 2009 and will make a series of five flights over three years covering more than 24,000 miles to sample the atmosphere in some of the most inaccessible regions of the world. Read HIPPO background here.
The goal of the mission is the first-ever, global, real-time sampling of carbon dioxide and other greenhouse gases across a wide range of altitudes in the atmosphere, from pole-to-pole.
Professor Mark Zondlo of Princeton University has taken measurements of water vapour in the atmosphere, from 14 km high to just above the sea ice, using a vertical cavity surface mini laser hydrometer.
Watch Zondlo video here.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here are some of Professor Zondlo’s observations so far:
We don’t really know how clouds are formed. Water vapour impacts the climate more than any other gas.
What we are finding is surprising. Large plumes of water vapour exist in areas we never expected to find them.
Learning how this fits into the puzzle is crucial for predicting climate and making smart policy decisions.
What does that mean? It means the climate models used so far were nothing more than junk, thus the same applies for their predictions. They completely neglected the water vapour factor (and who knows what other factors).
Climate forecasting is best left to real forecasters, and not tainted modelers.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe German transformation to green energies will fail due to wind power
By Prof. Fritz Vahrenholt, Die kalte Sonne

Not going to work, says German energy expert Prof. Fritz Vahrenholt
(German text translated/edited/subheadings by P. Gosselin)
The goals of the German transition to green energies are simple in terms of energy policy:
1. phase-out nuclear energy by 2022,
2. phase-out coal by 2035,
3. phase-out oil and gas in parallel and completely by 2050.
The energy needed for electricity, heat, mobility and industrial processes in climate-neutral Germany will then have to be supplied by wind and solar energy and a few percent by hydropower and biomass. This is at least according to the plans of the German government, which are supported by all major social players.
Is this realistic?
Today, wind and photovoltaics supply slightly less than 30% of the 600 terawatt hours of electricity (1 terawatt hour Twh is 1 billion kilowatt hours Kwh). Today 126 Twh is supplied by wind energy and 46 Twh by photovoltaics. For 600 TWh, the same mix would need 439 Twh of wind and 161 Twh of solar. For the sake of simplicity, let us assume that this amount of electricity should be generated by the largest wind turbines, namely 5 megawatt turbines positioned 1000 m apart. With an annual efficiency of 25%, a turbine produces an average of 5 MW x 0.25 x 8760 (hours) = 10,950 Mwh = 0.01095 Twh. For 439 Twh we would need 40,000 such turbines. To accomplish this, an area of 200 km x 200 km (40,000 sq km) would be required.
Too much unneeded surplus power
But we still would not reach the goal. Wind energy is produced when the wind blows, and not necessarily when the consumer needs it. With a power supply in Germany based solely on volatile sources, 36% of the electricity generated annually can be consumed directly (source: Dr. Ahlborn). The rest is surplus electricity that has to be stored. For economic reasons, storage in hydrogen alone is the best option here. For this purpose, a gigantic number of electrolysis plants would have to be installed.
Huge area required for electricity from wind turbines
However, it is completely uneconomical to dimension the capacity according to the extreme peaks of strong wind events. Therefore, about 12% of the wind energy has to be regulated. This now leaves 52% of the electricity generated that can be stored in hydrogen. Electrolysis of hydrogen, storage/methanization and conversion back into electricity leaves only 15.6% of the 52%. The entire conversion chain generates a loss of 2/3 of the electricity used. 36% plus 15.6% result in about 50% of the generated wind power being usable. Thus, we need twice as many turbines. The area for the 80,000 wind turbines becomes 80,000 km², which corresponds to an area of 283 km x 283 km (80,089 sq. km).
Now add the demand from transport and heating…
But we remain very far from the finish line. Up to now we have only covered the electricity demand with 2 x 439 Twh, but without supplying the demand from transport and heating. Also with demand from transport (today 600 Twh) and heat (today 1200 Twh) we have storage and conversion losses when the necessary electricity is generated by wind and solar. Here we only consider wind for this, because with photovoltaics, the annual efficiency of 10% full load hours is significantly lower and the land consumption is many times higher. This makes our calculation extremely conservative.
Devastating lack of efficiency


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Assuming that the transport sector can actually be powered by battery vehicles, which is justifiably doubtful, converting cargo transport, maritime transport or air cargo transport over to electricity is already adventurous. Instead, synthetic fuels would have to be used.
And here as well the electricity calculation is devastating. As Dr. Detlef Ahlborn was able to show, the Frankfurt airport alone consumes 14.7 million liters of kerosene per day (before Corona), which comes out 4.3 million tons annually. 4.3 million tons of kerosene correspond to an energy value of 47 Twh. If one wanted to synthesize kerosene from electricity with the help of hydrogen (assumed efficiency 50%), 100 Twh of electricity would be needed. Just for the Frankfurt airport alone, this comes out to being as much as the German wind energy industry currently produces (126 Twh).
Minimum 900 Twh for heating and transport
Next we conservatively assume that all passenger transport also can be powered with electricity and that only a quarter of the amount of the 600 Twh of energy consumed today (since electric cars are more efficient by this factor) is needed. However, we also want to drive a car when there is no wind, and as explained above, most of this electricity has to be put through the chain of hydrogen, storage, and re-electrification, thus doubling the input electricity to 300 Twh.
We further assume that the current demand of 1200 Twh for heating can be reduced to a quarter through electrification (heat pump) so that here too, due to the necessary intermediate storage of wind power via hydrogen, the necessary doubling of wind energy leads to 600 Twh. If synthetic gas from wind power, hydrogen, is used directly, the yield is even worse because the efficiency of the heat pump is not applicable. Transport and heat therefore in the best case lead to a wind power demand of 900 Twh. This results in an area requirement of another 80,000 km², thus we are up to 160,000 km² of area needed by wind turbines (approx. half the area of Germany).
Another 600 Twh for heavy industry
But we still haven’t reached the ultimate target because the most difficult part is still unsolved. Emissions from the steel, chemical and cement industries (10% of CO2 emissions) require 600 Twh, according to industry estimates (www.in4climate.nrw). This is easy to understand if one remembers the above example of Frankfurt Airport. And plastics, pharmaceuticals, insulating materials, paints, varnishes, adhesives, detergents and cleaning agents may then only be produced using CO2 plus hydrogen. The replacement of industrial CO2 emissions thus leads to a further 55,000 km² area for wind turbines, so now we are up to 215,000 km² – much more than half of Germany’s total area.
2/3 of Germany would end up plastered with wind turbines
Two thirds of Germany would now be outfitted with 200-meter tall rotating wind turbines at a distance of 1000m, no matter if there is a city, a river or a highway, a forest, a lake or a nature reserve.
Can we and policymakers imagine such a Germany?
Environmental catastrophe, obstinate policymakers
If you wish to know which effects wind power plants in large numbers have on the extinction of birds of prey, bats, the decline of insects already today, then read it in our book Unerwuenschte Wahrheiten (Unwanted Truths). There you’ll find the hidden fact that wind farms lead to a considerable warming in their area of influence of about 0.5 ° Celsius because the rotating blades compensate for the strong temperature gradient at night and shovel warmer air back to the ground. Numerous studies have shown that the soil in the windparks has dried up considerably.
10-fold higher electricity prices
But politicians refuse to discuss the environmental incompatibility of a massive expansion of wind power plants. Recently the German Bundestag decided that the so-called legal, suspensive effect of objection and action for rescission is no longer applicable to lawsuits against turbines taller than 50 meters. In this way, Germany can be now turned into a single giant wind park without all the annoying objection.
It is almost superfluous to point out that we are talking about astronomical costs. Electrolysis and power-to-gas plants cannot be operated free of charge.
From today’s point of view, one has to expect a tenfold higher electricity price. Any person can imagine the consequences for jobs and prosperity.
Prof. Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_1a0c8b531431dcb96168154efc24df57').on('change', function() {
			  jQuery('#amount_1a0c8b531431dcb96168154efc24df57').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I haven’t been following the debate over Sen. Dodd’s financial overhaul closely enough to have an opinion on the overall package, but Mike Masnick flags one aspect of the legislation that seems really troubling. Bob Litan explains:   




Under existing law, startup companies can raise money easily and quickly from “accredited investors” — individuals with substantial wealth or income. There is no need for the companies or the investors to gain approval from any state or regulatory official.   
  
  
All of this would change if Section 926 of the Dodd bill is included in any final reform legislation. That section would require, for the first time, companies seeking angel investment to make a filing with the Securities and Exchange Commission, which would have 120 days to review it. This would both raise the cost of seeking angels and delay the ability of companies to benefit from their funding.   
  
  
The negative impact of the SEC filing requirement would be aggravated by the proposed doubling of the net worth or income thresholds required for investors to be “accredited.” 



It’s hard to overstate how important a favorable regulatory climate is to the success of startups. Some of the most important startups have been founded by 20‐​somethings without the resources to hire lawyers or navigate regulatory bureaucracies. And startups frequently find themselves within weeks of insolvency before they have a big breakthrough. Having a crucial round of funding delayed by four months can be the difference between success and failure. If this description of the bill is accurate (and I have no reason to doubt that it is), this provision would be very bad for the future of high‐​tech innovation in the United States.
"
"

My coffee buddy, Butte County Sheriff Perry Reniff helps Alexis Dominguez exit the helicopter (Photo: Bill Husa, Chico Enterprise Record)
 Today was a good day. No, strike that, today was a GREAT day!
The saga of the Dominguez family lost in the snow looking for a Christmas tree hit home with me in a big way, because I had people from all over asking me what the weather was going to do to the search and rescue effort. I was the bearer of bad news, which I hated, because the winter storm bearing down made survival even less likely.
(Note: for national/international readers of this blog, this story unfolded in my home city and county)
Mountain weather is unforgiving. Fortunately, they knew what to do. They improvised a snow cave, wrote “HELP” in the snow, and stayed put until rescuers could find them. When they did, the relief was nation-wide.
Yes, its the best Christmas present anybody could ever have.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea213c461',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the May/​June 1983 issue of _Regulation_ magazine, the economist Bruce Yandle set forth a new theory of government intervention. At the time Yandle was executive director of the Federal Trade Commission, and, as an economist, he had recently become interested in the demand for and supply of social regulations. Where exactly do these regulations come from?



As he read about historical efforts to control alcohol by banning Sunday sales, Yandle found the answer in an unlikely coalition. Churchgoing teetotalers endorsed the prohibition on moral grounds, while bootleggers supported the restrictions in order to limit competition. As Yandle discovered more and more examples of this alliance — from environmental policy to interstate trucking — he concluded that “durable social regulation evolves when it is demanded by both of two distinctly different groups.” He referred to these groups as “Bootleggers,” who have an economic interest in the regulation, and “Baptists,” who have a moral argument.



In a new book called _Bootleggers & Baptists_, Yandle — along with Adam Smith, director of the Center for Free Market Studies at Johnson and Wales — revisits an old theory with new perspective. The book explores a political dynamic connecting interest groups who, for very different reasons, spend time and resources seeking government favors. “At its very root, it is a story about the demand for and supply of politically provided pork,” Smith and Yandle write.



The authors begin by surveying the explosive growth in federal regulation during the 1970s and 1980s — when the theory was germinating — presenting an array of Bootlegger and Baptist stories stretching from Magna Carta to today’s energy industry. This thick layer of rules is at the center of the narrative. “When the pace of regulation accelerates,” they write, “Bootleggers and Baptists are sure to barbecue while the political fire pits are hot.”



Smith and Yandle then spend time deconstructing the two groups. They show how the ultimate results of noble‐​minded efforts to effect change in the public interest prove to be neither noble nor in the public interest. Instead, a publicspirited group wraps a selfinterested lobbying effort in a cloak of respectability. As a result, “once Bootleggers and Baptists are locked into a successful coalition, their structural incentives change, making political wealth extraction more attractive than private wealth generation — to society’s detriment.”



Consider, for instance, Obamacare. The implementation of health care reform provided some amount of access to a larger share of the population. Yet, it also cartelized 17 percent of the U.S. economy, guaranteeing an expanded market for the country’s biggest political players. “In each of these efforts, a vast Baptist choir sang the praises of government‐​assisted health care,” the authors write. “But lurking in the background — and sometimes in the back row of the choir — were pharmaceutical, insurance, and other health care Bootleggers ready to expand sales to the regulated sector.” As Smith and Yandle detail in full, the story behind the rise of Obamacare is more complex than it seems — and much more interesting.



In one example after another — from the Troubled Asset Relief Program to climate change — the two economists reveal the interaction of lofty values with narrow selfinterest. In short, politicians who deliver pork to Bootleggers can justify their actions by appealing to higher “Baptist” morality. This phenomenon is driven by forces deeply rooted in our DNA, they explain. What, then, is the endgame? Is this marriage of strange bedfellows here to stay? “We are all at least a little bit Bootlegger, a little bit Baptist,” Smith and Yandle conclude, “which means as long as we remain human, the story of Bootleggers and Baptists will continue.”
"
"

The main political conflict in recent years is between experts or elites and non‐​experts. For lack of a better word, the non‐​experts are called populists. Their complaints have been specific: Elites and experts are arrogant, they have different values, they condescend in annoying ways, they ignore the sometimes legitimate concerns of populists, among others. Experts say that they should be listened to because they’re more knowledgeable. We see it in debates on every issue from climate change to trade, immigration, and everything in between.



The COVID-19 pandemic exposes another criticism of experts: They lie with noble intentions. And the consequences of those noble lies are quite negative.



A recent _New York Times_ __ op‐​ed by Zeynep Tufekci exposes the danger of noble lies when it comes to limiting the transmission of COVID-19. She details the claims by public officials and health experts that masks don’t limit transmission. She wrote:



Many health experts, no doubt motivated by the sensible and urgent aim of preserving the remaining masks for health care workers, started telling people that they didn’t need masks or that they wouldn’t know how to wear them.



Those claims were simply untrue. Yes, healthcare workers need masks, but masks (even those that are homemade) also reduce transmission outside of hospitals and clinics. Sick people who wear masks reduce their likelihood of transmitting the virus and healthy people who wear them reduce their likelihood of becoming infected. Tufecki pointed out the obvious contradiction: If masks don’t work, why do healthcare workers need them?



Noble lies are those knowingly propagated by elites or experts to advance a bigger agenda. I can’t think of a single noble lie that has led to better outcomes and most have done more harm than good. The arguments against mass use of face masks were noble lies intended for the good reason of attempting to reduce the mass consumption of face masks to conserve them for healthcare workers. However, they backfired quickly. Ultimately, that failure will cause even more harm down the line.



One source of harm is how social enforcers of new anti‐​COVID‐​19 norms respond. Enforcing these norms through pressure to not gather in large crowds, proper hand hygiene, to maintain social distance, and to stop shaking hands is positive. Those social enforcement mechanisms work best when everybody is basically on the same page about what works but they follow the norms to varying degrees. But if lots of people don’t trust the advice and they disagree about proper methods to limit the transmission of the disease because they’ve been misled by noble lies, social pressure will be contradictory and less effective at altering behavior.



Health experts, epidemiologists, medical researchers, scientists, and other experts have knowledge and experience that is valuable in containing COVID-19 and eventually wiping it out. They will eventually discover a vaccine and treatments that will benefit all of us. But without widespread trust in them, their jobs will be harder. Noble lies will reduce that trust and make it less likely that people will heed their advice and warnings. If some percentage of their guidance is a lie and we all know that they are sometimes lying, people will be less likely to listen or will cherry‐​pick which advice to follow. People will be more likely to consume snake oil, listen to grifters, and fall back on prejudices or other biases that will end up hurting themselves and others. And this will all happen rapidly in the current media market where information is cheap and available at a cost near zero, as it currently is.



Even worse, the noble lie does serious damage to expert culture as one noble lie can justify more lies that are increasingly less noble. Experts will justify less‐​noble lies on the precedence of previous lies that were nobler with no natural limiting principle. And they judge the nobility of the lie by the intent of the liar, which is a dangerous trap. This cycle can only destroy expert credibility.



A common justification for the noble lie is that people aren’t taking the current COVID-19 crisis seriously enough, so experts are justified in trying to “scare people straight” with a lie. The major problem, if the goal is to change other people’s behavior with additional information, is that they won’t be scared straight as soon as the lie is known. Thus, the noble lie will backfire.



Scaring people straight works better when scary truths are revealed rather than when lies are peddled. Emily Oster, economist and author of two superb books on pregnancy risks and raising young children, points out this problem in another area of medicine: alcohol consumption by pregnant mothers. She highlights a report published by the American Academy of Pediatrics with the headline finding that “no amount of alcohol should be considered safe in pregnancy.” Oster points out that the report itself contradicts that statement. She further details to another problem:



Reasonable people can differ, but when we lump together all levels of drinking—without really clearly focusing on what we should be concerned about—we risk losing sight of the groups that actually need help.



Heavy drinking during pregnancy is a big risk, but exaggerating it means that the public can lose sight of the people most negatively affected. Perhaps some pregnant mothers can’t limit themselves to a small amount of alcohol so, for them, the better advice is not to drink at all, but that does not translate into a warning that no expecting mothers should imbibe ever. Exaggeration could mute the actual message: Drinking a lot while pregnant can do serious, permanent harm to your baby.



Experts and elites are more trusted when they tell the truth and expose non‐​obvious tradeoffs. Every action has tradeoffs, even those that are obviously a net‐​benefit. For instance, arguing in favor of lockdowns, quarantines, and travel restrictions while acknowledging that those actions will severely disrupt economic activities and lead to other, different health problems and early deaths. Those extra health problems and deaths may be worth it, but being open about that tradeoff and making the case honestly is the best that experts can do.



This doesn’t mean that experts should consider every non‐​expert objection and weigh them equally when considering a response. Anti‐​vaxxers can be safely ignored during the COVID-19 crisis, for instance. But it does mean that experts need to present the facts honestly and openly. Populists may not believe them, but it’s better to make an honest case for an action that isn’t believed than it is to make a dishonest case that is later exposed as the long‐​term costs in lost credibility are high. The present value of trust in experts is too valuable to be squandered on an ephemeral change in behavior bought at the expense of a lie.



As a libertarian, my preference is for as few government rules and regulations as required to build and maintain a free, peaceful, and prosperous society. In the areas where rules and regulations are necessary, they should be well‐​considered and guided by experts who understand the issue that is being regulated. There should also be consequences for making errors and rewards for being correct. Trust in those experts is fragile in even the best of times, but crucial for widespread popular acceptance which is necessary for the enforcement of any new policy. When some experts commit noble lies, it damages their credibility and limits the extent of their wiser (compared to non‐​experts) recommendations.



Tufekci ended her piece with this prescient warning:



Research shows that during disasters, people can show strikingly altruistic behavior, but interventions by authorities can backfire if they fuel mistrust or treat the public as an adversary rather than people who will step up if treated with respect. Given that even homemade masks may work better than no masks, wearing them might be something to direct people to do while they stay at home more, as we all should.



Experts should commit themselves publicly to always telling the truth and to banish the noble lie from public debate. By limiting the transmission of noble lies, hopefully we can do something to limit the spread of COVID-19.
"
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"

In September 2005, the Danish newspaper _Jyllands‐​Posten_ printed a dozen cartoons — prompted by recent examples of self‐​censorship by the European media — related to Islam, one of which depicted the Muslim prophet Muhammad with a bomb wrapped in his turban. Their publication quickly spiraled into a violent international uproar, as Muslims around the world erupted in protest and the paper’s culture editor was branded by some as “the Danish Satan.” In _The Tyranny of Silence_ , a book published by the Cato Institute in November, Flemming Rose grapples with the difficult issues surrounding his decision to run those cartoons. At Cato’s 27th Annual Benefactor Summit in Naples, Florida, Rose spoke about the lessons he learned in the process of reconciling the tension between respect for cultural diversity and the protection of democratic freedom.



When I was writing my book back in 2009, I interviewed Salman Rushdie and he said something very important to me. I’d been having difficulty coming to terms with the fact that others were telling my story without, I felt, knowing who I was. Rushdie observed that from childhood, we use storytelling as a way of defining and understanding ourselves. It’s a phenomenon that derives from a language instinct that is universal and innate in human nature. It’s in fact one of the things that makes us different from other creatures. Any attempt to restrict that impulse and put limits on speech therefore isn’t just a violation of our political rights. It’s an act of violence against human nature, an existential assault that turns people into something they are not. What differentiates open and closed societies is the right to tell and retell our own and other people’s stories. In a democracy, no one can claim that exclusive right, be it an oppressive state or a minority.



Rushdie told me that the conflict over the right to tell a certain story was at the center of his own controversy. He said: “This goes back to the question of what sort of society we want. If you wish to live in an open society, it follows that people will talk about things in different ways, and some of them will cause offense and anger. From the moment you begin to talk about limiting and controlling certain expressions, you step into a world where freedom no longer reigns, and from that moment on, you are only discussing what level of un‐​freedom you want to accept. You have already accepted the principle of not being free.”



Rushdie’s words came just at the right time for me. They opened my eyes and helped me define my own project. Even though the Muhammad cartoons were conceived in a Danish and European context, the debate is global. It touches on issues fundamental to any kind of society: freedom of speech and of religion, tolerance and intolerance, immigration and integration, Islam and Europe, majorities and minorities, and globalization, to name but a few. And what I realized is that we are all entitled to tell whatever story we wish.



That insight is very fundamental. It goes to the heart of the relationship between the person who speaks and those who hear — between individuals and communities — and to what extent individuals, groups, and institutions have a right to determine speech limits. In the U.S. constitutional system, there is more focus on the speaker, on the individual. You have the right to autonomy. In much of the rest of the world, including the European Union, it’s the other way around. The community — those on the receiving end — have broad powers to determine what an individual is allowed to say.



And this difference in approach has had farreaching consequences for the concept of tolerance. Originally, tolerance implied one’s ability to bear what one couldn’t stand. Freedom of speech meant freedom for the speech we hate — that’s how Justice Oliver Wendell Holmes put it. But because the receiver of speech has so much discretion in many parts of the world, tolerance has been turned on its head. That’s a very dangerous development.



There are two factors that are driving the challenges of free speech in a globalized world. The first is migration: the fact that people are moving across borders in numbers never before seen in human history Every society is getting more and more diverse in terms of culture, ethnicity, and religion, which means that it’s a lot easier to get offended by what people around you say because we are increasingly exposed to different ways of living. How do we negotiate the right to freedom of expression and freedom of speech in this increasingly multicultural world?



The other factor that is driving this process is the digitization of communication technologies. Now when something is being published in one place, it is immediately published everywhere. And when information travels, context is lost. This creates a huge space for misunderstanding, not to mention outright manipulation. That is something I experienced personally during the cartoon crisis.



But migration and digitization also means that all of us are being impacted by what’s going on outside our own country. You have competing approaches to free speech that are beginning to clash. The disappearance of borders and the spread of technology means that there is a need for universal standards no matter where you live. To a certain extent this goes on within the United Nations. But often things seem to be moving in the opposite direction.



More and more countries are passing laws that fragment and undermine any universal standard, a point stressed by Miklós Haraszti, the former representative on freedom of the media for the Organization for Security and Co‐​operation in Europe. Haraszti writes that “the very notion of an international standard for limits on free speech become obsolete if the fragmentation into separate content‐​oriented, historically based, culturally defined, politically shaped, country‐​specific approaches to speech restriction becomes accepted.”



In other words, no international advocacy for free speech is possible without a shared assumption that only incitement of actual crimes should be illegal. Otherwise offensive speech should be countered by speech, not courts. Unfortunately, this fragmentation of the international standard, to a certain extent, started in Europe with the passing of Holocaust denial laws. And one of the big surprises I experienced writing my book was to find out that the vast majority of these laws were in fact passed after the fall of the Berlin Wall. This indicated to me that they were not passed right after the Holocaust to prevent incitement to violence, but for other reasons. It’s important to note that the horror of the Holocaust serves as the founding narrative legitimizing European integration, and it is the key motivation for hate‐​speech laws on the continent.



The Council of Europe Commissioner for Human Rights has called for all 47 member states to pass laws against Holocaust denial, based on a widely accepted interpretation of what led to the Holocaust. It says that anti‐ Semitic hate speech was the decisive trigger — that evil words beget evil deeds — and that if only the Weimar government had clamped down on verbal persecution of the Jews in the years prior to Hitler’s rise to power, then the Holocaust may never have happened.



In my research, I looked into what actually happened in the Weimar Republic and found that, contrary to what most people think, Germany did have hate‐​speech laws that were applied quite frequently. The assertion that Nazi propaganda played a significant role in mobilizing anti‐​Jewish sentiment is irrefutable. But to claim that the Holocaust could have been prevented if only anti‐​Semitic speech had been banned has little basis in reality. Leading Nazis, including Joseph Goebbels, Theodor Fritsch, and Julius Streicher, were all prosecuted for anti‐​Semitic speech. And rather than deterring them, the many court cases served as effective pubicrelations machinery for the Nazis, affording them a level of attention that they never would have received in a climate of a free and open debate.



In the decade from 1923 to 1933, the Nazi propaganda magazine _Der Stürmer_ — of which Streicher was the executive publisher — was confiscated or had its editors taken to court no fewer than 36 times. The more charges Streicher faced, the more the admiration of his supporters grew. In fact, the courts became an important platform for Streicher’s campaign against the Jews.



Alan Borovoy, general counsel of the Canadian Civil Liberties Foundation, points out that cases were regularly brought against individuals on account of anti‐​Semitic speech in the years leading up to Hitler’s takeover of power in 1933. “Remarkably, pre‐​Hitler Germany had laws very much like the Canadian anti‐​hate law,” he writes. “Moreover, those laws were enforced with some vigour. During the 15 years before Hitler came to power, there were more than 200 prosecutions based on anti‐​Semitic speech… As subsequent history so painfully testifies, this type of legislation proved ineffectual on the one occasion when there was a real argument for it.”



The same can be said about Yugoslavia. Before the 1990s, Yugoslavia had rather tough laws criminalizing incitement to national, racial, or religious hate. In fact, people were being put in jail for telling an ethic joke. Obviously, these laws did little to help prevent the ethnic violence that we saw in the wars following the disintegration of Yugoslavia. Nevertheless, the dominant view in Europe is that too much freedom of expression will destroy the peace. In that sense, the EU is driven by a vision of what I call a benign utopia, one that aims to eliminate hate and create an insult‐​free public space. This became particularly evident in 2012, when it was awarded the Nobel Peace Prize. In receiving the prize, the leaders of the EU made no reference to the close relationship between freedom and peace. Instead they focused on the EU’s efforts to avoid division and create a continent without conflict.



I believe that Europe would do itself a great service if the narrative about the Holocaust was integrated into a broader anti‐​totalitarian framework. Hate speech wasn’t the trigger for mass murder during World War II. It was the clash between two totalitarian powers in the center of Europe — the Nazi regime and the Soviet regime — that was the primary cause. And if that’s the case, it means that the destruction of Jews in Europe was closely connected to the destruction of freedom. Moving forward, it would mean that the struggle against evil doesn’t require less freedom, but in fact, quite the contrary.



It seems there are two available responses to threats against free speech. One option is, basically, “If you accept my taboos, I’ll accept yours.” If one group wants protection against insult, then all groups should be so protected. If denying the Holocaust or the crimes of communism is against the law, then publishing cartoons depicting the Muslim prophet should also be forbidden. But that option can quickly spiral out of control: before we know it, hardly anything may be said.



The second option is to say that, in a democracy, there is no “right not to be offended.” Since we are all different, the challenge is then to formulate minimum constraints on freedom of speech that will allow us to coexist in peace. A society comprising many different cultures should have greater freedom of expression than a society that is significantly more homogeneous.



That premise seems obvious to me, yet the opposite conviction is widely held, and that is where the tyranny of silence lurks. At present, the tendency in Europe is to deal with increasing diversity by constraining freedom of speech, whereas the United States maintains a long tradition of leading off in the other direction. And it appears that the United States will increasingly stand alone with its tradition of upholding near‐​absolute freedom of expression.



My personal view is that the Americans are right. Freedom and tolerance are, to me, two sides of the same coin, and both are under pressure. As noted earlier, the world is undergoing rapid change. Taking offense has never been easier, or indeed more popular: many have developed sensitivity so exquisite that it has become excessive.



It almost tempts one to ask Europe’s welfare states to spend some money, not on “sensitivity training” — learning what not to say — but on insensitivity training: learning how to tolerate. For if freedom and tolerance are to have a chance of surviving in the new world, we all need to develop thicker skin.
"
"
Share this...FacebookTwitterExpect this new study to be greeted by an angry mob with pitch forks and torches. Results Big Pharma, Bill Gates and social engineering technocrats don’t want to see. 
So picture this: tens of thousands of scientists, doctors and health authorities worldwide spending billions and billions in a frenzied search for new medicines and vaccines – while imposing economy-crippling lock downs – all to combat the COVID 19 virus. Meanwhile, a large part of the solution is likely just sitting right there on the supermarket shelf – to be had for just a few bucks!
Use a mouthwash, stupid!
That may be just the case, believe it or not, according a a recently published study appearing in the British journal FUNCTION titled: “Potential Role of Oral Rinses Targeting the Viral Lipid Envelope in SARS-CoV-2 Infection“.
It may be that simply gargling regularly in fact goes a long way in combating the spread of COVID-19, and letting us do away with the face mask circus we’ve been going through lately.
Promising results

A team of scientists led by Valerie B O’Donnell reviewed known mechanisms of viral lipid membrane disruption by widely available dental mouthwash components that include ethanol, chlorhexidine, cetylpyridinium chloride, hydrogen peroxide, and povidone-iodine and assessed their potential ability to disrupt the SARS-CoV-2 lipid envelope, based on their concentrations.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




 

Figure 1: Breaching the viral envelope. Source: O’Donnell et al 2020. 
Direct evidence: “Potential way to reduce transmission”
The preliminary results are promising enough to warrant serious further investigation, the authors say. Moreover, citing already published research on other enveloped viruses, they conclude that several deserve clinical evaluation.
These studies “directly support the idea that oral rinsing should be considered as a potential way to reduce transmission of SARS-CoV-2,” the authors say.
Mixing science and politics
If this turns out to be so, there are going to be lots of “experts” out there looking awfully silly. But so it is so often when social engineering politics get mixed with science.

Share this...FacebookTwitter "
"

Last year, Chairman of the Joint Chiefs of Staff Gen. Martin Dempsey contended that “we are living in the most dangerous time in my lifetime, right now.” This year, he was more assertive, stating that the world is “more dangerous than it has ever been.”



Is this accurate? At “Dangerous World? Threat Perception and U.S. National Security,” a Cato Institute Conference held in October, experts on international security assessed the supposed dangers to American security, examining the most frequently referenced threats, including wars between nations and civil wars within nations.



Historically, states have posed the greatest threats to international security. The first two panels discussed whether this is still the case, exploring the dangers not only from traditional nation‐​states but also from sub‐​state actors.



“The U.S. government has overreacted to terrorism relative to its direct physical costs,” Max Abrahms, assistant professor at Northeastern University, said. But the policy community has also inflated the risk that terrorism would spread throughout the world. “Just as the direct costs of terrorism have been overstated, so too has the political value.”



With a lack of credible state rivals since the end of the Cold War, fears have arisen in response to less traditional dangers, including cyberwar, climate change, and general instability. Mark G. Stewart, director of the Centre for Infrastructure Performance and Reliability at the University of Newcastle, subjected the worst‐​case scenarios of global warming to cost‐​benefit analyses. “My answer is that the impact of climate change on national security is manageable,” he concluded. “Change is going to be gradual—not abrupt—and there will be plenty of time to adapt.”



Given that many of these threats have been inflated, the question remains whether the global order depends on a single power enforcing the rules. In the final panel, scholars considered whether the United States must prevent general lawlessness in order to maintain our relative prosperity. Eugene Gholz, associate professor of political science at the University of Texas at Austin, challenged this thesis by focusing on how costly it is to fight wars.



“The claim that the global economy would become unhinged if the United States was not providing primacy and tamping down conflict around the world is just not true,” he concluded. In the end, many of those scholars that disagreed with the Institute’s positions nevertheless praised its scholarship—even on issues as divisive as foreign policy. “Cato scholars are very strong and clear advocates of the view that the U.S. should retrench,” said Stephen Brooks, associate professor of government at Dartmouth College. “In my view, this comprehensive version of retrenchment … is the one which is most interesting and most compelling as an alternative to the current U.S. grand strategy.”
"
"
Share this...FacebookTwitterHere’s another example illustrating just how volatile and unreliable wind energy really is.
Wind energy proponents like to claim that although turbines installed on land don’t produce so optimally, the ones at sea are wonderful because the wind there is always blowing and so it all kind of evens out.
The chart below shows the output of all wind turbines installed in Germany, both on land and offshore, from the five major German grid operators:

The dark horizontal line denoting 60,000 MW represents the so-called installed total capacity. Readers will note that less than 10% of rated capacity often gets produced. Only rarely does an output of 33% (20 MW) ever get reached.


		jQuery(document).ready(function(){
			jQuery('#dd_47d74c3ef530a413e0a50449d409a405').on('change', function() {
			  jQuery('#amount_47d74c3ef530a413e0a50449d409a405').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The “SnoMote Remote Controlled Weather Station”
At first, I though this must be a joke. But, it is not. They call it “an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.” It started life as the Ski-Doo® RC Snowmobile which is 28″ long, and runs for 30 minutes on a charge.
But in a press release from Georgia Tech on May 27th, seen below, it is clear that this is real, true, fully federally funded NASA science project. You can buy one here from Hammacher Schlemmer for $79.95 Ooops, sold out, looks like Georgia Tech bought them out.
My question is, when one of these gets stuck in a crack or crevasse, or simply runs out of power prematurely, do they just leave it there for the polar bears to play with or do they send the lowliest science intern out on the ice to fetch it back, lest it remain to pollute the sea and/or sea ice with it’s Lead or Nickel Cadmium rechargeable batteries?
UAV’s have already been used in the arctic.

Robots go where scientists fear to tread








SnoMote, an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.
Click here for more information.





ATLANTA ( May 27, 2008 ) — Scientists are diligently working to understand how and why the world’s ice shelves are melting. While most of the data they need (temperatures, wind speed, humidity, radiation) can be obtained by satellite, it isn’t as accurate as good old-fashioned, on-site measurement and static ground-based weather stations don’t allow scientists to collect info from as many locations as they’d like.
And unfortunately, the locations in question are volatile ice sheets, possibly cracking, shifting and filling with water — not exactly a safe environment for scientists.
To help scientists collect the more detailed data they need without risking scientists’ safety, researchers at the Georgia Institute of Technology, working with Pennsylvania State University, have created specially designed robots called SnoMotes to traverse these potentially dangerous ice environments. The SnoMotes work as a team, autonomously collaborating among themselves to cover all the necessary ground to gather assigned scientific measurements. Data gathered by the Snomotes could give scientists a better understanding of the important dynamics that influence the stability of ice sheets.








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





“In order to say with certainty how climate change affects the world’s ice, scientists need accurate data points to validate their climate models,” said Ayanna Howard, lead on the project and an associate professor in the School of Electrical and Computer Engineering at Georgia Tech. “Our goal was to create rovers that could gather more accurate data to help scientists create better climate models. It’s definitely science-driven robotics.”
Howard unveiled the SnoMotes at the IEEE International Conference on Robotics and Automation (ICRA) in Pasadena on May 23. The SnoMotes will also be part of an exhibit at the Chicago Museum of Science and Industry in June. The research was funded by a grant from NASA’s Advanced Information Systems Technology (AIST) Program.
Howard, who previously worked with rovers at NASA’s Jet Propulsion Laboratory, is working with Magnus Egerstedt, an associate professor in the School of Electrical and Computer Engineering, and Derrick Lampkin, an assistant professor in the Department of Geography at Penn State who studies ice sheets and how changes in climate contribute to changes in these large ice masses. Lampkin currently takes ice sheet measurements with satellite data and ground-based weather stations, but would prefer to use the more accurate data possible with the simultaneous ground measurements that efficient rovers can provide.
“The changing mass of Greenland and Antarctica represents the largest unknown in predictions of global sea-level rise over the coming decades. Given the substantial impact these structures can have on future sea levels, improved monitoring of the ice sheet mass balance is of vital concern,” Lampkin said. “We’re developing a scale-adaptable, autonomous, mobile climate monitoring network capable of capturing a range of vital meteorological measurements that will be employed to augment the existing network and capture multi-scale processes under-sampled by current, stationary systems.”








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





The SnoMotes are autonomous robots and are not remote-controlled. They use cameras and sensors to navigate their environment. Though current prototype models don’t include a full range of sensors, the robots will eventually be equipped with all the sensors and instruments needed to take measurements specified by the scientist.
While Howard’s team works on versatile robots with the mobility and Artificial Intelligence (A.I.) skills to complete missions, Lampkin’s team will be creating a sensor package for later versions of Howard’s rovers.
Here’s how the SnoMotes will work when they’re ready for their glacial missions: The scientist will select a location for investigation and decide on a safe “base camp” from which to release the SnoMotes. The SnoMotes will then be programmed with their assigned coverage area and requested measurements. The researcher will monitor the SnoMotes’ progress and even reassign locations and data collection remotely from the camp as necessary.
When Howard’s research team first set out to build a rover designed to capture environmental data from the field, it took a few tries to come up with an effectively hearty design. The group’s first rover was delicate and ineffective. But after an initial failure, they decided to move on to something designed for consistent abuse — a toy. Instead of building yet another expensive prototype, Howard instead opted to start with a sturdy kit snowmobile, already primed for snow conditions and designed for heavy use by a child.
Howard’s group then installed a camera and all necessary computing and sensor equipment inside the 2-foot-long, 1-foot-wide snowmobile. The result was a sturdy but inexpensive rover.
By using existing kits and adding a few extras like sensors, circuits, A.I. and a camera, the team was able to create an expendable rover that wouldn’t break a research team’s bank if it were lost during an experiment, Howard said. Similar rovers under development at other universities are much more expensive, and the cost of sending several units to canvas an area would likely be cost-prohibitive for most researchers, she added.
The first phase of the project is focused primarily on testing the mobility and communications capabilities of the SnoMote rovers. Later versions of the rovers will include a more developed sensor package and larger rovers.
The team has created three working SnoMote models so far, but as many SnoMotes as necessary can work together on a mission, Howard said.
The SnoMote represents two key innovations in rovers: a new method of location and work allocation communication between robots and maneuvering in ice conditions.
Once placed on site, the robots place themselves at strategic locations to make sure all the assigned ground is covered. Howard and her team are testing two different methods that allow the robots to decide amongst themselves which positions they will take to get all the necessary measurements.
The first is an “auction” system that lets the robots “bid” on a desired location, based on their proximity to the location (as they move) and how well their instruments are working or whether they have the necessary instrument (one may have a damaged wind sensor or another may have low battery power).
The second method is more mathematical, fixing the robots to certain positions in a net of sorts that is then stretched to fit the targeted location. Magnus Egerstedt is working with Howard on this work allocation method.
In addition to location assignments, another key innovation of the SnoMote is its ability to find its way in snow conditions. While most rovers can use rocks or other landmarks to guide their movement, snow conditions present an added challenge by restricting topography and color (everything is white) from its guidance systems.
For snow conditions, one of Howard’s students discovered that the lines formed by snow banks could serve as markers to help the SnoMote track distance traveled, speed and direction. The SnoMote could also navigate via GPS if snow bank visuals aren’t available.
While the SnoMotes are expected to pass their first real field test in Alaska next month, a heartier, more cold-resistant version will be needed for the Antarctic and other well below zero climates, Howard said. These new rovers would include a heater to keep circuitry warm enough to function and sturdy plastic exterior that wouldn’t become brittle in extreme cold.
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f135bf6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On May 28, Ali Larijani, former nuclear negotiator and close confidant of Iran’s Supreme Leader Ali Khamene’i, won the position of speaker of the Majlis, Iran’s parliament. Larijani is a member of the mainline conservative faction in Iran — which is different from the more radical faction led by President Mahmoud Ahmadinejad. (Iranian political observers have aptly borrowed the American term “neoconservative” to refer to the Ahmadinejad faction.)



Larijani’s rise was the first of a series of political changes in Iran. At about this time next year, Iran will hold a presidential election. Its outcome could depend, in part, on the outcome of the 2008 elections here in the United States. Given the serious disputes between the two countries and the prospect of another war in the Middle East, Americans — and American presidential candidates — should take a moment to think about how our election could influence Iran’s.





Despite soaring oil prices, Iran’s economy is in shambles. 



(Obligatory “to be sure” qualifier: Iranian elections are by no means free or perfect. Candidates for office in Iran can be approved or barred from running based on the whims of the clerical leadership. Even so, they reflect the direction of the political winds within the country’s controlled political climate.)



In the years preceding the last Iranian presidential election, which produced Ahmadinejad, American neoconservatives repeatedly minimized the differences between members of the Iranian leadership. Michael Rubin, for example, told _National Review_ readers in 2002 that then‐​President Mohammed Khatami was “neither a reformer nor a democrat” but rather “a fraud.” President Bush subsequently slotted Iran into the Axis of Evil and settled in for an indefinite occupation of neighboring Iraq, making little effort to reach out to Khatami’s government and spurning its offer of negotiations. What Rubin, Bush, and others failed to grasp was that, despite Khatami’s faults, things could get worse.



And get worse they did. During his 2005 campaign for the presidency, Ahmadinejad did not emphasize foreign policy, focusing instead on economic populism. Still, a vote for Ahmadinejad, a former member of the Islamic Revolutionary Guard Corps, was widely viewed as a vote for confronting America. And Ahmadinejad wasted no time proving that view right, from his “world without Zionism” conference to his flamboyant defiance of the United States.



In recent months, by contrast, Khatami has been busy making biting reformist speeches throughout Iran. In Tehran in March: “People want freedom … Freedom means people be allowed to question the ruling system and change it without use of force if the establishment doesn’t respond to their demands.” In Gilan in May: “What did the imam [the founder of the Islamic revolution, Ruhollah Khomeini] mean by exporting the revolution? Did he mean that we take up arms, that we blow up places in other nations and we create groups in other countries to carry out sabotage in other countries? The imam was vehemently against this and was confronting it.”



Disparaging Khatami and convincing ourselves that there is no difference between him and Ahmadinejad is foolish and counterproductive. Unless we want war or a nuclear‐​capable Iran, America needs a negotiating partner, and so much the better if he is similar to Khatami — or even Larijani — rather than Ahmadinejad.



Now, mercifully, the economic demagoguery of Ahmadinejad and his compatriots has been shown to be a disaster. Despite soaring oil prices, Iran’s economy is in shambles. But if anything can save the hardliners from the consequence of their own bumbling stewardship of the economy it is a vague sense that war with America looms just over the horizon. As early as January 2006, Ahmadinejad’s economic policies were wreaking havoc, but as one legislator critical of the president pointed out, “as the [foreign] pressure has increased, the safety margins for him to operate have widened.”



And so it may be in the coming presidential elections. In all likelihood, Iran’s neoconservatives will blame their economic woes — in the mold of Fidel Castro — on the American Colossus, which stands athwart Iranian development.



But there are even bigger prizes than the presidency in Iran. The aptly named position of supreme leader — the ultimate center of power in Iran — eventually will be at stake, and probably sooner rather than later. The current supreme leader, Ali Khamene’i, has been in office since 1989 and will be 69 years old this summer. Speculation about his health has been a parlor game in both Tehran and Washington.



While there are a number of potential candidates to replace him, one, Mohammed Taqi Mesbah‐​Yazdi, is the clerical equivalent of Ahmadinejad and a close spiritual adviser to the firebrand president. Should the international environment remain as poisonous as it is today, it is possible to envision the Assembly of Experts (a body of clerics that selects the Supreme Leader) selecting a hardliner such as Mr. Mesbah‐​Yazdi as Supreme Leader, which would be a hugely negative development in terms of U.S.-Iran relations.



Which man Americans select as their president will likely have a meaningful effect on U.S. foreign policy and on U.S.-Iran relations particularly. He could also have an effect on the nature of the next generation of Iran’s political leadership. One U.S. candidate has sung a song on the campaign trail about bombing Iran, and the other has called for lowering the temperature and making a forthright effort to negotiate.



There is no immutable law of politics that says moderation on one side will lead to moderation on the other. At the same time, it is difficult to see how electing a man wedded to the most wild‐​eyed neoconservative vision of foreign policy would cause Iranians to select more temperate leaders.



Either way, Americans’ choice could influence the nature of the next generation of Iran’s leaders — and with it, the contours of U.S.-Iran relations for decades. With so many Americans ruing the war in Iraq, they would be well‐​advised to consider the prospect of war in Iran, and what they can do to influence whether or not such a war comes to pass.
"
"
A guest post by David Smith


Recently I completed my tenth survey for Surfacestations.org. These surveys  are fun, almost like treasure hunts where the clues are good but not always  great, thus requiring some ingenuity. Also, the surveyor gets to see areas which  may otherwise never be visited. And, they’re for a good cause.
While I found no “poster-child” poor quality sites I did observe an array of  siting problems. Some thermometers were near the drip-lines of trees, some next  to buildings, one was near a concrete patio, one at a sewage plant, several sat  above poorly-drained soil and so forth.
These conditions are less than ideal, obviously. Perhaps more importantly,  these conditions can change over time. Trees and shrubs grow and die, ground  cover changes, concrete is added (and tends to darken over time), drainage may  improve or deteriorate, fences and other construction are added or removed, and  so forth. Each of these can subtly change the local temperature, a situation  which is especially important if one is looking for changes of a fraction of a  degree.
To what extent do these imperfections affect local temperature?  Well, we  really don’t know (or if anyone knows they’re not talking!).
So, to make a small and imperfect step in that direction, I’m running a few  local experiments. My goal is to examine, at least qualitatively, how local  microclimate factors like trees and concrete affect temperature. As you’ll see,  my methods are too crude to allow fractions of a degree determinations but I  should be able to quantify the magnitudes of the impacts of trees, concrete,  etc. Or at least that is my goal.
First, my instruments:

I’m using several temperature detector/recorders (”USB1″) like the gray  object shown in the photo. These electronic devices measure and log  the temperature to the nearest degree F and allow sampling on various schedules.  I use 30-minute sampling.
Note: Interested readers can buy these at:
http://www.weathershop.com/USB1_temperature_logger.htm
At this point I’m testing the hardware and developing my experimental  plan. But, I have made a few (literally) backyard tests and I’d like to share  one of those. This is to help illustrate the approach and, I hope, stimulate  helpful comments from other readers.
This initial run (sort of a beta test) was made in my backyard. It involved  two extremes. One is near my garage, above a dark-soil flower bed and landscape  bricks. This is near a wooden deck and walkway gravel. This spot gets direct  sunlight about 50% of the day.
The second extreme is deep shade, beneath low-tree (crepe myrtle) cover and  above thick, semi-tropical shrubbery.This is about twenty feet from sunlight. A  photo of the backyard is below, with red boxes marking the two locations:

I also use the temperature readings from an airport/airbase located four  miles west of my house. This airport provides professional-grade open-field  temperature readings which should reasonably approximate regional ambient  conditions.
A representative backyard temperature time series is below:

This shows pretty good agreement between the deep-shade max/min and the local  airport open-field max/min, which frankly surprised me. I’d expected the  deep-shade readings to show less variability (lower highs and higher lows).
More importantly is the contrast between #1 (sunlight and plant beds) and #2  (deep shade). The #1 spot stayed 5 to 10F hotter at midday than #2 (deep  shade) less than 50 feet away (and, as a matter of fact, #1 was 5 to 10 F warmer  than the high-quality nearby airport).
Why does this matter? well, suppose a co-op station had slowly drifted, over  several decades, from open-field conditions to those found at site #1. What  would that do to the apparent trend?  That’s an important question which is at  the heart of the surfacestation effort.
This backyard demonstration involved convoluted conditions. There is little  chance to untangle the relative contributions of so many variables (bricks,  soil, tomato plants, trees, etc). So, my plan is to reduce the number of  variables in the tests such that we might be able to make broad conclusions  about the relative impacts of trees, concrete, drainage and other factors which  may change over time.
This should be fun! Suggestions welcome.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fa983f4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe equatorial Pacific is now showing a distinct band of cool surface water developing. Many forecasters have already written the obituary for the now departing El Nino, which pushed global surface temperatures up by some 0.8°C over the last few months.
Developing La Nina
Source: unisys.com
The newly developing La Nina will probably erase much of that in the months ahead. For example, Accu Weather meteorologist Joe Bastardi predicts cooling over the next year or two. But he does say this year’s Arctic ice melt may challenge the record low set in 2007. One more but – he says that after that the ice will recover over the next 2 years and reach normal levels. So all you obsessed global warmists out there, milk it while you can. It’s your last chance (if Joe’s right).
Read the latest Wrap-Up from the Australian Bureau of Meteorology.

Share this...FacebookTwitter "
"



Earlier this week, the _New York Post_ published articles containing information about alleged emails between Hunter Biden, the son of Democratic presidential nominee Joe Biden, and employees at Chinese and Ukrainian energy firms. Twitter and Facebook both took steps to limit the spread of the articles, prompting accusations of “election interference.” Prominent Republican lawmakers took to social media to condemn Twitter’s and Facebook’s decisions. These accusations and condemnations reveal a misunderstanding of policy that could result in dramatic changes to online speech.



According to Twitter, the company restricted access to the _New York Post_ ’s articles because it violated the company’s policies against spreading personal and private information (such as email addresses and phone numbers) and hacked materials. Twitter cited the same policy when it prohibited users from sharing 269GB of leaked police files. Twitter users who click on links to the two _Post_ articles face a click‐​though “this link may be unsafe” warning. The articles in question include such information in images of the leaked emails. Those accusing Twitter of a double standard because the company allows users to share the recent _New York Times_ article based on the president’s leaked tax documents neglect the fact that the _New York Times_ did not publish images of the documents. Although consistent with Twitter’s policies, the decision to block the spread of the _Post_ ’s articles on Twitter absent an explanation or context was criticized by Twitter CEO Jack Dorsey.



According to a Facebook spokesperson, Facebook’s decision to restrict the spread of the _Post_ ’s Hunter Biden articles is “part of [Facebook’s] standard process to reduce the spread of misinformation.” Compared to Twitter’s response, Facebook’s was less clear.



Whatever one thinks about Twitter’s and Facebook’s decisions in this case the decisions were legal and consistent with Section 230 of the Communications Decency Act. Much of the online commentary surrounding restrictions on the _New York Post_ (head over to #Section230 on Twitter to take a look for yourself) makes reference to a non‐​existent “publisher” v. “platform” distinction in the law.



In brief, Section 230 states that interactive computer services (such as Twitter, the _New York Time_ ’s comments section, Amazon, etc.) cannot — with some very limited exceptions — be considered the publisher of the vast majority of third‐​party content. Twitter is not the publisher of your tweets, but it is the publisher of its own content, such as the warning that appears when users click on the two _New York Post_ article links. Section 230 applies to “platforms” and “publishers,” and does not prevent social media sites from fact‐​checking, removing, or limiting access to links.



Some “Big Tech” critics decided not to focus on Section 230 and instead focus on election interference. The conservative outlet The Federalist issued a statement making this claim, as did many others. According to those making the “election interference” claim, the _New York Post_ articles are embarrassing to Joe Biden, and Twitter’s and Facebook’s actions constitute a pro‐​Biden interference in the 2020 presidential election. Conservative pundits are not the only ones making this kind of claim. Senator Joshua Hawley (R-MO) wrote to Dorsey asking him to appear at a hearing titled “Digital Platforms and Election Interference.” Sen. Ted Cruz (R-TX)  wrote to Dorsey accusing Twitter of trying to influence the upcoming election. Later he accused Twitter of election interference and supported the Senate Judiciary Committee issuing a subpoena to Dorsey, which is expected to happen this coming Tuesday.  
  
It is one thing for conservative pundits to accuse a private company of interfering in an election. In today’s political climate it is expected. What should send chills down the spine of everyone who values the freedom of speech and the freedom of association is the sight of two of the most powerful politicians in the country making the same accusation and insisting that Twitter’s CEO appear before a hearing and hand over documents related to how Twitter conducts its business.  
  
To portray how Twitter and Facebook handled the _New York Post_ articles as “election interference” has significant implications. Twitter and Facebook limited access to an article that is potentially embarrassing to a political candidate. If such actions can be considered “election interference,” should every content moderation action by a private company taken against any politician or candidate be considered interference? If _The Wall Street Journal_ rejects an op‐​ed written by the Green Party’s presidential candidate is not that also “election interference”? When a music hall owner decides to allow the Trump campaign, but not the Biden campaign, to host a rally is that not “election interference”?  
  
“Election interference” is a term that ought to mean something useful. Unfortunately, conservative commentators seem intent on warping the term so that it means little more than, “moderating content.”



So‐​called “Big Tech” and content moderation will continue to make headlines next year regardless of who wins the presidential election next month. While conservative commentators and activists are convinced that “Big Tech” is engaged in an anti‐​conservative crusade, they should consider that the political left has its own complaints. Bipartisan anger towards Big Tech could result in Section 230 reform or other legislation that puts the freedom of speech and freedom of association at risk. As lawmakers continue to criticize the most prominent social media companies we should remember that attempts to regulate online speech could have disastrous consequences.
"
"
Share this...FacebookTwitterThe German Readers Edition reports that 3 leading scientists, among them alarmist Stefan Rahmstorf, are calling on Rajendra Pachauri to step down as Chairman of the IPCC because of management errors and the recent attacks on the IPCC and climate science.According to Stefan Rahmstorf’s blog Klimalounge:
I’m not calling for an end of Pachauri, but I could certainly imagine a better Chairman because in my view, among other reasons, he reacted in an unfortunate manner with respect to the media attacks on the IPCC. The role of the Chairman is not to decide the contents of the report (he should not get involved with our work). Rather he ought to well represent the IPCC externally.
Calls for Pachauri’s resignation are nothing new. In February director emeritus of the Max Planck Institute for Meteorology in Hamburg, Prof. Hartmut Graßl, told the Frankfurter Rundschau newspaper that Pachauri should clear the table and leave the job in other hands.
Hans von Storch, director of the GKSS coastal research center in Geesthacht, Germany, said the IPCC director was a burden because he permitted sloppiness in the reviews and checks of the 2007 climate report.
Readers Edition quotes the current issue of zeo2 titled: The zeo2 Climate Summit, which states:
“Pachauri should throw in the towel.”
 
Share this...FacebookTwitter "
"

Donald Trump’s cabinet of generals and billionaires looks poised to gain another controversial member: Rex Tillerson. The CEO of ExxonMobil is now the secretary of state nominee. His nomination raises several red flags, but the central one is: conflicts of interest. In that, he’s a perfect match for the Trump administration.



Tillerson faces a difficult confirmation process for a variety of reasons, including congressional Democrats’ determination to grill him on climate change issues. Yet some of the key issues cited by the media are actually less problematic than they may seem.



For one thing, while he has no governmental foreign policy experience, Tillerson has spent years running a multinational corporation so vast that various observers have likened it to a privately owned state. ExxonMobil, the world’s eighth‐​largest company, operates on six continents, and as CEO, Tillerson has successfully negotiated a number of complex and high‐​profile international deals.





Donald Trump’s secretary of state pick faces a dizzying level of conflicts of interest. These could pit national security against his personal gain.



While competitor BP was being pushed out of its Russian holdings by the government, for example, he struck an effective compromise with the energy giant Rosneft, allowing ExxonMobil to maintain its presence. Likewise, Tillerson’s effective negotiation with Venezuela meant that the company was one of the few to receive compensation — a whopping $900m — for Hugo Chavez’s expropriation of its assets.



Nor is it clear that Tillerson really is — as Senator Marco Rubio and others have argued — a close friend of Vladimir Putin. The Russian Order of Friendship, which Tillerson received in 2013, has been given to numerous Americans in recent years, and contacts with senior Russian officials are an unfortunate but necessary part of doing business in that corrupt country.



Indeed, in other circumstances, his strong working relationship with the Russian government could be highly beneficial, particularly after eight years of a strained personal relationship between US and Russian leaders.



Unfortunately, the nomination raises other problems. For one thing, it’s not actually clear what Tillerson’s views on foreign policy are. Despite concerns over other candidates for secretary of state — such as John Bolton and Mitt Romney — at least it was clear what kind of foreign policy approach they favored. We don’t know if Tillerson is hawkish or not, nor do we know his views on US alliances or international institutions.



The profit motive underlying Tillerson’s previous experience is also fundamentally different from national security interests. Tillerson may have good relations with some world leaders, but given the nature of the oil industry, many of these are poor, corrupt and authoritarian states; ExxonMobil’s recent agreements include such paragons of corruption as Russia, Guyana, Angola and Nigeria.



As a result, the relationships that Tillerson built during his time at ExxonMobil have the potential to be a double‐​edged sword. How long will his good relations with many of these leaders last when he is required to criticize their human rights records as secretary of state, or to oppose them in negotiations on security issues?



By far the biggest problem with Tillerson’s nomination is the serious conflict of interest created by his long history with ExxonMobil. Even if he cuts all direct ties to the company, he will still have a strong financial interest in its success. According to one estimate, he will still hold stock totaling around $218m, and a pension plan worth $70m. And there are many areas where Exxon’s profit motive conflicts with the US national interest.



The most obvious of these is undoubtedly US sanctions on Russia. ExxonMobil has already lost as much as $1bn from sanctions, and Tillerson has been a frequent visitor to the White House and the Department of the Treasury to lobby against them. But there are plenty of other potential clashes: a recent Exxon deal in Mexico would seem to conflict with Trump’s confrontational approach to that country, and the company last year directed a lobbying firm to monitor US energy‐​related sanctions on Iran for opportunities.



In these and many other cases, Tillerson’s personal financial interests will conflict with his duties as secretary of state. He’s certainly not alone in Trump’s cabinet in having such conflicts: the family of Elaine Chao, nominee for secretary of transportation, owns a shipping firm, and the president‐​elect has come under increasing scrutiny for his refusal to divest himself of involvement in his business empire.



Yet while Tillerson may be an excellent negotiator and a good fit for Trump’s nontraditional cabinet of businessmen and generals, these conflicts of interest call into question whether he can fulfill its duties effectively. The most important question remains unanswered: whose interests will he serve as secretary of state?
"
"

More indicators of a colder than normal winter continuing in the northern hemisphere.
From the London Telegraph:
Britain is enduring its most miserable Easter for 25 years as Arctic winds sweep in, bringing snow, hail and sleet.
Easter Sunday temperatures could drop to as low as -3C at night with a band of snow and sleet forecast to move down from the North. The bad weather is most likely to affect the Midlands but snow could even reach London, forecasters said.
From the Sofia news agency:
Bulgaria Meets Vernal Equinox With Snow, Sun Gleams
From This is London:
It’s Bad Friday: Britain braced for worst Easter weather in 25 years as country is battered by gales and sleet.
From the Stars and Stripes:
Snow hits Germany military bases with more possible for Easter.
From CTV.ca
‘Spring’ weather nasty for Eastern Canada
Also from CTV.ca
Six more weeks of winter, top weatherman forecasts
From KDKA-TV:
Snow Advisory In Effect For Parts Of Western Pa
From RedOrbit:
Nebraskans and Iowans heading east for the Easter weekend were experiencing flight delays or snow-covered roads today, and the troubles could continue into Saturday.
From the Detroit Free Press:
Heavy snow across Michigan and points west meant increasing cancellations and delays at Metro Airport today, with things getting worse as snow piled up.
From swissinfo.ch
The Easter break has started with heavy snowfall and strong winds in Switzerland, causing some disruption to traffic.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea08f2db6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Please click the picture then continue reading.
This is the city. Los Angeles, California. I study weather stations here. I carry a thermometer. My name’s Anthony. The story you are about to see is true; the names have been changed to protect the innocent.
The day was Monday, March 24th, four days after the vernal equinox. It started out like any other day, with a bad cup of coffee and a stack of reports on scumbags you normally wouldn’t give the time of day to. But then, just as I was about to down that last gulp of coffee, a tip came in on the email hotline. It was Goetz, and his side kick Foutch.  They said there has been a heist of a weather station on the southeast side. It had been moved, and then it was dumped mysteriously on the campus of USC.
9:15AM Goetz and Foutch told me they had picked up the trail of the weather station the night before. They knew it had been bagged, and that some g-men were hopping mad about it. The g-men had written a report on the crime. In it, they claimed that because of the heist, which had been orchestrated by some other g-men at NOAA, the great City of Los Angeles had been denied it’s due: A new rainfall record year of 2004-2005. Worse than that, the temperature of the city was going down.
I’d heard about this station. It was ugly, it was dirty, it was perched on a rooftop, and it was on the wrong side of town, out by the City Department of Water and Power, just south of the Santa Ana freeway. It hung out with utility trucks and those little red street racers the punks around here drive. There was only one single photo of it. It wasn’t the kind of pristine weather station you’d take home to introduce to your mother.
10:05 AM I knew this was going to be a tough case to crack without hard as nails proof, so I decided to setup surveillance. I called in a favor from a chopper pilot named Barney that I used to share a beat with. I asked him to get aerial photos, lots of them. He asked why. I told him it was because nobody would believe that a City of Los Angeles official weather station had been on a rooftop of a parking garage and now was a shell of it’s former self sitting over at the USC campus.
I told him that when they dumped it in a cool park at USC, they killed the heart and soul of the city’s temperature record with it. And worse, they not only moved the station, but they replaced the man who had sweated and toiled on the rooftop in the hot LA smog and sun to get that weather data with one of those sissy robot contraptions. They call it an ASOS, and it has a sleek look about it, but it could never do a man’s job.
12:01 PM So Barney sets me up with the aerial surveillance from this morning. He sends the photos. I took them down to the lunch counter of the corner drugstore to develop them on my laptop. I had a cup of coffee while I did that. It cost 25 cents, and included pie.
The first aerial photo was a little fuzzy, it was hard to make out the station:

Click for a live link
But I found it, and marked it with an arrow. It wasn’t a pretty sight, right in the middle of acres of blacktop and automobiles. I kept reminding myself I’d seen worse, like in Tucson, and down the street from that Ace hardware store parking lot in Lampasas, Texas. But still, it ate at me.
12:15 PM I finished the pie, and asked for refill on the coffee. The waitress looked at the first photo and just shook her head. Barney had made several passes from several angles, and he snapped one good photo of it that hit me between the eyes like the butt end of a .38 special. There it was, our beloved City of Angels Weather Station. It made me sick just to look at it. What kind of people would do something like this?

Click for larger image
But that wasn’t all, Barney got a picture from another angle out of the archives, and it showed the station even closer looking east. It was even uglier than the other photo. Just thinking about the albedo of the parking lot in the hot LA summer made my skin crawl.

1:05 PM Barney said he had other photos, but he couldn’t get them to me now. So he put them in a file, on something called a web server. And gave me something called a link. He said any citizen of our fair city who wanted to see the terrible place where they put the City of Angels weather station could click the link and look at the photos from all angles. Good man that Barney.
The photos were good, but not good enough. I knew that these photos would eventually be seen by Judge Rabett. Rabett has told us before that pictures don’t matter in his court, so I knew this wouldn’t be enough. I had to prove the connection to the single photo taken by the g-men for their report.
2:15 PM I had figured out a way to show that the single picture taken by the g-men in their report matched the aerial photos Barney took. To do that, I used the photo lab. The guy there is named Gimp, he walks with a limp from an old command line of fire injury. But he does good work. With Gimp’s help I was able to match the camera angle of the single land photo taken by the g-men with one of the aerial photos:

Click for a larger image
3:03 PM I’d finished up the aerial surveillance work of the original scene of the crime, but I still had to get photos of the place where the body of the weather station had been dumped in the park. All I had to go on was the single photo of the park taken by the g-men for their report. It sure looked like a nice cool park and final resting place. It had a little wrought iron fence around it and reminded me of a cemetery – a cemetery where the weather goes to die. It looked good, too good. I had a hunch it wasn’t as good as it looked.
3:05 PM I called up Barney, and asked about the aerial photos where they dumped the weather station; he said he had it covered. He said to check the file he left on the webserver for the street address where the park was.
3:15 PM I was running out of time, I had to get this wrapped up today. The webserver was slow, some punks were using it for a joyride. But I finally managed to open the file. and get the street address. It was out on South Vermont Avenue.
3:30 PM The aerial photos of the campus of USC where they had dumped the weather station proved my hunch was right. The picture the g-men took made it look like a perfect little park-like setting but in reality, it was just another cruddy location surrounded by acres of concrete and asphalt. The place where they dumped the station was only a few yards from the street:

Click for a live link

Source: https://www.bing.com/maps?v=2&cp=pp3hv95484k5&style=o&lvl=2&tilt=-90&dir=0&alt=-1000&scene=6986505&encType=1
The little bit of grass and the fact that it was closer to the beach made it a little cooler. The tennis courts probably didn’t help either.
Barney also left links for the close up aerial surveillance photos he’d done. When I pulled up the one looking West, it hit me. I knew why they had dumped the weather station there. There was a parking garage just across the street. It must have felt like home.

click for a larger image

4:00 PM It was getting late, I had figured out where the original crime had occurred, and where they dumped the body of the weather station. Now all I had to do was find it’s data and I was ready to close this case.
4:15 PM I found the data in a webserver called GISTEMP. Somebody had already plotted it. Sure enough, there it was, the smoking gun. The temperature had dropped about 1.5°C when they pulled this caper in 1999. The continuity of the record had been ruined and there was now a big step function in the data that hadn’t been removed by the g-men at NCDC.
No wonder the g-men who wrote the original report were so hopping mad about it.
Since I couldn’t undo the plot, I called in Gimp again. With his help I was able to separate the time-line into red and blue segments to show where in the time-line the data had been taken from:

Click to see original graph.
5:00 PM Quitting time. I had wrapped up this investigation into the sordid story of crime against temperature in the City of Angels and gotten all the documentation together to present for the court of public opinion. I’m feeling good, I’ve served the public interest. Thats’ my job. I think I’m going to go blow another quarter on pie and coffee.
9:30 AM Tuesday Foutch reports that he’s located the entire history of the station, which can be viewed here:
http://mrcc.sws.uiuc.edu/FORTS/histories/CA_Los_Angeles_Conner.pdf
The story you have just seen is true; the names were changed to protect the incompetent.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea071e2ea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’m travelling the next few days, moderation will be spotty. After tomorrow, you may find that comments may go for 12 -18 hours or more without approval.
In the meantime, you can debunk my own “hockey stick”.

click for larger image
Thanks to all who have come by and participated with comments and ideas!
Best Regards,
Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea09b8f84',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Well, Paul Krugman sure smeared me in his May 29 column (sub. req’d.) where he accused me of “fraud pure and simple” in congressional testimony eight (!) years ago.   
  
  
Krugman’s screed was just another salvo in the current global warming charm offensive, coinciding with Al Gore’s screeching movie, demonstrations against Max Mayfield, director of the National Hurricane Center, because he had the audacity to NOT blame last year’s Hurricane Katrina on global warming (which would have been “fraud pure and simple”), and multiple smearings of any climate scientist who dares to speak out against the current hysteria.   
  
  
Krugman was incensed with my July 27, 1998 testimony before the House Committee on Small Business. In it, my purpose was to demonstrate that commonly held assumptions about climate change can be violated in a very few short years.   
  
  
One of those is that greenhouse gas concentrations, mainly carbon dioxide, would continue on a constant exponential growth curve. NASA scientist James Hansen had a model that did just this, published in 1988, and referred to in his June 23, 1988 Senate testimony as a “Business as Usual” (BAU) scenario.   
  
  
BAU generally assumes no significant legislation and no major technological changes. It’s pretty safe to say that this was what happened in the succeeding ten years.   
  
  
He had two other scenarios that were different, one that gradually reduced emissions, and one that stopped the growth of atmospheric carbon dioxide in 2000. But those weren’t germane to my discussion. Somehow, Krugman labelled my not referring to them as “fraud.”   
  
  
The BAU scenario produced a whopping surface temperature rise of 0.45 degrees Celsius in the short period from 1988 through 1997, the last year for which there was annual data published by the United Nations’ Intergovernmental Panel on Climate Change at the time of my testimony. The observed rise was 0.11 degrees.   
  
  
I cited the reasons for this. In fact, the rate of carbon dioxide increase in the atmosphere was quite constant–rather than itself increasing like compound interest–during the period. Ten years later, Hansen published a paper in which he hypothesized that “apparently the rate of uptake by carbon dioxide sinks, either the ocean, or more likely the forests and soils, has increased.” This was not assumed in any of his scenarios. In fact, the general hypothesis has been that, as the planet warms, the ocean takes up carbon dioxide at a slower rate.   
  
  
Then, contrary to everyone’s expectation, the second most‐​important global warming emission, methane, simply stopped increasing. Some years have shown an actual drop in its atmospheric concentration. To this day, no one knows why.   
  
  
There’s also the nagging possibility that we haven’t yet figured out the true “sensitivity” of surface temperature to changes in carbon dioxide. Scientifically, that’s a chilling possibility.   
  
  
On May 30, Roger Pielke, Jr., a highly esteemed researcher at University of Colorado’s Center for Science and Technology Policy Research, examined Hansen’s scenarios. Of the two “lower” ones, he concluded, “Neither is particularly accurate or realistic. _Any conclusion that Hansen’s 1988 prediction got things right, necessarily must conclude that it got things right for the wrong reason_.” (italics in original)   
  
  
That’s precisely the keynote of my testimony eight years ago: in climate science, what you think is obviously true can literally change overnight, like the assumption of continued exponential growth of carbon dioxide, or how the earth responds.
"
"
Share this...FacebookTwitterA “potential connection” between anthropogenic global warming and the frequency or intensity of wildfires in California has yet to emerge in the trend observations.
Scientists have found a “lack of correlation between late summer/autumn wildfires” and “summer precipitation or temperature” in coastal California. In fact, “there is no long-term trend in the number of fires over coastal California” in the last 50 years (Mass and Ovens, 2019). 
Image Source: Mass and Ovens, 2019
Fire history in the Western USA has recently declined to the lowest point in the last 1,400+ years (Marlon et al., 2012).

Image Source: Marlon et al., 2012
As CO2 concentrations have risen from 300 ppm to 400 ppm (1900 to 2007), the decline in global burned area has been significant (Yang et al., 2014).

Image Source: Yang et al., 2014
The falling trends in global-scale wildfires can even be dectected over the short-term 2001-2016 period (Earl and Simmonds, 2018).

Image Source: Earl and Simmons, 2018
Share this...FacebookTwitter "
"

On Sunday I posted about the USHCN climate station of record in Tucumcari, NM highlighting its positive points since it has all the hallmarks of a well sited station with a long and uninterrupted record. But something was odd with the temperature record that didn’t quite make sense at first glance.
I also cross posted my report on Climate Audit since I always value additional input from the community there.
I noted that while this station is in fact well sited, and rates a CRN2, it has some oddities with it’s temperature record around the year 2000, something that looked like a step function to me.

Click for larger graph from NASA GISTEMP
Of course, even though this a truly rural station, 3 miles from the outskirts of town, Hansen and GISS apply adjustments to it anyway, which is part of the flawed “nightlights” algorithm incorrectly flagging this station for adjustment. Even though the adjustment makes the present cooler, it still seems misplaced given the station quality and history. Steve McIntyre said it best:
Here we have a rural station where there doesn’t seem to be any reason to adjust the temperature for population growth/UHI. But in this case, Hansen adjusts Tucumcari as though it were a city. Why is he even adjusting Tucumcari at all? (The “reason” is that its lights value removes it from the rural classification and it goes into the adjustment pool.) While Hansen sometimes seemingly cools rural stations in the past, for the GISS dset2 version here, he warms the past of the station (cools the present).
It’s more that this is a case of another unjustified adjustment by the “adjuster in chief” showing once again that the Hansen adjustments do not do what they are supposed to do – and the best that can be hoped from the Hansen adjustment program by users of this dataset is that the adjustments overall end up being pointless and random, rather than pointless and biased.
The adjustment by GISS looks like this:

Ok adjustments aside, the fact that there is a step at 2000 that remained unexplained until commenters on CA started looking at the data themselves. DaleC provided this graph:

Click for original image
Note the arrow that I place at the year 2000. Notice anything?
The annual average minimum temperature has exceeded 45°F and maintained the rise since 2000. For the first time in the station history going back to 1905, the minimum temperature has gone above that 45°F mark and stayed there. Yes there have been some previous brief excursions above 45°F, but none appear to have lasted more than 2 years. Note that average annual maximum temperatures did not increase during the same period.
What could cause that? We can rule out adjustments, since this is GHCN data before Hansen gets his adjuster mitts on it. We can rule out location change or equipment change, since according to NCDC metadata the station has been in the same place since at least 1946 and possibly longer. It still uses mercury max/min thermometers, so there’s no MMTS next to a building or parking lot to blame.
So what is left? Something around the station in the measurement environment that affects the nighttime readings. I recalled seeing this before. And back in early 2007, I had posted a story about a paper from Dr. John Christy of UAH where he studied a number of stations in the San Joaquin Valley in California because they had exhibited this same symptom:
The culprit? Irrigation. See my story and Christy’s press release
Christy remarks: “Another factor is the dry air, something common to all deserts. Water vapor is a powerful greenhouse gas. Desert air lacks water vapor. The air turns cold at night because it doesn’t retain much warmth from the daytime and it can’t trap what little heat might rise from the ground at night.”
Evaporation from irrigated fields adds water vapor to the air — a process that cools summer days but traps heat rising from the damp soil at night.
“If there is anything I’ve learned in Alabama, it is that humidity can make summer nights very warm,” said Christy, a Fresno, Calif., native who has lived in Alabama since 1987.
Once I mentioned this as a possibility to explain the increase in nighttime temperatures, it didn’t take Steve McIntyre long to find some anecdotal evidence that correlated:

http://cahe.nmsu.edu/news/1997/043097_irrigation_tour.html
A few years ago when cattle prices were high, we saw a tremendous increase in the number of irrigated grass acres in Quay County,” said Jeff Bader, Quay County Extension program director. “One reason was our limited water situation for irrigation, and high cattle prices made it look very attractive.”
When cattle prices dropped again, interest in irrigated pastures declined, but now the cattle market is improving, he said. Producers never really lost concern about irrigated pasture because it fits into the management scheme for water conservation so well in Quay County.
“Irrigated pastures fill a niche in this area because of their ability to produce under varying levels of irrigation,” said Rex Kirksey, superintendent of NMSU’s Agricultural Science Center in Tucumcari. “Pastures remain a viable option in many situations where irrigation water is too limited or unpredictable for corn or alfalfa production.”
What is interesting is that the director of the Ag Science Center, Rex Kirksey, is also the person that took these photos for the station survey. There’s quite a large water project in the area, called unsurprisingly, the Tucumcari Project.







Conchas Dam and Lake




Apparently they have quite a problem with water “disappearing” there as outlined in this report:
“The principal problem has been the loss of over half of the District’s surface water supply in the canal and distribution system that carries Canadian River water from Conchas Reservoir to the irrigated farms in the District.”
“The District’s report concluded that seepage losses from the system’s canals become greater, in quantity, each year.”
From that report I obtained a the study area map, and located where the town and the USHCN station is. Unsurprisingly, the USHCN station is situated close to the canals of the water project, and prevailing winds in the area tend to be from south to southwest:

With increased irrigation to pastureland for cattle, and a leaky canal system that loses half it’s volumes and demands ever more water to meet customer deliveries, it seems plausible that the Tucumcari area is becoming more humid, and with the increased humidity, per Christy, increased night time temperatures.
The studies of Roger Pielke Sr. show that land-use changes are an important factor in local and regional climate change. This effects of the changes in agriculture and irrigation on the local measurement of climate in Tucumcari might very well make a good case study.
I had hoped that the automated weather station that sets next to the Stevenson Screen might have humidity data that I could track, but alas it does not.

There is a fairly complete record though of temperature and precipitation at this link from the Western Regional Climate Center.
UPDATE 7/1/08 One of our commenters “AnonyMoose” has brought this well done historical weather and climate report by NMSU superintendent Rex Kirksey to our attention:
http://tucumcarisc.nmsu.edu/documents/rr751.pdf
This merits some further study for the data it contains. I’m committed to other projects today, so if anyone wants to have a go, be my guest and I’ll post it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e08f0a0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterNaomi Orsekes
My how the times have changed. Climategate, and all the other gates surrounding it, have turned things inside-out. The science is far from settled, as many of us have long suspected. The ranks of sceptic scientists are swelling, public opinion has swung; even the Royal Society has adopted a new position on climate science –  by George, there might be more to it than CO2 molecules after all!  The mainstream media is slowly coming around, too.
Yet, others refuse to hear it. 
Here’s a Youtube clip of Naomi Oreskes’ Truth About Denial presentation in 2007. Some of you may have watched it already. That presentation is in two parts.
Part 1: The Truth Part (CO2 drives global warming, there’s a consensus, science is settled).
Part 2: The Denial Part (There’s a disinformation campaign out there, denying it all).
Okay, that was back in 2007. Back then global warming science looked convincing, and so maybe such a position was plausible.
But here’s Oreskes in March 2010  in a presentation called the Merchants of Doubt, which is pretty much the same as her 2007 Truth About Denial. Despite all the new revelations, scandals and shifting scientific viewpoints, Oreskes continues to play the same music.  In the 2010 presentation she continues to ask (paraphrasing):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




How can there be so much scepticism in the public when there’s consensus among scientists?  Where does all the public doubt come from?
And answers by claiming it all stems from a tiny few merchants of doubt, who she describes as:
…a small but powerful group of people aided and abetted by well-funded think-tanks and a compliant mass media…not for money, but in defense of an ideology of laissez-faire governance, opposition to gevernment regulation in all forms.
Yes, ladies and gentlemen, Oreskes still believes, despite all the new revelations we’ve seen over the last few months, that all the scepticism and denialism out there today is still coming from the same sinister merchants of doubt. You’d think she’d would step back for a minute and re-evaluate her position. No chance.  Instead her reaction is to drive her head yet further into the sand.
Oreskes claims to be a science historian. My question is: Will she wake up and start a new chapter for the science history books? Or will she continue repeating her fairy tales? Don’t hold your CO2 breath.
UPDATE: Yesteday this post appeared at position No. 6 when one googled “Naomi Oreskes’ Denial”. Today it has dropped off to No. 17.
Share this...FacebookTwitter "
"
I found this over on Jerry Pournelle’s Chaos Manor. It seemed fitting given the discussion as of late. 
Some say the world will end in fire,
Some say in ice.
From what I’ve tasted of desire
I hold with those who favor fire.
But if it had to perish twice,
I think I know enough of hate
To say that for destruction ice
Is also great
And would suffice.
– Robert Frost


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0dde577',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Cato Institute will sponsor a week‐​long seminar near San Diego from August 1 to 7, 1999, as part of its Cato University program. Cato Sponsors will be invited to participate in the program featuring lectures and discussions on American history, law, economics, and philosophy.



Cato University allows busy adults to explore the fundamental ideas of liberty and limited government. In addition to the seminars, there is a separate 12‐​month home‐​study course that uses audiotapes, books, and an integrated study guide.



Faculty at the week‐​long program will include Alan Charles Kors, professor of history at the University of Pennsylvania and coauthor of The Shadow University; Randy Barnett, professor of law at Boston University and author of The Structure of Liberty; Don Boudreaux, president of the Foundation for Economic Education; and Tom G. Palmer, director of Cato University.



Guest lecturers will include historian Paula Baker of the University of Pittsburgh; psychologist Nathan‐​iel Branden, author of Taking Responsibility; and philosopher Christina Hoff Sommers, author of _Who Stole Feminism?_ Cato’s Edward H. Crane, David Boaz, Ted Galen Carpenter, and Robert Levy will also speak. 



In announcing the August seminar, Palmer said, “This program gives you the chance to recapture the intense intellectual atmosphere of your college days, in a climate where the lecturers and other participants share your fundamental ideas about freedom and justice. The schedule of lectures and discussions is designed to impart a great deal of information and analysis and encourage spirited discussion about the implications of the basic ideas.”



Cato University will be held at the beautiful Rancho Bernardo Inn, about 20 minutes from downtown San Diego. It will begin with dinner on Sunday, August 1, and conclude with lunch on Saturday, August 7. The cost, which includes all lectures and discussions, all meals, six nights in the inn, and a set of readings, is $1,500. Some scholarships are available for full‐​time students.



All Sponsors will receive a seminar brochure soon. Check the Web site at www​.cato​-uni​ver​si​ty​.org for more information or to register online, or call 202–789-5296. 



_This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)
"
"

Until now, most of the surface temperature measurement stations I’ve highlighted as substandard locations for measuring temperature accurately have been in the USA. Today, courtesy of Geoff Sherrington, we are treated to the sight of the main Australian historic site, Melbourne metropolitan, near LaTrobe St, Melbourne. He reports it has max-min temp records daily since 1855 to late 2007.
Yet look at the pictures, this station is only 2 meters from a sidewalk, and a couple of meters more from a major street intersection and voluminous traffic. Hardly the best place to measure temperature. This site demonstrates the growing trend of climate monitoring stations that have been gradually surrounded by increasingly closer urban influences, and demonstrates that the problem is not unique to the USA.
Here are some additional pictures, click for large versions.


And a satellite image of downtown Melbourne showing the intersection is available at Windows Live Maps
UPDATE: Kristen Brynes has offered a couple of photos she had available taken from different angles of the same site, see them below. Thanks Kristen.


Additionally, the Lat/Lon of this station is:
-37.8075, 144.9700
A PDF document from Australias BOM lists the METADATA for this site and is available here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea334d832',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe southwestern US was nearly a desert from about 9000 to 5000 years ago, when Holocene peaks in aridity, surface temperature, and wildfire rates occurred. Arctic sea ice was at its lowest extent of the Holocene during these years.

Image Source: Lachniet et al., 2020
A new extensively-referenced study (Lachniet et al., 2020) reviewing many dozens of climate records from across western North America has determined there is nothing unprecedented or even unusual about the modern climate for this region.
CO2 checked in at about 265 ppm during the Early and Middle Holocene, but today’s associated >400 ppm CO2 climate is much cooler and wetter, and there is much more Arctic sea ice present today.
During the Early to Middle Holocene (approximately  9 to 5 thousand years ago) this region could be characterized like this:
·2°C warmer, the warmest of the Holocene


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




·150 meters higher tree lines on mountains, indicating greater warmth
·5-17 m lower lake/pond levels
·“complete desiccation” or desert-like conditions in some areas
·peak wildfires
·“a warmer Arctic, reduced sea ice extent” – the lowest extent of the Holocene

Image Source: Lachniet et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_c46e1d5aedc7d8b197bc83e2f0539a56').on('change', function() {
			  jQuery('#amount_c46e1d5aedc7d8b197bc83e2f0539a56').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany’s Bundestag moves to enact a higher CO2 tax 
The “Corona pandemic”, despite the ever falling death rate, has given governments cover to enact draconian regulation and lockdowns, thus allowing their even wildest power wet dreams to turn into reality.
Just a year ago much of what we are seeing today was considered unimaginable. Yet, here we are.
Never before have modern “democratic” governments enacted such extreme lockdown and government intrusion measures like those we have seen in the current “Corona crisis”. And like real junkies, they need more.
There are other government crackdown opportunities left out there, among them the “climate crisis” – the Big Kahuna when it comes to government regulation, takeover and control. It’s not for nothing they’ve frittered away hundreds of billions propping up this fake crisis.
Germany is already seizing the opportunity, having pledged to ban internal combustion engines soon, modify human eating habits and restricting a host of other amenities we once took for granted. Soon these amenities will be redefined as privileges, and they will be easily available only to the wealthy and elite.
The latest is energy and heat.
Higher CO2 tax decided
The German media, e.g. NTV public broadcasting, have reported that the Bundestag has just decided on a higher CO2 tax beginning already next year, January 1st.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“It is intended to make fossil fuels less attractive. This means that fuel, heating oil and gas, among other things, will cost more,” reports  German public broadcaster NTV. “The CO2 price will be 25 euro per ton from the new year on. The levy acts like a tax and is to climb gradually to 55 euros by 2025.”
That also means higher gasoline and diesel fuel prices, which will make transportation more expensive.
Already German electricity is the most expensive in Europe.

But not to worry: Industries with particularly high energy requirements that are also in global competition will be relieved of the costs. And of course, the rich and elitists will also keep their cushy red carpet lives – so that they can continue effectively doing their important work – while the rest of us are forced to move out into the cold mud.
In the end it’ll be lower income workers and households left struggling with the higher prices for everything. Even heat will become a luxury.
And so continues the cycle of political demise 
And once governments get total control and surveillance over citizens across the world, they’ll try to tell us all just how much better things have become as a result, like the old communists used to do with their state controlled media. Of course, life in reality will become much worse, but we’ll be asked to pretend that it isn’t.
We all know what follows next: Revolution.


		jQuery(document).ready(function(){
			jQuery('#dd_4223c6fa640da5fd71799ecd0745c810').on('change', function() {
			  jQuery('#amount_4223c6fa640da5fd71799ecd0745c810').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
There has been a lot of discussion lately about the accuracy of measuring Sea Surface Temperatures prompted by a new study from Phil Jones from the University of East Anglia and Director of UEA’s Climatic Research Unit. The measurement issue for sea surface temperatures that Dr. Jones is studying was recently showcased in an article in the UK Independent.
I’m going to present the article here first, and then we’ll talk about how sea surface temperatures have been measured, and what sorts of issues the changes between cloth buckets, metal buckets, and engine inlets actually entails.
At first glance, I see this issue raised by Phil Jones as not being well thought through, and ignoring the measurement environment actuality, instead focusing on the change in bucket types as being “absolute”. I think it has a lot of grey area, and a lot of potential errors that haven’t been considered. I’ll cover those in the next part, but for now please read the article and let me know what you think.
Case against climate change discredited by study
By Steve Connor, Science Editor
Thursday,  29 May 2008

A difference in the way British and American ships measured the temperature  of the ocean during the 1940s may explain why the world appeared to undergo a  period of sudden cooling immediately after the Second World War.
 Scientists believe they can now explain an anomaly in the global temperature  record for the twentieth century, which has been used by climate change skeptics  to undermine the link between rising temperatures and increases in atmospheric  carbon dioxide.
The record for sea-surface temperatures shows a sudden fall after 1945, which  appeared to go against the general trend for rising global average temperatures  during the past century.
Skeptics have argued it supports the idea that rising temperatures have more  to do with increased solar activity – sunspots – than increasing levels of  man-made carbon dioxide exacerbating the greenhouse effect.
However, an international team of scientists has investigated the raw data  from the period. They found a sudden increase from 1945 onwards in the  proportion of global measurements taken by British ships relative to American  ships.
The scientists point out that the British measurements were taken by throwing  canvas buckets over the side and hauling water up to the deck for temperatures  to be measured by immersing a thermometer for several minutes, which would  result in a slightly cooler record because of evaporation from the bucket.
The preferred American method was to take the temperature of the water sucked  in by intake pipes to cool the ships’ engines. Those records would be slightly  warmer than the actual temperature of the sea because of the heat from the ship,  the scientists said.
Taking into account the difference in the way of measuring sea-surface  temperatures, and the sudden increase in the proportion of British ships taking  the measurements after the war, the result was an artificial lowering of the  global average temperature by about 0.2C, said Professor Phil Jones of the  University of East Anglia in Norwich.
“It occurred in the period of the 1940s when the number of observations of  sea-surface temperature were markedly fewer than either before or after that  period and most of the measurements were made by British and American ships.  This made the apparent anomaly more pronounced,” Professor Jones said.
The study, published in the journal Nature, found that the global average  temperatures in the late 1940s stayed roughly the same rather than falling.  David Thompson of Colorado State University, the team’s leader, said a drop was,  in effect, an artifact rather than a real observation.
“I was surprised to see the drop so clearly in the filtered data, and working  in partnership with others, realized it couldn’t be natural,” Dr Thompson  said.
Although the initial drop was significant, it did not last. By the 1960s,  many other nations began taking ship-borne measurements of ocean temperature, minimizing the discrepancy.
Professor Jones said that the study lends support to the idea that a period  of global cooling occurred later during the mid-twentieth century as a result of  sulphate aerosols being released during the 1950s with the rise of industrial  output. These sulphates tended to cut sunlight, counteracting global warming  caused by rising carbon dioxide.
“This finding supports the sulphates argument, because it was bit hard to  explain how they could cause the period of cooling from 1945, when industrial  production was still relatively low,” Professor Jones said.
A similar problem could be occurring now with the move from ship-borne  measurements to those from unmanned buoys, which tend to produce slightly lower  records. This could explain why global average temperatures in recent years have leveled off.

FYI: According to the American Meteorological Society:
bucket thermometer—A water-temperature thermometer provided with an  insulated container around the bulb.




It is lowered into the sea on a line until it has had time to reach the temperature of the surface water, then withdrawn and  read. The insulated water surrounding the bulb preserves the water reading and  is also available as a salinity sample. 






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f033b91',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

According to Sir David Attenborough, the famous British broadcaster and naturalist, “humans are threatening their own existence and that of other species by using up the world’s resources.” In a recent interview, Attenborough said that “the only way to save the planet from famine and species extinction is to limit human population growth.”   




We are a plague on the Earth,” he continued. “It’s coming home to roost over the next 50 years or so. It’s not just climate change; it’s sheer space, places to grow food for this enormous horde. Either we limit our population growth or the natural world will do it for us, and the natural world is doing it for us right now… We keep putting on programmes about famine in Ethiopia; that’s what’s happening. Too many people there.



In 2006, Sir David Attenborough was voted Britain’s greatest living icon. Popularity, however, is no substitute for wisdom. As I have explained in a previous blog post, “[The] rate of global population growth has slowed. And it’s expected to keep slowing. Indeed, according to experts’ best estimates, the total population of Earth will stop growing within the lifespan of people alive today. And then it will fall… the long‐​dreaded resource shortage may turn out not to be a problem at all.”   
  
  
Some of the reasons why Attenborough is as mistaken about the “over‐​population problem” today as Paul Ehrlich was when he published his infamous The Population Bomb in 1968, include:   




What is to be said about Attenborough’s take on the famine in Ethiopia? In a word: embarrassing.   
  
  
To start with, population density in Monaco is 17,676 people per square kilometer. It is 79 people per square kilometer in Ethiopia. Monaco is one of the richest countries in the world and Ethiopia one of the poorest. If anything, there is an inverse relationship between population density and poverty. Some of the world’s most populated places (Hong Kong, Singapore, The Netherlands, etc.) are very rich, while some of the least heavily populated countries (Central African Republic, Chad, the two Congos, etc.) are very poor.   
  
  
The real reasons for Ethiopian famines are altogether different. First, Ethiopia was a Marxist dictatorship and like many Marxist dictatorships (USSR, PRC and Cambodia), it experienced both economic collapse and civil war. Second, Ethiopia has almost no economic freedom. All land, to give one example, is owned by the state – and the state can take it away. As a consequence, farmers have little incentive to make long term plans and undertake necessary investment, and agricultural production suffers.   
  
  
Attenborough is, in many ways, a great man and I love watching his programs. But, he thinks he knows more than he does. A little intellectual humility would not be amiss.   
  
  

"
"

August, 2005 — Hurricane Katrina blows into the Gulf of Mexico and blasts New Orleans to smithereens. Environmentalists quickly blame the storm on global warming — or at the very least, claim that warming will inevitably lead to more Katrina‐​like hurricanes. Although there is no clear scientific consensus on what impact a warming world might have on the frequency of big Gulf hurricanes, it’s enough to move public opinion significantly on the question of whether federal, state, and local governments ought to do something about climate change.   
  
  
May, 2006 — Al Gore’s _An Inconvenient Truth_ opens in New York and Los Angeles. The companion book becomes the #1 paperback non‐​fiction book on the New York Times bestseller list in July. The movie goes on to become the fourth highest grossing documentary in U.S. history and wins an Academy Award.   
  
  
July, 2007 — _Live Earth_ concerts to save the planet feature 150 top musical acts in 11 cities around the world. While it’s unclear how many people actually watched those concerts, _Live Earth_ set a record for on‐​line entertainment with over 15 million video streams during the live concerts alone.   
  
  
October, 2007 — Al Gore and the Intergovernmental Panel on Climate Change win the Nobel Peace Prize.   
  
  
March, 2008 — The Heartland Institute sponsors a conference in New York City to showcase scientific skepticism about the seriousness of climate change. The event is received with uncharacteristically loud derision by the mainstream media.   
  
  
Now, with all of that in mind, wouldn’t you think that the public would be growing more — not less — worried about climate change? You might, but you would be wrong. According to today’s _Energy & Environment Daily _(subscription required), a new poll conducted by Princeton Survey Research Associates and released by the John Brademas Center for the Study of Congress at New York University finds that Americans are less worried about climate change than they were a couple of years ago.   
  
  
_E &E Daily_ reports that the survey’s margin of error was +/- 3 percent. Here are the highlights:   
  
  
The percentage of Americans who said global warming requires immediate attention declined from 77 in 2006 to 69 percent today.   
  
  
The percentage of Americans who said they were “very worried” about global warming increased from 31 percent in 2006 to 39 percent in 2008. But that’s misleading; everyone gets “more worried” about everything in a presidential election year. What’s striking to me is that the rise in the number of those “very worried” about global warming was less than the rise in the number of those “very worried” about the four other issues surveyed by Brademas Center (Medicare, Social Security, and energy).   
  
  
The declining number of those who said they were “somewhat worried” about global warming more than offset the increase of those who reported being “very worried.”   
  
  
There are several possible explanations for this data. My guess is that it’s a little of each of the following.   
  
  
Explanation #1 – The public has only limited patience for “end of the world” prognostications. If the world isn’t visibly ending from whatever boogey man is said to menace said world, most of us begin to lose interest. We’re all well aware that Earth has been sentenced to doom hundreds of times over by activists of various stripes but has somehow gained a reprieve time and time again.   
  
  
Explanation #2 – The time horizon of most voters is very, very short. Getting people to voluntarily sacrifice for “the grandkids” or whomever is a near‐​impossible task. It would probably take a Katrina‐​a‐​year … and even then, that might not be enough. The mathematical certainty regarding the economic train wreck about to be visited upon “the grandkids” as a consequence of the trillions of dollars of unfunded liabilities for present federal health care and retirement programs does not engender sacrifice. It engenders shrugs and accelerated wealth transfers from the future to the present.   
  
  
Explanation #3 – Global warming, if it plays out as the IPCC suspects, will be a slow‐​moving event. Panic over climate change has to compete with panic over Islamic terrorism, panic over housing markets, panic over globalization, panic over energy prices, panic over immigration, and episodic panic over dozens of other (usually dubious) worries. Simply put, global warming has a hard time competing with all of the other items on the policy agenda.   
  
  
So conservatives, take heart. Enviros, take a valium.
"
"

There’s a common misconception that people who are in favor of a free market are also in favor of everything that big business does. Nothing could be further from the truth.



As a believer in the pursuit of self‐​interest in a competitive capitalist system, I can’t blame a businessman who goes to Washington and tries to get special privileges for his company. He has been hired by the stockholders to make as much money for them as he can within the rules of the game. And if the rules of the game are that you go to Washington to get a special privilege, I can’t blame him for doing that. Blame the rest of us for being so foolish as to let him get away with it.



I do blame businessmen when, in their political activities, individual businessmen and their organizations take positions that are not in their own self‐​interest and that have the effect of undermining support for free private enterprise. In that respect, businessmen tend to be schizophrenic. When it comes to their own businesses, they look a long time ahead, thinking of what the business is going to be like 5 to 10 years from now. But when they get into the public sphere and start going into the problems of politics, they tend to be very shortsighted.



The most obvious example is protectionism. Can you name any major American industry that has really benefited from tariffs and protection? Alexander Hamilton, in his famous report on manufactures, praised Adam Smith to the sky while at the same time arguing that the United States was a special case in that it had infant industries that needed to be protected, including steel. Steel is still being protected 200 years later.



Commercial banking is another example. At the end of World War II commercial banking accounted for roughly half of the capital market. Today it accounts for about one‐​fifth. Why has it deteriorated? Why is the international financial market in London, not in New York? 



The answer is the long‐​term effect of the of the banking industry’s insistence on special government favors. In the early days, under what was known as Regulation Q, the government set a limit on the interest rates that banks could pay, including a rate of zero on demand deposits. The government‐​imposed interest rate of zero on demand deposits encouraged the emergence of money market funds and the growth of substitutes for and alternatives to banks. The banking industry consistently supported fixed exchange rates. When the dollar got into trouble, President Johnson introduced restrictions on foreign lending and an interest‐​equalization tax. The result was to drive the commercial banking industry to London. Both of those measures reduced the commercial banking industry from the predominant supplier of credit to a minor player. Again, a policy that was very shortsighted.



The easiest shot of all is the way in which corporations make contributions. The oil industry contributes to conservation organizations that are trying to sharply reduce the use of oil. The nuclear industry contributes to organizations that support nonnuclear energy. Recently, Capital Research Center analyzed grants from major corporations to public policy organizations and found that the major corporations made $3 in grants to the nonprofit left for every dollar they gave to the nonprofit right.



Why hasn’t the corporate world followed the excellent example that was set by Warren Buffett? From his earliest days, in sending a dividend check to his stockholders, he said, “We are prepared to distribute X dollars on your behalf for each share of stock to charity, to some organization. Let us know to whom you would like it sent, and we will send it on your behalf.” 



Why should corporations decide the charitable purposes that should be supported by the income of their stockholders? Why shouldn’t each stockholder decide that? And why is the business community in general so insistent on supporting its own enemies? 



Now consider education. As you know, I have long been in favor of trying to privatize schooling through a voucher system. One strong argument in favor of privatization has to do with the values instilled by our public education system.



Any institution will tend to express its own values and its own ideas. Our public education system is a socialist institution. A socialist institution will teach socialist values, not the principles of private enterprise. That wasn’t so bad when elementary and secondary education was more dispersed, so there could be more local control. When I graduated from high school there were 150,000 school districts in the United States. Today there are fewer than 15,000 and the population is twice as large.



What has been the business community’s attitude toward education? Members of the business community have been well aware that schools instill values that are unsympathetic to a free private enterprise system. They are also aware that it’s difficult to get employees with the appropriate skills. But have they been trying to promote a private enterprise education industry? Not at all. Their major activity has been to assign some of their employees to teach in public schools and to contribute computers and other items to public schools. I can’t blame an individual for what he does, but I think it’s tragic that Walter Annenberg contributed hundreds of millions of dollars for government schools, for public schools, not for private schools. I have not seen any movement in the business community in general, until very recently, to try to promote an educational system under which the customer, namely the parent and the child, has a real choice about the schooling the child gets.



Now we come to Silicon Valley and Microsoft. I am not going to argue about the technical aspects of whether Microsoft is guilty or not under the antitrust laws. My own views about the antitrust laws have changed greatly over time. When I started in this business, as a believer in competition, I was a great supporter of antitrust laws; I thought enforcing them was one of the few desirable things that the government could do to promote more competition. But as I watched what actually happened, I saw that, instead of promoting competition, antitrust laws tended to do exactly the opposite, because they tended, like so many government activities, to be taken over by the people they were supposed to regulate and control. And so over time I have gradually come to the conclusion that antitrust laws do far more harm than good and that we would be better off if we didn’t have them at all, if we could get rid of them. But we do have them.



Under the circumstances, given that we do have antitrust laws, is it really in the self‐​interest of Silicon Valley to set the government on Microsoft? Your industry, the computer industry, moves so much more rapidly than the legal process, that by the time this suit is over, who knows what the shape of the industry will be. Never mind the fact that the human energy and the money that will be spent in hiring my fellow economists, as well as in other ways, would be much more productively employed in improving your products. It’s a waste! But beyond that, you will rue the day when you called in the government. From now on the computer industry, which has been very fortunate in that it has been relatively free of government intrusion, will experience a continuous increase in government regulation. Antitrust very quickly becomes regulation. Here again is a case that seems to me to illustrate the suicidal impulse of the business community.



Now I come to the hard part: Why is there that suicidal impulse? Why do business people behave that way? I hope many of you in this room will think about it and try to come up with an answer. I will give you the few suggestions that I have, but none of them seems to me an adequate explanation. One reason was stated more than a century ago by a remarkable man, Gen. Francis A. Walker, a professor at Yale and subsequently president of M.I.T. He wrote:  




When it comes to economics, everybody is an expert who almost always gets it wrong—and business executives are no exception.



Schumpeter gave a very different explanation for this phenomenon. He argued that, within large corporations, the people in charge develop essentially bureaucratic‐​socialist attitudes and institutions. Belief in entrepreneurship and private enterprise tends to be replaced by a bureaucratic approach, leading to the emergence of a socialist system. I don’t believe that’s true. In a competitive society there is enough pressure around to prevent that from happening. But that would be an explanation.



The general climate of opinion, which treats government action as an all‐​purpose cure for every ill, is probably a more important factor. However, over the past 40 years, the climate of opinion has been changing. It is no longer taken for granted, as it used to be, that if there is a problem the way to solve it is to get the government involved. We have been winning the war of ideas even though we have been losing the war in practice. Governments today are far bigger and more intrusive than they were 40 or 50 years ago, at the same time that—partly as effect—the climate of opinion is much less favorable to government control than it was then. But I still don’t think that is an adequate explanation, so I confess that I have no good explanation. Yet I think the phenomenon calls for an explanation and that it’s in your self‐​interest to find one and change the pattern of business behavior in order to get rid of what is a clear suicidal impulse.



 _This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)
"
"
See related articles from the Guardian: Billions Wasted On UN Climate Programme and Discredited Strategy
“It looks like between one and two thirds of all the total CDM offsets do not represent actual emission cuts.” — David Victor, Stanford University and co-author of a study examining 3000 UN funded offset programs
This article below was reposted from TriplePundit
 

World’s Largest Carbon Market Facilitates Pollution


An article in the Guardian newspaper reveals that billions worth of ‘clean’ investment on the world’s largest carbon offsets market ends up polluting the environment. The article cites researchers who’ve reviewed the participating companies in the Kyoto Protocol Clean Development Mechanism (CDM). They issued a report which seriously undermines the credibility of the CDM.
The CDM certificates facilitate the funding of clean technology investments by Third World companies that are expanding their operations. Western companies can buy the certificates to offset their own pollution. But it turns out that in reality most of the funds go to coal and oil companies, builders of destructive dams and other enterprises that are not green in the slightest.
The research that revealed the practices is of major importance not least because policymakers are set to review the CDM in the near future as the Kyoto Protocol expires in 2012. CDM credits are the world’s largest offset market, with annual trading last year totalling around EUR40 billion. Most credits are currently traded on the European Trading System (ETS) by European countries and companies but when the US starts to participate, something that’s more or less a given, trading will rise to over EUR 100 billion within two years easily.
The Stanford scholars opened a can of worms. They say that “Much of the market does not reflect actual reductions in emissions, and that trend is poised to get worse.” They researched more than 3,000 projects that had been applying/granted for up to $10bn of credits for the next four years and said that most of the applications should be rejected. If the scheme operated in any way realistically, we’d see a much smaller market, they say cautioning that there’s hardly enough clean air available for the demand that will build up in the near future. That’s rather an important point to consider ahead of next week’s Warner-Lieberman cap and trade bill which proposes US companies are allowed to buy up to 15% of their needed carbon credits from the (successor to the) CDM.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f2ea773',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Most every paper in the country is trumpeting today that China has finally agreed to limit its emissions of carbon dioxide, gutting the principal objection of people opposed to unilateral and expensive reductions in ours.   
  
  
Too bad it’s not true.   
  
  
According to the official pronouncement, all China said was that they “intend” to cap their emissions “around 2030”. Anything new here? In November, 2009, prior to the (failed) UN climate fest in Copenhagen, they announced their “intention” to reduce their emissions per unit economic output (called “carbon intensity”) by 40–45% by 2020. Since then, things haven’t appreciably changed—so they now have five years to execute this huge drop, which isn’t going to happen.   
  
  
The road to global warming is paved with China’s good “intentions”.   
  
  
We also note that they “intend” to derive 20 per cent of their energy from non‐​carbon based sources by 2030. No doubt working late into last night (as did we; this story broke at 10:30), the estimable Roger Pielke, Jr., has already calculated that this means that the Chinese will have to put the equivalent of one nuclear power plant _per week_ on line between now and then. As Roger wryly noted, “some people take it seriously”.   
  
  
Don’t. But we should take seriously President Obama’s announcement that the US will double its scheduled emissions reductions by 2025. Thanks to the 2007 Supreme Court (5–4) decision that incredulously said that the 1992 Clean Air Act Amendments gave the President the power to command and control virtually our entire energy economy, he indeed can do what he just said.   
  
  
It would take an act of Congress to prevent him, an act that would most certainly be vetoed, without the necessary two‐​thirds majority to override.   
  
  
One might think that he would care about what the voters think—but that’s not the case. A careful read of election returns reveals that the cap‐​and‐​trade, and not health care, cost his party control of the House in 2010, and, in 2014, the epicenter of electoral carnage was in the coal mining regions of Kentucky and West Virginia, costing his party the Senate.   
  
  
While China has good “intentions” we get real “unemployment”. Such a deal!
"
"
One of the strangest things I’ve learned in the past year about the US Historical Climatological Network is the propensity for placement of weather stations at sewage treatment plants.
The reason of course has to do with putting a thermometer at a facility that is staffed 7 days a week. That thermomter must be manually read once a day and the readings transcribed into a logbook. Waste Water Treatment Plants (WWTP’s) fit that requirement (as they have an operator on duty, often 24/7) but they themselves are their own mini islands of waste heat and humidity, especially in winter and overnight. Yet, a significant portion of the US climate data comes from these locations.
Some have grassy areas where a climate monitoring station could be placed, such as the one in Morrison, IL, and you’d think they would place it there, away from the sewage tanks. Unfortunately, no.

Click for a larger image, additional photos available here at surfacestations.org
My sincere thanks to volunteer surveyor Scott Finegan for these photos.
The Stevenson Screen housing the thermometer is about 5 feet away from each tank, while the concrete building in the background is some 50 feet away. You’d think that they could have placed the station a little further away. Again, as we’ve seen time and time again, the placement is not often about the best location, it is about convenience for the observer.
The GISS graph of temperature over the station history shows a fairly strong warming trend from about 1980 to the present. The question is, how much of that is from increased throughput of the sewage treatment plant responding to population growth, and how much of it is climate change?

Click for source graph from NASA GISS
According to NCDC’s Multi Metadata System database, this station has been at this location since at least 1948, even though a lat/lon accuracy update makes it appear to have been relocated in 1997, it has in fact been at this location all that time.
A nearby station, 25km away, Clinton, IA, is also in the GISS database and shows less of a trend during the same period:

Separating a climate change signal from the waste heat (and increasing effluent volume of the WWTP due to population growth) may not be a simple matter to disentangle. Since each WWTP has different conditions, coming up with a blanket correction would not be easy. Therefore, since the USA is highly oversampled spatially with weather stations that report daily data which can used for climate, it would be prudent in my opinion, to remove stations like this from the climatic database since the data produced by USHCN stations at WWTP’s may not be truly representative of climate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f3e77b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
My good friend Jim Goodridge, former state climatologist for California, came to visit yesterday to offer some help on my upcoming trip, as well as to talk shop a bit about the state of affairs on climate change.
He had previously authored a paper that I had hoped to present on his behalf at ICCC, but unfortunately it got excluded from the schedule by an omission. Yesterday he decided to rework that paper to bring out it’s strongest point.
One of the best and simplest ways of seeing the solar connection is to look at accumulated departure. Here is Jim’s essay on the subject:
Solar – Global Warming Connection
Jim Goodridge
State Climatologist (Retired)
jdgoodridge – (at) – sbcglobal dot net
March 22, 2008
Solar irradiance has been monitored from satellites for three sunspot cycles. The sunspot numbers and solar irradiance were shown to be highly correlated. Since sunspot numbers have been increasing since 1935 the irradiance must also be increasing.
The sun was once considered to be constant in its output, hence the term “Solar Constant”. Recent observations suggest that the sun is a variable star. Observations of solar irradiance have been made with great precision from orbiting satellites since about 1978. These observations are from Wikipeda: http://en.wikipedia.org/wiki/Solar_variation
They clearly indicate that the solar irradiance varies with the historic sunspot numbers:

Click for a larger graph

Click for a larger graph:
Using this relationship, 307 years of solar irradiance is easily inferred.
Sunspot numbers since 1700 were plotted as accumulated departure from average in order to compare them with weather variables. The sunspot number index indicates a declining trend for the 1700 to 1935 period and an increase from 1935 to 2008. The eleven-year cycle is clearly visible.

An increase in sunspot activity, and by inference, irradiance since 1935 is plainly indicated.
Moderators note: And I want to also call attention to these graphs, which shows the change in solar irradiance since 1611 and Geomagnetic activity over the last 150 years:


Clearly, solar geomagnetic activity has been on the rise. There will be more interesting posts on sunpots coming in the next week or two, stay tuned -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0810d7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA few days ago I wrote a post about how Reuters wasted no time blaming the outbreak of a fungus disease in the U.S. Northwest and British Columbia on climate change: http://pgosselin.wordpress.com/2010/04/23/were-to-blame/.
Of course this was just another example of wreckless media speculation and wishful thinking by those who simply can’t wait for the coming manmade climate catastrophe. They really are pretty desparate. The claim has already been completely debunked at World Climate Report: http://www.worldclimatereport.com/.
Share this...FacebookTwitter "
"

This was forwarded to me by Russ Steele over at NCWatch. Titled: Climate Change – Is CO2 the cause? 
It’s a compelling presentation by Bob Carter of the facts of climate change to a recent public forum in Australia.  He is a research professor in the Marine Geophysical Laboratory at James Cook University, Australia. He’s featuring a “Count the torpedos” element.
My www.surfacestations.org project is highlighted in Part 4. It is gratifying to see my work used by others,
Part1
Part2
Part3
Part4


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3616c34',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe recall last year how projections of new lows in Arctic sea ice extents were boldly made, and eagerly publicised by the catastrophe-obsessed  media. It made for good headlines, but they were all wrong. See here June Outlook Report.

This year Germany’s Alfred Wegener Institute AWI here and the KlimaCampus of the University of  Hamburg are taking another shot at it, along with a dozen or so other fortune tellers. The AWI press release states:
The projection of the KlimaCampus of the University of Hamburg is 4.7 million sq km, which is more negative than the 5.2 million sq km made by the AWI scientists. Yet both research centres do not exclude a record low of 4.3 million set in 2007 being reached.
Decisive factors like ice thickness and how the rest of the summer develops are unknown and do not allow for accurate projections. Yet the AWI is projecting a sea level that is almost a million sq km over 2007. They know that the ice in the central Arctic is thick, but probably don’t want to say it publicly. Indeed, most of the sea ice fortune tellers are projecting 2010 to finish higher than 2008.
Both teams used different methods for their projections. Prof. Rüdiger Gerdes and his team at the AWI worked out a model together with the scientific companies OASys and FastOpt that uses oceanic drift buoys and satellite data for measuring the movement of ice. The projection will be revised each month during the summer. Dr. Gerdes says:
Currently we calculate with 80% probability that the sea ice extent will be between 4.7 million and 5.7 million sq km in September. The projections will become more precise as summer progresses.
Meanwhile the KlimaCampus-Team of Prof. Lars Kaleschke takes satellite photos of the Arctic sea ice for each day of 2010 and compares them to the same day of each year from 2003 to 2009.
The number and size of the ice-free areas are indicators for subsequent ice developments. These dark spots store more solar energy in early summer and thus enhance ice melt during the polar summer, as the sun does not disappear until September.
Share this...FacebookTwitter "
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"
Share this...FacebookTwitterIn the early 1900s, the globally-averaged distribution of calculated surface temperature estimates ranged between 14 and 15°C. For 1991-2018, HadCRUT, Berkeley, and NASA GISS also estimate today’s global temperature is about 14.5°C.
Scientists estimating Earth’s surface temperature has been an ongoing pursuit since the early 19th century.
A new study (Kramm et al., 2020) suggests the generally agreed-upon global temperature from 1877 to 1913 from dozens of calculated results was about 14.4°C.
Problematically, HadCRUT, Berkley, and NASA GISS also indicate the 1991-2018 had a global surface temperature of about 14.5°C.
This would suggest there has been “no change in the globally averaged near-surface temperature over the past 100 years”.

Image Source: Kramm et al., 2020
Share this...FacebookTwitter "
"
November 30th marks the official end of hurricane season. Below is some good news, courtesy of Ryan Maue at Florida State University COAPS :
The 2007 Atlantic Hurricane season did not meet the hyperactive expectations of the storm pontificators. This is good news, just like it was last year. With the breathless media coverage prior to the 2006 and 2007 seasons predicting a catastrophic swarm of hurricanes potentially enhanced by global warming a la Katrina, there is currently plenty of twisting in the wind to explain away the hyperbolic projections. The predominant refrain mentions something about “being lucky” and having “escaped” the storms, and “just wait for next year”.
Before we prepare for the obvious impending onslaught of the next “above-average” hurricane season, let’s review some very positive aspects of what 2007 offered:

The 2007 Atlantic Hurricane season was below-normal and tied for 2002 as the most inactive since the El Nino depressed 1997 season in terms of storm energy. Note: Hurricane Energy is measured through the Accumulated Cyclone Energy (ACE) index
The North Atlantic was not the only ocean that experienced quiet tropical cyclone activity. The Northern Hemisphere as a whole is historically inactive. How inactive? One has to go back to 1977 to find lower levels of cyclone energy as measured by the ACE hurricane energy metric. Even more astounding, 2007 will be the 4th slowest year in the past half-century (since 1958) .
Fewest Northern Hemisphere Hurricane Days since 1977. 3rd Lowest since 1958 (behind 1977 and 1973). See the Hurricane Days Graphic below.
When combined, the 2006 and 2007 Atlantic Hurricane Seasons are the least active since 1993 and 1994. When compared with the active period of 1995-2005 average, 2006 and 2007 hurricane energy was less than half of that previous 10 year average. The most recent active period of Atlantic hurricane activity began in 1995, but has been decidedly less active during the previous two seasons.
When combined, the Eastern Pacific and the North Atlantic, which typically play opposite tunes when it comes to yearly activity (b/c of El Nino), brushed climatology aside and together managed the lowest output since 1977. In fact, the average lifespan of the 2007 Atlantic storms was the shortest since 1977 at just over two days. This means that the storms were weak and short-lived, with a few obvious exceptions.

Hurricane Days by Year

2007 Departure from ACE and Climatic norms:


Basin
Current ACE
Climo ACE
% Departure


Northern Hemisphere
373.4
525.2
-28.9%


North Atlantic
67.7
93.8
-27.8%


Western Pacific
209.2
286.8
-27.1%


Eastern Pacific
52.2
131.2
-60.2% 




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea25e7b10',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
NOTE: This post is the second in the series from Dr. Roy Spencer of the National Space Science and Technology Center at University of Alabama, Huntsville. The first, made last Friday, was called Atmospheric CO2 Increases: Could the Ocean, Rather Than Mankind, Be the Reason?
Due to the high interest and debate his first post has generated, Dr. Spencer asked me to make this second one, and I’m happy to oblige. 
Here is part2 of Dr. Spencer’s essay on CO2 without any editing or commentary on my part.
(Side note: Previously, I erroneously reported that Dr. Spencer was out of the country. Not so. That was my mistake and a confusion with an email autoresponse from another person named “Roy”. Hence this new update.)

More CO2 Peculiarities: The C13/C12 Isotope Ratio

Roy W. Spencer
January 28, 2008

In my previous post, I showed evidence for the possibility that there is a natural component to the rise in concentration of CO2 in the atmosphere.  Briefly, the inter-annual co-variability in Southern Hemisphere SST and Mauna Loa CO2 was more than large enough to explain the long-term trend in CO2.  Of course, some portion of the Mauna Loa increase must be anthropogenic, but it is not clear that it is entirely so.
Well, now I’m going to provide what appears to be further evidence that there could be a substantial natural source of the long-term increase in CO2.
One of the purported signatures of anthropogenic CO2 is the carbon isotope ratio, C13/C12.   The “natural” C13 content of CO2 is just over 1.1%.  In contrast, the C13 content of the CO2 produced by burning of fossil fuels is claimed to be slightly smaller – just under 1.1%.
The concentration of C13 isn’t reported directly, it is given as “dC13”, which is computed as:
“dC13 = 1000* {([C13/C12]sample / [C13/C12]std ) – 1
The plot of the monthly averages of this index from Mauna Loa is shown in Fig. 1.

Now, as we burn fossil fuels, the ratio of C13 to C12 is going down.  From what I can find digging around on the Internet, some people think this is the signature of anthropogenic emissions.  But if you examine the above equation, you will see that the C13 index that is reported can go down not only from decreasing C13 content, but also from an increasing C12 content (the other 98.9% of the CO2).
If we convert the data in Fig. 1 into C13 content, we find that the C13 content of the atmosphere is increasing (Fig. 2).

So, as the CO2 content of the atmosphere has increased, so has the C13 content…which, of course, makes sense when one realizes that fossil-fuel CO2 has only very slightly less C13 than “natural” CO2 (about 2.6% less in relative terms).  If you add more CO2, whether from a natural or anthropogenic source, you are going to add more C13.
The question is: how does the rate of increase in C13 compare to the CO2 increase from natural versus anthropogenic sources?
First, lets look at the C13 versus C12 for the linear trend portion of these data (Fig. 3).

The slope of this line (1.0952%) represents the ratio of C13 variability to C12 variability associated with the trend signals.  When we compare this to what is to be expected from pure fossil CO2 (1.0945%), it is very close indeed: 97.5% of the way from “natural” C13 content (1.12372%) to the fossil content.
At this point, one might say, “There it is!  The anthropogenic signal!”.  But, alas, the story doesn’t end there.
If we remove the trend from the data to look at the inter-annual signals in CO2 and C13, we get the curves shown in Figures 4 and 5.


Note the strong similarity – the C13 variations very closely follow the C12 variations, which again (as in my previous post) are related to SST variations (e.g. the strong signal during the 1997-98 El Nino event).
Now, when we look at the ratio of these inter-annual signals like we did from the trends in Fig. 3, we get the relationship seen in Fig. 6.

Significantly, note that the ratio of C13 variability to CO2 variability is EXACTLY THE SAME as that seen in the trends!
BOTTOM LINE: If the C13/C12 relationship during NATURAL inter-annual variability is the same as that found for the trends, how can people claim that the trend signal is MANMADE??


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1489aab',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Mount Kilamanjaro – Tanzania, Africa – still snowy. Photo by Neil Modie, January 2008
Last week, I broke the story of a press release issued by NOAA where they publish an opinion smashing any link between hurricanes and global warming saying that “There  is nothing in the U.S. hurricane damage record  that indicates global warming has caused a  significant increase in destruction along our coasts.”
Many readers may recall that Al Gore used hurricanes prominently in An Inconvenient Truth, and mentions hurricane Katrina specifically. Gore claims that increased hurricane activity is caused by global warming.
Last week, when the NOAA press release came out smashing any link between hurricanes and global warming, I wrote to my local newspaper editor, David Little, and said to him “Do you care to bet that AP and Reuters won’t run this story?” He responded: “I hope they do, it seems newsworthy to me.”
Well here is is, 4 days later, not a peep.
A Google search of news stories for “NOAA increased hurricane” (keywords of the press release) reveals a tiny handful of stories about the press release. Could you imagine though if the story said the reverse?  What if NOAA claimed they had established a definitive link between global warming and hurricanes. Oh my, the humanity of it all! Gloom, doom, death, destruction, angst, and demands for action on Kyoto. If it bleeds it leads. Compare to all the stories still circulating about hurricane Katrina and global warming.
Here is another story about a point from Gore’s AIT hit parade; Mount Kilimanjaro. Mr. Gore asserted that the disappearance of snow on Mount Kilimanjaro in East Africa was expressly attributable to global warming; “Within the decade, there will be no more snows of Kilimanjaro.” That was in 2005 in his movie An Inconvenient Truth.
Deforestation seems to be causing Mount Kilimanjaro’s shrinking glacier. Researchers think deforestation of the mountain’s foothills is the most likely culprit. Without the forests’ evapotranspiration of humidity into the air, previously moisture-laden winds blowing across those forests now blow drier. The summit, no longer replenished with water from those winds, started shrinking. Studies show the ice is evaporating through a process called sublimation. You can witness this effect at home, have you ever noticed that ice cubes left in your freezer tend to shrink with time?
Last year, a British Court ruled Gore’s point about Kilimanjaro not to be true.
So when a news story crossed my desk today that said: “Mount Kilimanjaro: On Africa’s roof, still crowned with snow” I had to wonder, will we see this one covered in the main stream media? Or maybe those beacons of truth over at Real Climate will make a note of it?
Don’t hold your breath. But, at least the New York Times travel section covered it. It seems more of a touristy thing to have snow on Kilimanjaro than a scientific issue of truth I suppose.
UPDATE: Kate over at SDA created a collage over time showing the snow of Mt. Kilimanjaro:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1177354',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHighly anomalous terrain (an active volcano), 40 years of cooling temperatures, and a CO2 record that dramatically contrasts with fluctuating values from forests and meadows reaching 600-900 ppm all beg the question: Is Mauna Loa’s CO2 record globally representative?
Mauna Loa is the Earth’s largest land volcano. It has erupted over 3 dozen times since 1843, making this terrestrial landscape extremely unusual relative to the rest of the globe’s terrain. (Forests, in contrast, cover over 30% of the Earth’s  land surface.)
Mauna Loa has been thought to be the world’s best location to monitor global CO2 levels since 1958.
While Mauna Loa CO2 levels show a rise of 338 ppm to 415 ppm since 1980, Mauna Loa temperatures (HCN) show a cooling trend during this same time period. The only warming period in the last 65 years occurred between about 1975 and 1985.

Image Source: oz4caster
Forest CO2 fluctuations
As mentioned above, forests are orders of magnitude more terrestrially representative than the highly anomalous site of the Earth’s largest volcano.
In forests or tree-covered areas, CO2 rises from around 300 ppm in the warmth of the afternoon (~3 p.m.) to over 600 ppm before sunrise (~4 a.m.), when it is cooler (Fennici, 1986, Hamacher et al., 1994). This massive fluctuation occurs daily and CO2 values average out to be far higher than the Mauna Loa record suggests.

Image Source: Fennici, 1986

Image Source: Hamacher et al., 1994
Meadow CO2 fluctuations
In open fields, or meadows, air CO2 can vary between 266 ppm and 1,430 ppm. The average variance is from 280 ppm to 980 ppm 2 meters above the soil (Szaran et al., 2005).
Interestingly, just as in forests, temperature drops of 4 to 5°C are associated with rising levels of CO2.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Szaran et al., 2005
CO2 beneath snow and ice
Modern CO2 concentrations beneath snowpack and ice range from 600 to 1800 ppm. These concentrations can fluctuate by as much as 200 ppm within a period of just 4 days (Massman and Frank, 2006).
If this kind of rapid and wide-ranging variability can be observed for modern conditions, our capacity to accurately assess the “global” CO2 concentration for ice and snow thousands of years old becomes all the more suspect.

Image Source: Massman and Frank, 2006
CO2 near cave entrances
Within caves, CO2 levels can reach as high as 30,000 ppm. Even in the open air <1 meter from the entrance to a cave, CO2 levels can reach 11,500 ppm (Cowan et al., 2013).
CO2 levels vary by 10s of 1000s of ppm from one cave to the next in the same geographical region.

Image Source: Cowan et al., 2013
Mauna Loa CO2 is globally representative?
Cave entrances should probably be considered no less terrestrially unusual than the site of the world largest active volcano. And certainly forests and meadows are far more representative of the Earth’s terrestrial landscape than the Mauna Loa site.
And yet it has been decided, via consensus, that the rarified air above a Hawaiian island in the middle of the Pacific correctly monitors the CO2 levels for the entire globe.
Why?


		jQuery(document).ready(function(){
			jQuery('#dd_ec2a615e01c9d5f09bc7e5a514a859b6').on('change', function() {
			  jQuery('#amount_ec2a615e01c9d5f09bc7e5a514a859b6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Alengthy new papal encyclical is being rolled out today. A version of _Laudato Sii_ , or “Be Praised”—thought by most observers to be final, though the Vatican said otherwise—was leaked on Monday. It is a highly political discussion of the theology of the environment.



In fact, Pope Francis addresses not just fellow Catholics but “every person who inhabits this planet,” with whom he proposes “to enter into discussion… regarding our common home.” Climate change is high on his list. With the UN pushing a new agreement for December, Christiana Figueres, head of the UN Climate Change Secretariat, exulted that the encyclical “is going to have a major impact.”



It’s a difficult document to critique, especially since the release was in Italian, so the English versions circulating are poor online translations. Nevertheless, _Laudato Sii_ mixes heartfelt concern for the status of the environment and man’s connection with the world around him with an often limited or confused understanding of the problem of pollution and meaning of markets. The document also wanders widely, connecting most every human endeavor, from drug consumption in affluent societies, architecture, overcoming the “barriers of selfishness,” and cultural homogenization to the environment. Indeed, contended the Pontiff, “The disappearance of a culture can be as serious as or more than the disappearance of an animal or plant species.”





The new papal encyclical understands man and religion, but not economics and politics.



Moreover, the document mixes the indubitable and the dubious. Pope Francis rightly worries about the quality of life, “extreme consumerism,” and meaning in people’s lives. He also highlights God’s concern “for the poor and abandoned,” evident throughout Christian Scripture. As for the environment, he noted, “to insist in saying that the human being is the image of God should not make us forget that every creature has a function and nothing is superfluous.”



Despite his commitment to ecological values, the Holy Father acknowledges that “a return to nature cannot be at the expense of freedom and the responsibility of the human being, that is the part of the world tasked with cultivating its ability to protect and develop their potential.” He also rejects “deification of the earth, which would deprive us of the call to collaborate with it and protect its fragility.”



Nevertheless, humanity’s responsibility for the environment is complex and the Pope discusses ecological values in the context of economic development and care for the poor. How to creatively transform but at the same time gently preserve the natural world is not easy. Unfortunately, in its policy prescriptions _Laudato Sii_ sounds like it was written by an advocate, largely ignoring countervailing arguments. The resulting factual and philosophical shortcomings undercut the larger and more profound theological discussion.



For instance, the encyclical begins by referring to “the deteriorating global environment.” In fact, “the environment” is not a single thing. There are a host of environmental issues which vary dramatically across continent, country, region, and locality. The Pope warns about “the depletion of natural resources,” yet most resources, such as oil, have been growing relatively more abundant. This reality doesn’t negate the Pope’s insistence that “destruction of the human environment is something very serious,” but affects the application of his injunction.



Worse, the document complains much of capitalism, one of the “correct models of growth that seem incapable of guaranteeing respect for the environment,” as well as property rights, which, in the Pope’s view, allow selfish individuals to act in their individual rather than the public’s interest. In fact, no system guarantees respect for ecological values. Communism was the worst: the state controlled everything and the party demanded industrialization. Analysts referred to “ecocide” in the Soviet Union and Eastern Bloc. China looks little better.



In contrast, capitalism provides the resources and technology to improve environmental protection. Indeed, the Holy Father acknowledges that “science and technology are a wonderful product of human creativity that is a gift from God.” Of course, such advances offer no panacea, he warns. Nevertheless, wealthier societies produce more efficiently. They deploy better tools to cope with ecological ills. They are freer and allow people to demand political change.



Indeed, prices in a marketplace operate as signals. _Laudato Sii_ complains that disproportionate consumption steals “from poor nations and future generations,” and that “the rate of consumption waste and degradation of the environment has passed the possibilities of the planet, in such a way that the current lifestyle, being unsustainable, may only result in disaster.” No evidence of this claim is provided. In fact, rising resource prices encourage people to use less, producers to find more, manufacturers to operate more efficiently, and entrepreneurs to create substitutes. Claims that humanity was running out of resources and destroying the ecology go back centuries and so far have been proved wrong.



Markets also do a good job of comparing the costs and benefits of different means to achieve a common end. Costs matter, and not just to big corporations, as the encyclical suggests. For the poor environmental protection can be an unaffordable luxury. _Laudato Sii_ well describes the importance of work, but jobs are not created, like the earth, _ex nihilo_. The more regulatory dictates, higher energy prices, greater supply costs, and more, the fewer the jobs and the lower the salaries.



When it comes to solving specific problems, markets can be quite helpful. For instance, the document complains about water shortages and then criticizes the “tendency to privatize this scarce resource.” Yet monopoly public utilities are renowned for providing poor service at high cost. The poor’s lack of “access to drinking water” has much more to do with Third World poverty and government incompetence than privatization.



Most curious is _Laudato Sii_ ’s almost angry attack on emissions credits, which “can give rise to a new form of speculation and would not help to reduce the global emission of polluting gases.” Yet well‐​designed tradeable permits, like emission taxes, encourage those who can control emissions at the least cost to do so the most. This is not an assault “on the solidarity of all peoples,” as the document proclaims, but a means to most help those who have the least to give.



Moreover, markets and property rights are the most important means to provide people with what the Pontiff calls “a dignified life through work.” Even Marx acknowledged that capitalism raised the mass of humanity out of immiserating poverty. Commercial society destroyed the foundations of aristocratic oppression.



Alas, the document offers a confused, misguided criticism of mechanization and economies of scale. It was the new “machines” replacing jobs, decried by the encyclical, which created vast new economic and social opportunities. The Gutenberg Press put scribes out of business, while computers transformed whole industries.



 _Laudato Sii_ asserts the “principle of subordination of private property to the destination” and the “social function of any form of private property.” Property rights may not be absolute, but the legal right to land is most important for those who lack wealth and influence. The lack of such rights in the kleptocratic systems in Latin America with which the Holy Father is so familiar hampered the entrepreneurial poor, a phenomenon highlighted by Hernando de Soto.



Property rights also create incentives for environmental stewardship. Garrett Hardin famously wrote about the “tragedy of the commons,” how public ownership naturally leads to environmental degradation. Ownership vests both costs and benefits with a sole decision‐​maker who can be held responsible. Where the public “owns” land no one effectively does so.



That’s why many countries have created quota systems, creating a form of defined development right, to govern ocean fishing. In the case of the Amazon rain forest, mentioned by the Pontiff, indigenous peoples lacked formal legal rights to the land they used. The problem is _lack of_ property rights.



Most environmental problems occur because of what economists call externalities—costs and benefits that fall on others. For instance, Pope Francis speaks of “the obligation of polluters to take responsibility economically” and “assess the environmental impact of each work or project.” Without an appropriate legal regime, industry could spew emissions far and wide, the antithesis of property rights. The real environmental issue is over where to draw the line, which requires balancing complex interests: prosperity, liberty, ecology. _Laudato Sii_ seems to assume the correct outcome in every case is more of the latter.



Indeed, the encyclical lacks any sense of the flawed nature of government. The Pope is disappointed that environmental efforts “are often frustrated not only by the refusal of the powerful, but also by the lack of interest of the other.” However, public choice economists diagnosed this problem decades ago: concentrated benefits, diffuse costs. In this case environmental organizations are as prone as corporations to push their narrow preferences on everyone else.



This reality raises doubts about the Pope’s endorsement of the “precautionary principle,” which in practice would hold virtually every beneficial human innovation hostage to interest groups dedicated to the status quo. Equally dubious is the encyclical’s endorsement of the “essential development of international institutions stronger and effectively organized… with the power to sanction.” There is no reason to believe that ever more distant, unaccountable bureaucracies will operate for the common good rather than at the behest of whatever interests, corporate, labor, activist or other, wielding the greatest influence. In fact, the encyclical complains of the failure of global conferences due to “too many special interests.”



Politics also is far more open to the Holy Father’s complaint about “the earth’s resources” being “plundered due to ways … too tied to the immediate result.” The price of property incorporates perceived future value. Ruin it and you lose that value. Politicians’ decision‐​making time frame usually is years, often months. If opening up sensitive land for development will win a few votes in the next election, why worry about future generations, which don’t vote? Those in politics may talk the talk, but in practice they are no less selfish and neglectful than those in business, and respond to far more destructive incentives.



The encyclical includes a confusing discussion of trade and globalization. External debt “has become an instrument of control.” True, yet local political leaders borrowed much and wasted the proceeds. The document also claims that trade relations forbid “access to ownership of property and resources to meet [people’s] vital needs.” However, trade mandates and forbids nothing. Rather, it provides opportunities which, like industrialization, might not offer an easy path for the poor, but which usually are better than the alternatives. That is why polls consistently show the greatest support for globalization in the poorest nations.



 _Laudato Sii_ also argues for redefining progress, contending that “diversification of a production more innovative and with less environmental impact, can be very profitable.” If true, it will happen without legal mandate.



Yet the Pope argues that it is not sufficient to care for nature while enjoying financial profits, or practicing “environmental conservation with progress.” Without evidence the encyclical contends that this will only mean “a small delay in the disaster.” However, past doomsayers consistently have been proved wrong. The Pontiff certainly is right to question “technological and economic development that does not leave a better world and quality of life.” However, compare the lives of the average person, and especially poor person, today with a century ago and a century before that. The world and quality of life are dramatically better. The Holy Father should encourage people to ask, “How much is enough?” But it is important that those living in comfort in the industrialized West do not try to answer for those living in the impoverished Third World.



Although the Vatican often is treated as an independent state, its comparative advantage is not legislation. Yet at one point the encyclical discusses “household waste and commercial, demolition debris, clinical waste, electronic or industrial waste.” Also noted is the “special challenge” presented by “marine debris and the protection of marine areas.” Later the document asserts the importance of education on “how to avoid the use of plastic material or paper,” “cooking only what you can eat reasonably,” and turning off “unnecessary lights.” Indeed, there is a lengthy but not entirely fruitful discussion of urban planning, highlighted by the professed need to improve urban transportation.



The discussion of climate change is similarly specific but partisan. For instance, the encyclical takes an almost panicked view of the problem, even though it closes out the chapter noting the Church’s obligation to “listen and promote debate honest among scientists, respecting the diversity of opinion.” _Laudato Sii_ also blames extreme weather events on climate change while admitting in the same sentence “that we cannot attribute a cause scientifically determined for each particular phenomenon.”



The fact that there likely will be more warming does not mean it will be catastrophic. In fact, models failed to accurately predict past behavior, peer‐​reviewed research increasingly suggests warming toward the lower range, and it is impossible to accurately predict events a few years, let alone a century hence (virtually no one saw the Shale gas/​oil revolution coming, for instance). This argues against making draconian, expensive changes that may make little sense soon after they are implemented. Rather, it would be better to adapt to particular problems as they arise rather than to attempt to hold down temperatures by radically rolling back energy use. The resources saved are needed to meet many other human needs.



In contrast, the Pontiff truly is acting in his unrivaled role as spiritual leader when he advocates a personal, social, and spiritual transformation in how people relate to the environment. He promotes an “ecological spirituality that arises from convictions of our faith” and advocates human freedom being “put at the service of another kind of progress, healthier, more human, more social and more integral.” The Pope’s proposed “ecological conversion” should spark much discussion, since his application of basic Christian principles is plausible, if not necessarily convincing. Is it really true that the same principle of “brotherly love” requires “us to love and accept the wind, the sun or the clouds”? Nevertheless, throughout history too many Christians probably have practiced dominion and slighted stewardship. Even this untutored Protestant can appreciate the claim that “the Eucharist is even light source and motivation for our environmental concerns and directs us to be guardians of all creation.”



Moreover, Pope Francis warns that “we cannot think that the political agendas or the force of law is enough to avoid behaviors that affect the environment,” since the culture itself is corrupt. He contends: “if we feel intimately united with all that exists, sobriety and care will arise spontaneously.” Quite true. It is committed individuals who form the “innumerable variety of associations advocating on behalf of the environment,” cited by _Laudato Sii_ , and whose reformed buying behavior can “change the behavior of firms, forcing them to consider the environmental impact and production patterns.” Aligning desire and incentive is the best way to achieve what the Pontiff’s objective.



The Pope expresses the triumph of hope over experience in his call on politicians to act responsibly and the public to participate knowledgeably. Just as one should avoid “a magical concept of the market,” so should we beware the same for politics. Problems will not be magically resolved by investing politicians and bureaucrats with vast new powers.



Larger themes point through the encyclical, which warns that “the market alone does not ensure human development and full social inclusion.” The Gospel, unlike the market, reaches the empty hearts which the Pope sees. We must “not give up asking us questions about the purposes and on the sense of everything.”



The Vatican is not well‐​positioned to assess environmental problems and develop policy solutions. Rather, the Pontiff’s duty is much more fundamental: “the great wealth of Christian spirituality, generated by twenty centuries of personal and communal experiences, is a magnificent contribution to make the effort to renew humanity.” We desperately need such a renewal. Hopefully _Laudato Sii_ , despite its practical shortcomings, will advance the larger and more important theological mission.
"
"
Share this...FacebookTwitterAs the globe has warmed since the end of the Little Ice Age, alarms concerning retreating glaciers have been sounded worldwide. The reason for the warming remains hotly disputed: alarmists blame it on manmade CO2 while skeptics say natural factors are just as much at play, if not more so.

Image: Norwegian glace, for illustration purpose. Source: NASA/John Sonntag, public domain.
Very little retreat in Norway this past summer
Yesterday Norwegian NRK here reported “several of the largest glaciers have almost not shrunk” during this past summer.
“This year, several places in the country have almost not shrunk,” according to the Norwegian NVE.
Since 1962 experts have been monitoring the Nigardsbreen glacier, an arm of Jostedalsbreen located in Vestland county.  The summer of 2020 has seen the sixth slowest result in about half a century. “If we get more such summers to come, then the glacier front will grow forward again,” says Even Loe in Statkraft.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The glacier is named after the farm Nigard, which was crushed by the glacier in 1748. At that time the front of the glacier stopped about 4.5 km further ahead than it is today,” reports the NRK.
Experts attribute this past summer’s stagnation to “a good winter with a lot of snow.”
“The Nigardsbreen glacier has actually grown bigger.”
Glaciologist Hallgeir Elvehøywhich said the glacier retreated 4 meters, “something that is very small compared to previous ones.”
“The trend is largely the same elsewhere in the country,” he says.
Although many glaciers have decreased relatively little this year, the Norwegian experts still remain pessimistic about their future, should warming continue as the models project. “But in all the gloom, there is also a small glimmer of light, should the rainfall continue.”
“There is nothing in the way that the climate system can give us several years with so much snow, and then it will have an effect.”


		jQuery(document).ready(function(){
			jQuery('#dd_d715d3c01e279f99b82ff2c7fb93a8ba').on('change', function() {
			  jQuery('#amount_d715d3c01e279f99b82ff2c7fb93a8ba').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Kyushu region of Japan has been getting lots of rain lately, which has led to flooding and 62 reported deaths because local officials failed to properly heed warnings to evacuate.
The New York Times, however, blames it all on the “collision” of “demographic change and global warming” instead of incompetence by local authorities.
NYT claims: “More torrential rains”
“In recent years, climate change has spurred more torrential rains in Japan, causing deadly flooding and mudslides in a nation with many rivers and mountains,” reports Motoko Rich of the New York Times (NYT).
Unfortunately the NYT neither provides data nor cites any study showing this to be the case.
So we look at Japan precipitation trends going back decades – something the NYT journalists obviously neglected to do themselves (or they did, but then decided not to bring it up).
JMA data tell a whole different story
Using data from the Japan Meteorological Agency (JMA), Japanese blogger Kirye first plotted the annual precipitation anomaly for Japan since 1898:

Data source: here
The plotted data above show how Japan’s precipitation has indeed trended downward somewhat since 1898, and not risen.
The most recent decade in Japan has seen precipitation levels similar to that of the 1950s, and the very early part of the 20th century. Nothing unusual is happening here.
Next we look at the JMA annual precipitation data for Hitoyoshi, Kumamoto Prefecture itself since 1948:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: here
Here we observe very little trend change at all. The recent rainfall in the region over the past decade has been similar to that seen in the 1950s. Extremes are no more intense or frequent today than they were in the past.
No July precipitation trend changes
Going a step further into greater detail, we look at the southern Japan region’s rainfall for July, going back to 1948.

Data source: here
Also the here we have the same story: no major trend changes to speak of. Very wet July months in fact were just as frequent and more intense in 1950s.
Decreasing hot days
Finally we look at the region’s number of days recording a temperature of 30°C or higher.

Data source here
Here as well the REAL trend is in fact bucking what the NYT always likes to suggest: increasingly more hot days because of global warming.
Incompetent authorities after all
Later in the article, Rich does ultimately get around to the real cause of the recent flooding deaths in Kyushu: “The Japanese government issues standardized evacuation protocols, but they do not take into account the unique characteristics or terrain in different parts of the country, said Professor Tsukahara of Kyushu University.”
But all in all, very shoddy journalism here by the New York Times. They would do their readers a service by checking the data instead of lazily repeating old, exaggerated narratives.


		jQuery(document).ready(function(){
			jQuery('#dd_50a36dd65648004a6024adc66f4c2a06').on('change', function() {
			  jQuery('#amount_50a36dd65648004a6024adc66f4c2a06').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

No doubt about it, it’s been a good month for tornadoes even by the “spinny” standards of May, when most twisters occur. Even more predictable than the development of severe storms in spring, however, is the phenomenon of people trying to tie such bad weather to global warming. Witness Tom Toles’s cartoon in the May 7 Washington Post, which intoned, “These superpowerful tornadoes are the kind of storm we’re likely to see more of with global climate change.” Who’d he get that from, Al Gore?



It has become Standard Operating Procedure in climate change hype to never bother with inconvenient facts. Tons of tornado data are only a few mouse‐​clicks away. And they show that Toles was dead wrong in his implication that the recent storms show any link to the slight warming of the atmosphere that has occurred in recent decades. In fact, just the opposite may be occurring despite a perception of increased storminess.



Two interesting facts: The number of reported tornadoes has increased for decades while the number of deaths has dropped.



What’s going on is called “radar.” Thanks to an awful 1953 tornado in Worcester, Massachusetts (far from the Oklahoma and Texas “tornado alley”), the Weather Bureau (today’s National Weather Service) went on a crash program to develop a national network of weather radar. Spearheaded by David Atlas and Ted Fujita (whose “F‐​scale” rates tornado severity on a 1–5 basis, as is done for hurricanes), meteorologists soon learned that when the radar paints a thunderstorm that looks more like a comma than a blob, there’s often a tornado buried in the curliest point.



It took several years for the original radars, known as WSR-57’s, to cover the country. But by 1970 the job was nearly complete. As more radars came online, more and more tornadoes were reported. It’s interesting that as this network stabilized, from 1970 through 1990, so did the number or tornadoes.



Beginning in 1988, a new network began to take shape that was even better at detecting potential twisters. Instead of painting a picture of a thunderstorm, the new machines, called Doppler radars and designated as WSD-88’s, actually measure the change in a storm’s velocity by tracking the movement of raindrops. When those drops start to rotate, it’s not long before there’s a tornado warning. The rotation fields often develop before the comma shape, which means more tornado warnings. This gets people’s attention, and saves more and more lives. Not surprisingly, the number of tornadoes increased again, in the 1990’s this time proportional to the number of WSD-88’s, which now blanket the nation. By the beginning of this century, with the new network now in place, the number has stabilized again.



Any reporter (or cartoonist) doing his homework might have asked if indeed the number of big storms (categories 3–5 on the Fujita scale) is increasing. The fact is that the vast majority of tornadoes are in the “weenier” classes. Only about 5 percent reach category 3 or higher. (The severity data is at  http://​www​.spc​.noaa​.gov/​a​r​c​h​i​v​e​/​t​o​r​n​a​does/. Click on, graph it up, and you’ll see that the number of severe tornadoes is dropping.)



Where does the notion that tornadoes must increase because of global warming come from? Another panel in the Post’s cartoon reads: “With energy added to the atmosphere, more frequent and intense storms are a probable outcome.”



Perhaps a refresher course in high school earth science might be in order here. Tornadoes occur because a portion of a normally quiescent thunderstorm begins to spin. That spinning is done in large part by a dip in the strong westerly winds (“jet stream” in common parlance) that sometimes penetrates the U.S. when thunderstorms are common. The jet stream is the result of the temperature contrast between the poles and the tropics. Global warming reduces this contrast (warming the poles much more than the tropics) and reduces the spin. That means fewer tornadoes, not more.



Obviously, it’s a lot hotter in June, July, and August than it is in the peak of the tornado season in May. So much for the hot‐​air‐​tornado link. And why are there so many tornadoes in Mississippi in February? 



Rather, the key ingredient that spins garden‐​variety thunderstorms into killer tornadoes, the jet stream, is missing during the hottest part of the year, having migrated north to Canada for the summer. Warm it up and the migration will start earlier, it will move further north.



That may explain why the number of severe tornadoes is declining. They may be running out of spin, unlike stories attempting to relate these destructive storms to global climate change. 
"
"
Note: I don’t normally allow the discussion of things related to Nazi Germany here, including discouraging the use of the word “denier” due to it’s “Holocaust Denier” connotations. But this full page ad in the Sunday papers in Britain, touting “climate crime” and “climate cops” is just a bit over the top, and deserves some attention. It is particularly relevant since the sponsoring website climatecops.com has a teachers section, and we’ve just seen some sensibility from Schwarzenegger in Sacramento on this very issue. I find this method of indoctrinating school children to normal everyday living being harmful to the earth with the “climate crime” connotation as distasteful and wrong headed. I have no problems with energy conservation, in fact I encourage it. But combining  such advice with a “climate cop” idea is the wrong way to get the message across. Can you imagine what sort of reaction the neighbors will have to the kids hanging this door hanger on their front door? Will the result of this now be hiding your electric dryer behind false walls so the kids and neighbors don’t see it?
At the very least, npower could have chosen a different color scheme: red, black and white are the same three colors used in the flag of Nazi Germany What were they thinking? – Anthony
Reposted from the website EU referendum:

Can I be the only one more than a little disturbed by the latest campaign to be fronted by energy company npower?
Launched today with large colour ads in the Sundays, it appeals directly to children, urging them to enlist as “climate cops”, to root out “climate crimes“, and thus “save the planet”.
In a luridly-designed website, mimicking the style of “yoof” cartoons, it offers a bundle of downloads, including a pack of “climate crime cards“, urging its recruits to spy on families, friends and relatives, inviting each of them to build up a “climate crime case file” in order to help them ensure their putative criminals do not “commit those crimes again (or else)!”
Quite what the “or else!” should be is not specified, but since the “climate cops” are being encouraged to keep detailed written records (for those who can read and write), there is nothing to stop these being submitted to the “Climate Cops HQ” for further sanctions, the repeat offenders being sent to re-education camps. And for those “climate cops” that successfully perform the “missions” set (or turn in their own parents), there is the reward of “training” in the “Climate Cop Academy”.
In a system which has echoes of Hitler’s Deutsches Jungvolk movement, and the Communist regime Pioneers, perhaps successful graduates can work up to becoming block wardens, then street and district “climate crime Führers”, building a network of spies and informers.
How nicely this ties in with James Hansen’s call to put the chief executives of large fossil fuel companies on trial for high crimes against humanity and nature, accusing them of actively spreading doubt about global warming.
No doubt, with a willing band of “climate cops”, the prosecutors can spread their nets wider, reaching into the homes of all climate change deniers, until the insidious virus of doubt is exterminated (final solution, anyone?). Then we can all march on the sunlit uplands of a “carbon-free” planet – to the tune of Ode to Joy no doubt.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9df59f46',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterPrior to the transition from the last ice age to the current interglacial climate, when CO2 levels still lingered below 250 ppm, the relative sea levels in southern Greenland were “at least ∼32 m above present.”
Relative sea levels have undergone a series of major changes since the last glacial maximum, when global sea levels were 120 meters below today’s.
Sea levels rose at rates of up to 60 or 70 millmeters per year (6 to 7 meters per century, Tanabe, 2020) from about 12,000 to 8,000 years ago. Most of the globe experienced sea level high stands of 2 or 3 meters above present between about 7,000 to 5,000 years ago (King et al., 2020, Lopes et al., 2020, Martins et al., 2020).
But a new study (Steffen et al., 2020) proposes relative sea levels instead peaked at 32 meters above today’s levels in Nanotalik (southern Greenland) during the latter stages of the last ice age (13,800 years ago).

Image Source: Steffen et al., 2020

Image Source: Tanabe, 2020

Image Source: King et al., 2020

Image Source: Lopes et al., 2020

Image Source: Martins et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_0e946332d5df286d97dcf4a1c7d65536').on('change', function() {
			  jQuery('#amount_0e946332d5df286d97dcf4a1c7d65536').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently Hollywood director James Cameron participated in a panel discussion together with economist Tom Friedman, actress Sigourney Weaver, and MSNBC’s Joe Scarborough. Cameron called Climate Change ‘As Great As The Threat’ U.S. Faced in World War II. To me, watching some Hollywood stars give advice on saving the planet is like watching lumberjacks talk about how to do heart surgery. In both cases the result is the same: the patient dies.
http://www.cnsnews.com/news/article/64516
Cameron’s humble abode
Climate protection starts at home. According to more than one source. Cameron’s humbe abode has 8,272 square feet and boasts 6 bedrooms, 7 bathrooms, pool, tennis court and inner courtyard fit for Hollywood royalty – perfect to sit around in with like-minded stars and worry about the “dying planet”, all the while the private jet sits on the tarmac getting refuelled and the chauffeured limousine waits outside.
http://realestock.com/joomla/blog/Inside-the-Homes-of-2010s-Oscar-Nominees.html (scroll down)
Making everyone miserable
Perhaps Cameron’s arrogance, hypocrisy and insistence the rest of the working masses sacrifice more while he continues to live high on the hog has something to do with a character disorder.  According to Wikipedia collaborator and author Orson Scott Card calls Cameron, who has been married five times, selfish and cruel. He said working with him was “hell on wheels”.  Card added, “He was very nice to me because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way”.
Cameron’s real estate visions: community of a half dozen estates
http://www.therealestatebloggers.com/2007/06/03/james-cameron-selling-730-acres-in-malibu-near-pepperdine-university/
Share this...FacebookTwitter "
"

As the noted social critic H. L. Mencken once declared, “The whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



In _A Dangerous World? Threat Perception and U.S. National Security_ , a new book edited by Christopher A. Preble and John Mueller of the Cato Institute, a number of scholars ask to what degree the United States is threatened, examining not only the multiplicity of supposed dangers, but also the wisdom or folly behind the measures that have been proposed to deal with them. Paul Pillar, a visiting professor at Georgetown University’s Center for Security Studies, observes that “substate” threats — from terrorism to crime — pale in comparison to traditional threats posed by states. He urges readers to reexamine their preconceived notions of what is required to keep the United States both safe and free. Noting a disconnect between the severity of threats and how much alarm they generate, he voices dismay at the tendency to identify monsters abroad and “conceive of America’s place in the world largely as one of confrontation against them.” In the end, he cautions that the capacity of the United States to curb substate conflict is usually very limited. In fact, intervention itself can be counterproductive.



Eugene Gholz, associate professor of political science at the University of Texas at Austin, explores the economic effects of warfare. Although war itself has many highly undesirable effects, he finds that overseas tensions do not necessarily harm the U.S. economy. Daniel Drezner, professor of international politics at Tufts University, finds some merit in the global public good provided by overwhelming U.S. primacy. However, it is not obvious that military power is the primary driver of the benefit, while sustaining it comes at great cost.



Other contributors include Peter Andreas of Brown University on “Transnational Crime as a Security Threat,” Martin Libicki of the RAND Corporation on “Dealing with Cyberattacks,” and Mark G. Stewart of the University of Newcastle on “Climate Change and National Security.” Although the world will never be free from dangers, we should aspire to understand them clearly. By chipping away at the common perception that the world is getting more dangerous each day, the contributors to this volume attempt to tame the tendency to overreact.
"
"

Most economists once believed that monetary policy should aim at achieving full employment, but we now know that holding unemployment below its natural rate has dangerous consequences. Could it be that another supposed economic ideal — zero inflation — is similarly wrong‐​headed?





In his 1997 book _Less Than Zero_ , Cato’s George Selgin first made the case for allowing price levels to vary to reflect changes in productivity. Now, a new edition of _Less Than Zero_ from the Cato Institute updates this important and prescient argument for 2018.



  
In the introduction for this edition, Scott Sumner of the Mercatus Center at George Mason University makes the case that Selgin’s book was “ahead of its time” and that it is time to return to Selgin’s argument for a productivity norm, where prices would rise or fall inversely to changes in productivity.



Selgin himself, however, contends that his idea is not entirely innovative; over the course of his research, he found that similar arguments have been made by other early 20th‐​century economists he admired. “Eventually, it became clear to me that — far from being novel — my understanding of deflation had once been almost orthodox, having been shared by prominent economists of many different schools of thought, only to be flung aside in the wake of the Keynesian revolution,” he writes. Two decades after its first publication, the ideas in _Less Than Zero_ are no longer as radical as they once were, as more economists are arguing for nominal income targeting and speculating about the possibility of “good” deflation. With that new climate in mind, this edition revisits these important and thought‐​provoking ideas.
"
"
This from www.Spaceweather.com
As January comes to an end, sky watchers in Scandinavia are recovering from a veritable storm of nacreous clouds. After mid-month, hardly a night went by without someone spotting the phenomenon. “It was incredible! They were all over the sky,” says Morton Ross of Oslo, Norway. This picture, taken by Ross on Jan. 25th, shows a typical apparition:

Also known as “Mother of Pearl” clouds, nacreous clouds are peppered with tiny ice crystals that blaze with iridescent color when struck by light from the setting sun. It is these crystals that make nacreous clouds so rare: they require exceptionally low temperatures of minus 85 Celsius (-120 F) to form. Icy nacreous clouds float 9 to 16 miles high, curling and uncurling hypnotically as they are modulated by atmospheric gravity waves.
For much of January, these clouds rolled across the Arctic circle with puzzling regularity. Why the sudden abundance? Is the show over? No one knows. Stay tuned for February!
For more, see the 2008 Nacreous Cloud Gallery For the science behind nacreous clouds, please see this entry in Atmospheric Optics.
As for temperatures at high latitudes, its -35°F in Saskatoon at the surface this morning, so there’s a chance we’ll see more nacreous clouds in days ahead.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea13cef97',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I am Paul C. Knappenberger, Assistant Director of the Center for the Study of Science at the Cato Institute, a nonprofit, non‐​partisan public policy research institute located here in Washington DC, and Cato is my sole source of employment income. Before I begin my testimony, I would like to make clear that my comments are solely my own and do not represent any official position of the Cato Institute.



For the past 25 years, I've conducted research on topics of climate and climate change including hurricanes, heat-related mortality, and temperature trends as well as worked to quantify the projections of human-caused climate change.



This last topic, specifically how it relates to the proposed Keystone XL pipeline, will be the subject of my testimony.



When I refer to climate change in these remarks, I am specifically referring to that climate change which may occur as a result of human emissions of greenhouse gases, primarily carbon dioxide. Climate change may (and does) occur from other influences as well, both human and natural. But the primary concern raised over the Keystone XL pipeline involves the carbon dioxide emissions resulting from the burning of the oil that the pipeline will carry. So it is the potential climate change from these emissions that will be focus of my testimony.



In its Draft Environmental Impacts Statement (DEIS), the State Department has done a good job in quantifying the extra emissions that result from the extraction, transportation, refining, and eventual end use of the oil which will be transported by the Keystone XL pipeline. They find, and I think that there is broad agreement on this point, that a barrel of oil produced from the Canadian tar sands has about a 17 percent carbon dioxide emissions premium compared to the average barrel of oil finding its way into the U.S. market.



The emissions premium primarily arises from the relatively energy-intensive manner in which tar sands oil is currently extracted. In the DEIS, the State Department points out that this emissions premium may well shrink over time as new extraction methodologies are developed, as extraction in other regions, such as Saudi Arabia, becomes more energy intensive, or depending on the type of oil that is ultimately displaced by the oil carried by the Keystone XL pipeline.



The disagreement between the State Department, the Environmental Protection Agency, and several environmental groups, involves how many new carbon dioxide emissions this current 17 percent per barrel premium results in when applied to the 830,000 barrels of oil that the Keystone XL pipeline will carry each day when operating at full capacity.



The State Department concludes that the demand for the tar sands oil is great enough that it will come to market whether or not the Keystone XL pipeline is ever built. It thus finds very few additional carbon dioxide emissions resulting from the pipeline project—somewhere in the range of an additional 0.1 to 5.3 million metric tons of carbon dioxide emissions per year over the case where the pipeline is not built.



The EPA contends that the State Department is too quick to come to such a conclusion. The EPA suggests that without the pipeline, much of that oil will remain in the ground. Therefore, if the pipeline were to be built, oil would be produced from the tar sands to meet its capacity. While this won't result in more oil being used in the U.S., it will result in a 17 percent carbon dioxide emissions premium applied to the 830,000 barrels per day of delivered oil. The EPA cites an extra 18.7 million metric tons of carbon dioxide emissions per year over the situation of no pipeline.



Several environmental organizations take the view that while the Keystone XL pipeline may not increase the amount of oil used in the U.S., the oil that it displaces from the U.S. market will be consumed by other countries as the global demand for oil continues to grow. Thus, they calculate the full emissions from the 830,000 barrels per day plus the 17 percent emissions premium, and arrive at an additional 181 million metric tons of carbon dioxide emissions per year resulting from the existence of the Keystone XL pipeline.In terms of carbon dioxide emissions, these differences may appear large and contentious, and, in fact, much of the protestation involving the Keystone XL pipeline focuses on these emissions numbers.



But, these protests are largely misplaced.



It is very important to keep in mind that the end game is climate change and the potential need of climate change mitigation. _Carbon dioxide emissions are not climate change_. They influence climate change, but they are not a measure of it.



Therefore, before any type of assessment as to the potential climate impact of the Keystone XL pipeline can be made, it is essential to translate the additional carbon dioxide emissions that may result from it into climate units—such as the global average temperature. In other words, how much global warming will the Keystone XL pipeline produce?



Isn't that what everyone wants to know?



Why is it then, that such numbers are never given?



It is not as if there is no good way of calculating them—that is precisely what climate models are designed to do. These complex computer programs emulate the earth's climate system and allow researchers to change various influences upon it—such as adding additional carbon dioxide emissions—and seeing what the end effect is. These climate models produce the projections of future climate change from human activities that we are all familiar with using precisely this methodology.



General circulation climate models are very complex and computational expensive to run (both in time and money) and as a result have not been used to generate the global temperature effects of the Keystone XL pipeline.



However, in lieu of running a full climate model, climate model emulators have been developed which can run on a desktop computer. One such program is MAGICC, the Model for the Assessment of Greenhouse-gas Induced Climate Change. MAGICC is a climate model simulator developed by scientists at the U.S. National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency and other organizations.



MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models that is designed to produce an output that emulates the output one gets from much more complex climate models. MAGICC can produce in seconds, on a personal computer, results that complex climate models take weeks to produce running on the world's fastest supercomputers. MAGICC doesn't provide the breadth of output or level of detail that fully resolved climate models do, but instead simulates the general, broader aspects of climate change such as the global average temperature.



Moreover, MAGICC was developed, according to MAGICC's website, ""to compare the global-mean temperature and sea level implications of two different emissions scenarios."" So, using MAGICC to compare the climate change that is projected to result from the different Keystone XL pipeline carbon dioxide emissions scenarios fits precisely into the program's designed purpose.



Using MAGICC, I (and anyone else) can calculate the potential impact of the Keystone XL pipeline on the global average temperature based on the various carbon dioxide emissions estimates, and produce results very similar to ones that would be achieved by using a full climate model. In the base case, I run MAGICC using a mid-range, business-as-usual future emissions scenario as defined by the IPCC (SRES A1B). To examine the climate change impact using the EPA's Keystone XL carbon dioxide emissions scenario, I add 18.7 million metric tons per year to the global carbon dioxide mission total each year beginning in the year 2010 and continuing through the year 2100. To assess impact of the emissions scenario preferred by some environmental organizations, I add 181 million metric tons of additional carbon dioxide emissions to the global total beginning in 2010 and extending through 2100.



When running MAGICC as described, I find that no matter how the additional carbon dioxide emissions are calculated, the Keystone XL pipeline has an exceedingly and inconsequentially small impact on projected the course of global temperature.



In the case of the State Department's analysis, as there are very few additional carbon dioxide emissions, there is essentially no associated change in the global climate. The change in global average temperature resulting from the EPA's additional 18.7 million metric tons of carbon dioxide emissions per year from the Keystone XL pipeline, would be about 0.00001°C per year—that is one one-hundred thousandths of a degree. The 181 million metric tons per year from the assumption that all Keystone XL oil is additional oil in the global supply would result in about 0.0001°C of annual warming—one ten-thousandths of a degree.



In other words, if the Keystone XL pipeline were to operate at full capacity until the end of this century, it would, worst case, raise the global average surface temperature by about 1/100th of a degree Celsius. So after nearly 100 years of full operation, _the Keystone XL's impact on the climate would be inconsequential and unmeasurable_.



And even these tiny numbers are probably overestimates. In calculating them, I used the MAGICC default value for the magnitude of the earth's equilibrium climate sensitivity. A value, 3°C, that was based on the assessment of the equilibrium climate sensitivity given by the Intergovernmental Panel on Climate Change (IPCC). The equilibrium climate sensitivity is the amount that the earth's surface temperature will rise from a doubling of the pre-industrial atmospheric concentration of carbon dioxide. As such, it is probably the most important factor in determining whether or not we need to ""do something"" to attempt to mitigate future climate change. The lower the climate sensitivity, the less the temperature rise from human carbon dioxide emissions, and the lower the urgency to try to reduce them. If the sensitivity is low enough, carbon dioxide emissions confer a net benefit.



And despite common claims that the ""science is settled"" when it comes to global warming, we are still learning more and more about the earth complex climate system—and the more we learn, the less responsive it seems that the earth's average temperature is to human carbon dioxide emissions.



For example, the observed lack of statistically significant temperature rise over the past 16 years (and counting), is strong indication that climate models have a tendency to overestimate the amount of warming resulting from human greenhouse gas emissions (Figure 1).





Figure 1. Current (ending in December 2012) trends in three observed global surface temperature records of length 5 to 15 years (colored lines) set against the probability (gay lines) derived from the complete collection of climate model runs used in the IPCC Fourth Assessment Report under the SRES A1B emissions scenario (Knappenberger and Michaels., 2013).



I was involved in research that we published more than a decade ago pointing out that global temperatures were not rising as fast as climate model expectations (Michaels et al., 2002), and increasingly, there is a growing acknowledgement of this fact.





Figure 2. Climate sensitivity estimates from new research published since 2010 (colored, compared with the range given in the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4) (black). The arrows indicate the 5 to 95% confidence bounds for each estimate along with the best estimate (median of each probability density function; or the mean of multiple estimates; colored vertical line). Ring et al. (2012) present four estimates of the climate sensitivity and the red box encompasses those estimates. The right-hand side of the IPCC AR4 range is dotted to indicate that the IPCC does not actually state the value for the upper 95% confidence bound of their estimate and the left-hand arrow only extends to the 10% lower bound as the 5% lower bound is not given. The light grey vertical bar is the mean of the 14 best estimates from the new findings. The IPCC's ""best estimate"" (3.0°C) is 50% greater than the mean of recent estimates (2.0°C).



Over the past three years, a collection of findings in the peer-reviewed scientific literature has suggested that the IPCC's best estimate of the equilibrium climate sensitivity is likely too high by nearly 50 percent. Instead of the IPCC's 3.0°C, the new findings are indicating a value close to 2.0°C (see Figure 2).



Rerunning the MAGICC climate model simulator with an equilibrium climate sensitivity setting of 2.0°C, instead of the 3.0°C default value, drops the calculated warming impact from the Keystone XL pipeline by about 30 percent.



It is this information, not the information on carbon dioxide emissions that is required to properly assess the climate change aspect of the environmental impact of the Keystone XL pipeline.



In these terms, the difference between the State Department's Environmental Impact Statement and those of its critics all but vanish.



No matter whose carbon dioxide emissions estimate is used to calculate it, the climate impact of the oil carried by the Keystone XL pipeline is too small to measure or carry any physical significance.  
In deciding the fate of the Keystone XL pipeline, it is important not to let symbolism cloud these facts.



 **References:**



Aldrin, M., et al., 2012. Bayesian estimation of climate sensitivity based on a simple climate model fitted to observations of hemispheric temperature and global ocean heat content. _Environmetrics_ , doi: 10.1002/env.2140.



Annan, J.D., and J.C Hargreaves, 2011. On the generation and interpretation of probabilistic estimates of climate sensitivity. _Climatic Change_ , **104** , 324-436.



Hargreaves, J.C., et al., 2012. Can the Last Glacial Maximum constrain climate sensitivity? _Geophysical Research Letters_ , **39** , L24702, doi: 10.1029/2012GL053872



Intergovernmental Panel on Climate Change, 2007. Climate Change 2007: _The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change_. Solomon, S., et al. (eds). Cambridge University Press, Cambridge, 996pp.



Knappenberger, P.C., and P.J. Michaels, 2013. Policy Implications of Climate Models on the Verge of Failure. American Geophysical Union Science Policy Conference. Washington, DC, June 24-26, 2013, submitted.



Lewis, N. 2013. An objective Bayesian, improved approach for applying optimal fingerprint techniques to estimate climate sensitivity. Journal of Climate, doi: 10.1175/JCLI-D-12-00473.1.



Lindzen, R.S., and Y-S. Choi, 2011. On the observational determination of climate sensitivity and its implications. _Asia-Pacific Journal of Atmospheric Science_ , **47** , 377-390.



Michaels, P.J., Knappenberger, P.C., Frauenfeld, O.W., and R.E. Davis. 2002. Revised 21st century temperature projections, _Climate Research_ , **23** , 1-9.



Ring, M.J., et al., 2012. Causes of the global warming observed since the 19th century. _Atmospheric and Climate Sciences_ , **2** , 401-415, doi: 10.4236/acs.2012.24035.



Schmittner, A., et al. 2011. Climate sensitivity estimated from temperature reconstructions of the Last Glacial Maximum. _Science_ , **334** , 1385-1388, doi: 10.1126/science.1203513.



Wigley, T.M.L., et al. MAGICC/SCENGEN v5.3. Model for the Assessment of Greenhouse-gas Induced Climate Change/A Regional Climate Scenario Generator. http://www.cgd.ucar.edu/cas/wigley/magicc/



van Hateren, J.H., 2012. A fractal climate response function can simulate global average temperature trends of the modern era and the past millennium. _Climate Dynamics_ , doi: 10.1007/s00382-012-1375-3.
"
"
According to my stat counter, the Projects Page tab above has been getting a regular stream of interest, so it has been updated with relevant content as of today 10/22/07


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3457f2b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Here’s a roundup of bloggers who are writing about Cato research and commentary: 



Are you blogging about Cato, but not on the list? Drop us a line and let us know!
"
"
Question: Why does a major grocery store chain need a “comprehensive policy addressing climate change”?
Answer: They don’t.

The Atlanta Business Chronicle reports that one of the nations oldest and largest grocery firms, Kroger Inc., based in Cincinnati, OH rejected a shareholder proposal which called for the company to develop a comprehensive policy addressing climate change.
Having shopped at many a Kroger store myself, I’m glad I won’t be bombared with climate change messages while I shop. I really don’t need to know what the carbon footprint is on a can of soup or a head of lettuce.
Cincinnati-based Kroger (NYSE: KR) operates more than 2,400 supermarkets and multidepartment stores in 31 states.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e935c4a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Above: Earth in comparison, size wise to common sunspots
The Christion Science Monitor had a detailed article recently that brought in a surprisng source – NASA GISS – an entity that seems firmly entrenched in the AGW- CO2 theory of climate change. Here are some excerpts from the article:
Researchers say they’ve found puzzling correlations between changes in the sun’s output and weather and climate patterns on Earth. These links appear to rise above the level of misinterpreted data or faulty equipment.
“There are some empirical bits of evidence that show interesting relationships we don’t fully understand,” says Drew Shindell, a researcher at NASA’s Goddard Institute for Space Studies in New York.
For example, he cites a 2001 study in which scientists looked at cloud cover over the United States from 1900 to 1987 and found that average cloud cover increased and decreased in step with the sun’s 11-year sunspot cycle. The most plausible cause, they said: changes in the ultraviolet (UV) light the sun delivers to the stratosphere.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea38acdc3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSuper Pugh
Faster than a speeding bullet…more powerful than a locomotive…able to leap tall buildings in a single bound! Look, it’s a bird! No it’s a plane! It’s Super-Pugh!
When evil threatens the planet Earth, then it’s Super Pugh to the rescue.
Now Super Pugh is in Nepal, where he just completed a 1 km swim across an icy 2″C lake to save the planet from, yes, you guessed it, manmade global warming read here.
One or two summers ago, publicity-monger and Mr Save-The-Earth Lewis Gordon Pugh attempted canoeing to the North Pole to draw attention to the problem of AGW (not to himself). He didn’t make it, probably too much kryptonite up there.
But now, in Nepal, according to his Hero Website, this was a “Swim for Peace. It is a plea to every nation, to do everything it can, to put a stop to climate change.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Who knows, maybe Super Pugh will get the Nobel Peace Prize this year.
How did he get to Nepal? Like everywhere else his expeditions take him, it’s up up and away – in a jet! (Surely he offsets his super carbon footprints).
Action hero Pugh is concerned that AGW is not being taken seriously enough. According to the Daily Planet, er, the Beeb: 
He urged governments to make tackling climate change a priority and said he was disappointed the issue did not feature more prominently in the UK election. 
It’s not easy being an action hero when nobody cares. Finally, here’s a video of Super Pugh in action!
My hero.
Share this...FacebookTwitter "
"

Though well‐​intentioned, international agreements to protect the environment are ineffectual, misguided and very costly for future generations. 



The most pervasive environmental problems are local and regional — not global. Water pollution, waste disposal, groundwater contamination, urban smog and deforestation are manifestations of unique state policies and local geographic, demographic and industrial profiles. They can be effectively addressed only locally. 



International agreements tend to ignore those less ”mediagenic” problems and pursue sexier issues such as climate change and ozone depletion — purported problems of which there is little hard evidence. Result: Scarce resources are misspent, real problems go unaddressed. 



International agreements tend to codify a uniform approach to environmental protection. The world is infatuated with command and control policies that empower armies of bureaucrats no more capable of managing ecological health than they are of managing economic growth. The World Bank, for example, would be charged with administering global environmental programs despite its record of ecological mismanagement. 



Natural resources are better protected by individual owners with vested interests in their property than by absentee bureaucratic managers subject to the winds of political fortune. Treaties that centralize environmental management compound ecological damage. 



Environmental treaties are biased against economic growth (viewed as a “problem” to be “solved” or “managed”) despite the proven correlation between wealthy economies and healthy environments. Treaties that harm growth not only condemn the poor to continuing poverty but doom long‐​term progress in environmental protection.
"
"
Share this...FacebookTwitterBy Die kalte Sonne

Atlantic region near Iceland has cooled over the past 120 years. Image: NASA (public domain)
(German text translated by P. Gosselin)
There are areas of the world that stubbornly resist “global warming”. These include an oceanic region near Iceland where sea surface temperatures have cooled by almost 1°C in the last 120 years.
Allan & Allan 2019 have examined the “cold blob” more closely and suspect that the summer ice melt will cause cold melt water to flow into the ocean, which will then lead to the winter cold of the sea area.
The researchers disagree with the model by Stefan Rahmstorf from Potsdam, who suggested a weakening of the Gulf Stream as the cause of the “cold blob”.
Here’s the abstract of Allan & Allan 2019:
Seasonal Changes in the North Atlantic Cold Anomaly: The Influence of Cold Surface Waters From Coastal Greenland and Warming Trends Associated With Variations in Subarctic Sea Ice Cover
Worldwide sea surface temperatures (SST) have increased on average by about 1 °C since 1900 with the exception of a region of the North Atlantic subpolar gyre near 50°N which has cooled by up to 0.9 °C over the same period, generating the negative feature on temperature anomaly maps which has been colloquially described by Rahmstorf et al. (2015, https://doi.org/10.1038/nclimate2554) as the “cold blob” (abbreviated here CB). This unique long‐term surface cooling trend is most evident in February, but in August net warming is observed even at CB epicenter and the CB itself is reduced to a mere “warming hole.” These seasonal changes in the intensity of the CB are the product of two separate factors: (1) a long‐term winter cooling specific for the CB region which appears to be associated with cooling of Greenland coastal waters in autumn, plausibly linked to summer meltwater from icebergs and sea ice and (2) summer warming effects which derive from (a) dramatic reduction in summer sea ice cover in the sub‐Arctic over the last 30 years that allows enhanced absorption of sunlight by the new open water in summer and (b) an unusual period of increased summer sub‐Arctic ice cover in the early twentieth century, which lowers the SST baseline measured from 1900, thus increasing the calculated linear rate of change of SST with time. Both of these effects could contribute to the observed Arctic amplification of warming.”


		jQuery(document).ready(function(){
			jQuery('#dd_432284bd7a4adeca2d29179312ac45d3').on('change', function() {
			  jQuery('#amount_432284bd7a4adeca2d29179312ac45d3').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"
Share this...FacebookTwitterA new assumption about carbon budgets reveals climate scientists have been vastly underestimating (by a factor of 2) the amount of carbon absorbed by the ocean for decades. Every past carbon budget estimate has been twice as wrong as the current estimate.
When it comes to the ocean heat fluxes and source vs. sink carbon budget estimates, climate scientists have been providing little more than educated guesses for decades.
For example, climate models have long suggested the ocean heat fluxes may only vary around 1 W/m². But “objective” analyses of oceanic latent heat flux (LHF) using different assumptions (equations) reveals fluxes were likely closer to 10 W/m² during 1981-2005 (Yu and Weller, 2007). So our modeled guesses were off by a factor of 10 compared to newer analyses.

Image Source: Yu and Weller, 2007
Ocean carbon sink processes not understood and driven by natural variability
McKinley et al. (2017) analyzed ocean carbon sink estimates and was willing to admit that due to a lack of observation, we lack a “detailed, quantitative, and mechanistic understanding of how the ocean carbon sink works…”
In addition, because internal variability in oceanic carbon uptake is so massive and largely unobserved, we cannot yet detect an anthropogenic influence.
McKinley and co-authors go so far as to acknowledge the “change in CO2 flux over 10 years (1995-2005)…is due almost entirely to the internal variability” because in most ocean regions “the forced [human-induced] trends in CO2 flux are too small to be statistically significant” and the “variability in CO2 flux is large and sufficient to prevent detection of anthropogenic trends in ocean carbon uptake on decadal timescales.”

Image Source: McKinley et al. (2017)
The Southern Ocean absorbs more than 10 times less carbon than previously thought


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Southern Ocean is where the largest portion of anthropogenic carbon (from our emissions) is said to be absorbed, or Earth’s largest oceanic CO2 sink.
Just 2 years ago, a carbon uptake analysis (Gray et al, 2018 with a Physics Today press release) that utilized estimates from biochemical floats instead of estimates from ships suggested the exact opposite of what had been previously thought. Instead of absorbing close to 1 petagram of carbon (PgC) per year, the Southern Ocean is barely even a carbon sink at all – just 0.08 PgC of yearly absorption. In fact, large regions of the Southern Ocean near Antarctica are a net source of CO2 to the atmosphere.
In other words, when estimates are float-based rather than ship-based, one estimate is more than 10 times different than the other.

Image Source: Physics Today
The global ocean absorbs 2 times more CO2 than previously thought
And now a new study (Buesseler et al., 2020) has scientists insisting that all of our previous estimates of global ocean carbon uptake are substantially wrong because we’ve been measuring from a fixed depth rather than varying depths.
Previously scientists had been using flux estimates from the “canonical fixed 150-m depth.” The new-and-improved way to assess carbon uptake is from varying but often much shallower depths: the euphotic zone (Ez). This is the section of the upper ocean layer that sunlight is able to penetrate, and it can “vary from less than 20 m to almost 200 m” in depth.
When we use the Ez to estimate carbon absorption versus export, the absorption changes from 2.8 petagrams of carbon (PgC) per year to 5.7. So the global ocean sink can be more than doubled just by varying the depth of measurement rather than using a fixed depth.
So, up to this point, scientists’ past guesses about ocean carbon uptake have been emphatically wrong. We can be assured, though, that our current guesses are anywhere from 2 to 10 times less wrong than the last ones.

Image Source: Buesseler et al., 2020 and press release
Share this...FacebookTwitter "
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
One of the great things about our current state of technology is the nearly instant reporting we can get from remote sensing platforms. Thanks to  Dr. Roy Spencer & Dr. Danny Braswell, GHCC at the University of Alabama, Hunsville, we can watch global temperatures of the lower troposphere in near real-time at this page:
http://discover.itsc.uah.edu/amsutemps/
According to UAH: Daily averaged temperatures of the Earth are measured by the AMSU flying on the NOAA-15 satellite. The satellite passes over most points on the Earth twice per day, at about 7:30 am and 7:30 pm local time. The AMSU measures the average temperature of the atmosphere in different layers from the surface up to about 135,000 feet or 41 kilometers. During global warming, the atmosphere near the surface is supposed to warm at least as fast as the surface warms, while the upper layers are supposed to cool much faster than the surface warms.
But as I understand it, the lower troposphere is supposed to be closely coupled to CO2 induced forcings. As we’ve seen from comparison to surface data sets such as HadCRUT, the UAH MSU lower troposphere tracks fairly well with surface temps.
You can learn more about how the Advanced Microwave Sounder Unit on NOAA-15 works and what coverage it has here at my post on it the instrument.
According to the UAH data For 2008, we are averaging about .4 to .5 degrees C cooler than last year. See the graph and click it for a larger one:

Click for larger graph
This tracks with some of the anecdotal eveidence we’ve been seeing in the weather in the northen hemisphere this spring, with late snowfalls, late frosts, and below normal temperatures. The northern latitude areas such as Canada have been very slow to have a spring season.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ee7768e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
_\---_   
  
Methane is all the rage. Why? Because 1) it is a powerful greenhouse gas, that molecule for molecule, is some 25 times as potent as carbon dioxide (when it comes to warming the lower atmosphere), 2) it plays a feature role in a climate scare story in which climate change warms the Arctic, releasing methane stored there in the (once) frozen ground, which leads to more warming and more methane release, _ad apocalypse_ , and 3) methane emissions are also be linked to fossil fuel extraction (especially fracking operations). An alarmist trifecta!   
  
Turns out, though, that these favored horses aren’t running as advertised.



While methane is a more powerful greenhouse gas in our atmosphere than carbon dioxide, its lifetime there is much shorter, even as the UN’s Intergovernmental Panel on Climate Change can’t quite say how long the CO2 residence time actually is. This means that it is harder to build-up methane in the atmosphere and that methane releases are more a short-term issue than a long-term one. If the methane releases are addressed, their climate influence is quickly reduced.   
  
This is why methane emissions from fracking operations—mainly through leaks in the wells or in the natural gas delivery systems—really aren’t that big of a deal. If they can be identified, they can be fixed and the climate impact ends. Further, identifying such leaks are in the fracking industry’s best interest, because, in many cases, they represent lost profits. And while the industry says it has good control of the situation, the EPA isn’t so sure and has proposed regulations aimed at reducing methane emissions from new and existing fossil fuel enterprises. The recent scientific literature is somewhat split on who is right. A major paper recently published in _Science_ magazine seemed to finger Asian agriculture as the primary suspect for recent increases in global methane emissions, while a couple of other recent studies seemed to suggest U.S. fracking operations as the cause (we reviewed those findings here).   
  
And as to the runaway positive feedback loop in the Arctic, a new paper basically scratches that pony.   
  
A research team led by University of Colorado’s Colm Sweeney set out to investigate the strength of the positive feedback between methane releases from Arctic soil and temperature (as permafrost thaws, it releases methane). To do this, they examined data on methane concentrations collected from a sampling station in Barrow, Alaska over the period 1986 through 2014. In addition to methane concentration, the dataset also included temperature and wind measurements. They found that when the wind was blowing in from over the ocean, the methane concentration of the air is relatively low, but when the wind blew from the land, methane concentration rose--at least during the summer/fall months, when the ground is free from snow and temperature is above freezing. When the researchers plotted the methane concentration (from winds blowing over land) with daily temperatures, they found a strong relationship. For every 1°C of temperature increase, the methane concentration increased by 5 ± 3.6 ppb (parts per billion)—indicating that higher daily temperatures promoted more soil methane release. However (and here is where things get real interesting), when the researchers plotted the change in methane concentration over the entire 29-yr period of record, despite an overall temperature increase in Barrow of 3.5°C, the average methane concentration increased by only about 4 ppm—yielding a statistically insignificant change of 1.1 ± 1.8 ppm/°C. The Sweeney and colleagues wrote:   




The small temperature response suggests that there are other processes at play in regulating the long-term [methane] emissions in the North Slope besides those observed in the short term.



As for what this means for the methane/temperature feedback loop during a warming climate, the authors summarize [references omitted]:   




The short- and long-term surface air temperature sensitivity based on the 29 years of observed enhancements of CH4 [methane] in air masses coming from the North Slope provides an important basis for estimating the CH4 emission response to changing air temperatures in Arctic tundra. By 2080, autumn (and winter) temperatures in the Arctic are expected to change by an additional 3 to 6°C. Based on the long-term temperature sensitivity estimate made in this study, increases in the average enhancements on the North Slope will be only between -2 and 17 ppb (3 to 6°C x 1.1 ± 1.8 ppb of CH4/°C). Based on the short-term relationship calculated, the enhancements may be as large as 30 ppb. These two estimates translate to a -3 – 45% change in the mean (~65 ppb) CH4 enhancement observed at [Barrow] from July through December. Applying this enhancement to an Arctic-wide natural emissions rate estimate of 19 Tg/yr estimated during the 1990s and implies that tundra-based emissions might increase to as much as 28 Tg/yr by 2080. This amount represents a small increase (1.5%) relative to the global CH4 emissions of 553 Tg/yr that have been estimated based on atmospheric inversions.



In other words, even if the poorly understood long-term processes aren’t sustained, the short term methane/temperature relationship itself doesn’t lead to climate catastrophe.   
  
The favorite thoroughbreds of the methane scare are proving to be little more than a bunch of claimers.   
  
  
  
**Reference:**   
  
Sweeney, C., et al., 2016. No significant increase in long-term CH4 emissions on North Slope of Alaska despite significant increase in air temperature. _Geophysical Research Letters_ , doi: 10.1002/GRL.54541.   
  
__


"
"
Share this...FacebookTwitterResearchers can no longer blame inconvenient cold winters on Arctic warming. No scientific basis, new study shows.
Image: NASA (public domain)
By Die kalte Sonne
(German text translated by P. Gosselin)
The Arctic is warming faster than the mid-latitudes. It’s the so-called Arctic amplification (AA). According to a study by Polvani et al. 2020, half of the Arctic warming is due to ozone-depleting substances – and not CO2. This was of course a great surprise, as we have already reported here. So no wonder that the climate models are going crazy, since they have so far attributed almost the entire warming to CO2.
CO2 has been hopelessly overbooked, as it now turns out.
In Nature Climate Change, a paper by Cohen et al. 2020 has been published and it shakes another myth. In the past, researchers were too happy to tell us that strong Arctic warming leads to harsh winters in mid-latitudes. However, there is a problem: the models are largely unable to reconstruct this presumed relationship: The paper finds:
Divergent consensuses on Arctic amplification influence on midlatitude severe winter weather
The Arctic has warmed more than twice as fast as the global average since the late twentieth century, a phenomenon known as Arctic amplification (AA). Recently, there have been considerable advances in understanding the physical contributions to AA, and progress has been made in understanding the mechanisms that link it to midlatitude weather variability. Observational studies overwhelmingly support that AA is contributing to winter continental cooling. Although some model experiments support the observational evidence, most modelling results show little connection between AA and severe midlatitude weather or suggest the export of excess heating from the Arctic to lower latitudes. Divergent conclusions between model and observational studies, and even intramodel studies, continue to obfuscate a clear understanding of how AA is influencing midlatitude weather.”


		jQuery(document).ready(function(){
			jQuery('#dd_dd3235f998397df12e765fa01f9ebb45').on('change', function() {
			  jQuery('#amount_dd3235f998397df12e765fa01f9ebb45').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Click image for movie – note download is large 2.4MB
A guest post by Michael Ronayne
Note: Mike has created a movie (solar_cycle_23-24_sunspots.gif large (2.4MB) animated GIF) that shows how the cycle 23 forecast has progressed through time. Given that NASA’s David Hathaway recently commented on SpaceWeather that we are still seeing Cycle 23 spots, this seemed like a good time to post Mike’s effort.
The Space Weather Prediction Center (SWPC) at http://www.swpc.noaa.gov/ issues weekly reports on solar activity know as Preliminary Report and Forecast (PRF) of Solar Geophysical Data or “The Weekly”. Generally on the week following the end of the month a monthly summary is issued which includes graphics for the past month.
In the summary is the “ISES Solar Cycle Sunspot Number Progression” graphic which shows past, present and predicted average sunspot numbers by month. SWPC maintains a compressed archive of all weekly PRD reports in PDF format since 1996 which is available here.
Individual weekly reports for 2007 are available here  and current reports for 2008 are available here .
The most current graphic is always here.
All of “The Weekly” reports were inspected to identify the monthly summaries and determine the quality of the “ISES Solar Cycle Sunspot Number Progression” graphic contained therein. It was determined that the graphs prior to April 30, 2003 were in a significantly different format, had quality control problems and skipped months, therefore only graphs from April 30, 2003 to present were used.
Using Adobe Acrobat Professional the “ISES Solar Cycle Sunspot Number Progression” graphics was extracted from each of “The Weekly” PDF reports as oversized TIFF graphics to preserve resolution. The standard publication size for the graphic was 720×550 pixels but the aspect ratio for some of the graphs was not preserved within the PDF document. When the oversized TIFF graphic were resized to 720×550 without preserving the aspect ratio within the PDF the original 720×550 graphic was recovered in all cases. The 720×550 TIFF graphic was then converted to a GIF graphic for use in the animation sequence.
While extracting the “ISES Solar Cycle Sunspot Number Progression” graphs it was found that January 31, 2008 monthly summary had not been generated, a fact which SWPC confirmed in response to an Email inquiry. The February 29, 2008 graphic was hand edited at the pixel level to recreate the missing month and is identified in the animation sequence “proxy200801.gif”. The remaining graphics are all identified by the PRF document number.
The Advanced GIF Animator program was used to create the animation sequence. With the exception of January 31, 2008 all of the frames are prefixed by PRF9999 when 9999 is the document number of the original PDF report from which the graphic was extracted.
When the animated frames were inspected in sequence it was found that there was a discontinuity between July 31, 2006 (PRF1510), August 31, 2004 (PRF1514) and the September 30, 2004 (PRF1520) frames. The causes of the discontinuities were:

Data was retroactively changed on the August 31, 2004 frame.
The August 31, 2004 data point was not plotted on the August 31, 2004 frame.

These three frames were not altered or correct in anyway and are displayed as published. This technique is very good at identifying data discontinuity problems.
Excluding the problems noted above the reconstructed graphic went very well and there was no discernible flicker between frames indicating that the PDF extraction process was near prefect. With the exception of the problem about August 31, 2004 and the missing monthly summary for January 31, 2008 the SWPC product has been amazingly consistent since April 30, 2003.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea00d55cf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’ve spent a lot of time on this blog showing how badly maintained and situated the stations in the USHCN network are. And rightly so, the majority of them have issues. But, finding the good ones is actually more important, because they are the ones that hold the true unpolluted temperature signal. Unfortunately, the “good ones” are few and far between.
But when one comes along that is a real gem, it deserves to be highlighted. I present the USHCN climate station of record for Tucumcari New Mexico, COOP ID # 299156, located at the Agricultural Experiment station about 3 miles outside of the edge of town.
I “had” (he just moved to St. Louis) a nephew who lived in Tucumcari, and he just happened to be friends with the director of the experiment farm. Before my nephew left they both helped me get this survey done.

Click picture for additional images
Surfacestations.org image gallery link
This station has several advantages:

Length of continuous record – going back to at least 1946 at this location, possibly to 1905 but NCDC MMS metadata stops at 1946.
Length of continuous instrumentation – using mercury max/min thermometers
Length of continuous data record – there doesn’t appear to be any missing years
Lack of encroachment – 3 miles from the northeast edge of town, little development, little UHI. Tucumcari is well off the beaten path of development. Population actually declined 12% in recent years.
Good siting – the station rates a CRN2 due to distant trees and sun angle, and one small asphalt road 70 meters away.

See the station survey report here (PDF) You can also make out the station on Google Earth using this link. After opening Google Earth, zoom in and the fenced outline and screen will be visible.
Eyeballing, you can see that the temperature data trend for Tucumcari is slightly positive over the last century, about 0.5°C, but there is a “bump” in 2000, which brings it to about 0.9°C. This same bump appears in neighboring stations such as in San Jon (33km away) and in Boys Ranch (135km away). There is nothing in the metadata location or equipment record to suggest a reason for the bump. So, either the bump is naturally occurring, or there is something we don’t know about that changed in the local environment, or we have another data set splicing error like the GISS Y2K debacle from last year.

Click for larger graph from NASA GISTEMP
I plotted the data provided by GISS (which you can find here) to show the effect of the “bump” at year 2000 on the overall trend:

Click for larger graph
Here is the data plot after the GISS homogeneity adjustment, I’ve hue shifted my saved version to red to help keep the graphs visually separate:

Click for larger graph from NASA GISTEMP
And here is the overlay of the USHCN data from GISTEMP and the data from the GISTEMP homogenization process:

In this case, the GISTEMP homogenization code appears to do what would be reasonably expected; reduce temperatures in the present to account for population growth and UHI. I’ve pointed out more than a few times that the GISTEMP homogenization adjustment often becomes flawed for truly rural sites like this when there are large cities within the 250km up to 1200km (depending on process) adjustment zone that Hansen uses, that have accelerating UHI trends. Due to these cities, often the past of a rural station gets adjusted cooler, resulting in an increased temperature trend, such as what happens at Cedarville, CA. Hopefully we’ll have a detailed analysis of that adjustment from John Goetz soon.
If you look at this list, you’ll see that there are a lot of rural stations within 250km. Tucumcari has the advantage of being truly in the middle of nowhere when it comes to other big cities. The closest big cities are Amarillo and Lubbock, but as I understand the algorithm used, when they are near the edge of the 250 km zone, their weighted value decreases.
In this case though, the GISTEMP homogeneity adjustment doesn’t take Tucumcari’s declining population into account, it only uses nightlights, and while the population may dwindle, town infrastructure usually doesn’t; streetlights counted around the station likely remain.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e2be9d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I was initially concerned that my stats were down this month, then I remembered that April has 30 days and March has 31.

Of course there’s that nice spring weather, and I recall that TV station ratings suffer a drop during the spring since people are digging out from their winter igloos. But even though I broke even, there was a nice surprise at the end of the month, WordPress put me as the top “hawt” post, even if only for awhile:

It was a nice way to end the month, thanks to all my readers for your help in getting me to NCDC @ Asheville and for the continued patronage!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f61a903',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

After the Second World War, the entrepreneur virtually disappeared from economic analysis (Baumol 1968). This neglect followed from the emerging models of general equilibrium that formed one aspect of the core of economic theory.1 By assumption, the Walrasian auctioneer knew the appropriate prices necessary to equate quantity supplied with quantity demanded in each market. In addition, the auctioneer knew when and by how much to adjust prices when an exogenous factor changed such as income or production technology. Trade only occurred at equilibrium prices so that markets cleared. No market participant chose or changed prices; it occurred exogenously.



Kenneth Arrow recognized the lack of real world mechanisms to determine and adjust prices in competitive markets. He identified a logical gap in the perfectly competitive model. He wrote that “there is no place for a rational decision with respect to prices as there is with respect to quantities” (Arrow 1959: 42). Prices exist independent of consumer and firm behavior. A complete model would have to provide a solution to the conundrum.2



Israel Kirzner (1967, 1971, 1973) responded to Arrow’s challenge. He argued for the reintroduction of the entrepreneur into economic analysis in order to explain how markets work. He offered a novel interpretation of the role of the entrepreneur in explaining how markets adjust to changes in conditions. Entrepreneurs recognized profit opportunities that no one else had. What appeared as an equilibrium price was not; it was a disequilibrium price that once recognized would yield profits. The fundamental aspect of entrepreneurship is alertness. Entrepreneurs, wrote Kirzner (1997: 72), notice “hitherto unnoticed profit opportunities” that arise from disequilibrium prices.



Kirzner did not devote many pages in his writings to discussing economic development. He focused on microeconomic processes, especially those pertaining to the mechanisms necessary to attain or approach equilibrium states. He devoted less time to understanding the role of entrepreneurship in explaining the differences in income per capita around the world. However, early in the development of his theory of entrepreneurship, he did critically examine the role of the entrepreneur in explaining comparative economic development.



In “Entrepreneurship and the Market Approach to Development,” Kirzner (1971) offered a definition of the entrepreneur that predated his more well‐​known 1997 definition. He argued that development economics in the 1960s, outside of the Schumpeterian variety, did not include the entrepreneur and, as a result, explanations regarding the differences in income around the world were not adequate. Like general equilibrium models, models of economic development lacked an endogenous source of innovation, invention, and resource reallocation.



Kirzner’s critique of development economics included a citation to P.T. Bauer and Basil Yamey’s 1957 book, _The Economics of Under‐​Developed Countries_. He argued that their contribution missed the central feature of entrepreneurship—namely, alertness to new opportunities to make a profit. I disagree. Indeed, Bauer and Yamey (1957: 106) identified the same aspect of entrepreneurship that Kirzner would later stress—”hitherto unsuspected opportunities for profitable economic activity.” Even though Kirzner (2005: 465) referred to the Bauer‐​Yamey book as a “classic” in development economics, he failed to fully recognize the pioneering contributions they made to understanding the true nature of entrepreneurship.



Neoclassical economics, as characterized by Arrow‐​Debreu general equilibrium, does not explain how prices emerged from the trading process. Prices existed prior to exchanges. The Walrasian auctioneer knew the necessary information regarding consumers’ preferences and information as well as the cost curves facing the firms. After collecting all the information, the auctioneer identified the vector of prices necessary to clear markets. Trade only occurred after prices were announced. Arrow identified this puzzling aspect of neoclassical theory; it lacked a theory of how prices change in a competitive market: “Each individual participant in the economy is supposed to take prices as given and determine his choices as to purchases and sales accordingly; there is no one left over whose job its is to make a decision on the price” (Arrow 1959: 43). Prices existed independent of the decisions of individuals. No one within the model set or changed prices.



Interest in the entrepreneur increased in development eco­nomics in the early 1960s as alternatives to general equilibrium theorizing appeared. Irma Adelman (2001) argued that entrepreneurship became the central variable in development policy from 1958 to 1965, while McClelland (1961), Hagen (1963), Baumol (1968), and Leibenstein (1968) each offered their own attempt to include the entrepreneur. Peter Kilby (1971) identified no less than 13 aspects of entrepreneurship related to economic development. Entrepreneurship appeared on the intellectual agenda but its essential component—alertness—did not.



Kirzner (1971) developed the arguments that would later appear in _Competition and Entrepreneurship_ (1973). He criticized development and growth economics for misunderstanding the role of the entrepreneur:



In a footnote following the above quote, Kirzner cites Bauer and Yamey (1957). He includes them in the group of economists who have discussed entrepreneurship but failed to address its central feature of alertness—perceiving hitherto unnoticed profit opportunities. He should not have, because Bauer and Yamey identified the central aspect of entrepreneurship that Kirzner stressed. They, too, recognized and discussed entrepreneurial alertness.



Kirzner goes on to argue that:



Kirzner recognized the importance of the entrepreneur in explaining economic growth and development as did many others in the 1960s. Unlike the others, Kirzner identified an aspect of entrepreneurship they did not. In order for economies to grow, someone had to grasp “the knowledge which might otherwise remain unexploited” (Kirzner 1971: 197). But Kirzner was not the first to recognize the importance of alertness.



Although much of development economics in the 1950s and 1960s neglected the entrepreneur, the contributions of P. T. Bauer and Basil Yamey did not. Rather, they emphasized the importance of the entrepreneur in their earliest writings. Bauer clearly identified entrepreneurship as a vital but neglected aspect of orthodox development economics. His early studies on trade in West Africa (Bauer 1954) provided ample evidence that entrepreneurship was omnipresent and was vital in understanding how economies evolve from low to high levels of income per capita. For example, Bauer (1954: 30) wrote that the trader‐​entrepreneur (as he referred to entrepreneurs) in Nigeria and the Gold Coast exhibited the following characteristics: “exceptional effort, foresight, resourcefulness, thrift and the ability to perceive economic opportunity.” Trader‐​entrepreneurs, at least those in Nigeria and the Gold Coast, perceived profit opportunities. They were alert. Entrepreneurs recognized the gains that emerged from changes in relative scarcities, new information, new ideas, or serendipity.



In other writings, Bauer continued to argue that the trader‐​­entrepreneurs existed throughout the developing world. His fieldwork in sub‐​Saharan Africa provided plenty of evidence. According to Bauer ([1963] 1972: 347), “The prominence of foreigners in African commerce reflects technical and administrative skills, thrift, [and] the ability to perceive and take advantage of economic opportunity.” Once again, entrepreneurs perceive economic opportunity when it arises. They do not simply respond to a given set of prices, production techniques, or information. Entrepreneurs engage in more than arbitrage. They recognize profit opportunities no one else had and develop new means to attain their goals.



Bauer and Yamey (1957) offered a comprehensive discussion of the source of economic development that extended beyond conventional models at the time that stressed capital formation or the rate of savings. They stressed a number of factors including the quality of public institutions and policies, the importance of international and intranational trade, values, and attitudes. More importantly, central to their argument, they stressed entrepreneurial alertness and its perception of “hitherto unsuspected opportunities.”



Bauer and Yamey began their discussion of the entrepreneur by noting that entrepreneurship occurs quietly through small changes that raise productivity. Better knowledge of prices and costs allow the entrepreneur to make profits. Knowledge of productivity increasing techniques also represents an aspect of entrepreneurship. But they extended entrepreneurship beyond greater knowledge of existing conditions. In some cases, it leads to significant changes. In particular, “Innovation and the exercise of entrepreneurship in the sense of creating or taking advantage of _hitherto unsuspected opportunities for profitable economic activity_ are often dramatic in their impact,” wrote Bauer and Yamey (1957: 102, emphasis added). They went on to argue that “the ability of individuals to perceive new opportunities for profit and the ability and willingness to exploit them are indeed crucial in economic development” (ibid.). From these passages, it is clear that Bauer and Yamey held views similar to those of Kirzner regarding the central features of entrepreneurship.



Bauer and Yamey (1957: 102) pointed to how entrepreneurs help generate new ideas and new techniques that foster economic development:



Kirzner’s contribution to the theory of entrepreneurship clearly has its antecedents in the works of Bauer and Yamey. Yet, their contribution appears to have been forgotten. There is no mention of their pioneering work on entrepreneurship in Kirzner’s seminal book, _Competition and Entrepreneurship_ (1973); nor in Kirzner’s 1997 _Journal of Economic Literature_ article, which is a survey of his theory of entrepreneurship. Neverthelss, in Kirzner’s (2005) contribution to a conference volume in honor of Bauer after his death, he called Bauer and Yamey’s 1957 book a “classic.”



Bauer and Yamey’s (1957) identification of entrepreneurship with “hitherto unsuspected opportunities for profitable economic activity” prior to Kirzner does not imply that he did not make a scientific contribution to the theory of entrepreneurship. Originality is only one aspect of scientific progress. As George Stigler (1955: 294) noted, “Scientific originality in its important role should be measured against the knowledge of a man’s contemporaries. If he opens their eyes to new ideas or to new perspectives on old ideas, he is important in the scientifically important sense.”



Even though Bauer and Yamey identified an aspect of entrepreneurship that had eluded development economists and the profession more broadly, Kirzner’s discussion opened the eyes of others. For example, Schultz (1975, 1990) developed an alternative theory of entrepreneurship that emphasized the role of human capital partially in response to Kirzner in order to better understand the process of economic development. Moreover, Baumol (1990) differentiated between productive and unproductive entrepreneurship, and differentiated his approach from Kirzner’s. Although, Bauer and Yamey emphasized the role of entrepreneurship in explaining the process of economic development, Kirzner brought a new perspective to an old idea.



Adelman, I. (2001) “Fallacies in Development Theory and Their Implications for Policy.” In G. Meier and J. Stiglitz (eds.), _Frontiers of Development Economics: The Future in Perspective_ , 103–34. New York: Oxford University Press.



Arrow, K. J. (1959) “Toward a Theory of Price Adjustment.” In M. Abramovitz et al. (eds.), _The Allocation of Economic Resources: Essays in Honor of Bernard Francis Haley_ , 41–51. Stanford, Calif.: Stanford University Press.



Bauer, P. T. (1954) _West African Trade: A Study of Competition, Oligopoly and Monopoly in a Changing Economy_. Cambridge, UK: Cambridge University Press.



__________ ([1963] 1972) “The Study of Underdeveloped Economies.” In _Dissent on Development_ , chap. 8. Cambridge, Mass.: Harvard University Press.



Bauer, P. T., and Yamey, B. (1957) _The Economics of Under‐​Developed Countries._ Chicago: University of Chicago Press.



Baumol, W. (1968) “Entrepreneurship in Economic Theory.” _American Economic Review_ 58: 64–71.



__________ (1990) “Entrepreneurship: Productive, Unproductive, and Destructive.” _Journal of Political Economy_ 98: 893–921.



Fisher, F. M. (1983) _Disequilibrium Foundations of Equilibrium Economics_. New York: Cambridge University Press.



Gintis, H. (2007) “The Dynamics of General Equilibrium.” _The Economic Journal_ 117 (October): 1280–1309.



Hagen, E. (1963) “How Economic Growth Begins: A Theory of Social Change.” _Journal of Social Issues_ 19 (1): 20–34.



Kilby, P. (1971) “Hunting the Huffalump.” In P. Kilby (ed.), _Entrepreneurship and Economic Development_ , 1–40. New York: Free Press.



Kirzner, I. M. (1967) “Methodological Individualism, Market Equilibrium, and Market Process.” _Il Politico_ 32: 787–99.



__________ (1971) “Entrepreneurship and the Market Approach to Development.” In F. A. Hayek et al. (eds.) _Toward Liberty: Essays in Honor of Ludwig von Mises_ , Vol. 2, 194–208. Menlo Park, Calif.: Institute for Humane Studies.



__________ (1973) _Competition and Entrepreneurship_. Chicago: University of Chicago Press.



__________ (1997) “Entrepreneurial Discovery and the Competitive Market Process: An Austrian Approach.” _Journal of Economic Literature_ 35 (1): 60–85.



__________ (2005) “Human Attitudes and Economic Growth.” _Cato Journal 25_ (3): 465–69.



Leibenstein, H. (1968) “Entrepreneurship and Development.” _American Economic Review_ 58: 72–83.



McClelland, D. (1961) _The Achieving Society_. Princeton, N.J.: Van Nostrand.



Schultz, T. (1975) “The Value of the Ability to Deal with Disequilibria.” _Journal of Economic Literature_ 13: 827–46.



__________ (1990) _Restoring Economic Equilibrium: Human Capital in the Modernizing Economy._ Williston, Vt.: Basil Blackwell.



Stigler, G. (1955). “The Nature and Role of Originality in Scientific Progress.” _Economica_ 22: 293–302.



Yates, A. (2000) “The Knowledge Problem, Entrepreneurial Discovery, and Austrian Market Process Theory.” _Journal of Economic Theory_ 91: 59–85.



J. Robert Subrick is Associate Professor of Economics at James Madison University.



ShowHide

Endnotes



1 Keynesian macroeconomics formed another prominent aspect of economic theory. It too lacked any use for an entrepreneur as it focused on the movement of statistical aggregates with little concern about the underlying microeconomic processes.



2 Few have followed up on Arrow’s concerns. See Fisher (1983), Yates (2000), and Gintis (2007) for exceptions.
"
"

The Supreme Court’s decision in _King v. Burwell_ upheld President Obama’s massive power grab, allowing him to tax, borrow, and spend $700 billion without congressional approval. This establishes a precedent that could let any president modify, amend, or suspend any enacted law at his or her whim.



As it stands, Obamacare will continue to disrupt coverage for sick Americans until Congress repeals it and replaces it with reforms that make health care better, more affordable, and more secure. Despite the ruling, Obamacare remains unpopular with the American public and the battle to set in place a health care system that works for all Americans is far from over. At a Capitol Hill Briefing in July, Michael Cannon, director of health policy studies at Cato, and Ilya Shapiro, a senior fellow in constitutional studies at the Institute, came together to discuss the impact of _King v. Burwell_ on health care reform, the separation of powers, and the rule of law.



 **MICHAEL CANNON** : The ink wasn’t yet dry on the ruling in _King v. Burwell_ before supporters declared that the debate over repealing Obamacare is over. Well, that debate has been declared over so many times at this point that I’ve lost count. But I believe that Obamacare supporters dodged a bullet with this ruling. The way the Affordable Care Act (ACA) was written, approved by both chambers of Congress, and signed into law by the president, it gave states the power to block major provisions of the law. States can block the subsidies that are supposed to flow through the health insurance exchanges. They can block the employer mandate and, to a large extent, the individual mandate. The law as written gave states these powers.



Obamacare supporters dodged a bullet because 34 or 38 states — depending on how you count — did not establish health insurance exchanges, which they needed to do in order for those provisions to take effect. They effectively exercised the vetoes that Congress gave them over portions of the ACA. And eliminating those subsidies would have revealed the full cost of this coverage to enrollees in Health‐ Care​.gov. That is what the Obama administration and its supporters fear. (I do not consider them ACA supporters, by the way, because they do not really support that law as written.) Nevertheless, the repeal debate is not over and there are a lot of reasons why. First, more than six years after the first draft of Obamacare was introduced in the House, it remains unpopular. In fact, it is as unpopular now as it was when it was enacted more than five years ago. And this is a year and a half into implementation, a year and a half after people have been receiving the benefits under this law, as rewritten by the Obama administration and the Supreme Court. And a lot of the costs of the law have not even taken effect yet: the Cadillac tax, some premium hikes on the horizon, the fact that some of the temporary programs designed to mitigate adverse selection will expire. This law has been unpopular for six solid years, and you know what? Unpopular laws, in a democracy, are always up for repeal. Second, the Obamacare repeal debate is going to keep going on because Obamacare hurts the sick. Yes, it does insure more people by throwing lots and lots of money at health insurance. But it also threw millions out of their health care plans in 2013 — including cancer patients and others with severe illnesses — leaving them with inferior coverage. It threatened to throw millions more out of their plans again until the Supreme Court amended the law in this latest ruling. And it is going to continue to threaten coverage for cancer patients and others as long as that law remains on the books.



It is not just opponents of the law that are noticing this. If you look at the January 29 issue of the _New England Journal of Medicine_ , the lead article is about how Obamacare is pushing insurers into a race to the bottom by jettisoning coverage for HIV patients. There are other studies that have found the same thing happening with other high‐​cost chronic conditions, like mental illness, diabetes, rheumatoid arthritis, and cancer. This has happened in other markets with Obamacarelike bans on discrimination on the basis of preexisting conditions. There are a number of examples where community rating rules have encouraged insurers to avoid, mistreat, and ultimately dump the sick. And it does not matter that it is only happening with a few insurers now. The _New England Journal of Medicine_ article said that a quarter of the insurance plans that they studied displayed these characteristics. But eventually all insurers will have to follow suit. Supporters of the law will likely blame this on greedy insurance companies. But the truth is that it is Obamacare that forces these companies into this race to the bottom, even when insurers are not trying to discriminate against the sick.



Another reason the repeal debate will continue is because Obamacare, and the way it has been implemented, demeans voters. And they feel it. They sense that it demeans them. The way that the Obamacare’s architects designed, sold, enacted, and implemented this law has been an ongoing string of insults to the intelligence, the compassion, the dignity, and the sense of fair play of Americans who oppose this policy. Obamacare’s architects have lied to the public. They have called voters stupid. They have called opponents evil. President Obama and the Supreme Court have rewritten the ACA now in so many ways that it disempowered and disenfranchised Obamacare opponents.



Finally, this debate is going to continue because there are ways to provide more secure health coverage to the sick. No one wants the pre‐​Obamacare world where government had already been making health insurance less secure. But in spite of the tax exclusion for employer‐​sponsored insurance and all the other things that the government has done to cripple private health insurance markets, they were still innovating to develop products that made health insurance more secure.



One example is guaranteed renewability. Another example is preexisting condition insurance, an innovation that was happening right underneath Congress’s nose as they were debating the Affordable Care Act. It was first introduced in late 2008, and UnitedHealthcare was getting regulatory approvals in early 2009. What was this product? It was basically an insurance product where, at a cost of just 20 percent of what you would pay for health insurance, you can buy the option to purchase that health insurance plan at any time, no matter how sick you get. Even if you develop a preexisting condition, you pay the same rate as everyone else. This was available in 2008. There were further innovations that markets were likely to develop that encouraged insurers to compete to cover the sick rather than to avoid them.



But Obamacare destroyed these innovations. When you think about it, if Obamacare were such an improvement over the status quo ante as it existed before the law, then why do 65 percent of Health​Care​.gov enrollees want the freedom to purchase their pre‐​Obamacare plans? Obviously something is amiss there. Unfortunately, these innovations are not coming back until we get rid of Obamacare, specifically its community rating price controls, and that’s why the debate is going to keep happening.



Now, what should Congress do in the wake of _King v. Burwell_? It should stay focused on what it has always been focused on with regard to Obamacare — which is repealing it. We are not going to get lower costs and more secure health insurance for the sick until this law is off the books. Repeal is actually more important after _King v. Burwell_. With that ruling, the president and the Supreme Court just created entitlements and imposed taxes on 70 million Americans — employers and individuals — that no Congress ever approved and from which Congress specifically exempted those 70 million employers and individuals. The fact that tens of millions of Americans are currently subject to taxes that Congress never approved makes it all the more important that Congress repeal Obamacare. _King v. Burwell_ shows that what we are living under right now is an illegitimate law.



I like to say that the Affordable Care Act is imperfect. But, gosh, it is a lot better than what we’ve got.



 **ILYA SHAPIRO** : Look, I’m a simple constitutional lawyer. Essentially everything I know about health care I’ve learned through litigating two cases, _NFIB v. Sebelius_ three years ago and now _King v. Burwell_. Originally, when we were planning this forum, we thought the case could go either way. It was really a 50/50 toss‐​up, and I thought I could add some value by explaining the nuanced rules of decision: for instance, what does this mean for health care or other types of regulatory policy going forward? Given the Court’s opinion, however, all I can say is that there’s really very little law, as it were, in the majority opinion. It’s as if the whole opinion just said “Affirmed.” The reasons don’t matter because they don’t make any sense whatsoever. Words have whatever meaning the writer wants them to have — which is a far cry from the following judicial opinion: “It is not our job to protect the people from the consequences of their political choices.” Now who wrote that? Is that from Justice Antonin Scalia’s dissent here in _King v. Burwell_? No. Is it from Justice Anthony Kennedy’s (combined with Scalia’s) dissent in _NFIB v. Sebelius_? No. It was Chief Justice John Roberts in the majority opinion in _NFIB_.



What’s going on here is an unholy confluence of liberal judicial activism and conservative judicial passivism that has found a perfect home with John Roberts. It’s not that the Court is liberal. Nor is John Roberts “evolving,” as so many justices have in the past. What’s going on here is that, well, Obamacare is special. As Scalia pointed out in his dissent, all the normal rules of constitutional — and now statutory — interpretation go out the window when it comes to the Affordable Care Act. Mind you, it wasn’t a matter of enforcing the text of the Affordable Care Act. It was rewriting the text in a different way to do what John Roberts thought in his infinite wisdom would cause the least disruption in public policy or the health care system. And it shows why we don’t want judges making these kinds of extra‐​legal determinations, because again this is not a liberal decision. In _NFIB_ , the individualmandate case, Justice Ruth Bader Ginsburg’s partial concurrence/​partial dissent said that there are no constitutional limits on federal power. That’s the liberal position.



Similarly here, the liberal position would have been to say that the IRS gets to do whatever it wants, applying what is known as _Chevron_ deference. _Chevron_ is a legal doctrine named after a case from more than 30 years ago, which says that when a law is ambiguous, courts are to defer to an agency’s interpretation of that law, unless that agency is being arbitrary and capricious. So even if courts might disagree with the agency’s determination, as long as it’s not completely crazy they’ll defer to it. In other words, A, B, C, D, and E are all somewhat plausible interpretations. One might be better; one might be worse. But as long as the agency doesn’t go for X, Y, or Z — which are all completely out of left field — they’ll be okay. Roberts specifically said in _King v. Burwell_ said that that was not what he was doing. This was not an administrative‐ law, agency‐​deference case. And that was backed up in June in the _Michigan v. EPA_ case, where Justice Clarence Thomas wrote a concurring opinion on the need to narrow _Chevron_. Justice Scalia’s majority opinion there said that the agency’s determination was indeed unreasonable and therefore sent it back to the drawing board.



But that doesn’t mean, I don’t think, that in some future instance _Chevron_ is going to be narrowed. I don’t know if the Court has the five votes necessary to do that. Some people are saying the fact that _Chevron_ wasn’t expanded is the silver lining in _King v. Burwell_. It sort of is. But again, this is kind of a sui generis opinion, good for the Affordable Care Act only. I’m sure some lower‐​court judges will use it to buttress some future fanciful statutory interpretations — to say that A is equivalent to Not‐​A, as it is here — that “exchange established by the state” means “exchange not established by the state.” Possibly. But really the way that it’s written, John Roberts’s goal in _King_ , as it was in _NFIB_ , was to achieve a certain result without really changing legal doctrine, without expanding federal power.



Let me extrapolate from that. Looking forward, there’s a lesson we can draw to avoid that unholy alliance of liberal activism — rewriting the law — with conservative passivisim — restraining and bending over backwards to let Congress do whatever it wants. The way we avoid that is to learn the lessons of history.



In the late 1930s and early 1940s, the Supreme Court started going off the rails and eviscerating the doctrine of limited and enumerated powers. It began eviscerating federalism and bifurcating (even trifurcating) our rights, such that some rights are more equal than others. Then, from the 1950s to the 1970s, when the Warren and Burger Courts continued these “evolving” notions of what the Constitution means, the conservative response to that sort of activism was not: “You’re wrong. Here’s the correct theory of constitutional or statutory interpretation.” Instead and alas, the response was: “Why are you not deferring to the political branches? You are unelected judges. You should be restrained. You should be deferring. You should be sitting on your hands.” And that’s why we have what we have now.



The answer is to appoint judges who are actually committed to judging. We should be fighting for judges who have a proven track record of saying what the Constitution actually is — of engaging with the law — rather than trying to bend over backwards and defer to agencies or to Congress. The judicial branch is a branch of the federal government for a reason. It’s there to check and balance the others.



John Roberts’s background was too smooth in many ways. He checked all the right boxes and excelled at the legal craft, but he never identified as an originalist or movement conservative. It’s clear that he’s a Republican. But it’s never been clear that he was committed to any particular theory of judicial interpretation. So congratulations to him, but future Republican administrations will have to be more careful about what kind of conservative they want on the Supreme Court: someone who focuses on restraint (a judicial mode) or someone who has a particular substantive theory of jurisprudence.



Those of you, especially here on Capitol Hill, who are going to the barricades to fight for a proper judiciary, make sure you’re fighting for the right people so that it’s worth the effort. Someone who has displayed loyalty to generic conservatism or some kind of “Red Team, Blue Team” fight is not enough. We need judges who actually are willing to make the difficult “balls and strikes” calls — as John Roberts said at his confirmation hearings — rather than kick the plate a little bit, squint, and call it a strike because Congress “intended” it to be a strike. This is not going to change overnight. It’s not a matter of winning the White House. It’s a matter of picking the right judges and understanding the climate of ideas such that the proper judicial philosophy isn’t being conservative or minimalist or incrementalist or restrained. It’s about judging in a particular way and applying the standard tools of statutory and constitutional interpretation regardless of where the political chips may fall.
"
"

A new USC study shows that Deep-sea temperatures rose 1,300 years before atmospheric CO2 rose, ruling out the greenhouse gas as driver of meltdown, says a study in Science.
Carbon dioxide did not cause the end of the last ice age, a new study in Science suggests, contrary to past inferences from ice core records. “There has been this continual reference to the correspondence between CO2 and climate change as reflected in ice core records as justification for the role of CO2 in climate change,” said USC geologist Lowell Stott, lead author of the study, slated for advance online publication Sept. 27 in Science Express. “You can no longer argue that CO2 alone caused the end of the ice ages.” Deep-sea temperatures warmed about 1,300 years before the tropical surface ocean and well before the rise in atmospheric CO2, the study found.
The finding suggests the rise in greenhouse gas was likely a result of warming and may have accelerated the meltdown – but was not its main cause. The study does not question the fact that CO2 plays a key role in climate. I don’t want anyone to leave thinking that this is evidence that CO2 doesn’t affect climate,” Stott cautioned. “It does, but the important point is that CO2 is not the beginning and end of climate change.” While an increase in atmospheric CO2 and the end of the ice ages occurred at roughly the same time, scientists have debated whether CO2 caused the warming or was released later by an already warming sea.
The best estimate from other studies of when CO2 began to rise is no earlier than 18,000 years ago. Yet this study shows that the deep sea, which reflects oceanic temperature trends, started warming about 19,000 years ago. “What this means is that a lot of energy went into the ocean long before the rise in atmospheric CO2,” Stott said. But where did this energy come from” Evidence pointed southward. Water’s salinity and temperature are properties that can be used to trace its origin – and the warming deep water appeared to come from the Antarctic Ocean, the scientists wrote. This water then was transported northward over 1,000 years via well-known deep-sea currents, a conclusion supported by carbon-dating evidence. In addition, the researchers noted that deep-sea temperature increases coincided with the retreat of Antarctic sea ice, both occurring 19,000 years ago, before the northern hemisphere’s ice retreat began.
Finally, Stott and colleagues found a correlation between melting Antarctic sea ice and increased springtime solar radiation over Antarctica, suggesting this might be the energy source. As the sun pumped in heat, the warming accelerated because of sea-ice albedo feedbacks, in which retreating ice exposes ocean water that reflects less light and absorbs more heat, much like a dark T-shirt on a hot day.  “The climate dynamic is much more complex than simply saying that CO2 rises and the temperature warms,” Stott said. The complexities “have to be understood in order to appreciate how the climate system has changed in the past and how it will change in the future.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea37da97f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterOnly a few thousand years ago, when CO2 levels were both stable and low (~265 ppm), the  (1) Arctic had far less ice and more vegetation than it does now and (2) the massive rate of ice melt in Antarctica rendered modern melt rates negligible by comparison.
A new study (Cherezova et al., 2020) reveals that until about 6,500 years ago Bolshevik Island in the Russian High Arctic brimmed with grass, birch and willow trees, and large herbivores grazing on grass year-round. At that time sea levels were rising at rates of 7.9 mm/yr, which is more than 5 times faster than the  global sea level rise trend since 1958 (~1.4 mm/yr, Frederikse et al., 2018).
Today this same High Arctic island is treeless with “very scarce vegetation.” It is locked in sea ice and mean annual temperatures only reach -13°C, which is a “similar climate to the Lateglacial.” Modern climate warming “is not observed” in either the meteorological or ice core data (Tvyordoe Lake) for this region. The ice caps are today about the same size as they were during the peak of the last ice age (~20,000 years ago).

Image Source: Cherezova et al., 2020
Another new study (Jones et al., 2020) reveals that from about 7,500 to 4,500 years ago, when CO2 was about 150 ppm lower than today, Antarctica’s Ross Sea glaciers abruptly lost 220 meters (!) of ice surface height. This ice loss – at times reaching >400 cm per year – occurred throughout the region regardless of the topography. This strongly implies the “overarching external driver” of the glacier retreat was an ocean warming trend.
The authors point out that the ice surface lowering may have “continued below the present-day glacier surface,” only to advance again during the last few hundred years.

Image Source: Jones et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A recent study (Sinclair et al., 2012) indicates the sea surface temperatures in this region (western Ross Sea) have been rapidly cooling (-1.59°C/decade) since 1979, and there has been no net ocean warming since 1882.

Image Source: Sinclair et al., 2012
Still another study from this region (Yokoyama et al., 2016) corroborates a “widespread collapse of the Ross Ice Shelf” between 5,000 and 1,500 years ago. Modern melting rates are the slowest in the last 5,000 years and there was “much warmer water beneath the ice shelf at 5 ka compared with the present.”
These studies strongly imply modern polar climate changes and ice melt rates in both hemispheres are neither unprecedented or unusual. In fact, if there is anything anomalous about today’s polar climates, it’s that they are colder and ice melt is less pronounced than just about any time in the last 8,000 years.


Images Source: Yokoyama et al., 2016


		jQuery(document).ready(function(){
			jQuery('#dd_1b239021c037dc1f44fcc32cd693e702').on('change', function() {
			  jQuery('#amount_1b239021c037dc1f44fcc32cd693e702').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The Cato Institute is committed to holding seminars throughout the country each year, on the basic premise that important questions of policy and governance should not be confined to Washington. At the Policy Perspectives 2012 event in New York City in March, Steve Forbes, chairman and editor‐​in‐​chief of Forbes Media, focused on the economic headwinds holding the country back. The true source of prosperity, he said, is the free market — and these barriers are standing in the way. Mary Anastasia O’Grady, an editorial board member of the _Wall Street Journal_ , detailed the intellectual roots of underdevelopment in Latin America — and, more importantly, the lessons they hold for economic growth in the United States.



 **Steve Forbes** : We all know, even though the economy is doing better this year, that we are still witnessing a punk performance. It’s like being on a superhighway and getting the automobile to go from 20 miles an hour to 40. It should be going 70 or 75 — and, when no one is looking, 125. But something is inhibiting us. I’d like to take a few moments to touch upon the major headwinds that are pushing us back, at a time when we should be moving forward rapidly.



The first, of course, is monetary policy — possibly the most boring subject in the world. The best way to think about monetary policy is to continue the above metaphor. Let’s say you have an automobile, a magnificent vehicle in its own right. If you don’t have sufficient fuel for that car, you’ll stall. With too much fuel, you’ll flood the engine. Only with the right amount do you have the chance to move forward.



The same is true of an economy, even one that has all of the basic strengths. If you don’t supply enough money to meet the organic needs of the marketplace, you’ll stall. If you print too much money, you get the economic equivalent of a flooded engine. The Federal Reserve has been on a bender recently, printing excess money ever since the early part of the last decade. We never would have had a housing bubble if the Fed had not provided the juice for it. Money, of course, is not created by government. It is generated by individuals who make transactions in the marketplace.



Money is simply a facilitator of those transactions. Before money, we had to barter — which means that if I wanted to sell an ad in _Forbes_ , I might have to accept a herd of goats as payment. If I then wanted to turn around and purchase iPads for our writers, I’d have to be prepared for the Apple storeowner to demand sheep rather than goats, which would then take me to a sheep herder, who may prefer red wine to my white wine … . and so on. Simply put, commerce becomes cumbersome.



With money, however, it’s much easier to create capital, invest in the future, and bring that future into the present. The key, however, is that money has to be stable in value, like ounces in a pound or inches in a foot. Can you imagine what would happen if Washington did to the hour what it does to the dollar? Sixty minutes in an hour one day, 42 minutes the next, 77 the following … you would soon need hedges, derivatives, and futures to figure out how many hours you’re working. If you hire someone for $15 an hour, you’d have to specify if that’s a New York hour, an Illinois hour, or a Bangladesh hour. The key is simplicity.



The way to do this is to re‐​link the dollar to gold. For all its imperfections, the question is: What works better, gold or politicians? Without a stable currency measure, people invest in existing hard assets — which results in capital flight from this country because people are not willing to take risks. Equity prices are supposed to reflect the present value of discounted future income flows. But if you don’t know what the income flows are denominated in, that depresses values today. Stability works. Make money a fixed measure of value. Period.



Another headwind that has been holding us back, which Cato has been at the forefront of, is taxes. What the mandarins in Washington don’t understand is that taxes are a price and a burden. A tax on income is the price you pay for working. A tax on capital gains is the price you pay for taking risks.



A tax on profits is the price you pay for success. And the idea is a simple one: when you lower the price of good things like productive work, risk taking, and success, you get more of them. When you raise the price, you get less. Yet the political class keeps trying to raise taxes to solve their own spending problem.



The current tax code, though, goes way beyond raising revenue. It is ultimately a source of power and manipulation, and the biggest source of lobbying in Washington. Last year, we spent 6.5 billion hours filling out tax forms — the equivalent of three million full‐​time jobs. The code has been changed more than 14,000 times since 1986 and, no matter what your faith, it is beyond redemption. The only thing to do with this monster is drive a stake through its heart, bury it, and hope it never rises. We need to start all over.



I am personally in favor of a flat tax: a single rate, with generous exemptions for adults and children, and no federal income tax on the first $46,000 for a family of four. Beyond that, the rate should be 17 percent with no taxes on savings or death. We should be able to leave this world unmolested by the IRS — or, as the Founders would say, no taxation without respiration. The same rules should apply on the corporate side. If you do that, the dollar is as good as gold.



The next barrier to growth, not surprisingly, is spending. Of course, it’s not just that public spending is wasteful. It’s ultimately a source of power. What the government does is, it takes resources from you, puts it through the sausage factory, takes a cut, and then spits it out to politically anointed recipients. That’s not stimulus; it’s stagnation. One reason the current president doesn’t like achieving real reductions in spending is because he knows, given his background, that it is a source of power.



The more resources you control, the more power you have at the center. John Maynard Keynes, after all, said it doesn’t matter whether you dig a hole and fill it up again — it doesn’t matter where the money goes — as long as you have that power. It’s about power, which is why they love it when the government goes from 20 percent of GDP to 25 percent. The more the better.



Regulations are another form of taxation, a burden on the economy. The cost of complying with regulations each year in this country is $1.75 trillion. The regulatory state, in other words, is bigger than every economy in the world except for the top three or four. We do that to ourselves with regulations.



James Madison, the father of the Constitution, wrote in _Federalist 51_ : “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.” We need sensible rules for the road. We need speed limits in school zones, for instance — but this is very different from the government telling you what, where, and when to drive.



C. Northcote Parkinson was a British naval historian and author of the bestseller _Parkinson’s Law_. Back in the 1920s, he noticed that the British Navy was sharply downsized after World War I, when they thought they weren’t going to have any more wars to fight. They had far fewer ships, far fewer crews, and far fewer dock workers. But what Parkinson noticed in particular was that the Admiralty, which ran the navy, was bigger than it was when the war ended! And he concluded that organizations grow — like weeds — until somebody stops them, no matter what may be the work at hand.



The same is true of all organizations. They all, if you leave them alone, lose sight of why they were originally created. But private markets serve as a check on this tendency. If you expand to fill the allotted time, without meeting the needs and wants of the market, you fail. Why, then, is the FDA still testing based on a model from 60 years ago — a period during which there have been countless major advances in medicine? Why are they letting countless cancer patients lead shortened lives by refusing to approve medications? It’s a power play, in a sea of cumbersome rules, with no incentive to streamline the process.



Ronald Reagan was right. He said that if you want to change minds in Washington, the best way to do it is through the heat of public opinion. It is not enough to have a change at the top and get a few new faces on Capitol Hill. Ideas matter — and we need to make the case that free enterprise works. Markets create social trust. Government destroys it. In the real world, even if you lust for money, you don’t get it unless you provide for others — and, without you knowing it, that creates circles of cooperation. Markets force you to look toward the future. That’s what the Cato Institute understands, and that’s the mentality that we must encourage to get others to understand as well.



 **MARY ANASTASIA O’GRADY** : Many of you are no doubt wondering what Latin America could possibly teach the United States — what with our muscular Constitution, open markets, limits on federal power, and independent central bank. (No snickering, please.) I was once like you. But, in the last few years, I have seen a number of frightening parallels between this country and our neighbors to the south. To be clear, those parallels did not begin with this president, but they have certainly become more pronounced under the current administration.



The fashionable explanation for Latin American underdevelopment blames corruption, lack of education, poor infrastructure, and — my personal favorite — a shortage of money. But these things are symptoms of bad policies, which I sum up as the Three Ps of poverty: populism, protectionism, and prohibition. Our challenges are, how do we keep politicians from turning us into government dependents? How do we keep markets open? How do we change drug laws in a way that prevents organized crime from replacing democratic institutions? Yet, I’m increasingly convinced that, just as corruption and poor infrastructure are byproducts of the Three Ps, so are the Three Ps byproducts of something else. The source of our economic troubles — in both Latin America and here — is, I believe, much more fundamental.



Consider two simple observations. First, to borrow a fundamental principle of the Cato Institute, ideas matter. To be more specific, those ideas that prevail in society as legitimate are what matter. And second, without entrepreneurship, it is impossible for a society to achieve prosperity.



Looking beyond the immediate policy challenges in Latin America, it becomes clear that it is the ideas of academia — and of intellectuals more broadly — that have played the most important role in undermining the entrepreneurial spirit in Latin American over the past century. Ideas that are hostile to entrepreneurship are not only part of the popular culture, they are embedded in the basic institutions of these countries.



At their core, these ideas hold that profits are morally suspect and private property is not justified, and it is these ideas that strike directly at the heart of prosperity for hundreds of millions of Latin Americans. How did this happen? As John Maynard Keynes wrote, “The ideas of economists and political philosophers, both when they are right and when they are wrong, are more powerful than is commonly understood. Indeed the world is ruled by little else.



Practical men who believe themselves to be quite exempt from any intellectual influence are usually the slaves of some defunct economist. Madmen in authority” — we won’t mention names — “who hear voices in the air are distilling their frenzy from some academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas.” This is a truism that Latin Americans did not understand until it was too late — and it is how we too will lose if we don’t emphasize the moral case for the market. Latin Americans, of course, have no problem being entrepreneurial.



Immigrants to the United States have a long history of starting their own businesses once they’ve landed. So why don’t they display these same skills at home? I submit to you that it is because the dominant ideas in the region over the last century have been hostile to entrepreneurship.



In a new book entitled _Redeemers: Ideas and Power in Latin America_ , the Mexican historian Enrique Krauze profiles 12 individuals who he believes represent the major political ideas in the region from the middle of the 19th century through the 20th.



He starts with José Martí and ends with Hugo Chávez — and along the way he includes profiles on Eva Perón, Che Guevara, Octavio Paz, Gabriel Garcia Márquez, and Bishop Samuel Ruiz, among others. These individuals, Krauze argues, were the ones who sowed the dominant political ideas over that time period. And those ideas focused on hostility toward individualism. Collectivism, economic equality, and the socialization of risk were the chosen themes of political philosophy — and it was the dissemination of these ideas that molded the norms and values of their respective countries. Not one name listed here, by the way, is an entrepreneur. I should add that Krauze also includes Mario Vargas Llosa in the group. He is not a collectivist but he is the exception to the rule.



The power of ideas was well understood among intellectuals on the left throughout the 20th century. They made it their business to get control of academia, and they succeeded. Take for example Venezuela, where the left got total control of the universities, and in the classroom a new narrative emerged. It gave the moral high ground to the state and denounced the market as immoral. Venezuela is reaping the fruits of that indoctrination today. Millions of Latin American students around the region have been marinated in that same stew. This view — that government redistribution is the source of justice and the market is greedy and wrought with failure — has had a profound effect on the political and economic climate in the region.



Today, the ideas of Che Guevara and Eva Perón have been discredited. Modern socialists — those who reject communism and fascism but favor some other form of collectivism — do not attack private enterprise head on. That would be suicidal because the market has created so much prosperity. They therefore emphasize not the wealth of nations, but the immorality of inequality. This, for socialists, is the soft underbelly of the market.



In societies where the morality of the market is understood, vigorously defended, and imparted to young minds, the ethics of collectivism doesn’t do very well. But Latin America shows what can happen when the market is not defended. Even in a society that has made economic gains by adopting free‐​market policies, if the population is not convinced of the legitimacy of the market, it will attempt to destroy what it has achieved.



Take Chile, where since last year students have been running wild in the streets, making all kinds of demands from their government, and accusing those who don’t give in of immorality. The tragedy is that the country’s establishment — including the president — has not been able to put up a strong defense. This is in Chile, the one place in the region that actually reduced poverty significantly. We should be thankful for scholars like José Piñera for carrying the torch of liberty in Chile. But the fact remains that while Chileans are beneficiaries of the market system, they don’t seem convinced of the morality of private property — and of differing outcomes.



Outside of Chile, things are even worse. In most of the region, the idea that equality is the highest goal was handed down from the ivory towers and enshrined in the constitutions themselves. Latin American constitutions are hundreds of pages long. They have objectives like guaranteed national development, the eradication of poverty, and the protection of cultural heritage. The 1988 Brazilian constitution offers constitutional rights for everything from health to education. It guarantees minimum salaries, yearend bonuses, and vacation pay. The section dedicated to sports specifies that “the government shall encourage leisure as a form of social promotion.”



Of course, who can object when the goal is to make the poor child more equal to the wealthy entrepreneur? The problem with a constitution that guarantees equality of outcome is that it cannot protect individual rights. It gives the government not only the power, but the obligation to use coercion toward that end. The fundamental problem with Latin development is this lack of liberty, which emanates from constitutional mandates that intrude on every aspect of human action.



What I’m describing originates with the intellectual class, of course, but many of these bad ideas in Latin America gained influence because the business class supported them. The 1961 Venezuelan constitution was, by most accounts, a fairly sound document. But factions, as James Madison would have called them, began to pick it apart. The business community played a key role.



Venezuelan journalist Carlos Ball described the process like this: “Many in the business community did not rebel against the growing state intrusion because they saw it was easier to convince one cabinet minister than a market of consumers. I’ll never forget watching Venezuelan businessmen cheering the nationalization of foreign oil companies, not realizing that the politicians would soon come after them with more controls, regulations, and taxes.”



The lesson is that when the state seizes the moral high ground in matters of personal decisions, there is no end to the steps that it will take to restrain liberty in the name of social justice. Our neighbors to the south have demonstrated it. You may think this can’t happen in the United States of America. Unfortunately, I am nowhere near as convinced.
"
"
This is environmentalism jumping the shark:

Click image above to play the game
I don’t know where to begin, except to say that when we see things like this, we should complain loudly and incessantly. The Australian Broadcasting Corporation has crossed a line beyond science, beyond decency, and beyond rational thought.
This is what you get after pressing “start”:
 
The screen above says: When you’re done, click on the (skull and crossbones) to find out what age you should die at so you don’t use more than your fair share of Earth’s resources!
Hat tip to CallonJim who writes:
This “kids” games at the Australian Broadcasting Corporation. Tell’s kids depending on their magical “carbon footprint” how long they should live?
The actual title is “Professor Schpinkee’s Greenhouse Calculator – find out when you should die!”
The thing I find amazing is the average foot print is 24.6 tonnes of CO2, which calculates out to 9.3 years old! Where it tells the child “YOU SHOULD DIE AT THE AGE 9.3!” Guess what age this kids games is marketed to? That’s right, 9 year olds.
What is most disgusting about this is that ABC ignores their own published Code of Practice
In section 2.12 they talk about content for children:
2.12 Content for Children. In providing enjoyable and enriching content for children, the ABC does not wish to conceal the real world from them. It can be important for the media, especially television, to help children understand and deal with situations which may include violence and danger. Special care should be taken to ensure that content which children are likely to watch or access unsupervised should not be harmful or disturbing to them.
I venture that any child who takes this carbon footprint test “unsupervised” without mommy and daddy around, and who may be old enough to read, but not old enough to understand he/she is being brainwashed by an agenda, would be “disturbed” find they should die at age nine, since just clicking through with default choices gives you that age.
Here is where you can contact the ABC and give them an inbox full of your opinion. This kind of propaganda needs to be removed.
http://www.abc.net.au/contact/contactabc.htm
UPDATE: There is a row developing in the Austrailian press over this.
UPDATE2: The New York Post highlights this site on June 1st with the headline “Enviro Mental Institution“


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9eb64813',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
For now, we have about 1 year of significant cold phase tendency in the Pacific Decadal Oscillation (PDO), here is the last 108 years of the PDO index, plotted from monthly values:

Click for larger image – source Steven Hare, University of Washington
Compared to the negative magnitudes seen from 1946 to 1977, our current PDO phase shift magnitude is relatively mild. But that could change. Don J. Easterbrook, a retired professor from the Dept. of Geology, Western Washington University, in Bellingham, WA sends this analysis:
la-nina-and-pacific-decadal-oscillation-cool-the-pacific (PDF)
The announcement by NASA’s Jet Propulsion Laboratory that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase (Fig. 1) is right on schedule as predicted by past climate and PDO changes.
Global temperatures peaked in 1998 and have not been exceeded since then. Pacific Ocean temperatures began a cooling phase in 1999 that was briefly interrupted by El Nino and dramatic cooling in 2007-2008 appears to be a continuation of a global cooling trend set up by the PDO cool phase (Fig. 1) as predicted [shown in the figure below].

Thus, we seem to be headed toward several decades of global cooling, rather than the catastrophic global warming predicted by IPCC. 
If we are lucky, this PDO will be a short event. 2-4 years. If we are unlucky, and it is the “full Monty” phase switch at 20-30 years as Easterbrook suggests, we may be in for extended cooler times. This may result in some significant extended worldwide effects, notably on agriculture.
UPDATE! Professor Easterbrook adds in comments:
“The projected warming from ~2040 to ~2070 is NOT driven by CO2, it’s merely a continuation of warm/cool cycles over the past 500 years, long before man-made CO2 could have been a factor. We’ve been warming up from the Little Ice Age at rate of about 1 degree or so per century and the 2040-70 projection is simply a continuation of non-AGW cycles. 
An interesting question is the similarity between what we are seeing now with sun spots and global temperature and the drop into the Little Ice Age from the Medieval Warm Period. Could we be about to repeat that? Only time will tell–We might see a more pronounced cool period like the 1880 to 1910 cool cycle (when many temp records were set) or a milder cooling like the 1945-1977 cool cycle. In any case, the setting up of the cool phase of the PDO seems to suggest cooler times ahead, not the catastrophic warming predicted by IPCC and Al Gore.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f700a29',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Veteran Meteorologist Joe Bastardi of AccuWeather on Al Gore’s 60 minutes interview:
I am absolutely astounded that someone who refuses to publicly debate anyone on this matter and has no training in the field narrated a movie where frames of nuclear explosions were interspersed in a subliminal way in scenes of droughts and flood, among other major gaffes, can say these things and then have them accepted… by anyone.
See the complete writeup here on the AccuWeather Blog
If you wish to write letters to CBS New regarding the issue, see my post on the same subject here.
(h/t Jim Arndt)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea000efca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Would a liability insurance mandate for firearm owners provide an effective means of gun control? In the latest issue of _Regulation_ , Stephen G. Gillis and Nelson Lund examine this alternative, which rests on the principle of competitive pressure.



Insurance companies would have an incentive to keep premiums for low‐​risk gun owners low, while charging higher premiums to those who are more likely to cause injury to others. “The benefits to public safety would be modest,” the authors ultimately conclude, “but such a regulation would be preferable to many politically popular gun control proposals that would be ineffective, unconstitutional, or both.”



Timothy D. Lytton next considers our inadequate system of food regulation — pointing in particular to misrepresentative food labeling and outbreaks of illness. These problems, he notes, underscore “the shortcomings of government food regulation and the inadequacy of industry self‐​regulation.”



Yet, there is one niche market in the industry that points to an alternative solution. “The success of kosher food certification offers a model of independent, private certification that could improve food safety and labeling,” Lytton writes, “and point the way toward regulatory reform in other areas such as finance and health care.”



Patrick J. Michaels and Paul C. Knappenberger ask why climate change assessments overlook the differences between the models being used and the empirical data. Jagadeesh Gokhale considers a new approach to Social Security disability insurance reform, noting that more of the disabled would return to work if they faced better incentives.



Other contributors include Ike Brannon, who tackles the economics of sports stadiums in “Could Dan Snyder End Publicly Financed Stadiums?” and M. Todd Henderson, who considers ways to improve corporate governance in “Reconceptualizing Corporate Boards.”



The Fall 2013 issue features book reviews on the causes of and response to the financial crisis, what ended history’s great empires from ancient Rome to modern America, and why state‐​led humanitarian efforts typically fall short of their stated goals. It wraps up with editor Peter Van Doren’s survey of recent academic papers, as well as a final word from Tim Rowland on the demise of automotive dealerships.
"
"
Share this...FacebookTwitterIn an interview with flagship daily Frankfurter Allgemeine Zeitung (FAZ here), Max-Planck Institute for Meteorology (MPIM) Director Dr. Jochen Marotzke said predicting how many degrees of warming we need to prepare for was like reading tea leaves and that he is not worried about “climate tipping points”. 
He also spoke of the wide disagreement among climate models.

Max-Planck Institute for Meteorology (MPIM) Director Dr. Jochen Marotzke told the FAZ he doesn’t worry about climate “tipping points”, but worries about panic. Image: MPIM. 
He told the FAZ that the worst case scenarios put out by some models were useful for the purpose of risk assessment, i.e. scenarios that are unlikely but cannot be ruled out. “In the latest generation of models, there are some models that are much more sensitive to greenhouse gases than previous models in terms of their temperature increase,” he said.
Five degrees “very very unlikely”
When asked about the results of the French model released earlier this year, which assumes five degrees of warming for a doubling of atmospheric CO2, Marotzke expressed his amazement, telling the FAZ what he thought of the French scientists: “My God, what are you doing? Because it is very, very unlikely that the true climate is as sensitive as these new models show.”
“The issue of climate sensitivity is extremely complex. Therefore, the results of a model should first be treated with caution,” Marotzke said.
When asked why the French model produced such a high warming for a doubling of CO”, Marotzke said he didn’t know why: “No one understands why they published it without first reflecting. The British did it differently, they said the new value is a mystery to us. They first want to investigate what the reason is and whether the warming rate is realistic.”
No worries about climate tipping points
Later in the interview, the FAZ touched on the so-called “tipping points in the climate system”, which are “threshold values that set irreversible processes in motion that, once started, can no longer be stopped.” Possible tipping points named by some scientists include the Greenland Ice Sheet, Gulf Stream, West Antarctica:, coral reefs, Amazon dying etc.
On whether they could happen, Marotzke views it as “conceivable” and that it “cannot be ruled out” and with “almost all of them we don’t know where we stand.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




When asked which one is most worrying, he replied: “None”.
“I don’t see any risk with Greenland”
And not even the melting of the Greenland ice sheet worries the MPIM Director. He told the FAZ:  “It’s gonna take so long – a couple thousand years. I don’t see any risk with Greenland.”
Arctic not a tipping element
On the subject of the Arctic, Marotzke says he is “quite sure that it is not a tipping point” – and that the ice albedo feedback “is not the dominant effect”.
“The ice comes back every year – in winter, said Marotzke, who has been Director at the MPIM in Hamburg since 2003. “When the temperature goes down again, the sea ice will come back.”
No worries about thawing permafrost
He is also not worried about the permafrost thawing, saying the contribution to warming “is relatively small.”
“Besides, even if the permafrost thaws, it is uncertain how much of the methane actually reaches the atmosphere,” said Marotzke. “Methane can be converted by bacteria to CO₂. I am not worried about methane.”
Worries “panic will backfire”
When asked about what he is worried about, he replies: “That the panic will backfire.” Marotzke warns against spreading panic: ” It can become incendiary. The question is, at what point do the risks of climate protection measures exceed the risks of climate change? Panic does not help here, only relatively sober analysis and weighing up – and a democratic discussion will help.”
Hat-tip: Die kalte Sonne. 


		jQuery(document).ready(function(){
			jQuery('#dd_1ad68e0372d18ffba913ecc7236487c6').on('change', function() {
			  jQuery('#amount_1ad68e0372d18ffba913ecc7236487c6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Europe is slowly disarming. For decades the continent could rely on America to fill the gap. No longer. That realization has given France pause. Maybe other European states also will start taking their security responsibilities more seriously.



A new report from the European Union’s Institute for Security Studies acknowledged an unpleasant reality—centuries of the West’s and especially Europe’s “dominance are currently giving way to a more multipolar and less governable world system.” That wouldn’t be such a problem if the change was not combined with diminishing military capabilities, again especially in Europe.





There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



Noted ISS: “Failing to act, therefore, means that a mixture of acute budgetary pressures, lack of investment in research development, and widespread reluctance to make the maintenance of effective armed forces a political priority could cause additional reductions in EU military capacity as well as a potential exodus of the defense industry and a loss of technological leadership.” The Europeans are spending ever less, with “the budget cuts carried out so far have been made without any coordination and consultation among allies.” Moreover, European governments are spending unwisely, emphasizing personnel and land‐​based facilities, for instance.



A continuing reduction in capabilities seems likely if not quite inevitable because Europe no longer faces any serious, let alone existential, threats. Russia is a poor replacement for the Soviet Union. It is impossible to build a plausible scenario for Russian troops threatening Warsaw or Berlin, let alone Paris or London. Moscow still might beat up on its neighboring constituent republics, such as Georgia, but the latter actually started their war.



China might become a peer competitor of America, but it has no European ambitions. Balkan instability is no substitute for potential aggression from whomever. North Africa and the Middle East generate continual geopolitical complications, but getting involved usually creates even greater problems. Even a nuclear Iran—an unpleasant prospect, to be sure—seems unlikely to target Europe. About all that’s left for Europe’s militaries are distant nation building, anti‐​pirate sea patrols, and play‐​acting like a _Weltmacht_.



EU leaders still might talk about creating a continental foreign policy and military, and national politicians still might want armed forces capable of doing more than providing an honor guard for foreign dignitaries, but European peoples exhibit little interest in paying the resulting bill. Spending more efficiently and collaborating more extensively would help, but the continent’s ongoing Euro crisis, recession, and heavy indebtedness all encourage further retrenchment. One visiting NATO official told a private, off‐​the‐​record gathering in Washington, “There is no chance for budget increases, not even for keeping spending levels as they are.” Earlier this year Rasmussen declared, “There is a lower limit on how little we can spend on defense.” But what is it?



This is a prescription for eventual European disarmament, but a slight sign of hope is flickering in France. Although modern French presidents don’t look much like reincarnations of Emperor Napoleon, they are not shrinking violets internationally. Both Presidents Nicolas Sarkozy and Francois Hollande had wars they wanted to fight—Libya and Mali, respectively. However, they both found Paris to be unable to fight without assistance, primarily from America.



Europe’s rising enthusiasm for war is ironic. Observed Philip Stephens in the _Financial Times_ : “Europeans have caught the interventionist bug just as the U.S. has shaken it off.”



However, France’s financial difficulties created pressure for additional cuts in military outlays. The Hollande government recently released its defense review, known as the _livre blanc_. Although the government reduced its rapid‐​deployment forces, it “opted to keep France’s air, ground and sea capabilities, while freezing defense budgets over six years,” noted the _Economist_. Outlays will shrink in real terms and as a percentage of GDP, but “dark talk of the loss of 50,000 jobs proved unfounded. The planned yearly cuts will be smaller than under the previous president, Nicolas Sarkozy. France will maintain its capability for expeditionary warfare, and boost special forces.”



One reason for this is Gallic pride, even ego. President Hollande explained: “France’s destiny is to be a global nation and our duty is to guarantee not only our own security but that of our allies and partners.” In doing so, he added, “France wants to maintain its ability to react alone.” How could it be any other way?



Opposition legislators complained that the proposed force was inadequate for such a role. Vincent Desportes, former director of a military school, told the _New York Times_ that the plan “makes France a really minor actor in coalition operations.” However, a budget increase was inconceivable in today’s economic climate.



The second reason is more significant. Paris apparently realized that if it is going to continue to be a “global nation,” it no longer could expect as much help from across the Pond. As the _livre blanc_ delicately put it, Americans will “prove more selective in their overseas engagements.” This led to one conclusion. Noted the _Economist_ : “One arresting element is the recognition that France may have to step up militarily in the Mediterranean and Africa as America pulls back.”



That requires not just sufficient forces but the right forces. Defense Minister Jean‐​Yves Le Drian called some of his nation’s deficiencies “incomprehensible,” requiring Paris to spend more on aerial refueling and other specialties. Said Le Drian, new investments “seem to me inevitable, like intelligence and special forces.”



This may be a seminal moment for European defense policy. Explained Francois Heisbourg of the Foundation for Strategic Research: “Planning to operate in a world where the Americans will be in only a supporting role changes everything. It is essential that we get the right kit to do it.”



Hallelujah!



It long has been obvious that Washington’s promise to protect prosperous and populous allies created a disincentive for them to do more for their own defense. During the Cold War the Europeans routinely violated their promises to hike military expenditures, even in the face of the numerically superior Red Army. Japan hid behind its pacifist constitution and kept military (“self defense”) outlays below 1 percent of GDP. Since the mid‐​1990s South Korea has skimped on its armed‐​forces budgets while providing the North with $10 billion worth of assistance as part of the Sunshine Policy—even as North Korea threatened to turn Seoul into a “lake of fire.”



A lack of capacity did not stop Britain and France from pushing for war with Libya, though they received only limited support from other European states and had to go to Washington for additional assistance. However, American officials have demonstrated far greater reluctance to join the Syrian civil war. As the U.S. further reduces both capabilities and obligations, even Paris realizes that Washington might say no to its next war proposal.



Which means France must do more than it really wished. But Paris apparently will do what it must.



U.S. policymakers should learn from this experience. Instead of bashing the Europeans, insisting that they spend more when they see no compelling reason to do so, Washington should simply shed the burden of Europe’s defense. Inform America’s long‐​time friends and allies that the cheap ride is over. Then let the Europeans decide how much they want to spend to defend what. And allow them to bear the consequences.



The same goes for the Balkans, Mediterranean, Central Asia and Middle East. Whether the issue is Kosovo, Libya, Georgia or Syria, absent a compelling interest for America military action should be up to Brussels, or Paris, London and Berlin. If they decide not to act, no worries. There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



There’s still substantial room for security cooperation. And Washington obviously could help the Europeans become militarily self‐​sufficient. But the time for a U.S.-dominated alliance is over.



Economists long have told us that incentives matter. France’s behavior proves that they do. When Paris believed that it could rely on Uncle Sucker, the former did one thing. When the French realized that the Yanks really might not be coming, they did something different. Washington needs to send the same message to the rest of its defense dependents.
"
"
U.S. Senate Report: Over 400 Prominent Scientists Disputed Man-Made Global Warming Claims in 2007 
from this link:
http://epw.senate.gov/public/index.cfm?FuseAction=Minority.SenateReport
The variety and reach of this is quite large. This is a report gathered from many independent publications, it is not one of those “Internet Petitions” which can be easily loaded up with fake names by those that seek to minimize it.
I think the gist of this is that the pronouncements of “the science is settled”, the “debate is over” and “scientific consensus” may be a bit premature.
Of course I’m sure we’ll have those that will denounce this for a variety of the usual reasons, such as the favorite “they are all employed or supported by the fossil fuel industry”.  But given the diversity on this list that will be pretty hard to prove.
For those interested in my work on the www.surfacestations.org project, this set of preliminary data posted here on 460 out of 1221 USHCN climate stations in the continental USA pretty well sums it up:

 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea203e59c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Current Wisdom is a monthly series in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



The Current Wisdom only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



This year’s installment of the United Nations’ annual climate summit (technically known as the 16th meeting of the Conference of the Parties to the Framework Convention on Climate Change) has come and gone in Cancun. Nothing substantial came of it policy‐​wise; just the usual attempts by the developing world to shake down our already shaky economy in the name of climate change. News‐​wise probably the biggest story was that during the conference, Cancun broke an all time daily low temperature record. Last year’s confab in Copenhagen was pelted by snowstorms and subsumed in miserable cold. President Obama attended, failed to forge any meaningful agreement, and fled back to beat a rare Washington blizzard. He lost.



But surely as every holiday season now includes one of these enormous jamborees, dire climate stories appeared daily. Polar bear cubs are endangered! Glaciers are melting!!



Or so beat the largely overhyped drums, based upon this or that press release from Greenpeace or the World Wildlife Fund.



And, of course, no one bothered to mention a blockbuster paper appearing in Nature the day before the end of the Cancun confab, which reassures us that Greenland’s ice cap and glaciers are a lot more stable than alarmists would have us believe. That would include Al Gore, fond of his lurid maps showing the melting all of Greenland’s ice submerging Florida.



Ain’t gonna happen.



The disaster scenario goes like this: Summer temperatures in Greenland are warming, leading to increased melting and the formation of ephemeral lakes on the ice surface. This water eventually finds a crevasse and then a way down thousands of feet to the bottom of a glacier, where it lubricates the underlying surface, accelerating the seaward march of the ice. Increase the temperature even more and massive amounts deposit into the ocean by the year 2100, catastrophically raising sea levels.



According to Christian Schoof of the University of British Columbia (UBC), “The conventional view has been that meltwater permeates the ice from the surface and pools under the base of the ice sheet… .This water then serves as a lubricant between the glacier and the earth underneath it… .”



And, according to Schoof, that’s just not the way things work. A UBC press release about his Nature article noted that he found that “a steady meltwater supply from gradual warming may in fact slow down the glacier flow, while sudden water input could cause glaciers to speed up and spread.”



Indeed, Schoof finds that sudden water inputs, such as would occur with heavy rain, are responsible for glacial accelerations, but these last only one or a few days.



The bottom line? A warming climate has very little to do with accelerating ice flow, but weather events do.



How important is this? According to University of Leeds Professor Andrew Shepherd, who studies glaciers via satellite, “This study provides an elegant solution to one of the two key ice sheet instability problems” noted by the United Nations in their last (2007) climate compendium. “It turns out that, contrary to popular belief, Greenland ice sheet flow might not be accelerated by increased melting after all,” he added.



I’m not so sure that those who hold the “popular belief” can explain why Greenland’s ice didn’t melt away thousands of years ago. For millennia, after the end of the last ice age (approximately 11,000 years ago) strong evidence indicates that the Eurasian arctic averaged nearly 13°F warmer in July than it is now.



That’s because there are trees buried and preserved in the acidic Siberian tundra, and they can be carbon dated. Where there is no forest today — because it’s too cold in summer — there were trees, all the way to the Arctic Ocean and even on some of the remote Arctic islands that are bare today. And, back then, thanks to the remnants of continental ice, the Arctic Ocean was smaller and the North American and Eurasian landmasses extended further north.



That work was by Glen MacDonald, from UCLA’s Geography Department. In his landmark 2000 paper in Quaternary Research, he noted that the only way that the Arctic could become so warm is for there to be a massive incursion of warm water from the Atlantic Ocean. The only “gate” through which that can flow is the Greenland Strait, between Greenland and Scandinavia.



So, Greenland had to have been warmer for several millennia, too.



Now let’s do a little math to see if the “popular belief” about Greenland ever had any basis in reality.



In 2009 University of Copenhagen’s B. M. Vinther and 13 coauthors published the definitive history of Greenland climate back to the ice age, studying ice cores taken over the entire landmass. An exceedingly conservative interpretation of their results is that Greenland was 1.5°C (2.7°F) warmer for the period from 5,000‑9000 years ago, which is also the warm period in Eurasia that MacDonald detected. The integrated warming is given by multiplying the time (4,000 years) by the warming (1.5°), and works out (in Celsius) to 6,000 “degree‐​years.” 



Now let’s assume that our dreaded emissions of carbon dioxide spike the temperature there some 4°C. Since we cannot burn fossil fuel forever, let’s put this in over 200 years. That’s a pretty liberal estimate given that the temperature there still hasn’t exceeded values seen before in the 20th century. Anyway, we get 800 (4 x 200) degree‐​years.



If the ice didn’t come tumbling off Greenland after 6,000 degree‐​years, how is it going to do so after only 800? The integrated warming of Greenland in the post‐​ice‐​age warming (referred to as the “climatic optimum” in textbooks published prior to global warming hysteria) is over seven times what humans can accomplish in 200 years. Why do we even worry about this?



So we can all sleep a bit better. Florida will survive. And, we can also rest assured that the UN will continue its outrageous holiday parties, accomplishing nothing, but living large. Next year’s is in Durban, South Africa, yet another remote warm spot hours of Jet‐​A away.



 **References:**



MacDonald, G. M., et al., 2000. Holocene treeline history and climatic change across Northern Eurasia. Quaternary Research 53, 302–311.  
Schoof, C., 2010. Ice‐​sheet acceleration driven by melt supply variability. Nature 468, 803–805.  
Vinther, B.M., et al., 2009. Holocene thinning of the Greenland ice sheet. Nature 461, 385–388.
"
"
Share this...FacebookTwitterOur friend “SnowFan” here looks at the claims that September 2020 was the warmest ever recorded. It turns out that other measurement advanced satellites don’t agree.
According to the much ballyhooed data, temperatures in Europe in September this year were on average 0.2 degrees Celsius higher than in the previous record September 2018. The service providing the data is part of the European earth observation program Copernicus.
But the satellite data from the UAH and RSS both agree that this is not really the case!

Above the global satellite data from UAH (left) and from RSS (right) in the tables clearly clearly show the monthly deviations from the WMO mean 1981-2010 (UAH) and from the climate mean 1979-1998 (RSS): September 2020 was not the warmest since satellite measurements began in 1979. At UAH, September 2019 was slightly warmer while at RSS even September 2017 was warmer.
Strong La Nina may be in the works


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Overall the globe’s surface continues to cool since the peak of the 2015/16 El Nino, and that cooling will very likely continue if NASA and the US National Weather Service projections are correct. Both Agencies see a significant La Nina in the pipeline for 2021.

The current ENSO forecasts of NASA (left) and NOAA (right) from October 6, 2020, predict an unusually strong La Niña in the equatorial Pacific with temperature deviations down to -3°C and with unusually long duration until the NH summer of 2021.
Such a strong event would certainly lead remarkable surface cooling. Source: BOM ENSO models with additions.


 


		jQuery(document).ready(function(){
			jQuery('#dd_e2c39a96e5018d087a7844243e936550').on('change', function() {
			  jQuery('#amount_e2c39a96e5018d087a7844243e936550').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Some things are sacred to scientists: Facts, data, quantitative analysis, and _Nature_ magazine, long recognized as the world’s most prestigious science periodical. 



Lately, many have begun to wonder if Jayson Blair has a new job as their science editor. On page 616 of the April 8 issue, _Nature_ published an article using a technique that they said, on page 593 _of the same issue,_ was “oversold”, was inappropriately influencing policymakers, and was “misunderstood by those in search of immediate results.” 



The technique is called “regional climate modeling,” which attempts to simulate the effects of global warming over areas the size of, say, the United States. 



As reported by Quirin Schiermeier, scientists at a Lund, Sweden climate conference, “admitted privately that the immediate benefits of regional climate modeling have been oversold in exercises such as the Clinton administration’s US regional climate assessment, which sought to evaluate the impact of climate change on each part of the country.” 



Then, 23 pages later, _Nature_ published an alarming and completely misleading article predicting the melting of the entire Greenland ice cap in 1,000 years, thanks to pernicious human economic activity, i.e., global warming, using a regional climate projection. 



The lower 48 states comprise 2 percent of the globe. Schiermeier reported that the consensus of scientists is that climate models on such a small scale are inappropriate for policy purposes. Greenland covers 0.4 percent of the planet. If the models are no good over the U.S., they’re worse over Greenland. Yet the authors “conclude that the Greenland ice‐​sheet is likely to be eliminated by anthropogenic climate change unless much more substantial emission reductions are made than those envisaged by the IPCC [a United Nations Panel].” 



The Greenland paper, by Jonathan Gregory and two others, was profoundly misleading, offering any climate alarmist an incredible sound bite attributable to our most prestigious science publication. 



The first paragraph states: “The Greenland ice‐​sheet would melt…if the annual average temperature in Greenland increases by more than 3°C [5.4°F]. This could raise global average sea‐​level by 7 meters [23 feet] over a period of 1,000 years or more.” 



Guaranteed, that quote will be on _Hardball_ on May 28, the day that the non‐​science fiction global warming flick, _The Day After Tomorrow_ comes out. It’s ironclad. After all, it’s from _Nature._



And it’s also deceptive. It’s not a warming of 5.4°F that causes the massive meltdown. Instead, it’s an annual warming of an impossible 14°F. Given the way greenhouse warming splits between summer and winter, this implies an outlandish 30°F change in the winter, fueled by a world that would have to be producing carbon dioxide at a rate far beyond anything remotely possible. It is the most extreme scenario in a pack of outlandish future emission scenarios that the U.N. cooked up a few years ago. They actually call them “storylines,” which is appropriate, since they make little sense. 



For example, one of the major storylines assumes that people increasingly favor personal wealth over environmental protection, which is absurd. The richer a nation is, the richer a city is, or the more affluent a neighborhood is, the more it protects its environment. 



How did the first paragraph get by the editors at _Nature?_ Either they weren’t looking or they thought it was OK. Take your pick. 



It’s not the first time, either. Just as scientists “admitted privately” that the models don’t work, so have prestigious environmental journalists told me privately that they are concerned about _Nature’_ s handling of global warming stories, both in terms of increasingly shoddy reviews and timing clearly designed to influence policy. No one has forgotten that in 1996 _Nature_ featured a paper, right before the most important U.N. conference leading to the Kyoto protocol, “proving” that models forecasting disastrous warming were right. The paper was subsequently found to have used data selectively to generate its dire result. 



Note to _Nature:_ Even journalists, normally your friends on global warming, are getting suspicious. 



The Greenland paper is truly an exercise in virtual reality. The threshold for melting is based upon a uniform annual temperature rise. But, every scientist knows that greenhouse effect warming is much greater in winter, when the authors say “no melting takes place.” When they account for this (not reporting how they did so), fully one‐​third of their scenarios fall below the melt threshold. Only when they assume what is patently untrue do almost all the scenarios result in a net melting, and only the most extreme, illogical ones completely melt things. 



This is nothing but tragic, junk science, published by what is (formerly?) the most prestigious science periodical in the world. There’s been a lot of hype‐​much of it from scientists themselves‐​over global warming, but nothing as sacrilegious as this, in such a sacred place. 
"
"
Share this...FacebookTwitterLast year Germany’s Potsdam Institute (PIK) boasted that it had a superior El Niño one-year forecasting model, claiming 80% certainty. Today, a year later, its forecast emerges totally wrong and the prestigious institute is left humiliated. 
Hat-tip: Snowfan
In 2019, Germany’s Potsdam Climate Institute (PIK) boasted that it had a superior El Niño forecasting model, claiming one year in advance and with 80% certainty, there would be an El Niño event late in 2020 (upper curve is just an El Niño illustration). But the PIK model forecast flopped totally. The opposite has in fact emerged. Chart source: BOM (with additions).
One year ago, together with researchers of the Justus Liebig University Giessen (JLU), and Bar-Ilan University in Ramat Gan in Israel, Germany’s alarmist yet highly regarded Potsdam Institute for Climate Research (PIK) boldly declared in a press release there would “probably be another ‘El Niño’ by the end of 2020.”
PIK even boasted forecast model superiority
The PIK November 2019 press release bragged that its team of researchers had developed a new, far better model – which they said was capable of forecasting a late 2020 El Niño event a year in advance: “The prediction models commonly used do not yet see any signs of this,” the PIK press release wrote.
The PIK press release then called the early forecasting model approach “groundbreaking”, claiming it was based on a “novel algorithm” developed by its team. Their forecast relied “on a network analysis of air temperatures in the Pacific region and which correctly predicted the last two ‘El Niño’ events more than a year in advance.”
The results were even published in a journal: https://arxiv.org/abs/1910.14642


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Conventional methods are unable to make a reliable ‘El Niño’ forecast more than six months in advance. With our method, we have roughly doubled the previous warning time,” stressed JLU physicist Armin Bunde, who initiated the development of the algorithm together with his former PhD student Josef Ludescher.
John Schellnhuber: “80% certainty”…”pretty significant”
Prof. Hans-Joachim (John) Schellnhuber, Director Emeritus of PIK, explained: “This clever combination of measured data and mathematics gives us unique insights – and we make these available to the people affected.” He pointed out that, of course, the prediction method did not offer one hundred percent certainty: “The probability of ‘El Niño’ coming in 2020 is around 80 percent. But that’s pretty significant.”
The 20% uncertainty ends up humiliating PIK physicists
Using data from the past and with the help of of their algorithm, the PIK scientists said El Niño events could then be “accurately predicted the year before”.
Today, one year later, in November 2020, we see that the opposite is in fact occurring, see chart above. Now the equatorial Pacific is entering a La Niña event instead of the almost certain El Niño claimed earlier by the now embarrassed PIK researchers.
Can’t even get one climate component over a single year right
The PIK’s “high certainty” forecast misses totally and so underscores the risks and pitfalls of being overconfident when it comes to still poorly understood complex systems.
And if scientists struggle predicting just one single regional component of the entire climate for just one year, then imagine what the reliability of their complete climate system predictions going out decades has to be. GIGO!


		jQuery(document).ready(function(){
			jQuery('#dd_6a44e38d298891ff14bbe070b492c213').on('change', function() {
			  jQuery('#amount_6a44e38d298891ff14bbe070b492c213').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

One of the classic examples of the failure of politicians to communicate with the citizenry is found in a video of Romanian tyrant Nicolae Ceausescu, giving what turned out to be his last speech to the teeming masses gathered in a square in Bucharest.



Oblivious to the mood of the people, Ceausescu is at his bombastic, self‐​important best until he realizes that the chants from the crowd below are not praise, but something rather to the contrary. The look on his face is, as they say in the MasterCard commercials, priceless.



America is a democratic republic, complete with an excellent Constitution that politicians still feel compelled to acknowledge, if not take seriously. So, the growing communications gap between the average American and the average politician, while worrisome, is not irreparable. Solving it should be a high priority for all involved.



The communication problem involves the accelerating realization on the part of many Americans that the essence of America, namely, a respect for the dignity of the individual, which inherently involves the government leaving the individual alone, has been pretty much forgotten by the politicians in Washington, D.C., the state capitals, and city councils around the nation. Which explains why public employees now make on average 30 percent more than their private sector counterparts — and 70 percent more in benefits. The political class seems to believe they have carte blanche to do as they please. They turn a deaf ear to increasingly vocal expressions of frustration by the American people.



Take, for example, a town hall meeting in Washington state last summer in which a young Marine veteran said to six‐​term Rep. Brian Baird, “Now I heard you say tonight about educating our children, indoctrinating our children, whatever you want to call it.” The congressman denied wanting to indoctrinate, but the young father simply responded, “Stay away from my kids.” Virtually all of the 400 or so people in the hall rose as one in loud applause. It was a Ceasusescu moment. The congressman had no clue the people of his district weren’t interested in the federal government concerning itself with the education of their children.



The politicians simply do not get it. The Declaration of Independence says governments are created to secure our rights to life, liberty, and the pursuit of happiness. In other words, to leave us the hell alone. That is what makes for American exceptionalism, despite President Obama’s claim that all nations are exceptional. No, they are not, not in the way America is.



As I write these words, across my desk comes a press release from Bloomberg telling me that 18- term Rep. Henry Waxman wants Congress to ban the use of smokeless tobacco in Major League Baseball dugouts. This is part of our communications problem. Read my lips, Henry: It is none of Congress’s business if baseball players want to use smokeless tobacco (or any other kind of weed, for that matter). And this is the encouraging thing about the Tea Party movement. It is made up of average Americans who are sick to death of politicians regulating, taxing, controlling, and limiting individual choice.



This bipartisan communications problem is also exemplified by a joint press conference held just before the start of the lamentable 111th Congress by Senate Majority Leader Harry Reid and Minority Leader Mitch McConnell. Said Reid, “Sen. McConnell and I believe … that we are going to work in a bipartisan basis … to solve the problems of the American people.” Whoa! See how simple the communications problem is? They think we sent them to Congress to solve our problems when we actually sent them there to see to it that we are left alone to solve our own problems.



Add to that the fact that many, if not most, of our problems have been created by Congress in the first place and we have the basis for a healthy peaceful revolution. Some 85 percent of Americans like their healthcare, so Congress shoves a government‐​mandated system down our throats. Taxes are way too burdensome, so Congress is contemplating a valueadded tax to add to our burden. We spend billions of dollars on wars in the Middle East for no rational reason. Climate change proves to be a wildly exaggerated issue, yet Congress still plans on raising taxes on energy to solve this non‐​existent problem. The list is long, and the frustration grows daily. Talk about a failure to communicate. According to a recent Pew Research Center Poll, 78 percent of Americans don’t trust the federal government. As Ronald Reagan famously put it, “The nine most terrifying words in the English language are: ‘I’m from the government and I’m here to help.’ ”
"
"
Earlier I wrote up an essay on the NOAA climate station at Cordova, AK.
Click thumbnail at left for a larger image. 
  
This station was directly next to the village diesel power plant. That station also happens to be part of the NASA GISS surface temperature record used for climate research. The problem is the proximity to nearby human caused heat sources, which may not be accurately adjusted for in the record. Of course the real issue is that if the stations were properly setup and maintained by NOAA, paying attention to their own 100 foot rule, such potential bias would not be an issue. Today I’d like to show you a few other NOAA climate stations in Alaska.
Click thumbnails below for larger images.
Thanks to John Papineau for these photographs






English Bay – note the MMTS temperature sensor within about 1 foot of the building.No cold winter nights for this sensor!


 



Moose Pass – note the concrete structure which is a fish hatchery
NCDC record says: HATCHERY, OUTSIDE & 3 MI NW OF PO AT MOOSE PASS, AK


 




Susitna Landing – note proximity to building this was installed on May 21st, 2003

NCDC record says: FLAT GRAVEL AREA NEAR CONFLUENCE OF KASHWITNA AND SUSITNA RIVERS. How would a researcher know about the building proximity from this?




 


  
  
Seward 19N – note proximity to building
NCDC Record says: OBSERVERS HOME, OUTSIDE & 19.5 MI N OF PO AT SEWARD, AK Again, how would a researcher know about the building proximity?





 





Seward #2 – note proximity to street and shading issues. You can see the station location in Google Earth.




 



Tutka Bay – note proximity to building and weathering of old Stevenson Screen shelter.


 


As I’ve been saying, the MMTS temperature sensor and it’s cable is systematically forcing measurements closer to human influences. They problem clearly is not unique to the continental United States as these photos from Alaska demonstrate.
In all of Alaska’s open wilderness, are these truly representative of the climate? It seems that every station is close to the small packets of towns and villages that dot Alaska, and necessarily so, since a human observer is required to read and record the thermometer.
Surely though, a better job at station siting could have been done.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea15855be',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

**_Study one of first to look at little‐​known success story_**



Report: Drug Decriminalization Works in Portugal 

In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 

On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 

With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 

Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 

Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.



In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 



On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 



With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 



Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 



Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.
"
"

“In December 1919, Carlo ‘Charles’ Ponzi approached a group of friends and acquaintances in Boston with a new investment opportunity,” Cato senior fellow Michael Tanner writes. What followed has become immortalized as one of the most infamous investment scams in history. In **“Social Security, Ponzi Schemes, and the Need for Reform”** (Policy Analysis no. 689), Tanner considers recent calls comparing this fraudulent operation with the current U.S. social insurance program. The two programs, he says, have several similarities. Social Security, for instance, “does not actually save or invest any of a participant’s payments” — relying instead on inflows from future contributors to finance the system. This, in turn, provides “a windfall to the first participants, but declining returns to subsequent joiners” — also similar in operation to a Ponzi scheme. Finally, Social Security is “a system that worked well when demographics were favorable,” yet it’s “facing insolvency as the ratio of recipients to contributors increases.” Despite these similarities, there is in the end one crucial distinction between the two. “Social Security is not a Ponzi scheme,” Tanner concludes, “because Charles Ponzi didn’t have a gun.” As such, the debate over epithets obscures a much deeper issue: Social Security is unable to pay promised benefits with current levels of taxation. “In short,” Tanner writes, “the program is facing insolvency without fundamental reform.”



 **Drug Violence Flaring in Mexico**  
In December 2006, President Felipe CalderÃ³n of Mexico launched a military‐​led offensive against his country’s increasingly violent narcotics trade. In **“Undermining Mexico’s Dangerous Drug Cartels”** (Policy Analysis no. 688), Cato senior fellow Ted Galen Carpenter argues that this campaign is not simply ineffective: “It is a futile, utopian crusade that has produced an array of ugly, bloody side effects,” he writes. Many are now questioning whether Mexico is on its way to becoming a “failed state.” While Carpenter determines that these fears are overblown, he nevertheless acknowledges that “the overall trend is troubling.” By the same token, he notes that the extent of a spillover of violence and corruption into the United States has been limited — yet the possibility of turf battles becoming proxy wars is “a harbinger of deterioration of the security situation on our southern border.” By examining several alternatives to the current approach, Carpenter finds that one stands out above the rest. “The most feasible and effective strategy to counter the mounting turmoil in Mexico is to drastically reduce the potential revenue flows to the trafficking organizations,” he writes. This hinges on abandoning the prohibitionist model in favor of full legalization. “The fire of drug‐​related violence is flaring to an alarming extent in Mexico,” he concludes. Restricting the damage will require swift action, “before that fire consumes our neighbor’s home and threatens our own.”



 **War and the Practice of Politics**  
Article I of the United States Constitution vests the power to “declare War” in Congress, leaving to the executive the power to “repel sudden attacks.” But in the years since the Cold War, the practice of initiating limited conflicts has blurred these constitutional distinctions. In **“Congress Surrenders the War Powers: Libya, the United Nations, and the Constitution”** (Policy Analysis no. 687), John Samples, director of Cato’s Center for Representative Government, examines a number of smaller wars — namely, those in Bosnia, Somalia, Kosovo, Iraq, and most recently, Libya — and reaches several conclusions. First, the president has arrogated “largely unfettered powers” to launch wars that are “half‐​made” — conflicts he feels “are essential to fight and yet beyond constitutional propriety.” Second, while sometimes critical, Congress tends to defer to presidential command when these wars are both brief and popular. Third, Congress’s active “investigations and criticisms can affect the conduct of a limited war but not its inception.” On the other hand, while the public is often skeptical that limited conflicts are worth the cost, their “desire for congressional authorization of such wars goes unfulfilled.” Finally, Samples finds that, in Libya in particular, an incremental transfer of these powers to international institutions — also known as weak internationalism — ” contravenes values central to American republicanism.” As such, he concludes, “law becomes over time a function of, not a constraint on, the practice of politics.”



 **The Ivory Tower’s Burden**  
“If you follow higher education — or just live near a college or university — you’ve probably heard the complaint: government keeps axing higher education funding,” writes Neal McCluskey, associate director of Cato’s Center for Educational Freedom, in **“How Much Ivory Does This Tower Need? What We Spend on, and Get from, Higher Education”** (Policy Analysis no. 686). The problem is that there is little evidence to support this claim. While most analysts rely on public funding as a share of overall school revenues, McCluskey examines the burden of postsecondary education borne by taxpayers — the most direct measure of public support — and one that is “typically ignored in anecdote‐​driven media stories.” What do these numbers suggest? “No matter how you slice it, the burden of funding the Ivory Tower has grown heavier on the backs of taxpaying citizens,” he writes. In fact, the burden on the individual taxpayer has risen from $426 in 1995 to $532 in 2010, a 25‐​percent increase. But this is only part of the higher‐​education story. The real question is whether human capital has expanded along with this increased investment. McCluskey finds that the increased flow of dollars has “underwritten poor academic results, rampant price inflation, and considerable college inefficiencies.” “The money taken from taxpayers,” he concludes, “to ‘invest’ in higher education has been on the rise, and it appears to be hurting both taxpayers individually and society as a whole.”



 **Malpractice Caps Hurt Patients**  
Supporters of capping court awards for medical malpractice argue that such caps will make health care more affordable. But is this necessarily the case? In **“Could Mandatory Caps on Medical Malpractice Damages Harm Consumers?”** (Policy Analysis no. 685), economist Shirley Svorny of California State University, an adjunct scholar at the Cato Institute, says that it may not be so simple. In reviewing the structure of the medical liability insurance industry, Svorny begins by offering a key insight. “The decades‐​old conventional wisdom holds that medical malpractice insurers rarely adjust premiums to reflect an individual physician’s risk,” she writes. This assumption, however, is misplaced. As Svorny illustrates, the industry has developed a complex, “interdependent system of physician evaluation, penalties, and oversight” — all of which is based upon the threat of legal liability for negligence. Patients, in turn, derive protections from this oversight. In short, she writes, “the evidence presented here shows that physicians pay the price for putting patients at risk.” Svorny draws on interviews with underwriters and brokers, published sources, and an extensive analysis of state insurance company rate filings to make her case — showing that premiums “act as signals that steer physicians toward higher‐​quality care.” As such, the implication is clear. “Capping court awards, all else equal, will reduce the resources allocated to medical professional liability underwriting and oversight,” she argues, “and make many patients worse off.” The study generated a lively online discussion at the Manhattan Institute’s PointofLaw website.



 **Lessons from Deepwater Horizon**  
On April 20, 2010, an explosion on the Deepwater Horizon offshore drilling unit led to the largest accidental oil spill in the history of the petroleum industry. What lessons have emerged in the year since the well has been declared “effectively dead”? Richard L. Gordon, professor emeritus of mineral economics at Pennsylvania State University and an adjunct scholar at the Cato Institute, argues in **“The Gulf Oil Spill: Lessons for Public Policy”** (Policy Analysis no. 684) that the resulting political backlash uncovers longstanding issues with the attempt to regulate commercial activities. “The underlying problem is a mythology that holds that public lands are precious resources needing careful government management,” he writes. This isn’t the case. By examining the political response — particularly the Waxman‐​Markey bill — he underscores the real issue. “The failure was in fact due to the impotence of the very policy initiatives that the Obama administration wishes to expand,” he writes. Gordon carefully deconstructs the “tangential campaigns” against foreign oil imports, oil consumption, and climate change — making it clear that “the only thing these concerns have in common is their invalidity.” The ideal solution, he contends, is privatization of federal lands. In the interim, Gordon demonstrates that the Gulf oil spill reflects the problems associated not only with our command‐​andcontrol energy strategy, but with government oversight in general. “The real lesson of the oil spill,” he concludes, “is the familiar point that bad policies beget bad consequences.”
"
"
Share this...FacebookTwitterThe prominent German online news magazine  FOCUS  reports that 2010 may set a new NASA high temperature record. The cause of the recent warmth is El Nino. But FOCUS then throws ice-cold water on any warmist dream of an overheating planet, at least for the next few years, and writes that scientists believe: “Womöglich aber sind die warmen Zeiten für unseren Globus bald vorüber”.
In English:
Quite possibly, the warm times for the planet will soon be over.
The FOCUS report looks at three factors, which I present in 3 parts. 
Part 1: La Nina
FOCUS first zeroes in on La Nina, and quotes AccuWeather meteorologist Joe Bastardi:
There are wild cards in the climate system that have changed the previous climate events. Now we’ve got a weak solar cycle and the prospect of increased volcanic activity. Together with a La Nina, it all could be a troublesome triple whammy.
FOCUS also quotes Joe D´Aleo of TV Weather Channel:
We’ll have La Nina conditions before the summer is over, and it will intensify further through the fall and winter. Thus we’ll have cooler temperatures for the next couple of years.
Part 2: Solar Activity
The next big factor is the sun, which has worried a number of scientists over the last couple of years. It refuses to start-up with a new cycle. 2008 had 266 spotless days and 2009 had 261.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




FOCUS writes:
Now some scientists fear the solar slumber could herald in a new Little Ice Age. This period, which extended from the 15th to the 19th century, was characterised by bitter cold winters and cool, wet summers which left grains and crops rotting in the fields.
FOCUS magazine then acknowledges the Maunder and Dalton Minimums, thus indirectly refuting Mann’s version of climate history. The German media is waking up!
FOCUS then quotes Joe D’Aleo:
If the number of spots does not climb over 40 or 50 during the next maximum, which would mean a low level of solar energy, then we have to reckon with much lower temperatures in the coming years.
Part 3: Volcanoes
Volcanoes in Iceland are coming alive. So far the ash clouds have been too small to have any effects on the climate. The real risk, however, is that it may be a foreboding of something much worse to come – the eruption of the mighty neighbouring Katla volcano. Katla has a far more immense chamber of magma. It erupts on average every 70 years and in tandem with Eyjafjalla. The last Katla eruption was in 1918, thus making an eruption overdue.
According to Joe Bastardi:
Katla could be a game-changer. If it erupts and throws ash and sulfur particles into the stratosphere, then the global temperature will plummet.
The triple whammy of La Nina, low solar activity and increased volcanic activity all acting together would certainly put global warming on the back burner for a while. But some scientists, like Prof. Mojib Latif of the University of Kiel, insist that warming will resume once the cooling factors fade off, and that global temperature increases of 5°C by the end of the century cannot be excluded.
In the meantime, get ready for cooling.
Share this...FacebookTwitter "
"
I happened across a NOAA internal training manual a couple of weeks ago that contained a photo of a USHCN official climate station that I thought I’d never get a photo of.  The Baltimore Customs House.
 
Baltimore USHCN station circa 1990’s photo courtesy NOAA, click for more images
What is interesting about this station, is that it is a rooftop station, like we’ve seen in San Francisco, Eureka, and many other US cities. Rooftop stations are suspected to impart a warm bias to the surface temperature records, for obvious reasons. The NWS/NOAA has been reluctant to change these stations to ground-level, wanting to keep a continuous record. The Baltimore USHCN station closed in 1999 and has not been replaced at this location.

From this single photo, and with the help of Google Earth and Microsoft Live Earth, I was able to complete this station survey, post mortem, giving it a CRN 5 rating. Below is one of the aerial photo links:
http://maps.live.com/default.aspx?v=2&cp=qjf9bk8mg9ks&style=o&lvl=2&tilt=-90&dir=0&alt=-1000&scene=7161999&encType=1
The Baltimore customs house is centered in the picture, you can see the old platform for the USHCN station is still there.

Earlier this year, www.Surfacestations.org volunteer John Goetz located an early historical photo of the Baltimore station, seen below:
 
But, this NOAA internal training manual not only cited this example photographically, they also did a correlation study that proved that the rooftop placement was actually warmer. 
Here is what NOAA said about the Baltimore USHCN station in their training manual:
Instrument exposure standards are really compromised with rooftop locations (figure 11).

Figure 11: An Early Rooftop Meteorological Station.
While the number of NOAA rooftop climate stations has remained at about 40 for the last decade, the number of private rooftop stations has grown during that period into the thousands. Rooftop exposures have an advantage of increased instrument security and good exposure for wind sensors (standard height is about 33 feet). However, there are also drawbacks. Access for maintenance can be difficult and exposure for precipitation and temperature instrumentation is clearly non-compliant, being elevated to high above the ground. Additionally the instrument exposure is usually over environmentally nonrepresentative surfaces (metal, black tar, shingle, stone etc.), while at the same time being close to a wide variety of roof surfaces which are subject to change.

No argument there. The trade off is security versus representivity of climate. But climate usually loses in a rooftop station instance.
They go on to say:
The unrepresentative-ness of rooftop temperature and precipitation data was discovered long ago after studies quantified the biases. The late Professor Helmut Landsburg, considered the “father of climatology”, stated in his 1942 “bible of climatology” textbook, “Physical Climatology” that:

“Climate derived from records of roof stations may by no means be representative of those at the ground level.”

In another published paper 28 years later, Professor Landsburg again reiterated his concern about rooftop exposures with respect to the urban warming issue: “They [rooftop stations] are certainly of little value in a full assessment of the climatic changes brought about by urbanization.”
That leads one to wonder why they kept these stations at all, let alone appoint them as “High Quality” USHCN stations for use in climate research. The Baltimore Custom House is also a GISS station. They also write:
Rooftops make good observation sites if you live work, play, or grow your food on a roof. Unfortunately, few people do any of the above. Rooftop exposures have been shown to exhibit biases towards warm temperatures (both maximum and minimum) and lower precipitation when compared to ground based stations. The warm temperature biases likely result from extreme daytime heating of artificial rooftop surfaces, reduced cooling of the roof at night, and from heat flow from within the building, especially in winter. The biases can be substantial. One limited study indicates 5 to 10 degrees on summer days with bright sun and light winds. Biases have been found to vary significantly, depending on many factors (location on the roof, color of roof, type of roof surface (rock, metal, etc.) time of year, etc when compared to standard ground-based sensors. On the flip side, if a station has been on a non-changing roof for decades, the site may have good continuity (value) for tracking climate change and variability. For some climate applications, consistency with a long record can be more important than accuracy with a shorter record. 
The best of both worlds is to have an exposure compliant, long-term station.
But the thing that really hit me was the data they compiled, comparing to other nearby stations, and thus proving the case for rooftop bias with this station:

They cite the table with:
The table to its right summarizes a comparison of 12 months of overlapping data that was collected on the rooftop and at the new relocated site (for data continuity), relocated several blocks away at ground level with other nearby standard, ground based stations. A combination of the rooftop and downtown urban siting explain the regular occurrence of extremely warm temperatures. Compared to nearby ground-level instruments and nearby airports and surrounding COOPs, it is clear that a strong warm bias exists, partially because of the rooftop location.
Maximum and minimum temperatures are elevated, especially in the summer. The number of 80 plus minimum temperatures during the one-year of data overlap was 13 on the roof and zero at three surrounding LCD airports, the close by ground-based inner Baltimore harbor site, and all 10 COOPs in the same NCDC climate zone. Eighty-degree minimum are luckily, an extremely rare occurrence in the mid-Atlantic region at standard ground-based stations, urban or otherwise. Temperatures can be elevated on roofs due to the higher solar radiation absorption and re-radiation associated with many roof surfaces including black tar, shingles, stone, and metal. During the colder months, ongoing upward heat transfer through the roof from the heated interior of the building also can contribute to the warm bias although stronger winter winds tend to create better mixing and minimize this impact.

The table shows that the rooftop station has Tmax >90°F more than twice as often  as other stations and a Tmax >100°F  13 times where no nearby station achieved it. Similarly we have this station recording a Tmin >80°F where no other stations did.
Yet amazingly, knowing all this, stations like this, and stations that have instrumental biases such as Tucson, with its parking lot placement (USHCN) and HO83 problems(GISS) still remain as part of the USHCN and GISS datasets. The official all-time high temperature record of Tucson of 117°F still stands, set by a known faulty HO83 thermometer.
In the case of Baltimore, the question is, in the plot below what really has been measured? Is it city growth, building energy use/dissipation, rooftop albedo variations, nearby building changes, or climate change? Given that it is impossible to disentangle all these things, the data, in my opinion, should be deemed compromised and discarded.
         
GISTEMP Plot of Baltimore City USHCN station #180470
In any other line of scientific study or in engineering, data that has been so badly compromised would likely be forced out by peer review, or the researchers themselves once the errors were discovered. Yet here we are today, keeping this station record for use in climatological study.
Reference: NOAA Professional Competency Unit 6 (PCU6) manual (PDF)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1894ab0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFrom environmental heroes – to national shame
The whole publicity stunt was probably supposed to go something like as follows: To protest against forest clearance to make way for a new stretch of autobahn, a group of 9 masked German tree-huggers would rappel from a speedway overpass and hang a banner demanding that the planned deforestation be stopped. And for their courageous activism, they’d surely make the regional news – maybe even the national news – and draw needed attention to man’s ruthless destruction of nature. Of course they’d be adored by the public as environmental heroes. They’d maybe even hold press conferences – and be surrounded by TV cameras and mikes.
Except for the national attention, things didn’t quite work out that way for the group of German radical environmentalist tree-huggers after tragedy struck.
According German daily Bild here, they ended up causing an 8-kilometer traffic jam and one “horror accident” as a 29-year old driver suffered serious injuries and had to be airlifted to a trauma center.
One German national daily labelled the activists as environmental idiots.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image cropped from Bild online here: 
According to Bild, the “anti-autobahn activists” blocked the A3 autobahn near Idstein as two of them “rappelled off an overpass” and hung a banner across it. Their publicity stunt caused the fast moving traffic to jam to a sudden halt, causing an 8-kilomter traffic jam. The 29-year old driver mentioned above failed to noticed the end of the traffic jam in time and crashed into the rear of a truck.
“A 29-year-old drove into the end of the traffic jam and his Skoda crashed into the back of a truck – the man was seriously injured,” online Bild reports.
“Horror crash because of this idiot,” was the headline in Bild’s hard copy print edition this morning, with an arrow pointing at a young female activist hanging from the overpass. Another Bild photo showed authorities dragging off one of the protesters.
The online Bild here also reports that one protester is still being detained by the police. But: “Six have been released.”
“A protest that endangers human life has no legitimacy,” said Home Secretary of Hesse, Peter Beuth. “Anyone who endangers his fellow citizen has to be punished severely.”


		jQuery(document).ready(function(){
			jQuery('#dd_0755afe815319cbec0ef6652a11c042f').on('change', function() {
			  jQuery('#amount_0755afe815319cbec0ef6652a11c042f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe COVID 19-pandemic has led the German public to grow weary and distrustful of the “follow the science” mantra and to realize that it means losing liberty and fundamental rights.
Political shifts and public opinion changes are afoot in Germany as libertarians and conservatives become every more disenchanted with the restrictions.

Tens of thousands gathered in Berlin in August 1st to protest government restrictions. Image cropped here, Michael Ballweg.
Tide change in Berlin 
Last weekend tens of thousands (estimates range from 10,000 to over 1,000,000) demonstrated in Berlin to protest government restrictions aimed at limiting the spread of the COVID-19 virus. The protest was also about the limitation of free speech and the right to assemble.
Desperate, ruling politicians and their allied mainstream media responded by defaming the protesters as nutcases rather than treating them as respectable working citizens and taking their grievances seriously. The gross mistreatment by politicians and media only confirmed to many demonstrators that they’ve gone too far.
Citizens demonstrating in Stuttgart
This weekend German citizens have turned out by the thousands in Stuttgart and other cities to continue their protests and demand their freedom and rights back. A large part of the public refuses to “just believe” the government experts. In their view the science is not settled at all, and there’s no reason to continue refraining from their liberties.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




German Research Institute reverses
Another sign of changing opinion tides and distrust was the recent reversal by the German Research Foundation (DFG) which earlier had taken down a climate science critical comment by high profile satirist Dieter Nuhr from its website. In his comment Nuhr warned its reckless to “follow the science” because the science behind climate and COVID-19 is far from settled.
Under mass public pressure, the DLG reinstated Nuhr’s comment.
State now considered “a threat to civil liberty”
Moreover, the German Tagespost here warns German leaders to be wary of the current trend, and sees the political right in Germany to be transitioning away from statist traditions.
“At the latest since Hegel and in accordance with Prussian and Protestant traditions, the state had a promising, even metaphysical sound. That is currently changing,” the Tagespost comments.
US right-wing liberal thinking gaining influence
“While Chancellor Merkel was accused of state failure in the migration crisis, and thus still demanded a strong state that is able to protect its borders, the state itself is now increasingly becoming a problem. As in the USA, right-wing liberal thinking is gaining influence. The state is considered a threat to civil liberty.” adds the Tagespost. “Corona is currently whirling the relationship of the Germans to the state in a colorful confusion.”
The next major demonstration is planned for August 29 in Berlin.


		jQuery(document).ready(function(){
			jQuery('#dd_8e919b1a54fb1ca36f88be78165d8b92').on('change', function() {
			  jQuery('#amount_8e919b1a54fb1ca36f88be78165d8b92').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I was forwarded a slide show presentation done by Thomas Lowell et al of the University of Cincinnati titled: Organic Remains from the Istorvet Ice Cap, Liverpool Land, East Greenland: A Record of Late Holocene Climate Change
It was presented last week at AGU’s Greenland Climate Change Past and Present session. It has some very interesting data in it. In summary it has a report on occurrence of subfossil organic remains, with organics recovered in locations presently void of plant growth.

Picture of Istorvet organic remnants at edge of glacier melt.
The preliminary conclusion from the data collected in the field work is that presently the small ice caps at high latitudes in Greenland are retracting to locations where they were at 1000 years ago.  The presence of subfossil vegetation was found within 280 vertical meters of ice cap summit and where comparable modern assemblages do not exist. The implication seems to be that there were warmer periods in these areas prior to today, warm enough for plant growth.
According to the study, the organic material in Liverpool Land radiocarbon dates from 400 to 1015 AD. It is interesting to note that the Vikings settled in Greenland around 974 AD and the study indicates that ice cap expansion began around 1015 AD.

While the UC team that did the field work still has more work to do to reconstruct temperatures from this data, the study lends support to the idea that Greenland’s climate was warmer approximately 1000 years ago. One of the organic samples recovered at another location was dated to 910BC. This makes one wonder just how often shifts in Greenland’s climate occurs.
More study is needed, but this is certainly interesting. You can view the abstract here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea222cbf0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From the days of Wine and Gore department: The Second annual Climate Change and Wine Conference is scheduled for Feburary 15th and 16th in Madrid, Spain. Al Gore will be the featured speaker.
http://www.climatechangeandwine.com/eng/index.php
SALUD!
Wine snobbery and climate change together.  What’s not to like?
No word yet on whether California wineries will be attending.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1a1dcff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 
Click for magnified view of the sun showing the most recent spot.
Sunspot 987, 988, and now newly emerging 989 are shown above.
With all being near the equator, they are still a cycle 23 spots. A cycle 24 spot would be at a much higher latitude.
The most recent magnetogram shows them to have the magnetic polarity of cycle 23 spots, in addition to being near the equator.

Cycle 24 remains late. There was one sunspot of high latitude and reversed magnetic polarity on January 4th, 2008, but none have been seen since:

Click for a larger image
UPDATE 2: The solar holographic image shows a potentially large spot on the far side of the sun, we’ll have to wait until it comes around to see what it is. The method is not always perfect.

Darker area is the far side of the sun.
Seismic waves propagating through the sun are used to image potential spots on the far side. Here is a description of how it is done.
UPDATE 3:
It looks as if the spot seen yesterday on the far side of the sun via the holographic technique has disappeared. As I said “The method is not always perfect.”

The two spots above are earthward, 987, and 988.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea04b9a7d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Looks like its support is splintering:
Without widespread corporate support, passage of the bill – already a long shot at best – becomes even more unlikely this year. President Bush remains opposed. House Democrats have been slow to act.
From CNN Money.
According to the WSJ even Hillary and McCain are likely to stay away from it. Voting to increase your local energy prices due to a flawed cap and trade carbon tax scheme which will create 5 new government bureaucracies is never a good thing for somebody trying to get elected.
Even with chances of passage dwindling, write your senator to tell them how you feel about it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ed9440e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’m working on porting over to a new blogging platform. So there may be a delay in new content here. I’m trying out some ideas and themes, and it is looking promising.
When it’s all done, the URL will be announced. Stay tuned. Thanks to all who gave me feedback.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3aa00f2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"
One of the most surprising things I’ve learned from the surfacestations.org project is that for some odd reason, there are a number of climate monitoring stations of record in the USA at sewage treatment plants. If you’ve ever driven by one of these in the wintertime, they tend to look like steam saunas. They are localized heat bubbles from the waste-water processing.
At Orangeburg, SC not only is the official USHCN climate station of record at a sewage treatment plant, it’s also a nonstandard thermometer (a climate station normally looks like this), and strapped to the side of a telephone pole. I don’t know about you, but experience with creosote treated telephone poles tells me that they’d tend to create a hotter local measurement environment.

Photo by www.surfacestations.org volunteer surveyor Don Kostuch. See the complete image gallery here.
Then there’s the brick building to radiate heat at night, the asphalt parking lot, the effluent channel running nearby, and the overall sewage treatment plant waste heat to consider. I doubt there is an easily applicable set of equations which can untangle the myriad of potential microsite biases.
Then there’s sewage. As population growth occurs, sewage plants add more vats and equipment to handle the increased volume. The increased volume of effluent loses some of it’s heat at this location during the purification process.

Click graph for larger version, data from NASA GISS
The real question is: What are we actually measuring at this location? Are we measuring temperature as an indicator of climate change or are we measuring waste heat from increases in sewage processing that mirrors local population growth?
UPDATE and CORRECTION:
I made an error, this is not a sewage treatment plant. It does treat water, and the description in the site survey from the surveyor was “water filtration plant” which I mistook to mean “sewage treatment” since so many other locations have been at sewage treatment facilities. For example, one of the worst is Titusville, FL, which has been highlighted in this blog in Part 31 and also surveyed by the same volunteer. I looked at the photos he provided, and did not discern initially that the tanks were not for sewage treatment.  Some sharp eyed readers have pointed out the identification problem, which I’m happy to correct.
The questions about the validity of the temperature measurement environment in the midst of a sewage treatment plant are still valid, but do not apply to this location.  So the question we now have for this location is; do the large water filtration pools on this site provide an evaporative cooling effect or do they release heat?
The water vapor impacts at the facility are likely a factor, possibly for Tmin overnight, which is more prone to such effects. Note that there has been new construction at this location, and given the apparently new water filtration pools added on site, there may still be an effect of measuring the local population increases by proxy.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea279d29e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’t know what it is with weather stations at some universities. Of course we have the station at University of Arizona Tucson in the parking lot, and this one isn’t too far from that arrangement. It has a long and uninterrupted history, but what is it really measuring?

Click for a larger image
More pictures here
Thanks to surfacestations.org surveyor Craig Limesand we get to see the official USHCN climate station of record at Mount Mary College in Milwaukee, Wisconsin. You can see that the Stevenson Screen is just a few feet from parked cars.
This aerial view shows it better:

Click for a larger live interactive image
Note that in addition to being surrounded by asphalt and parked cars, the station is also about 35 yards from the college power plant.
According to NCDC MMS database, the station has been in this location since at least 1948, unmoved and using mercury max-min thermometers even today.
But without doing a historic evaluation to look at what transpired around the station during that history, how would we know how much is this signal is “climate change”, “UHI from Milwaukee”, or “increased parking capacity” or all of the above?
From NASA GISS, click for original source plot
 As much as I like weather stations, it is becoming clearer to me that looking for a clean climate change signal in surface data is a complex excercise in uncertainty.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dd7a201',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The purpose of this report is to provide a framework for doing research on the problem of bias in science, especially bias induced by Federal funding of research. In recent years the issue of bias in science has come under increasing scrutiny, including within the scientific community. Much of this scrutiny is focused on the potential for bias induced by the commercial funding of research. However, relatively little attention has been given to the potential role of Federal funding in fostering bias. The research question is clear: does biased funding skew research in a preferred direction, one that supports an agency mission, policy or paradigm?



Federal agencies spend many billion dollars a year on scientific research. Most of this is directly tied to the funding agency mission and existing policies. The issue is whether these financial ties lead to bias in favor of the existing policies, as well as to promoting new policies. Is the government buying science or support?



 **Our working definition of “funding‐​induced bias” is any scientific activity where the prospect of funding influences the result in a way that benefits the funder.**



While the basic concept of funding‐​induced bias is simple, the potential forms that this bias might take are far from simple. Science is a complex social system and funding is a major driver. In order to facilitate research into Federal funding and bias it is necessary to isolate specific kinds of bias. Thus the framework presented here is a taxonomy of funding‐​induced bias.



For the purposes of future research the concept of funding‐​induced bias is analyzed in the following ways:



1) the practices in science where bias can occur,  
2) how agency policy can create bias,  
3) the level at which bias is fostered, and  
4) indicators of bias.



Fifteen different scientific practices are analyzed, ranging from the budgeting and funding for research to the publishing and communication of results. For each of the fifteen practices there is a snapshot of the existing research literature on bias, plus a brief discussion of the directions that new research might take in looking into funding‐​induced bias. The potential for quantifying the extent of bias is also addressed.



In order to provide examples along the way there is a special focus on climate change. Federal policies on climate change and federal funding of climate research are both extensive. The linkage between these policies and research has become a major topic of discussion, including numerous allegations of bias.



The research framework provided here applies to the study of all funding‐​induced bias in science, not just to climate change science. The linkages between Federal policy and federally funded science are extensive and not well understood. Moreover, these linkages have policy implications, especially if they are inducing bias in scientific research. However, policy is not our topic here. Rather we are addressing the needed research that might lead to new policies.



In this report we are mostly concerned with individual types of funding induced bias. But there is an intrinsic sequence to the various biases we have identified and this raises the possibility of cascading amplification. By amplification we mean one biased activity is followed by another, such that the first bias is increased.   
  
A simple, and perhaps common, example of amplification might be when the hype in a press release is exaggerated in a news story. Let’s say the press release overstates the importance of the research result, but with some qualification. The news story then reports the result as a great breakthrough, far more strongly than the press release, ignoring the latter’s qualifications. In this way the original bias has been amplified. 



Cascading amplification when one biased activity is followed by multiple instances of amplification. Using our example, suppose a single biased press release generates many different news stories, which vie with one another for exaggeration. This one‐​to‐​many amplification is properly termed a cascade.



Moreover, there is the possibility of cascading amplification on a very large scale and over multiple biased stages. Here is an example of how it might work.



1) An agency receives biased funding for research from Congress.



2) They issue multiple biased Requests for Proposals (RFPs), and



3) multiple biased projects are selected for each RFP.



4) Many projects produce multiple biased articles, press releases, etc, 



5) many of these articles and releases generate multiple biased news stories, and



6) the resulting amplified bias is communicated to the public on a large scale.



One can see how in this instance a single funding activity, the agency budget, might eventually lead to hundreds or thousands of hyperbolic news stories. This would be a very large scale cascading amplification of funding‐​induced bias.



 _Climate Change Examples_



In the climate change debate there have been allegations of bias at each of the stages described above. Taken together this suggests the possibility that just such a large scale amplifying cascade has occurred or is occurring. Systematic research is needed to determine if this is actually the case. 



The notion of cascading systemic bias, induced by government funding, does not appear to have been studied much. This may be a big gap in research on science. Moreover, if this sort of bias is indeed widespread then there are serious implications for new policies, both at the Federal level and within the scientific community itself.
"
"

There is a lot not to like about the Quadrennial Defense Review, which comes out today (the _National Journal_ posted a leaked copy Friday). Like past QDRs, this one uses vague, trendy ideas about international relations to inflate threats and justify our massive defense budget. As usual, we hear the evidence‐​free claims that non‐​state actors are getting more powerful and that the world is getting more complex and unpredictable (“change continues to accelerate”). I believe that states are hanging onto or even gaining power relative to other sorts of social organizations and that the world is no less predictable than it was in 1900 or 1950. The QDR also says that climate change is a national security problem. That’s a popular line, which as near as I can tell is a marketing gimmick. Then there the usual tripe about how great our alliances are, how strategic every country with a Marine in it is, how terrific interagency cooperation is, and so forth.   
  
  
The good news is that it doesn’t really matter. Newspapers confuse the QDR with law, but it is closer to PR. It’s like a particularly important speech. It sells what Secretary of Defense is selling and justifies what the Department of Defense does. Because it comes in part from agencies it is supposed to guide, it rationalizes rather than leads. Because it is largely a consensus document, it says only what half of the Pentagon can agree on—various strains of mush. Can anyone explain what past QDR’s have accomplished? I think nothing. Sure, there are interesting tidbits about forces structure plans, but these are in the budget documents too. At best it causes DoD to justify itself, giving us analysts something to argue about.   
  
  
The administration’s proposed defense budget, also being released today, matters much more to policy. It reveals more about the nation’s defense strategy than the vacuous documents that purport to do so.   
  
  
Policy types love strategy documents because they are mostly technocratic idealists. They want government polices to be made by rational processes that reveal national interests, which are then laid out in plans like the QDR. They want policy to be like science. But democratic government is the push and pull of competing ideologies and interests. Public plans or strategies are part of that process. Congress should thank DoD for these mind‐​numbing 120 pages, throw them away, and focus on the budget.
"
"
Share this...FacebookTwitterHans von Storch’s blog brings our attention to an excellent German report by normally green ZDF public television.
The report takes a critical view of Europe’s energy policy and reaches the conclusion that it’s a failure. My last post Billions Of Euros For Nothing Called A Success Story illustrates this beautifully.
The ZDF interviews a leading finance researcher, Professor Dr Kai Konrad, and here’s what the ZDF report says:
– Start clip (German)-, content in English:
After 20 years of conference after conference after conference, a sort of traveling climate circus on a worldwide tour, Copenhagen became the highpoint of absurdity in December of last year – a political and media overkill with the aim of nothing less than to rescue the planet. The conference failed yet again. It all gets down to money.
Professor Dr. Kai Konrad is a distinguished finance researcher at the prestigious Max Planck Institute in Munich and a close advisor to the Federal Ministry of Finance. He and a team of researchers drew up an expert assessment of Germany’s climate policy.
The assessment was so damning that the Ministry quickly removed it from its website.
The assessment took a hard look at the 1st Commandment of climate policy: reduce CO2 emissions, and how a relatively small group of countries decided – unilaterally – to reduce CO2 emissions. The researchers writing the assessment deemed this a grave error. Professor Konrad says:
When a small group of countries sit down and say they want to  do something good for the climate, and reduce their emissions, it has practically no effect on the total amount of emissions worldwide. It means the rest of the world picks up the slack and just emits more.
In effect it means that the countries who cut emissions incur all the costs but no benefits. And the countries that don’t cut emissions, profit. So it’s highly worth it for these so-called “free-riders” who don’t sign on. What has the Kyoto protocol produced?
Since 1990 worldwide CO2 emissions have increased 36% and the few countries that have reduced their emissions have had immense costs, estimated to be $150 billion.
When it comes to CO2 emissions, the European Union is a global power. Especially Germany has been a leader in cutting emissions – already 20% less than 1990. Professor Konrad says:
The fact that Europe is a leader in cutting emissions will only lead to other countries slacking off, and thus the costs are merely shifted from the countries that don’t play along to Europe. So whatever progress Europe makes in cutting emissions just gets lost to countries like USA and China.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And so the circus goes on. The other countries are happy about the cuts, and the EU carries all the costs. Europe’s Climate Commissar estimates the costs will be:
€500 billion ($620 billion) in the next 10 years.
Germany is the leader in this craziness, and is expected to cut emissions by 40% by 2020. This is to be accomplished by Germany’s EEG Gesetz, or Energy Feed-in Act, which forces power companies to purchase renewable energy at exorbitant prices from anyone who produces them and to deliver them to consumers, who then must pay through the nose. Professor Konrad says (in summary):
From a theoretical point of view, the EEG brings no benefit. It brings nothing because the system of buying CO2 emissions certificates doesn’t work.
All the certificates do is ensure that the CO2 gets produced elsewhere. Professor Konrad:
The Feed-in Act is to be criticised in my view because it is no longer transparent as to what an enormous redistribution it creates and the huge subsidies that flow out of the pockets of consumers and into the hands of those who profit from it.
By the end of the year German consumers will have paid €62 billion ($75 billion) without seeing any CO2 reduction. In Professor Konrad’s and his colleagues’ view:
The policy of avoiding the production of CO2 is a failure, nationally and globally.
As a result, Professor Konrad’s recommendation is to use a different strategy (one that even the earliest and most primitive of man used):
A D A P T A T I O N
The researchers say this policy would be much more successful, and certainly much cheaper than the current CO2 elimination policy.
– End clip –
Now, I wonder if our clever politicians will muster the intelligence that even our early Neanderthal ancestors had millions of years ago, and adopt this strategy?
Don’t hold your breath.
Share this...FacebookTwitter "
"
The week was productive, 21 USHCN stations visited, 20 surveyed, one dropped due to access problems (Southport, NC which turned out to be at an Army Depot). My trip odometer said 1828 miles when I turned in the car in Nashville tonight.
Here is the map of my travels this week:

Click for an interactive map
The highlight of the week was of course my 2 day visit to NCDC and the survey of the new CRN station west of Asheville. Another fun moment in the trip came when I visited the Lewisburg, TN Agricultural Experiment Station. It was quite a pretty setting for a station:

While I was doing the survey, and looking for the MMTS which wasn’t near the Stevenson Screen but was indicated by the NCDC equipment log, a farm cat came by to say hello. He was quite the talker. He gave me the grand tour and followed me while I was looking around.

I asked him: “hey Kitty, have ya seen Hansen’s Bulldog around” ? He answered simply “meow” and then took off to the cattle barn. I kid you not.
Interesting thing about this trip, I identified two stations that have undergone undocumented station moves in the last year, which look like good test cases for detecting undocumented changes points via the new USHCN2 methodology. More on that later.
Footnote: While this is a lot of miles, it’s nothing compared to the mileage that Don Kostuch, Eric Gamberg, Russ Steele, and others have put in over the life of this project. I wish to thank them too.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fbaff9e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

With the newspapers full of crises, it can be hard to maintain a proper perspective on the progress humanity has made, and to remember that there are individuals striving every day to make the world a better place. In a recent interview, businessman and philanthropist Bill Gates discussed the improving state of humanity, and the work that he is doing through private charity to help those in need. He said,   




I think the idea that people are worried about problems, like climate change or terrorism or these challenges of the future, that’s okay. But boy, they really lose perspective of what’s happened over the last few hundred years. And how science and innovation have been a central factor of that. And I think that’s too bad, because people are lucky to live now. And they should see that progress is actually taking place faster during their lives than at any time in history.



One of the major initiatives of the Gates Foundation, for example, aims to eliminate polio. The data bear out how much progress has already been made towards that end:   






In 1980, about half of all children received the polio vaccine. Today, around 90% of children receive the vaccine, and eradication of the condition is in sight – just as people eradicated smallpox in 1979.   
  
  
Gates is also among the many caring individuals working to eliminate malaria and malnutrition, areas where humanity has already made great strides. Insecticide‐​treated mosquito nets, for example, protect more children from malaria in Sub‐​Saharan Africa:   






Malnutrition among children is also declining. In populous developing regions, such as East Asia and the Pacific, malnutrition affected about 20% of children in 1990. More must be done, but today malnutrition affects fewer than 6% of children in those areas.   






Even one child afflicted by polio, malaria, or malnutrition is too many, but the dramatic improvements the world has made on these fronts should be celebrated. Like Gates, while working to make the world better we must not lose a proper perspective on the progress humankind has already made.
"
"

**May 1:** In the name of “universal service,” the Telecommunications Act of 1996 empowers the Federal CommunicationsCommission to create public entitlements to advanced telecommunications services. At a Policy Forum on “UniversalService: Socializing the Telecommunications Infrastructure,” Milton Mueller, professor of communications at RutgersUniversity; Wayne Leighton, senior economist at Citizens for a Sound Economy; and Lawrence Gasman, director oftelecommunications and technology studies at the Cato Institute, addressed the question: Can mandated and subsidizedtelecommunications serve consumers better than competition? 



**May 8:** Auctions are heralded as the most beneficial means of allocating the airwaves that have been set aside for advancedtelevision services (ATV), including high‐​definition television. But if federal regulators insist that broadcasters be subject topublic‐​interest controls, must they offer broadcasters free ATV spectrum as a quid pro quo? That question was the focus of“Beyond Budgetary Concerns: A Free‐​Market Perspective on ATV Spectrum Auctions,” a Policy Forum featuringTom Hazlett, visiting scholar at the American Enterprise Institute; James Gattuso, vice president for policy research at Citizensfor a Sound Economy; and Bob Okun, vice president of NBC. 



**May 14:** Cato hosted a delegation from the Hungarian Embassy at a Roundtable Luncheon. The discussion with Cato staffand policy analysts centered on the Hungarian perspective on European security issues, expanding NATO, and U.S. troops inHungary. 



**May 15:** During a Policy Forum titled “Red Resurgence or Revitalized Reform? Russia’s Political Future,” SusanEisenhower, chairman of the Center for Post‐​Soviet Studies; Dmitry F. Mikheyev, senior fellow at the Hudson Institute; andAriel Cohen, senior policy analyst at the Heritage Foundation, discussed the prospects and implications of a possiblecommunist victory in Russia’s June election. 



**May 16:** Although natural gas deregulation is generally considered an economic success pregnant with valuable lessons forother industries, a web of regulatory oversight still surrounds the industry. Jerry Ellig of the Center for Market Processes andJoseph Kalt of Harvard University appeared at a Cato Book Forum to discuss their new book, _New Horizons in NaturalGas Deregulation_ , a collection of papers originally presented at a 1995 Cato conference. Ellig and Kalt reviewed the pastfailure of natural gas regulation, the lessons of regulatory reform, and how further deregulation should proceed in the 1990s. 



**May 22:** As chairman of the U.S. House of Representatives’ Task Force on Privatization, Rep. Scott Klug (R‐​Wis.) is aleading proponent of privatization in the 104th Congress. At a Policy Forum titled “Privatization: New Zealand’s SuccessStory,” Klug discussed his fact‐​finding mission to New Zealand and the success of that country’s privatization program. Inmeetings with railroad executives and sheep and dairy farmers living without subsidies, Klug learned lessons that Americansshould heed. 



**May 23:** The Cato Institute held its 14th Annual Monetary Conference, “The Future of Money in the Information Age.“The full‐​day conference addressed the technological viability and economic implications of digital currency, or “E-money.“Speakers included Scott Cook, chairman of Intuit, manufacturer of the popular business software “Quicken”; Rep. MichaelCastle (R‐​Del.), chairman of the House Subcommittee on Domestic and International Monetary Policy; and Jerry L. Jordan,president and CEO of the Federal Reserve Bank of Cleveland. For the first time, a Cato event was carried live via interactivetelevideo to nine sites around the country as well as broadcast on the Internet. Over 175 people attended the event inWashington, D.C., while others participated in New York, San Francisco, Chicago, Silicon Valley, and other places. 



**May 28:** As telephone services in New Zealand were deregulated, bureaucrats took a more “hands‐​off” approach than in theUnited States. Should U.S. regulators take a similarly minimalist approach, allowing even the terms of interconnection to be setby private negotiations? That was the topic of discussion during “New Zealand Telephone Deregulation: What Lessonsfor the United States?” a Policy Forum featuring Milton Mueller of Rutgers University, Jeff Rohlfs of Strategic PolicyResearch, and Joseph Farrell of the Federal Communications Commission. 



**May 29:** This year’s decertification of Colombia and the confirmation of Gen. Barry McCaffrey as “Drug Czar” suggest astepped‐​up effort in the war on drugs. With that in mind, Cato asked, “Does the International Drug War Make Sense?“at a Policy Forum featuring Robert Gelbard, assistant secretary of state for international narcotics and law enforcement affairs,and Kevin Jack Riley, author of _Snow Job? The War against International Cocaine Trafficking._ Gelbard reviewed thelogic of Washington’s international narcotics control strategies and explained how the United States plans to significantlyreduce the flow of drugs across its borders. Riley questioned the supply‐​side campaign, examined its impact on drug‐​sourcecountries, and assessed its prospects for success. 



**June 6:** Cato held its midyear Board of Directors Meeting. Board members set the Institute’s course and were brought upto date on Cato’s policy activities, progress in fundraising, and fiscal standing. 



**June 17:** In Seattle Cato hosted a City Seminar, “Leviathan and the New Millennium: An Agenda for Real Reform,“that featured a keynote address by Lawrence Kudlow, economic counsel at Laffer Advisors, Inc., and a panelist on CNBC’s _Strictly Business._ Other speakers included José Piñera, co‐​chairman of Cato’s Project on Social Security Privatization,Edward H. Crane, president of the Cato Institute, Stephen Moore, Cato’s director of fiscal policy studies, and MichaelTanner, Cato’s director of health and welfare studies. 



**June 18:** America’s security commitments abroad remain largely unchanged despite the end of the Cold War. Nowhere is thatmore evident than on the Korean peninsula, where a commitment of nearly 40,000 U.S. troops, costing billions of dollars ayear, threatens to draw the United States into any conflict that might erupt in Northeast Asia. Cato senior fellow DougBandow appeared at a Book Forum to discuss his new Cato book, _Tripwire: Korea and U.S. Foreign Policy in aChanged World._ Bandow argued that it is time to phase out the American military commitment to South Korea, which hastwice the population of North Korea and an economy 18 times as large as that of the North. That step would free the UnitedStates of an obsolete obligation and give South Korea responsibility for its own security. 



**June 19:** Cato hosted a City Seminar in San Francisco on “Leviathan and the New Millennium: An Agenda for RealReform.” The keynote address was given by Ward Connerly, a member of the Board of Regents of the University ofCalifornia and chairman of the California Civil Rights Initiative, which would outlaw racial quotas in state policy. Otherspeakers included José Piñera, co‐​chairman of Cato’s Project on Social Security Privatization and Edward H. Crane, StephenMoore, and Michael Tanner of the Cato Institute. 



**June 21:** In the postcommunist era Russia and Eastern Europe have implemented systems of parental choice in educationsimilar to the U.S. voucher concept. At a Book Forum for _Educational Freedom in Eastern Europe,_ author Charles L.Glenn, professor of education at Boston University, discussed his survey of educational reforms in 10 East European countriesand the lessons that America might learn. Denis P. Doyle of the Heritage Foundation commented. 



**June 26:** Living standards and rates of growth differ dramatically around the world. Robert J. Barro, Robert C. WaggonerProfessor of Economics at Harvard University and author of _Getting It Right: Markets and Choices in a Free Society,_ spoke at a Book Forum on what accounts for those disparities. He discussed the relationships among material progress anddemocracy, domestic institutions, and government policies and concluded that the rule of law has enormous explanatory poweras a factor in economic growth and that governments should provide markets with a stable framework of rules and then get outof the way. 



**June 27:** Sixty years ago the New Deal Supreme Court began unraveling the Constitution of limited government byreinterpreting first the general welfare clause and then the commerce clause. Recently, scholars and the Court have begun toreexamine the commerce clause jurisprudence that gave us the modern regulatory state, but little has been done with thejurisprudence of the general welfare clause that gave us the modern redistributive state. At a Book Forum, Leonard R.Sorenson, professor of history at Assumption College, discussed _Madison on the “General Welfare” of America,_ his newbook that provides a detailed refutation of scholars on whom members of today’s Court were schooled. Comments wereprovided by Judge Douglas H. Ginsburg of the U.S. Court of Appeals for the District of Columbia Circuit. 



**June 28:** International negotiations addressing the issue of global climate change have resumed in Geneva, and a recent reportfrom the Intergovernmental Panel on Climate Change (IPCC) has introduced a sense of urgency to those negotiations. Doesthat report really justify immediate governmental action to address global warming, or is it just another example of scientificsensationalism? At a recent Policy Forum, “The New IPCC Report: Scientific Consensus of Scientific Meltdown?“William O’Keefe of the Global Climate Coalition argued that the scientific “finds” of the report have been heavily anddisingenuously edited by political activists. Patrick Michaels, climatologist at the University of Virginia, similarly maintained thatthe report is so riddled with basic scientific errors as to be a completely unreliable guide for policymaking. 
"
"
Share this...FacebookTwitterA new paper reveals that climate models have failed to take important natural factors, such as the North Atlantic Oscillation, into account in their climate models on which leaders have been basing their policies. 

 A new paper in Nature says NAO not taken adequately into account by climate models. Image: see video (German) here.
Paper in Nature Criticizes NAO Hole: Medium-Term Climate Far More  Predictable Than Climate Models Suggest
By Die kalte Sonne
(German text translated/edited by P. Gosselin)
Can the ups and downs of the climate of the coming years and decades be predicted? A large group of researchers (Smith et al. 2020) affirms this and describes in a Nature article that much more is possible here than current climate models suggest.

Image Source: Smith et al., 2020
North Atlantic climate far more predictable than models imply
Quantifying signals and uncertainties in climate models is essential for the detection, attribution, prediction and projection of climate change1,2,3. Although inter-model agreement is high for large-scale temperature signals, dynamical changes in atmospheric circulation are very uncertain4. This leads to low confidence in regional projections, especially for precipitation, over the coming decades5,6. The chaotic nature of the climate system7,8,9 may also mean that signal uncertainties are largely irreducible. However, climate projections are difficult to verify until further observations become available. Here we assess retrospective climate model predictions of the past six decades and show that decadal variations in North Atlantic winter climate are highly predictable, despite a lack of agreement between individual model simulations and the poor predictive ability of raw model outputs. Crucially, current models underestimate the predictable signal (the predictable fraction of the total variability) of the North Atlantic Oscillation (the leading mode of variability in North Atlantic atmospheric circulation) by an order of magnitude. Consequently, compared to perfect models, 100 times as many ensemble members are needed in current models to extract this signal, and its effects on the climate are underestimated relative to other factors. To address these limitations, we implement a two-stage post-processing technique. We first adjust the variance of the ensemble-mean North Atlantic Oscillation forecast to match the observed variance of the predictable signal. We then select and use only the ensemble members with a North Atlantic Oscillation sufficiently close to the variance-adjusted ensemble-mean forecast North Atlantic Oscillation. This approach greatly improves decadal predictions of winter climate for Europe and eastern North America. Predictions of Atlantic multidecadal variability are also improved, suggesting that the North Atlantic Oscillation is not driven solely by Atlantic multidecadal variability. Our results highlight the need to understand why the signal-to-noise ratio is too small in current climate models10, and the extent to which correcting this model error would reduce uncertainties in regional climate change projections on timescales beyond a decade.
The authors looked at the behavior of the North Atlantic winter climate over the last six decades and found that there is basically good predictability. Yet, current climate models do not make use of this as they underestimate the influence of the North Atlantic Oscillation (NAO) by an entire order of magnitude. The noise is still too high in the climate models, and the real signal is lost.
The paper is so important that even Science commented on it at the end of July 2020
Missed wind patterns are throwing off climate forecasts of rain and storms
Climate scientists can confidently tie global warming to impacts such as sea-level rise and extreme heat. But ask how rising temperatures will affect rainfall and storms, and the answers get a lot shakier. For a long time, researchers chalked the problem up to natural variability in wind patterns—the inherently unpredictable fluctuations of a chaotic atmosphere.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now, however, a new analysis has found that the problem is not with the climate, it’s with the massive computer models designed to forecast its behavior. “The climate is much more predictable than we previously thought,” says Doug Smith, a climate scientist at the United Kingdom’s Met Office who led the 39-person effort published this week in Nature.
[…]
The study, which includes authors from several leading modeling centers, casts doubt on many forecasts of regional climate change, which are crucial for policymaking. It also means efforts to attribute specific weather events to global warming, now much in vogue, are rife with errors. “The whole thing is concerning,” says Isla Simpson, an atmospheric dynamicist and modeler at the National Center for Atmospheric Research, who was not involved in the study. “It could mean we’re not getting future climate projections right.”
You can read the entire commentary at Science.
Forecasts based on weak models
Now medium-term forecasts based on the weak computer models are being put to the test. Unfortunately, these are exactly the same forecasts that have already been used for political planning purposes. Now it is probably dawning on the latter: “the Science is NOT settled”.
Climate snake oil
Even the “attribution game” of extreme weather events, which is so popular in the media, is now being put on the back burner. Not good news for Friederike Otto, who has already been treated as an attribution superstar in the media, as she claimed to be able to calculate with percentage accuracy how much man was involved in a storm, flood or drought.
But wait, the German-language media did not even report on this important new paper…perhaps it did not fit into the climate narrative given from above. Fortunately, there is the Die kalte Sonne climate blog: That’s the first place you’ll hear it.
NAO ignored too long… stirring resistance
By the way, the systematic influence of the NAO on European temperatures has also been described by Lüdecke et al. 2020 a few months earlier. So it was a logical further step by Smith et al. 2020 to actively demand improvements in climate models.
The effect of the NAO has been known for decades. Surprisingly, it is only now that resistance is stirring in the scientific community, since it was actually clear that the models did not reflect these empirically well-documented relationships.


		jQuery(document).ready(function(){
			jQuery('#dd_b898986594013ea7fb3b1091acdc7faf').on('change', function() {
			  jQuery('#amount_b898986594013ea7fb3b1091acdc7faf').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Today we look at October mean temperatures for the emerald island country of Ireland, the Scandinavian country of Sweden and Finland.
Global warming alarmists claim that the globe is warming, which intuitively would tell us summers should be getting longer, which in turn would mean the start of fall is getting pushed back. In such a case, September and October temperatures should be warming, but they are not!
Cooling Ireland
First we plot the mean temperature for 7 stations in Ireland for the month of October, for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years:

Data source: JMA
Seven of 7 stations in Ireland have seen a strong cooling trend for October since 1995.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Nothing unusual happening in Greta’s Sweden
Next we look at the October trends for 6 stations in climate activist Greta Thunberg’s home country of Sweden.

Data Source: JMA. 
Three of 6 stations in Sweden show no warming trend for October. Greta can begin to calm down and stop worrying herself to death about climate doom. As is the case for all the charts shown here, we plot the stations for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years
Stable climate in Finland
Finally we look at Sweden’s Nordic neighbor of Finland. Here as well there’s little indication of a widespread warming.
Data source: JMA.
Three of 6 stations in Finland in fact showed a modest cooling trend for October over the past quarter century. There’s nothing to be alarmed about.


		jQuery(document).ready(function(){
			jQuery('#dd_8dedc33634186ee6cf8964dc52c4f36c').on('change', function() {
			  jQuery('#amount_8dedc33634186ee6cf8964dc52c4f36c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Guest post by John Goetz

I keep an active watch of the news for progress being made in the areas of renewable and alternative energy sources. One area that has caught my eye is algal fuel (biofuel produced by algae). One company that has been in the news lately is Sapphire Energy, which claims to be able to produce ASTM compliant 91-octane biogasoline. Sapphire Energy says their technology “requires only sunlight, CO2 and non-potable water – and can be produced at massive scale on non-arable land”.
I am not trying to pick on any one solution or Sapphire Energy in particular. I simply wondered how massive a scale of CO2 and non-arable land is needed to make a noticeable dent in our gasoline demand.
First, how much CO2 do we need? The IPCC guidelines for calculating emissions require that an oxidation factor of 0.99 be applied to gasoline’s carbon content to account for a small portion of the fuel that is not oxidized into CO2. To calculate the CO2 emissions from a gallon of fuel, the carbon emissions are multiplied by the ratio of the molecular weight of CO2 to the molecular weight of carbon, or 44/12. Thus, the IPCC says the CO2 emissions from a gallon of gasoline = 2,421 grams x 0.99 x (44/12) = 8,788 grams = 8.8 kg/gallon = 19.4 pounds/gallon.
Now let’s assume Sapphire Energy simply reverses the process and consumes the CO2 to produce gasoline. In other words, we take 19.4 pounds of CO2 out of the atmosphere for every gallon of gasoline we produce. This seems like is a nice “carbon neutral” process.
What is the cubic volume of atmosphere required to make 1 gallon of gas? Let’s assume for the moment an efficiency factor of 100%, meaning our process will consume 100% of the atmospheric CO2 it is fed. This is unrealistic, but it is unrealistic on the “optimistic” side. According to the EPA, one cubic meter of CO2 gas weighs 0.2294 lbs. At an atmospheric concentration level of 385ppm, one cubic meter of atmosphere contains 0.000088319 lbs of CO2. Thus, 19.4 / .000088319 = 219658 cubic meters (yes, I am ignoring the atmospheric density gradient as one moves from the ground upward, but hang with me). This equates to roughly 4553 gallons of gasoline per cubic kilometer of air.
According to the US Energy Information Administration, US gasoline consumption is currently averaging (4-week rolling) 9.027 million barrels of gasoline per day, or about 379 million gallons (42 gallons per barrel). Thus, to completely replace US gasoline consumption, Sapphire Energy would need to “scrub”, at 100% efficiency, just over 83000 cubic kilometers of air per day. Certainly there is plenty of air available – this volume represents less than 0.02% of the volume of air in the first 1 km of atmosphere. Nevertheless, it is an enormous  amount to process each day.
Of course, Sapphire Energy’s near-term goals are much more modest. As CEO Jason Pyle told Biomass Magazine, “the company is currently deploying a three-year pilot process with the goal of opening a 153 MMgy (10,000 barrel per day) production facility by 2011 at a site yet to be determined.” Using my fuzzy math above, that equates to a minimum of 92 cubic kilometers of air a day. Still seems like a lot.
So where will all of the CO2 come from?
Presumably the answer is coal-fired power plants. But let’s see if that makes sense. According to Science Daily, the top twelve CO2-emitting power plants in the US have total emissions of 236.8 million tons annually, or 1.3 billion pounds per day. Now, if that can be converted completely to gasoline, it would amount to 67 million gallons per day, or roughly 1/6 of the daily gasoline consumption.
(Science Daily refers to the twelve as the “dirty dozen,” which I found somewhat humorous given that CO2 is colorless and odorless, and is presumably needed to sustain some forms of life. But then again, so is dirt.)
Sounds great, except that a lot of land is needed to grow all that algae. According to Wikipedia, between 5,000 and 20,000 gallons of biodiesel can be produced per acre from algae per year. Assume for the moment that biogasoline can be produced at the same rate per acre. If we attempted to produce 67 million gallons of gasoline from our “dirty-dozen” every day, we would need between 1.2M and 4.9M acres of land to do this on. The low-end of the scale puts the area needed at more than that of Rhode Island. The high-end adds in Connecticut.
I kind of doubt there is that much land around each of the dirty dozen facilities. This means the gas would have to be sent by pipeline to a giant algae field. Given our ability to pipe oil and natural gas all over the place, sending CO2 across the country via pipeline is probably doable. There may also be plenty of unused or abandoned land (think abandoned oil fields) available to produce the gasoline. Nevertheless, the production scale and transportation logistics required to make this a viable alternative do indeed look massive.
So while the technology holds promise at the micro-scale, it remains to be seen what can actually be done at a scale that matters.
Talk among yourselves.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e6e362c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

There’s nothing wrong with your computer monitor, do not attempt to adjust the picture.
Normal blogging will resume shortly.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea39aedd2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMerkel No Longer Backs World Climate Treaty
That’s the headline announcing a report in the upcoming issue of Der Spiegel. German Chancellor Angela Merkel is retreating from the objective of reducing global CO2 emissions through a binding global treaty. http://www.spiegel.de/spiegel/vorab/0,1518,691013,00.html. 
…Merkel wants to avoid another debacle for Germany and Europe in the UN climate negotiations.
According to Der Spiegel, the climate conference that began in Petersberg near Bonn on Sunday shifts the focus to climate protection measures that can lead to measurable results without a binding treaty.
Federal Minister of the Environment Norbert Röttgen of Merkel’s CDU party told SPIEGEL of the new approach:
It’s not about giving up on the 2°C target; rather it is about finding new ways to reach it. At Petersburg  we want to create a new level on which we not only want to reach CO2 targets from the top,  but also to start projects from the bottom that lead to measurable results.
This includes protection of forests and more concrete cooperation in the transfer of environmentally friendly technologies.
Make no mistake about it, without the cooperation of Germany any global binding treaty mandating CO2 reductions becomes extremely unlikely, and that sends a clear signal that global Cap & Trade is all but dead. The US Senate can (and should) now kill Cap & Trade for good.
H/t: Rudolf Kipp at http://www.science-skeptical.de/
UPDATE…read the entire Der Spiegel article here in English (h/t Brian H): http://www.spiegel.de/international/germany/0,1518,691194,00.html
Share this...FacebookTwitter "
"
Recently I had some of my readers comment that they thought that The Weather Channel and USA Today (which uses TWC graphics) temperature maps seemed to look “hotter”. They suspected that the colors had changed. I tend to watch such things since my own company (IntelliWeather) produces similar maps.
I searched Google images for some saved older TWC maps, but found none. So I can’t be absolutely sure they have or have not changed.  But looking at the color scheme, nothing sticks out in my recollection of the temperature map colors.
But I decided that it would be an interesting exercise to compare USA national temperature maps from the commonly used services today. I saved national CURRENT temperature isotherms/gradient maps from around 03Z (11PM Eastern Time) tonight. All were generated within about an hour of each other.
What I found was surprising. Here they are in alphabetical order:
Intellicast: (probably the ugliest national temp map I’ve ever seen)



IntelliWeather:

NOAA-NWS:

Unisys:

Weather Central:

Weather Channel:

WeatherForYou:


Weather Underground:

A couple of notes on the graphics: The Weather Channel does not show their color key, nor does IntelliCast. From experience it appears the with the exception of the IntelliWeather map, all maps have fixed color schemes. The IntelliWeather map uses a sliding scale of color based on the max and min temps presented in the data. Also, I tried to include AccuWeather, but could not locate a current national temperature map from that company. They had everything else but that.
UPDATE: I decided that even though AccuWeather did not have a CURRENT temperature map, the color and color key on their HIGH TEMPERATURE FORECAST map would suffice for this comparison, since it a similar range of temperatures presented, from (50’s to 90’s) so here it is:

Note the color scale and where the perceived “cooler” colors start on the AccuWeather map.
So what do you think?
Is it just me or does there appear to be a warm bias in the color temperature presentation of the majority of providers shown here? Just an FYI, I designed my color scheme for the IntelliWeather Map in 2001, well before I started blogging, so please no suggestions that I skewed this comparison with my own map color scheme.
Along those lines, I’ll point out that the color choices are usually done either by a meteorologist, or a graphic artist/programmer or both. Usually the color scheme is the result of the input from a couple people. In my case, myself and my graphic artist made the choice. In places like TWC or AccuWeather, the choice may be made initially by one or two then approved by a larger group.
The point I’m trying to make is that each map represents the color and temperature perception of the presenting organization, as I don’t know of any “standard” for map colors used for air temperature presentation. Having said that, somebody will probably put one in front of me that I’ve never known about. 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e8352aa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter


A new study documents the dominance of internal variability in decadal-scale global temperature changes and suggests we may experience a global cooling trend during the next 15 or even 30 years despite rising greenhouse gases.
Maher et al. (2020) acknowledge that internal variability in global surface temperature variations is “a difficult concept to communicate” because we have very few observations of its impact and so we must rely on assumptions about how the climate system might work.
Those who try to explain how internal variability affects global surface temperature often use the “Butterfly Effect” paradigm; they assume that small changes now can lead to larger changes decades from now.

Because global temperature trends are “largely determined by internal variability”, global cooling or another warming hiatus could very well be observed over the next decade. Actually, as Maher and colleagues explain, “even out to thirty years large parts of the globe (or most of the globe in MPI-GE and CMIP5) could still experience no-warming due to internal variability“.




Image Source: Maher et al., 2020


Share this...FacebookTwitter "
"
NOTE: Mike alerted me in comments about this article he wrote along the lines of my story on Color and Temperature: Perception is everything. I thought this would be good to examine again.  This article below is re-posted from John Daly’s website, and was originally published July 7th, 2002. – Anthony

By: Michael Ronayne

In a story titled “Coloring Climate Change” by Nick Schulz, Tech Central Station reported that key documents, in a US government report titled “The National Assessment of the Potential Consequences of Climate Variability and Change“, were “doctored” to distort public perceptions of climate change. The report was published by the United States Global Change Research Program. According to their own web page, the USGCRP coordinates the research of ten Federal departments and agencies with active global change programs and provides liaison with the Executive Office of the President. The budget of the USGCRP in fiscal year 2002 was approximately $1.7 billion US dollars.
The National Assessment report has served as the basis for parts of the 2001 National Academy of Sciences’ report “Climate Change Science: An Analysis of Some Key Questions” prepared for President Bush on the state of climate science and, most recently, for the highly controversial “U.S. Climate Action Report – 2002“, covertly issued by climate alarmists within the Environmental Protection Agency, with the objective of embarrassing the Bush Presidency.
The TCS story displays two graphics, shown below. The graph on the left is the one which was circulated during the public comment period after the original draft was developed. It compares the Canadian Model with the Hadley Model for the lower 48 States for the summer months of June through August, over the next 100 years. The TCS story provides additional background on the two graphs and is highly commended to your attention. Then the disparity between the two models’ future forecasts, cast doubt on the predictive capacity of the Canadian and Hadley models, the USGCRP issued the final report on the right, with the color scale altered to obscure the differences between the two models.











Unfortunately for the USGCRP, the two models show the areas of warming and cooling to be occurring in widely different sections of the United States. The USGCRP’s solution to this conundrum was to alter the temperature color scale by eliminating yellow and green, and extending the color orange into negative temperature ranges as low as -1.0°F, thereby implying warming,  when in fact the models were showing no temperature change or cooling for some localities.






 




Above: When the “Draft” and “Final” copies of the USGCRP graphs are animated, employing a technique used elsewhere on this web site, the amateurish nature of the deception becomes painfully obvious.
Not only was the distorted temperature color scale used to obscure the next 100 years of temperature models, it was also used to change the perception of the United State’s         past climatic history. The page “Overview:  Looking at America’s Climate” contains a graphic titled “Temperature Change” (shown below), which attempts to minimize the significant cooling which occurred in the Southeastern United  States during the 20th Century. This is achieved by coloring even the zero or `no change’ temperatures in light orange, and blending colors in such a way as to make it almost impossible to differentiate anything between about 0° and 5°.  Not even the IPCC has as yet stooped to this level of deception. 

On the same web page, there is another graph titled “Summer Maximum and Winter Minimum Temperature Change” (shown         below), which contains the USGCRP’s final version of the Canadian and Hadley 21st Century Summer and Winter Models, again with a choice of color scheme which blends         everything from 0° to 5° into a deceptive spread of orange.  Even         areas which these models show will not change, are colored in orange.         What other purpose can this peculiar coloring scheme serve but to suggest         future warming         in areas where none is actually predicted by the model?
 
“The National Assessment of the Potential Consequences of Climate Variability and Change” report is comprised of three separate sections which represent themselves as addressing increasing levels of detail. The descriptions are those used by the USGCRP:
1. Overview Report:  Concise, well illustrated summary.
2. Foundation Report: Volume, more detailed than the Overview Report.
3. Background Information:  Learn more about the National Assessment.
The Overview Report is published in both HTML and PDF formats and contains all of the USGCRP graphs and most of the URLs, previously referenced. This report is clearly intended for the media and the general public. Its primary message is one of impending doom, associated with anthropogenic global warming.
I am not sure why the USGCRP expended the effort to create the Foundation Report. It has so many technical flaws, in terms of electronic publishing techniques, that anyone who attempted to read it, would be quickly discouraged from delving into its contents. The report is only published in two PDF formats. Each subsection of the report is comprised of two PDF files, one which is black and white, with extremely low resolution gray scale graphics. The second PDF file contains the color figures and graphs but only the text associated with each figure. As the figures associated with the text report are all but useless, because of the poor quality, the serious reader must have  two PDF files open and switch between both files to comprehend the report. What is interesting is that the PDF file titled “Potential Consequences of Climate Variability and Change“, which contained color figures, shows in Figure 13, the US temperatures using the altered color temperature scale, but in Figure 20, the Global temperatures are displayed using the  original color temperature scale found in the draft report. The only function of the altered color temperature scale is to obscure the differences between the Canadian and Hadley models for the 21st Century United States.  By contrast, the 21st Century Global graphs were not altered in this way.
In the Background Information section, things become interesting. On a deeply buried page at “VEMAP Trend Maps” the original high resolution images, on which the draft graphics were based, can still be found. The individual graphs are: “CGCM1 Maximum Temperature Trend (JJA)” and “HadCM2 Maximum Temperature Trend (JJA)“.
One could engage in endless speculation as to why the USGCRP went to the trouble of altering the first two sections yet failing to alter the third, which contained the most incriminating information. The two most likely explanations are: (1) the Background Information section was overlooked and (2) the USGCRP did not expect anyone to find the original graphs from the Canadian and Hadley Models. Also, on the “VEMAP Trend Maps” page the Canadian and Hadley Models are not compared side-by-side, so the inconsistencies between the models are not as obvious. 
Of course, the USGCRP may not even care if the real results from the Canadian and Hadley Models are found. As long as the media continues to endlessly report only the results from the first two sections, the voices of a few skeptics can be safely ignored. 
Last year in another story, a question was asked for which no reply has been forthcoming:   If the evidence for global warming is that compelling, why is it necessary for those who believe in global warming, to misrepresent data in this manner to support their cause?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e5e5da4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterEvidence that temperature swings of ±17°C occurred during the end-Triassic mass extinction event imply that CO2 would have needed to increase 8- to 1,024-fold (3 to 10 doublings) to have induced that magnitude of temperature change. It didn’t.
Evidence from a new study (Petryshyn et al., 2020) suggests “repeated” temperature swings of 16-17°C occurred in Cotham Marble (CM, southwest United Kingdom) at the end of the Triassic epoch, when the worst extinction event of the last 500 million years occurred.
However, Petryshyn and colleagues acknowledge they “cannot resolve millennial-scale increases in temperatures in the region, implying that, at least locally, the initial extinction is not attributable to extreme warming.”
But even if the end-Triassic mass extinction (ETME) could be attributable to extreme warming, CO2 would be “implausible” as a mechanism.
According to models, CO2 increases up to 8 times the pre-industrial baseline (280 ppm) could only increase sea surface temperatures 5.4°C at most (Petryshyn et al., 2020). CO2 concentrations would need to increase up to 1,024-fold to elicit temperature changes reaching 16 or 17°C.
Therefore, “the initial onset of the biodiversity crisis may necessitate another mechanism.”

Image Source: Petryshyn et al., 2020
Share this...FacebookTwitter "
"

 **A Week at Cato University**



While many people took their summer vacations getting away from it all, more than 200 professionals, business people, college students, and retirees spent a week in early August exploring the ideas of liberty at the Cato University Summer Seminar. Held August 1–7 at Rancho Bernardo Inn, about 20 miles from San Diego, the program featured lectures and discussions on American history, law, economics, psychology, philosophy, and public policy.



In announcing the August seminar, Cato University director Tom G. Palmer said, “This program gives you the chance to recapture the intense intellectual atmosphere of your college days, in a climate where the lecturers and other participants share your fundamental ideas about freedom and justice. The schedule of lectures and discussions is designed to impart a great deal of information and analysis and encourage spirited discussion about the implications of the basic ideas.”



The faculty included some of the country’s most spirited and brightest defenders of liberty. Alan Charles Kors, professor of history at the University of Pennsylvania and coauthor of _The Shadow University_ , discussed the roots of liberty and the state of academia today. Randy Barnett, professor of law at Boston University and author of _The Structure of Liberty_ , gave a sneak preview of his forthcoming book during his talk “The Constitutional Presumption of Liberty.” (Excerpts of Barnett’s remarks are available on the September edition of CatoAudio.) Don Boudreaux, president of the Foundation for Economic Education, discussed the economics of law and the illogic of politics. Historian Paula Baker of the University of Pittsburgh discussed the growth of the American welfare state and American liberty in the 19th and 20th centuries. Guest lecturers included psychologist Nathaniel Branden, author of _Taking Responsibility_ , who examined liberty and responsibility from a psychological viewpoint, and philosopher Christina Hoff Sommers, author of _Who Stole Feminism?_ who discussed the way America has become “the republic of feelings.” Cato’s Edward H. Crane, David Boaz, Ted Galen Carpenter, Robert A. Levy, and Tom Palmer also spoke.



The attendees heaped glowing praise on the summer seminar. “The Cato University Summer Seminar was one of the most intellectually exciting times of my life,” said Kyle Larsen of Valrico, Florida. “All of the speakers were engaging and entertaining, and the friendships I have built with some of the fellow participants have far outlasted the week of the conference.”



“Excellent organization and production,” said Lyn Weingarten of Austin, Texas. “Talks were the right length with plenty of time for clarification and discussion.”



“Overall we enjoyed the week immensely, felt uplifted and educated, and are mulling over how much we can increase our annual Cato donation,” said David and Shirley Gilbreath of St. George, Utah.



Scholarships from the Opportunity Foundation allowed 40 students to participate in the program.



The Cato University program also includes a separate 12‐​month home‐​study course that uses audiotapes, books, and an integrated study guide. Another week long seminar and a weekend seminar will be held in 2000. More information about Cato University is available on the Cato University Web site.



 _This article originally appeared in the November/​December 1999 edition of_ Cato Policy Report.



 _This article originally appeared in the November/​December 1999 edition of_ Cato Policy Report.
"
"

The National Academy of Sciences (NAS) has just issued yet another report on global warming. A substantial part of it is based upon the “U.S. National Assessment” (USNA) of global warming, yet another government report that came out right before the last election. In turn, it was based, in large part, on computer models used in yet another government report on global warming, from the United Nations’ Intergovernmental Panel on Climate Change.



Together, the best I can tell, these were produced by a total of a couple thousand people. Together, they were dead wrong about the most fundamental aspect of climate change, namely how we are changing our atmosphere.



First, a little physics. It has been known since at least 1872 that carbon dioxide–a byproduct of combustion, or the meta‐​respiration of civilization, dependent upon your point of view–traps warming radiation. It has also long been known that its warming effect becomes less at increasingly high concentrations. As a result, a constant increase in atmospheric carbon dioxide results in less and less warming over time.



So, the only way to keep warming the atmosphere at a constant rate is to add carbon dioxide at an increasing, or exponential rate. This is what the U.N., the USNA, and the National Academy all assume … at least inasmuch as the Academy report states its parentage is the USNA, in its section titled “Consequences of Increased Climate Change”.



The fact is that carbon dioxide has not accumulated in the atmosphere at an exponential rate for the last quarter‐​century: This is obvious to anyone with an Internet connection (in order to download a graph of the carbon dioxide history), eyeballs and a ruler. You will see that the behavior of the last 25 years looks a lot more like a straight line than an upward‐​pointing curve. Those with statistical expertise could also enter the data into a program like Excel and see if drawing an up‐​curve through the data results in a significant improvement over a straight line. The answer, for the last 25 years, is no.



This can only mean one thing. The linear change in carbon dioxide for the last quarter‐​century will result in an inevitable and inexorable slowing of global warming in coming decades.



So why isn’t carbon dioxide increasing exponentially, even as the number of people are? Two reasons: We are becoming increasingly efficient, and the planet is getting greener.



We now produce a (deflated) dollar’s worth of stuff using about half as much energy as we used to. Neither the U.N. nor the EU, despite their blustering, forced us to do this. Instead, stockholders made it happen, demanding more output for less cost. There’s every reason to expect this behavior to continue.



The earth got greener because more carbon dioxide made the plants grow better, and a warming, primarily of the winter, lengthened the growing season. Will this greening stop, as some fear, when forests become mature and fall over? Not if they’re turned into houses, which last for hundreds of years. This is one very good argument for managed, as opposed to “natural” forestry.



How could the Academy, the National Assessment Team, and the United Nations fail to notice that they got the basic behavior of carbon dioxide (and therefore, future warming) wrong? Could thousands of scientists simply miss what anyone with a hard drive and a ruler can see? Of course not. But where would my profession be if we couldn’t scare you into funding us any more?



In a world where he who presents the scariest argument gets the most funding, everything is threatening and nothing is benign.



It’s not just in climate science, either. How about cancer? We spend just about as much money there as we do on global warming. The government regales us with impressively weak associations between diet, urban air, polar ozone depletion and death, when the lion’s share of cancer deaths would go away if people would simply choose not to smoke ciggie butts. Which causes more cancer–increasing ultraviolet radiation by 2 percent from ozone depletion (itself maybe too large an estimate) or going to the beach and taking off 98 percent of your clothes? But simple behavior changes cashier armies of regulators, who, thank you, would much rather be employed. So we tout the obscure while ignoring the obvious.



Which, sadly, is why thousands of the best minds in America aren’t eager to tell you that changes in atmospheric carbon dioxide have been so slow that global warming is likely to slow down in future decades. Exactly when, though, no one knows. Please pass the funding until I figure this out.
"
"
BUMPED for visibility. Originally published on 6/24. Bumped on 6/28 and again on 6/30
This poll will gauge reader perception to the issue that Dr. Hansen of NASA has recently raised that I cover in my post here. One vote per computer, and please spread this permalink to the poll far and wide to get a good mix of input across the blogosphere.

Click on a dot, then click the little yellow vote icon. Poll closed.
I will run this poll 1 week until next Wednesday at 9AM PST, at which time it will close. The results will be submitted to a member of the U.S. Senate for distribution, NASA’s director, and will also be mailed to Dr. Hansen at NASA GISS.
You can subscribe to the results of this poll by RSS. Simply copy the link below into your RSS reader.
http://polldaddy.com/pollRSS.aspx?id=49940E93EC30ACAF
NOTE: A couple of Pro-Hansen sites have staged a “crash party” for this poll. This has accounted for a huge increase in the votes for the first question overnight. This sometimes happens with online polls when agenda driven activists decide to skew it, which is the biggest weakness of online polls.
Addendum: Some other sites that are not Pro Hansen have also now linked to this poll, so I suppose it is becoming a battle between opposing views now. Agenda driven activists on both sides are at work now. 
Update 7/1 It appears that about 8000 votes were added for question 1 overnight. -Anthony
Update 7/2 9 AM PST Poll is closed, more here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e1960fd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When the Federal Reserve was created in 1913, its powers were limited and the United States was still on the gold standard. Today the Fed has virtually unlimited power and the dollar has no backing. Limited, constitutional government requires a rules‐​based, free‐​market monetary system with a stable‐​valued dollar. “For this reason,” F. A. Hayek wrote in 1960, “all those who wish to stop the drift toward increasing government control should concentrate their effort on monetary policy.”



To that end, the Cato Institute has launched the Center for Monetary and Financial Alternatives. By leveraging the Institute’s reputation for objective research and sound analysis, the Center will make a comprehensive economic, political, and philosophical case for reform by exploring alternative monetary arrangements. “We’ve assembled a group of scholars who will challenge the Federal Reserve in a way it hasn’t been challenged in 100 years,” Cato president John Allison says. Ultimately, the goal is to build the intellectual foundation for a freemarket banking system.



In November, following the announcement of the Center, the Institute held its 32nd Annual Monetary Conference, bringing together leading scholars and advocates for reform in order to examine the case for sound money. The event was directed, as always, by Cato vice president for monetary studies, James A. Dorn.



In his keynote address, James Grant, the founder and editor of _Grant’s Interest Rate Observer_ , declared that the need for sound money is clear and urgent. “Money is as old as the hills, and credit — the promise to pay money — is as old as trust,” he said. “Yet we still search for an answer.” Grant went on to explain that the notion of sound money is neither clear nor urgent to those who own so much of the other kind.



“I will count us victorious when the name of the chairman of the Federal Reserve Board is just as obscure as that of the chairman of the Weights and Measures Division of the Department of Commerce,” he said. “Come to think of it, the monetary millennium will arrive when the dollar reverts to a tangible weight or measure.”



Throughout the day, panelists discussed a wide range of topics — from the bitcoin revolution and the future of cryptocurrencies to the role of gold in a decentralized monetary regime — before considering the path toward fundamental reform. Judy Shelton, codirector of the Atlas Network’s Sound Money Project, explained that money is supposed to be a tool for measuring value, not a means for implementing economic and social policy. “This monetary anti‐​system we have today is anathema to free trade, to the ideals of Bretton Woods,” she said. “If America still believes in the power of free markets and the potential of free people, we need to fix what broke.”



Gerald P. O’Driscoll Jr., senior fellow at the Cato Institute, proposed the formation of a committee for monetary reform. “To get from talk to action, I propose that those committed to actual monetary reform plan to meet regularly,” he said, “not to discuss current policy but to devise a concrete plan for monetary reform.” Norbert Michel, research fellow at the Heritage Foundation, offered several near‐​term solutions — including reversing quantitative easing and removing the Fed’s regulatory role — that would complement any structural changes that came about.



In his luncheon address, Patrick Byrne, the CEO and chairman of Over​stock​.com, said that the vulnerabilities of the current system stem in part from the vulnerabilities of regulators. “They can be captured by the very same people they’re supposed to go after,” he said. Yet the intellectual climate has never been more open to a critical analysis of existing institutions, both here and abroad.
"
"
Magicians and Illusionists Penn and Teller have a popular TV show on the Showtime channel called, ahem, “Bullshit”. In homage to their debunking mentor, James Randi, they take on a number of subjects they feel could use a little “clarity”.

Click image to watch the video
They recently (last Thursday night) took on Al Gore and carbon credits. The entire 30 minute show is available via the website VREEL (update You Tube has it now, VREEL started installing  Zango a couple of days ago – a spyware) 
See YouTube Part1 Part2 Part3

Warning: more than a few obscenities are uttered in the show, but mostly for comic effect.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9de644e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On May 4, 2012, the Cato Institute honored economist Mao Yushi as the recipient of the 2012 Milton Friedman Prize for Advancing Liberty. One of the most outspoken activists for individual rights and free markets in China, Mao has emerged over the last several decades as an instrumental voice against the nation’s heavy‐​handed, authoritarian grip. Before the presentation of the award, attendees heard a keynote address by Chris Christie, the governor of New Jersey. Governor Christie stressed the significance of communicating the ideas of freedom, and how his experience in office holds lessons for the nation at large.



GOV. CHRIS CHRISTIE: It’s a pleasure to be here this evening — to leave all that’s exciting in New Jersey on a Friday night, come down here to this sleepy little hamlet, and speak before you all.



Back in 2008, I remember Barack Obama talking about the lack of hope around the country. And although he and I always defined the solutions to that problem differently, the environment in which I found myself shortly thereafter was not significantly different.



When I first took office in New Jersey in January 2010, optimism was a hard thing to find. In the eight years before I became governor, our state had raised taxes _115 times_. From 2000 to 2009, New Jersey had — literally — a zero job growth decade. In the four years before I became governor, $70 billion in wealth had left the state — not diminished wealth, _departed wealth_. Our unemployment rate was over 10 percent, with 115,000 private sector jobs lost during the four years of my predecessor.



New Jersey had the highest tax burden in the country, the worst climate for small business, and a bloated state government that contained the most public workers per square mile in the country — yeah, you can laugh unless you live there. And it only got worse. In my second week in office, my state treasurer told me that, in the subsequent five weeks, we had to find $2.2 billion in cuts from money that had already been appropriated.



We essentially had to impound the money back from certain departments just to meet payroll — all in what was the second wealthiest state per capita in America. If you need any greater example of what happens to an economy when a government overtaxes, overspends, overborrows, and overregulates, just visit New Jersey in January 2010.



So what did we do? Thanks to New Jersey’s unique constitutional structure, which allows spending to be cut by executive order, my staff and I sat in a room over the course of three weeks and went over all 2,400 line items in the state budget that I inherited. The result was finally cutting $2.2 billion. And the great thing about operating by executive order was that, at first, I didn’t have to tell anybody.



But, after delivering the news in my first speech before the joint session, you can imagine the reaction from the legislature.



Reporters descended upon the floor as the Democrats began calling me names: Julius Caesar, Napoleon Bonaparte — all of those great leaders of the past that I admire. And I realized something. The way I confronted my first substantial problem in office set the tone for my administration. I made clear from the first day that decades of fiscal irresponsibility were no longer going to be tolerated. As I said on the campaign trail, I was ready to go to Trenton and turn it upside down.



Last year we passed a $2.3 billion tax cut for businesses, with nearly 70,000 new private sector jobs created. We’ve cut spending in every department of our state government, from areas that folks told me were the third rails of politics. Given that I was still upright, I decided to go after public pensions and benefits next. And what happened? For the first time in 10 years, a majority of New Jerseyans recently polled believe the state is back on the right track. On election day in 2009, that number was 19 percent. Today, it’s 53 percent.



The American people are ready to hear the truth. They know our government is out of control. And the only thing they care more about than today is tomorrow — because tomorrow is about our children and grandchildren, and today is just about us.



The bottom line is we took action — we did it with solid principles and strong leadership — putting our state’s interests ahead of partisan ones. We turned Trenton upside down. And in the difficult times that America is in now, the only way to govern is by treating our citizens as adults — by telling them the truth about the depth of our challenges and the difficulty of the solutions.



When we fail to do this, we pay the price as a country many times over. The domestic price is obvious: growth slows, unemployment persists, and we make ourselves even more vulnerable to the unpredictable behavior of rightfully skittish markets.



But there’s also a foreign policy price to pay. To begin with, we diminish our ability to influence the thinking and ultimately the behavior of others. Democracy is the best protector of human dignity, liberty, and freedom — and history shows that mature democracies are less likely to resort to force against both their citizens and their neighbors. Yet, all across the world — in the Middle East and Asia and Africa and Latin America — people are debating their own political and economic futures. They’re looking for inspiration, and we have a stake in the outcome of those debates. There’s no better way to reinforce the likelihood that others in the world will opt for more open societies and marketbased economies than to demonstrate that our own system is working well.



At one time in our history, our greatness was a reflection of our country’s innovation, determination, ingenuity, and the strength of our democratic institutions. When there was a crisis in the world, Americans found a way to come together to help our allies and fight our enemies. When there was a crisis at home, we put aside parochialism and put the greater public interest first.



Today, our ability to effect change has been diminished because of our own inability and unwillingness to effectively deal with our problems. Now, I understand full well that succeeding at home and setting an example is not enough. But it’s a start. And I realize that what I’m calling for requires a lot of our elected officials and our people. I plead guilty to that. But I also plead guilty to being an optimist, because I believe in what this country and its citizens can accomplish _if they understand what’s being asked of them_.



We seem to have forgotten that this is a human business. Day after day, I’ve spent time sitting with colleagues on both sides of the aisle, convincing them of my intentions and letting them know that I don’t believe compromise is a dirty word. There’s always a boulevard between compromising your principles and getting everything you want. You should never compromise your principles.



But you also need to understand that you’re not always going to get everything you want. The job of a leader is to find your way onto the boulevard between the two without driving into the ditch of compromising what you believe. And trust me, if you can do this in New Jersey, you can do it anywhere. That’s where my optimism comes from. See, I’m not looking to be loved. I get plenty of love at home — and when you’re looking for love in this job, that’s when deficits get run up.



However, if you make people understand that you’re willing to say no, but you’re also always willing to listen — that you’re willing to stand hard on principles, but you’re also willing to compromise when those principles won’t be violated — then respect will come. It’s about being consistent. It’s about leading by example. It’s about standing up for the things that we believe in, instead of simply trying to figure out which way the wind’s blowing. There’s no need for varnish anymore. In fact, I don’t think we have the luxury to put it on. Liberty and freedom and the human spirit are the most powerful things in the world — and we need to say that directly to the American people. They’re ready to hear it.



I want to thank the Cato Institute for setting an example of why liberty and freedom are so important to the future greatness of America. But please never forget that it’s not going to come without a fight. We need to fight hard, even harder than we are now because the stakes are too great to do anything less. Only then can we allow the United States to, once again, export hope and liberty and freedom around the world, not just because those values are a part of our past, but because we will be acting to make them a bedrock of our future.



 **MAO YUSHI:** Ladies and gentleman, I bring you all my humble greetings from China. Tonight, we are here together in this “Shining City upon a Hill” to celebrate our common beliefs, our common hopes, and our common commitment to the values that make the Cato Institute so very special. Those Cato values bring us together today, united as common citizens of the world. I like to think of us all as “Cato citizens.” The values of which I speak, of course, are peace, free markets, limited government, and the preservation of individual liberty.



Tonight, we celebrate those timeless values — the common thread that runs through all great civilizations of the world from the beginning of humanity. Regardless of whether your traditions and cultural roots are in Africa, the early settlements along the Yellow River, the Tigris and Euphrates, the Nile, or the valleys and mountain ranges of Meso‐​America, these universal values are our common heritage. They touch each heart and resonate in the basic moral fiber of our souls.



I am personally honored and humbled to be recognized as the recipient of the Milton Friedman Prize for Advancing Liberty. I cannot express enough my grateful appreciation for this recognition and for Cato’s many decades of invaluable guidance on the long road to liberty in China.



Over the last 83 years, I’ve endured many threats and fearful nights, years of deprivation and political persecution. My family and friends, however, provided the love, the loyalty, the dignity, and the moral compass to continue that journey, regardless of the headwinds.



They helped me remember the lessons of our nation’s past heroes and heroines, as well as our moral responsibilities to the future generations. They provided the light in the storm so that we could stay the course. I could not be here today without them.



My family and I are honored tonight, but we realize that we are not here just as sons and daughters of China. We are also here on behalf of three additional constituencies who join us in _absentia_.



First, we stand in the shadows of earlier honorees from Estonia, the United Kingdom, Peru, Venezuela, and Iran. Each represents societies that have traveled on their own Homeric Odyssey, which Cato has appropriately recognized. We honor them individually and we salute their great peoples and cultures.



Secondly, we stand in the foreshadow of Cato honorees yet to emerge. To these future Cato citizens, let them know that we see their courage. We feel their heartbeats, their yearning, and their allegiance to the values of the Cato Institute. In the years to come, let everyone here be our proxy and congratulate them on their journey to a shared better tomorrow.



Thirdly, and most importantly, we receive this honor on behalf of two constituencies in China.



The first is the tens of thousands of grassroots organizations who currently work every day to serve the common citizen of China, who strive to build a better and more humane tomorrow. Countless scholars, workers, peasants, teachers, students, volunteers, and friends struggle against the common enemies of humanity: tyranny, poverty, disease, and war. They are the real honorees of tonight’s prize.



The second constituency we represent is the tens of millions of Chinese over the last century who have sacrificed their lives along the road to overthrow feudal dynasties, defeat warlordism, and defend liberty against foreign colonialism and imperialist invasion. They have proven countless times that freedom is more precious than life itself, and their struggle is also revered here tonight. This award is accepted on behalf of them, with the solemn promise that your torch will be carried by each succeeding generation with the same energy, faith, and devotion you brought to your endeavors. We do this to continue to brighten our country’s future and to deliver the inalienable and universal rights of all human beings to our descendants.



This glow — when combined with the lamps carried by fellow Cato citizens around the world — will become beacons of light from the “Shining City upon a Hill,” spreading Cato values to the dark corners of the world. In your hands, more than mine, will rest the final success and failure and hopes for liberty of our peoples.



Many have sacrificed for China’s people, for her dignity and her liberty. Despite my eight decades — my now weak hearing and failing sight — I still remember the names of those who sacrificed for our country. I see their faces. I hear their voices. I feel their souls. Tonight I speak for those who cannot be here. Their sacrifices have not been in vain, and I thank the Cato Institute for giving their lives renewed meaning.



China is a very old country. She is a noble and wise civilization, with a grand history of fine art, science, medicine, philosophy, exploration, hard work, tolerance, and openness. It is a society based on balance known as the Golden Mean, or the middle way. This theme runs through our great traditions of Daoism, Buddhism, and Confucianism. Our people have always sought balance between the needs of the collective and the individual — and against extreme government.



China understands the power of economic liberalism. Her people know that free markets and liberty nourish each other, acting together as a force for social progress. Our long history is full of examples of people rejecting the arbitrary power of the state, refusing to subordinate the rights of the individual, and recognizing that unchecked collectivism stifles human creativity and productivity.



Systemic checks and balances against unlimited government power, corruption, and improper privileges — mechanisms which include free markets, the rule of law, an independent judiciary, and the shift to a smaller, more accountable government — need to be built and put in place permanently.



These are so important because of China’s horrific past. This generation has personally experienced mass repression through the government apparatus, witnessing the more than 50 million Chinese who died directly as a result of the influence of the rule of man over the rule of law. Both our leaders and our people know what is at stake for China’s future. They are all personally invested in ensuring that Cato’s values — tailored to China’s realities — can help build a strong, prosperous, and harmonious country in the finest traditions of our ancestors.



In modern times, many countries have recalibrated their societies to fundamentally improve them with these balanced values. In the early 1970s, there were about 40 democracies in the world. As the 20th century ended, there were 120 diverse countries with some form of self‐​government crafted by the people — a largely peaceful trend that continues to this day. China needs to learn from these powerfully instructive experiences.



In order to preserve societal harmony and build China’s better tomorrow, the government should further extend liberty, freedom, and free markets — and reestablish the peaceful rise of “good neighbor” policy to preserve regional and world peace. If China’s leaders can implement the essential principles common to every successful society, she can further contribute to the world’s peace, prosperity, and harmony.



I remain optimistic that China’s government will hear her people’s desire to make this vision a reality. If one examines our country objectively, they will see that there is great reason to be hopeful given China’s tremendous progress already. Edmund Burke, the great British parliamentarian, once warned that “a state without the means of some change is without the means of its conservation.” China has been changing and progressing — a process which does not require disruptive instant transformation.



Our country has successfully raised more than 300 million people from poverty — a huge number, even in China. Knowingly or not, accomplishments like this have been achieved by the balanced implementation of Cato values in the context of the Chinese society. China’s successful evolution over her long history has been rooted in the balance between rights and responsibilities — a balance that is in focus now as we approach a transition in leadership at the end of this year.



Those of us in China know that the rights of the individual do not come from the generosity of the state. In fact, we have very limited expectations from government. We ask only that the system lives up to our constitution, abides by our laws, and complies with the international covenants of the world community. The people of China know that the fruits of society are not the sole prerogative of the powerful and privileged few. As the next generation steps forward to battle against tyranny, poverty, disease, and war itself, I believe that our journey into the future will be long, but successful. All big rivers come from small streams. Our efforts in China are but one small stream.



Tonight’s constituencies — from the people of China to the other tributaries inspired by the timeless wisdom of the Cato Institute — will join together as a mother river to nourish the human spirit and wash away the hardships of our imperfect world. Thank you once again for this great honor.
"
"

On January 10, CBS News began a series of six, count ‘em, separate global warming scare stories. On January 27, President Clinton called global warming “the greatest environmental challenge of the new century.” On February 1, Roger Ballentine, Clinton’s deputy assistant for environmental initiatives, sent a “Dear Interested Party” letter elaborating on the president’s position. On February 9, Bill Stevens, the New York Times global warming reporter (and advocate of Clinton/​Gore policies; see his new book The Change in the Weather) called, saying he’s writing a new, comprehensive feature article on the subject. On April 22, Earth Day, presidential candidate Al Gore will release a new edition of his 1992 bestseller, Earth in the Balance. 



Is there a pattern here? 



After avoiding the issue like the political plague that it is, the Gore campaign has decided to go into high dudgeon over climate change. 



The Gore team is banking on some type of national weather disaster this summer. They hope to call attention to global climate change and their belief that uncaring Republicans refuse to pass the Kyoto Protocol on global warming. This U.N. document will cost the country a fortune and has the potential to relegate an amazing percentage of our land — the United Nations calls it “Kyoto lands” — to their watchful eyes. They are about to release a report that puts just about all U.S. forested land in this category, as well as much of our farmland. That’s easily half the country. 



This makes it a good idea to examine what is coming out of the White House as it ramps up the weather horror machine. On February 1, Deputy Assistant Ballentine wrote, “You may have noticed the steady stream of new scientific studies suggesting that global warming is . . . occurring more rapidly than previously thought.” 



What “steady stream?” Fact: Of hundreds of global warming papers that appeared last year, only one, published in Geophysical Research Letters, says this, and it does so by using 16 months of data to forecast the next 100 years. The only “steady stream” it has created is a torrent of scientific criticism. 



Rather, the balance of scientific evidence, according to the U.N. Intergovernmental Panel on Climate Change is to the contrary. In its last comprehensive report, the United Nations stated, “When increases in greenhouse gases only are taken into account, most [climate prediction models] produce a greater mean warming than has been observed.” In other words, the computer models that gave rise to the initial concern predicted too much warming. 



From Mr. Ballentine: “Reports by … the National Climatic Data Center … found that since 1976 the planet has been warming at a rate of 0.35 degrees Fahrenheit per decade.” 



Fact: Integrated over the troposphere–the earth’s active weather zone — the planetary warming since 1976 has been a mere 0.07ºF/decade, or far beneath normal background fluctuations in this region. According to NASA scientist John Christy, writing in Nature magazine, the originally forecast tropospheric warming rate was around 0.70ºF/decade. This is 10 times what has been observed. Because those models, in the United Nation’s words “produce[d] a greater mean warming than has been observed,” the integrated tropospheric warming forecast was lowered, by 1997, to 0.4ºF/decade. This is still an egregious error, and a new report by the National Research Council has finally admitted that it casts serious doubt on current computer forecasts of global warming. 



More Facts: It is seriously misleading to report the temperature of “the planet” in disregard of the distribution of observed surface warming. Had Mr. Ballentine consulted the latest issue of Climate Research, he would have seen that by far the greatest warming is occurring in the coldest winter air masses of Siberia and northwestern North America. Northern Hemisphere cold‐​season warming outside those regions averages one‐​tenth of what is being observed within them, which is below normal variability. 



Still More Facts: The very air masses that are warming are those under which winter mortality is four times greater than summer mortality. Furthermore, in almost every year that surface temperatures have warmed, global food production has risen. This results from improved technology, benign weather and the same carbon dioxide that makes the coldest air of winter less deadly. Finally, as surface temperatures have warmed, we have witnessed the greatest democratization of wealth and expansion of longevity in human history. 



None of this matters when the hype is on, and Gore knows a lot about American weather. He has been told, for sure, that conditions in the industrial Midwest, Texas and Southern California are fairly dry, predisposing the region to a very mediagenic drought, just in time for the nominating conventions. 



And Gore surely has been told that the way the federal government measures moisture status — please sit down — puts an average of 20 percent of the Electoral College in drought each summer. This year, thanks to where the dry conditions are, it’s closer to one‐​third, or a mere New York+Florida from putting Gore in the White House.
"
"
Share this...FacebookTwitterHave Trump/Barr led the Biden campaign into a lethal trap? Was a legal FISA warrant obtained to surveil campaign?
If you watched the Trump Team’s press conference yesterday like I did, you probably wondered why on earth they held it where they did: Four Seasons Total Landscaping, owned by Marie Siravo, located in an industrial zone next to an adult bookstore and crematorium in the outskirts of Philadelphia!
The media was completely baffled by the location choice. Slate, for example, hints the Trump team was probably so incompetent that they mixed up the business with the Four Seasons hotel in Philadelphia.
But then earlier this morning I stumbled upon a Twitter thread that provided an interesting viewpoint. Normally I’d dismiss such things as a conspiracy theory, yet my gut feeling tells me maybe there’s something behind it. Here’s that Twitter thread. I’ve cut and pasted it fearing that it might disappear.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Could federal agents and crime fighting units really be that smart? If this were true, Barr and Trump would be absolute geniuses. We all know Giuliani is a top notch organized crime fighter with half a century of experience. He’s seen it all. Maybe he’s sending a message to the Democrats: We’re miles ahead of you.
Could this business also be the location for ballot fraud operations? Has it been bugged and surveilled by the authorities for weeks? Oh what irony this would be.
Or maybe that location was simply chosen to fool potential protesters and Antifa from showing up and disturbing the press conference.
Anyway, it’s just a really weird place for a press conference. Maybe some Democrats are panicking. One thing is clear: There’s mounting hard evidence of massive voter fraud committed and the Democrat operatives are desperate to keep a tight lid on it.
Thoughts?


		jQuery(document).ready(function(){
			jQuery('#dd_9acea29c387eba69c7b9a05eaf776dc7').on('change', function() {
			  jQuery('#amount_9acea29c387eba69c7b9a05eaf776dc7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

This spring the Pew Research Center released its eighth annual report on the state of American journalism. “In some ways, new media and old, slowly and sometimes grudgingly, are coming to resemble each other,” the study says. The traditional platforms of the Fourth Estate are changing, and last year, online news consumption outstripped print media for the first time in terms of both advertising revenue and readership. The tipping point has arrived. The trend line is clear. And the Cato Institute, it seems, has been ahead of the curve.



Since 2005 _Cato Unbound_ has given readers access to a state‐​of‐​the‐​art virtual trading floor in the intellectual marketplace. A unique online magazine, it reflects an appreciation of the way ideas are exchanged in the digital age. Every month one of the world’s leading thinkers presents an essay on a topical issue. A panel of distinguished experts responds, each offering his case before challenging and refining the arguments in an ongoing conversation. Readers are then encouraged to join the dialogue by offering their own thoughts through websites, blogs, and letters to the editor. These contributions are pulled together into an easily accessible forum, creating a media product that is virtually distinct within the digital realm.



Yet _Cato Unbound_ is also designed to avoid the pitfalls of its platform. For starters, the site revolves around the value of debate. All too often, the sheer availability of personalized news today allows readers to give in to confirmation bias — to seek out only the information that reinforces their existing beliefs. The internet, by any measure, caters to the obstinate. At _Cato Unbound_ , however, contributors are forced to confront their critics, and the tendency to selectively ignore the opposition is mitigated.



The site also hinges on the importance of perspective. The current news climate is subject to certain kinds of pressure: readers increasingly look for minute‐​by‐​minute updates. Many sites therefore suffer from a lack of depth by becoming preoccupied with instantaneous delivery. _Cato Unbound_ is different. “We try to step back, take a deep breath, and focus on the larger picture,” Jason Kuznicki, the site’s editor, explains.



In the latest issue, “Targeted Killing and the Rule of Law,” the editors ask whether the executive branch can lawfully kill. Lead essayist Ryan Alford, assistant professor at the Ave Maria School of Law, argues that it cannot. In fact, the “presidential death warrant” is so repugnant to our constitutional tradition, he says, that the Founders didn’t even think it necessary to make an explicit statement about the practice. At the time of the Revolution, British kings hadn’t enjoyed such a power for centuries, and it was thought to be the very antithesis of the rule of law. The distinguished panel of legal and historical experts responding to Alford includes John C. Dehn of the U.S. Military Academy at West Point, Gregory McNeal of Pepperdine University, and Carlton Larson of the University of California at Davis.



Other past issues have included



These monthly conversations have received attention from publications like the _New York Times_ , the _Washington Post_ , and _The Economist_. The site has featured a lineup of prominent contributors, including James M. Buchanan, the Nobel laureate and founder of the public choice school of political economy; Richard H. Thaler, professor of economics and behavioral science at the University of Chicago; James R. Flynn, a pioneer in the study of IQ; Clay Shirky, the renowned social media theorist; and Jorge Castañeda, former foreign minister of Mexico. Over the years, this forum has shown a depth of exchange and an accessible format that few other outlets offer.



An idea can be bound between covers, bound by convention, or bound for the dustbin of history. The ideas of _Cato Unbound_ , we hope, are none of the above.
"
"
I don’t have a lot of time today, but I found this interesting. Commenter “Basil” has offered this for discussion. So I’m putting this up without comment on my part. See also the decadal trends table below. Have at it folks.

Click for full sized image

1979:01-1992:12
---------------------------------------------
GISS        0.000783764**     (0.094C/decade)
HadCRUT     0.000460122**     (0.055C/decade)
RSS_MSU     0.000498964       (0.060C/decade)
UAH_MSU 	1.71035E-05       (0.002C/decade)
1993:01-2001:12
---------------------------------------------
GISS        0.00174741**      (0.210C/decade)
HadCRUT     0.00147990**      (0.178C/decade)
RSS_MSU     0.00221135**      (0.265C/decade)
UAH_MSU     0.00217023**      (0.260C/decade)
2002:01-2008:1
---------------------------------------------
GISS       -0.00091450       (-0.110C/decade)
HadCRUT    -0.00270338**     (-0.324C/decade)
RSS_MSU    -0.00208111       (-0.250C/decade)
UAH_MSU    -0.00130882       (-0.157C/decade)



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0aae336',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHere’s another excellent post by Eduardo Zorita at the Klimazwiebel.
In this BBC podcast (takes a minute or so to load), the view of green elitists is that we have casus belli. Thus democracy has to be suspended and common sense authoritarianism has to take over – just for a while, until things are put back in their proper order. The general population is just too stupid to understand it, and is only getting in the way. (Actually, and thankfully, they’re too informed and many people understand precisely what this is about).
“The situation is urgent, the world is going to hell in a handbasket – let us rescue the planet. Trust us,” we are constantly told.
I’m trying to think of a veggie or fruit that’s green outside and brown inside. The closest thing I can think of is a rotten avocado. For me it’s even disturbing that the BBC even gives equal time and weight to the green nutjobs who propose suspending democracy and taking us back to the German Democratic Republic – East Germany, behind the Berlin Wall, for those of you who may have already forgotten. “Trust us” just isn’t good enough. History shows that populations have been burned by this all too often.
The good news is that authoritarianism only works if there’s consent. But there can be no consent unless there is a genuine debate. That’s where the problem lies for the kook warmists. They’ll never win this debate, and they know it. Indeed consent has been massively eroding lately. Their science has been exposed as a hoax. They’ve lost the case and their desperation has caused them to lose any rationality they may have once had.
Update: Bishop Hill has found the perfect food staple to symbolise enviro-leftists: pistachios. Green only on the surface, brown inside, and in total, a nut throughout.
Share this...FacebookTwitter "
"

Net primary production (NPP) represents the net carbon that is fixed (sequestered) by a given plant community or ecosystem. It is the combined product of climatic, geochemical, ecological, and human effects. In recent years, many have expressed concerns that global terrestrial NPP should be falling due to the many real (and imagined) assaults on Earth's vegetation that have occurred over the past several decades—including wildfires, disease, pest outbreaks, and deforestation, as well as overly-hyped changes in temperature and precipitation.   
  
The second “National Assessment” of the effects of climate change on the United States warns that rising temperatures will necessarily result in the reduced productivity of major crops, such as corn and soybeans, and that crops and livestock will be “increasingly challenged.” Looking to the future, the National Assessment suggests that the situation will only get worse, unless drastic steps are taken to reduce the ongoing rise in the air's CO2 content (e.g., scaling back on the use of fossil fuels that, when burned, produce water and CO2).   
  
But is this really the case? If growing crops are increasingly affected, damage should also be showing up in the global ecosystem. Is the productivity of the biosphere in decline?   
  
In a word, **_no!_** Observational data indicate that just the _opposite_ is occurring (see, for example, the many studies reviewed previously on this topic here). Rather than withering away, biospheric productivity is _increasing_ , thanks in large measure to the growth-enhancing, water-saving, and stress-ameliorating benefits of atmospheric CO2 enrichment.



The latest study to confirm as much comes from the research team of Li _et al_. (2017). Working with a total of 2,196 globally-distributed databases containing observations of NPP, as well as the five environmental variables thought to most impact NPP trends (precipitation, air temperature, leaf area index, fraction of photosynthetically active radiation, and atmospheric CO2 concentration), Li _et al_. analyzed the spatiotemporal patterns of global NPP over the past half century (1961–2010).   
  
Results of their analysis are depicted in the figure below, which shows that global NPP increased significantly from 54.95 Pg C yr-1 in 1961 to 66.75 Pg C yr-1 in 2010 (Figure 1a). That represents a linear increase of 21.5 percent in the last half-century. In quantifying the relative contribution of each of the five variables impacting NPP trends (Figure 1b), Li _et al_. report that “atmospheric CO2 concentration was found to be the dominant factor that controlled the interannual variability and to be the major contribution (45.3%) of global NPP.” Leaf area index, which is also enhanced by increasing atmospheric carbon dioxide, was the second most important factor, contributing an additional 21.8 percent, followed by climate change (precipitation and air temperature together) and the fraction of photosynthetically active radiation, which accounted for the remaining 18.3 and 14.6 percent increase in NPP, respectively. Li _et al_. also report that the vast majority of the observed rise in NPP occurred in the middle and high latitude regions, with 61.1 percent of the increase occurring between 30 and 60 degrees of latitude and 26.4 percent between 60 and 90 degrees of latitude of both hemispheres (see Figure 1c).   






**_Figure 1_** _. (A) Annual variations in global NPP between 1961 and 2010. (B) Changes in NPP in recent decades that resulted from multiple environmental factors including climate, leaf area index (LAI), fraction of photosynthetically active radiation (fPAR), and CO 2, and the relative contribution rate (%) of each factor during the study period. (C) Spatial distribution of the trend in NPP during the period 1961–2010. Source: Li _et al. _(2017)._



The observed increase in global NPP over the past five decades is quite an accomplishment for the terrestrial biosphere, especially when one considers all the negative stories—nary a day goes by without notice of some environmental disaster (human- or naturally-caused) occurring somewhere in the world and wreaking havoc on nature. Since 1980, the Earth has experienced three of the warmest decades in the modern instrumental temperature record, has weathered a handful of intense and persistent El Niño events, and suffered large-scale deforestation, ""unprecedented"" forest fires, disease and pest outbreaks, and episodes of persistent, widespread, and severe droughts and floods. Yet, despite each of these factors, and every other possible negative influence that has occurred over the past half century, terrestrial net primary productivity has increased by 21.5 percent! And it has done so largely because of the ongoing rise in atmospheric CO2. How ironic it is, therefore, that the supposed chief _culprit_ behind the many real (and imagined) assaults on Earth’s vegetation—rising atmospheric CO2—has been found to be the primary _cause_ of an ever-greener planet.   
  
**Reference**   
  
Li, P., Peng, C., Wang, M., Li, W., Zhao, P., Wang, K., Yang, Y. and Zhu, Q. (2017) ""Quantification of the response of global terrestrial net primary production to multifactor global change."" _Ecological Indicators_ **76** : 245–255.


"
"
Share this...FacebookTwitterIn search of particles
Envirozealots are now moving against street sweepers, burning firewood and wood floors, claiming they emit dangerous aerosols. Expect microscopic aerosols to become the next environmental catastrophe.  
The Swiss online news magazine Die Weltwoche has a report by journalist Alex Reichmuth called Environmental Protection Ad Absurdum (in German). 
Environmental protection in Switzerland, like much of Europe,  has fallen into the hands of envirozealots. European ministries of environment are increasingly becoming armies of white-gloved snoopers in search of single molecules of contaminants. And the envirowacko journalists are chiming in, of course.  
In Switzerland the latest environmental catastrophe are airborne microscopic aerosols ( now joining biodiversity, ocean acidification, water consumption and climate change). It’s gotten so bad that now even environmental groups are now getting annoyed.
For example since 1988  it has been a tradition for environmental awareness group Alps Initiative to light a bonfire every August to remind people to protect the Alps from air pollution. This year, however, the event has been banned by the local environmental authorities. The reason, reports Reichmuth:  
The bonfire would harm the climate and pollute the air with microscopic aerosols.  
The Alps Initiative reacted:  
This is making a mountain out of a molehill.  


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed it is. But modern environmentalism has always been about making molehills into mountains, hasn’t it? Just look how life-giving CO2 has been made the culprit for the coming global Armageddon.  
Reichmuth serves up more ad absurdum cases.  
Another example: residents in small villages in Graubünden and in Tessin have been discovered to be suffering from microscopic aerosols emitted by homes burning wood in fireplaces in the wintertime. Yes, it’s about time to close up them romantic fireplaces in Swiss chalets.  
Even street sweepers are now deemed a microscopic-aerosol producing problem. A local newspaper wrote:  
When sucking up dirt, dangerous fine particles are emitted into the air by the sweeper’s air exhaust. And depending on the manufacturer, at alarming rates!  
Wait, it gets worse! That beautiful flooring you have in the rooms in your home? It may be emitting fine aerosols that are dangerous to your health too. Reichmuth writes:  
Anyone with wood or wood laminate floors is living dangerously. According to a German study, rooms with smooth floors produce concentrations of microscopic aerosols that are considerably higher than rooms with carpeting. Concentrations on average were even higher than Swiss daily limits. Taking into account all the victims who have died as a result of the aerosols, then we have to call manufacturers of natural wood floors and wood laminate mass murderers.  
Sounds loony, but let it be a warning of what can happen if you don’t stand up and push this movement back. Although Cap & Trade is in a coma in the US, waiting to wake up after the November elections, the EPA is waiting to swoop down and run every aspect of your lives.
Share this...FacebookTwitter "
"

Representatives of some 160 nations are gathered in Kyoto, Japan, this week to negotiate an international treaty to control emissions of greenhouse gases. While the summit has all the trappings of a substantive event, the gathering in Kyoto is more an exercise in public relations than in serious statecraft. Hot air, rather than cold reason, will dominate the conference and political sleight of hand will be its product. 



That’s because, according to the International Panel on Climate Change (the United Nation’s body of experts devoted to the study of global warming and known as the IPCC), even the most aggressive and costly proposals on the table this week would shave only a fraction of a degree off temperature increases projected by computer models for the year 2100. To achieve those emissions reductions, each American would have to pay an additional $1,000-$3,000 annually in higher energy prices. Such proposals would, according to Yale economist and sometime Clinton adviser William Nordhaus, take us back to the days of semipermanent energy crises like those of the 1970s. 



Both greenhouse alarmists and skeptics agree that actually preventing the global warming projected by computer models would require the world to reduce carbon dioxide emissions by 60–80 percent. Only by virtually abandoning the use of oil, gas and coal could we achieve such reductions. President Clinton’s assurances about “free lunch” climate change policies notwithstanding, nobody is proposing any real policy to prevent climate change because no one wants to usher in a permanent global depression. The idea, then, is to get the world to commit to a slow‐​motion control policy, one that would ease us into higher energy costs, a reordered industrial world and, as National Public Radio reporter Richard Harris puts it, “a whole new society” structured around less energy use.



But if the computer models are correct about global climate change (the data thus far are inconclusive), what choice do we have? Isn’t it prudent to hedge our bets with a control strategy now in order to avoid far more costly economic crash planning later? Well, no. All indications are that the “cure” for global warming is far worse than the “disease” of rising temperatures.



First, only about 2 percent of America’s economy is sensitive to weather conditions. No matter how ruinous climate change might be, it couldn’t possibly have a serious long‐​term impact on the United States. Even the most alarmist projections of ocean rise (about 3 feet or so) are trivial. If Amsterdam could figure out a way to hold back an even larger sea rise hundreds of years ago, it’s clear that a wealthier and more technologically advanced United States could counter a 3‐​foot rise. Foreign aid to help poorer countries adopt would be far less expensive than control policies.



Second, it’s not altogether clear that a warmer world would be a less habitable world. A temperature rise of 4.5 degrees Fahrenheit (the median computer‐​predicted result of a doubling of atmospheric carbon dioxide in the next 100 years) was exactly what occurred about a thousand years ago (A.D. 850 — 1300) in a period climatologists refer to as “the little climate optimum” (note that they don’t refer to it as “the little climate hell”). The result? A longer growing season, rapid economic development, a minor cultural renaissance, an expansion of fertile crop and forestland and a decrease in mortality rates. Since the data indicate that the small amount of warming we have detected over the last 100 years has largely been confined to winter evenings in the far northern latitudes, we have every reason — both empirical and theoretical — to believe that warming would be a benign, not a deleterious, event.



There are still open questions about how much if anything man has had to do with the slight amount of warming detected over the past 100 years and how much warming might eventually occur (the IPCC estimates range from insignificant to moderately significant). The IPCC report itself states that it will be another decade or so before scientists will know for certain. So why not wait? Nature magazine reported last year that waiting 20 years for better scientific information before acting will only cost us .36 degree Fahrenheit, at worst, over the next 100 years. 



In the face of this kind of uncertainty, the best “insurance policy” we could buy is one that increases the amount of wealth at society’s disposal to handle whatever problems might occur in the decades to come. Impoverishing society today to avoid a very uncertain problem tomorrow would harm, not help, future generations.
"
"
Share this...FacebookTwitterMelting sea ice is causing sea levels to rise 49 micorons per year (3/16 of an inch over 100 years), according to research published in Geophysical Research Letters. Read more here:  http://www.sciencedaily.com/releases/2010/04/100428142258.htm
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMany of us have made complaints about how the MSM is biased and reluctant to cover topics like Climategate and other scandals. Well take heart! People are waking up to the fact that they are being denied information and facts, and as a result they’re turning their backs on the main stream print media. Sure a part of it has to do with bad economic times (can’t blame that on Bush anymore), but another factor is that the internet offers an alternative. Here are the gory numbers for newspaper circulation in the US. Only the conservative Wall Street Journal has made gains. The rest are bleeding massively.
The Wall Street Journal 2,092,523 +0.5%
USA Today 1,826,622 -13.58%
New York Times 951,063 -8.47%
Los Angeles Times 616,606 -14.74%
Washington Post 578,482 -13.06%
Dallas Morning News 260,659 -21.47%
San Francisco Chronicle 241,330 -22.68%
The Star-Ledger, Newark, N.J. 236,017 -17.79%
Read the complete story here: http://www.editorandpublisher.com/eandp/news/article_display.jsp?vnu_content_id=1004086334
I’ve always encouraged tree-hugging subscribers to heed the advice from newspapers to cut CO2 emissions. And what better way to begin than to scale back the energy-guzzling industry of newspaper production, distribution and newspaper disposal? This can be done by cancelling subscriptions. This also rescues a lot of trees. So hats off to the tree-huggers out there who choose to save a tree by ending a subscription.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEchoing the determination of NASA scientists, a new study suggests the natural variability in cloud cover allowing more solar radiation to be absorbed by the Earth’s oceans drove the 2014-2020 global warming.
NASA scientists (Loeb et al., 2018) used satellite data to assess the 2014-2017 warming was driven by a +0.83 W/m² shortwave forcing due to the downward trend in cloud cover.

Image Source: Loeb et al., 2018
A new study (Ollila, 2020) affirms this analysis and suggests the 2018-2020 temperature changes can also be explained by shortwave cloud forcing.
“…the pause was over at the end of 2014, and the major cause was not the anthropogenic forcing, but it was the SW [shortwave] radiation forcing”
“Decreases in low cloud cover were the primary driver of the decrease in reflected SW…”

Image Source: Ollila, 2020
Share this...FacebookTwitter "
"
Its all quiet on the solar front. Too quiet. It has now been almost 2 and a half months since the last counted cycle 24 sunspot has been seen on April 13th, 2008. There was a tiny cycle 24 “sunspeck” that appeared briefly on May 13th, but according to solar physicist Leif Svalgaard, that one never was assigned a number and did not “count”. It is just barely discernable on this large image from that day.

The sun today: spotless
NASA’s David Hathaway updated his solar cycle prediction page on June 4th. The start of cycle 24 keeps getting pushed forward while the ramp up line starts to look steeper into 2009.

Click for full sized image
The most recent forecast ( June 27th, 2008 ) from the Space Weather Prediction Center says little that would suggest our spotless streak would end any time soon:
Solar Activity Forecast: Solar activity is expected to be very
low. 
Analysis of Solar Active Regions and Activity from 26/2100Z
to 27/2100Z: Solar activity was very low. No flares occurred during
the past 24 hours and the solar disk remains spotless.

 So when will solar cycle 24 really get going? It seems even the best minds of science don’t know for certain. A NOAA press release issued last year in April 2007 calls for Cycle 24 to be up to a year late, but they can’t decide on the intensity of SC24. That argument is ongoing.
Meanwhile the NOAA SEC Solar Cycle Progression Page looks pretty flat in all metrics charted. 
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e4ce774',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Matt Ridley’s new book, _The Rational Optimist: How Prosperity Evolves_ , is garnering rave reviews. Ridley, science writer and popularizer of evolutionary psychology, shows how it was trade and specialization of labor–and the resulting massive growth in technological sophistication–that hauled humanity from its impoverished past to its comparatively rich present. These trends will continue, he argues, and will solve many of today’s most pressing problems, from the spread of disease to the threat of climate change.   
  
  
The Cato Institute has now presented three different looks at the book, with a review in the _Cato Journal_ , another in _Regulation_ , and an event at Cato with Matt Ridley himself.   
  
  
**Powell on Ridley**   
  
  
My colleague Aaron Powell published the first Cato review of _The Rational Optimist_ with a piece in the Fall 2010 edition of the _Cato Journal_ (pdf). 



What _The Rational Optimist_ makes clear, in perspicuous prose and enchanting storytelling, is that, just as biological evolution populated the world with the wondrous variety of life, exchange allowed one of those species to achieve a wondrous standard of living that will only improve and become more uniform as we trade and invent.



Powell doesn’t find the book flawless, however. He identifies two problems that weaken Ridley’s argument, the first dealing with the “circular and unconvincing” nature of his claim that trade caused our human ancestors to achieve humanity. The second concern is broader. Powell writes, 



It would be easy to get the impression Ridley is Pollyannaish. If nuclear annihilation, super flus, and starvation are nothing to be worried about, what possibly could be? Unfortunately, Ridley’s response to this critique is less convincing than it could be, for he fails to adequately draw a line between when an anticipated disaster is real and when it’s just pessimism writ large.



 **Henderson on Ridley**   
  
  
David R. Henderson reviews the book in the latest issue of _Regulation_ (pdf). Like Powell, Henderson enthusiastically endorses the style and substance of Ridley’s book, though without identifying the weaknesses highlighted in the former review. His only point of contention with _The Rational Optimist_ is a “jarring misstatement” regarding trade and value. Henderson writes, 



Given the important role of trade in Ridley’s theory, and given his obvious understanding of trade, it is surprising that he makes a jarring misstatement: “For barter to work,” he writes, “two individuals do not need to offer things of equal value. Trade is often unequal, but still benefits both sides.” The correct statement is: “For barter or trade to work, individuals _must_ offer things of _unequal_ value.” If I valued what I give up the same as what I get in return, there would be no point in trading. Trading is _always_ an exchange of unequal values.



Henderson goes on to defend Ridley against the negative appraisal his book received in the _New York Times_. That review, written by famous foreign‐​aid critic William Easterly, attacked _The Rational Optimist_ for its take on Africa and for failing to “confront[] honestly all the doubts about the ‘free market.’ ” “Really?” Henderson responds. “All the doubts? I do not know if such a book could be written with the requisite amount of evidence and have under 3,000 pages.”   
  
  
**Ridley on Ridley**   
  
  
And then, of course, there’s the source himself. In May, Ridley spoke at a Cato Institute book forum about _The Rational Optimist_. He discussed the core arguments of his book and concluded (optimistically) that technology and trade have now made it possible to stop trying to keep the world from getting worse, and instead focus on making it better.   
  
  
As with all Cato events, full video and audio are available for download on www​.cato​.org. Or watch it right here:   

"
"
Share this...FacebookTwitterReconstructions of past temperatures show much colder periods with higher CO2 levels or as-warm or warmer periods with much lower CO2 levels.
A new study (Paus, 2020) indicates modern July temperatures center around 7.5 to 8°C in the Scandes Mountains (Norway). Today’s CO2 atmospheric concentration has reached 410 ppm. 
During the latter stages of the last ice age (19,000 to 17,000 years ago), Late Glacial (LG) CO2 fluttered near 200 ppm. But with the discovery of temperature-sensitive tree species in the area it can be affirmed that July temperatures were also “at least 7-8°C” in the Scandes at this time. Despite more than a doubling of CO2, there has been no consequent summer temperature increase in this remote location.

Image Source: Paus, 2020
In another new study from Eastern Europe, Blagoveshchenskaya (2020) has determined January temperatures were almost 11°C warmer than the Little Ice Age (700 to 300 yrs ago) and 4°C warmer than today from about 6,000 to 4,500 years ago.
CO2 levels were 270 ppm when this region was 11°C warmer and 275 ppm when 11°C colder. It is 4°C colder today, at 410 ppm, than it was when CO2 was 270 ppm.
So, once again, reconstructions of Holocene temperatures do not support the narrative that CO2 and temperature changes are correlated.

Image Source: Blagoveshchenskaya, 2020


		jQuery(document).ready(function(){
			jQuery('#dd_674ed8c89aead8cfe23d2bf9d8a55622').on('change', function() {
			  jQuery('#amount_674ed8c89aead8cfe23d2bf9d8a55622').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterJohn Kerry is a co-sponsor of the latest Cap & Trade bill, which thank God has been put off indefinitely. The bill, should it become law, God forbid, would force all working Americans to pay more for energy and to live more humbly.
But living humbly and paying more taxes to the government applies only to the little guys. For the rich, elite and privileged, like Kerry, they’d continue to fly around in private jets, be chauffeured in limousines and frolic the seas in big yachts. Watch Kerry’s reaction when confronted about skirting Massachusetts taxes by birthing his new 76 foot $7 million yacht in Rhode Island:
[SEE VIDEO]
By berthing the yacht in Rhode Island, Kerry skirts paying nearly $450,000 in sales tax and a yearly $70,000 excise tax bill to his own home state. Look at how pissy he gets when confronted by the media.
Worse, Kerry, who claims to be fighting for American jobs, had the yacht built in New Zealand. The same yacht could have been built at a yard in his own state of Massachusetts. So much for American jobs. And what about environmental friendliness?
What does a 76-foot yacht include?
According to the Boston Herald, Kerry’s yacht has two cabins, a pilot house fitted with a wet bar and cold wine storage.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I wasn’t able to get further specific information on Kerry’s yacht, but looking at other typical 76-foot yachts on the market, we find that they are far from spartan. Take this 76-foot Monte Fino for only $2,450,000 – a real bargain when compared to Kerry’s $7 million cruiser. The Monte Fino includes a 10,000 liter diesel fuel tank, 1500 hp twin engines and is filled with high-tech electronic doo-dads.
God knows what Kerry got for his $7 million.
Kerry spokesman David Wade said Friday the boat is being kept at Newport Shipyard not to evade taxes, but “for long-term maintenance, upkeep and charter purposes.”
John Kerry is married to Theresa Heinz, who is millionaire heiress to the Heinz ketchup fortune, and is a philanthropist and environmentalist.
When you’re rich, you can do things like this. It’s okay. But these rich people should not be making it much harder for the rest of us to make ends meet.
When confronted by the media in the above clip, Kerry defends berthing the floating palace in tax-haven Rhode island, claiming he is paying his taxes. Then he scuttles away – not on a bicycle or in a hybrid car – but in a chauffeured SUV.
“Can I get outa here please!”, he orders his chauffeur.
And let’s not forget Sen. Jeff Greene and 145 ft yacht dragging anchor through coral reef: http://www.miamiherald.com/2010/07/23/1743175/greene-denies-his-anchor-damaged.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSnowFan here reports on the latest winter forecasts for the 2020/21 Europe winter. History and statistics show Europe could be in for a frosty winter. 
Currently a significant La Nina is shaping up, and history shows that these events in the Pacific have an impact on Europe’s winters:

The NOAA reanalysis above shows the temperature deviations (left) and for precipitation (right) from the WMO average 1981-2010 during the six La Niña years of winter in Europe. Large parts of Europe have average temperatures and precipitation is distributed differently, with Germany being slightly drier overall than the WMO average. Is a 2020/21 winter in Germany under La Niña conditions shaping up to have average temperatures and slightly less humidity?
Strong winter-solar correlation
A more important factor determining winter in Europe may be solar activity. Data from the German DWD national weather service since 1954 show a remarkable higher frequency of cold winters in times of low solar activity, such as we are now in the midst of.
The following chart shows the December-January-February cold temperature anomalies occurring in the times of low solar activity (circled):

After the current minimum of solar activity in December 2019, statistically it leads us to expect a crisp winter 2020/21 – not only in Germany. Source: DWD time series with supplements.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It could look like the chart below because the NOAA reanalysis shows the high statistical probability of cold winters in Europe during the weakest solar cycles after 1948. What follows is a chart showing the temperature anomalies for the winters occurring at times of low solar activity:

The winters in Europe since 1948 with the weakest solar activity so far have all been significantly colder than the 1981-2010 WMO climate average. The solar activity cycle ending in December 2019 was one of the weakest cycles ever since observations began. Source: NOAA reanalysis and long-term weather. 
But with the different statistical approaches, one thing already seems to be clear: The winter 2020/21 will probably not be particularly mild in Europe…
Also IRI expects cool 2020/21 winter
Last message: IRI continues to expect a rather supercooled winter 2020/21 in Central Europe with slightly more precipitation than average.

Like in September 2020, the IRI October forecast of Columbia University in New York also predicts a slightly cooler winter 2020/21 with slightly increased precipitation over Central Europe. Source: IRI Seasonal Forecast

Thanks to SnowFan for this report.


		jQuery(document).ready(function(){
			jQuery('#dd_61345aa3ea6c80009ddf1466cb136d50').on('change', function() {
			  jQuery('#amount_61345aa3ea6c80009ddf1466cb136d50').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterToday we present two papers on climate reconstruction using proxy data. One about East Antarctica and the other about belize. Hat-tip reader Mary Brown.
AMO behind sea surface temperatures
First we look at a paper authored by a team of German scientists: “Great Blue Hole (Lighthouse Reef, Belize): A continuous, annually-resolved record of Common Era sea surface temperature, Atlantic Multidecadal Oscillation and cyclone-controlled run-off“.
The team looked at 2000 years of proxy data from Belize and found interesting natural cycles at play. According to the authors, the Atlantic Multidecadal Oscillation (AMO) occurred 1885 years back in time and that it controls the SW Caribbean sea surface temperature patterns on multi-decadal time scales.
The authors note that the Holocene (<11.7 kyr BP) has been characterized by several periods of distinct climate changes and that the climate remains difficult to predict “due to the lack of comprehensive, annually-resolved and continuous sea-surface temperature (SST) data”.
So what about them models?
Examining an 8.55 m long sediment core from the bottom of the Great Blue Hole (Lighthouse Reef, Belize), the scientists were able to extract “an annually-resolved, continuous and unique south-western Caribbean climate record for the last 1885 years”.
The result? The data imply a general SST rise within the south-western Caribbean and that the modulation of SST within the time series likely operated on two different time levels: (1) Solar (e.g., “Gleissberg Cycles”) and volcanic activity triggered climate changes, which in turn induced responses of the Atlantic Multidecadal Oscillation (AMO), the North Atlantic Oscillation (NAO) and the El-Niño-Southern Oscillation (ENSO).
The authors conclude further in the abstract:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




We suspect long-term positive AMO and NAO modes as the primary key control mechanisms of the Dark Ages Cold and Medieval Warm Period SST patterns. ENSO mode modulation likely exerted primary control on regional SST variability during the Little Ice Age and the Modern Global Warming. (2) Our δ18O data further indicate a striking secondary control on multi-decadal time scales: δ18O variations occur with 32–64 years periodicity. This signal is clearly evidence of SST modulation controlled by AMO phase changes (50–70 years) over almost the entire Common Era. Our carbon isotope record (δ13C) exhibits two remarkable negative anomalies and a long-term up-core decreasing trend. The first excursion (drop of 0.5‰) occurred with the onset of the Medieval Warm Period, which is reconstructed to be a peak time in south-western Caribbean tropical cyclone (TC) activity. This overlap is stressing a potential context between TC activity, enhanced coastal run-off and increased soil-erosion reflected by 13C-depleted carbon isotopes. A second anomaly (>1900 CE) is more likely the result of the “Suess Effect” (anthropogenic impact of the Industrial Revolution on carbon isotopes composition) than another reflection of a TC peak activity interval.”
But since 1900, man has taken over control of the earth’s climate, the authors seem to be suggesting. That was probably written in witha wink to the funders.
Antarctica suddenly lost 400 meters of ice
In another new paper: Abrupt Holocene ice-sheet thinning along the southern Soya Coast, Lützow-Holm Bay, East Antarctica, revealed by glacial geomorphology and surface exposure dating, a team of Japanese scientists led by Moto Kawamata examined the deglacial history of the East Antarctic Ice Sheet (EAIS).

Image: Figure 1 here.
The authors found that it had thinned from at least 400 m a.s.l. during the Early to Mid-Holocene (9–5 ka) and say the abrupt thinning was likely caused by the natural inflow of modified Circumpolar Deep Water via submarine valleys in Lützow-Holm Bay.
Abstract:
Geological reconstruction of the retreat history of the East Antarctic Ice Sheet (EAIS) since the Last Glacial Maximum (LGM) is essential for understanding the response of the ice sheet to global climatic change and the mechanisms of retreat, including a possible abrupt melting event. Such information is key for constraining climatic and ice-sheet models that are used to predict future Antarctic Ice Sheet AIS melting. However, data required to make a detailed reconstruction of the history of the EAIS involving changes in its thickness and lateral extent since the LGM remain sparse. Here, we present a new detailed ice-sheet history for the southern Soya Coast, Lützow-Holm Bay, East Antarctica, based on geomorphological observations and surface exposure ages. Our results demonstrate that the ice sheet completely covered the highest peak of Skarvsnes (400 m a.s.l.) prior to ∼9 ka and retreated eastward by at least 10 km during the Early to Mid-Holocene (ca. 9 to 5 ka). The timing of the abrupt ice-sheet thinning and retreat is consistent with the intrusion of modified Circumpolar Deep Water (mCDW) into deep submarine valleys in Lützow-Holm Bay, as inferred from fossil foraminifera records of marine sediment cores. Thus, we propose that the mechanism of the abrupt thinning and retreat of the EAIS along the southern Soya Coast was marine ice-sheet instability caused by mCDW intrusion into deep submarine valleys. Such abrupt ice-sheet thinning and retreat with similar magnitude and timing have also been reported from Enderby Land, East Antarctica. Our findings suggest that abrupt thinning and retreat as a consequence of marine ice-sheet instability and intrusion of mCDW during the Early to Mid-Holocene may have led to rapid ice-surface lowering of hundreds of meters in East Antarctica.”
Today, if an ice sheet loses 60 cm, it’s deemed a crisis by climate bedwetters. Just imagine if an ice sheet in Antarctica were to lose 400 meters thickness.


		jQuery(document).ready(function(){
			jQuery('#dd_671ec5cfb78244fc3fa18cd6e7e3d7df').on('change', function() {
			  jQuery('#amount_671ec5cfb78244fc3fa18cd6e7e3d7df').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

From this page (h/t Dave Hagen)
The U.S. Environmental Protection Agency (EPA) is inviting comment from all interested parties on options and questions to be considered for possible greenhouse gas regulations under the Clean Air Act. EPA is issuing an advance notice of proposed rulemaking (ANPR) to gather information and determine how to proceed.
The Advance Notice
The ANPR is one of the steps EPA has taken in response to the U.S. Supreme Court’s decision in Massachusetts v. EPA. The Court found that the Clean Air Act authorizes EPA to regulate tailpipe greenhouse gas emissions if EPA determines they cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare. The ANPR reflects the complexity and magnitude of the question of whether and how greenhouse gases could be effectively controlled under the Clean Air Act.
The document summarizes much of EPA’s work and lays out concerns raised by other federal agencies during their review of this work. EPA is publishing this notice at this time because it is impossible to simultaneously address all the agencies’ issues and respond to the agency’s legal obligations in a timely manner.
Key Issues for Discussion and Comment in the ANPR:

Descriptions of key provisions and programs in the CAA, and advantages and disadvantages of regulating GHGs under those provisions;
How a decision to regulate GHG emissions under one section of the CAA could or would lead to regulation of GHG emissions under other sections of the Act, including sections establishing permitting requirements for major stationary sources of air pollutants;
Issues relevant for Congress to consider for possible future climate legislation and the potential for overlap between future legislation and regulation under the existing CAA; and,
Scientific information relevant to, and the issues raised by, an endangerment analysis.

EPA will accept public comment on the ANPR for 120 days following its publication in the Federal Register.
Background
In April 2007, the Supreme Court concluded that GHGs meet the CAA definition of an air pollutant.  Therefore, EPA has authority under the CAA to regulate GHGs subject to the endangerment test for new motor vehicles – an Agency determination that GHG emissions from new motor vehicles cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare.
A decision to regulate GHG emissions for motor vehicles impacts whether other sources of GHG emissions would need to be regulated as well, including establishing permitting requirements for stationary sources of air pollutants.
How to Comment

Comments should be identified by the following Docket ID Number: EPA-HQ-OAR-2008-0318
Comments should be submitted by one of the following method

www.regulations.gov: Follow the on-line instructions for submitting comments.
Email: a-and-r-Docket@epa.gov
Fax: 202-566-9744
Mail: Air and Radiation Docket and Information Center, Environmental Protection Agency, Mailcode: 2822T, 1200 Pennsylvania Ave., NW., Washington, DC 20460. In addition, please mail a copy of your comments on the information collection provisions to the Office of Information and Regulatory Affairs, Office of Management and Budget (OMB), Attn: Desk Officer for EPA, 725 17th St. NW., Washington, DC 20503.
Hand Delivery: EPA Docket Center, EPA West Building, Room 3334, 1301 Constitution Ave., NW, Washington DC, 20004. Such deliveries are only accepted during the Docket’s normal hours of operation, and special arrangements should be made for deliveries of boxed information.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dc4ba27',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



 _The Current Wisdom_ only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper.



The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.



Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year‐​by‐​year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas” — there really is not much of it around.



The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.



Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).



So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now — assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.



The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed



At any rate, improvements to the Japanese‐​developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.



The high resolution version (MIROC3.2(hires)) was quite a doozy — responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise.



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse‐​gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.



The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high‐​resolution at 4.3°C (7.7°F) and the medium‐​resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2).



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).



Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.



The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) — more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.



And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme — which produces clouds which compare more favorably with satellite observations — projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2‐​induced warming, low clouds are now projected to retard it.



Here is how Watanabe _et al_. describe their results:



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing… .



MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).… This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes — still among the most uncertain of climate processes — is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.



Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!



 **References:**



Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report [4], available at http://​www​.ipcc​.ch.  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312–6335.  
[1] Watanabe et al. report that the sensitivity of MIROC3.2 (medres) is 3.6°C (6.5°), which is less that what was reported in the 2007 IPCC report. So 25% is likely a conservative estimate of the reduction in warming.  
[2] Whether enhanced cloudiness enhances or cancels carbon‐​dioxide warming is one of the core issues in the climate debate, and is clearly not “settled” science.  
[3] Degrees Kelvin (K) are the same as degrees Celsius (C) when looking at relative, rather than absolute temperatures.
"
"

The recession of 2007–2009 knocked the wind out of state government budgets. Yet, as revenues have risen steadily in recent years, some governors have pursued reforms to reduce tax burdens on families and make their states more competitive. Other governors have used rising revenues to expand programs. In their biennial survey, **“Fiscal Policy Report Card on America’s Governors 2014” (White Paper)** , Nicole Kaeding, a Cato budget analyst, and Chris Edwards, director of tax policy studies at the Institute, use statistical data to grade the governors on their taxing and spending records. “Reading the report card and other works by the institute may change some minds,” according to Forbes​.com. “But more importantly, it broadens the debate over the role of fiscal policy in particular and government more generally.” Four governors were awarded an “A” on this report card: Pat McCrory of North Carolina, Sam Brownback of Kansas, Paul Le‐ Page of Maine, and Mike Pence of Indiana. Eight governors were awarded an “F”: Mark Dayton of Minnesota, John Kitzhaber of Oregon, Jack Markell of Delaware, Jay Inslee of Washington, Pat Quinn of Illinois, Deval Patrick of Massachusetts, John Hickenlooper of Colorado, and Jerry Brown of California. “With the economy currently growing, governors and legislatures are having few problems balancing their budgets in the short run, but the states face major budget challenges down the road,” the authors write. At the same time, global economic competition is making it imperative that states improve their investment climates.



 **IS PRESCHOOL EFFECTIVE?**  
Demands for universal preschool programs have now become commonplace, reinforced by President Obama’s call for “highquality preschool for all” in 2013. Yet as David J. Armor, professor emeritus at George Mason University, points out in **“The Evidence on Universal Preschool” (Policy Analysis no. 760)** , any program that could cost state and federal taxpayers $50 billion per year warrants a closer look at the evidence on its effectiveness. This paper reviews the major evaluations of preschool programs, including both traditional programs such as Head Start and those considered high quality. As it turns out, these evaluations do not paint a generally positive picture. “The most methodologically rigorous evaluations find that the academic benefits of preschool programs are quite modest, and these gains fade after children enter elementary school,” Armor writes. This is the case for Head Start, Early Head Start, and also for the “high‐​quality” Tennessee preschool program. Two other high‐​quality programs have been evaluated using a rigorous experimental design, and have been shown to have significant academic and social benefits, including long‐​term benefits. These are the Abecedarian and Perry Preschool programs. However, the groups studied were very small, they came from single communities several decades ago, and both programs were far more intensive than the programs being contemplated today. Armor concludes, “Before policymakers consider huge expenditures to expand preschool, especially by making it universal, much more research is needed to demonstrate true effectiveness.”



 **GOOD INTENTIONS, IMPOVERISHED RESULTS**  
Over the last half century, federal and state governments have spent more than $19 trillion fighting poverty. But what have we really accomplished? In **“War on Poverty Turns 50: Are We Winning Yet?” (Policy Analysis no. 761)** , Michael Tanner, a Cato senior fellow, and Charles Hughes, a research associate at the Institute, argue that, although far from conclusive, the evidence suggests that we have successfully reduced many of the deprivations of material poverty. However, these efforts were more successful among socioeconomically stable groups such as the elderly than low‐​income groups facing other social problems. “Moreover, other factors like the passage of the Civil Rights Act, the expansion of economic opportunities to African Americans and women, increased private charity, and general economic growth may all have played a role in whatever poverty reduction occurred,” the authors write. Nevertheless, even if the War on Poverty achieved some initial success, the programs it spawned have long since reached a point of diminishing returns. In recent years we have spent more and more money on more and more programs, while realizing few, if any, additional gains. We may have made the lives of the poor less uncomfortable, but we have failed to truly lift people out of poverty. This should serve as an object lesson for policymakers today. “Good intentions are not enough,” Tanner and Hughes conclude.



 **WORK DISINCENTIVES**  
The Social Security Disability Insurance (SSDI) program faces imminent insolvency. Annual expenditures totaled $143 billion in 2013, but program receipts amounted to $111 billion—a shortfall that is projected to continue indefinitely. In **“SSDI Reform: Promoting Gainful Employment while Preserving Economic Security”(Policy Analysis no. 762)** , Jagadeesh Gokhale, senior fellow at the Cato Institute, points out that, according to the Social Security Trustees, the program’s trust fund will be fully depleted in 2016, compelling either a large benefit cut or a large tax hike. Neither option will be politically popular. Regardless of the program’s insolvency, SSDI creates substantial work disincentives, causing many with medical impairments who could work to withdraw from the labor force and apply for SSDI. Gokhale advocates a change in the structure of SSDI’s benefit payments to those admitted to the program. Shifting benefits at the margin toward paying beneficiaries to work rather than to remain out of the work force would encourage beneficiaries with residual capacities to return to work. “That shift would serve as a backstop to reduce the economic loss from wrongful allowances of applicants into SSDI,” Gokhale writes. “Such a switch in benefit design can be accomplished without compromising benefit eligibility for those who cannot work.” In this analysis, he explains how to implement such a change to SSDI’s benefit structure and the advantages that would accrue from it.



 **DISTORTING TRADE**  
The use of antidumping measures to protect certain domestic industries may be the most widely abused trade policy instrument worldwide,” writes K. William Watson, trade policy analyst at the Cato Institute. In **“Will Nonmarket Economy Methodology Go Quietly into the Night? U.S. Antidumping Policy toward China after 2016” (Policy Analysis no. 763)** , Watson argues that U.S. authorities reserve their most punitive and abusive practices for goods from China. In those cases, the United States sets antidumping duties using what is called nonmarket economy (NME) methodology. The practice gives license to the U.S. Department of Commerce to ignore Chinese producers’ cost and price data and to turn, instead, to estimates for those data that are punitive and unrealistic. Current WTO rules permit the United States to maintain this discriminatory approach, but that condition will expire in December 2016. Absent a major change in the mindset of U.S. trade officials with respect to Chinese treatment in antidumping proceedings, it is unlikely that the United States will bring its policy into compliance. Watson presents some of the alternative scenarios that might unfold as the expiration date approaches. “The policy that would best serve a strong U.S. trade agenda and the American public is to end NME treatment of China by no later than December 2016,” he concludes. Nondiscriminatory treatment of Chinese imports would bring U.S. trade policy into compliance with WTO rules while reducing the distorting effect of antidumping measures on the U.S. economy.
"
"
Share this...FacebookTwitterData from NASA point to a powerful Pacific La Nina event in the works, and so with it could bring a considerable drop in the mean global surface temperature in 2021. 
According to the latest report issued by the Australian Bureau of Meteorology (BOM), the La Niña conditions continue in the tropical Pacific: “International climate models suggest it is likely to continue at least through February 2021.”

Peak La Niña conditions expected in January, 2021. Chart source: BOM. 
Central and eastern tropical Pacific Ocean sea surface temperatures (SSTs) are at La Niña levels, and remain similar compared to two weeks ago, reports the BOM. “Models continue to suggest some possibility that central and eastern tropical Pacific SSTs could briefly reach levels similar to 2010–12, with the peak most likely in December 2020 or January 2021.”
The BOM uses the ACCESS–S model for generating its forecasts.
NASA: temperature deviation to -3°C


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Mean while Snowfan here reports that NASA prognoses are in fact expecting an “unusually powerful La Niña development, with cooler than normal surface anomalies extending into the summer of 2021 and which could become an extended year-long event, which also occurred in 2010 – 2012.

Source: BOM NASA/GMAO-ENSO-Prognosen
“November 2020 sees an unusually strong La Niña in the equatorial Pacific with temperature deviations down to below -3°C and with unusually long duration until NH summer 2021,” Snowfan writes. “If the current NASA forecast is correct, a multi-year La Niña could develop into 2022, just like in 2010-2012.”
Strong and long La Niña events cool the Earth by several tenths of a degree Celsius with a time lag. The overall global cooling occurring since 2016 will therefore continue at least until 2021 and could even last until 2022.
Fuel for future bush fires
The BOM notes that La Niña events “typically enhance spring rainfall in northern, central and eastern Australia” and that during a La Niña summer, “above average rainfall is also typical for much of eastern Australia, but particularly eastern Queensland”. But such good news comes with a price: Rainfall means more vegetation growth, which in turn will lead to much more fuel ffor uture bush fires when drought conditions return, as they always inevitably do.


		jQuery(document).ready(function(){
			jQuery('#dd_30ad4b406255c528e276dfb072615ea2').on('change', function() {
			  jQuery('#amount_30ad4b406255c528e276dfb072615ea2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Just two days after sunspot 983 was reported, it has now disappeared. They just aren’t sticking around like they used to. This is yet another indication of the bottomed out solar minima we are in.
It will be very interesting to see if the cycle 24 predictions by Hathway at NASA for an even stronger cycle will materialize.

Though there does seem to be more discussion of a weak cycle 24 than a strong one as of late. Personally, I think this graph of Average Planetary magnetic index (Ap) is quite telling in the step that occurred in 2005. From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below:

click for a larger image
What is most interesting about the Geomagnetic Average Planetary Index graph above is what happened around October 2005. Notice the sharp drop in the magnetic index and the continuance at low levels.
From this story on space.com where they talk about the opposing views solar scientists have for cycle 24 they offer some opinions. NOAA Space Environment Center scientist Douglas Biesecker, who chaired the panel, said in a statement:
 […] despite the panel’s division on the Sun cycle’s intensity, all members have a high confidence that the season will begin in March 2008.
We shall soon see if they are correct, March starts this Saturday.
Nature will truly be the final arbiter of this argument.
UPDATE: Jeff C writes
I thought you might find this chart interesting.  Since sunspot cycles overlap and there is no clear start/stop, the “start” of the new solar cycle is usually defined as the smoothed sunspot minimum between cycles (as opposed to the appearance of the first reversed-polarity spot). Although different definitions are sometimes used, this seems to be the most common and accepted variation.
The enclosed chart shows the transition from cycle 22 to cycle 23 back in 1996.  It is interesting how the first new cycle sunspots appeared over a year before the commonly accepted May 1996 start date of the new
cycle.
I’m unsure of the cycle start date definition used by Douglas Biesecker, but if it is the commonly accepted definition, he will be way off.  It will be interesting to see if they claim the appearance of a few reversed cycle sunspots count as a “start”.  If so, then cycle 23 actually started back in March 1995 and is 13 years old.

Click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0b64fad',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Surfacestations.org volunteer surveyor Russ Steele brings us this gem of a climate monitoring station from Panguitch, UT. I’ve seen stations over asphalt, such as the University of Arizona station in Tucson, but this one has a special feature; they made a concrete traffic island especially for the station so that it wouldn’t get collided with by nearby parked vehicles. How’s that for diligence? The station mount was set right into the concrete. So much for the 100 foot rule away from asphalt, concrete and buildings issued by NOAA

Click image for larger version
The station was recently closed, and the instruments and wooden portion of the shelter put into storage, which is why you don’t see any Stevenson Screen shelter in the picture above, only the mount. Since it’s permanently set into the concrete, they couldn’t easily remove it. Not well thought out, I’d say.
The GISS temperature plot has an offset just before the year 2000, care to bet when the concrete for the traffic island was poured?



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2c58e84',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAccording to MSNBC, the Katla volcano in Iceland is about to blow her top – hat tip Joe Bastardi Joe Bastardi blog. Katla is the big sis of Eyjafjallajökull;I mean the really big sis. And according to an initial research paper by the University College of London Institute for Risk and Disaster Reduction:
Analysis of the seismic energy released around Katla over the last decade or so is interpreted as providing evidence of a rising … intrusive magma body on the western flank of the volcano.
and
 We conclude that given the high frequency of Katla activity, an eruption in the short term is a strong possibility.
A Katla eruption could be an order of magnitude greater than Eyjafjallajökull and possibly emit significant quantities of ash and sulphur particles into the stratosphere. Read the part about volcanoes at FOCUS warming will end. If that happens, then it’s a game-changer.
And look for a lot of uneasy alarmists to use it as a back door out of an increasingly embarrassing situation.
Share this...FacebookTwitter "
"
As I mentioned in my post here about one of the satellite data sets (RSS) that showed a marked cooling globally in 2008, La Niña and PDO seem to be drivers of this change. Here is Joe D’Aleo’s take on it below. – Anthony
By Joseph D’Aleo, CCM ICECAP
Evidence is growing this La Niña will be a longer term event. Most similar important La Niñas are often multi year events (1949-1951,1954-1956, 1961-63, 1970-1972, 1973-1976, 1998-2001). Though the easternmost Pacific near South America has warmed at the surface as the seasonal weakening of the tropical easterlies led to weakened upwelling, it is still cold beneath. Below you can see the latest depth-section of ocean temperatures (top) and anomalies (bottom). Temperature are in degree Celsius. Note the large reservoir of subsurface anomalously cold water (up to 4 degrees C) in the eastern tropical Pacific at 50 to 100 meters.

 Also see the latest CPC depicted ocean heat content in the tropical Pacific. This shows the heat content remains at near maximum deficit levels.
 
These suggest as the easterlies increase again, cooling will return to the east Pacific and La Niña will persist at least well into 2008. The Pacific Decadal Oscillation (PDO) has dropped strongly negative (latest value from NCEP is -1.54 STD). This decline may represent another Great Pacific Climate Shift as the PDO warm and cold phases tend last 25 to 30 years and the last change , to a warm Pacific, occurred in 1976. See more in this pdf here.  If indeed the PDO shift is the real deal, we might expect more La Niñas and fewer weaker El Niños over the next few decades with a net tendency for cooling. Add to that a quieter sun and eventually a cooling Atlantic, and you have a recipe for global cooling.
However, this has its own drawbacks, La Ninas bring more drought and summer heat waves, landfalling hurricanes, large tornado outbreaks, spring floods, winter snows and cold outbreaks than their more famous counterpart, El Niño, which has dominated during the warm PDO era. A while back, Stan Changnon did an interesting analysis which I reported on recently here that suggests the era we have gone through since the late 1970s with dominant El Niños was unusually benign with more benefits than damages and will be looked on as the golden era, a modern climate optimum. Even if all this is correct, you might expect the media and enviro-alarmists ‘evidence’ we are affecting our climate to morph from warming and ice melt to the climate extremes characteristic of La Niñas.
See full pdf here. 
<!– –>


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0c25407',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It was a term more notable for the remarkable amount of unanimity than its blockbuster decisions. Nevertheless, the 2010–2011 term of the United States Supreme Court provided plenty of judicial fodder to provoke a day’s worth of discussion at the Cato Institute’s Annual Constitution Day Conference. This year marked the 10th, and as always, it coincided with the release of the new _Cato Supreme Court Review_.



The conference, “The Supreme Court: Past and Prologue: A Look at the October 2010 and October 2011 Terms,” featured leading legal experts discussing the most pertinent cases of the last term and what we can expect in the near future from the Supreme Court.



David Post, law professor at Temple University and a member of the _Review_ ’s editorial board, examined _Brown v. Entertainment Merchants Association_ — teasing out the First Amendment’s “doctrinal oddities” in a provocative essay on the so‐​called violent video games dispute. “The case presents a fascinating snapshot of the state of [free speech] in the early years of the 21st century,” Post said, “and contains enough peculiarities … to keep law professors and their students busy for years to come.”



The _Review_ includes additional articles on the First Amendment — by far the highest‐​profile issue of the term — with case analyses on funeral protests, campaign financing, and much more. John Eastman, law professor and former dean at Chapman University, addressed _Bond v. United States_ — “your typical sordid tale of adultery, toxic chemicals, and federalism,” as _Review_ editor‐​in‐​chief Ilya Shapiro described it. The case involved a defendant being brought up, not on charges of assault, but on violating an international chemical‐​weapons treaty. The judicial lesson? “Don’t mess with the husband of someone who works in a chemical lab!” Eastman quipped.



The day closed with the annual B. Kenneth Simon Lecture, during which a distinguished legal scholar presents a paper that will be included in the following year’s _Cato Supreme Court Review_. This September, the Hon. Alex Kozinski — chief judge on the U.S. Court of Appeals for the Ninth Circuit — discussed the influence of new technologies on emerging expectations of privacy (See page 9). While he acknowledged the public’s willingness to trade certain boundaries for convenience, the story is much more complex than that. “I think it’s fair to say that privacy is not dead as an ideal,” he argued.



The last decade has seen a reanimation of many such ideals. In the foreword to the inaugural volume, Roger Pilon, Cato’s vice president for legal affairs, expanded on the purpose of the _Cato Supreme Court Review_. “We will examine the Court’s decisions and upcoming cases in the light cast by the nation’s first principles — liberty and limited government — as articulated in the Declaration of Independence and secured by the Constitution,” he wrote. The mission, modestly laid out, was to play a part in changing the climate of ideas to one more conducive to a constitutional government of delegated, enumerated, and thus limited powers. The Center for Constitutional Studies, established in 1989, has been a critical institution in making that vision a reality.



“In short, the Court cannot roll back Leviathan on its own,” Pilon says, “but it can put a brake on it and chip away at its substance” — or, perhaps, lack thereof.
"
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"
Share this...FacebookTwitterFormer Tropical Storm Edouard brings bitter temperature drop to Germany
By Kalte Sonne
(Translated/edited and image added  by P. Gosselin)
Stupidity clicks well in Germany. Alarmist messages about tropical storm Edouard now running through the Internet on numerous websites have been no better in quality than the earlier reports of an impending summer of heat shocks (The opposite has been true so far this summer).
On the other hand, reports like those by Fabian Ruhnau of Kachelmannwetter are beneficial, which assess former tropical strom Edouard somewhat differently, without neglecting the powerful thunderstorms:
“On Friday, a small, but rather weather-intensive low-pressure system will sweep over Northern Germany. It is ‘formerly EDOUARD’, the low was once a tropical storm, but is now just a normal low. In the run-up to the cold front, hot air reaches the south and southeast, where powerful thunderstorms can form. During the weekend the weather will calm down and get colder again.”

Image cropped from kachelmannwetter.com 
The kneejeck digital excitement and constant alarms are clearly a sign of our times. But a very dangerous one, because permanent alarm dulls the senses.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHat-tip: Die kalte Sonne
According to a new study, the expansion of offshore wind energy planned to date could lead to less electricity actually being produced at higher costs because, according to current planning, wind farms are taking the wind away from each other.
The researchers from the Technical University of Denmark in Roskilde and the Max Planck Institute for Biogeochemistry in Jena, Germany have investigated the topic. The study entitled “Making the Most of Offshore Wind” was commissioned by the Agora Energiewende and Agora Verkehrswende think tanks.
The report looks at that the question whether energy models used today by wind farm planners and investors can adequately capture the interaction effects between turbines stemming from very large areas covered with offshore wind farms at high installed capacity density.
Among the study’s key findings:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Offshore wind power needs sufficient space, as the full load operating time may otherwise shrink
from currently around 4,000 hours per year to between 3,000 and 3,300 hours. The more turbines
are installed in a region, the less efficient offshore wind production becomes due to a lack of wind
recovery. If Germany were to install 50 to 70 GW solely in the German Bight, the number of full-load
hours achieved by offshore wind farms would decrease considerably.”
Countries on the North and Baltic Seas should cooperate with a view to maximizing the wind yield
and full-load hours of their offshore wind farms. In order to maximize the efficiency and potential of
offshore wind, the planning and development of wind farms – as well as broader maritime spatial
planning – should be intelligently coordinated across national borders. This finding is relevant to
both the North and Baltic Seas. In addition, floating offshore wind farms could enable the creative
integration of deep waters into wind farm planning.”

Chart source: Study: “Making the Most of Offshore Wind“, Agora Energiewende and Agora Verkehrswende.
More unexpected costs, inefficiency
In a nutshell: a central pillar of the German and European transition to green energies threatens to become even more inefficient and more expensive than planned.


		jQuery(document).ready(function(){
			jQuery('#dd_75682c91c950f13c05f6eed99c42cb66').on('change', function() {
			  jQuery('#amount_75682c91c950f13c05f6eed99c42cb66').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

**Presidential Power**   
  
  
Op‐​Ed: “New President Won’t Tame Presidential Power,” by Gene Healy in the _Orange County Register._



After seven years of an administration that has recognized few, if any, limits on executive power, it’s only natural that many people look to the Obama‐​Biden ticket to put the presidency back in its proper constitutional place.



Article: “Obama’s the Candid Candidate on Energy,” by Jerry Taylor and Peter Van Doren in _Forbes._



Sen. Obama’s frank confession about what his climate change policies will mean to electricity consumers is one very good reason why so many conservative and libertarian intellectuals are gravitating toward his candidacy. It’s not that right‐​wing thinkers necessarily endorse his climate change policies. It’s that right‐​wing thinkers are increasingly tired of Republican hypocrisy and make‐​believe policy fights.



Article: “Obama’s Tax Deceptions,” by Alan Reynolds in _National Review Online_. 



Barack Obama famously claims, “I’ll give a tax break to 95% of workers and their families.” The Obama team never explained that figure, because they made it up.…Obama said, “If you work, pay taxes, and make less than $200,000, you’ll get a tax cut.” That too is flatly false. Single workers who make more than $80,000 (or joint returns above $155,000) would not get a tax cut under Obama’s plan.



 __  
  
  
Podcast: “The Obama Agenda: Free Political Speech,” featuring John Samples 
"
"
After a near brush with death, GOES 12 is back up and running, our full disk image  is now 100% The loop may take some time to get synced but images are being produced from GOES 12 correctly now.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea231e8fe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Just released: new DOE figures showing that the US has reduced CO2 production 1.5% last year in 2006, even without the US signing on to Kyoto.  You can read the full report here (Adobe PDF file).
 Here are some of the numbers for 2006:

• Total U.S. greenhouse gas emissions in 2006 were 1.5 percent below the 2005 total—the first annual drop since 2001 and only the third since 1990.
• The total emissions reduction, from 7,181.4 million metric tons carbon dioxide equivalent (MMTCO2e) in 2005 to 7,075.6 MMTCO2e in 2006, was largely a result of reductions in carbon dioxide (CO2) emissions. There were smaller reductions in emissions of methane (CH4) and man-made gases with high global warming potentials (high-GWP gases)
• U.S. carbon dioxide emissions in 2006 were 110.6 million metric tons (MMT) below their 2005 level of 6,045.0 MMT, due to favorable weather conditions; higher energy prices; a decline in the carbon intensity of electric power generation that resulted from increased use of natural gas, the least carbon intensive fossil fuel; and greater reliance on non-fossil energy sources.
Despite my stance on the measurement and interpretation errors associated with the surface temperature record, I’ve always felt that reducing pollution is a good thing. At the same time I’ve always felt that our environmental movement is too often focused on panic driven ideas.
Coupled with the news about the 2007 hurricane season being very low in my post below, I believe we’ve seen evidence that things aren’t all they are claimed to be, particularly by Gore. I think the best approach overall is to not panic, and to work on alternate energy solutions and better efficiency as a way to wean ourselves from foreign oil. The key here is slow change. It took us 100 years to get to this point, it will probably take us at least half that to reverse the trends in a sensible way with new technology.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2506bff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the new Cato study “Long Hot Year: Latest Science Debunks Global Warming Hysteria” (Policy Analysis no. 329), climatologist Patrick J. Michaels reports that Vice President Al Gore’s latest alarmist claim—that 1998’s warmer than normal temperatures resulted from global warming—isn’t supported by the scientific evidence. Michaels, professor of environmental science at the University of Virginia and senior fellow in environmental studies at the Cato Institute, writes that “the record temperatures were largely the result of a strong El Niño superimposed on a decade in which temperatures continue to reflect a warming that largely took place in the first half of this century.” Satellite data show clearly that “the warmth of 1998 is an anomalous spike rather than a continuation of a warming trend.” Michaels notes that “imposing an El Niño upon an already warm decade creates the illusion of rapid global warming,” as he predicted it would in his 1992 Cato Institute book Sound and Fury. The fact is that “observed global warming remains far below the amount predicted by computer models that served as the basis for the United Nations Framework Convention on Climate Change.”



 **Perils of Government Investing**



The questions at the center of the upcoming debate on Social Security’s future will be, What kind of private investment, and who should do the investing? Michael Tanner, director of Cato’s Project on Social Security Privatization, warns in “The Perils of Government Investing” (Briefing Paper no. 43) that those are critical questions because government investment of Social Security funds could make the federal government the largest shareholder in American corporations. Tanner points out that Federal Reserve chairman Alan Greenspan says that it is impossible to “insulate” government investment “from the political process.” Government investment of Social Security payroll taxes would result in a dangerous mix of government involvement in corporate governance and “social investing.” 



**Failed Intervention in Bosnia**



The three‐​year‐​old Dayton Agreement has failed to accomplish its main objective and should be abandoned, writes Cato foreign policy analyst Gary Dempsey in a new study, “Rethinking the Dayton Agreement: Bosnia Three Years Later” (Policy Analysis no. 327). “The Clinton administration’s continued and uncritical devotion to the agreement is compromising U.S. national security and saddling the United States with an expensive yet futile nation‐​building operation of unknown duration.” The study finds that the “goal of creating a unitary, multiethnic Bosnian state is not realistic.” The Clinton administration has refused to consider changing course, however. “The administration needs to jettison its presumption that there are only two options for U.S. policy on Bosnia: adhere to the Dayton Agreement or cut and run. There is another option: a negotiated three‐​way partition of Bosnia overseen by a European‐​led transition force. That is the most politically feasible way to create the conditions necessary to allow the departure of U.S. troops at the earliest possible date.” 



Throw the Other Guy’s Bums Out, Too In the new Cato study “What Term Limits Do That Ordinary Voting Cannot” (Policy Analysis no. 328), Harvard Law School professor Einer Elhauge addresses the questions: Why do the same voters who vote for term limits also routinely vote to return senior incumbents to office? Why don’t they vote the bums out? The answer is straightforward: “Voting your bum out is not a solution when what you want to do is oust the other districts’ bums. For that you need term limits.” The fact that incumbents tend to get reelected at very high rates even though large majorities of voters favor term limits is perfectly logical, he notes. “A district that ousts its senior incumbent suffers a loss of relative clout in the legislature. To avoid that loss of power, it behooves individual districts to vote to retain their incumbents.” The solution is also straightforward: “If all the districts collectively could agree to oust their senior incumbents simultaneously, no district would suffer a loss of relative power, and each district would gain more accurate representation. Term limits are effectively just such an agreement.”



 **U.S. Foreign Policy Spawning Terrorism**



One‐​third of all terrorist attacks worldwide in 1997 were perpetrated against U.S. targets. That is a very high percentage “considering that the United States—unlike nations such as Algeria, Turkey, and the United Kingdom—has no internal civil war or quarrels with its neighbors that spawn terrorism,” writes Ivan Eland, Cato’s director of defense policy studies. “The major difference between the United States and other wealthy democratic nations is that it is an interventionist superpower.” In “Does U.S. Intervention Overseas Breed Terrorism? The Historical Record” (Foreign Policy Briefing no. 50), Eland points out that the Pentagon’s own Defense Science Board finds that “a strong correlation exists between U.S. involvement in international situations and an increase in terrorist attacks against the United States.” Eland recommends that the United States adopt a policy of military restraint: “The United States could reduce the chances of devastating—and potentially catastrophic—terrorist attacks by adopting a policy of military restraint overseas.”



 **Nuke the Test Ban Treaty**



The U.S. Senate should reject the proposed Comprehensive Test Ban Treaty and fund the resumption of limited testing, writes defense analyst Kathleen C. Bailey in a new Cato paper. In “The Comprehensive Test Ban Treaty: The Costs Outweigh the Benefits” (Policy Analysis no. 330), Bailey argues that the treaty is unenforceable, unverifiable, and unwise policy. Signed by President Clinton in September 1996 and to be considered by the Senate this year, the CTBT has limited political benefits and is “not worth the high cost to U.S. national security.” Weapons testing is essential to U.S. national security, according to Bailey, because “evolution in technologies for safety, nuclear delivery systems, and enemy defenses may render the now‐​modern U.S. nuclear arsenal technologically obsolete or less safe.” She notes that “at present, the United States is two years or more away from being able to conduct a nuclear test. This lack of readiness will inevitably worsen as skilled experts retire and die, equipment ages or becomes obsolete, and financial support erodes.” Bailey believes that, “from a purely technical standpoint, it would be most prudent for the U.S. Senate to reject the CTBT and to allocate funds for resumption of U.S. testing and for reconstruction of the U.S. nuclear weapons production infrastructure.” But she notes as well that “it may be politically desirable to undertake some limitations on testing.” 



**Trashing Government Intervention in Refuse**



One of the biggest environmental issues at the state and local levels is garbage—how to collect it, dispose of it, recycle it, and pay for doing so. In a new Cato study, “Time to Trash Government Intervention in Garbage Service” (Policy Analysis no. 331), Peter VanDoren, assistant director of environmental studies at Cato, challenges the reigning orthodoxy that the government must decide those questions for citizens. That belief, VanDoren points out, is grounded in the assumption that economies of scale and collection route density mean the government must have a monopoly on trash collection. VanDoren’s research on the economics of refuse markets reveals that government management of garbage service is unnecessary and counterproductive. He argues that homeowners should be allowed to choose among competing collection firms and that homeowners, not bureaucrats, should have the final say about what kind of service they want.



 _This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)



<em><a href=”/people/patrick-michaels”>Patrick J. Michaels</a> is Director of the Center for the Study of Science at the Cato Institute.</em>
"
"
Share this...FacebookTwitterFrom about 80,000 to 20,000 years ago, Greenland temperatures abruptly warmed by about 10°C in just a few decades on at least 20 occasions. And then, about 870 to 1,500 years later, CO2 rose. 
Li and Born (2019) document 8-16°C climate warmings (Dansgaard-Oeschger events) in Greenland that extended to both hemispheres between about 80 and 20 thousand years ago. (Though global in scope, temperature changes were less pronounced outside Greenland.)
These abrupt warmings occurred within decades (or less). It has been suggested the warm-ups may have required no external forcing, as they’re considered an “unforced oscillation”.

Image Source: Li and Born (2019)
A new study (Shin et al., 2020) suggests the about 1,000 years after these warming events occurred, CO2 concentrations rose.
Despite the millennial-scale duration of this lag relative to the decadal-scale temperature changes, there are many who believe CO2 changes are a driver of warming.
“However, the CO2 decrease did not always start at exactly the same time as the onset of the DO warming, and the lag itself varied. For example, during Marine Isotope Stage (MIS) 3, atmospheric CO2 maxima lagged behind abrupt temperature change in Greenland by 870±90 yrs. During MIS 5, the lag of atmospheric CO2 maxima with respect to abrupt temperature warming in the NH was only about 250±190 yrs (Bereiter et al., 2012). … During MIS 6d which corresponds to CDM 6d.1 and 6d.2, CO2 concentrations show a much slower increase over a duration of ~3.3 kyr. Here, CO2 lags behind the onset of the NH abrupt warming by 1,500±280 yrs and 1,300±450 yrs, respectively (1,400± 375 yrs on average).”

Image Source: Shin et al., 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Jouwatch
(Translated/edited by P. Gosselin)
Since everyone is preoccupied with Corona, hardly anyone notices what is being decided to continue destroying Germany:
The German government now wants to make the use of renewable energies a question of national security. “The use of renewable energies for electricity generation is in the public interest and serves public security,” says the draft of the new German Renewable Energy Sources Act, on which the newspaper “Welt am Sonntag” reported.
From the point of view of experts, the decision is of enormous significance.
It concerns a energy-political turning point, say legal experts of energy law at the law firm of Luther, Gernot, Engel, reports Die Welt am Sonntag.
In the controversy over the building  of wind parks, for example, the reference to “public security” may fundamentally impact court rulings. In court proceedings in connection with the expansion of bioenergy, wind and solar power, the reference to “public safety” could restrict the impact rulings by judges, business representatives fear, according to the “Welt am Sonntag”.
The new norm threatens to become a basis for far-reaching state intervention.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The federal government confirmed to Die Welt am Sonntag that the new state consecrations for eco-energy should make it easier to enforce building applications. “The regulation stipulates an overriding public interest in electricity generation from renewable energies as well as a public security interest,” the Federal Ministry of Economics announced in response to an inquiry by the newspaper.
The specification is important for discretionary and public interest rulings by authorities and institutions.
Latest government power-grab
If this law passes, and it will pass because there is no real opposition apart from the AfD party, the path is cleared for Germany. Then wind turbines will be forced to be built directly next to residential areas, and ownership rights will be undermined.
That is the revolution from above. That is energy fascism. Resistance must be stirred up here – and it fatally reminds us of the power grabbing in these times of Corona!
The massively green electricity damaged Wattenrat East Friesian comments on this new underhanded approach as follows:
The renewable energy industry is insatiable, ideologically consolidated and closely linked to politics – and above all very inventive,
if it concerns the preservation of its ecclesiastical  income, which is paid by all current customers through the EEG green energy feed in act to the tune of double-digit billions annually.
Now the use of renewable energies is even supposed to “serve public safety”, says the draft of the new Renewable Energy Sources Act. This would make wind farm sites easier to implement. This is incredibly brazen and wrong because the renewable energies (wind and sun) only work depending on the weather. Especially wind power plants endanger the security of supply due to the erratic feed-in through grid instability unstable power grids, are therefore a public safety risk.”


		jQuery(document).ready(function(){
			jQuery('#dd_cd206d699e3eb9d49f563e7d100375a7').on('change', function() {
			  jQuery('#amount_cd206d699e3eb9d49f563e7d100375a7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Last summer, progressive legal scholar Simon Lazarus offered a commentary on the shifting landscape of the U.S. legal establishment.



“For decades, and as recently as Barack Obama’s first year in the White House, libertarians were marginalized within the conservative pantheon,” he wrote in the _New Republic_. “Now they rival, and in important areas threaten to displace social conservatives and big‐​government conservatives.”



This “upheaval” — which Lazarus concluded was “potentially seismic” — has had a discernible impact on more than just the conservative movement. Since taking office in 2009, President Obama has suffered a string of losses before the Supreme Court, racking up 20 unanimous high court defeats over the last five‐​and‐​a‐​half years. “The fact that his track record is as bad as it is in the Supreme Court,” Sen. Mike Lee (R-UT) recently said, “is yet another indication of the fact that we’ve got a president who is playing fast and loose with the Constitution.”



The Cato Institute has been at the center of this reversal. In another successful term at the marble palace at One First Street NE, the Center for Constitutional Studies went 10–1 in cases where it filed amicus briefs. This is on the heels of the Institute’s 15–3 record last term. Notably, the solicitor general’s office most recently went 11–9 on the year. “Perhaps the government would be better served following our lead on constitutional interpretation, advocating positions that reinforce our founding document’s role in securing and protecting individual liberty,” Ilya Shapiro, senior fellow at the Institute, wrote in response. Cato was the only organization in the country to file on the winning side of this term’s three highest‐​profile 5–4 cases.



The year’s most highly anticipated case was _Burwell v. Hobby Lobby_ , in which a familyowned business filed suit challenging the Affordable Care Act’s contraceptive mandate, citing religious objections. The Court ultimately sided with Hobby Lobby. After the ruling, Cato’s vice president for legal affairs Roger Pilon identified the core issue in the case. “Religious liberty is treated today as an ‘exception’ to the general power of government to rule — captured, indeed, in the very title of the statute on which the Hobby Lobby decision rests: the Religious Freedom Restoration Act,” he wrote on Cato’s blog. “That Congress had to act to try to restore religious freedom — to carve out a space for it in a world of ubiquitous, omnipresent government — speaks volumes.”



In _Harris v. Quinn_ , the Court ruled that government does not have the power to force public employees to associate with a labor union. At issue was an Illinois law claiming that home‐​care workers were public employees, ostensibly for one purpose: collective bargaining. The forced unionization of homebased workers has spread to nearly a dozen states, providing a substantial number of new workers — and dues — to the labor movement. “[This] decision will slow, and perhaps eventually end, that flow of funds, as workers decide they can represent their own interests and would prefer to keep their earnings for themselves and their families,” wrote Cato adjunct scholar Andrew M. Grossman.



Finally, the Court issued its latest blockbuster ruling on campaign finance with _McCutcheon v. FEC_ , striking down the “aggregate” contribution limits on how much money any one person can contribute to election campaigns. As Chief Justice John Roberts wrote for the majority, “If the First Amendment protects flag burning, funeral protests, and Nazi parades — despite the profound offense such spectacles cause — it surely protects political campaign speech despite popular opposition.” Shapiro, for his part, put a finer point on the decision. “In a truly free society, people should be able to give whatever they want to whomever they choose, including candidates for public office,” he wrote.



In one sense, these developments indicate the weight of Cato’s work today. But that impact is the cumulative effect of more than 30 years of intellectual debate. Over that period, the Institute’s mission has been to change the climate of ideas to one more conducive to a government of delegated, enumerated, and limited powers. Since 1989 the Center for Constitutional Studies has been a critical institution in that pursuit. As such, we may find that we are now approaching the Court’s libertarian moment.
"
"
Share this...FacebookTwitterUlli Kulke of the German online Die Welt national newspaper has written a piece: How Sceptics Are To Be Converted. He reports on the recent Global Media Forum held by German public broadcaster Deutsche Welle dubbed “The Heat Is On – Climate Change and the Media”, see here for background and here. According to Kulke the real objective of the forum:
The media are to warn the public of the dangers of climate change even more effectively and powerfully than before, and of course to make it even more clear that it’s the fault of man.
One well-attended workshop was: How To Deal With Climate Scepticism. Its own stated objective:
This workshop aims to point out what journalists must know about climate change policy, whom to trust and when to question their own professional procedures.
and warned:
Falling back on a “neutral” journalistic position can mean playing into the hands of the skeptics at the expense of the basis of life.
According to the workshop’s moderator, Bernhard Pötter of the newspaper Tageszeitung,
For journalists, climate change is the most important topic of the 21st century.
The “How To Deal With Climate Scepticism” workshop was designed to provide assistance to frustrated editors, authors and other journalists on how to best deal with the unwanted confrontation with a climate sceptic.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Oreskes’s Propaganda
One notable speaker at the workshop was Naomi Oreskes, who, according to Kulke, requests journalists eliminate the use of the word “scepticism” from their reporting. Kulke reports on Oreskes:
‘Scepticism” is too positive, and is indeed even a virtue in science. It’s better to use the word “contrarian’, which one can translate as ‘adversary’ or ‘dissenter’, says Oreskes. “Also it’s a no-no to use the term climate debate’.
‘It’s no wonder,’ complained Oreskes, ‘that people think science is still debating climate change when everywhere in newspapers one reads about a ‘debate’. Debate has long been in the history books. Climate change is a scientifically proven fact.’ It’s important for journalists to stress that the debate is over.
Ulli Kulke wonders what newspapers Oreskes could be possibly reading out in California, which would lead her to conclude the press is playing down climate change. Kulke writes:
In the years leading up to and after the last IPCC assessment report in 2007, the press and television reported daily on the coming end of the world in America and Europe.
But this has changed over the last half-year. Inconsistencies, cover-ups, big blunders and, most of all, exaggerations by climate scientists have been exposed. Some have admitted their errors. Even plots by scientists against their sceptic colleagues came to light. As a result the media have toned down their alarmism a little. And one even gets the impression that, since Climategate, journalistic principles have made a comeback. But some people have got a problem with that.
Like Oreskes.
Much to her chagrin, parts of the German press, such as Ulli Kulke, are not ready to abandon the principles of journalism. That’s good news.
Expect scepticism contrarianism to grow in Europe.
Share this...FacebookTwitter "
"

USHCN-M station at Greensboro, AL
While I was at NCDC, Grant Goodge showed and provided me with a PowerPoint presentation about the plan to update the USHCN manual observing network to USHCN-M or “modernized”. In a nutshell, it is a “light” version of the Climate Reference Network. The summary of benefits goes like this.
More Accurate Data Through:

Redundant Sensors
Near Real Time Diagnostics
Time Resolution of Five Minutes vs The Current Daily
Automated data collection via GOES satellite uplink, eliminating human error of reading and transcription
No adjustments to the data post reception. Time of Observation is now irrelevant.
No more routinely missing data, such as on weekends (fire station at Marysville, CA for example) and thus no need to fill in estimated data using the FILNET adjustment any more.

The station looks much like a Climate Reference Network station, but has some economy considerations, especially in having one aspirated IR screen instead of three, but it contains triple temperature sensors so that issues with instrumentation drift or offset events can easily be spotted in the data stream.

This will be a huge step forward in data quality and quality control.
His PowerPoint presentation is available here at this link: why-modernize-hcn (PPT 9 MB)
Interestingly, it included what appears to be a photo of the rooftop station in Asheville, NC, at the old NCDC (Federal Building) I’m waiting on a  positive ID.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9014bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
With apologies to Robert Duvall in Apocalypse now-
Kilgore: Smell that? You smell that?
Lance: What?
Kilgore: Sewage, son. Nothing in the world smells like that.
[kneels]
Kilgore: I love the smell of sewage in the morning.  The smell, you know that rotten eggs smell… Smells like… victory. Someday this war’s gonna end…

USHCN at Tullahoma, TN Wastewater Treatment Plant – Visible light

USHCN at Tullahoma, TN Wastewater Treatment Plant – Infra red
You know it seems like every morning this week that I prepare to start my day’s worth of surveys, I find that I’m going to visit another USHCN climate station of record at a sewage treatment plant. And so is the case today, my last day of surveys. I’m gonna take a loooong shower when I get home.
I know you all want to hear more about NCDC and USHCN2, and I’ll get into those details next week, but for now, another sewage treatment plant beckons.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fc997a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI know I shouldn’t go here, but the temptation is just too strong. I’ll let the readers make up their own minds. Here are some links to read. But do research more.UPDATE 1: Oh! Oh! Drudge has got Gore as the big headline.
1. Serious accusations
2. 3 reasons not to believe the accusations 
3. $540 massage(?) at ritzy hotel
4. Gore assault NYT
5. FULL POLICE REPORT
This eventually will boil down to the question: Can we really trust Al Gore? We saw how far he took the level of propaganda in his AIT film, which was carefully crafted to pull at the heart-strings and to mislead viewers. It was slammed by a British High Court. His jet-setting, mansion-buying lifestyle is in complete contradiction to what he preaches. He constantly ducks debate and keeps his head in the sand.
Not long ago he separated from his wife, indicating possible breach of trust in the relationship. He once claimed to have invented the internet. Just how believable is this guy?
My personal opinion is that Gore is as great a fraud as one will ever find, and he’s living high on the hog because of it. But that’s just my opinion, which is based on the so many words that have come out of his mouth and on his actions.
Indeed this has the potential to be much bigger than Climategate.
“But what does that have to do with the science,” one may ask? Gore is not a scientist, but he is a big messenger who has a message he wants (demands) everybody to believe. And it so happens that he has a huge interest in that message.
Look for the media to build a massive bulwark around Gore, and to come out blasting with everything they’ve got.

Share this...FacebookTwitter "
"
One of the things that happens when your work becomes well known is that people send you things to look at. Such is the case for today’s subject. Here we have a NOAA COOP station which is on the side of a mountain, well away from large cities. Only problem is, they put it right next to a parking lot.
A reader of this blog, Brad Herrick, sent me these photos of the Mt. Charleston weather station on State Route 157 west of Las Vegas.  For those that don’t know, Mt. Charleston is the large mountain to the west that overlooks Las Vegas. NOAA lists it on it’s COOP-A list, meaning that it reports for the climatic database. It’s been in operation since 1949. Its been moved 3 times, but all within about 1/2 mile as the fire station changed and grew.
According to NOAA’s MMS database, here is the description: Elevation, 7600 feet. NV DIV OF FORESTRY FIRE STN KYLE CYN OUTSIDE AND 30 MI NW PO AT LAS VEGAS NV. Topographic Details: RUGGED DEEP CANYON .25 – .5 MILE WIDE, RISING TO PEAKS 3000-5000 FEET HIGHER TO NORTH, SOUTH AND WEST A DISTANCE OF 2 TO 4 MILES. Lat/Lon 36.2597, -115.6452 , COOP ID 265400
Seems pretty rural, with a mental image of “way up in the mountains” if you were researching this station. By James Hansen’s figuring, it would also be a “lights=0” station since I doubt there is municipal street lighting for this area.
It’s certainly well enough away from the super sized Las Vegas concrete and asphalt heat island.
Here is the view from Google Earth:

click for larger view
Except for a few houses, it certainly looks “rural”.  Any researcher at NCDC or maybe a university that might use this station in some research report would certainly think this station was well away from the building/concrete/asphalt influence of bustling Las Vegas wouldn’t they?
But then we see this:

and this:

and this:

click for larger images
Unfortunately, I don’t have a time series temperature graph of this station to show you since I haven’t found a place at NCDC yet to graph COOP stations that are COOP-A. If anybody knows of such a link, please let me know. 
There’s nothing like convenient parking to convert a rural station to urban. But lets not forget the maintenance of the Stevenson Screen roof (see pic #2 -large), hillside, shade bushes, fire station building revisions, and portable storage unit. When did all that happen? We have no idea.
Surely, it’s easy to disentangle all that from the temperature record. Quick! Somebody create an adjustment equation.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1307288',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I had just finished reading a speech by George W. Bush in which he calls for creating a Compassion Capital Fund whereby the federal government would finance a wide variety of local social initiatives that “worked” when I received a letter from Milton and Rose Friedman asking me to give a speech at the upcoming Vancouver Mont Pelerin Society meeting making the optimistic case for the future of liberty. I didn’t know whether to laugh or to cry. After all, when the leading candidate of the so‐​called market‐​oriented party in the U.S. thinks that federal bureaucrats could or should engage in such activities, one despairs for the future of liberty. But one also doesn’t turn down Milton and Rose Friedman, so here I am.



Which is fine, because the truth be known, I genuinely am optimistic about the future for liberty in the world and, specifically in the United States of America. I apologize in advance, by the way, to our non-U.S. members and guests for the mostly provincial nature of my comments, but I think there is generally a universal applicability to the points I’ll be making. To begin with, what do we mean by liberty? To this audience the definition won’t be as contentious as it is for most of society, because as Hayek points out in the beginning of _The Constitution of Liberty_ , Abraham Lincoln said, “The world has never had a good definition of the word liberty … We all declare for liberty: but in using the same word, we do not mean the same thing.” Which is true, of course, but Hayek typically cuts quickly to the chase, saying we should seek “that condition of men in which coercion of some by others is reduced as much as possible in society.” He contrasts liberty to slavery by saying that liberty involves “the possibility of a person’s acting according to his own decisions and plans, in contrast to the position of one who was irrevocably subject to the will of another.”



By that standard the march of freedom that Milton described on Monday has indeed been impressive. And Milton said something else in his talk that is equally important in terms of assessing the prospects for liberty. He talked about the overwhelming support for socialism and the “tiny pockets of people who believed in freedom” back in 1947 when our Society’s founders met on Mont Pelerin. Everything must be put in perspective. My friend and one of the great champions of liberty in the Twentieth Century, the late Julian Simon, was keenly aware of this fact. Whenever a left‐​wing environmentalist would point to some trend starting in, say, 1985, the first thing Julian would ask was, Why 1985? Why not 1975 or 1875?



In a debate with environmentalist Hazel Henderson in 1996 Julian was confronted with a with a chart purporting to show the decline in pollution levels in London since the passage of London’s Clean Air Act of 1956. In his rebuttal, Julian produced his own chart showing the smoke levels in London dating back to the 1800s and, as reported in a great _Wired_ magazine profile of Simon, “the line from the 1920s on showed a constant and uniform downward slope. ‘If you look at all the data,’ Julian said, ‘you can’t tell that there was a clean‐​air act at any point.’ ”



And so in projecting the prospects for liberty, it’s probably useful to stand back a bit from the Compassion Capital Funds of 1999 and put things a bit more in perspective. In 1772 when there were 775,000,000 people inhabiting the world, it is estimated that only about 33,000,000 of them lived under relatively free governments. Some 95 percent of humanity lived lives described by historian Arthur Young as “miserable slaves of despotic tyrants.” As late as 1848, according to Julian Simon, the share of serfs among the population of Austria was about 72 percent, and in Hungary about 50 percent.



By another measure, worldwide per capita income in 1800 was $100; by 1900 is was about $500; next year it will be about $5000 and by the end of the next century some estimates put it to be in excess of $40,000, or higher than the average Western income of today. It could, of course, turn out to be much greater than even that.



In addition, any long term assessment of human liberty has to take into account the collapse of communism. Hundreds of millions of people today are free from the yoke of communist totalitarianism under which they labored just a decade or so ago. The change has been dramatic, even in Russia, despite all its difficulties. In those nations that have really moved toward capitalism, the past decade has been nothing short of exhilarating. As _Business Week_ noted a couple of months ago, for instance, “Poland has enjoyed brisk economic growth for most of the decade because it chose radical reform, and despite the pain, stuck with it.”



The only remaining communist country of any consequence is China which, as Milton has pointed out, for all it’s human rights failings is nevertheless clearly headed in a capitalist direction. The Associated Press distributed a photo recently of a protester in Tiananmen Square sporting an umbrella painted with the slogan “Privatize. Give all state property to the people.” He was arrested, to be sure, but when such subversive ideas are alive in the land, the end is near for the thugs in Beijing. 



And not all of the Chinese leaders in Beijing are thugs. A couple of years ago in Shanghai Jose Pinera and I met with an individual from Beijing who has been charged with responsibility for creating a public pension system in China. His name is Sun Jianyong and as he approached us in the lobby of the Peace Hotel, Jose remarked, “But you are much younger than I had anticipated.” To which Sun, who appeared to be in his late thirties, replied, “But you were only thirty when you privatized Social Security in Chile.” At the meeting Sun convinced us that he was a great admirer of the Chilean system because of the higher income at retirement, the economic boost from increased savings, and, he said, because it gave people the dignity of not depending on the state for their retirement income. Some communist.



Speaking of the collapse of communism, I think one of the clear indicators of the fact that liberty has the long‐​term momentum today is what Vaclav Klaus and John O’Sullivan talked about on Monday. The so‐​called Third Way. Because, believe me, Bill Clinton, Tony Blair and those other European politicians wouldn’t be adopting that phrase — the Third Way — if socialism wasn’t as thoroughly discredited as it is. They are leftists who are trying desperately to hide that fact from the voters. To a large degree they’ve succeeded. But such deceit won’t be successful over the long haul as it becomes increasingly evident that whatever they call themselves, they always end up promoting more state intrusion into civil society. Bill “The era of big government is over” Clinton, for instance, offered no fewer than 95 new or expanded federal interventions in his January State of the Union Address. The Third Way politicians are trying to sugar‐​coat statism in the rhetoric of free markets and reinventing government, but in the Information Age they will sooner or later — sooner, probably — be exposed for the frauds that they are.



I mentioned the interest in China in setting up a private, individually capitalized pension system. There is, of course, tremendous interest in doing so in the United States also, in large part, because of the work of the Cato Institute, the Heritage Foundation and the National Center for Policy Analysis, each of which, in turn, is indebted to the incredible work of the international Pied Piper of pension reform, our good friend Jose Pinera. Even if we live in an era of Bill Clinton and George W. Bush in the U.S., the fact is that by any objective standard, classical liberal ideas are making remarkable progress in the national policy debate there. Privatizing Social Security is supported by two‐​thirds of the population in the United States, and if you talk to people under 50, it’s nearly unanimous. Men and women, Republicans, Democrats and Independents, union workers, blacks, whites, Asians, and Hispanics all overwhelming favor junking Social Security, the centerpiece of the New Deal. When asked if when a system is set up to allow the purchase of stocks and bonds government or individual workers should be allowed to invest the funds, by a margin of nearly five to one, Americans say individuals should be allowed to invest on their own. They also say the present government‐​run pay‐​as‐​you‐​go system is riskier than the market. This is all from a Zogby International poll we commissioned recently that will be released in a week or so. In looking over those poll results, by the way, I was reminded of a poll from the Pew Research Center last year that asked government officials this question: “Do Americans know enough about issues to form wise opinions about what should be done?” Here are the results. Among members of Congress 31 percent said yes, 47 percent said no. Among Presidential appointees, 13 percent said yes, and 77 percent said no. Civil servants also are disdainful of the American people, with 14 percent saying the public can form wise decisions and a whopping 81 percent saying no, they can’t. This huge gulf between the political class and the people in the U.S., it seems to me, is another cause for optimism.



Getting back to Social Security, it’s true that neither political party has had the courage to call for complete privatization today, but that’s what the people want and it may well turn out to be a decisive issue in next year’s presidential campaign. Dr. Pinera, who’s working with Cato in our efforts in that regard, has already succeeded in bringing some form of privatization to pension systems in no less than eight Latin American countries and, most recently, Poland. To achieve such a thing in the U.S. would not only dramatically change our political dynamics in a very favorable direction from a classical liberal perspective, I believe it would also put tremendous pressure on the European Union and Japan to follow suit. They cannot compete with the U.S. or survive forever with public pension systems that feature unfunded liabilities of two or three hundred percent of GDP.



There are other significant policy gains evident in the U.S. today — which is not to say that we’ve won them — but that progress is clearly being made. Today the education monopoly is under attack as never before. The teachers unions are in rapid retreat, throwing Charter Schools at the snarling masses in the hopes of placating them before they tear down the walls of their monopoly. Ten years ago, not to mention when Milton Friedman first wrote about school vouchers, the unions were impervious to criticism. 



In the area health care, there is a growing understanding that it’s the third party payer system, whether government or first dollar insurance coverage, that’s to blame for bureaucratized and expensive health care in America. Hillary Clinton’s effort to sell essentially the Canadian system as the model for the U.S. broke down when it became common knowledge that people in this country travel south when they have serious health problems, despite the deficiencies of the U.S. system. There is a serious effort underway now to expand Medical Savings Accounts and, indeed, to separate health insurance from employment through equal tax treatment, something a growing number of major corporations in the U.S. now favor. All of this undermines efforts to socialize medicine in the United States.



In other policy fronts, the welfare establishment has never really recovered from the assault on its hegemony by Charles Murray’s _Losing Ground_ and today lives with the reality that welfare is no longer a federal entitlement. Most people clearly understand the counterproductive nature of the dole and are determined to hold their fellow citizens responsible for their own actions, as they did prior to the advent of the paternalistic Great Society programs of the Sixties.



There is also a growing consensus that scrapping the 9000‐​plus page IRS code in the U.S. would be a good thing to do. Tax simplification is something all politicians now must at least pay lip service to. At Cato we frequently have forums on the flat tax or replacing the income tax altogether, not with a VAT, but with a retail sales tax. It is virtually impossible to get a politician or even someone from the IRS to defend the current system at these events. Radical simplification of the tax code not only would be good economically, but it would end the patronizing policies of politicians who now use the tax code to socially engineer citizen behavior. It would also create some taxpayer solidarity in the movement to sharply lower taxation in America. We are making progress in this area, including creating a consensus to abolish both the capital gains tax and the death tax.



Further, trade policy has clearly been on a positive trend in the U.S. for decades. Free traders have won the intellectual battle. The United States today has lower tariffs, as measured by the ratio of tariff income to the value of imports, than at any time in our history. In 1929 with the Smoot‐​Hawley tariff, that number stood at nearly 60 percent. Today it is less than 4 percent. Furthermore, trade and foreign investment income as a percentage of GDP in the U.S. is at an all‐​time high of 30 percent, when as recently as three decades ago it was only 15 percent of GDP. Internationally, a large number of countries ranging from Chile, to Mexico, Argentina, Australia, New Zealand, the transition countries of Central and Eastern Europe and even, to a certain degree, India, are following suit. As, indeed, they must if they’re to survive in the new global economy.



One other positive development in the United States has been a series of court decisions that may portend the end of a very sorry history of jurisprudence in the U.S. dating back to 1937 when Franklin Roosevelt threaten to pack the Supreme Court unless it agreed to ignore its clear constitutional responsibilities and capitulate to FDR’s grand social schemes. Thomas Jefferson once said that “The natural progress of things is for government to gain ground and for liberty to yield.” Kind of an early Public Choice analysis. What Jefferson and most of the American Founders understood so well was that there is an inherent built‐​in bias for the state to expand. The statist imperative, if you will. Without some kind of institutional constraints — in the case of the United States, the Constitution — the majoritarian instinct in a democracy would naturally lead to the tendrils of the state reaching into every corner of civil society.



As they pretty much have since 1937. But, as I say, that all may be changing. The father of the Constitution, James Madison, said that the courts were to be the “bulwark of our liberties” against the inevitable majoritarian onslaught from the two political branches of the national government. In recent years the federal courts have once again started defending property rights, have been firm in support of free speech rights, have challenged Congress not to delegate its power to unelected bureaucrats, and even have resurrected the essence of the Constitution, the Doctrine of Enumerated Powers, whereby if the power is not specifically delegated to the national government it is reserved to the states or to the people. A renaissance of respect for the Constitution, which seems to be taking place in the States, is imperative if the prospects for liberty are to be as positive as they should be.



So, in conventional terms, the prospects for liberty are, if you stand back far enough, pretty bright. But as the people in this room are well aware, there are other forces at work which augur even more brightly for a global future with far less political society and far more civil society. I speak, of course, of the Information Age and the two most dramatic things it brings to society: widespread, diversified and instantaneous access to knowledge, and on the financial side of the ledger, what economist Richard McKenzie accurately calls “quicksilver capital” — the ability of capital to move anywhere in the world with the click of a mouse. An ancillary benefit of the financial revolution that is occurring, and to which Milton briefly referred, is what our colleague Richard Rahn refers to as “the end of money.” There are brochures on his book of the same name at the Cato table outside and I highly recommend that you order a copy of the book, which must be giving central bankers around the world severe cases of heartburn.



At the Cato Institute we prefer to discuss the political battle, that is, man’s relationship to the state, in terms of civil society versus political society, rather than liberal versus conservative or even libertarian. In a civil society you make the choices about your life — how to spend your money, where to send your children to school and so forth — in a political society, based as it is on coercion, somebody else — a politician or a bureaucrat — makes those decisions. The goal, it seems to us, should be to minimize the role of political society consistent with the protection of our individual liberties.



Political society, of course, has historically derived its power from three main sources: Geographical territory, which is to say land; control of the flow and nature of information because knowledge is power; and control over capital flows and the value of a nation’s currency. The Information Age is eating away at those three sources of power just as surely as the sun rises in the East.



Geographical territory and natural resources, as Hong Kong let anyone who was paying attention know decades ago, become increasingly irrelevant with the advent of the new Global Economy made possible by the information revolution in knowledge and finance. Indeed, the computer‐​challenged Soviet Union ended up finding geographical territory a liability in its contest with the information‐​rich West. Let me read to you from a book that’s been on the _New York Times_ bestseller list for four months now. It’s called _The Lexus and the Olive Tree_ and it’s written by the _Times_ ’ chief foreign correspondent, Thomas Friedman. Friedman, I should say, unlike our Friedmans, is a liberal in the bad sense of the word — an Al Gore Democrat. But the first half of the book is really terrific. He refers in the following quote to the “Golden Straitjacket,” by which he means that in order to benefit from the new global economy, nations must play by certain rules. Here’s what he writes:



“To fit into the Golden Straightjacket a country must either adopt, or be seen as moving toward, the following golden rules: making the private sector the primary engine of its economic growth, maintaining a low rate of inflation and price stability, shrinking the size of its state bureaucracy, maintaining as close to a balanced budget as possible, if not a surplus, eliminating or lowering tariffs on imported goods, removing restrictions on foreign investment, getting rid of quotas and domestic monopolies, increasing exports, privatizing state‐​owned industries and utilities, deregulating capital markets, making its currency convertible, opening its industries, stock, and bond markets to direct foreign ownership and investment, deregulating its economy to promote as much domestic competition as possible, eliminating government corruption, subsidies and kickbacks as much as possible, opening its banking and telecommunications systems to private ownership and competition, and allowing its citizens to choose from an array of competing pension options and foreign‐​run pension and mutual funds.…As your country puts on the Golden Straightjacket,” he writes, “two things tend to happen: your economy grows and your politics shrinks.”



Not bad for a liberal Democrat, is it? Before you Americans decide to vote for Al Gore, however, I should point out that the second half of _The Lexus and the Olive Tree_ is truly awful — full of mush‐​minded environmentalism, bleeding‐​heart calls for more funding for the IMF and World Bank, and more taxing of the rich. So, he doesn’t really get it, but Friedman’s analysis of the nature of the new global economy is brilliant. So brilliant, in fact, that I think much of the analysis came directly from Walter Wriston’s wonderful 1991 book, _The Twilight of Sovereignty_. That book, written in anticipation of the Internet, has to be one of the most thoughtful, prescient books of all time. Walt Wriston simply sees things the rest of us can’t.



In it he writes, “Intellectual capital is becoming relatively more important than physical capital. Indeed, the new source of wealth is not material, it is information, knowledge applied to work to create value. The pursuit of wealth is now largely the pursuit of information.” And in competition with the private sector today, government can’t possibly keep up in the pursuit of information. Individuals are being empowered irrespective of borders; irrespective of what politicians have done throughout the sorry history of government domination of society, which is happily coming to an end: The twilight of sovereignty, as Walter puts it.



And as Walter knows so well, one of the great sources of power for the state has been its ability to control capital flows by regulating major financial institutions, among other means. But one of the great aspects of the information revolution has been disintermediation — the decreasing need for the middleman, for major institutions — and the increasing ability of people to deal with one another directly, anywhere in the globe. Consider, for instance, the fact that in 1997 the singer David Bowie raised $55 million in capital based on his projected royalties. The ability of capital markets to securitize virtually any future income flow, combined with the ability of companies to set up operations virtually anywhere on the globe, means that developing nations can expect explosive growth in the next century and that IMF and World Bank bureaucrats can start looking for honest work.



Richard Rahn writes in his book, “The world’s people will be neither truly prosperous nor free unless governments retreat from their seemingly never‐​ending desire to control the production and use of money.” He then goes on to persuasively demonstrate that they have no choice but to give up that control. Private, digital, encrypted money is already a reality and it will become the norm early in the next century. It is likely that those nations that wish to preserve their sovereignty in the future will do so only in a superficial sense, and then only by pursuing policies of very low taxation and free and open trade.



We live in interesting times. When the Agricultural Age turned into the Industrial Age virtually no one was aware of what was happening. But as the Industrial Age turns into the Information Age, by virtue of the age it’s turning into, virtually everyone is aware of it. It’s estimated that by the end of the year 2000 some 100 million Americans will be plugged into the Internet. Some even suggest that the Internet may come to Canada. _Wired_ magazine has dubbed those individuals who participate on the Net, Netizens. In a classic article from 1997 in _Wired_ , Jon Katz wrote, “The Digital Nation constitutes a new social class. Its citizens are young, educated, affluent. They inhabit wired institutions and industries — universities, computer and telecom companies, Wall Street and financial outfits, the media.…and some of their common values are clear: they tend to be libertarian, materialistic, tolerant, rational, technologically adept, disconnected from conventional political organizations — like the Republican or Democratic parties — and from narrow labels like liberal or conservative.…The digital young, from Silicon Valley entrepreneurs to college students, have a nearly universal contempt for government’s ability to work; they think it’s wasteful and clueless. On the Net, government is rarely seen as an instrument of positive change or social good. Politicians are assumed to be manipulative or ill‐​informed, unable to affect reform or find solutions, forced to lie to survive.”



Katz went on to suggest that this Netizen community will fuse technology with politics in such a manner as to advance civil society. I think he’s right. The twilight of sovereignty means the dawning of a new age of liberty and the empowerment of individual choice. The world is moving toward pluralism, capitalism, and civil society. It will take time, but it likely will happen. But it will happen because as the world community grows, as we get to know one another and work with one another around the globe, independent of the political process, civil society will flourish. Increasingly, it will be groups like the Mont Pelerin Society, and not political parties, that lead the way. I’m reminded of that famous quote from the French politician Alexandre Ledru‐​Rollin, who was quoted while among the mob during the Paris revolt of 1848 as saying, “There go the people. I must follow them, for I am their leader.”



Politicians and political society are not the answer. The Mont Pelerin Society, uncountable other voluntary organizations and civil society are the answer. Thus, let me conclude with the same plea my colleague Bill Niskanen made at last year’s meeting in Washington, D.C. We do live in the information age today. There clearly was a time in 1947 and through the Fifties when the idea of secret, off‐​the‐​record talks at MPS made sense, given the intellectual climate of the time. But my goodness, our members win Nobel Prizes now. We should be celebrating the fact that classical liberals from dozens of nations attend these events. We should invite the media in and drop the conceit that we have something to hide, or something for which the outside world is somehow not worthy. The future of liberty does indeed look bright, but it won’t happen automatically. It will require leadership and openness. I trust we can all work together toward that goal. Thank you very much.


"
"
Since this blog has main focused on air temperature measurement, and has not done any discussion of manual measurement techniques of Sea Surface Temperature measurements, I thought it would be good to first review some of the instrumentation used.
Sea Surface Temperature Measurement Instruments:
Standard Thermometer

Measures: Temperature in degrees, typically used in the bucket thermometer
Operates: At any depth by cable or line or by hand
Notes: Mercury in original thermometer has been replaced in many standard
thermometers by less toxic materials

Bucket Thermometer

Measures: Water temperature near the surface
Operates: At the surface by hand or line
Notes: Typically lowered about 1 meter into the water, left there for one
minute, and then retrieved deck side for reading.

Reversing Thermometer (for Nansen Bottle)

Measures: Water temperature at a specific predetermined depth
Operates: Only when turned 180 degrees (the mercury breaks in the special loop
and will not get back together until reset) Temperature at depth can be recorded
with a 180 degree flip (as is done with the Nansen Bottle) and there will be no
change on the way up.
Notes: A Nansen bottle
is a device for obtaining samples of seawater at a specific depth. It was
designed in 1910 by the early explorer and oceanographer Fridtjof Nansen.
The bottle, a metal or plastic cylinder, is lowered on a cable into the sea, and
when it has reached the required depth, a brass weight called a “messenger” is
dropped down the cable. When the weight reaches the bottle, the impact tips the
bottle upside down and trips a spring-loaded valve at the end, trapping the
water sample inside. The bottle and sample are then retrieved by hauling in the
cable.

Bathythermograph (BT)

Measures: Water temperature over a range of depth
Operates: Over any depth with a cable or line by hand or with a hydraulic winch
Notes: This model records the information inside and is retrieved however there
are expendable models (XBTs) that free fall on a copper line and transmit the
temperature and depth information through the copper wire before dropping to the
bottom


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ef4fb93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFirst at Twitter, aspiring meteorologist Chris Martz posted a chart of tornado activity since 1954. Contrary to what climate alarmists have claimed, tornado frequency has not trended upwards:

Hurricane claim involves “massive error”
While on the topic of violent weather, the Washington Post recently published another alarmism-fraught article on the alleged increasing strength of hurricanes due to global warming and how there’s now a “slower decay of landfalling hurricanes in a warming world.”
However, Dr. Roger Pielke Jr. responded saying that there was a “massive error” in the new Nature paper, which  the Washington Post was citing.
“Says it shows hurricanes decaying slower over land post-landfall (more damaging). But they forgot to remove storms that landfall & then go back over the ocean,” Pielke tweeted.
In the Twitter thread, Dr. Ryan Maue pointed out that some of the hurricanes plotted were not “a typical example” of inland hurricane decay.
No real trend


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Examining global landfall cyclone data since 1970, There’s no observed noteworthy trend: Dr. Ryan Maue also tweeted a chart on tropical cyclone activity over the past 50 years:

Declining in some regions
The often heard claim of more frequent and intense cyclones is not supported by the data. Another paper by Zhao et al from 2018 in fact shows Western North Pacific cyclones becoming less frequent since the start of the century.

Moreover, data from the Japan Meteorological Agency (JMA) show that the number of typhoons formed since 1951 has trended somewhat downward:

So it’s a mystery as to why the Washington Post and Nature would create alarm and make false claims over tropical cyclone activity. That’s science no one should consider, let alone follow.


		jQuery(document).ready(function(){
			jQuery('#dd_baa535913b025a18770bfce7c5c773ee').on('change', function() {
			  jQuery('#amount_baa535913b025a18770bfce7c5c773ee').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Below is an opinion from the Washington Times by Geophysicist David Denning, to which I’ve added photos and links to the events he’s written about.
 
A caveat: I caution the reader that annual weather does not equate to long term climate. Yes we have had a number of record cold events in 2007, and the winter in the Northern Hemisphere is already shaping up to be colder than normal. But fortunes of weather can turn on a dime. I’d also point out that we are in a solar minimum right now and predictions of solar cycle 24’s peak range from it being very low (colder) to very high (warmer). The next few years will be telling.

Snow in Buenos Aires, July 9th, 2007



Year of global cooling
By David Deming
December 19, 2007


South America this year experienced one of its coldest winters in decades. In Buenos Aires, snow fell for the first time since the year 1918. Dozens of homeless people died from exposure. In Peru, 200 people died from the cold and thousands more became infected with respiratory diseases. Crops failed, livestock perished, and the Peruvian government declared a state of emergency.

Unexpected bitter cold swept the entire Southern Hemisphere in 2007. Johannesburg, South Africa, had the first significant snowfall in 26 years. Australia experienced the coldest June ever. In north-eastern Australia, the city of Townsville underwent the longest period of continuously cold weather since 1941. In New Zealand, the weather turned so cold that vineyards were endangered.

Last January, $1.42 billion worth of California produce was lost to a devastating five-day freeze. Thousands of agricultural employees were thrown out of work. At the supermarket, citrus prices soared. In the wake of the freeze, California Gov. Arnold Schwarzenegger asked President Bush to issue a disaster declaration for affected counties. A few months earlier, Mr. Schwarzenegger had enthusiastically signed the California Global Warming Solutions Act of 2006, a law designed to cool the climate. California Sen. Barbara Boxer continues to push for similar legislation in the U.S. Senate.

In April, a killing freeze destroyed 95 percent of South Carolina’s peach crop, and 90 percent of North Carolina’s apple harvest. At Charlotte, N.C., a record low temperature of 21 degrees Fahrenheit on April 8 was the coldest ever recorded for April, breaking a record set in 1923. On June 8, Denver recorded a new low of 31 degrees Fahrenheit. Denver’s temperature records extend back to 1872.

Recent weeks have seen the return of unusually cold conditions to the Northern Hemisphere. On Dec. 7, St. Cloud, Minn., set a new record low of minus 15 degrees Fahrenheit. On the same date, record low temperatures were also recorded in Pennsylvania and Ohio.

Extreme cold weather is occurring worldwide. On Dec. 4, in Seoul, Korea, the temperature was a record minus 5 degrees Celsius. Nov. 24, in Meacham, Ore., the minimum temperature was 12 degrees Fahrenheit colder than the previous record low set in 1952. The Canadian government warns that this winter is likely to be the coldest in 15 years.

… If you think any of the preceding facts can falsify global warming, you’re hopelessly naive. Nothing creates cognitive dissonance in the mind of a true believer. In 2005, a Canadian Greenpeace representative explained ‘global warming can mean colder, it can mean drier, it can mean wetter.’ In other words, all weather variations are evidence for global warming. I can’t make this stuff up.’
Note from Anthony: That’s what I call CYA forecasting 😉
No; but others can, and do. However, maybe at long last the penny is dropping. The New Statesman, no less, this week publishes a piece by sensible David Whitehouse which says flatly:… The fact is that the global temperature of 2007 is statistically the same as 2006 as well as every year since 2001. Global warming has, temporarily or permanently, ceased. Temperatures across the world are not increasing as they should according to the fundamental theory behind global warming – the greenhouse effect. Something else is happening and it is vital that we find out what or else we may spend hundreds of billions of pounds needlessly.

… For the past decade the world has not warmed. Global warming has stopped. It’s not a viewpoint or a sceptic’s inaccuracy. It’s an observational fact…. So we are led to the conclusion that either the hypothesis of carbon dioxide induced global warming holds but its effects are being modified in what seems to be an improbable though not impossible way, or, and this really is heresy according to some, the working hypothesis does not stand the test of data.

It was a pity that the delegates at Bali didn’t discuss this or that the recent IPCC Synthesis report did not look in more detail at this recent warming standstill.A pity indeed, that the entire western ruling class has been taken in by this scam.But now the cavalry appears at last to have arrived. According to this story, a US Senate report documents the opinion of hundreds of prominent scientists from around the world who say global warming and cooling is a cycle of nature and cannot legitimately be connected to man’s activities.The report compiled observations from more than 400 prominent scientists from more than two dozen nations who have voiced objections to the so-called ‘consensus’ on ‘man-made global warming.’ Many of the scientists are current or former participants in the United Nation’s Intergovernmental Panel on Climate Change, whose present officials, along with former Vice President Al Gore, have asserted a definite connection.

The new report comes from the Senate Environment and Public Works Committee’s office of the GOP ranking member, and cites the hundreds of opinions issued just in 2007 that global warming and man’s activities are unrelated. [My emphasis]…‘Many scientists from around the world have dubbed 2007 as the year man-made global warming fears “bite the dust”’, the introduction said. And there probably would be many more scientists making such statements, were it not for the fear of retaliation from those aboard the global-warming-is-caused-by-SUVs bandwagon, the report said.And it details some of this intimidation.
Looks like man-made global warming theory is melting away faster than you can say Al Gore. A lot of reputations are now going to disappear along with it: all those who were part of the famous ‘consensus’ (not).Those people should never be taken seriously again.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1f56227',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
NOTE: This essay represents a collaboration over a period of a week via email between myself and Basil Copeland. Basil did the statistical heavy lifting and the majority of writing, while I provided suggestions, reviews, some ideas, editing, and of course this forum. Basil deserves all our thanks for his labor. This is part one of a two part series.  -Anthony

Evidence of a Significant Solar Imprint in Annual Globally Averaged Temperature TrendsBy Basil Copeland and Anthony Watts 
It is very unlikely that the 20th-century warming can be explained by natural causes. The late 20th century has been unusually warm.
 
So begins the IPCC AR4 WG1 response to Frequently Asked Question 9.2 (Can the Warming of the 20th Century be Explained by Natural Variability?).  Chapter 3 of the WG1 report begins:
Global mean surface temperatures have risen by 0.74°C ± 0.18°C when estimated by a linear trend over the last 100 years (1906-2005). The rate of warming over the last 50 years is almost double that over the last 100 years (0.13°C ± 0.03°C vs. 0.07°C ± 0.02°C per decade).
Was the warming of the late 20th century really that unusual?  In recent posts Anthony has noted the substantial anecdotal evidence for a period of unusual warming in the earlier half of the 20th century.  The representation by the IPCC of global trends over the past 100 years seems almost designed to hide the fact that during the early decades of the 20th century, well before the recent acceleration in anthropogenic CO2 emissions beginning in the middle of the 20th century, global temperature increased at rates comparable to the rate of increase at the end of the 20th century.
I recently began looking at the longer term globally averaged temperature series to see what they show with respect to how late 20th century warming compared to warming earlier in the 20th century.  In what follows, I’m presenting just part of the current research I’m currently undertaking.  At times, I may overlook details or a context, or skip some things, for the sake of brevity.  For example, I’m looking at two long-term series of globally averaged annual temperature trends, HadCRUTv3 and GHCN-ERSSTv2.  Most of what I present here will be based on HadCRUTv3, though the principal findings will hold true for GHCN-ERSSTv2.
I began by smoothing the data with a Hodrick-Prescott (HP) filter with lambda=100.  (More on the value of lambda later.) The results are presented in Figure 1.

Figure 1 – click for a larger image
The figure shows the actual data time series, a cyclical pattern in the data that is removed by the HP filter, and a smoothed long term low frequency trend that results from filtering out the short term higher frequency cyclical component. Hodrick-Prescott is designed to distinguish short term cyclical activity from longer term processes.
For those with an electrical engineering background, you could think of it much like a bandpass filter which also has uses in meteorology:
Outside of electronics and signal processing, one example of the use of band-pass filters is in the atmospheric sciences. It is common to band-pass filter recent meteorological data with a period range of, for example, 3 to 10 days, so that only cyclones remain as fluctuations in the data fields.
(Note: For those that wish to try out the HP filter, a freeware Excel plugin exists for it which you can download here)
When applied to globally averaged temperature, it works to extract the longer term trend from variations in temperature that are of short term duration.  It is somewhat like a filter that filters out “noise,” but in this case the short term cyclical variations in the data are not noise, but are themselves oscillations of a shorter term that may have a basis in physical processes.
For example, in Figure 1, in the cyclical component shown at the bottom of the figure, we can clearly see evidence of the 1998 Super El Niño.  While not the current focus, I believe that analysis of the cyclical component may show significant correlations with known shorter term oscillations in globally averaged temperature, and that this may be a fruitful area for further research on the usefulness of Hodrick-Prescott filtering for the study of global or regional variations in temperature.
My original interest was in comparing rates of change between the smoothed series during the 1920’s and 1930’s with the rates of change during the 1980’s and 1990’s.  Without getting into details (ask questions in comments if you have them), using HadCRUTv3 the rate of change during the early part of the 20th century was almost identical to the rate of change at the end of the century. Could there be some sense in which the warming at the end of the 20th century was a repeat of the pattern seen in the earlier part of the century?  Since the rate of increase in greenhouse gas emissions was much lower in the earlier part of the century, what could possibly explain why temperatures increased for so long during that period at a rate comparable to that experienced during the recent warming?
As I examined the data in more detail, I was surprised by what I found.  When working with a smoothed but non-linear “trend” like that shown in Figure 1, we compute the first differences of the series to calculate the average rate of change over any given period of time.  A priori, there was no reason to anticipate a particular pattern in time (or “secular pattern”) to the differenced series.  But I found one, and it was immediately obvious that I was looking at a secular pattern that had peaks closely matching the 22 year Hale solar cycle.  The resulting pattern in the first differences is presented in Figure 2, with annotations showing how the peaks in the pattern correspond to peaks in the 22 year Hale cycle.
Besides the obvious correspondence in the peaks of the first differences in the smoothed series to peaks of the 22 year Hale solar cycle, there is a kind of “sinus rhythm” in the pattern that appears to correspond, roughly, to three Hale cycles, or 66 years.  Beginning in 1876/1870, the rate of change begins a long decline from a peak of about +0.011 (since these are annual rates of change, a decadal equivalent would be 10 times this, or +0.11C/decade) into negative territory where it bottoms out about -0.013, before reversing and climbing back to the next peak in 1896/1893.  A similar sinusoidal pattern, descending down into negative annual rates of change before climbing back to the next peak, is evident from 1896/1893 to 1914/1917.  Then the pattern breaks, and in the third Hale cycle of the triplet, the trough between the 1914/1917 peak and the 1936/1937 peak is very shallow, with annual rates of change never falling below +0.012, let alone into the negative territory seen after the previous two peaks.  This same basic pattern is repeated for the next three cycles: two sinusoidal cycles that descend into negative territory, followed by a third cycle with a shallow trough and rates of change that never descend below +0.012.  The shallow troughs of the cycles from 1914/1917 to 1936/1937, and 1979/1979 to 1997/2000, correspond to the rapid warming of the 1920’s and 1930’s, and then again to the rapid warming of the 1980’s and 1990’s.
While not as well known as the 22 year Hale cycle, or the 11 year Schwabe cycle, there is support in the climate science literature for something on the order of a 66 year climate cycle.  Schlesinger and Ramankutty (1994) found evidence of a 65-70 year climate cycle in a number of temperature records, which they attributed to a 50-88 year cycle in the NAO.  Interestingly, they sought to infer from this that these oscillations were obscuring the effect of AGW.  But that probably misconstrues the significance of the mid 20th century cooling phase.  In any case, the evidence for a climate cycle on the order of 65-70 years extends well into the past.  Kerr (2000) links the AMO to paleoclimate proxies indicating a periodicity on the order of 70 years.  What I think they may be missing is that this longer term cycle shows evidence of being modulated by bidecadal rhythms.  When the AMO is filtered using HP filtering, it shows major peaks in 1926 and 1997, a period of 71 years.  But there are smaller peaks at 1951 and 1979, indicating that shorter periods of 25, 28, and 18 years, or roughly bidecadal oscillations.  There is a growing body of literature pointing to bidecadal periodicity in climate records that point to a solar origin.  See, for instance, Rasporov, et al, (2004).  A 65-70 year climate cycle may simply be a terrestrial driven harmonic of bidecadal rhythms that are solar in origin.
In terms of the underlying rates of change, the warming of the late 20th century appears to be no more “unusual” than the warming during the 1920’s and 1930’s.  Both appear to have their origin in a solar cycle phenomenon in which the sinusoidal pattern in the underlying smoothed trend is modulated so that annual rates of change remain strongly positive for the duration of the third cycle, with the source of this third cycle modulation perhaps related to long term trends in oceanic oscillations.  It is purely speculative, of course, but if this 66 year pattern (3 Hale cycles) repeats itself, we should see a long descent into negative territory where the underlying smoothed trend has a negative rate of change, i.e. a period of cooling like that experienced in the late 1800’s and then again midway through the 20th century.

Figure 2 – click for a larger image
Figure 2 uses a default value of lambda (the parameter that determines how much smoothing results from Hodrick-Prescott filtering) that is 100 times the square of the data frequency, which for annual data would be 100.  This is conventional, and is consistent with the lambda used for quarterly data in the seminal research on this technique by Hodrick and Prescott.  I’m aware, though, of arguments for using a much lower lambda, which would result in much less smoothing.
In Part 2, we will look at the effect of filtering with a lower value of lambda.  The results are interesting, and surprising.
Part 2 is now online here
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0602b82',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman skeptic site Die Kalte Sonne here debunks a recent alarmist article appearing in Spiegel aimed at shocking its readers. The reality, it turns out, is not shocking at all.
Greenland ice doomed?
According to Spiegel, the Greenland ice sheet is already doomed (that is unless we skip the usual democratic process and just act immediately).
Spiegel claims Greenland “glaciers are continuously losing huge masses of ice” and that the system there is “dramatically off balance”. The leftist Hamburg-based weekly reported:
The melting of the glaciers on Greenland has apparently passed the point of no return. Even if the global rise in temperature were to stop immediately, the ice sheet would continue to retreat, report researchers led by Michalea King of Ohio State University report in the journal “Communications Earth and Environment“.
Read more at Spiegel

4°C warmer 11,000 years ago
But Die kalte Sonne wondered if this were really so, and needed only 2 mouse clicks to find a recent temperature reconstruction for Greenland’s past (Lecavalier et al. 2017, pdf here). The paper’s Figure 4a  shows the temperatures, with the temperature of 1950 at the far right which in paleo-climatology is always meant as “present”.
 

Thus, 11,000 years ago, it was up to 4°C warmer than in 1950 over long periods of thousands of years, and today the warming has been about 1°C since then. Since we can see an ice sheet of 2,850,000 km³ (that is roughly Gt) today, the “point of no return” cannot have been exceeded 10,000 years ago. How does the heading then come about? We take a look at the associated work by King et al. 2020:
Dynamic ice loss from the Greenland Ice Sheet driven by sustained glacier retreat


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Greenland Ice Sheet is losing mass at accelerated rates in the 21st century, making it the largest single contributor to rising sea levels. Faster flow of outlet glaciers has substantially contributed to this loss, with the cause of speedup, and potential for future change, uncertain. Here we combine more than three decades of remotely sensed observational products of outlet glacier velocity, elevation, and front position changes over the full ice sheet. We compare decadal variability in discharge and calving front position and find that increased glacier discharge was due almost entirely to the retreat of glacier fronts, rather than inland ice sheet processes, with a remarkably consistent speedup of 4–5% per km of retreat across the ice sheet. We show that widespread retreat between 2000 and 2005 resulted in a step-increase in discharge and a switch to a new dynamic state of sustained mass loss that would persist even under a decline in surface melt.
Is there any talk of an irreversible end of the Greenland ice sheet? From the abstract:
We show that widespread retreat between 2000 and 2005 resulted in a step-increase in discharge and a switch to a new dynamic state of sustained mass loss that would persist even under a decline in surface melt.“
The authors see an acceleration in melting towards the ocean in the period 2000-2005, with not enough snowfall to compensate for the losses. They find a loss of about 500 Gt/year.
Only 0.15% of total ice mass
Unfortunately, they do not address the highly accurate gravity measurements with satellites in their paper. These data show a linear mass loss of only 275 Gt/year between 2003 and 2019 (with a gap in 2017 and 2018 due to a satellite change), so that in 17 years about 4200 Gt were lost, which is 0.15% of the total sheet.
What exactly do they say about the future?
Ultimately, predictions of future change will require improved understanding of the ice/ocean boundary and controls on glacier calving.“
Low-fact propaganda
This is much more cautious than what is being served up to us as “doomed” with the usual “overconfidence”. A look into the past is enough to unmask the media scream for what it is: low-fact propaganda.
Also read here at Ice Age Now.
Share this...FacebookTwitter "
"

You know, for as much as we humans think we really have control over our planet, nature tends to remind us from time to time that we are just flyspecks in the vastness of space and energy. Take for example the amount of energy we get from the sun: 174.0 PetaWatts – (10^15 watts) which is the total power received by the Earth from the Sun. Now compare that to this news item.
From Slashdot: Astronomers are still speculating as to what could have caused an abnormally strong five millisecond burst to be detected six years ago when it completely saturated their recording equipment. From the article: ‘The burst was so bright that at the time it was first recorded it was dismissed as man-made radio interference. It put out a huge amount of power (10^33 Joules), equivalent to a large (2000MW) power station running for two billion billion years.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3705f38',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterStar in Easy Rider
Share this...FacebookTwitter "
"

Representatives of some 160 nations are gathered in Kyoto, Japan, this week to negotiate an international treaty to control emissions of greenhouse gases. While the summit has all the trappings of a substantive event, the gathering in Kyoto is more an exercise in public relations than in serious statecraft. Hot air, rather than cold reason, will dominate the conference and political sleight of hand will be its product. 



That’s because, according to the International Panel on Climate Change (the United Nation’s body of experts devoted to the study of global warming and known as the IPCC), even the most aggressive and costly proposals on the table this week would shave only a fraction of a degree off temperature increases projected by computer models for the year 2100. To achieve those emissions reductions, each American would have to pay an additional $1,000-$3,000 annually in higher energy prices. Such proposals would, according to Yale economist and sometime Clinton adviser William Nordhaus, take us back to the days of semipermanent energy crises like those of the 1970s. Both greenhouse alarmists and skeptics agree that actually preventing the global warming projected by computer models would require the world to reduce carbon dioxide emissions by between 60 percent and 80 percent. Only by virtually abandoning the use of oil, gas and coal could we achieve such reductions. President Clinton’s assurances about “free lunch” climate change policies notwithstanding, nobody is proposing any real policy to prevent climate change because no one wants to usher in a permanent global depression. 



The idea, then, is to get the world to commit to a slow‐​motion control policy, one that would ease us into higher energy costs, a reordered industrial world and, as National Public Radio reporter Richard Harris puts it, “a whole new society” structured around less energy use. 



But if the computer models are correct about global climate change (the data thus far are inconclusive), what choice do we have? Isn’t it prudent to hedge our bets now with a control strategy in order to avoid far more costly economic crash planning later? Well, no. All indications are that the “cure” for global warming is far worse than the “disease” of rising temperatures. 



First, only about 2 percent of America’s economy is sensitive to weather conditions. No matter how ruinous climate change might be, it couldn’t possibly have a serious long‐​term impact on the United States. Even the most alarmist projections of ocean rise (about 3 feet or so) are trivial. If Amsterdam could figure out a way to hold back an even larger sea rise hundreds of years ago, it’s clear a wealthier and more technologically advanced United States could counter a 3‐​foot rise. Foreign aid to help poorer countries adopt would be far less expensive than control policies. 



Second, it’s not altogether clear that a warmer world would be a less habitable world. A temperature rise of 4.5 degrees Fahrenheit (the median computer‐​predicted result of a doubling of atmospheric carbon dioxide in the next 100 years) was exactly what occurred about a thousand years ago (A.D. 850‑1300) in a period climatologists refer to as “the little climate optimum” (note that they don’t refer to it as “the little climate hell”). The result? A longer growing season, rapid economic development, a minor cultural renaissance, an expansion of fertile crop and forestland and a decrease in mortality rates.



Since the data indicate that the small amount of warming we have detected over the last 100 years has largely been confined to winter evenings in the far northern latitudes, we have every reason — both empirical and theoretical — to believe warming would be a benign, not a deleterious, event. 



There are still open questions about how much if anything man has had to do with the slight warming detected over the past 100 years and how much warming might eventually occur (the IPCC estimates range from insignificant to moderately significant). The IPCC report itself states it will be another decade or so before scientists will know for certain. So why not wait? Nature magazine reported last year that waiting 20 years for better scientific information before acting will only cost us .36 degree Fahrenheit, at worst, over the next 100 years. 



In the face of this kind of uncertainty, the best “insurance policy” we could buy is one that increases the amount of wealth at society’s disposal to handle whatever problems might occur in the decades to come. Impoverishing society today to avoid a very uncertain problem tomorrow would harm, not help, future generations.
"
"
Today I obtained the paper: LaDochy, S., R. Medina, and W. Patzert. 2007. Recent California climate variability: spatial and temporal patterns in temperature trends. Climate Research, 33, 159-169 You can download the paper in PDF format in its entirety here: ca_climate_variability_ladochy.pdf
I’ll post more on this paper later, but I wanted to make it available for everyone to read beforehand.
This paper references my good friend and colleague, Jim Goodridge, former California State Climatologist in its bibliography. As you may recall, I posted on Jim’s work here a couple of months ago. One of the maps that Jim has prepared, seen below, closely matches the mapped results from the LaDochy et al paper.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea24130c6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSomething is rotten with the GHE
By Erich Schaffer
Introduction
The greenhouse effect (GHE) is a well established theory which most people consider a solid fact, even those who are otherwise “critical” over global warming. On the other side there are some voices who “deny” the GHE with flatearther-like arguments, which seemingly only adds to the credibility of the theory. This is a very odd situation, since the are huge issues with the GHE hidden in plain sight.
“Without GHGs, the Earth would be a frozen planet with a temperature of only -18°C, or 255°K”. This definition is all too familiar to us all and the experts naming it are legion. The 255°K isthe result of a (relatively) simple formula.
(342 x ((1-0.3) / 1)  / 5.67e-8) ^0.25 = 255
342W/m2 is the amount of solar radiation (the exact number may vary), 5.67e-8 is the Stefan-Boltzmann constant and ^0.25 (or the 4th root) represents the Stefan-Boltzmann law according to which radiation is a function of temperature to the power of 4.
Black body assumption trouble
The interesting part however is (1-0.3 / 1). 0.3 is the albedo of Earth and 1-0.3 os thus the absorbtivity, which is the share of solar radiation the Earth absorbs (~70%). The 1 below the comma, which is usually omitted, represents emissivity, which is the share of LWIR emitted by the Earth relative to a perfect black body of the same temperature. In other words, it is being assumed Earth would be emitting just like a perfect black body if it were not for GHGs. And that is where the trouble starts.
The basic problem
Quite obviously there are two factors that “violate” the assumption named above.

The surface of the Earth, mainly consisting of water, is not a perfect emitter, pretty much like any real surface. Although it is not the scope of this article, it can be shown there is a significant deviation from 1 (in the 0.91 to 0.94 range). One needs to look up Fresnel equations, the refractive index of water and so on to sort out this subject.
Clouds interfere massively with LWIR emissions. Actually this is common wisdom, as “clear nights are cold nights” and most people have made the according experience. Even the IPCC states clouds would block a 50W/m2 of SW radiation, retain a 30W/m2 of LWIR and thus have a net CRE (Cloud Radiative Effect) of -20W/m2 [1]. Of course those -50W/m2 of SW CRE are already included in the formula above (part of the 30% albedo), while the 30W/m2 of LW CRE are not.

For this reason we need to make some minor corrections to the GHE as presented above. Basically Earth receives some 240 W/m2 of solar radiation (= 0.7 x 342) and is meant to emit some 390 W/m2 at 288°K at the surface. Next to a temperature of 33°K, the GHE would thus amount to about 150W/m2 respectively (=390-240).
Since in reality the surface is not a perfect emitter, the 390W/m2 are totally inaccurate. In fact it is easy to call it “fake science” whenever someone claims this number, or an even higher one. Rather we need to reduce this figure by at least 20 W/m2 to allow for a realistic surface emissivity. Next we need to allow for the 30 W/m2 that clouds provide and thus our GHE shrinks from 150 W/m2 to a maximum of only 100 W/m2 (150 – 20 – 30).
For the sake of clarity we should rename the GHE to GHGE (greenhouse gas effect) as this is the pivotal question. How much do GHGs warm up the planet? It is important so see that GHGs were attributed with their specific role in a kind of “diagnosis of exclusion”. If it were not for GHGs, what would be the temperature of Earth? Any delta to the observed temperature can then be attributed to GHGs.
Such a “diagnosis of exclusion” is always prone to failure, be it in the medical field or anywhere else. Essentially a large number of variables need to be taken into account and the slightest mistake in the process, will necessarily cause a faulty outcome. For that reason it should be considered an approach of last resort, maybe helpful to treat a patient or solve a criminal case. As a starting point in physics it is a no-go, and as we can see, it delivers wrong results. But maybe that is the reason why it was chosen in the first place. Faulty approaches give a certain freedom of creativity.
GHGE being notoriously exaggerated
Still, we have not broken any barriers so far. Yes, the GHGE is notoriously being exaggerated and anyone who claims Earth would be 255°K cold if it were not for GHGs, is either incompetent, or simply lying. You cannot excuse such a claim as “simplification”, since exaggerating the GHGE by some 50% at least is certainly beyond negligible.
On the other side, this does not deny the global warming narrative at all. One might consider downgrading climate sensitivity a bit, which would only result in climate models better matching reality. Even then, this will only put things on a healthier and more appropriate basis, eventually supporting the theory of CO2 induced global warming.
Digging deeper
So far I have not introduced anything substantially new, but only pointed out to what is known and yet constantly forgotten. Especially the CRE in its quoted magnitude is pretty much an undisputed fact of science. Although I do not know exactly what the origins of these estimates were, experts like Veerabhadran Ramanathan already zeroed in on it in the 1970s. Satellite driven projects like ERBE or CERES later confirmed and specified those estimates.
The net CRE of -20W/m2 thus can be found in the IPCC reports, NASA gives detailed satellite data on it, and even “sceptics” like Richard Lindzen name and endorse it[2]. Such a solid agreement is not just good for my argumentation above, it is also great for the GHGE itself. In fact the negative CRE is pretty much a conditio sine qua non. If clouds were not cooling the planet, the scope for GHGs might become marginal.
There are indeed some issues with the CRE I need to talk about and things are not nearly as settled as I just suggested.

Whatever experts name a net CRE of about -20 W/m2, they refer to the same sources, which are ERBE and CERES satellite data.
This are not satellite data at all, but models which are getting fed with some satellite data, among others.
These models were largely developed by the same people who predicted the negative CRE in the first place. They might not even have a (significant) GHGE if the result would not turn out how it did.
A closer look on these model results show totally inconsistent outcomes over time. Regions with massively negative local CREs turned into having positive CREs, and vice verse.[3]
The only thing which really held constant over time was the overall negative CRE of the named magnitude. Of course, that is a precondition to the GHGE and cannot be put into question, if “climate science” wants to have an agenda.

There is yet another side to it. Obviously the net CRE is the sum SW and LW CREs, which can easily be formulated as CREsw + CRElw = CREnet. Since the CRElw is what is being forgotten so notoriously (as it diminishes the GHGE), we could assume there might be a motivated tendency to minimize the CRElw. Given the logical restrictions, this can be achieved by making the CREnet as negative, and the CREsw as small as possible. In other words, there is a trinity of issues with the CRE.

The -50 W/m2 of SW CRE. This figure is pretty low as compared conventional wisdom, according to which clouds make up for about 2/3s of the albedo, or almost -70W/m2.
The net CRE of some -20 W/m2. We are going to have a look into this hereafter.
The LW CRE of +30 W/m2 which is reducing the GHGE as shown above, but for some strange reason tends to be “forgotten”.

Putting things to the test
Since the net negative CRE is “confirmed” by nothing but models of dubious nature, since logic might suggest the opposite (to cut a long story short) and the whole GHGE theory totally depends on it, this question made a perfectly legit target for fact checking. It is the one pivotal question it all boils down to. Is the CRE negative indeed and how could we possibly put it to the test?
As on my previous works in forensic science it seemed mandatory to pass by any conventional approach subject to predictable restrictions. Rather you will have to go beyond the understanding of those who might conspire so that their possible defences turn futile. And of course this would require brute force of intellect, creativity and a bit of luck to find an appropriate leverage.
At least the latter turns out to be a friendly gift by the NOAA. Under the title “QCLCD ASCII Files” the NOAA provided all METAR data from US weather stations[4]. Regrettably they pulled these valuable data from their site soon after I downloaded it, and the alternative “Global-Hourly Files” is not quite working[5].
The METAR data, as far I understand, are taken at airports and contain, next to usual meteorologic data, cloud conditions originally meant to assist aircraft operating around these airports. The data are anything but perfect for our scope and are subject to a couple of restrictions. As a rule, cloud condition is only reported up to 12,000 ft, yet individual exceptions may occur. Then this cloud condition is reported in 5 different “flavours”, which are CLR, FEW, SCT, BKN and OVC, or combinations of which. For our purpose any combination will be reduced to the maximum cloud condition.
Even if this is not an ideal data pool, it meets a lot of necessary requirements. First it is a totally independent data source, which has never been meant to be used for climate research. Second these data have been collected by many people, who may have made individual mistakes in the process, but were certainly not systemically biased. Third these data are thus “democratic” in nature, not controlled by the bottle neck of a few experts. Eventually, and that is the most important point, we need no models here, but we can look straight onto the empiric evidence.
The Result


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




First I need to tell how such basic research gives you amazing insights otherwise not available anywhere.
You will not get to see what you like to, or expect to see, but what there is. Just like Christopher Columbus searching for India and finding America, you will have to take things for face value. Analyzing the data back and forth, using different perspectives, this was not a simple look up to confirm a certain expectation, but rather a process of continuous learning. Accordingly there are lots of results giving excellent insights into the nature of clouds, or their impacts on climate respectively.

This graph was taken from Harvard’s educational site [6] on the subject. Here, like in later iterations of the ERBE / CERES modelling, the northern Pacific is meant to be one of the areas with a massively negative CRE, which are of special interest to me.
Since the Aleutian islands are US territory, my NOAA data set included 10 stations located right there.

For the years 2016 and 2017, these stations report about 325,000 valid datasets, with almost 60% of which being overcast. So it is indeed a very cloudy region.

Once we resolve the cloudiness/temperature correlation by season, we find a very typical outcome. OVC skies are correlated with lower temperatures in spring and early summer, and with higher temperatures throughout the rest of the year. This pattern has been seen in all subsets featuring distinct seasons and is due to surface temperatures lagging behind solar intensity.

It is an analogy to the day/night cycle, where clouds hold down day time temperatures, while keeping nights relatively warm. It is about the relation of incoming SW to outgoing LW radiation. As clouds interfere with both radiative fluxes, their primary effect will be relative to which of these fluxes is stronger. In spring surface temperatures lag behind solar intensity, LW emissions will be relatively weak and thus clouds are cooling. In autumn this relation naturally reverses and then clouds are warming.
Note: These “tidal effects” are a direct representation of the LW CRE. Although it goes way beyond the scope of this article, such data are very helpful in assessing the actual magnitude of the LW CRE.
A huge surprise
Finally, if we add up the above results and look at the annual average (thus seasonally adjusted), we are in for a huge surprise (or possibly no more at this point). The correlation between clouds and temperature is strictly positive. The more clouds, the warmer it is, and that is in a region where models suggest a massively negative CRE.
Obviously something is totally wrong here.

I am confident the METAR data are correct, and I am certain my analysis is correct, since I have gone over it many times and the outcome is consistent with all the different perspectives. Instead, the ERBE/CERES models are wrong when compared to empiric evidence a.k.a. “reality”. That is not much of a surprise given a track record of inconsistent results.
And as much as the Bering Sea looks like “a perfect match” to fact check these models, the problems go far beyond the region. No matter where ever I looked, a negative CRE could not be found.
Just the tip of the ice berg
Yet this is just the tip of the iceberg. Of course you need to check for biases to see how much a correlation also means causation, and there are a few. Humidity, as much as it may serve as an indicator for the assumed GHG vapour, is indeed correlated with cloudiness (78% rel. humidity with CLR, 85% with OVC), but this delta is a) influenced by rain and b) too small to explain what we see.
More importantly, this analysis is all about low clouds up to 12.000 ft and it is undisputed the net CRE turns more positive the higher up clouds are. Then there is the subject of rain chill, which makes clouds look statistically colder than they are. Finally temperatures are sluggish relative to ever changing cloud conditions and we would certainly see a larger delta in temperatures if respective cloud conditions were permanent.
Systemic GHE failure
Unlike what I named before, this is not just a scratch on the GHE theory, but systemic failure. If clouds warm the planet indeed, and all the evidence points this way, the very foundation of the theory is getting annihilated. Not that GHGs might not play a certain role in Earth’s climate, but the size of the GHGE will be only a fraction of 33°K, and one that is yet to be precisely determined.
[1] 5th AR of the IPCC, page 580
[2] https://wattsupwiththat.com/2020/06/29/weekly-climate-and-energy-news-roundup-414/
[3] https://www.researchgate.net/figure/Comparison-of-annual-mean-SW-LW-and-net-CRE-of-E55H20-E61H22-and-E63H23-to-CERES-40_fig2_335351575
[4] https://www.ncdc.noaa.gov/data-access/quick-links
[5] Documentation does not fit the data format, for some reason I am unable to locate temperature readings, the format itself is hard to read, and finally for some reason these data, station by station, do not correspond to those of the “QCLCD ASCII Files”
[6] https://www.seas.harvard.edu/climate/eli/research/equable/ccf.html


		jQuery(document).ready(function(){
			jQuery('#dd_c8e1fc71e470391995075f099bc5906a').on('change', function() {
			  jQuery('#amount_c8e1fc71e470391995075f099bc5906a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA new temperature reconstruction indicates today’s sea surface temperatures are colder than all but a few millennia out of the last 156,000 years.
A Southern Ocean site analyzed in a new study (Ghadi et al., 2020) has averaged 1-2°C during glacials and 4°C during interglacials. Today, with a 410 ppm CO2 concentration, this location has again plummeted to glacial/ice age levels (2°C).
The site was 2°C warmer than now when CO2 concentrations were 180 ppm about 20,000 years ago, or during the peak of the last ice age. During the Early Holocene (10,000 to 8,000 years ago), summer sea surface temperatures were also 2°C warmer than today.
There is no indication that CO2 concentration changes are in any way correlated with temperature changes throughout this entire 156,000-year epoch.

Image Source: Ghadi et al., 2020
Share this...FacebookTwitter "
"

The additional power that is being granted to experts under the Obama administration is indeed striking. The administration has appointed “czars” to bring expertise to bear outside of the traditional cabinet positions. Congress has enacted sweeping legislation in health care and finance, and Democratic leaders have equally ambitious agendas that envision placing greater trust in experts to manage energy and the environment, education and human capital, and transportation and communications infrastructure.



However, equally striking is the failure of such experts. They failed to prevent the financial crisis, they failed to stimulate the economy to create jobs, they have failed in Massachusetts to hold down the cost of health care, and sometimes they have failed to prevent terrorist attacks that instead had to be thwarted by ordinary civilians.



Ironically, whenever government experts fail, their instinctive reaction is to ask for more power and more resources. Instead, we need to step back and recognize that what we are seeing is not the vindication of Keynes, but the vindication of Hayek. That is, decentralized knowledge is becoming increasingly important, and that in turn makes centralized power increasingly anomalous.



 **THE AGE OF THE EXPERT**



Populists often make the mistake of bashing experts, claiming that the “common man” has just as much knowledge as the trained specialist. However, trained professionals really do have superior knowledge in their areas of expertise, and it is dangerous to pretend otherwise.



I have faith in experts. Every time I go to the store, I am showing faith in the experts who design, manufacture, and ship products.



Every time I use the services of an accountant, an attorney, or a dentist, I am showing faith in their expertise. Every time I donate to a charity, I am showing faith in the expertise of the organization to use my contributions effectively.



In fact, I would say that our dependence on experts has never been greater. It might seem romantic to live without experts and instead to rely solely on your own instinct and know‐​how, but such a life would be primitive.



Expertise becomes problematic when it is linked to power. First, it creates a problem for democratic governance. The elected officials who are accountable to voters lack the competence to make well‐​informed decisions. And, the experts to whom legislators cede authority are unelected. The citizens who are affected by the decisions of these experts have no input into their selection, evaluation, or removal.



A second problem with linking expertise to power is that it diminishes the diversity and competitive pressure faced by the experts.



A key difference between experts in the private sector and experts in the government sector is that the latter have monopoly power, ultimately backed by force. The power of government experts is concentrated and unchecked (or at best checked very poorly), whereas the power of experts in the private sector is constrained by competition and checked by choice. Private organizations have to satisfy the needs of their constituents in order to survive. Ultimately, private experts have to respect the dignity of the individual, because the individual has the freedom to ignore the expert.



These problems with linking expertise with power can be illustrated by specific issues. In each case, elected officials want results. They turn to experts who promise results. The experts cannot deliver. So the experts must ask for more power.



 **JOB CREATION**



With the unemployment rate close to 10 percent, there is a cry for the government to “create jobs.” But the issue of job creation illustrates the increasingly decentralized nature of the necessary knowledge.



A job is created when the skills of a worker match the needs of an employer. I like to illustrate this idea using an imaginary game in which you draw from two decks of cards, one of which contains workers and one of which contains occupations. For example, suppose that you drew “Arnold Kling” from the deck of workers and you drew “fisherman” from the deck of occupations. That would not be a good match, because my productivity as a fisherman would be zero.



You could do worse — my marginal product as an oral surgeon would be negative. However, you could do better if you were to draw an occupation card that said “financial modeler” or “economics teacher.” One hundred years ago, if you had played this game, you had a good chance of finding a match just by picking randomly. Most jobs required manual labor, and for most people manual labor was the most productive use of their working hours.



Today’s work force is more highly educated and more differentiated. As a result, the task of creating jobs requires much more knowledge than it did in the past. A New Deal program like the Public Works Administration or the Civilian Conservation Corps would not have much appeal for a recent law school graduate or laid‐​off financial professional.



Production today is more roundabout than it was 50 years ago. Only a minority of the labor force is engaged in activities that directly create output. Instead, a typical worker today is producing what George Mason University economist Garett Jones calls “organizational capital.” This includes management information systems, internal training, marketing communications, risk management, and other functions that make businesses more effective.



When production was less roundabout, there was a tight relationship between output and employment. When a firm needed to produce more stuff, it hired more workers.



Today, additional demand can often be satisfied with little or no additional employment.



Conversely, the decision to hire depends on how management evaluates the potential gain from adding new capabilities against the risks of carrying additional costs. The looser relationship between output and employment is implicit in the phrase “jobless recovery.” So how does the economy create jobs? There is a sense in which nobody knows the answer. In his essay, “I, Pencil,” Leonard Read famously wrote that not a single person on the face of this earth knows how to make a pencil. Pencils emerge from a complex, decentralized process. The same is true of jobs.



What the issue of job creation illustrates is the problem of treating government experts as responsible for a problem that cannot be solved by a single person or a single organization.



Economic activity consists of patterns of trade and specialization. The creation of these patterns is a process too complex and subtle for government experts to be able to manage.



The issue also illustrates the way hubris drives out true expertise. The vast majority of economists would say that we have very little idea how much employment is created by additional government spending. However, the economists who receive the most media attention and who obtain the most powerful positions in Washington are those who claim to have the most precise knowledge of “multipliers.”



 **HEALTH CARE**



Despite the many pages contained in the health care legislation that Congress enacted, the health care system that will result is for the most part to be determined. The design and implementation of health care reform was delegated to unelected bureaucrats, as was done in Massachusetts.



In Massachusetts, the promises of propo‐​nents have proven false, and the predictions of skeptics have been borne out. Costs have not been contained; they have shot up. Emergency room visits have not been curtailed; they have increased. The mandate to purchase health insurance has not removed the problem of adverse selection and moral hazard; instead, thousands of residents have chosen to obtain insurance when sick and drop it when healthy. The officials responsible for administering the Massachusetts health care system are no longer talking about sophisticated ways of making health care more efficient.



Instead, they are turning to the crude tactic of imposing price controls.



Once again, we have legislators putting unrealistic demands on experts. This results in the selection of experts with the greatest hubris, shutting out experts who appreciate the difficulty of the problem. When the selected experts find that their plans go awry, they take out their frustrations by resorting to more authoritarian methods of control.



 **THE SECURITY APPARATUS**



In July 2010, the Washington Post ran a series of stories on the size and complexity of the national security apparatus that has developed in response to the terrorist attacks of September 11, 2001. Yet with all this manpower and budget, we still have incidents like the Christmas bomber, a would‐​be terrorist who was stopped by citizens.



There are an infinite number of potential terrorist threats. In response, one could devise an infinite number of agencies and policies. There is little or no scope for anyone to question the relationship between costs and benefits.



More than 10 years ago, scientist and author David Brin wrote The Transparent Society, a book that anticipated the problems of surveillance and terrorism in the context of technological advance. Brin advocated making surveillance tools accessible to ordinary citizens. As counterintuitive and potentially disturbing as this sounds, Brin argued that it is better than the alternative, which is giving surveillance tools to government experts only. The latter approach threatens liberty without providing security. Unfortunately, that is the approach that the United States government has adopted, and it has grown out of control.



 **ENERGY AND ENVIRONMENT**



The Department of Energy has decided that it has the expertise to select specific energy projects, such as the electric car that is being developed by Fisker Automotive of California, the recipient of a $500 million loan guarantee. In theory, if the economic prospects for this electric car were good enough, venture capitalists would be willing to risk money on its development. Now, with a loan guarantee, private investors enjoy only the potential gains while taxpayers bear the risk. Many citizens who would never have considered investing in this electric car company are now partners in the venture, except that we have only the downside and no upside.



The officials who are putting taxpayer money at risk may or may not have better expertise than venture capitalists who put their partners’ money at risk. What the officials certainly have is more power.



The threat of climate change, like the threat of terrorism, can be characterized in such a way as to justify an unlimited attempt at expert control. Regardless of whether experts really can accurately measure, predict, and explain climate change, some will be tempted to exercise power as if their analysis were precise and certain.



 **FINANCIAL REGULATION**



The financial crisis spawned demands for new regulatory powers. However, the crisis itself clearly resulted from the misuse of regulatory power in the first place. It was government policy that attempted to promote home “ownership” by encouraging lending with little or no money down to speculators and inexperienced borrowers. It was government capital regulations that steered banks toward AAA‐​rated securities, with no need to investigate the true underlying risks. It was the view of leading regulators at the Federal Reserve and the International Monetary Fund in 2005 and 2006 that the financial system had become adept at managing and distributing risk. The regulators were not powerless to stop the risky behavior; instead, they were convinced that they had everything under control.



If the regulatory experts could not prevent the financial crisis of 2008, the most reasonable inference to make is that financial crises cannot be prevented. There is no such thing as a financial system that is “too regulated to fail.” The recent Dodd‐​Frank legislation gives broad new discretionary powers to regulators.



Many of the important rules, such as bank capital regulations, are left up to the experts. The decision to use new authority to break up or take over risky financial institutions is discretionary.



Unfortunately, the resolution of troubled financial institutions requires rules rather than discretion. With discretion, there is a problem of time inconsistency. No matter how loudly the regulators proclaim that they will not bail out failing institutions, history shows that when a crisis comes the officials in charge would rather do a bailout than face the uncertainty associated with shutting an institution down. Large failing banks will only be closed if there are strict rules in place that tie the regulators’ hands to make bailouts impossible.



Discretionary resolution authority is authority that will never be used. Banks and their counterparties know this, and they will behave accordingly.



 **THE KNOWLEDGE-POWER DISCREPANCY**



As Hayek pointed out, knowledge that is important in the economy is dispersed. Consumers understand their own wants and business managers understand their technological opportunities and constraints to a greater degree than they can articulate and to a far greater degree than experts can understand and absorb.



When knowledge is dispersed but power is concentrated, I call this the knowledgepower discrepancy. Such discrepancies can arise in large firms, where CEOs can fail to appreciate the significance of what is known by some of their subordinates. I would view the mistakes made by AIG, BP, Freddie Mac, Fannie Mae, and other well‐​known companies as illustrations of this knowledge‐​power discrepancy in practice.



With government experts, the knowledge‐ power discrepancy is particularly acute.



As we have seen, the expectations placed on government experts tend to be unrealistically high. This selects for experts with unusual hubris. The authority of the state gives government experts a dangerous level of power.



And the absence of market discipline gives any errors that these experts make an opportunity to accumulate and compound almost without limit.



In recent decades, this knowledge‐​power discrepancy has gotten worse. Knowledge has grown more dispersed, while government power has become more concentrated.



The economy today is much more complex than it was just a few decades ago. There are many more types of goods and services.



Consumers who once were conceived as a mass market now have sorted into an everexpanding array of niches. In the 1960s, most households had one television, which was usually tuned to one of just three major networks. Today, some households have many televisions, with each family member watching a different channel. Some people still watch major networks, but many others instead focus on particular interests served by specialty cable channels. Still others watch very little TV at all.



This increased diversity of consumer tastes in a world of tremendous variety makes the problem of aggregating consumer preferences more difficult. It becomes harder for government experts to determine which policies are in consumers’ interests. For example, is a national broadband initiative going to give consumers access to something they have been denied or something that they do not want? The advances of science are leaving us with problems that are more complex. As fewer Americans die of heart ailments or cancer in their fifties and sixties, more of our health care spending goes to treat patients with multiple ailments in their eighties and nineties. Given the complexity of each individual case, it seems odd that health care reformers believe that government can effectively set quality standards for doctors.



In business, performance evaluation of professionals is undertaken by other professionals who are in the same work group, observing their workers directly, and who understand the context in which the professionals are working. Even then, performance evaluation and compensation‐​setting are challenging tasks. In health care, proponents of government “quality management” propose to evaluate the decision‐​making of professionals and adjust their compensation on the basis of long‐​distance reports. Taking into account the knowledge‐​power discrepancy, this notion of quality management from afar is utterly implausible.



Financial transactions have gotten extremely complex. Some critics blame the use of quantitative risk models and derivative securities.



However, removing these tools would not remove financial risk, and in many respects could make it more troublesome.



One consequence of modern finance is that it exacerbates the knowledge‐​power discrepancy.



It is as futile for financial regulators to try to track down all sources of risk as it is for security agencies to try to keep track of all possible terrorist threats.



How can we deal with the knowledgepower discrepancy in government? It would be great if we could solve the problem by increasing the knowledge of government experts. Unfortunately, all experts are fallible.



If anything, expert knowledge has become more difficult for any one individual to obtain and synthesize. Analysts of the scientific process have documented a large increase in collaborative work, including papers with multiple authors and patent filings by groups and organizations. Scientists tend to be older when they make their key discoveries than was the case in the first half of the 20th century.



When he was an executive at Sun Microsystems, Bill Joy said, “No matter who you are, the smartest people work for someone else.” Joy’s Law of Management applies to government at least as much as to business. There is no way to collect all forms of expertise in a single place.



Instead, the way to address the knowledge‐ power discrepancy is to reduce the concentration of power. We should try to resist the temptation to give power to government experts, and instead allow experts in business and nonprofit institutions to grope toward solutions to problems.



 **LIVING IN A COMPLEX WORLD**



To summarize: We live in an increasingly complex world. We depend on experts more than ever. Yet experts are prone to failure, and there are no perfect experts.



Given the complexity of the world, it is tempting to combine expertise with power, by having government delegate power to experts. However, concentration of power makes our society more brittle, because the mistakes made by government experts propagate widely and are difficult to correct.



It is unlikely that we will be able to greatly improve the quality of government experts.



Instead, if we wish to reduce the knowledgepower discrepancy, we need to be willing to allow private‐​sector experts to grope toward solutions to problems, rather than place unwarranted faith in experts backed by the power of the state.
"
"

_Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Let us see if we can help _New York Times_ ’ global warming reporter Justin Gillis out.   
  
In his article yesterday about the upcoming _Fifth Assessment Report_ of the U.N.’s Intergovernmental Panel on Climate Change (IPCC), Gillis laments that the IPCC seems to be tamping down some of the more alarmist scenarios when it comes to the projected rate of rise of global temperatures and sea level.   
  
Concerning projections of sea level rise, Gillis bemoans that the IPCC looks like (the final version of the Summary for Policymakers of the new report isn’t scheduled for release until the end of the this month at the conclusion of an IPCC editorial meeting in Stockholm) it will discount the “outlier” estimates that the rise this century will exceed five feet. Gillis writes “The drafters of the report went with the lower numbers, choosing to treat the outlier science as not very credible.”   
  
When it comes to how fast the global average temperature is projected to rise, Gillis rues the possibility that the IPCC will lower its assessed value of the climate sensitivity, writing “In this case, the drafters of the report lowered the bottom end in a range of temperatures for how much the earth could warm, treating the outlier science as credible.”



Gillis can’t wait for the explanation:   




…[I]t would be nice to hear an explanation from the drafters of this coming report as to why they made decisions that effectively play up the low-end possibilities. But with the report still officially under wraps, they are not speaking publicly. We are thus left wondering whether it is a matter of pure professional judgment — or whether they have been cowed by the attacks of recent years.   
  
Assuming these decisions withstand final review, it will be fascinating to hear the detailed explanations in Stockholm.



We’ll end the suspense for him.   
  
The reason that the IPCC should discount the possibility that the sea level will rise more than three feet by the year 2100 is that such a possibility has largely been discredited in the scientific literature and well as simply by looking out the window (i.e., the observed rate of sea level rise is only about 1.25 inches per decade and there is A LOT of ice in Greenland and Antarctica that is not going anywhere fast).   
  
We documented this is numerous places, notably here, here, here, and here.   
  
And the reason that the IPCC should lower its estimates of the earth’s climate sensitivity (i.e., how much the earth’s average temperature rises as a result of a doubling of the atmospheric concentration of carbon dioxide) is that the overwhelming majority of the recent findings in the scientific literature show that the most likely value is far beneath what it was assessed at in previous IPCC reports. In fact, the new equilibrium climate sensitivity estimates are so low as to put the IPCC in a quandary—if they were to fully embrace the new findings, they would have to discredit all the future climate projections made in the new report as they were generated by climate models with and average climate sensitivity that is nearly 75 percent higher than the new findings suggest. So if anything, the IPCC will likely be too conservative in lowering its assessment of the climate sensitivity in the final version of its _Fifth Assessment Report_.   
  
Of course, we have explained all of this as well. See here, here, here, and here for starters.   
  
So, pure and simple, the reason that the IPCC should embrace estimates of a slower global temperature increase and a lower global sea level rise is because that is what the current science supports.   
  
Gillis could have saved himself a lot of wondering (and ink) had he only been reading these pages!   
  
  
  
  
  



"
"
Share this...FacebookTwitter19 dead and up to 400 injured, many seriously.These are the latest gruesome numbers from yesterday’s Duisburg Love Parade, crowd-control disaster. It’s a classic case of what can go wrong when warnings are ignored or played down. City officials were warned that the location was seriously inadequate, but nobody wanted to be a party-pooper.
Germany’s techno-music Love Parade first started in 1989. Most have been held in Berlin, until the city got tired of the chaos and filth they left behind, and the event had lost money anyway.
Yet, everybody hates to see a good party end, and so other places were sought to host the million-plus visitor event. This year’s choice proved to be a disaster.
The catastrophe occurred at a tunnel under a street that served as the main entrance to the event. The following video vividly shows the catastrophe in motion. Especially interesting is the 0:31 mark of the clip. there you see the fully packed ramp leading down to the tunnel.
LOVE PARADE DISASTER VIDEO
Just the shear physics of the situation alone are staggering.
If you estimate 100,000 people on that sloped ramp, each with an average weight of 70 kg, you have a total weight of 7000 tons trying to move forward. If the ramp has a slope of 3°, then sine 3° times 7000 tones yields a gravity force vector of 366 tons pressing down against the wall. Not a good place to be. Granted that’s just a real rough calculation, but it gives you an idea.
A sure sign that the old rail yard location, where the event took place, was not going to work was its relatively small size. It had an area of 2.2 million square feet, meaning that the place was going to be overly packed with just half a million people. Organisers expected 1 million, 1.4 million showed up. 1.5 sq ft per person. Experts say the area was suitable for a maximum of only 300,000.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Throw in youth, alcohol and drugs and you have all the ingredients for a crowd control disaster.
The police had warned city officials that the risks were too high and had advised against approving the event. But reports say no top city official wanted to be the party-pooper. The Love Parade got the green light.
Now, after the catastrophe, with the nation and continent in shock, officials are scrambling and the finger-pointing has begun. According to German newspaper Bild:
Duisburg mayor Adolf Sauerland defended the safety concept of the Love Parade against criticism.
In his view the reasons for the tragedy were not due to a poor safety concept, but rather very likely had more to do with individual weak points.
Tell that to the prosecuting attorneys Herr Burgermeister.
The State’s Attorney has announced that an investigation for negligent homicide will be conducted, and all city and official documents relating to permitting and organisation of the event have been confiscated by police.
The Love Parade organisation has announced today that there will not be any more Love Parades in the future.
UPDATE – USEFUL LINKS:
http://www.thelocal.de/national/20100726-28737.html
http://www.spiegel.de/international/germany/0,1518,708462,00.html
Share this...FacebookTwitter "
"

Advocates for robust American global leadership are having a bad decade. Donald Trump’s election was clearly a wake‐​up call to the foreign policy establishment in Washington. In contrast to decades of bipartisan consensus that the United States was the “indispensable nation,” Trump appears to be monumentally indifferent to America’s role in the world.



His tense relations with longstanding allies and his decisions to withdraw from the Trans‐​Pacific Partnership and the Paris climate treaty have moved critics like former national security adviser Susan Rice to argue that Trump is “undoing American leadership on the international stage.”



Fears about Trump, however, simply echo concerns voiced throughout the Obama administration. Critics point to Obama’s withdrawal from Iraq, his failure to intervene in the Syrian civil war, and his failure to check Russia over Crimea and Ukraine as evidence of unhealthy retrenchment resulting in “the desperation of our allies and the glee of our enemies.”





The United States, and the world, would be better off if America led less often and more thoughtfully.



The real issue, however, is not America’s failure to lead; it is the failure of American leadership. Since the end of the Cold War the United States has flexed its muscles repeatedly. The problem is that this has too often produced resentment, conflict and instability, precisely the opposite of what its proponents have promised. The fundamental reason for this failure is that American officials have too much faith in their power to dictate outcomes around the world, especially through the use of military force.



The past 15 years provide ample evidence of the perils of leadership. After the 9/11 attacks, the Bush administration launched a war on terrorism based on a strategy combining military intervention, regime change and nation building. The goal was to kill terrorists in the short run, destroy their organizations in the medium run, and over the long run to reshape the politics of nations to prevent terrorism from sprouting up in the first place.



The Obama administration mostly followed suit, scaling back in Iraq but pursuing regime change in Libya, surging in Afghanistan and expanding the drone war against terrorists in seven countries. Today the Trump administration has begun to escalate the fight against the Islamic State and al‐​Qaeda, empowering the Pentagon and the military to determine troop levels and make swifter battlefield decisions.



The problem in the Middle East hasn’t been the lack of leadership; the problem has been the failure to recognize that the American strategy has been a failure. Political leaders exaggerated the terrorist threat to the United States and then applied the wrong tools to the problem. Military intervention turned out to be great for getting rid of governments, but completely ineffectual at defeating terrorist organizations.



Since 2001 the number of terrorist groups and jihadist fighters has skyrocketed, al‐​Qaeda franchises continue to operate, and the invasion of Iraq inadvertently caused the chaos that helped the Islamic State take root. Everywhere the United States has intervened — whether by drone or by invasion — since 2001, in fact, is less stable and more violent today than it was before.



Nor has the nation‐​building game gone any better. The United States has spent billions of dollars on nation‐​building efforts in Iraq and Afghanistan rebuilding infrastructure, training the police and military troops, and providing internal defense against terrorists. The hard truth, however, is that neither country is a functioning democracy, neither is stable, and neither would last long without outside support.



Meanwhile, the failure of the war on terror has come with astronomical costs, both for the United States and for the Middle East. The United States has already spent trillions of dollars and seen 7,000 Americans killed in the fighting, while according to NGOs somewhere between 1.3 million and 2 million Iraqis, Afghans and Pakistanis have died. This doesn’t count those in Libya, Yemen, Syria or elsewhere whose deaths are a result of U.S. intervention and its consequences.



Sadly, despite this recent history, there is little sign that Washington is ready to recognize the limits of American leadership. Though the Trump administration may frustrate the foreign policy establishment on certain issues, it is clear that American reliance on military intervention in the Middle East is here to stay.



American leadership can indeed be a powerful influence for good, but the United States is neither all‐​powerful nor faultless. The United States, and the world, would be better off if America led less often and more thoughtfully.
"
"
Former Virginia State Climatologist Patrick J. Michaels wrote an op-ed about his paper with Ross McKitrick from Canada’s University of Guelph in an American Spectator column today about the surface temperature record. This paragraph really caught my eye: “Weather equipment is very high-maintenance. The standard temperature shelter is painted white. If the paint wears or discolors, the shelter absorbs more of the sun’s heat and the thermometer inside will read artificially high. But keeping temperature stations well painted probably isn’t the highest priority in a poor country.”
The Stevenson Screen experiment that I had setup this summer is living proof of this.
Compare the photo of the whitewash paint screen on 7/13/07 when it was new with one taken today on 12/27/07. No wonder the NWS dumped whitewash as the spec in the 70’s in favor of latex paint. Notice that the Latex painted shelter still looks good today while the Whitewashed shelter is already deteriorating.

Click image for larger view

Click image for larger view

Whitewashed Screen on 7/13/07

Whitewashed Screen on 12/27/07
The whitewash coating I used was from a formula and method provided to me by a chemist at the US Lime Corporation, who is an expert on whitewash. He said the formula was true to historical records of the time when whitewash was used on the shelters. I was amazed to find that after just a few short months, my whitewash coating had lost about 40-50% of it’s surface area. Perhaps there was a mistake in the formula, or perhaps whitewash really is this bad at withstanding weathering.
In any event the statement of Patrick Michaels “Weather equipment is very high-maintenance. The standard temperature shelter is painted white. If the paint wears or discolors, the shelter absorbs more of the sun’s heat and the thermometer inside will read artificially high.” seems like a realistic statement in light of the photos above. The magnitude of the effect in the surface temperature record has yet to be determined, but it seems clear that shelter maintenance, or lack thereof, is a significant micro-site bias factor that has not been adequately investigated nor accounted for in the historical temperature record.
I’ll have more on this experiment soon including temperature time series graphs showing the difference between bare wood, latex painted, and whitewashed shelters.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1bd3f8a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Prof. Fritz Vahrenholt
The global mean temperature of the satellite-based measurements remained almost unchanged in August compared to July. The deviation from the 30-year average (1981 to 2010) was 0.43 degrees Celsius.

Temperature measurements on land and in the sea continue to decrease, as the graph of the JRC analysis shows, especially in the southern hemisphere (blue).

Approaching La Nina
The research institutes predict with high probability a La Nina in the Pacific Ocean next winter. Therefore, a further decrease in global temperatures is expected until next spring. The following diagram shows the incipient cooling effect in the Pacific.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterClimate journalism is like being at a third-world bazar where the media behave like merchants all shouting, pitching their catastrophe stories.Die Welt’s recent piece From Proud Jordan River, To A Smelly Trickle (roughly translated) features the crisis of water consumption and the injustice of water’s uneven distribution. Now water needs to be redistributed, along with wealth and misery.
Although the article is mainly a rant against Israeli water policy, its other objective is to admonish western societies for their profligate use of water.
This is a theme that’s steadily gaining traction on the environmental front here in Europe – along with biodiversity, ocean acidification, manmade microscopic aerosols and climate change. It’s the latest hot-seller catastrophe joining the enviro-bazar.
Die Welt doesn’t hold back citing environmental and activist groups for its reliable, “unbiased” and shocking information. At first the story focusses on Israeli water management and how it’s unfair to neighboring countries.
Die Welt writes:
According to Amnesty International, the average Israeli consumes 300 liters of water daily, while a Palestinian consumes only 70 liters. In poor regions a mere 20 liters is available daily for each person.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the end of the story, Die Welt admonishes western lifestyles and its excessive use of water.
The story concludes with a photo gallery that informs readers how much water consumption is needed to manufacture some basic daily products we enjoy in our daily lives. Examples:
1 hamburger: 2400 litres
1 hardboiled egg for breakfast: 135 litres
1 slice of bread: 40 litres
10 grams of cheese: 50 litres
1 cup of coffee: 140 liters
1 German breakfast: 365 liters
200 grams of potato chips: 185 liters
2-gram computer chip: 32 liters
1 sheet of paper: 10 liters
1 cotton T-shirt: 4100 liters!
1 pair of cowhide shoes: 8000 liters
1 new car: 450,000 liters
The idea is to tell us consumers that we are simply consuming too much water and that it’s having catastrophic impacts on the environment and poor people. It’s unfair and it has to be regulated. We need to feel guilty about it.
Wikipedia lists the potential manifestations of excessive water consumption:
There are several principal manifestations of the water crisis.
– Inadequate access to water for sanitation and waste disposal for 2.5 billion people
– Groundwater over drafting (excessive use) leading to diminished agricultural yields
– Overuse and pollution of water resources harming biodiversity
– Regional conflicts over scarce water resources sometimes resulting in warfare.
Expect the water crisis to get worse (not in reality, but in the media and political world). Get ready to hear a lot more about this in the future. Water-saving devices will be joining energy-saving devices soon in the government’s force-the-people-to-buy-list.
Share this...FacebookTwitter "
"

Florida State University’s COAPS (Center for Ocean-Atmospheric Prediction Studies) says that hurricane season 2007, which ends November 30th, is looking well below normal, in fact they are calling it “historic inactivity”.
According to COAPS: “Unless a dramatic and perhaps historical flurry of activity occurs in the next 11 weeks (ACE is based on calendar year, not traditional June-November hurricane season) , 2007 will rank as a historically inactive Tropical Cyclone year for the entire Northern Hemisphere. During the past 30 years, only 1977, 1981, and 1983 have had less activity to date (Jan-December). For the period of June 1 – October 19, 2007, only 1977 experienced LESS tropical cyclone activity.”
ACE Departure from Climatology thru October 24th, 2007 
Northern Hemisphere  -31% **** 316 (458) (Historic inactivity, 16% of season to go)
North Atlantic  -28% **** 63 (87) (Bill Gray wants 4 more (huh?, Season 91% over)
Eastern Pacific  -59% **** 52.2 (128) (Kiko helping out a little, Season 95% over)
Western Pacific  -25% **** 179 (237) (Still 21% of yearly activity to go)
PDI Departure from Climatology thru October 24th 2007
(PDI = Power Dissipation Index)
Northern Hemisphere  -24% **** 29687 (39101)
North Atlantic  -8% **** 6533 (7095) Effects of the Category 5’s
Eastern Pacific  -63% **** 3875 (10510) Includes Kiko
Western Pacific  -18.3% **** 17189 (21037) Includes Kajiki
Here are the named storms so far and their PDI:
Andrea 2.3 (Subtropical)
Barry 3.4
Chantal 2.5
Dean 386
Erin 1.3 (weak weak weak)
Felix 215
Gabrielle 4.0
Humberto 8.2
Ingrid 2.8
Jerry 2.4
Karen 17.2
Lorenzo 6.7
Melissa 1.9
There are some caveats:
Climatology based upon ACE (Bell et al. 2000) from 1970-2006 for each basin. ACE is not a perfect metric and does not account for storm size. Northern Hemisphere includes Northern Indian Ocean after 1976, which accounts for less than 3% of the yearly total. Data quality is a tremendous issue. The NHC declared extratropical observations were not included, which can account for up to 20% a year in additional ACE. The JTWC only started keeping track of EX phases in 2004, so there are literally 1,000 observations since the 1950s that are likely extratropical in the database (as phished out from the JMA database).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea324212a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterLike in many countries in Europe, politcal parties in Germany, whether right or left, are big boosters of re-engineering society in order to save it from the fantasized self-inflicted climate catastrophe. People who speak up face risk feeling the wrath of the many climate-doctrine-following drones and zombies. And as the level of absurdity reaches intolerable levels, people are indeed speaking  up.
One such person is mayor Hans-Martin Moll of the town of Zell am Harmersbach in Germany. He has written a letter addressed to Tanya Gönner, Minister of Environment in the state of Baden Wuerttemberg and a member of the conservative CDU party.  The European Institute For Climate and Energy (EIKE) features Moll’s letter here in German.
Mr Moll, who is also a CDU member,  has become very concerned about the CDU’s aimless drift, led by Angela Merkel, in the direction of “green illusions” over the last years. Chancellor Merkel is advised by alarmists like Hans Joachim Schellnhuber and Stefan Rahmstorf.
What takes the cake for Mr Moll is Tanya Gönner’s declaration that Germany’s EEG Act is a complete success, and that it ought to be continued. The EEG Act forces power companies to buy renewable energy from anyone who produces it at fixed, exhorbitant prices that are guaranteed for years. (More info on the EEG Act here). Moll writes:
Producing power with coal or nuclear reactors costs between 2.5 and 4 cents per kwh. The EEG forces the consumers and the economy to pay 43 cents per kwh for photovoltaic power, or about 15 times more than the reliable, steady supply, conventional power.
And to make this hugely subsidised power of any use, billions of euros more are needed for expanding the power grid, for adding necessary over-capacity, and for “imaginary storage technologies”, which are physcially and geographically completely illusionary.
 
You call this a success story? I call it a political swindle of the citizens. Only in a communist centrally planned economy has such a thing ever been done.
Consumers and the economy had to fork out already 12 billion euros in 2009 for a completely useless and ideological nonsense. This EEG Act which you call a success story will cost hundreds of billions of euros.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moll does not mince any words. Indeed the amount of CO2 that Germany may save by 2020 will be offset by Chinese economic growth within just a few months. It amounts to nothing. But it is a very expensive nothng.  CO2 reductions in Germany will have zero impact on the climate, assuming that added CO2 has a noticeable impact on climate. Moll writes:
With this kind of politics, the only thing that is sustainable is the harm done to the consumer, the economy and the jobs for our future generations.
This swindle must not only be reduced, it has to be eliminated completely. The same is true for wind energy.
Energy policy is going precisely in the Green parties’ direction. Their target is not the environment, rather it is the dismantling of industry.
Mr Moll concludes with:
I do hope the conservative CDU party will wake up soon, recognise this huge error, and that it will endeavour to pursue a real energy policy that is based on natural science and common sense.
I couldn’t agree more with Mr Moll. As people start speaking up, other people will start listening.
Share this...FacebookTwitter "
"
There’s a story at the Telegraph UK from Christopher Booker where he states “…the latest US satellite figures showing temperatures having fallen since 1998, declining in 2007 to a 1983 level…”
Wanting to make sure that there was some data to reference this claim for my readers, I’ve presented some graphs of satellite microwave sounder data below.
MSU data are produced by Remote Sensing Systems and sponsored by the NOAA Climate and Global Change Program. Data are available at www.remss.com 
Below are the trend graphs for the data since 1979. Note that these graphs are multi channel, which represent different microwave sounder wavelength channels from the spacecraft. These channels represent different measured levels of the atmosphere.
Channel. TLT – Lower Troposphere
 
Channel TMT – Middle Troposphere

Channel TTS – Troposphere and Stratosphere combined

Channel TLS – Lower Stratosphere

Above: Global, monthly time series of brightness temperature anomaly for channels TLT, TMT, TTS, and TLS.
All matter emits microwave radiation that varies with its temperature, among other factors. Microwave sensors on weather satellites can take more than 60,000 temperature measurements of oxygen in the atmosphere, from the surface to about 10 km (6 mi) altitude.
NOAA and it’s affiliated researchers have compiled almost three decades of data showing how atmospheric temperature has behaved over the entire globe.  At UAH (University of Alabama, Huntsville) where Dr. John Christy and Dr. Roy Spencer have been keeping watch on this trend for some time as well, they have tabular data online should you care to plot it. Here is an ongoing history of the data. You can see some of their other work here.
For Channel TLT (Lower Troposphere) and Channel TMT (Middle Troposphere), the anomaly time series is dominated by ENSO events and slow tropospheric warming. The three primary El Niños during the past 20 years are clearly evident as peaks in the time series occurring during 1982-83, 1987-88, and 1997-98, with the most recent one being the largest. Channel TLS (Lower Stratosphere) is dominated by stratospheric cooling, punctuated by dramatic warming events caused by the eruptions of El Chichón (1982) and Mt Pinatubo (1991). Channel TTS (Troposphere + Stratosphere combined) shows a mixture of both effects.
Temperatures in the lower troposphere (for non weather geeks, that is the portion of the atmosphere where we live) have shown a series of ups and downs since 1979, mostly in a ±0.4oC band, with negligible trends over that period. This contrasts with the near surface temperature record that shows a warming during the same period of time. The graph below is from Wikipedia.

Note in the TLT graph above, the strong 1997-98 El Niño event caused significant lower tropospheric warming in late 1997, and record warmth in February 1998 as evidenced by the spikes shown in the TLT, TMT, and TTS graphs above.
Satellite measurements of the lower stratosphere (TLS) reveal two marked warm periods (as much as 1.5oC warmer), caused by sulfuric acid aerosols deposited in this layer by the eruptions of El Chichón in 1982 and Pinatubo in 1991.
These two warm periods are concurrent with a strong cooling trend over the 19-year period that has been attributed to ozone depletion in the lower stratosphere. In 1997, record low stratospheric temperatures were recorded.
On the TLT graph, for the years 1998 to present, there appears to be a slight downward trend in lower stratospheric temperature, and this is what I believe Christopher Booker is referencing in his article in the Telegraph.  Note that there have been other downward trends in the nearly 30 year measurement history, but the overall trend in the TLT, TMT, and TTS channels has been positive, so a short downward trend doesn’t necessarily prove anything. The TLS channel shows a negative trend, and along with the ozone depletion factor, indicates that we aren’t getting much heat transport from the troposphere into the stratosphere.
The real question is whether this small downturn in the tropospheric temperature trend is a short term anomaly, or something indicative of a longer term event. Only time will tell.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2a961b0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Rounding out a review of California weather stations this week we visit Gilroy, CA, the garlic capital. This COOP station has an MMTS temperature sensor on a pole just a few feet from a concrete slab. We’ve seen a lot of that lately. But look closely – roasted garlic anyone?

Photo from NWS, San Franciso/Monterey CA
While it’s likely the BBQ grill is not used daily, one has to wonder just how much bias it’s proximity imparts into the temperature record. This station  COOP number is 04-3417 as is part of NOAA’s “A” network which reports climate to NCDC. It is located at the Fire station in Gilroy, seen below. Notice that is is also near a large parking lot and major intersection downtown. So much for NOAA’s 100 foot rule for station siting.


Click on the picture for a larger interactive view
The recently released paper from LaDochy et al. showed that “urban” stations warmed at a rate of 0.20°C per decade while the “non-urban” stations warmed only 0.08°C per decade, with the lack of attention to the measuring environment such as we see here, is it any wonder?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea26beeda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDr Oleg Pokrovsky has kindly taken the time to provide further information on his recent remarks, which have been widely quoted. We thank him for doing so. He writes as follows and includes a link to a ppt. presentation (see below):
Dear Colleagues,
Thank you for discussion of my conclusions presented at recent IPY conference
occurred in AARI (St.Petersburg, Russia).
My vision of future climate is based on comprehensive analysis of climate index series analysis, which permits to reveal fundamental quasi-periodical oscillations in most components of climate system:
-Solar activity
-SST of ocean (AMO and PDO)
-Surface air temperature
– Surface irradiance
-Precipitations
-Ice extent in Russian Arctic Seas
I found that that those are in strong coherence when inter-annual climate noise was removed in each of them
My motivation might be illustrated by a set of figures presented at recent Arctic Frontiers Conference (Tromso, Norway)
http://www.arctic-frontiers.com/index.php?option=com_docman&task=doc_download&gid=242&Itemid=155
Please keep your comments focused on the contents Dr Pokrovsky’s presentation.
Share this...FacebookTwitter "
"

Did Al Gore really deserve that Oscar for “An Inconvenient Truth”? The Left says yes — only the ideologically disabled or intellectually dishonest deny that the four horsemen of the environmental apocalypse (drought, disease, sea rise, and hurricanes) will soon devastate our fair planet. Reporter William Broad in the _New York Times_ today, however, says not so fast — a backlash is brewing among REAL scientists who are getting sick and tired of bed‐​wetting hysteria surrounding climate change.



The gist of their concern is this: while most (but not all) scientists are willing to accept that industrial emissions are an important driver in the planetary warming we’ve experienced since the late 1970s, they aren’t anywhere near so eager to embrace politically inspired warnings from non‐​scientists about how “the end is near.” Al Gore, according to many of the scientists interviewed by William Broad, is too shrill and too apocalyptic given the scientific evidence. 



Case in point: Al Gore warns in his documentary that sea levels will rise over 20 feet if warming continues. Yeah, well maybe in a thousand years or so if trends continue indefinitely, but the former Vice President leaves that little bit of perspective out of the movie. What might happen during our lives and the lives of our children and grandchildren? A sea rise of 23 inches, max, according to the new report just out from the Intergovernmental Panel on Climate Change. That’s hardly going to flood Manhattan, but acknowledging that would spoil the wonderful special effects visuals offered in the slideshow, now wouldn’t it? 



Gore’s scientific advisors, friends, and admirers defend the documentary and the book that followed by conceding that he may be a bit dodgy here and there, but that he gets the big picture right. That’s ridiculous. The fact that the planet is warming and that industrial emissions might well have something to do with it is not what this debate is ultimately about. This debate is whether we should or should not care. And if the former, how much should we be willing to sacrifice to do something about it? 



To say that Al Gore is to some extent out to lunch on the “should we care” argument but relatively sound on the question about whether we’re warming the planet (at least, if we measure these things by that most holy of metrics, the “scientific consensus” as defined by the IPCC) is akin to saying that the fellow proclaiming that a wrathful God is about to incinerate the planet is contributing to social welfare by usefully pointing out to the unbelievers that there is a God. That bit about God being particularly angry or plotting to destroy the world — Well, that’s a bunch of nonsense, but hey, he got the big picture right.



One of the scientists interviewed in the article — Roger Pielke, Jr. — wrote an essay recently for our own _Regulation_ magazine pointing out that science is inevitably corrupted when politicians decide to effectively delegate policymaking power to those who wear white frocks. So if you want to know why scientists aid and abet this kind of thing, go there. 
"
"
Calling cycle 24, calling cycle 24……where are you? 

Image from SOHO, inset added by the author
The SIDC in Belgium just issed an end to their “all quiet alert”
:Issued: 2008 Feb 26 1255 UTC
:Product: documentation at http://www.sidc.be/products/quieta
#——————————————————————–#
# From the SIDC (RWC-Belgium): “ALL QUIET” ALERT                     #
#——————————————————————–#
END OF ALL QUIET ALERT
………………….
The SIDC – RWC Belgium expects solar or geomagnetic activity to
increase. This may end quiet Space Weather conditions.
The first new sunspot in weeks has emerged today. The spot that has emerged is small and on the equator, so it appears that it is a cycle 23 spot rather than one from the cycle 24 that is gave one spot on January 8th, signaling a start of cycle 24, but has given no cycle 24 type spots since.
Based on what we know about the sun, a cycle 24 spot would be reverse polarity to cycle 23 spots and high latitude. The longer cycle 24 continues to delay producing its spots heightens the concern that we may be in for a longer inactive period on the sun, such as a Dalton type minimum.
A thought occurred to me. Given that all of the sunspots seen recently during our solar minimum are very small, I wonder if they could be resolved at all with the primitive equipment available during periods like the Maunder Minimum? Today we have satellites and advanced solar telescopes with hydrogen spectra filters that are available to amateurs, so catching any sunspot, even if small, is now easy. In fact this sunspot was was first noted by an amateur observer, Howard Eskildsen, in Ocala, FL, showing that amateurs still have a role in science.
It makes me wonder if an extended minimum really isn’t an absence of sunspots altogether, but just an absence of larger easily observable sunspots.  It is possible that primitive equipment of the period could not easily resolve smaller sunspots.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0fe56b2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe June SEARCH report of September sea ice outlook here shows the predictions of 16 research institutes for 2010:
According to the report:
 A quick calculation (leaving out the outliers of 3.2 million in 2009 and 1.0 million in 2010) shows an average prediction of 4.72 in June 2009, and 5.05 million this year.
On average their predictions for 2010 are 300,000 km² more than last year’s average prediction. This is probably due to the embarassment they had to endure last year, when ice melt was far less than all the institutes had predicted. Last year sea ice area bottomed out at 5.4 million km², see below:
As you can see from the above chart, they all fell short. This year they probably thought twice about making headline-grabbing claims like they did last year.
Share this...FacebookTwitter "
"

What explains the enormous differences in income per capita that exist across the world today? The question has been posed many times over. The gaps in prosperity that surround us in the modern age are much wider than those that motivated Adam Smith to write _The Wealth of Nations_ in 1776, which of course is where the modern discipline of economics began.



Whereas back then the gap between the richest and poorest nations was 4- or 5- fold, today it is over 40‐​fold. Why is it, then, that certain nations are distinguished from others in terms of wealth and poverty, health and sickness, food and famine? Theories abound. If you turn to the popular media — or even some respectable journals such as Science and Nature — you will most likely come across articles that argue that geographic factors are what explain these differences. Climate, soil quality, disease, and the environment have all been put forth as the determining elements of prosperity. Yet, when you look at the evidence, these geographic factors don’t seem to be all that important. The same countries that are very rich today were once poorer than others with the same soil quality, for instance.



An even more popular explanation is the importance of cultural factors. You will hear, for example, that it is the difference between Catholics and Protestants (as Max Weber argued), or perhaps between Christians and Muslims and Judeo‐​Christians that leads to economic differences. Others have focused on Asian versus non‐​Asian values, or differing social attitudes toward work. The significance of cultural factors is a popular explanation for the differences that exist between North America and the Iberian cultures of Latin America, as well.



Popular among academics and journalists is the notion that “enlightened leadership” is what matters — meaning that either leaders or their advisers have the right ideas about what drives prosperity. It’s no surprise that this has some appeal to economists, who, of course, are in the business of developing the best micro‐ and macroeconomic policies — ones believed to be so critical to a nation’s ultimate success.



However, once again, these all seem to have relatively little explanatory power. Remember that it was only four decades ago that many scholars were talking about the deleterious effects of Confucian values — the same cultural traits that are now touted as the foundation upon which Chinese growth has been built. And while economic policies that condemn nations to poverty abound, it will soon become clear that those policies are not adopted by mistake. They are adopted by design. It is not in the ignorance of leaders, in other words, that we should look for the causes of poverty. It is in their incentives. Let me explain.



 **INSTITUTIONS: INCLUSIVE VERSUS EXTRACTIVE**  
Our theory rests upon the nature of institutions — meaning the rules, both formal and informal, that govern our economic and political life. It should not come as any surprise that there are certain sets of economic institutions — property rights, enforcement of contracts, and so on — that create incentives for investment and innovation. Those institutions that create a level playing field through which a nation can best deploy its talents are referred to as “inclusive economic institutions.”



Inclusive economic institutions, however, are the exception rather than the rule. That holds true throughout history as well as around the world today. Instead, many nations today and in the past operate under extractive institutions, which do not create property rights, generate law and order, create secure contract environments, or reward innovation. They certainly do not create a level playing field, and therefore they do not encourage sustained economic growth.



As I have already mentioned, however, these extractive institutions do not develop by mistake. They are designed by the politically powerful to extract resources from the mass of society for the benefit of the few. Such institutions are in turn sustained by extractive political institutions, which concentrate power and opportunity in the hands of an elite. This elite, in essence, designs, maintains, and benefits from these extractive institutions.



So the question is: Why do these extractive institutions emerge and persist? This is where politics enters into the equation. When extractive political institutions concentrate power in the hands of the few, those groups that monopolize political power can maintain these institutions in spite of the fact that they fail to create incentives for economic growth. Let me offer an example.



 **CASE STUDY: SOUTH AMERICA**  
There is no better laboratory that demonstrates how extractive institutions emerge and persist than the New World. The Americas provide a brilliant example for understanding how different institutions form, how they become supported within different political frameworks, and how that, in turn, leads to huge economic divergences.



The economic and political institutions in the New World have been largely shaped by their colonization experience starting at the beginning of the 16th century. While the tales of Francisco Pizarro and Hernán Cortés are quite familiar, I’d like to start with Juan Díaz de Solís — a Spaniard who in 1516 initiated the colonization of the southern cone of South America, in what is today Argentina and Uruguay. Under de Solís’s leadership, three ships and a crew of 70 men founded the city of Buenos Aires, meaning “good airs.” Argentina and Uruguay have very fertile lands, with a climate that would later become the basis of nearly a century of very high income per capita because of the productivity of these areas.



The colonization of these areas itself, however, was a total failure — and the reason was that the Spaniards arrived with a given model of colonization. This model was to find gold and silver and, perhaps most importantly, to capture and enslave the Indians so that they could work for them. Unfortunately, from the colonists’ point of view, the native populations of the area, known as the Charrúas and the Querandí, consisted of small bands of mobile huntergatherers.



Their sparse population density made it difficult for the Spaniards to capture them. They also did not have an established hierarchy, which made it difficult to coerce them into working. Instead, the Indians fought back — capturing de Solís and clubbing him to death before he could make it into the history books as one of the famous conquistadors. For those that remained, there were not enough Indians to act as workhorses, and one by one the Spaniards began to die as starvation set in.



The rest of the crew moved up the perimeter to what is now known as Asunción, Paraguay. There the conquistadors encountered another band of Indians, who on the surface looked similar to the Charrúas and the Querandí. The Guaraní, however, were a little different. They were more densely settled and already sedentary. They had also established a hierarchical society with an elite class of princes and princesses, while the rest of the population worked for the benefit of the elite.



The conquistadors immediately took over this hierarchy, setting themselves up as the elite. Some of them married the princesses. They put the Guaraní to work producing food, and ultimately the remainder of de Solís’s original crew led a successful colonization effort that survived for many centuries to come.



The institutions established among the Guaraní were the same types of institutions that were established throughout other parts of Latin America: forced labor institutions with land grants for the elite Spaniards. The Indians were forced to work for whatever wages the elites would pay them. They were under constant coercive pressure — forced not only to work but also to buy what the elites offered up for sale. It is no surprise that these economic institutions did not promote economic growth. Yet it’s also no surprise that the political institutions underpinning this system persisted — establishing and continuously recreating a ruling class of elites that did not encourage economic development in Latin America.



Yet, the question still remains: Could it have been geography, culture, or enlightened leadership — rather than institutional factors — that played a critical role in the distinct fates of the two teams of explorers?



 **CASE STUDY: NORTH AMERICA**  
Roughly a thousand miles north, at the beginning of the 17th century, the model of the Virginia Company — made up of the elite captains and aristocrats who were sent to North America — was actually remarkably similar to the model of the conquistadors. The Virginia Company also wanted gold. They also thought that they would be able to capture the Indians and put them to work. But unfortunately for them, the situation they encountered was also quite similar to what the conquistadors witnessed in Argentina and Uruguay.



The joint stock companies found a sparsely populated, very mobile band of Indians who were, once again, unwilling to work in order to provide food for the settlers. The settlers therefore went through a period of starvation. However, while the Spaniards had the option of moving up north, the captains of the Virginia Company did not have this option. No such civilization existed.



They therefore came up with a second strategy. Without the ability to enslave the Indians and put them to work, they decided to import their own lower strata of society, which they brought to the New World under a system of indentured servitude. To give you a sense of this, let me quote directly from the laws of the Jamestown colony, promulgated by the governor Sir Thomas Gates and his deputy Sir Thomas Dale:



No man or woman shall run away from the colony to the Indians upon pain of death. Anyone who robs a garden, public or private or a vineyard or who steals ears of corn shall be punished with death. No member of the colony will sell or give any commodity of this country to a captain, mariner, master, or sailor to transport out of the colony or for his own private use upon pain of death.



Two things become immediately apparent in reading these laws. First, contrary to the image that English colonies sometimes garner, the Jamestown colony that the Virginia Company was chartered to establish was not a happy, consensual place. Pretty much anything the settlers could do would be punished by death. Second, the company encountered real problems that were cause for concern — namely, that it was extraordinarily difficult to prevent the settlers they brought to form the lower strata of society from running away or engaging in outside trade. The Virginia Company therefore fought to enforce this system for a few more years, but in the end they decided that there was no practical way to inject this lower stratum into their society.



Finally, they devised a third strategy — a very radical one in which the only option left was to offer economic incentives to the settlers. This led to what is known as the headright system, which was established in Jamestown in 1618. In essence, each settler was given a legal grant of land, which they were then required to work in exchange for secure property rights to that plot. But there was still one problem. How could the settlers be sure that they had secure rights to that property, particularly in an environment in which a stolen ear of corn was punishable by death?



The very next year, in order to make these economic incentives credible, the General Assembly offered the settlers political rights as well. This, in effect, allowed them to advance above the lower strata of society, to a position in which they would be making their own decisions through more inclusive political institutions.



 **LESSONS**  
These historical examples illustrate several important lessons. The first is that there is clear positive feedback between inclusive economic and political institutions. Inclusive economic institutions are not only more conducive to economic growth than extractive ones. They are also supported by, and support, inclusive political institutions, which distribute political power widely, while still achieving some amount of political centralization so as to establish law and order, the foundations of secure property rights, and an inclusive market economy.



Second, this example illustrates that none of the alternative theories has much explanatory power. The large disparities in prosperity that exist around us today formed mostly in the 19th and early 20th centuries. But why did they form? The examples we have considered give us several insights.



It wasn’t geography that caused the divergence between South and North America. If anything, much of South America had higher agricultural productivity, supporting a greater population density, at the time of colonization. But South America ended up poorer than North America. This reversal cannot be accounted for by the impact of geographic factors. It wasn’t some sort of culture either. In fact, it’s remarkable how similar the objectives and chosen methods of the Spanish and English colonialists were. Even if their religion and culture were different, they were after the same thing and they had the same way of going about getting it. But the conditions on the ground meant that the Spanish could achieve their goals and the English could not. And the divergence wasn’t related to enlightened leadership. If anything, the Spanish leaders were more successful because they could achieve what they wanted. The Virginia Company, Sir Thomas Dale, and Sir Thomas Gates could not.



Instead, the root cause of the divergence between South and North America is in the different economic and political institutions that developed in these territories. Because the Spanish were successful in setting up extractive institutions to enrich themselves and their king, the long‐​run economic development of most of their empire was hampered. Because the English failed in setting up similar extractive institutions — and instead inclusive institutions started developing there — the United States would be much better placed to take advantage of new technologies and economic opportunities come the 19th century.



The history of the Americas is illustrative because it shows how the trajectory of institutions and economic development depends on whether elites bent on setting up extractive institutions succeed or fail. But the Americas are not fully representative of the rest of the world. In many other parts of the world, extractive institutions are not so much imposed from the outside, but are created by domestic elites. The crucial part of the story, therefore — which _Why Nations Fail_ tries to explain in detail — is the process of institutional change.



 **CONCLUSION**  
A key lesson of the framework we present in Why Nations Fail is the importance of politics. Of course, it is economic institutions that determine economic incentives and the resulting allocation of resources, investment, and innovation. But it is politics that shapes how economic institutions work and how they have evolved. Most societies suffering under extractive economic institutions do so because political power is concentrated in the hands of an elite ruling under extractive political institutions.



The recent events in the Middle East and North Africa also highlight the role of politics. The Arab Spring has shaken not only Tunisia, where it started, but Egypt, Libya, Yemen, Bahrain, and Syria, even if the governments in the latter two countries are still holding onto power. The roots of discontent in these countries are economic and social, but those are in turn shaped by political factors. The general population has been repressed and excluded from political power for generations. The protesters in Tahrir Square in Egypt understood this and this is why they demanded not just handouts or concessions from the existing regime, but fundamental political change.



This all implies a simple but critical conclusion: You can’t succeed economically if you don’t get your politics right. And that’s where the difficulty lies, because there is no formula for getting politics right. This is illustrated, for example, by the challenges lying ahead for the Middle East and North Africa — in particular, Egypt and Tunisia. Do we expect democracy or extremism to triumph in Egypt? Have the events in Tahrir Square changed the nature of politics irrevocably or will a similar economic and political structure reemerge under a different guise? Have they opened the way to a new authoritarian regime under the auspices of the Muslim Brotherhood? Central though these questions are for understanding the economic trajectory of the region, unequivocal answers are not possible. It’s only the details of politics and how the contingent path of history will play out that will determine how successful politically and thus economically these nations will be.
"
"
Share this...FacebookTwitterThe German version of RIA Novosti reports that Russia hopes to gain more precise weather forecasts, new findings on global warming and improved exploration of new oil and gas reserves from its planned, new Arktika Satellite system. http://de.rian.ru/science/20100429/126119398.html
The Arktika System, which is made up of 5 satellites,  is a whole new instrument that will deliver absolutely new data on climate change says Alexander Bedrizki, Climate Appointee of the Russian President.  The project will allow continous observation of the Arctic 24 hours per day and be able to measure water temperature and ice thickness. The project will also have economic value because the Arctic holds huge oil and gas reserves. The project will also enable commerical flights to pass over the Arctic.
Alexander Frolow Director of the Russian Weather Service hopes to generate more accurate weather forecasts and to better assess events such as the recent Iceland volcano eruption which was above 60° north latitude.  Current satellite systems were not able to accurately track the cloud of ash from afar.
Russian aerospace company Lawotschkin will begin work on the project this year. Two communications satellites, two waether satellites and a radar satellite for measring ice  and exploring natural resources will be developed and launched into space.
Share this...FacebookTwitter "
"

La Scala to stage Gore’s ‘Inconvenient Truth’
MILAN, Italy (AP) — First it was the film and the book. Now the next stop for Al Gore’s “An Inconvenient Truth” is opera.
La Scala officials say the Italian composer Giorgio Battistelli has been commissioned to produce an opera on the international multiformat hit for the 2011 season at the Milan opera house. The composer is currently artistic director of the Arena in Verona.
Bring your marshmallows.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f207d1f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
