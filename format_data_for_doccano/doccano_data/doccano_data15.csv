"
Share this...FacebookTwitter UPDATED LIST:   !! CLICK HERE !!
What follows is only the first orignal list of climate gates, click above for the newest list.
Climate science has produced an entire warehouse full of scandals. In fact the scandals are in such big supply that you can now get them at the House of Climate Science for a dime a dozen. No other science has produced such a huge inventory.
It reminds me of a scene in the cult film Dusk till Dawn by Quentin Tarantino where an unseemly character named Chet makes a famous sales pitch outside a certain locale. I simply substituted the obscene term, for what some consider to be the world’s oldest known commodity, with the different “gates” we’ve seen in climate science.  Here’s the new sales pitch:
Come on in gate lovers!! Here at Sleazy Science we’re slashing gates in half! Give us an offer on our best selection of gates! This is a gate blowup! All right, we got drought-gate, Gore-gate, Sting-gate, Himalaya-gate, ursus-gate, we got Amazon-gate, China-gate, Russia-gate, Dutch-gate, Toad-gate
We got data-deletion-gate, we got Overpecker-gate, Hansen-gate, we got Inconvenient-gate, Africa-gate, boot-gate, Finland-gate, we even got Freedom of Information-gate, Greenpeace-gate, Yamal-gate. Come on you want gates, come on in gate lovers! If we don’t got it you don’t want it! Come on in gate lovers!  

(Warning! Should you get the idea to watch the original Chet speech in Dusk Till Dawn, then first clear the kids and ladies out of the room.)
Anyway, that said, here’s my abridged list of gates in climate science for your future reference. There’s something there for everyone! We now got:
1. Acceleration-gate
2. Africa-gate
3. AIT-gate
4. Amazon-gate
5. Antarctic sea-gate
6. Bangladesh-gate
7. Boot-cleaning manual-gate
8. China-gate and here
9. Climate Camp-gate
10. Climate-gate
11. CRU data deletion-gate
12. Dog-ate it-gate
13. Discernable influence-gate
14. Drought-gate
15. EPA-gate  h/t Climate Depot
16. Five-star WWF-gate
17. Finland-gate
18. Flooded house-gate
19. FOI-gate
20. Fungus-gate
21. Gatekeeping-gate
22. GISS Metar-gate
23. Gore private jet-gate
24. Greenpeace-gate
25. Hansen 1930s hot-gate
26. Hansen stagecraft-gate
27. Himalaya-gate and here
28. Hockey-stick-gate and here and here WCR
29. Hollywood hypocrites-gate and Dave Matthews
30. Hurricane-gate
31. Jesus Paper-gate
32. Kilimanjaro-gate
33. Malaria-gate and here (new!)
34. Meat-gate h/t reader Catalina
35. Mega-mansion-gate
36. Met Office computer-gate
37. NASA/NCDC bad data-gate and here
38. New Zealand-gate
39. NOAA adjustment-gate and here, and here
40. NOAA/GISS data selection-gate and here
41. NYT alarmism-gate and here
42. Overpeck get rid of MWP-gate
43. Oxbourgh-gate and here (bishop hill)
44. Pachauri-gate and here and here
45. Peer-review-gate 1
46. Peer-review-gate 2
47. Persecute and execute-gate
48. Polar bear-gate and here
49. Porn(soft)-gate
50. Rahmstorf smoothing-gate and here and here
51. Revelle-gate
52. Russia-gate and and here video
53. Solar-gate  Spain solar-gate
54. Sting-gate
55. Student dissertation-gate
56. Surface stations-gate and here
57. Toad-gate
58. UNEP-gate
59. UN natural disasters-gate
60. Ursus-gate
61. Windmill-gate and here
62. Wikipedia William Connelly-gate h/t to rechauffementmediatique.org
63. Yamal-gate 
      Come on in gate lovers we got lots of gates coming in!  We even got gates for a penny! Gates for only a penny! Come on in!
Hopefully this list will be helpful for the observers of climate science.
Share this...FacebookTwitter "
"
Dr. Roger Pielke Sr. has started up his Climate Science blog again, sans comments, as an informational source only. This is some very good news. See his post below.
November 27, 2007: Climate Science Is Relaunching As An Information Source
As a result of very positive encouragement from many Climate Science readers, I have decided to relaunch the website. The format will be different than in the past, however, in that comments will not be permitted. The posting of information will not be on a schedule, but when new information on a climate science issue is available that is otherwise not very visible, or has been misrepresented in the media.
The presentation of climate science in the media, unfortunately, remains biased, as has been documented numerous times on Climate Science. Thus, I have decided to reenter this mechanism of providing information. While comments will not be permitted on the website, guest presentations will be invited when there is value in providing this source of information.
Climate Science will thus provide a source of information on climate that, hopefully, will be useful to others, as part of a much needed effort to provide a balanced view of climate science.
Thanks to those who have found my website of value and take the time to read it!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea298c767',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Pacific yew tree is a fairly small and slow growing conifer native to the Pacific Northwest. The Gila monster is a lizard with striking orange and black markings from the drylands of the Southwestern US and Mexico. Two very different organisms, but with a fascinating connection. They’ve both given us drugs that have saved and improved the lives of millions of people. Paclitaxel, originally isolated in 1971 from the bark of the Pacific Yew tree, is so important for treating various cancers that it is one of the World Health Organisation’s “Essential Medicines”. This compound has been studied in more than 3,000 clinical trials. It’s safe and effective and it generates sales of around US$80-100m per year. Exenatide, a synthetic version of a compound found in the saliva of the Gila monster, is an injected treatment used by as many as two million people with type 2 diabetes. In 2014, exenatide products generated sales of US$767m. Not only that, but exenatide has also been shown to have potential in the treatment of Parkinson’s Disease. These two examples illustrate how significantly compounds sourced from nature can benefit public health, but they also tell a deeper story of how we fail to protect nature. Up until fairly recently, paclitaxel had to be isolated from the bark of wild Pacific yew trees, which meant stripping the bark and killing these rare and slow growing trees.  In 1977, demand for the drug generated an order for 7,000lbs (3,175kg) of bark, which yielded a mere 132 grams of paclitaxel. This demand destroyed 1,500 trees, damaging the native environments in which they grow. This destruction continued until 1994 when chemists came up with a way of synthesising paclitaxel. The production of exenatide does not require the maceration of thousands of Gila monsters, thankfully. However, despite the huge sums generated by the sale of the compound, the delicate habitats where this lizard and countless other species live are threatened by development and climate change. When it comes to discovering medicinal products found in nature, we’ve barely scratched the surface. With every habitat that falls to the chainsaw or disappears under the plough or concrete, we impoverish nature and deprive ourselves of potential medicines.  The molecular diversity of life on Earth is effectively limitless, but it is under threat. Conservative estimates suggest that we are losing one important drug every two years because of our onslaught on the natural world.  Perversely, this onslaught comes amid a new golden age of discovery. Tools, such as DNA sequencing, can reveal “new” species hiding in plain sight, while advances in mass spectrometry, genomics and genetic engineering have allowed us to harness their molecular diversity without excessive harvesting of wild specimens. Although the potential of natural products is undisputed, the enormous amount of effort and resources required to bring a promising molecule to market is offputting. Not only that, but unscrupulous “bioprospectors” have illegally collected living material, often from developing countries. These predatory practices prompted legislation that now hinders legitimate natural products research that seeks to protect biodiversity.  


      Read more:
      Drugs from bugs: the next blockbuster medicine could be lurking inside an insect


 Magainin was the first anti-microbial protein discovered in an organism, isolated from the skin secretions of the African clawed frog. The discovery stems from the observation that surgical wounds on these frogs rarely become infected despite non-sterile procedures and conditions. Efforts to commercialise this molecule were mired in difficulties and today, despite the promise of a potentially transformative drug to treat infections, no magainin products are available. Natural products give us a compelling angle for the protection of overlooked species and their habitats, but we need an ethical and transparent approach for developing them. To some extent, this is the goal of the Convention on Biological Diversity and the Nagoya Protocol – international agreements on sharing the benefits derived from biodiversity fairly.  But as a result of these treaties, the academics working to find new drugs in nature must meet the same regulatory requirements as companies with commercial intent. Extensive permitting requirements mean many academic scientists are avoiding international collaborations to study biodiversity altogether – hampering the discovery of new molecules.  Governments need to support research efforts and collaboration between scientific disciplines such as ecology and biochemistry, with investment and infrastructure. Building trust with communities that live where natural products are sourced is also critical. These steps could create a system of natural product research and development with a greater appreciation of nature’s value. Ultimately, the equal sharing of benefits derived from drug discoveries will help conserve nature. However, the clock is ticking and with every day that passes species and their unique chemistry are lost forever. Nature is a public library of information waiting to be accessed. The science, technology and political will to read this library is being ignored while destruction continues and companies hoard resources that have been harnessed from nature for profit. We are destroying the best library in the world in order to build a writer’s workshop, open only to the few."
"
I’m travelling the next few days, moderation will be spotty. After tomorrow, you may find that comments may go for 12 -18 hours or more without approval.
In the meantime, you can debunk my own “hockey stick”.

click for larger image
Thanks to all who have come by and participated with comments and ideas!
Best Regards,
Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea09b8f84',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’t have a lot of time today, but I found this interesting. Commenter “Basil” has offered this for discussion. So I’m putting this up without comment on my part. See also the decadal trends table below. Have at it folks.

Click for full sized image

1979:01-1992:12
---------------------------------------------
GISS        0.000783764**     (0.094C/decade)
HadCRUT     0.000460122**     (0.055C/decade)
RSS_MSU     0.000498964       (0.060C/decade)
UAH_MSU 	1.71035E-05       (0.002C/decade)
1993:01-2001:12
---------------------------------------------
GISS        0.00174741**      (0.210C/decade)
HadCRUT     0.00147990**      (0.178C/decade)
RSS_MSU     0.00221135**      (0.265C/decade)
UAH_MSU     0.00217023**      (0.260C/decade)
2002:01-2008:1
---------------------------------------------
GISS       -0.00091450       (-0.110C/decade)
HadCRUT    -0.00270338**     (-0.324C/decade)
RSS_MSU    -0.00208111       (-0.250C/decade)
UAH_MSU    -0.00130882       (-0.157C/decade)



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0aae336',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
One of the most surprising things I’ve learned from the surfacestations.org project is that for some odd reason, there are a number of climate monitoring stations of record in the USA at sewage treatment plants. If you’ve ever driven by one of these in the wintertime, they tend to look like steam saunas. They are localized heat bubbles from the waste-water processing.
At Orangeburg, SC not only is the official USHCN climate station of record at a sewage treatment plant, it’s also a nonstandard thermometer (a climate station normally looks like this), and strapped to the side of a telephone pole. I don’t know about you, but experience with creosote treated telephone poles tells me that they’d tend to create a hotter local measurement environment.

Photo by www.surfacestations.org volunteer surveyor Don Kostuch. See the complete image gallery here.
Then there’s the brick building to radiate heat at night, the asphalt parking lot, the effluent channel running nearby, and the overall sewage treatment plant waste heat to consider. I doubt there is an easily applicable set of equations which can untangle the myriad of potential microsite biases.
Then there’s sewage. As population growth occurs, sewage plants add more vats and equipment to handle the increased volume. The increased volume of effluent loses some of it’s heat at this location during the purification process.

Click graph for larger version, data from NASA GISS
The real question is: What are we actually measuring at this location? Are we measuring temperature as an indicator of climate change or are we measuring waste heat from increases in sewage processing that mirrors local population growth?
UPDATE and CORRECTION:
I made an error, this is not a sewage treatment plant. It does treat water, and the description in the site survey from the surveyor was “water filtration plant” which I mistook to mean “sewage treatment” since so many other locations have been at sewage treatment facilities. For example, one of the worst is Titusville, FL, which has been highlighted in this blog in Part 31 and also surveyed by the same volunteer. I looked at the photos he provided, and did not discern initially that the tanks were not for sewage treatment.  Some sharp eyed readers have pointed out the identification problem, which I’m happy to correct.
The questions about the validity of the temperature measurement environment in the midst of a sewage treatment plant are still valid, but do not apply to this location.  So the question we now have for this location is; do the large water filtration pools on this site provide an evaporative cooling effect or do they release heat?
The water vapor impacts at the facility are likely a factor, possibly for Tmin overnight, which is more prone to such effects. Note that there has been new construction at this location, and given the apparently new water filtration pools added on site, there may still be an effect of measuring the local population increases by proxy.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea279d29e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The _Washington Post_ recently ran a shocking above‐​the‐​fold article warning us of “Escalating Ice Loss Found in Antarctica.” A new paper by Eric Rignot of NASA’s Jet Propulsion Laboratory shows a net loss of ice where most scientists thought the opposite would occur.



The _Post_ went full‐​bore with this one, spreading the article on to an entire interior page. The piece ends by noting that Rajenda Pachauri, head of the United Nations’ Intergovernmental Panel on Climate Change (IPCC), is so concerned that he’s is personally going down to inspect the situation.



He should. Before he even gets to Antarctica, Pachauri is going to see something even more surprising than Rignot’s finding. Despite a warming Southern Ocean, the amount of ice surrounding Antarctica is now at the highest level ever measured for this time of the year, since satellites first began to monitor it almost thirty years ago. This represents a continuation of the record set last winter (our summer).



Thanks to the miracles of modern technology, we can also look at the departure from the average for ice mass in a given month. At present, the coverage of ice surrounding Antarctica is almost exactly two million square miles above where it is historically supposed to be at this time of year. It’s farther above normal than it has ever been for any month in climatologic records. Around now, because it’s summer down there and the ice is headed towards its annual low point, there should be about seven million square miles of it. That means, as data in University of Illinois’ web publication Cryosphere Today shows, that there is nearly 30% more ice down in Antarctica than usual for this time of the year.



All of the IPCC’s models of Antarctica in the 21st century forecast a gain in ice, as a warmer surrounding ocean evaporates more water, which subsequently falls in the form of snow when it hits the continent. It’s simply too cold for rain in Antarctica, and it’ll stay that way for a very long time.



Concerning Antarctica as a whole, the IPCC’s new climate compendium notes “the lack of warming reflected in atmospheric temperatures averaged across the region.” Other studies, such as Peter Doran’s in _Nature_ in 2003, show actual cooling in recent decades. (There is a small area of significant warming in the peninsula that points towards South America, but this is less than 2% of Antarctica’s total land mass.)



There’s brand new evidence, just published in mid‐​January in _Geophysical Research Letters_ , of a striking increase in snowfall over that peninsula. The few snowfall records that are available elsewhere in Antarctica show considerable variation from decade to decade, so discriminating the “signal” of increased snowfall caused by global warming from all the rest of the “noise” may be very difficult indeed.



We see the same problem with hurricanes and global warming. Their strength and numbers vary considerably from year to year. 2005 was the most active year ever measured in the Atlantic Basin, while 2007 was one of the weakest in history. How do you find the fingerprint of global warming amidst such variation?



So it’s not warming up, and the snowfall data are equivocal, yet the continent is experiencing a net loss of ice. How can this be, and is it even important? The current hypothesis is that warmer waters beneath the surface are somehow loosening the ice. That’s plausible, but again, there’s precious little proof of it.



And further, the bottom line is that there is more ice than ever surrounding Antarctica.



One of the tired tropes that reverberate throughout global warming reporting is that inconvenient facts get left out. In this case, it’s blatant. Midway through the _Post_ ’s page‐​long article comes a statement that “these new findings come as the Arctic is losing ice at a dramatic rate.” Wouldn’t that have been an appropriate place to note that, despite a small recent loss of ice from the Antarctic landmass, the ice field surrounding Antarctica is now larger than ever measured?
"
"The average family throws away about £700 worth of food each year. This is not just a drain on our finances, but also has significant environmental impacts – both in terms of production and waste management – and Christmas is no different. A report by Unilever said that each year in the UK the equivalent of 4m Christmas dinners are wasted – the equivalent to 2m turkeys, 74m mince pies and 5m Christmas puddings. And that is before you consider the before and after Christmas buffets, teas and food from other social gatherings. Research from Loughborough University explored the reasons behind all of this consumer food waste. It is not inherently anybody’s fault, but a symptom of the way that the UK’s food provisioning system has evolved. And it turns out that many of the reasons are solvable. With this in mind, here are some practical approaches that you can take to reduce your food waste this Christmas (and the rest of the year), which will also save you money and reduce your carbon footprint. The primary reason for food waste is overbuying. If you are having a large Christmas gathering, plan how much food you will need for the number people attending. Don’t buy extra just in case: you are very unlikely to have too little.  If you feel it absolutely essential to keep some food in reserve, then make sure you buy food that will keep longer. Serve your short shelf life food first, and then if it is eaten, bring out the longer life food – when the prawn ring runs out, bring out the cheese and crackers. Before you even set foot in the supermarket (real or online) make sure that you write your shopping list. Then stick to it. Don’t get drawn in by buy–one–get-one-free (BOGOF) or special offers. They are not normally as good a deal as you think and you will buy more food than you need. You will likely end up wasting it, or consuming too much. Either way, there’s no benefit. When buying meat or dairy or other fresh products, be conscious of “use by” dates. Make sure the food you buy will still be good to eat when you plan to eat it. These dates are an important indicator for when food may become harmful to eat due to bacteria growth. You do not want to get your turkey curry buffet and find that your raita sauce is not safe to eat. Do not however confuse “use by dates” with “best before dates”. Best before dates are a rough guide to indicate when food might have gone past its best, but they are very conservative and for most foods, quite unnecessary. In most countries, best before dates do not exist. Assuming the food is not several years old, it is likely to be perfectly safe to eat and still delicious long past its best before date. We should reinstate common sense in determining when food is good to eat or not. Most fruit, vegetables and cooked meats will last longer if stored in their packaging and in the fridge, so you should keep them there. A full festive fruit bowl might look good, but you are likely to end up throwing items away which have gone past their best. Partially consumed refrigerated items should be placed in a reusable, resealable tub and put back in the fridge. Refrigeration slows down the growth of bacteria and will therefore keep your food edible for longer. But there are certain foods that don’t fare so well in the fridge, such as bananas, avocados, cake and melon. So it’s worth checking to get the most out of your food. It might sound like common sense, but take time to consider how much people might want to eat and cook that amount. Don’t cook extra, it will not only take longer, but it will also cost you more and it won’t get eaten. Many people end up leaving food on the plate. So think about the right amount for a nice meal, not to force your guests to over-consume and struggle with indigestion from those three extra roast potatoes and two pigs in blankets. Christmas may be about feasting, but it is not about gluttony. Despite your best efforts, there may be some food remaining. Make sure you cover it and once cooled store it in the fridge rather than leaving it to fester on the counter top. To me “leftovers” is a dirty word. They are not leftovers, they are delicious ingredients for your next meal. If you really have too much, invite some friends around and get them to help you eat it. They will thank you for it and you will have a better Christmas. Most importantly, once you have tried these easy approaches to reducing food waste, continue to use them. You will be sure to save money – and help save the planet at the same time."
"
Share this...FacebookTwitterHere’s another excellent post by Eduardo Zorita at the Klimazwiebel.
In this BBC podcast (takes a minute or so to load), the view of green elitists is that we have casus belli. Thus democracy has to be suspended and common sense authoritarianism has to take over – just for a while, until things are put back in their proper order. The general population is just too stupid to understand it, and is only getting in the way. (Actually, and thankfully, they’re too informed and many people understand precisely what this is about).
“The situation is urgent, the world is going to hell in a handbasket – let us rescue the planet. Trust us,” we are constantly told.
I’m trying to think of a veggie or fruit that’s green outside and brown inside. The closest thing I can think of is a rotten avocado. For me it’s even disturbing that the BBC even gives equal time and weight to the green nutjobs who propose suspending democracy and taking us back to the German Democratic Republic – East Germany, behind the Berlin Wall, for those of you who may have already forgotten. “Trust us” just isn’t good enough. History shows that populations have been burned by this all too often.
The good news is that authoritarianism only works if there’s consent. But there can be no consent unless there is a genuine debate. That’s where the problem lies for the kook warmists. They’ll never win this debate, and they know it. Indeed consent has been massively eroding lately. Their science has been exposed as a hoax. They’ve lost the case and their desperation has caused them to lose any rationality they may have once had.
Update: Bishop Hill has found the perfect food staple to symbolise enviro-leftists: pistachios. Green only on the surface, brown inside, and in total, a nut throughout.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHave Trump/Barr led the Biden campaign into a lethal trap? Was a legal FISA warrant obtained to surveil campaign?
If you watched the Trump Team’s press conference yesterday like I did, you probably wondered why on earth they held it where they did: Four Seasons Total Landscaping, owned by Marie Siravo, located in an industrial zone next to an adult bookstore and crematorium in the outskirts of Philadelphia!
The media was completely baffled by the location choice. Slate, for example, hints the Trump team was probably so incompetent that they mixed up the business with the Four Seasons hotel in Philadelphia.
But then earlier this morning I stumbled upon a Twitter thread that provided an interesting viewpoint. Normally I’d dismiss such things as a conspiracy theory, yet my gut feeling tells me maybe there’s something behind it. Here’s that Twitter thread. I’ve cut and pasted it fearing that it might disappear.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Could federal agents and crime fighting units really be that smart? If this were true, Barr and Trump would be absolute geniuses. We all know Giuliani is a top notch organized crime fighter with half a century of experience. He’s seen it all. Maybe he’s sending a message to the Democrats: We’re miles ahead of you.
Could this business also be the location for ballot fraud operations? Has it been bugged and surveilled by the authorities for weeks? Oh what irony this would be.
Or maybe that location was simply chosen to fool potential protesters and Antifa from showing up and disturbing the press conference.
Anyway, it’s just a really weird place for a press conference. Maybe some Democrats are panicking. One thing is clear: There’s mounting hard evidence of massive voter fraud committed and the Democrat operatives are desperate to keep a tight lid on it.
Thoughts?


		jQuery(document).ready(function(){
			jQuery('#dd_9acea29c387eba69c7b9a05eaf776dc7').on('change', function() {
			  jQuery('#amount_9acea29c387eba69c7b9a05eaf776dc7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Greenhouse gas (GHG) emissions created by humans are a primary cause of global warming. While carbon dioxide emissions from the burning of fossil fuels and changes in land use make up the largest share of these emissions, non-CO2 greenhouse gases such as methane (CH4) and nitrous oxide (N2O) contribute substantially to overall warming and livestock – and the meat that we eat – is a big contributor.   The Intergovernmental Panel on Climate Change (IPCC) provides guidelines for estimating livestock emissions on a regional level. Direct emissions of methane and nitrous oxide from livestock worldwide has been recently estimated: they represent about 9% of total GHG emissions caused by human activity. The IPCC guidelines offer a relatively simple and robust accounting for estimating GHG emissions produced in each country.  However, producing countries are not the only ones responsible for emissions: the goods and services being produced are exported for consumption in other countries. In other words, the responsibility for emissions from meat lies with consumers as much as producers.  Along with other researchers we have developed alternative accounting systems that re-allocate GHG emissions from producers to consumers. In a study we recently published in Environmental Research Letters, we estimated the total non-CO2 emissions from beef, pork and chicken consumption in 237 countries between 1990 to 2010, allocating emissions embodied in trade to countries on the basis of consumer demand for meat.  These emissions were attributed to consumer countries after first estimating the livestock emissions produced by the digestive process that leads to methane release in animals, along with manure management and manure left on pasture in the origin country. We then used data from the new FAOSTAT database to work out emissions from the import and export of meat. The study highlighted the countries with the largest difference between production and consumption emissions and revealed where fluxes of non-CO2 emissions are greatest.  We found that in 2010, 36.1 Mt (millions of tonnes) of CO2-equivalent emissions were related to meat produced in one country but consumed in a different country. Of this total, 72% was methane and 28% was nitrous oxide. In particular, 26.7Mt of CO2-eq (74%), 7.3 Mt of CO2-eq (20%) and 2.1Mt of CO2-eq (6%) emissions related to beef, pork and chicken respectively that were produced in one country and consumed in another.  Although the emissions embodied in traded beef were greater than those of pork or chicken, the emissions from traded beef grew at a slower rate (4%) between 1990-2010 than those related to pork and chicken (81% and 360%, respectively).  The dominant global fluxes are the export of emissions embodied in meat from Brazil and Argentina to Russia (2.8 and 1.4 Mt of CO2-eq, respectively). Meat exported to Russia embodied 5.2 Mt of CO2-eq emissions; we found that in Russia, 18% of meat-related emissions were traded internationally in 2010. In the same year, emissions embodied in US imports of meat from Canada were equal to emissions embodied in US exports to Mexico: 1.2 Mt of CO2-eq. Australian meat exports to South Korea also embodied substantial emissions: 1.0 Mt of CO2-eq. But Figure b reveals that trade among European countries was quite substantial. In particular, meat exported from France to Italy and Greece embodied 1.4 Mt and 1.2 Mt of CO2-eq emissions, respectively. In addition, Italian imports of meat from Poland, Germany and Netherlands embodied 0.7 Mt, 0.6 Mt, and 0.7 Mt of CO2-eq emissions, respectively. We found that in Italy about 30% of meat-related emissions were traded internationally in 2010. Elsewhere in Europe, meat exported from Ireland to the UK embodied 1.0 Mt of CO2-eq emissions.  Globally, percent changes in traded emissions from 1990 to 2010 vary by region and type of meat. The graphs below show the emissions from production and consumption of beef, pork and chicken in several countries in 2010, as well as the percentage change under consumption-based accounting in the same year (negative percentages indicate net export of embodied emissions and positive percentages indicate net import of embodied emissions). We concluded that long-term growth in the international trade of meat since 1990 means that methane and nitrous oxide emissions from beef, pork and chicken produced in one country are increasingly related to meat consumption in a different country. In particular, we found that large transfers of emissions from meat between North American countries, from South America to Russia, and between European countries. Our findings are important because they quantify the superimposed effects of three important global trends: the growth of international trade; the industrialisation and intensification of meat production; and the increasing consumption of meat.  The overall growth in emissions from the meat trade indicates that the emissions related to increasing consumption of meat are also increasingly disconnected in space from the point of consumption. This spatial disconnect of production and consumption is a challenge for regional or national policies that regulate livestock emissions, because all existing policies neglect any emissions embodied in trade.  The main drivers of meat-related emissions in international trade are the volume and type of meat traded. The trade of beef is the largest source of livestock-related emissions. This is due to the large volume of beef traded internationally and the emission intensity of non-dairy cattle, which is substantially higher than pork and chicken mainly due to greater GHG emissions released during enteric fermentation. Consequently, beef releases more emissions than pork and chicken per ton of meat traded. Dietary preferences are a strong driver of livestock emissions, with beef generally related to substantially more GHG emissions per ton of meat traded than pork and chicken, and much more than vegetables. Therefore, substituting pork, chicken or vegetables for beef in the diet could reduce livestock emissions."
"The UK foreign secretary, Dominic Raab, has called on Australia to work with other countries to bring down carbon pollution as it works towards the “challenge” of achieving net zero emissions by 2050. Raab, who met with Australia’s foreign affairs minister Marise Payne on Thursday, said the pair had a “good constructive conversation” about Britain’s goal to reduce emissions as it prepared to host the United Nations climate summit, COP26, in Glasgow later this year.  The visit to Australia is the first overseas visit for the UK foreign secretary since Britain officially exited the European Union six days ago. When asked about Australia’s policy record on climate change, which was a “politically contentious” issue in the wake of the summer bushfire crisis, Raab said the UK’s approach was to “try to make a success” of COP26. He said Britain wanted a “step change” in the international response to climate change, which he described as the “challenge of our times”. “We are going to talk to all the countries about their contributions in getting emissions down,” Raab said. “I think there is an exciting opportunity, there is a challenge but there is also an opportunity, and we are want to lead in rising to that challenge and we have had a really good conversation about it today.” He said there was potential for Australia and the UK to be “natural collaborators” on reducing carbon emissions, naming the greening of financial services and “technical cooperation” as possible areas. While he declined to comment on whether more ambitious climate policy would be a requirement of the post-Brexit free trade agreement to be signed between Australia and the UK, he said Britain was looking for a “team effort right across the board”. “We hope to be leading by example with our commitment to get emissions down to net zero by 2050 but there is a whole range of other things that we need to talk about and what we have got to try to do is make it a win-win,” he said. “It is a huge challenge … but we have got, and I know that Australia has got, the innovators, the entrepreneurs who can come up with green technology which can help find a way forward. “We have had a really good constructive conversation about it and we will continue that conversation with our Australian friends, but also all of the other big players in that debate.” Payne said in the lead-up to COP26, the Australian government would be looking at what “practical climate action we can take together”. Raab was also due to meet with the prime minister, Scott Morrison, on Thursday afternoon. Morrison has refused to back the goal of net zero emissions by 2050, despite signing on to a communique at the Pacific Islands Forum that committed to develop a 2050 strategy this year. Following calls from the UK prime minister, Boris Johnson, for other countries to join Britain in striving for net-zero 2050 target, Morrison said the Australian government would not adopt policies that were about “putting taxes on people, putting their electricity prices up or driving industries out of regional areas”. “I would never make a commitment like that if I couldn’t tell the Australian people what it would cost them,” Morrison said. The prime minister is under pressure from within his own party room to increase the government’s climate change action, but moderate MPs are facing pushback from conservatives. On Thursday, Morrison was asked how he would balance a proposed transition to cleaner energy with calls from Queensland Nationals MPs to build new coal-fired power stations and retrofit existing coal plants to reduce emissions. The prime minister said the government had adopted a “common sense position” that recognised that the reliability of Australia’s electricity system depended on coal. “It understands the need for the maintenance and sweating of those assets which are providing reliability to the system,” Morrison said. “And where those types of assets in the future can be developed in the way that would be required under the environmental standards, then that’s not ruled out either. So it’s common sense. It’s a common sense, well-balanced policy. And I think that embraces everybody, not just in the Liberal party room, but right across the Coalition.” When asked if this meant the government could use taxpayer funds to retrofit existing coal-fired power stations to reduce emissions, Morrison left the option open. “We work with all the energy companies because we know that we don’t want to force people’s power prices up and we don’t want to see a loss of capacity out of the system that is unnecessary. And so we take all of these decisions in the national interest.”"
"An exotic parasite is spreading through the world’s honey bees and global warming is making it worse, according to a study that shows it will present increasing problems in North America and Europe. All animals are afflicted by a wide range of pests and parasites. Many are relatively benign. But for the honey bee one gut parasite in particular is a major risk, especially as summers become warmer. Bees are fairly used to parasites. A native single-celled (microsporidian) parasite of the honey bee called Nosema apis has  probably co-existed with its host for millions of years. Back in the 1990s Ingemar Fries, a world expert in honey bee pathology, visited Beijing where he came across a subtly different and novel variant of N. apis in the eastern honey bee. He named it Nosema ceranae.  Fries thought nothing more of his scientific paper describing the species, until the same parasite was found a decade later in western honey bees (the species native to Europe) in Spain, Taiwan and Vietnam. This prompted a team of scientists, including me, to search for the parasite in honey bees across the world. If a recently discovered virus had jumped species and was spreading globally, that was big news for us bee experts. By 2007 we had confirmation: it had indeed gone global, being found across the Americas, Africa and Australasia as well as Europe and its native Asia. This represents a dramatic spread, probably brought on by increasing world trade and the movement of goods, and all within a short space of time – maybe only five years. Worryingly, the exotic N. ceranae is far worse for western honey bees than the native parasite. It is more virulent and it weakens the bees causing them to die while away from their colony – sometimes leading to total colony collapse. In Spain, the parasite has lead to honey bee colony collapse within 18 months of infestation.  And the exotic virus is displacing its native sister species. We staged competitions between the two in a series of laboratory experiments which demonstrated N. ceranae can indeed outcompete its rival. Our findings are published in the latest edition of the journal of the Royal Society. For now the exotic parasite remains a rarity in northern Europe and North America. But it won’t stay this way for long. We used a simple mathematical model to analyse the dynamic between the two species of parasite and we found the exotic N. ceranae’s competitive advantage increases the warmer it gets, perhaps a legacy of its origins in the warmer east Asian climate. We predict that, as the world warms, beekeepers in cooler regions will suffer increasing problems. Though we don’t yet know how to solve the problem of the exotic invader, our research does at least help us to understand why warmer summers may paradoxically lead to great problems for honey bees; forewarned is forearmed.  And it may not only be our honey bees which suffer. Earlier this year we have shown that the exotic microsporidian – unlike the native – is also widespread in bumble bees, so it may cause knock-on problems for our wild pollinators too."
"

On May 28, Ali Larijani, former nuclear negotiator and close confidant of Iran’s Supreme Leader Ali Khamene’i, won the position of speaker of the Majlis, Iran’s parliament. Larijani is a member of the mainline conservative faction in Iran — which is different from the more radical faction led by President Mahmoud Ahmadinejad. (Iranian political observers have aptly borrowed the American term “neoconservative” to refer to the Ahmadinejad faction.)



Larijani’s rise was the first of a series of political changes in Iran. At about this time next year, Iran will hold a presidential election. Its outcome could depend, in part, on the outcome of the 2008 elections here in the United States. Given the serious disputes between the two countries and the prospect of another war in the Middle East, Americans — and American presidential candidates — should take a moment to think about how our election could influence Iran’s.





Despite soaring oil prices, Iran’s economy is in shambles. 



(Obligatory “to be sure” qualifier: Iranian elections are by no means free or perfect. Candidates for office in Iran can be approved or barred from running based on the whims of the clerical leadership. Even so, they reflect the direction of the political winds within the country’s controlled political climate.)



In the years preceding the last Iranian presidential election, which produced Ahmadinejad, American neoconservatives repeatedly minimized the differences between members of the Iranian leadership. Michael Rubin, for example, told _National Review_ readers in 2002 that then‐​President Mohammed Khatami was “neither a reformer nor a democrat” but rather “a fraud.” President Bush subsequently slotted Iran into the Axis of Evil and settled in for an indefinite occupation of neighboring Iraq, making little effort to reach out to Khatami’s government and spurning its offer of negotiations. What Rubin, Bush, and others failed to grasp was that, despite Khatami’s faults, things could get worse.



And get worse they did. During his 2005 campaign for the presidency, Ahmadinejad did not emphasize foreign policy, focusing instead on economic populism. Still, a vote for Ahmadinejad, a former member of the Islamic Revolutionary Guard Corps, was widely viewed as a vote for confronting America. And Ahmadinejad wasted no time proving that view right, from his “world without Zionism” conference to his flamboyant defiance of the United States.



In recent months, by contrast, Khatami has been busy making biting reformist speeches throughout Iran. In Tehran in March: “People want freedom … Freedom means people be allowed to question the ruling system and change it without use of force if the establishment doesn’t respond to their demands.” In Gilan in May: “What did the imam [the founder of the Islamic revolution, Ruhollah Khomeini] mean by exporting the revolution? Did he mean that we take up arms, that we blow up places in other nations and we create groups in other countries to carry out sabotage in other countries? The imam was vehemently against this and was confronting it.”



Disparaging Khatami and convincing ourselves that there is no difference between him and Ahmadinejad is foolish and counterproductive. Unless we want war or a nuclear‐​capable Iran, America needs a negotiating partner, and so much the better if he is similar to Khatami — or even Larijani — rather than Ahmadinejad.



Now, mercifully, the economic demagoguery of Ahmadinejad and his compatriots has been shown to be a disaster. Despite soaring oil prices, Iran’s economy is in shambles. But if anything can save the hardliners from the consequence of their own bumbling stewardship of the economy it is a vague sense that war with America looms just over the horizon. As early as January 2006, Ahmadinejad’s economic policies were wreaking havoc, but as one legislator critical of the president pointed out, “as the [foreign] pressure has increased, the safety margins for him to operate have widened.”



And so it may be in the coming presidential elections. In all likelihood, Iran’s neoconservatives will blame their economic woes — in the mold of Fidel Castro — on the American Colossus, which stands athwart Iranian development.



But there are even bigger prizes than the presidency in Iran. The aptly named position of supreme leader — the ultimate center of power in Iran — eventually will be at stake, and probably sooner rather than later. The current supreme leader, Ali Khamene’i, has been in office since 1989 and will be 69 years old this summer. Speculation about his health has been a parlor game in both Tehran and Washington.



While there are a number of potential candidates to replace him, one, Mohammed Taqi Mesbah‐​Yazdi, is the clerical equivalent of Ahmadinejad and a close spiritual adviser to the firebrand president. Should the international environment remain as poisonous as it is today, it is possible to envision the Assembly of Experts (a body of clerics that selects the Supreme Leader) selecting a hardliner such as Mr. Mesbah‐​Yazdi as Supreme Leader, which would be a hugely negative development in terms of U.S.-Iran relations.



Which man Americans select as their president will likely have a meaningful effect on U.S. foreign policy and on U.S.-Iran relations particularly. He could also have an effect on the nature of the next generation of Iran’s political leadership. One U.S. candidate has sung a song on the campaign trail about bombing Iran, and the other has called for lowering the temperature and making a forthright effort to negotiate.



There is no immutable law of politics that says moderation on one side will lead to moderation on the other. At the same time, it is difficult to see how electing a man wedded to the most wild‐​eyed neoconservative vision of foreign policy would cause Iranians to select more temperate leaders.



Either way, Americans’ choice could influence the nature of the next generation of Iran’s leaders — and with it, the contours of U.S.-Iran relations for decades. With so many Americans ruing the war in Iraq, they would be well‐​advised to consider the prospect of war in Iran, and what they can do to influence whether or not such a war comes to pass.
"
"Something you might have missed amid all the horserace and app-failure coverage of the Iowa caucuses: a deep discussion took place over the past year about the climate crisis and agriculture that could change the way our food system operates. Every leading Democratic campaign now endorses an aggressive approach to conservation that could dramatically reduce greenhouse gases, improve water quality and enhance rural prosperity.  Candidates lined up to tour Matt Russell’s organic farm in central Iowa over the past year to learn about how a diversified cropping system involving livestock can suck carbon out of the air and sequester it in the soil to feed us better. Pete Buttigieg and Elizabeth Warren came up with plans that would restructure farm policy to direct funding away from subsidizing production and toward conservation. Warren would increase funding 15-fold for the Conservation Stewardship Program (CSP), to $45bn. The CSP pays farmers for regenerative agriculture practices – planting soil-saving cover crops, reducing chemical use and tillage, and all the while sequestering carbon in the soil. Export markets for American ag commodities are falling apart. The world has been telling us through markets for years that we are growing about 30% too much corn and soy. Meanwhile, we are killing the Gulf of Mexico with excessive commercial fertilizer, which washes down the Mississippi River. California and Australia burn in part because we are spewing too much nitrogen – as problematic as CO2 for global warming – from our broken agrichemical system. Increasing numbers of midwestern farmers who watched their fields wash away in last spring’s scouring torrents are showing up at field days offered by the Practical Farmers of Iowa, which preaches the gospel of making money on the farm by saving soil and reducing chemical costs. They watch the weather closer than anyone, and they’re ready to look into the old way of doing things – grazing in rotation with a diverse series of carbon-capturing crops – to find a way forward. Candidates started to explore the topic at a rural forum organized in Storm Lake last March by the Iowa Farmers Union. The discussion intensified during the summer as a loose coalition of Iowans, led by the former agriculture secretary Tom Vilsack, pushed candidates to pay farmers for environmental services instead of insuring them for planting in a flood plain. Bernie Sanders is all-in with the Green New Deal. Joe Biden, advised by Vilsack, came up with his own comprehensive plan. Buttigieg is now conversant in how microbial activity in the soil can reverse nitrogen loss to air and surface water. Candidates embraced the idea that renewable energy – wind, solar, hydrogen – can not only ameliorate the climate crisis but also create high-paying technical jobs in rural communities hemorrhaging people. You wouldn’t know it by the non-stop coverage of the percentage fractions separating the leading Democratic campaigns, or whether Sanders insulted Warren, or how Senator Susan Collins equivocated again after lunch. But, as the Amazon shrinks, our quiet revolution in agriculture policy might be the most important story of the news cycle. Markets are telling us they don’t need all that corn. Iowa State University tests of the kernel tell us that soil degradation is eroding protein content. Wheat production in China is falling because of it. General Mills is up on the news, and is urging growers in the Dakotas to go organic because consumers demand it. Kellogg is phasing out glyphosate from its acres. The latest poison from Bayer, dicamba, faces a new wave of class-action lawsuits from angry farmers. In other words, the gig is up on the last 50 years of chemical- and export-driven food production. It hasn’t worked for farmers or rural communities, and they know it. That’s the message from the fallow fields of Iowa covered in snow, and in a few places winter rye holding soil together and nitrogen in its place. There would be a lot more of it if the government would catch on and quit rigging everything for the agrichemical supply chain. The ball is rolling because farmers know Nature is calling the shots. Eventually politics catches up. Climate was a priority for Iowans in this cycle, unlike before. The conversation has changed, and not a moment too soon. Art Cullen is editor of the Storm Lake Times in north-west Iowa, where he won the Pulitzer prize for editorial writing. He is author of the book Storm Lake: Change, Resilience, and Hope from America’s Heartland, recently released in paperback"
"The inauguration of Brazil’s new president, Jair Bolsonaro, has triggered fears that rates of deforestation in the Amazon will increase. There are indeed good reasons for concern about Bolsonaro’s administration. But several factors, both domestic and transnational, could constrain its ability to wreak environmental damage. First, some bad news: Bolsonaro and his cabinet do seem to view environmental concerns as an obstacle to development. For instance, the new environment minister, Ricardo Salles, said that the debate over climate change was a “secondary issue” and was recently convicted in court of fraudulently favouring mining companies when he was state secretary for the environment in São Paulo. Under Salles’ leadership, the ministry will probably suffer budget cuts, and it has already lost key departments.  Furthermore, Bolsonaro has said he wants to restrict the ability of IBAMA, the forest protection agency, to fine individuals and companies that illegally deforest and pollute. And, while the rate of deforestation in the Brazilian Amazon fell overall by roughly 75% between 2004 and 2017, it has gone back up again even before Bolsonaro took office. Between August 2017 and July 2018, deforestation increased by an estimated 13.7%.  Bolsonaro also recently tweeted that he wants to free Brazilian agribusiness from dependence on imported fertiliser (75% comes from abroad). However, mining the ingredients in Brazil could do further environmental damage. For example, the largest recently discovered deposit of potassium, used to make fertiliser, is on the banks of the River Madeira in the Amazon. The new president also appears to favour more dam-building (there are proposals to build 334 dams in the Amazon). He also backed away from the previous commitment of the Brazilian government to host the next UN climate conference later this year. And, on his first day in office, Bolsonaro signed a provisional measure transferring authority to demarcate indigenous lands from the justice  ministry to the agriculture ministry, thereby making it highly likely that – as he promised – no new indigenous reserves will be created on his watch.  Bolsonaro does face some constraints. The new president speaks as if agribusiness and the protection of the environment are incompatible – and appears to want to sacrifice the environment for farming, mining and logging. But other voices will have a say, and at least some heed will be given to the view that sustainable agriculture which preserves biodiversity is better both for Brazil’s development prospects and for the world’s climate. Before his inauguration, Bolsonaro said that he wanted to subordinate the environment ministry to the agriculture ministry. He was persuaded to drop this idea, due in part to criticisms from environmental NGOs and federal civil servants in environmental agencies. Some agricultural interests even spoke out, because they fear that their international image and access to markets, especially the European Union, could be damaged by being associated with deforestation. Brazil also has an environmental movement that is as old as its counterparts in Europe and North America. It was the strength of this movement that ensured the country’s 1988 constitution has several ecological safeguards in place, including conservation areas, indigenous reserves and the environmental licensing system. José Lutzenberger, an environmental pioneer and former environment minister, helped to organise the Eco 92 conference in Rio and demarcate the huge Yanomami indigenous reserve.  The Rio conference was part of a process that eventually led to the 2015 Paris Agreement, where Brazilian participation was important. And, in his last days in office, outgoing president Michel Temer delivered a report to his successor that recommended that Brazil stay in the Paris Agreement and pursue the goal of achieving a zero-carbon economy by 2060. External actors can also pressure the Bolsonaro administration. For example, the government of Norway has contributed 93% of the money disbursed by the Amazon Fund to 102 different projects, amounting to hundreds of millions of dollars. These funds provide incentives to enforce environmental laws and create sustainable livelihoods in the rainforest.  Norway’s contributions are tied to maintaining rates of deforestation to specified limits, a fact Temer was reminded of by his hosts on a visit to Oslo in June 2017. The Bolsonaro administration is likely to move quietly to achieve some of its objectives. In addition to weakening the environment ministry it could informally signal to state governors and congressional delegations that the laws regarding deforestation will no longer be rigorously enforced. Observers therefore have to be attentive to facts on the ground. Civil society organisations and journalists in the Amazon working for publications such as InfoAmazonia and O Eco are particularly good sources of information. There is some transnational support for these journalists. For example, the Pulitzer Centre is administering a Rainforest Journalism Fund, financed by the Norwegian government, which gives grants to journalists reporting on deforestation. Brazil’s foreign minister Ernesto Araújo claims that initiatives such as the 2015 Paris Agreement are liberal, “globalist” and part of a gigantic “cultural Marxist” propaganda machine. From this perspective, international NGOs and foreign states are violating Brazilian sovereignty by interfering in the Amazon.  But this is a smokescreen. In the Paris Agreement the Brazilian government voluntarily committed to reduce its greenhouse gas emissions by 37% by 2025 and 43% by 2030, with 2005 as the baseline year. The Brazilian Climate Change Forum that produced this commitment had input from 340 different government entities, businesses, NGOs, and academics. And the country already has various advantages when it comes to making the transition to a low-carbon economy, including relatively clean energy and 60m hectares of degraded pasture land that could be reforested.  Preserving the Amazon rainforest is of fundamental importance to the planet, and there are many people in Brazil who want to do that. They reject the notion that development and environmental protection are mutually exclusive, and support reorienting the Amazonian economy towards sustainable livelihoods. It remains to be seen whether their vision will prevail in the years to come."
"Tackling climate change will require huge changes in society. Decarbonising energy, restoring habitat and making food supply sustainable are all critical, but methods for motivating these actions have typically taken the wrong approach – by highlighting the urgency of the issues and the disastrous consequences of failing to act. Research increasingly suggests that trying to promote behavioural change through fear can be counterproductive, leading to anxiety or depression that results in an issue being avoided, denied or met with a sense of helplessness. However, in education, news and fiction, stories with positive role models and which focus on the positive outcomes of solutions are much more likely to inspire action to solve it. I set out to explore the impact of such stories. As part of my research, 91 volunteers were given two stories to read, each concerned with the negative impacts of climate change: one about a woman caught in a flood and the other set at the end of the world.  The same readers were also exposed to two positive stories: one about a terrorist planting a flower bomb, which populates a bare area with flowers, the other about a young boy who, having watched Blue Planet, takes to collecting plastic to stop it entering the oceans – starting with his fish tank. Afterwards the readers were asked how the stories made them feel and to reflect on what kinds of behaviours they inspired. While the negative stories motivated action for a few, most said they were discouraged. “I’d rather not think about it,” said one. “It made me angry and I switched off,” said another. Many also reported a sense of passive despair. “I felt hopelessness. If indeed, the heavy rain was caused by climate change, what can we do about it?”  However, there were no signs of avoidance among readers of the positive stories. “It made me want to flower bomb land and do something positive and I felt happier after reading it,” said one reader.  “I felt inspired by the way the characters behaved … [the story] made me think about what I could do.” This is concerning because almost all stories set in the future, whether in books, films or TV shows, are dystopian. The popular TV show Black Mirror tells cautionary tales about modern life and technology with often terrifying consequences. These stories elicit anxiety, pessimism and a feeling of passive fatalism.  I realised from my research that we desperately need cultural offerings with positive visions of what a sustainable society might look like, to inspire hope and positive change. The University of Southampton runs writing competitions that ask people to read about green solutions and integrate them into stories. These ideas include replacing how much people buy, represented as GDP – the current measure of how successful society is – with a measure of well-being. Another looks at the potential of a “sharing economy”, in which more people borrow goods others have without needing to buy more themselves. It can be hard for politicians to support green policies such as these when green issues evoke catastrophe in the minds of voters they’d rather not think about. Reframing issues in terms of their solutions and highlighting them through engaging characters and stories might be a more effective way to encourage change. One winning short story was Come Help Me by Nancy Lord – a romance about an American fisherman and a Russian marine scientist. The protagonist is inspirational and proactive: he spots a tension between the scientists concerned with the marine environment and the fisherman who needs to make a living. The writer finds a way to help them work together. We also loved the runner up, The Buildings are Singing, by Adrian Ellis, which made us laugh out loud.  This short story imagines a future world where buildings are alive – covered with photosynthesising plants which create energy, light and shade for the occupants. The flora operates an artificial intelligence system which helps occupants live sustainably. Insects drawn to the foliage become nourishing protein bars and life for the humans is low carbon and almost utopian – unless you do something wrong.  Some stories are specifically about sustainable societies, whereas others showcase ideas that would seem radical in otherwise familiar tales, such as Just in Case, which imagines a society where we borrow rather than buy much of our stuff. The woman who runs the “library of things” in the story, plays matchmaker with two customers who she can tell are compatible by their borrowing patterns. The transition to a sustainable society requires profound changes, but to imagine how all of these aspects can come together is currently the domain of creative fiction. If we want a better world then the first step is to imagine one."
"

Representatives of some 160 nations are gathered in Kyoto, Japan, this week to negotiate an international treaty to control emissions of greenhouse gases. While the summit has all the trappings of a substantive event, the gathering in Kyoto is more an exercise in public relations than in serious statecraft. Hot air, rather than cold reason, will dominate the conference and political sleight of hand will be its product. 



That’s because, according to the International Panel on Climate Change (the United Nation’s body of experts devoted to the study of global warming and known as the IPCC), even the most aggressive and costly proposals on the table this week would shave only a fraction of a degree off temperature increases projected by computer models for the year 2100. To achieve those emissions reductions, each American would have to pay an additional $1,000-$3,000 annually in higher energy prices. Such proposals would, according to Yale economist and sometime Clinton adviser William Nordhaus, take us back to the days of semipermanent energy crises like those of the 1970s. Both greenhouse alarmists and skeptics agree that actually preventing the global warming projected by computer models would require the world to reduce carbon dioxide emissions by between 60 percent and 80 percent. Only by virtually abandoning the use of oil, gas and coal could we achieve such reductions. President Clinton’s assurances about “free lunch” climate change policies notwithstanding, nobody is proposing any real policy to prevent climate change because no one wants to usher in a permanent global depression. 



The idea, then, is to get the world to commit to a slow‐​motion control policy, one that would ease us into higher energy costs, a reordered industrial world and, as National Public Radio reporter Richard Harris puts it, “a whole new society” structured around less energy use. 



But if the computer models are correct about global climate change (the data thus far are inconclusive), what choice do we have? Isn’t it prudent to hedge our bets now with a control strategy in order to avoid far more costly economic crash planning later? Well, no. All indications are that the “cure” for global warming is far worse than the “disease” of rising temperatures. 



First, only about 2 percent of America’s economy is sensitive to weather conditions. No matter how ruinous climate change might be, it couldn’t possibly have a serious long‐​term impact on the United States. Even the most alarmist projections of ocean rise (about 3 feet or so) are trivial. If Amsterdam could figure out a way to hold back an even larger sea rise hundreds of years ago, it’s clear a wealthier and more technologically advanced United States could counter a 3‐​foot rise. Foreign aid to help poorer countries adopt would be far less expensive than control policies. 



Second, it’s not altogether clear that a warmer world would be a less habitable world. A temperature rise of 4.5 degrees Fahrenheit (the median computer‐​predicted result of a doubling of atmospheric carbon dioxide in the next 100 years) was exactly what occurred about a thousand years ago (A.D. 850‑1300) in a period climatologists refer to as “the little climate optimum” (note that they don’t refer to it as “the little climate hell”). The result? A longer growing season, rapid economic development, a minor cultural renaissance, an expansion of fertile crop and forestland and a decrease in mortality rates.



Since the data indicate that the small amount of warming we have detected over the last 100 years has largely been confined to winter evenings in the far northern latitudes, we have every reason — both empirical and theoretical — to believe warming would be a benign, not a deleterious, event. 



There are still open questions about how much if anything man has had to do with the slight warming detected over the past 100 years and how much warming might eventually occur (the IPCC estimates range from insignificant to moderately significant). The IPCC report itself states it will be another decade or so before scientists will know for certain. So why not wait? Nature magazine reported last year that waiting 20 years for better scientific information before acting will only cost us .36 degree Fahrenheit, at worst, over the next 100 years. 



In the face of this kind of uncertainty, the best “insurance policy” we could buy is one that increases the amount of wealth at society’s disposal to handle whatever problems might occur in the decades to come. Impoverishing society today to avoid a very uncertain problem tomorrow would harm, not help, future generations.
"
"Industrial greenhouse gas emissions in Australia have risen 60% in the past 15 years, putting the country on a path that, if it continues, will lead to it missing the target set at the Paris climate conference. That is the conclusion of an analysis by energy and carbon consultants RepuTex, which examined the rise in industrial carbon pollution – including from oil and gas extraction, mining and large-scale transport – in the period covered by Australia’s 2030 emissions target, starting in 2005.  The resulting report highlights the failure of the Coalition government’s “safeguard mechanism” policy, which was promised to limit carbon pollution rises so cuts paid for by taxpayers through the emissions reduction fund, the main national climate policy, were not just wiped out by increases elsewhere. RepuTex found emissions in sectors covered by the safeguard mechanism had grown from 89m tonnes in 2005 to 142m tonnes in 2019, and were projected to reach 187m tonnes by 2030. If the projection is correct, industrial emissions will have increased 110% over the period in which Australia has promised, as part of the Paris agreement, to cut national emissions by 26-28%. RepuTex’s executive director, Hugh Grossman, said emissions from electricity generation had fallen 9% since 2005, but those gains were being eroded by unchecked carbon pollution in other areas. “While we have seen record levels of investment in renewable energy technology in the electricity sector, the industrial sector has been largely missing from the national emissions reduction challenge,” he said. Grossman said unrestrained industrial emissions remained an issue many MPs did not want to acknowledge or discuss. They were expected to pass those from electricity in 2023 to be the nation’s largest polluting sector. “Ultimately all sectors need to play a part in long-term decarbonisation, to varying degrees, either by reducing emissions or just offsetting emissions growth,” he said. The analysis found the biggest relative surge in emissions since 2005 had been in the oil and gas industry, reflecting the rise of one of the world’s biggest liquefied natural gas (LNG) export industries over the past decade from a low base. LNG emissions were found to be 621% higher than 15 years ago. Other significant increases were from direct combustion at mining sites, venting of fugitive emissions in fossil fuel extraction, and metals, chemicals and minerals processing. The safeguard mechanism started operating in 2016. It sets a pollution limit, known as a baseline, for every industrial facility across the country that emits more than 100,000 tonnes annually. The limit was initially based on either a facility’s historic emissions or an independent forecast of future emissions. Companies that exceeded their baseline were expected to buy carbon credits to offset the additional emissions, or pay a penalty, but in practice this has applied in only some cases. Many facilities have applied for, and been granted, an increased limit. Last week, BHP coal and iron ore mines in Western Australia and Queensland, Alcoa’s Portland aluminium smelter in Victoria and a Boggabri coalmine in New South Wales were each given the green light to emit more. An analysis by RepuTex found the allowed emissions approved by the Clean Energy Regulator increased 32% between 2015 and 2018. Not all companies emitted up to their baseline. Actual emissions under the scheme increased 12% over that time. Recently adjusted national emissions data suggests pollution has stayed flat since 2014 under the Coalition. As RepuTex suggests, there have been decreases from electricity, and agriculture due to the drought, but increases from industry and transport. While the government claims it is meeting and beating climate targets, emissions this year are expected to be only 0.3% lower than 20 years ago. The national target over that time is a 5% cut. In 2016, the then environment minister, Greg Hunt, said the safeguard mechanism would ensure emissions cuts contracted through the emissions reduction fund were not offset by significant increases above business-as-usual levels elsewhere in the economy. The description later changed. A government climate policy document released before last year’s election said the mechanism required Australia’s largest emitters to “measure, report and manage” their emissions. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on emissions intensity – how much they expect to emit per unit of production. As currently proposed, if companies lift production they will be able to increase carbon pollution without penalty. In theory, these limits per unit of production could be reduced over time to cut emissions and encourage a shift to clean practice - a change Labor proposed without detail before the 2019 election. The Morrison government is divided on climate action and the future of coal-fired power. It has commissioned a review of its climate policies led by the businessman Grant King, and has promised it will this year release a technology investment roadmap, an electric vehicle policy and a long-term emissions strategy. Official projections suggest national emissions will be about 16% less than 2005 levels by 2030 under current policies – not enough to meet Australia’s target of a 26-28% cut without the use of a controversial accounting measure that was criticised by other countries at the UN climate conference in Madrid in December. The government is facing calls from business leaders and the energy industry to consider a climate action bill by the independent MP Zali Steggall that includes a proposal for a target of net zero emissions by 2050, an emissions budget, and assessments every five years of national climate change risk."
"
Note: This is my analysis of a new paper by Joe D’Aleo, I’ve tried to simplify and explain certain terms where possible so that  it can reach the broadest audience of readers. You can read the entire paper here.
Joe D’Aleo, an AMS Certified Consulting Meteorologist, one of the founders of The Weather Channel and who operates the website ICECAP took it upon himself to do an analysis of the newly released USHCN2 surface temperature data set and compare it against measured trends of CO2, Pacific Decadal Oscillation, and Solar Irradiance. to see which one matched better.
It’s a simple experiment; compare the trends by running an R2 correlation on the different data sets. The result is a coefficient of determination that tells you how well the trend curves match. When the correlation is 1.0, you have a perfect match between two curves. The lower the number, the lower the trend correlation.



Understanding R2 correlation



R2 Coefficient
Match between data trends


1.0
Perfect


.90
Good


.50
Fair


.25
Poor


 0 or negative
no match at all


If CO2 is the main driver of climate change this last century, it stands to reason that the trend of surface temperatures would follow the trend of CO2, and thus the R2 correlation between the two trends would be high. Since NCDC has recently released the new USHCN2 data set for surface temperatures, which promises improved detection and removal of false trends introduced by change points in the data, such as station moves, it seemed like an opportune time to test the correlation.
At the same time,  R2 correlation tests were run on other possible drivers of climate; Pacific Decadal Oscillation (PDO), Atlantic Multidecadal Oscillation (AMO), and Total Solar Irradiance (TSI).
First lets look at the surface temperature record. Here we see the familiar plot of temperature over the last century as it has been plotted by NASA GISS:
 
The temperature trend is unmistakeably upwards, and the change over the last century is about +0.8°C. 
Now lets look at the familiar carbon dioxide graph, known as the Keeling Curve, which plots atmospheric CO2 concentration measure at the Mauna Loa Observatory:


CDIAC (Carbon Dioxide Information Analysis Center – Oak Ridge National Lab) also has a data set for this that includes CO2 data back to the last century (1895) extracted from ice core samples.  That CO2 data set was plotted against the new USHCN2 surface temperature data as shown below:


A comparison of the 11year running mean of the USHCN version 2 annual mean temperatures with the running mean of CO2 from CDIAC. An r-squared of 0.44 was found.
The results were striking to say the least. An R2 correlation of only 0.44 was determined, placing it between fair and poor in the fit between the two data sets.

Now lets look at other potential drivers of climate,  TSI and PDO.
Scafetta and West (2007) have suggested that the total solar irradiance (TSI) is a good proxy for the total solar effect which may be responsible for at least 50% of the warming since 1900. To test it, again the same R2 correlation was run on the two data sets.

In this case, the correlation of TSI to the surface temperature record is better than with CO2, producing an R2 correlation of 0.57 which is between fair and good.
Finally. Joe ran the R2 correlation test on PDO, the Pacfic Decadal Oscillation. He writes:
We know both the Pacific and Atlantic undergo multidecadal cycles the order of 50 to 70 years. In the Pacific this cycle is called the Pacific Decadal Oscillation. A warm Pacific (positive PDO Index) as we found from 1922 to 1947 and again 1977 to 1997 has been found to be accompanied by more El Ninos, while a cool Pacific more La Ninas (in both cases a frequency difference of close to a factor of 2). Since El Ninos have been shown to lead to global warming and La Ninas global cooling, this should have an affect on annual mean temperature trends in North America.
This PDO and TSI to surface temperature connection has also been pointed out in previous post I made here, for former California State Climatologist, Jim Goodridge. PDO affects the USA more than the Atlantic cycle (AMO) because we have prevailing westerly wind flow.
Here is how Joe did the data correlation:

Since the warm modes of the PDO and AMO both favor warming and their cold modes cooling, I though the sum of the two may provide a useful index of ocean induced warming for the hemisphere (and US). I standardized the two data bases and summed them and correlated with the USHCN data, again using a 11 point smoothing as with the CO2 and TSI. 
This was the jackpot correlation with the highest value of r-squared (0.83!!!). 


An R2 correlation of 0.83 would be considered “good”. This indicates that PDO and our surface temperature is more closely tied together than Co2 to surface temperature by almost a factor of 2.
But he didn’t stop there. He also looked at the last decade where it has been commonly opined that the Top 11 Warmest Years On Record Have All Been In Last 13 Years to see how well the correlation was in the last decade:

Since temperatures have stabilized in the last decade, we looked at the correlation of the CO2 with HCSN data. Greenhouse theory and models predict an accelerated warming with the increasing carbon dioxide. 


Instead, a negative correlation between USHCN and CO2 was found in the last decade with an R or Pearson Coefficient of -0.14, yielding an r-squared of 0.02. 


According to CO2 theory, we should see long term rise of mean temperatures, and while there may be yearly patterns of weather that diminish the effect of the short term, one would expect to see some sort of correlation over a decade. But it appears that with an R2 correlation of only 0.02, there isn’t any match over the past ten years.
As another test, this analysis was also done on Britain’s Hadley Climate Research Unit (CRU) data and MSU’s (John Christy) satellite temperature data:

To ensure that was not just an artifact of the United States data, we did a similar correlation of the CO2 with the CRU global and MSU lower tropospheric monthlies over the same period. We found a similar non existent correlation of just 0.02 for CRU and 0.01 for the MSU over troposphere. 


 So with R2 correlations of .01 and .02 what this shows is that the rising CO2 trend does not match the satellite data either.
Here are the different test correlations in a summary table:

And his conclusion:

Clearly the US annual temperatures over the last century have correlated far better with cycles in the sun and oceans than carbon dioxide. The correlation with carbon dioxide seems to have vanished or even reversed in the last decade. 
Given the recent cooling of the Pacific and Atlantic and rapid decline in solar activity, we might anticipate given these correlations, temperatures to accelerate downwards shortly. 

While this isn’t a “smoking gun” it is as close as anything I’ve seen. Time will give us the qualified answer as we have expectations of a lower Solar Cycle 24 and changes in the Pacific now happening.
References: 
US Temperatures and Climate Factors since 1895 , Joeseph D’Aleo, 2008
Persistence in California Weather Patterns,  Jim Goodridge, 2007
Phenomenological reconstructions of the solar signature in the Northern Hemisphere surface temperature records since 1600  Scafetta and West, 2007
The USHCN Version 2 Serial Monthly Dataset, National Climatic Data Center, 2007


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea177569c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Boris Johnson has set out his vision for forging a new global consensus on the climate crisis promising “we will crack it”, amid news that he approached former prime minister David Cameron to lead the UK’s preparations for a crucial summit. Johnson has brought forward the UK’s phaseout of diesel and petrol vehicles by five years to 2035, and hastened the phaseout of coal-fired power by a year to 2024. He reaffirmed the UK’s pledge to switch to a net-zero emissions economy by 2050, and urged other nations – without naming any – to do the same. “I hope that we can as a planet and as a community of nations get to net zero within decades,” Johnson said at the COP 26 launch on Tuesday. “We’re going to do it by 2050, we’re setting the pace, I hope everybody will come with us. Let’s make this year the moment when we come together with the courage and the technological ambition to solve manmade climate change and to choose a cleaner and greener future for all our children and grandchildren.” But the troubled start to the UK’s presidency of this year’s crunch UN climate talks with the sacking of Claire O’Neill as COP president has unsettled observers, who are hoping Johnson will play a pivotal role in bringing world leaders together with a new resolve to drastically reduce greenhouse-gas emissions before it is too late. On Tuesday night it emerged that Johnson had approached Cameron to take over from O’Neill but he declined the role. Lord Barker of Battle, who served as an energy and climate change minister under Cameron and is a close friend and ally of the former prime minister, said he understood reports that he was offered the role to be correct. “My understanding is that he felt it was just a little too soon for him personally to come back into a frontline political role,” he told BBC Two’s Newsnight. William Hague was also reported to have been offered it but also declined. The appointment last year of O’Neill as president – the official who will take the leading role in convening and chairing the fortnight-long UN talks, and seeing them through to a deal which will require the consensus of 196 nations – appeared to give the UK a good headstart in the talks. O’Neill was formerly an energy minister in the Department of Business, Energy and Industrial Strategy, below the secretary of state level but with rights to attend Theresa May’s cabinet meetings. She resigned as the MP for Devizes, a staunchly Conservative seat, after taking on the role, saying she wanted to concentrate on the COP presidency. Brexit was also a major factor. O’Neill campaigned to remain in the referendum, and was scathing of hard Brexiters, whom she accused of being “hysterical” and “like jihadis”. She rebelled to vote for a Brexit bill amendment that would give parliament the final say on any deal to leave the EU, though she voted with her party on other legislation. She wished Johnson good luck with Brexit when she gave notice last September, well before a election had been called, that she would not contest the seat again. O’Neill was a surprise choice for the president’s role, but the political turmoil last autumn made her a relatively safe pick: with the government preoccupied with Brexit, and Johnson both struggling to get the general election he wanted and filling the cabinet with pro-Brexit supporters of his leadership campaign, to appoint a serving minister would have been tricky. Other possible candidates included Zac Goldsmith, whose shaky reelection prospects were borne out by his defeat in Richmond, or members of the House of Lords. O’Neill’s experience as a transport minister and energy minister seemed to offer some assurance. However, her record was also spotted. In November 2018 three unions wrote to the Department for Business, Energy and Industrial Strategy to raise allegations of shouting and swearing at civil servants. In her post-sacking letter this week, O’Neill rebuffed those allegations. The Guardian was given mixed reports of her conduct in the UN negotiations by people present. She is said to have created a good impression among some countries, and at some meetings at last December’s climate conference in Madrid. However, she fell out with some senior officials in the UK, and gave conflicting messages about the UK’s position and strategy at the talks. O’Neill’s letter to Johnson also gives clues to the concerns over her conduct. In it, she is strongly critical of the COP structure, rules and bureaucracy, but without showing much awareness of what supporters see as the value of the process – which gives all nations an equal voice, and progresses by consensus – or respect for the negotiators, many of whom have served their governments for decades. “COP is difficult, but you need to understand it and work within it,” said one long-time participant in the UN talks. “The French did [when they led the 2015 Paris agreement].” Another former high-level diplomat and COP veteran said: “A good COP president makes all the difference between success and failure. They direct the negotiations, they play the key role in determining the outcome.” It is also clear, however, that O’Neill has suffered from a lack of support within government, and a lack of focus from the prime minister on the UK’s COP 26 plans. She complained that the cabinet sub-committee on climate, supposed to be chaired by Johnson, has not yet met. The relationship between the COP 26 unit and other government departments is also unclear in parts, and it is not apparent that the Foreign and Commonwealth Office has been pushing climate to the top of the agenda in its embassies around the world, as the French did before Paris. Bedevilling all this has been Brexit. Several NGOs and developing country representatives told the Guardian about serious concerns that the UK could not give COP 26 the attention needed while still working out its new relationships and trade deals. Some observers see a potential conflict of interest, as British diplomats seek at once to gain support for a COP 26 resolution that will require other countries to set out stretching goals on cutting emissions, while also negotiating post-Brexit trade deals. “I am very concerned about how they can play this,” said the head of one international civil society group. “This is a very delicate dance.” Paul Bledsoe, a former climate advisrr to Bill Clinton, said: “Sacking O’Neill and making the post more directly reportable to Number 10 increases the pressure on Johnson to appoint an aggressive climate policy figure, especially one who will actually hold the Chinese and Americans accountable.” With O’Neill’ssacking, climate activists and COP participants are hoping that now Johnson and his government can move on to forge a clear strategy and timetable for gaining the support and buy-in they need from capitals across the world. But they warn that the prime minister is running out of time. “The prime minister has given a very clear and strong message, which is good,” said Lord Stern, the climate economist. “He has made a personal commitment, and that is now crystal clear. Now we need someone in a very senior position to be COP president. The challenge now is to accelerate.” • This article was amended on 6 February 2020 because an earlier version referred to O’Neill’s “resignation letter”. As the article and that letter makes clear, she was sacked from her post."
"
 
Click for magnified view of the sun showing the most recent spot.
Sunspot 987, 988, and now newly emerging 989 are shown above.
With all being near the equator, they are still a cycle 23 spots. A cycle 24 spot would be at a much higher latitude.
The most recent magnetogram shows them to have the magnetic polarity of cycle 23 spots, in addition to being near the equator.

Cycle 24 remains late. There was one sunspot of high latitude and reversed magnetic polarity on January 4th, 2008, but none have been seen since:

Click for a larger image
UPDATE 2: The solar holographic image shows a potentially large spot on the far side of the sun, we’ll have to wait until it comes around to see what it is. The method is not always perfect.

Darker area is the far side of the sun.
Seismic waves propagating through the sun are used to image potential spots on the far side. Here is a description of how it is done.
UPDATE 3:
It looks as if the spot seen yesterday on the far side of the sun via the holographic technique has disappeared. As I said “The method is not always perfect.”

The two spots above are earthward, 987, and 988.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea04b9a7d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The lord mayor of Sydney, Clover Moore, has demanded the prime minister, Scott Morrison, explain to parliament how his energy minister, Angus Taylor, came to rely on a false document that was used to accuse her of hypocrisy on climate change, after the Australian federal police declined to investigate. The AFP announced on Thursday it would not investigate the doctored document scandal further, saying it was unlikely a commonwealth offence had been committed and there was no evidence that Taylor had been involved in falsifying the document.  The decision has outraged Moore and the opposition, which had initially referred the matter to the NSW police. Moore said she still did not know how Taylor came by a page of the City of Sydney’s annual report, which contained incorrect figures and which showed that the City of Sydney had spent $15m on travel. The real figures, available online, show it was a fraction of that. “He didn’t give me any explanation about how that happened. I still don’t know why the minister did that, why he signed that letter,” she told ABC radio. The incorrect figures were included in a letter Taylor sent to Moore criticising her declaration of a climate emergency and suggesting she curtail her travel instead. The letter was then leaked to the Telegraph, which mocked her declaration publicly. Moore said that Taylor’s apology, made in parliament a month ago, was not sufficient. “Does he not think that climate change is important? That my reputation is not important?” Moore said the prime minister should now investigate the matter and report to parliament, as the matter still raised important questions of ministerial conduct. But she added that she would not be pursuing the matter further and would extend focus on the business of the city. The City of Sydney announced on Thursday that it would bring forward its goal of reaching zero net emissions from 2050 to 2040 – ten years earlier. In a clear criticism of the Morrison government’s record and Taylor, the minister for emissions reduction, she said: “It is incumbent on us as a rich country to do all we can to reduce emissions so we can then put pressure on the large emitters to do their part.” Taylor has welcomed the news that the AFP would not be investigating further and accused Labor of a “track record of using police referrals as a political tool”. “The leader of the opposition [Anthony Albanese] and shadow attorney general [Mark Dreyfus]’s pursuit of this matter is a shameful abuse of their office and a waste of our policing agencies’ time,” he said in a statement. Dreyfus and the shadow climate change minister, Mark Butler, said that “serious questions remain unanswered” about the scandal because “two police investigations have now failed to clarify where Angus Taylor got his dodgy figures from”. Guardian Australia is currently seeking a review of a freedom of information request that sought documents. Taylor’s office refused to release several documents, including one which was described as an email from “an external third party”. It was withheld on the grounds that it would adversely affect the business affairs of that person."
"

**Presidential Power**   
  
  
Op‐​Ed: “New President Won’t Tame Presidential Power,” by Gene Healy in the _Orange County Register._



After seven years of an administration that has recognized few, if any, limits on executive power, it’s only natural that many people look to the Obama‐​Biden ticket to put the presidency back in its proper constitutional place.



Article: “Obama’s the Candid Candidate on Energy,” by Jerry Taylor and Peter Van Doren in _Forbes._



Sen. Obama’s frank confession about what his climate change policies will mean to electricity consumers is one very good reason why so many conservative and libertarian intellectuals are gravitating toward his candidacy. It’s not that right‐​wing thinkers necessarily endorse his climate change policies. It’s that right‐​wing thinkers are increasingly tired of Republican hypocrisy and make‐​believe policy fights.



Article: “Obama’s Tax Deceptions,” by Alan Reynolds in _National Review Online_. 



Barack Obama famously claims, “I’ll give a tax break to 95% of workers and their families.” The Obama team never explained that figure, because they made it up.…Obama said, “If you work, pay taxes, and make less than $200,000, you’ll get a tax cut.” That too is flatly false. Single workers who make more than $80,000 (or joint returns above $155,000) would not get a tax cut under Obama’s plan.



 __  
  
  
Podcast: “The Obama Agenda: Free Political Speech,” featuring John Samples 
"
nan
"
Share this...FacebookTwitterFirst at Twitter, aspiring meteorologist Chris Martz posted a chart of tornado activity since 1954. Contrary to what climate alarmists have claimed, tornado frequency has not trended upwards:

Hurricane claim involves “massive error”
While on the topic of violent weather, the Washington Post recently published another alarmism-fraught article on the alleged increasing strength of hurricanes due to global warming and how there’s now a “slower decay of landfalling hurricanes in a warming world.”
However, Dr. Roger Pielke Jr. responded saying that there was a “massive error” in the new Nature paper, which  the Washington Post was citing.
“Says it shows hurricanes decaying slower over land post-landfall (more damaging). But they forgot to remove storms that landfall & then go back over the ocean,” Pielke tweeted.
In the Twitter thread, Dr. Ryan Maue pointed out that some of the hurricanes plotted were not “a typical example” of inland hurricane decay.
No real trend


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Examining global landfall cyclone data since 1970, There’s no observed noteworthy trend: Dr. Ryan Maue also tweeted a chart on tropical cyclone activity over the past 50 years:

Declining in some regions
The often heard claim of more frequent and intense cyclones is not supported by the data. Another paper by Zhao et al from 2018 in fact shows Western North Pacific cyclones becoming less frequent since the start of the century.

Moreover, data from the Japan Meteorological Agency (JMA) show that the number of typhoons formed since 1951 has trended somewhat downward:

So it’s a mystery as to why the Washington Post and Nature would create alarm and make false claims over tropical cyclone activity. That’s science no one should consider, let alone follow.


		jQuery(document).ready(function(){
			jQuery('#dd_baa535913b025a18770bfce7c5c773ee').on('change', function() {
			  jQuery('#amount_baa535913b025a18770bfce7c5c773ee').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Calling cycle 24, calling cycle 24……where are you? 

Image from SOHO, inset added by the author
The SIDC in Belgium just issed an end to their “all quiet alert”
:Issued: 2008 Feb 26 1255 UTC
:Product: documentation at http://www.sidc.be/products/quieta
#——————————————————————–#
# From the SIDC (RWC-Belgium): “ALL QUIET” ALERT                     #
#——————————————————————–#
END OF ALL QUIET ALERT
………………….
The SIDC – RWC Belgium expects solar or geomagnetic activity to
increase. This may end quiet Space Weather conditions.
The first new sunspot in weeks has emerged today. The spot that has emerged is small and on the equator, so it appears that it is a cycle 23 spot rather than one from the cycle 24 that is gave one spot on January 8th, signaling a start of cycle 24, but has given no cycle 24 type spots since.
Based on what we know about the sun, a cycle 24 spot would be reverse polarity to cycle 23 spots and high latitude. The longer cycle 24 continues to delay producing its spots heightens the concern that we may be in for a longer inactive period on the sun, such as a Dalton type minimum.
A thought occurred to me. Given that all of the sunspots seen recently during our solar minimum are very small, I wonder if they could be resolved at all with the primitive equipment available during periods like the Maunder Minimum? Today we have satellites and advanced solar telescopes with hydrogen spectra filters that are available to amateurs, so catching any sunspot, even if small, is now easy. In fact this sunspot was was first noted by an amateur observer, Howard Eskildsen, in Ocala, FL, showing that amateurs still have a role in science.
It makes me wonder if an extended minimum really isn’t an absence of sunspots altogether, but just an absence of larger easily observable sunspots.  It is possible that primitive equipment of the period could not easily resolve smaller sunspots.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0fe56b2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Unprecedented wildfires have recently devastated California, the Amazon, southern Europe, Siberia and Australia. It’s safe to say that we’ve entered the era of the climate-fuelled megafire. But because fire conditions depend on local vegetation, topography and climate, each of these great conflagrations is different. Australia’s bushfires of the last four months have been true megafires, creating their own weather and becoming so vast in their impact that more than half of all Australians have been directly affected by them. As I write, fires continue to burn around Canberra, and though rain has begun to fall in northern New South Wales, 17 are “yet to be contained” according to the fire service. Meanwhile, what is traditionally the worst part of the fire season for Victoria and South Australia is just commencing. Conditions have been so severe that firefighters have often been unable to stop fires joining up, generating massive dry thunderstorms that spread fire with thousands of lightning strikes.  So far the fires have burned an area around the size of England, killed more than 30 people and destroyed about 6,000 buildings. They have left deep psychological scars, and while it seems impossible to shift the government’s disastrous climate policies, the fires will alter the way that Australians view themselves and their country. There’s a British saying that fire is a good servant but a bad master. But in Australia, with its unique vegetation and climate, fire can also be a terrifying predator. Like all good predators, it remains hidden until it’s ready to strike, so even in this fire-plagued year, most Australians have not seen the flames that lurk in the forest, taking life seemingly at random. But many have smelt its stench. Sometimes we wake at night to thickening smoke, and lie there wondering where the beast is prowling. Some mornings we peer out the window and decide that it is not a safe day to be outside. This year fire struck Australia when its guard was down, trapping tens of thousands of people while they were enjoying their summer vacations. The sight of Australians disembarking from warships on naval regulation gangways reminded us that, in this new Australia, the fiery beast that lurks in the forest can make anyone a temporary climate refugee. Increasingly, Australian fire harasses its victims for weeks or months before striking. You prepare for its onslaught, only for the flames to turn aside at the last moment. You prepare the buckets, hoses and other equipment again and again, while fire weakens you with its smoke and debilitating heat. It can also trap you, as if you’re a mouse in a corner. You wait, with fire all around, and all exits cut, your fate in the hands of the wind. If you are lucky enough to survive, you can remain trapped for weeks as you wait for roads to be cleared and made safe. When the predator finally does pounce, it does so with extraordinary fickleness. One house is left standing, while all around are smouldering ashes; one fire truck is picked up by a fire tornado and overturned, while nearby no wind is felt. Why me, both the fortunate and unfortunate ask? The psychological impact of such experiences is profound. I’ve lost a house to bushfire, and successfully defended another. When I look into the eyes of the fire-hunted, I see the same expression I’ve seen in the eyes of returned soldiers or traumatised refugees. Bushfire triggers a primeval fear, one that has been plaguing our ancestors since the first ape was plucked from its family, carried away and consumed by a big cat. Few things matter as much to Australians as their homes and communities. More than 2,000 homes have been destroyed so far by the fires. And even when those affected do have insurance, history shows that most people are underinsured. The fires will doubtless also spark new regulations, which will make rebuilding more expensive. Some people now face the prospect of leaving or remaining homeless in stricken regional economies. All of us are less resilient to future shock – and future shocks there will be. Natural climate variation might have brought a year as hot as 2019 to Australia once every 360 years, but greenhouse gas pollution has increased that frequency to one in eight. And every day the coalfires burn in Australia and elsewhere, we’re shortening those odds – adding to the severity of the fires of tomorrow. Right now, many Australians are simply exhausted. They have been deprived of their summer vacations and beset by debilitating smoke. And as the immediate trauma slowly fades, other impacts will emerge. Around a quarter of Australians are already reporting health effects from inhalation of bushfire smoke, and over the longer term more and more consequences will be felt. Some medical researchers fear that the consequences of smoke exposure from this year’s fires will be felt for generations. Any credible response to such a massive national catastrophe needs to be proportionate to the danger. Instead the government response has been risible. It wants to plant more trees to capture carbon, but also wants more coal burned and more forest cleared, so there will be less to catch fire when the next big fire comes. Its previous climate denialism has been revealed as a catastrophic error, but it remains paralysed in the face of a disaster it helped create. If Australia was being threatened by an external enemy, Scott Morrison’s government would be doing everything within its power to recruit allies. It would put the economy on a war footing, and raise arms. But when it comes to climate-fuelled catastrophes, it claims that there’s nothing it can do: Australians have to remain among the world’s worst greenhouse gas polluters, and keep spoiling international efforts to address the problem. To do otherwise, the government says, would be “economy wrecking”. But guess what? There is a solution – burn more fossil fuels, especially gas. To an increasing number of Australians these lies are nauseating. The words of our prime minister sound like the words of a traitor.  What does one do when immense evil is occurring in one’s country? As far as swift climate action is concerned, all good choices have gone up in smoke, and only difficult, divisive options remain. Australians marched against the Vietnam war, and prevailed. But they also protested against the horrific treatment of asylum seekers, and so far have failed. Now we shall see how communities, businesses and politicians respond to this crisis in “the lucky country”."
"
Share this...FacebookTwitterMelting sea ice is causing sea levels to rise 49 micorons per year (3/16 of an inch over 100 years), according to research published in Geophysical Research Letters. Read more here:  http://www.sciencedaily.com/releases/2010/04/100428142258.htm
Share this...FacebookTwitter "
"We have just 12 years left to reduce emissions and achieve the Paris Agreement’s highest ambition of limiting warming to 1.5°C.  We have been warned, repeatedly, of the high stakes of our present climate gamble. If we continue on our current course, radical solutions are going to be needed sooner rather than later. Champions of solar radiation management (SRM) say this is the answer we’ve been looking for. SRM techniques cool the planet by reflecting sunlight away from it. The most discussed SRM technique involves continuously injecting tiny reflective particles – most often, sulphur – into the stratosphere to evenly cover the planet and shield us from the sun’s rays. Fleets of drones, or sprayers attached to enormous tethered balloons, could deliver these particles. Spray a little more, and the temperature drops; spray a little less, and the temperature rises.  Unlike the other dominant form of climate engineering, carbon dioxide removal, SRM does not extract greenhouse gases from the atmosphere. Rather, it masks the warming caused by those gases. Advocates say that SRM could give humanity a decent shot at getting its act together on mitigation. They also say the costs of SRM would be a fraction of the costs of the climate impacts we are facing without this technology. However, there are reasons for caution, if not outright scepticism, about the wisdom of this techno-fix for climate change. One important worry relates to who governs research into SRM, given the risks it presents and its seductiveness as a “solution” to the climate crisis. There are some proposed self-regulations by SRM researchers, and more attention is being paid to international governance of SRM research, but a vacuum of governance remains. To fill this vacuum in morally acceptable ways requires establishing governance to address the danger of sleepwalking into SRM “lock-in”, where a society becomes locked-in to a technology that has become too costly to abandon, even though alternative technologies may be better. Once SRM deployment has begun, a status quo bias could quickly establish the new normal of a geoengineered world. It could combine with environmental generational amnesia, a phenomenon where generations of people forget previous environmental states and so don’t notice their deterioration. Those with their hands on the global thermostat – political leaders focused on short electoral cycles, and corporate leaders feeding the bottom line – would be tempted to continuously extend deployment to enable more and more business as usual. Imagine we become locked-in to a permanent geoengineered world in which SRM is being widely used but catastrophic climate change has been avoided. What’s the problem?  One potential big risk of widespread deployment of SRM is something called termination shock, where the technology is abruptly stopped, for example, by war, natural disaster, or sabotage. This would cause temperatures to rise very quickly to reflect the atmospheric concentration of greenhouse gases. If societies are locked into the technology, these speedy rises could be truly catastrophic. Even if SRM were to be used only temporarily, the long atmospheric life of C0₂ means that abrupt termination would still lead to a massive, swift warming effect. SRM deployment is not an insurance policy against climate catastrophe. It creates new and terrible dangers of its own. We must not have a Panglossian view of the protection offered to us by existing institutions and the current global order. There needs to be governance of SRM research now that guards against lock in."
"Most African countries have made strong progress in achieving major development goals in the last few years. Despite this much needed progress, the past decade has seen flooding damage or destroy much of this same infrastructure, affecting millions and killing hundreds every year. In 2018 alone – up to September 15 – based on conservative estimates, flooding across Sub-Saharan Africa has destroyed more than 10,000 homes and affected more than 2m people.  A recent study suggested that floods cost Tanzania US$2 billion annually. In 2012, Nigeria experienced one of its largest floods in a century, destroying assets worth nearly US$10 billion. In Mozambique, one of the poorest countries in the world, floods in 2013 were estimated to cost over US$500m – nearly 9% of GDP. These figures are significant, especially when considering that this money could have been invested in other developmental goals. The simple fact remains that people depend on infrastructure to meet their needs. Without roads, people are not able to sell their goods on the market. School closures disrupt learning for millions of children. Damaged roads and bridges restricts access to health services. The destruction of critical infrastructure such as power and telecommunications leads to untold economic costs. Hectares of destroyed farmland and livestock can be killed. Health emergencies, including cholera outbreaks, can emerge due to poor sanitation and a lack of access to clean water.  These examples highlight the implications of these recent floods for the 17 sustainable development goals (SDGs) set out by the UN.  In Africa, about 3,000 people die daily from diseases linked to poor sanitation, poor hygiene and contaminated water, particularly diarrhoea and malaria. This situation is worsened by flooding, which will make it harder to achieve SDG 6– ensuring access to water and sanitation for all.  In order to lessen the consequences of flooding, African societies must make two major changes. First is to place greater focus on adaptation strategies, alongside mitigation. Before now, most of the efforts across Africa have been “hard” engineered routes to mitigate flood impacts. This means working against rather than with nature, for instance by building dams or embankments.  However, there is now a general recognition that while floods cannot be entirely prevented, steps can be taken to minimise their impact and speed up recovery. Such a shift in thinking may inspire donor agencies, who provide aid or rehabilitation post-flooding, to consider using the same amounts of money to fund flood adaptation initiatives as well.  For instance, after a recent flood in Nakuru county, just north of Nairobi in Kenya, the EU alone provided €1.5m to help flood victims. Such funds can be channelled towards helping communities adapt to flooding and increase their resilience. This would mean, for instance, factoring in current and likely future flood conditions when constructing or managing new infrastructure. A particularly good initiative that incorporated such adaptation strategies was the Makoko floating school. The school is a prototype floating structure, built for the historic water community of Makoko, located on the lagoon heart of Nigeria’s largest city, Lagos. As a pilot project, it has taken an innovative approach to address the community’s social and physical needs in view of the impact of climate change and a rapidly urbanising Africa. More efforts should be channelled towards strengthening and scaling up such initiatives. The second major change is to scale up “soft” non-structural adaptation measures, such as ecosystem-based approaches to flooding. These involve measures which work with the natural flood cycle rather that struggling against it. These solutions include the widening of natural flood plains, planting more trees, protecting and expanding wetlands, and investing in urban green spaces to reduce water run off.   This could be beneficial considering that most African countries do not have sufficient finances to justify the costs of dams and other “hard” engineering. As competing with education and agriculture expenditure priorities is unlikely to end well in such a poor economy, “soft” non-structural adaptation measures may provide a more promising route.  While the increased flooding can be linked to climate change, some of the causes have roots in the destruction of ecosystems. Take, for example, the unprecedented land reclamation projects along the coastal areas of Lagos. The result has been enormous environmental damage because much of what is being drained for housing are coastal wetlands, which are traditionally known for their ability to control flooding. Policy makers often see flooding as a humanitarian issue only – they must be reminded that there is an economic dimension too. There is evidence that ecosystem-based adaptations are helping people, particularly women and children, adapt to climate variability and reduce their vulnerability to climate impacts.  By adopting an ecosystem-based adaptation, a recent study showed how Thua Thien-Hue Province in Vietnam was able to introduce cleaner urban areas with more opportunities for recreational activities, all the while reducing damage to infrastructure. Less damage to properties means smaller repair costs and an overall safer environment, whereas increases in tourism, or recreation suitability, can lead to better employment and business opportunities.  With the impacts of flooding increasingly being felt across Africa, there is an urgent imperative to adapt infrastructure to flood hazards while fostering sustainable economic development on local, national and regional levels. Without this, all the developmental achievements made so far will be completely undone."
"The record-breaking, El Niño-driven global temperatures of 2016 have given climate change deniers a new trope. Why, they ask, hasn’t it since got even hotter? In response to a recent US government report on the impact of climate change, a spokesperson for the science-denying American Enterprise Institute think-tank claimed that “we just had […] the biggest drop in global temperatures that we have had since the 1980s, the biggest in the last 100 years.” These claims are blatantly false: the past two years were two of the three hottest on record, and the drop in temperature from 2016 to 2018 was less than, say, the drop from 1998 (a previous record hot year) to 2000. But, more importantly, these claims use the same kind of misdirection as was used a few years ago about a supposed “pause” in warming lasting from roughly 1998 to 2013. At the time, the alleged pause was cited by many people sceptical about the science of climate change as a reason not to act to reduce greenhouse pollution. US senator and former presidential candidate Ted Cruz frequently argued that this lack of warming undermined dire predictions by scientists about where we’re heading. However, drawing conclusions on short-term trends is ill-advised because what matters to climate change is the decade-to-decade increase in temperatures rather than fluctuations in warming rate over a few years. Indeed, if short periods were suitable for drawing strong conclusions, climate scientists should perhaps now be talking about a “surge” in global warming since 2011, as shown in this figure: The “pause” or “hiatus” in warming of the early 21st century is not just a talking point of think-tanks with radical political agendas. It also features in the scientific literature, including in the most recent report of the Intergovernmental Panel on Climate Change and more than 200 peer-reviewed articles. Research we recently published in Environmental Research Letters addresses two questions about the putative “pause”: first, is there compelling evidence in the temperature data alone of something unusual happening at the start of the 21st century? Second, did the rise in temperature lag behind projections by climate models? In both cases the answer is “no”, but the reasons are interesting. Reconstructing a historical temperature record from instruments designed for other purposes, such as weather forecasting, is not always easy. Several problems have affected temperature estimates for the period since 2000. The first of these was the fact that uneven geographical distribution of weather stations can influence the apparent rate of warming. Other factors include changes in the instruments used to measure ocean temperatures. Most of these factors were known at the time and reported in the scientific literature, but because the magnitudes of the effects were unknown, users of temperature data (from science journalists to IPCC authors) were in a bind when interpreting their results. A more subtle problem arises when we ask whether a fluctuation in the rate of warming is a new phenomena, rather than the kind of variation we expect due to natural fluctuations of the climate system. Different statistical tests are needed to determine whether a phenomena is interesting depending on how the data are chosen. In a nutshell, if you select data based on them being unusual in the first place, then any statistical tests that seemingly confirm their unusual nature give the wrong answer. (The statistical issue here is similar to the fascinating but counterintuitive “Monty Hall problem”, which has caught out many mathematicians). When the statistical test is applied correctly, the apparent slowdown in warming is no more significant than other fluctuations in the rate of warming over the past 40 years. In other words, there is no compelling evidence that the supposed “pause” period is different from other previous periods. Neither is the deviation between the observations and climate model projections larger than would be expected. That’s not to say that such “wiggles” in the temperature record are uninteresting – several of our team are involved in further studies of these fluctuations, and the study of the “pause” has yielded interesting new insights into the climate system – for example, the role of changes in the Atlantic and Pacific oceans. There are lessons here for the media, for the public, and for scientists. For scientists, there are two lessons: first, when you get to know a dataset by using it repeatedly in your work, make sure you also still remember the limitations you read about when first downloading it. Second, remember that your statistical choices are always part of a cascade of decisions, and at least occasionally those decisions must be revisited. For the public and the media, the lesson is to check claims about the data. In particular, when claims are made based on short periods or specific datasets, they are often designed to mislead. If someone claims the world hasn’t warmed since 1998 or 2016, ask them why those specific years – why not 1997 or 2014? Why have such short limits at all? And also check how reliable similar claims have been in the past.  The technique of misinformation is nicely described in a quote attributed to climate researcher Michael Tobis: “If a large data set speaks convincingly against you, find a smaller and noisier one that you can huffily cite.” Global warming didn’t stop in 1998. Don’t be fooled by claims that it stopped in 2016 either. There is only one thing that will stop global warming: cuts to greenhouse gas emissions."
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Today we plot the Japan Meteorological Agency (JMA) data for Northern Europe for the month of August, 2020.
We have selected this region because it is the home of 17-year old climate alarmist/activist Greta Thunberg, who thinks the planet is heating up rapidly and so we’re all doomed.
We plot the data for the stations for which the JMA has sufficient data going back over 2 decades. First we plot the August data for Sweden, Greta’s home country:

Data: JMA
Five of the 6 stations plotted show a cooling trend. So it’s a mystery how Greta thinks her country is warming up. The data suggest that summers have been shortening a bit. Over the course of Greta’s life, she has yet to see warming in August.
Next we examine Norway, Greta’s western neighbor:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data: JMA
Here we see 6 of the 11 stations have seen an August cooling trend over the past quarter century. The colder stations have warmed somewhat, while the warmer ones have cooled. Overall, no warming to speak of, really.
The story is similar in Finland (further from the Atlantic), but here the colder stations have cooled, while the warmer stations have warmed slightly – but statistically insignificantly:

Data: JMA
Finally we plot the data for the emerald island, Ireland, next to the tempestuous Atlantic:

Data: JMA
Four of the six stations in Ireland have been cooling in August. Those warming have done so insignificantly. Overall the emerald island has been cooling during the month of August since 1983!


		jQuery(document).ready(function(){
			jQuery('#dd_0f4f1e38f8790e798fa8564c196424a6').on('change', function() {
			  jQuery('#amount_0f4f1e38f8790e798fa8564c196424a6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
nan
"The climate science might be gloomy but at least governments seem to be doing something about it. The number of laws passed to address climate change is steadily increasing across the world. By last year 127 countries had renewable energy support policies, for instance. But this is only half the story. Examination of public policy developments in the US, EU and China, the world’s three largest economies by far, has shown that side by side with policy initiatives designed to cut greenhouse gas emissions have come new policies that have the opposite effect: increased emissions.  This is a class of policies that we don’t talk about because it doesn’t have a name. Let’s call them “anti-climate policies”. We are not talking here about the numerous existing policies that perpetuate emissions. Anti-climate policies are new initiatives that increase emissions: steps backwards in the fight against climate change. Their existence means that strengthening climate policies will not be enough to defeat climate change alone; anti-climate policies will need to be tackled as well.  There are lots of anti-climate policies out there – subsidies for householders’ energy bills, support for energy-intensive manufacturing or chemical industries, or building new roads and airports – but three stand out as the most damaging. First, there is the licensing of new fossil fuel-fired power stations, especially in China. Figures from the US Energy Information Administration indicate that between 2000 and 2011 fossil fuel electricity generation capacity rose by 34% in China, 6% in the US, and 15% in the EU-27. Then you have the new and ever higher subsidies for fossil fuels. Numerous new tax breaks for exploration have been introduced in the US, for instance. In the EU new tax breaks have mainly focused on fossil fuel use in energy-intensive industries and transportation, although in the UK tax breaks for exploration have been expanding rapidly. The International Energy Agency reports that in 2011 fossil fuel subsidies worldwide came to US$523 billion, six times the level of support for renewable energy. International trade liberalisation is the other main anti-climate policy. Despite the fact that more trade increases greenhouse gas emissions by expanding economic activity and increasing the use of cross-border transportation services, governments keep signing them. The most recent significant agreement was the one that required China to remove trade barriers in order to join the World Trade Organisation in 2001. Between 2000 and 2010 the EU, US and China concluded new bilateral trade agreements with each other and other countries almost every year. Some progress has already been made. Both the US and UK have moved to introduce emissions limits for new power stations that conventional coal-fired power plants cannot meet, effectively banning new such plants.  Although the projected increase in electricity demand in China is so huge that banning new coal-fired stations altogether would cripple the economy, in 2013 the Chinese government introduced a ban on approvals of new coal-fired power stations in three of the country’s most important industrial regions. This is in addition to a programme that has been closing down small inefficient thermal power stations since 2008.  Efforts to extend blocks on new coal-fired power stations to other countries may strengthen as renewables become more credible and increase their lobbying power.  G20 communiqués announcing agreements to phase out fossil fuel subsidies have not been matched by action. Governments appear to view exploration subsidies, for example, as investments that will bring in more tax once oil and gas fields are in production. Industry lobbying is also much in evidence, especially in the US, judging by trends in political contributions.  What is needed is a spotlight on the hypocrisy of governments that expand fossil fuel subsidies while claiming to care about climate change. We also need to tackle deceits such as David Cameron’s assertion that fracking will reduce emissions by displacing coal with fracked gas. What does he think will happen to the displaced coal? It will be used by someone else. Trade liberalisation continues to be vigorously pursued. Global negotiations launched in 2001 in Doha are aimed at reaching a major multilateral trade-opening agreement, while the US and EU are currently negotiating a bilateral Transatlantic Trade and Investment Partnership. As there is a broad consensus that trade opening boosts economic growth, direct opposition to trade is unlikely to be successful, although the fact that trade opening increases emissions needs to be publicised.  A cannier tactic would be to support the efforts of groups that for other reasons stand to lose by new trade deals, such as US and EU farmers. And the failure so far of the Doha round to reach agreement suggests that such deals can be blocked. Bringing greenhouse gas emissions under control is going to be difficult. To succeed we need to take all relevant factors into account. This means that more attention needs to be paid to anti-climate policies and how they can be countered."
"
Share this...FacebookTwitterHow long have we been hearing this kind of talk from alarmists? How often are we told that we have to make lots of sacrifices and give governments unlimited power – otherwise the earth will be destroyed? The answer is: almost everyday.
High energy taxes, loss of freedom, massive regulation, constant monitoring, surveillance over how we do things will be huge inconveniences; but it’s necessary, and so just suck it up.
Can’t you see all the destruction all over the planet? It’s spreading everywhere, and soon it will be at your doorstep, unless of course you suck it up and give them the power they need.
I ask,  just what kind of person does one have to be to heed that kind of advice? Pretty clueless I think.
 Just suck it up, otherwise the planet is going to be destroyed

I know as a climate-blogger I’m going out on a limb with this. But I got a feeling we have not heard the end of this story by any means. Where’s there’s smoke, there’s fire. My eyes and ears are perked.
I can understand the other climate blogs not wanting to touch this with a 10-foot pole, claiming whatever righteous reason. But I think someone has to observe and report on this. I got a hunch, and I#m going to follow it..
I happen to think Al Gore is not an okay person, and so it would not surprise me if this story turns out to be true. I’m not saying the story and allegations are true. Yellow journalism and the such are not what I draw my conclusions on. That’s for the IPCC to use (before asking us to suck it up).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The problem is that Al Gore unwittingly revealed a lot about himself and his movement when he made the film AIT. The film was carefully designed to mislead and manipulate its viewers in a mean kind of way. And it was carefully crafted to demonize anyone who refused to fall in line. It uses bully psychology.
Does AIT show any respect for the very element that is needed for science to progress? The answer is of course: none whatsoever!  There was absolutely zero tolerance for sceptical views. Sceptics were ridiculed, mocked and called flat-earthers, and other nasty things.
There are lots of other things in the film that are troublesome, which speak volumes about the persons involved in making it. Of course some involved were just gullible and innocently believed the rubbish.
And let’s not even discuss how Gore lives his life, his business ventures, and so on. These also speak volumes about what kind of guy he is. To me it’s crystal clear. Gore in the film is a pretty bitter and quite peeved chap and he’s capable of a lot of things. 
But we’ll see how the story turns out soon enough. I’m not going to stop blogging about it until the story ends and the (inconvenient?) truth is out.
And my feeling is that the other climate bloggers will be joining in with the commentary before too long. Like it or not, it’s a climate story. But, my hunches could be wrong.
FULL POLICE REPORT HERE FOR YOUR READING PLEASURE!
Update: By yellow journalism, I don’t mean the Examiner, rather the Nation Enquirer, who broke the story, and whose website for whatever reason I can’t access from Germany.
Other Links:
http://www.kptv.com/news/24102273/detail.html
Fox News video

Share this...FacebookTwitter "
"Extravagance is as much a part of awards season as the awards themselves, a lavish procession of parties, campaigning and red carpet opportunities leading to a ceremony that’s rumoured to cost more than $40m. But as this year’s Oscars approaches, there’s been a noticeable shift, from plant-based menus to “repeat tuxedos”, a culture of more trying to grapple with the need for less.  It’s not the first year that the climate crisis has affected Hollywood’s most indulgent period (since 2013, the Oscars ceremony has had a net zero carbon imprint) but, as alarmed discourse about the environment has increased outside the industry, a more pronounced change was needed inside. After best actor favourite Joaquin Phoenix lobbied the Hollywood Foreign Press Association to implement a vegan menu at the Golden Globes, other ceremonies followed suit and on 27 January, the Academy announced that for the first time all pre-show food would be entirely plant-based while the post-show Governors Ball menu would be 70% vegan. “We would have liked to have seen the Oscars commit to a 100% plant-based menu,” said Matt Turner of The Vegan Society. “But it’s an important step in the right direction. People are starting to realise that their individual actions as part of a collective movement can make a tangible difference.” Celebrity caterer Kathleen Schaffer has said that while many of her clients have embraced similar menus, “the omnivore is alive and well in Los Angeles”. Phoenix has also closely aligned himself with Extinction Rebellion, starring in Guardians of Life, a short film made by the activists intended to bring more awareness to the climate crisis. The group also intends to protest near the Hollywood sign on the eve of the Oscars. “Some stars of Hollywood are aware of the scale of the climate crisis, and some have started to take action,” a statement read. “But we do not believe that Hollywood as a whole has taken an acceptable stance on the climate crisis.” The week’s festivities also include gifting suites, produced by third party companies who match stars with high-end products provided by a range of companies. Nathalie Dubois, whose established lounge has previously welcomed guests such as Charlize Theron and Scarlett Johansson, says there’s been an increased attempt to include eco-conscious products and experiences, such as sponsoring coral which will then be replanted. Since 2003, marketing company Distinctive Assets has been providing nominees from the four acting categories and those up for best director with extravagant goody bags. This year’s is estimated to be worth around $215,000, with gifts including a $78,000 fully-inclusive 12-day cruise on a luxury yacht. Distinctive Assets founder Lash Fary, who has been putting the bags together for 18 years, said that eco-consciousness is “certainly” on his mind and so a plant-based meal delivery service and a DIY candle kit that encourages re-use of holders have both been included. While the Academy has released a list of the ways in which they are working towards their goal of becoming carbon neutral, campaigners are still raising the alarm on one of the most damaging components of a season that brings together talent from across the globe: air travel. Anna Hughes, director of the Flight Free UK campaign, believes that more recognition is needed by the Academy and the entertainment industry in general, of the impact that such a “highly polluting” form of travel has on the environment. “In terms of emissions, the savings from providing plant-based food are likely to be overshadowed many times over by air travel to the ceremony,” she said. While it’s difficult to calculate exactly, Hughes estimates that if an attendee is flying to Los Angeles from New York or further in business class or on a private or chartered plane, they will be emitting more carbon than the average Brit does in a whole year. “At the very least, the Academy awards needs to acknowledge this, not ignore it,” she said."
"

Though well‐​intentioned, international agreements to protect the environment are ineffectual, misguided and very costly for future generations. 



The most pervasive environmental problems are local and regional — not global. Water pollution, waste disposal, groundwater contamination, urban smog and deforestation are manifestations of unique state policies and local geographic, demographic and industrial profiles. They can be effectively addressed only locally. 



International agreements tend to ignore those less ”mediagenic” problems and pursue sexier issues such as climate change and ozone depletion — purported problems of which there is little hard evidence. Result: Scarce resources are misspent, real problems go unaddressed. 



International agreements tend to codify a uniform approach to environmental protection. The world is infatuated with command and control policies that empower armies of bureaucrats no more capable of managing ecological health than they are of managing economic growth. The World Bank, for example, would be charged with administering global environmental programs despite its record of ecological mismanagement. 



Natural resources are better protected by individual owners with vested interests in their property than by absentee bureaucratic managers subject to the winds of political fortune. Treaties that centralize environmental management compound ecological damage. 



Environmental treaties are biased against economic growth (viewed as a “problem” to be “solved” or “managed”) despite the proven correlation between wealthy economies and healthy environments. Treaties that harm growth not only condemn the poor to continuing poverty but doom long‐​term progress in environmental protection.
"
"Moderation is the last thing on people’s minds at Christmas. Shopping, travelling and eating reach peak levels – putting pressure on our planet. Even Santa poses a problem. If you don’t believe in flying reindeers, that sleigh must be rocket-fuelled to reach the supersonic speeds needed to travel around the world to visit hundreds of millions of children in just one night using conventional engineering. The example goes to show just how many presents we buy and send each Christmas –  creating problems with packaging and transport. And as the population increases, so does the pile of presents. To get round this, presents have got smaller and virtual gifts such as an experience day  have risen in popularity.   This has an added benefit of reducing packaging and transport problems. But virtual presents have a carbon footprint too. Electronic downloads still have an impact, as data has to be stored and transferred, using energy.  So everything we buy has some impact, even through the electronic process of buying.  So how can we have a greener, more sustainable but generous Christmas? Here are five gold circular things!  The amount of food wasted at Christmas has a massive carbon (and water) footprint. Using less and storing excess in a winter wonderland – your freezer – is a great way to avoid waste. If leftover food doesn’t go in the freezer, cooked turkey and vegetables will keep for up to three days in the fridge.    However, not producing excess in the first place is the best way to avoid waste. Portion size is a big part of this and so is cooking things you actually like. Just because something is traditional does not make it compulsory. For instance, sprouts can be very controversial – so, if you don’t like them, skip them. You could also try an alternative to the traditional meat option, such as a nut roast. Vegetarian and vegan choices at the Christmas dinner table can massively cut the impact of your Christmas.    Lower the impact of gifts through choices of paper and packaging. A lot of seasonal wrapping is non-recyclable as it is coated in plastic. This is concerning as plastic tends to spread everywhere – it has even been detected at the North Pole. A better approach would be to use wrapping paper made entirely out of paper. Gift bags are another great option – they can be reused and therefore help cut a massive amount of waste.  You can give twice if you buy your presents second hand from charity shops –  supporting worthwhile projects while also reducing consumption. You can also buy locally produced goods and support your local economy. Buying second hand potentially halves the carbon footprint. A typical T-shirt alone has a footprint of around 8.77kg of carbon dioxide and 2700 litres of water. If 1% of the 55.6m people in England alone bought just one second-hand T-shirt instead of a new one, they would be saving around 4.9m tonnes of carbon dioxide, the equivalent of driving 1,049 passenger cars for a year, and a whopping 1.5 billion litres of water. Christmas decorations and fashion are basically the same every year. So celebrate your Christmas collection and reuse it, over and over again. It is a tragedy that only one in four Christmas jumpers are ever reused. According to the Carbon Trust, an artificial tree needs to be used around 10 times to have an equivalent footprint as its real counterpart.    There are few holidays that are so focused on being caring, helpful and generous as Christmas. So celebrate this and try to avoid buying unnecessary stuff that people don’t want anyway. Donations and acts of kindness really lighten the load on that sleigh. A colleague once bought me a toilet for a family in Sierra Leone. No wrapping, no plastic: the best present ever – and Santa didn’t have to lift a finger!"
"

_Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Let us see if we can help _New York Times_ ’ global warming reporter Justin Gillis out.   
  
In his article yesterday about the upcoming _Fifth Assessment Report_ of the U.N.’s Intergovernmental Panel on Climate Change (IPCC), Gillis laments that the IPCC seems to be tamping down some of the more alarmist scenarios when it comes to the projected rate of rise of global temperatures and sea level.   
  
Concerning projections of sea level rise, Gillis bemoans that the IPCC looks like (the final version of the Summary for Policymakers of the new report isn’t scheduled for release until the end of the this month at the conclusion of an IPCC editorial meeting in Stockholm) it will discount the “outlier” estimates that the rise this century will exceed five feet. Gillis writes “The drafters of the report went with the lower numbers, choosing to treat the outlier science as not very credible.”   
  
When it comes to how fast the global average temperature is projected to rise, Gillis rues the possibility that the IPCC will lower its assessed value of the climate sensitivity, writing “In this case, the drafters of the report lowered the bottom end in a range of temperatures for how much the earth could warm, treating the outlier science as credible.”



Gillis can’t wait for the explanation:   




…[I]t would be nice to hear an explanation from the drafters of this coming report as to why they made decisions that effectively play up the low-end possibilities. But with the report still officially under wraps, they are not speaking publicly. We are thus left wondering whether it is a matter of pure professional judgment — or whether they have been cowed by the attacks of recent years.   
  
Assuming these decisions withstand final review, it will be fascinating to hear the detailed explanations in Stockholm.



We’ll end the suspense for him.   
  
The reason that the IPCC should discount the possibility that the sea level will rise more than three feet by the year 2100 is that such a possibility has largely been discredited in the scientific literature and well as simply by looking out the window (i.e., the observed rate of sea level rise is only about 1.25 inches per decade and there is A LOT of ice in Greenland and Antarctica that is not going anywhere fast).   
  
We documented this is numerous places, notably here, here, here, and here.   
  
And the reason that the IPCC should lower its estimates of the earth’s climate sensitivity (i.e., how much the earth’s average temperature rises as a result of a doubling of the atmospheric concentration of carbon dioxide) is that the overwhelming majority of the recent findings in the scientific literature show that the most likely value is far beneath what it was assessed at in previous IPCC reports. In fact, the new equilibrium climate sensitivity estimates are so low as to put the IPCC in a quandary—if they were to fully embrace the new findings, they would have to discredit all the future climate projections made in the new report as they were generated by climate models with and average climate sensitivity that is nearly 75 percent higher than the new findings suggest. So if anything, the IPCC will likely be too conservative in lowering its assessment of the climate sensitivity in the final version of its _Fifth Assessment Report_.   
  
Of course, we have explained all of this as well. See here, here, here, and here for starters.   
  
So, pure and simple, the reason that the IPCC should embrace estimates of a slower global temperature increase and a lower global sea level rise is because that is what the current science supports.   
  
Gillis could have saved himself a lot of wondering (and ink) had he only been reading these pages!   
  
  
  
  
  



"
"After a month of tranquillity, fracking has resumed at the Preston New Road site near Blackpool triggering the biggest tremor to date. There have been 12 tremors over a four-day period, including the biggest so far – the 1.5 magnitude quake. In total, 36 earthquakes were recorded in the area between the middle of October and early November. Most of these are too weak to be felt at the surface, but can be measured using seismometers. These are instruments that measure ground motions, caused by such events as earthquakes and volcanic eruptions, among other factors. Local residents are concerned the earthquakes may cause cracks in the fracking well’s casing, which could potentially lead to contamination issues. Some scientists claim the impact of these seismic events at surface is equivalent to dropping a melon onto the floor. But government officials and those in the fracking industry have dismissed the tremors – suggesting they are inconsequential. As a social scientist living in Lancashire, I have been researching the social impacts of shale gas developments since 2015. From what I have seen, there is much more to the tremors than just ground movements. The impact of the quakes that occurred far below ground reverberated strongly throughout the community living on the surface. To understand why this is the case it is important to understand local people’s experiences of shale gas exploration in the UK. The same operator, Cuadrilla, was fracking for shale gas in the area seven years ago. Two bigger and around 50 smaller earthquakes occurred over an eight-month period as a result of injecting fluid into a geological fault zone.  In 2018 – and under new seismicity controls – Cuadrilla was required to halt its fracking operations twice when the monitoring equipment detected tremors bigger than 0.5 local magnitude. The system was introduced to set “gold standard” regulations for this new industry. After the quakes, Cuadrilla’s CEO warned that making fracking commercially viable would be extremely challenging under the existing seismic monitoring system in the UK. He wanted the government to reconsider its position on seismic monitoring within weeks.   Weeks passed by, the activity at the site was subdued for a month and no further seismic events were recorded until December 10 2018. Cuadrilla did not publicly confirm it had suspended hydraulic fracturing between early November and December. But it did say it was planning to engage with the regulators to change the upper limit on seismic monitoring. In the Blackpool area, earthquakes have been on everyone’s radar. Many local residents refresh the British Geological Survey website that records all recent earthquakes in the country almost hourly. At the observation point at Preston New Road known as the “gate camp”, protesters watch and listen carefully for the signs of fracking activity, proudly asserting: “This is the most watched site in the UK.” The reason they are watching so carefully is because they have serious concerns about how regulatory monitoring and corporate transparency works. Take the seismic monitoring system which was originally designed to reassure communities they would be protected from harm. After Cuadrilla’s recent announcements, the prospect of relaxing the seismic controls seems real. For local communities, new seismic thresholds would not be just numbers, but a sign that politicians are willing to further extend the industry’s authority over society. Relaxing regulations because they make business more difficult is a narrowly economic rationale – there’s certainly nothing democratic about it. This is the palpable sense of injustice you get when you talk to people at the side of Preston New Road.  In my experience, the regular liaison meetings with the company and regulators do little to reassure the local communities. Instead, they have made residents dissociate transparency from openness. In their view, the liaison meetings, consultations and the lengthy planning process have become a field of corporate practice. They limit residents’ ability to determine their common future – but the process provides the industry with a veneer of democratic legitimacy.  What this generates for local residents are feelings of disenfranchisement and distrust – and a sense of social injustice. This is why the impact of the earthquakes can’t be separated from the social reality on the surface. For local communities, there is an implicit analogy between the fracking pad with its well bore that extends kilometres out of sight and underground and the non-transparent ways in which the UK government and the industry are perceived to impose hydraulic fracturing on local populations.  Residents worry that the same attitude that the government and industry espouse on the surface, would also govern the way they tackle potential problems that arise underground – as a result of fracking. Of course, it is true that any new industry – such as shale gas exploration – is bound to face hurdles as it tries to identify suitable operational procedures. But to understand why communities in Lancashire have found it so difficult to trust government agencies and industry, it’s important to consider how seismic events operate in the reality of social, rather than merely geological, environments."
"Air pollution from burning fossil fuels is responsible for more than 4m premature deaths around the world each year and costs the global economy about $8bn a day, according to a study. The report, from Greenpeace Southeast Asia and the Centre for Research on Energy and Clean Air, found that burning gas, coal and oil causes three times the number of deaths as road traffic accidents globally.  Children, especially those living in low-income countries, are particularly affected with an estimated 40,000 dying each year before they reach their fifth birthday because of exposure to particulate pollution from fossil fuels. “Air pollution is a threat to our health and our economies,” said Minwoo Son, clean air campaigner at Greenpeace East Asia. “Every year, air pollution from fossil fuels takes millions of lives, increases our risk of stroke, lung cancer and asthma, and costs us trillions of dollars.” The study, released on Wednesday, analysed global datasets of surface level concentrations of common pollutants PM2.5, ozone and NO2 to calculate the health impact and the subsequent economic cost for 2018. It found: NO2, from petrol and diesel vehicles, power plants and factories, is linked to roughly 4m new cases of asthma in children each year. Approximately 16 million children live with the condition due to exposure to fossil fuel pollution. Tiny particulate pollution – known as PM2.5 – is attributed to roughly 1.8bn days of work absence because of illness each year. China, the US and India are hardest-hit financially by the impact of dirty air with estimated costs of $900bn, $600bn and $150bn each year respectively. The study argues that the solutions to the air pollution crisis are clear – and would also help tackle the climate emergency. It says moving to a clean energy and transport system would have economic as well as health benefits. It cites research published in the US recently by the Environmental Protection Agency that shows every $1 invested under the US Clean Air Act yielded at least $30 in return. Likewise, a weekly car-free day in Bogota, Colombia, yielded up to $4 in health benefits for every $1 invested. “This is a problem that we know how to solve,” said Son. “By transitioning to renewable energy sources, phasing out diesel and petrol cars, and building public transport. We need to take into account the real cost of fossil fuels, not just for our rapidly heating planet, but also for our health.”"
"
See related articles from the Guardian: Billions Wasted On UN Climate Programme and Discredited Strategy
“It looks like between one and two thirds of all the total CDM offsets do not represent actual emission cuts.” — David Victor, Stanford University and co-author of a study examining 3000 UN funded offset programs
This article below was reposted from TriplePundit
 

World’s Largest Carbon Market Facilitates Pollution


An article in the Guardian newspaper reveals that billions worth of ‘clean’ investment on the world’s largest carbon offsets market ends up polluting the environment. The article cites researchers who’ve reviewed the participating companies in the Kyoto Protocol Clean Development Mechanism (CDM). They issued a report which seriously undermines the credibility of the CDM.
The CDM certificates facilitate the funding of clean technology investments by Third World companies that are expanding their operations. Western companies can buy the certificates to offset their own pollution. But it turns out that in reality most of the funds go to coal and oil companies, builders of destructive dams and other enterprises that are not green in the slightest.
The research that revealed the practices is of major importance not least because policymakers are set to review the CDM in the near future as the Kyoto Protocol expires in 2012. CDM credits are the world’s largest offset market, with annual trading last year totalling around EUR40 billion. Most credits are currently traded on the European Trading System (ETS) by European countries and companies but when the US starts to participate, something that’s more or less a given, trading will rise to over EUR 100 billion within two years easily.
The Stanford scholars opened a can of worms. They say that “Much of the market does not reflect actual reductions in emissions, and that trend is poised to get worse.” They researched more than 3,000 projects that had been applying/granted for up to $10bn of credits for the next four years and said that most of the applications should be rejected. If the scheme operated in any way realistically, we’d see a much smaller market, they say cautioning that there’s hardly enough clean air available for the demand that will build up in the near future. That’s rather an important point to consider ahead of next week’s Warner-Lieberman cap and trade bill which proposes US companies are allowed to buy up to 15% of their needed carbon credits from the (successor to the) CDM.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f2ea773',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterOnly a few thousand years ago, when CO2 levels were both stable and low (~265 ppm), the  (1) Arctic had far less ice and more vegetation than it does now and (2) the massive rate of ice melt in Antarctica rendered modern melt rates negligible by comparison.
A new study (Cherezova et al., 2020) reveals that until about 6,500 years ago Bolshevik Island in the Russian High Arctic brimmed with grass, birch and willow trees, and large herbivores grazing on grass year-round. At that time sea levels were rising at rates of 7.9 mm/yr, which is more than 5 times faster than the  global sea level rise trend since 1958 (~1.4 mm/yr, Frederikse et al., 2018).
Today this same High Arctic island is treeless with “very scarce vegetation.” It is locked in sea ice and mean annual temperatures only reach -13°C, which is a “similar climate to the Lateglacial.” Modern climate warming “is not observed” in either the meteorological or ice core data (Tvyordoe Lake) for this region. The ice caps are today about the same size as they were during the peak of the last ice age (~20,000 years ago).

Image Source: Cherezova et al., 2020
Another new study (Jones et al., 2020) reveals that from about 7,500 to 4,500 years ago, when CO2 was about 150 ppm lower than today, Antarctica’s Ross Sea glaciers abruptly lost 220 meters (!) of ice surface height. This ice loss – at times reaching >400 cm per year – occurred throughout the region regardless of the topography. This strongly implies the “overarching external driver” of the glacier retreat was an ocean warming trend.
The authors point out that the ice surface lowering may have “continued below the present-day glacier surface,” only to advance again during the last few hundred years.

Image Source: Jones et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A recent study (Sinclair et al., 2012) indicates the sea surface temperatures in this region (western Ross Sea) have been rapidly cooling (-1.59°C/decade) since 1979, and there has been no net ocean warming since 1882.

Image Source: Sinclair et al., 2012
Still another study from this region (Yokoyama et al., 2016) corroborates a “widespread collapse of the Ross Ice Shelf” between 5,000 and 1,500 years ago. Modern melting rates are the slowest in the last 5,000 years and there was “much warmer water beneath the ice shelf at 5 ka compared with the present.”
These studies strongly imply modern polar climate changes and ice melt rates in both hemispheres are neither unprecedented or unusual. In fact, if there is anything anomalous about today’s polar climates, it’s that they are colder and ice melt is less pronounced than just about any time in the last 8,000 years.


Images Source: Yokoyama et al., 2016


		jQuery(document).ready(function(){
			jQuery('#dd_1b239021c037dc1f44fcc32cd693e702').on('change', function() {
			  jQuery('#amount_1b239021c037dc1f44fcc32cd693e702').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

USHCN-M station at Greensboro, AL
While I was at NCDC, Grant Goodge showed and provided me with a PowerPoint presentation about the plan to update the USHCN manual observing network to USHCN-M or “modernized”. In a nutshell, it is a “light” version of the Climate Reference Network. The summary of benefits goes like this.
More Accurate Data Through:

Redundant Sensors
Near Real Time Diagnostics
Time Resolution of Five Minutes vs The Current Daily
Automated data collection via GOES satellite uplink, eliminating human error of reading and transcription
No adjustments to the data post reception. Time of Observation is now irrelevant.
No more routinely missing data, such as on weekends (fire station at Marysville, CA for example) and thus no need to fill in estimated data using the FILNET adjustment any more.

The station looks much like a Climate Reference Network station, but has some economy considerations, especially in having one aspirated IR screen instead of three, but it contains triple temperature sensors so that issues with instrumentation drift or offset events can easily be spotted in the data stream.

This will be a huge step forward in data quality and quality control.
His PowerPoint presentation is available here at this link: why-modernize-hcn (PPT 9 MB)
Interestingly, it included what appears to be a photo of the rooftop station in Asheville, NC, at the old NCDC (Federal Building) I’m waiting on a  positive ID.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9014bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterToday we present two papers on climate reconstruction using proxy data. One about East Antarctica and the other about belize. Hat-tip reader Mary Brown.
AMO behind sea surface temperatures
First we look at a paper authored by a team of German scientists: “Great Blue Hole (Lighthouse Reef, Belize): A continuous, annually-resolved record of Common Era sea surface temperature, Atlantic Multidecadal Oscillation and cyclone-controlled run-off“.
The team looked at 2000 years of proxy data from Belize and found interesting natural cycles at play. According to the authors, the Atlantic Multidecadal Oscillation (AMO) occurred 1885 years back in time and that it controls the SW Caribbean sea surface temperature patterns on multi-decadal time scales.
The authors note that the Holocene (<11.7 kyr BP) has been characterized by several periods of distinct climate changes and that the climate remains difficult to predict “due to the lack of comprehensive, annually-resolved and continuous sea-surface temperature (SST) data”.
So what about them models?
Examining an 8.55 m long sediment core from the bottom of the Great Blue Hole (Lighthouse Reef, Belize), the scientists were able to extract “an annually-resolved, continuous and unique south-western Caribbean climate record for the last 1885 years”.
The result? The data imply a general SST rise within the south-western Caribbean and that the modulation of SST within the time series likely operated on two different time levels: (1) Solar (e.g., “Gleissberg Cycles”) and volcanic activity triggered climate changes, which in turn induced responses of the Atlantic Multidecadal Oscillation (AMO), the North Atlantic Oscillation (NAO) and the El-Niño-Southern Oscillation (ENSO).
The authors conclude further in the abstract:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




We suspect long-term positive AMO and NAO modes as the primary key control mechanisms of the Dark Ages Cold and Medieval Warm Period SST patterns. ENSO mode modulation likely exerted primary control on regional SST variability during the Little Ice Age and the Modern Global Warming. (2) Our δ18O data further indicate a striking secondary control on multi-decadal time scales: δ18O variations occur with 32–64 years periodicity. This signal is clearly evidence of SST modulation controlled by AMO phase changes (50–70 years) over almost the entire Common Era. Our carbon isotope record (δ13C) exhibits two remarkable negative anomalies and a long-term up-core decreasing trend. The first excursion (drop of 0.5‰) occurred with the onset of the Medieval Warm Period, which is reconstructed to be a peak time in south-western Caribbean tropical cyclone (TC) activity. This overlap is stressing a potential context between TC activity, enhanced coastal run-off and increased soil-erosion reflected by 13C-depleted carbon isotopes. A second anomaly (>1900 CE) is more likely the result of the “Suess Effect” (anthropogenic impact of the Industrial Revolution on carbon isotopes composition) than another reflection of a TC peak activity interval.”
But since 1900, man has taken over control of the earth’s climate, the authors seem to be suggesting. That was probably written in witha wink to the funders.
Antarctica suddenly lost 400 meters of ice
In another new paper: Abrupt Holocene ice-sheet thinning along the southern Soya Coast, Lützow-Holm Bay, East Antarctica, revealed by glacial geomorphology and surface exposure dating, a team of Japanese scientists led by Moto Kawamata examined the deglacial history of the East Antarctic Ice Sheet (EAIS).

Image: Figure 1 here.
The authors found that it had thinned from at least 400 m a.s.l. during the Early to Mid-Holocene (9–5 ka) and say the abrupt thinning was likely caused by the natural inflow of modified Circumpolar Deep Water via submarine valleys in Lützow-Holm Bay.
Abstract:
Geological reconstruction of the retreat history of the East Antarctic Ice Sheet (EAIS) since the Last Glacial Maximum (LGM) is essential for understanding the response of the ice sheet to global climatic change and the mechanisms of retreat, including a possible abrupt melting event. Such information is key for constraining climatic and ice-sheet models that are used to predict future Antarctic Ice Sheet AIS melting. However, data required to make a detailed reconstruction of the history of the EAIS involving changes in its thickness and lateral extent since the LGM remain sparse. Here, we present a new detailed ice-sheet history for the southern Soya Coast, Lützow-Holm Bay, East Antarctica, based on geomorphological observations and surface exposure ages. Our results demonstrate that the ice sheet completely covered the highest peak of Skarvsnes (400 m a.s.l.) prior to ∼9 ka and retreated eastward by at least 10 km during the Early to Mid-Holocene (ca. 9 to 5 ka). The timing of the abrupt ice-sheet thinning and retreat is consistent with the intrusion of modified Circumpolar Deep Water (mCDW) into deep submarine valleys in Lützow-Holm Bay, as inferred from fossil foraminifera records of marine sediment cores. Thus, we propose that the mechanism of the abrupt thinning and retreat of the EAIS along the southern Soya Coast was marine ice-sheet instability caused by mCDW intrusion into deep submarine valleys. Such abrupt ice-sheet thinning and retreat with similar magnitude and timing have also been reported from Enderby Land, East Antarctica. Our findings suggest that abrupt thinning and retreat as a consequence of marine ice-sheet instability and intrusion of mCDW during the Early to Mid-Holocene may have led to rapid ice-surface lowering of hundreds of meters in East Antarctica.”
Today, if an ice sheet loses 60 cm, it’s deemed a crisis by climate bedwetters. Just imagine if an ice sheet in Antarctica were to lose 400 meters thickness.


		jQuery(document).ready(function(){
			jQuery('#dd_671ec5cfb78244fc3fa18cd6e7e3d7df').on('change', function() {
			  jQuery('#amount_671ec5cfb78244fc3fa18cd6e7e3d7df').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
NOTE: Please note that part 2 is now online, please see it here.
I recently plotted all four global temperature metrics (GISS, HadCRUT, UAH, RSS) to illustrate the magnitude of the global temperature drop we’ve seen in the last 12 months. At the end of that post, I mentioned that I’d like to get all 4 metrics plotted side-by-side for comparison, rather than individually.
Of course I have more ideas than time these days to collate such things, but sympathetic reader Earle Williams voluntarily came to my rescue by collating them and providing a nice data set for me in an Excel spreadsheet last night.
The biggest problem of course is what to do with 4 different data sets that have different time spans. The simplest answer, at least for a side by side comparison is to set their time scales to be the same. Satellite Microwave Sounder Unit (MSU) data from the University of Alabama, Huntsville (UAH), and Remote Sensing Systems (RSS) of Santa Rosa, CA only go back to 1979. So the January 1979-January 2008 period is what we’ll concentrate on for this exercise as it very nearly makes up a 30 year climate period. Yes, I know some may call this an arbitrary starting point, but it the only possible point that allows comparison between land-ocean data-sets and the satellite data-sets.
Here is the first graph, the raw anomaly data as it was published this month by all the above listed sources:

Here is the source data file for this plot and subsequent plots.
4metrics_temp_anomalies.txt
I also plotted a magnified view to show the detail of the lat 12 months with notations added to illustrate the depth of the anomaly over the past 12 months.

March 2005 to January 2008, magnified view – click for larger image
I was particularly impressed with the agreement of the 4 metrics during the 1998 El Niño year as well as our current 2008 La Niña year.
I also ran a smoothed plot to eliminate some of the noise and to make the trends a bit more visible. For this I used a 1 year (12 month) average.

Again there is good agreement in 1998 and in 2008. Suggesting that all 4 metrics picked up the ENSO event quite well.
The difference between these metrics is of course the source data, but more importantly, two are measured by satellite (UAH, RSS) and two are land-ocean surface temperature measurements (GISS, HadCRUT). I have been critical of the surface temperature measurements due to the number of non-compliant weather stations I’ve discovered in the United States Historical Climatology Network (USHCN) from my www.surfacestations.org project.
One of the first comments from my last post on the 4 global temperature metrics came from Jeff in Seattle who said:
Seems like GISS is the odd man out and should be discarded as an “adjustment”.
Looking at the difference in the 4 times series graphs, differences were apparent, but I didn’t have time to study it more right then. This post today is my follow up to that examination.
Over on Climate Audit, there’s been quite a bit of discussion about the global representivity of the GISS data-set due to all of the adjustments that seem to have been applied to the data at locations that don’t seem to need any adjustments to compensate for things like urban heat islands. Places like Cedarville, CA and Tingo Maria, Peru both illustrate some of the oddities with the adjustment methodology used by NASA GISS. One of the issues being discussed is the application of city nightlights (used as a measure of urbanization near the station) as a proxy for UHI adjustments to be applied to cities in the USA. Some investigation has suggested that the method may not work as well as one might expect. There’s also been the issue of whether of not stations classified as rural are truly rural.
So with all of this discussion, and with this newly collated data-set handed to me today, it gave me an idea. I had never seen a histogram comparison done on all four data-sets simultaneously.
Doing so would show how well the cool and warm anomalies are distributed within the data. If there is a good balance to the distribution, one would expect that the measurement system is doing a good job of capturing the natural variance. If the distribution of the histogram is skewed significantly in either the negative or positive, it would provide clues into what bias issues might remain in the data.
Of course since we have a rising temperature trend since 1979, I would expect all 4 metrics to be more distributed on the positive side of the histogram as a given. But the real test is how well they match. All four metrics correlate well in the time series graphs above, so I would expect some correlation to be present in the histogram as well. The histograms you see below were created from the raw data from 1979-2008. No smoothing or adjustments of any kind were made to the data. The “unadjusted” data in this source data file were used: 4metrics_temp_anomalies.txt
First we have the satellite data-set from UAH:

University of Alabama, Huntsville (UAH) Microwave Sounder Data 1979-2008 – click for larger image
The UAH data above looks well distributed between cool and warm anomaly. A slight warm bias, but to be expected with the positive trend since 1979.
Next we have the satellite data-set from RSS:

Remote Sensing Systems (RSS) Microwave Sounder Data 1979-2008 – click for larger image
At first I was surprised at the agreement between UAH and RSS in the percentages of warm and cool, but then I realized that these data-sets both came from the same instrument on the spacecraft and the only difference is methodology in preparation by the two groups UAH and RSS. So it makes sense that there would be some agreement in the histograms.
Here we have the land-ocean surface data-set from HadCRUT:

Hadley Climate Research Unit Temperature data 1979-2008 – click for larger image
Here, we see a much more lopsided distribution in the histogram. Part of this has to do with the positive trend, but other things like UHI, microsite issues with weather station placement, and adjustments to the temperature records all figure in.
Finally we have the GISS land-ocean surface data-set:

NASA Goddard Institute for Space Studies data 1979-2008 – click for larger image
I was surprised to learn that only 5% of the GISS data-set was on the cool side of zero, while a whopping 95% was on the warm side. Even with a rising temperature trend, this seems excessive.
When the distribution of data is so lopsided, it suggests that there may be problems with it, especially since there appears to be a 50% greater distribution on the cooler side in the HadCRUT data-set.
Interestingly, like with the satellite data sets that use the same sensor on the spacecraft, both GISS and HadCRUT use many of the same temperature stations around the world. There is quite a bit of data source overlap between the two. But, to see such a difference suggests to me that in this case (unlike the satellite data) differences in preparation lead to significant differences in the final data-set.
It also suggests to me that satellite temperature data is a more representative global temperature metric than manually measured land-ocean temperature data-sets because there is a more unified and homogeneous measurement system, less potential bias, no urban heat island issues, no need of maintaining individual temperature stations, fewer final adjustments, and a much faster acquisition of the data.
One of the things that has been pointed out to me by Joe D’Aleo of ICECAP is that GISS uses a different base period than the other data-sets, The next task is to plot these with data adjusted to the same base period. That should come in a day or two.
UPDATE1: I’ve decided to make this a 3 part series, as additional interest has been generated by commenters in looking at the data in more ways. Stay tuned for parts 2 and 3 and we’ll examine this is more detail.
UPDATE2: I had mentioned that I’d be looking at this in more detail in parts 2, and 3. However it appears many have missed seeing that portion of the original post and are saying that I’ve done an incomplete job of presenting all the information. I would agree for part1, but that is what parts 2 and 3 were to be about.
Since I’m currently unable to spend more time to put parts 2 and 3 together due to travel and other obligations, I’m putting the post back on the shelf (archived) to revisit again later when I can do more work on it, including show plots for adjusted base periods.
The post will be restored then along with the next part so that people have the benefit of seeing plots and histograms done on both ways. In part 3 I’ll summarize 1 and 2.
In the meantime, poster Basil has done some work on this of interest which you can see here.
UPDATE3: Part 2 is now online, please see it here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0ceeee4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter


A new study documents the dominance of internal variability in decadal-scale global temperature changes and suggests we may experience a global cooling trend during the next 15 or even 30 years despite rising greenhouse gases.
Maher et al. (2020) acknowledge that internal variability in global surface temperature variations is “a difficult concept to communicate” because we have very few observations of its impact and so we must rely on assumptions about how the climate system might work.
Those who try to explain how internal variability affects global surface temperature often use the “Butterfly Effect” paradigm; they assume that small changes now can lead to larger changes decades from now.

Because global temperature trends are “largely determined by internal variability”, global cooling or another warming hiatus could very well be observed over the next decade. Actually, as Maher and colleagues explain, “even out to thirty years large parts of the globe (or most of the globe in MPI-GE and CMIP5) could still experience no-warming due to internal variability“.




Image Source: Maher et al., 2020


Share this...FacebookTwitter "
"It’s one of the big mysteries in my career as a marine biologist. Something lurking in the seas off Britain has been chomping away at local porpoises and none of the usual suspects fit the bill. Now scientists have finally identified the cuddly culprit. I first became aware of the attacks on porpoises – beakless, smaller relatives of dolphins – as I run the UK shark tagging programme and am often asked to comment on possible sightings. In both 2010 and 2011 I was sent pictures from the Norfolk coast of porpoise carcasses with a considerable amount of tissue bitten away.  The common assumption was that these were due to shark attacks. However none of the photographs of the wounds showed the characteristic punctures caused by the multiple rows of shark teeth, such as displayed by human victims of shark attacks.  The UK isn’t exactly known for its deadly sea beasts. While a number of shark species do live round the coast and in the North Sea, the most common are too small to inflict the bites we were dealing with. Larger sharks capable of removing that amount of tissue are very rare.  Invariably the popular press claims attacks like these are evidence of great white sharks. After the 2011 find near Great Yarmouth, Norfolk, the Daily Mail thought a “giant shark or killer whale could be stalking the coast”, The Sun even said it might be Jaws himself. Given that there are no fully substantiated reports of white sharks in UK waters and that in any case such a shark could quickly dispose of a whole porpoise, this is fanciful.  But at last, we have a more likely explanation. The unfortunate Norfolk porpoises aren’t alone. In fact, huge numbers of harbour porpoises have been washing up on the shores of the North Sea. They shared the same nasty-looking bite marks. A team of Dutch scientists has now identified grey seals as the culprits. Their work, reported in the Royal Society journal, reveals DNA from grey seals has been found in bite marks on porpoise carcasses. The researchers examined 721 dead porpoises in detail. The sheer numbers enabled them to identify the key characteristics of seal bites including substantial loss of skin and blubber, puncture wounds (often repetitive) to the head, tail and flippers, plus series of parallel scratches anywhere on the body left by seal claws.  A flow chart has been produced using these marks to determine if the porpoise was attacked by a seal and if it could have escaped. This will be invaluable to those undertaking autopsies of porpoises in the future.  The majority of confirmed seal attacks are on juveniles. Since many of the porpoise carcasses were found on Dutch bathing and surfing beaches the authors offer up the thought that humans may be at risk. Perhaps this serves us right, as it seems humans may have triggered this change in seal diet. Along with their close relatives dolphins and whales, porpoises often become entangled in fishing gear and some attacks may simply be seals scavenging trapped porpoises. Having got used to the scavenged fare, the authors speculate that seals may have turned to attacking live porpoise. Porpoises have a hard life. Even dolphins attack them. They are victims of the fishing industry and subject to an increasing amount of noise from boats, oil rigs and wind turbines.  Now, possibly triggered by our activities, there is yet another pressure on this species: hungry seals."
"
Share this...FacebookTwitterHans von Storch’s blog brings our attention to an excellent German report by normally green ZDF public television.
The report takes a critical view of Europe’s energy policy and reaches the conclusion that it’s a failure. My last post Billions Of Euros For Nothing Called A Success Story illustrates this beautifully.
The ZDF interviews a leading finance researcher, Professor Dr Kai Konrad, and here’s what the ZDF report says:
– Start clip (German)-, content in English:
After 20 years of conference after conference after conference, a sort of traveling climate circus on a worldwide tour, Copenhagen became the highpoint of absurdity in December of last year – a political and media overkill with the aim of nothing less than to rescue the planet. The conference failed yet again. It all gets down to money.
Professor Dr. Kai Konrad is a distinguished finance researcher at the prestigious Max Planck Institute in Munich and a close advisor to the Federal Ministry of Finance. He and a team of researchers drew up an expert assessment of Germany’s climate policy.
The assessment was so damning that the Ministry quickly removed it from its website.
The assessment took a hard look at the 1st Commandment of climate policy: reduce CO2 emissions, and how a relatively small group of countries decided – unilaterally – to reduce CO2 emissions. The researchers writing the assessment deemed this a grave error. Professor Konrad says:
When a small group of countries sit down and say they want to  do something good for the climate, and reduce their emissions, it has practically no effect on the total amount of emissions worldwide. It means the rest of the world picks up the slack and just emits more.
In effect it means that the countries who cut emissions incur all the costs but no benefits. And the countries that don’t cut emissions, profit. So it’s highly worth it for these so-called “free-riders” who don’t sign on. What has the Kyoto protocol produced?
Since 1990 worldwide CO2 emissions have increased 36% and the few countries that have reduced their emissions have had immense costs, estimated to be $150 billion.
When it comes to CO2 emissions, the European Union is a global power. Especially Germany has been a leader in cutting emissions – already 20% less than 1990. Professor Konrad says:
The fact that Europe is a leader in cutting emissions will only lead to other countries slacking off, and thus the costs are merely shifted from the countries that don’t play along to Europe. So whatever progress Europe makes in cutting emissions just gets lost to countries like USA and China.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And so the circus goes on. The other countries are happy about the cuts, and the EU carries all the costs. Europe’s Climate Commissar estimates the costs will be:
€500 billion ($620 billion) in the next 10 years.
Germany is the leader in this craziness, and is expected to cut emissions by 40% by 2020. This is to be accomplished by Germany’s EEG Gesetz, or Energy Feed-in Act, which forces power companies to purchase renewable energy at exorbitant prices from anyone who produces them and to deliver them to consumers, who then must pay through the nose. Professor Konrad says (in summary):
From a theoretical point of view, the EEG brings no benefit. It brings nothing because the system of buying CO2 emissions certificates doesn’t work.
All the certificates do is ensure that the CO2 gets produced elsewhere. Professor Konrad:
The Feed-in Act is to be criticised in my view because it is no longer transparent as to what an enormous redistribution it creates and the huge subsidies that flow out of the pockets of consumers and into the hands of those who profit from it.
By the end of the year German consumers will have paid €62 billion ($75 billion) without seeing any CO2 reduction. In Professor Konrad’s and his colleagues’ view:
The policy of avoiding the production of CO2 is a failure, nationally and globally.
As a result, Professor Konrad’s recommendation is to use a different strategy (one that even the earliest and most primitive of man used):
A D A P T A T I O N
The researchers say this policy would be much more successful, and certainly much cheaper than the current CO2 elimination policy.
– End clip –
Now, I wonder if our clever politicians will muster the intelligence that even our early Neanderthal ancestors had millions of years ago, and adopt this strategy?
Don’t hold your breath.
Share this...FacebookTwitter "
"

Government policies encourage Americans to live in risky places on seacoasts and along flood‐​prone rivers. Disasters happen, governments bail people out, they rebuild in the same places, bad incentives stay in place, further disasters strike and more dollars and lives are lost.   
  
  
The _Washington Post_ reported on recent flooding along the Mississippi River, which I’ve excerpted below.   
  
  
But first, here are some general points about flooding and governments:   




These points are developed further here, here, here, here, here, here, and here.   
  
  
Here is what the _Washington Post_ reported:   




The city [St. Charles, MO] of about 70,000 has long grappled with flooding from the Missouri and Mississippi rivers, as well as rising water from creeks and streams — making it one of the most flood‐​prone regions in the state, with some $18 million in flood insurance claims paid out since 1970 by the Federal Emergency Management Agency.   
  
  
Yet that hasn’t stopped the city from planning a $1.5 billion riverfront development along the Missouri’s banks, 120 acres of upscale shops, restaurants and apartments mostly in the river’s flood plain, an area that has been partly submerged this summer.   
  
  
Several hundred miles to the south, Louisiana is experiencing the fallout from decisions like that one — decades of rampant development behind tall levees that have cut the Mississippi and its many tributaries off from the vast open floodplains the rivers once carved for themselves.   
  
  
Tropical Storm Barry, which made landfall Saturday as a hurricane, is predicted to travel up the Mississippi toward St. Louis, deluging surrounding areas as it goes, with rainfall that will be channeled back toward the Louisiana coast in days to come — the latest potential catastrophe.   
  
  
“The major cause of record, recent flooding is entirely man‐​made — the dramatic constriction of our large rivers by oversized levees, flood plain development and structural narrowing for barge traffic,” said Robert Criss, professor emeritus with the Department of Earth and Planetary Sciences at Washington University in St. Louis.   
  
  
This year’s historic floods throughout the Midwest caused billions of dollars in damages; washed out highways, bridges and dozens of levees; swamped crop lands and cities; sent residents fleeing for their lives; and left a death toll in several states.   
  
  
Decades of development have contributed to the problem. Claims to FEMA’s flood insurance program have increased rapidly in the past two decades and spread beyond coastal regions.   
  
  
… Yet with millions of people living in flood plains and shipping and tourism economies built on these key waterways, there is little political will for change. Environmentalists charge that jurisdictions hungry for tax revenue are continuing to plan risky projects without taking floods’ worsening intensity into account, heedless of the economic and human consequences.   
  
  
“It’s lunacy,” said David Stokes, executive director of the Great Rivers Habitat Alliance. “They’re continuing to build in places where Mother Nature intended water to go. And there’s no end to it.”


"
"
Share this...FacebookTwitterResearchers can no longer blame inconvenient cold winters on Arctic warming. No scientific basis, new study shows.
Image: NASA (public domain)
By Die kalte Sonne
(German text translated by P. Gosselin)
The Arctic is warming faster than the mid-latitudes. It’s the so-called Arctic amplification (AA). According to a study by Polvani et al. 2020, half of the Arctic warming is due to ozone-depleting substances – and not CO2. This was of course a great surprise, as we have already reported here. So no wonder that the climate models are going crazy, since they have so far attributed almost the entire warming to CO2.
CO2 has been hopelessly overbooked, as it now turns out.
In Nature Climate Change, a paper by Cohen et al. 2020 has been published and it shakes another myth. In the past, researchers were too happy to tell us that strong Arctic warming leads to harsh winters in mid-latitudes. However, there is a problem: the models are largely unable to reconstruct this presumed relationship: The paper finds:
Divergent consensuses on Arctic amplification influence on midlatitude severe winter weather
The Arctic has warmed more than twice as fast as the global average since the late twentieth century, a phenomenon known as Arctic amplification (AA). Recently, there have been considerable advances in understanding the physical contributions to AA, and progress has been made in understanding the mechanisms that link it to midlatitude weather variability. Observational studies overwhelmingly support that AA is contributing to winter continental cooling. Although some model experiments support the observational evidence, most modelling results show little connection between AA and severe midlatitude weather or suggest the export of excess heating from the Arctic to lower latitudes. Divergent conclusions between model and observational studies, and even intramodel studies, continue to obfuscate a clear understanding of how AA is influencing midlatitude weather.”


		jQuery(document).ready(function(){
			jQuery('#dd_dd3235f998397df12e765fa01f9ebb45').on('change', function() {
			  jQuery('#amount_dd3235f998397df12e765fa01f9ebb45').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Serious poisoning by plants is very rare in the UK so the death of a gardener in Hampshire after brushing against a deadly flower was extremely unusual. Despite the British countryside’s genteel reputation there are a surprisingly large amount of poisonous plants growing both in the wild and in gardens. Some just cause discomfort, but others have the potential to kill. Here are five to watch out for. Wolfsbane belongs to the plant genus Aconitum, a group of plants which are all poisonous.  The native plant, also called monkshood, has large leaves with rounded lobes and purple hooded flowers. Although it can be found throughout the UK, cases of accidental poisoning are very rare.  Still, people plant it in their gardens, possibly unaware of the potential hazard.   It is one of the most toxic plants that can be found in the UK, the toxins in the plant can cause a slowing of the heart rate which can be fatal and even eating a very small amount can lead to an upset stomach. But its poison can also act through contact with the skin, particularly if there are open wounds.  The roots are thought to be especially poisonous but even so, people have been known to eat the roots and survive so it is very difficult to know how much contact is needed to kill someone.  As with any poisonous plant, the best way to avoid it is to learn to recognise what it looks like.  Once you can recognise it then you can make sure you don’t eat it and only handle it with gloves on. Foxglove grows in woodlands and hedgerows. It is a common garden plant, popular due to its tall purple flowers.  Its large soft leaves grow in a rosette.  If any part of the plant is eaten it causes vomiting and diarrhoea together with other unpleasant symptoms, and just like wolfsbane it can slow the heart down causing heart attacks.  Even contact can cause irritation to the skin.   However, foxglove has saved more lives than it has cost as drugs derived from the plant are used to treat heart conditions. The cuckoo pint (Arum maculatum) or lords and ladies, is found growing in woodlands and hedgerows. Its flowers are poker-shaped surrounded by a green leaf-like hood but it is the bright red and orange berries of this plant that are poisonous.  If eaten, the berries cause irritation in the mouth and throat which leads to swelling and pain and can result in difficulty breathing. It also causes an upset stomach. As its name suggests, deadly nightshade is another poisonous plant. Deadly nightshade is most common in central, southern and eastern England but is also found less commonly in other parts of the UK. It is a shrubby plant with purple bell-shaped flowers and shiny black berries.   In the first instance poisoning results in symptoms including dilated pupils, loss of balance and a rash but it can eventually lead to hallucinations and convulsions. Atrophine, a drug extracted from nightshade, is used in eye examinations to dilate the pupil. It’s even used as a nerve gas antidote. Hemlock isn’t native to the UK but can be found in most areas.  It grows in ditches and riverbanks and in disturbed area such as waste ground and rubbish tips.   Hemlock is a tall green plant with purple spots on its stem and leaves similar to the carrot plant, it has white flowers.  If it is eaten hemlock causes sickness and in severe cases it can kill by paralysing the lungs."
"

In the new Cato study “Long Hot Year: Latest Science Debunks Global Warming Hysteria” (Policy Analysis no. 329), climatologist Patrick J. Michaels reports that Vice President Al Gore’s latest alarmist claim—that 1998’s warmer than normal temperatures resulted from global warming—isn’t supported by the scientific evidence. Michaels, professor of environmental science at the University of Virginia and senior fellow in environmental studies at the Cato Institute, writes that “the record temperatures were largely the result of a strong El Niño superimposed on a decade in which temperatures continue to reflect a warming that largely took place in the first half of this century.” Satellite data show clearly that “the warmth of 1998 is an anomalous spike rather than a continuation of a warming trend.” Michaels notes that “imposing an El Niño upon an already warm decade creates the illusion of rapid global warming,” as he predicted it would in his 1992 Cato Institute book Sound and Fury. The fact is that “observed global warming remains far below the amount predicted by computer models that served as the basis for the United Nations Framework Convention on Climate Change.”



 **Perils of Government Investing**



The questions at the center of the upcoming debate on Social Security’s future will be, What kind of private investment, and who should do the investing? Michael Tanner, director of Cato’s Project on Social Security Privatization, warns in “The Perils of Government Investing” (Briefing Paper no. 43) that those are critical questions because government investment of Social Security funds could make the federal government the largest shareholder in American corporations. Tanner points out that Federal Reserve chairman Alan Greenspan says that it is impossible to “insulate” government investment “from the political process.” Government investment of Social Security payroll taxes would result in a dangerous mix of government involvement in corporate governance and “social investing.” 



**Failed Intervention in Bosnia**



The three‐​year‐​old Dayton Agreement has failed to accomplish its main objective and should be abandoned, writes Cato foreign policy analyst Gary Dempsey in a new study, “Rethinking the Dayton Agreement: Bosnia Three Years Later” (Policy Analysis no. 327). “The Clinton administration’s continued and uncritical devotion to the agreement is compromising U.S. national security and saddling the United States with an expensive yet futile nation‐​building operation of unknown duration.” The study finds that the “goal of creating a unitary, multiethnic Bosnian state is not realistic.” The Clinton administration has refused to consider changing course, however. “The administration needs to jettison its presumption that there are only two options for U.S. policy on Bosnia: adhere to the Dayton Agreement or cut and run. There is another option: a negotiated three‐​way partition of Bosnia overseen by a European‐​led transition force. That is the most politically feasible way to create the conditions necessary to allow the departure of U.S. troops at the earliest possible date.” 



Throw the Other Guy’s Bums Out, Too In the new Cato study “What Term Limits Do That Ordinary Voting Cannot” (Policy Analysis no. 328), Harvard Law School professor Einer Elhauge addresses the questions: Why do the same voters who vote for term limits also routinely vote to return senior incumbents to office? Why don’t they vote the bums out? The answer is straightforward: “Voting your bum out is not a solution when what you want to do is oust the other districts’ bums. For that you need term limits.” The fact that incumbents tend to get reelected at very high rates even though large majorities of voters favor term limits is perfectly logical, he notes. “A district that ousts its senior incumbent suffers a loss of relative clout in the legislature. To avoid that loss of power, it behooves individual districts to vote to retain their incumbents.” The solution is also straightforward: “If all the districts collectively could agree to oust their senior incumbents simultaneously, no district would suffer a loss of relative power, and each district would gain more accurate representation. Term limits are effectively just such an agreement.”



 **U.S. Foreign Policy Spawning Terrorism**



One‐​third of all terrorist attacks worldwide in 1997 were perpetrated against U.S. targets. That is a very high percentage “considering that the United States—unlike nations such as Algeria, Turkey, and the United Kingdom—has no internal civil war or quarrels with its neighbors that spawn terrorism,” writes Ivan Eland, Cato’s director of defense policy studies. “The major difference between the United States and other wealthy democratic nations is that it is an interventionist superpower.” In “Does U.S. Intervention Overseas Breed Terrorism? The Historical Record” (Foreign Policy Briefing no. 50), Eland points out that the Pentagon’s own Defense Science Board finds that “a strong correlation exists between U.S. involvement in international situations and an increase in terrorist attacks against the United States.” Eland recommends that the United States adopt a policy of military restraint: “The United States could reduce the chances of devastating—and potentially catastrophic—terrorist attacks by adopting a policy of military restraint overseas.”



 **Nuke the Test Ban Treaty**



The U.S. Senate should reject the proposed Comprehensive Test Ban Treaty and fund the resumption of limited testing, writes defense analyst Kathleen C. Bailey in a new Cato paper. In “The Comprehensive Test Ban Treaty: The Costs Outweigh the Benefits” (Policy Analysis no. 330), Bailey argues that the treaty is unenforceable, unverifiable, and unwise policy. Signed by President Clinton in September 1996 and to be considered by the Senate this year, the CTBT has limited political benefits and is “not worth the high cost to U.S. national security.” Weapons testing is essential to U.S. national security, according to Bailey, because “evolution in technologies for safety, nuclear delivery systems, and enemy defenses may render the now‐​modern U.S. nuclear arsenal technologically obsolete or less safe.” She notes that “at present, the United States is two years or more away from being able to conduct a nuclear test. This lack of readiness will inevitably worsen as skilled experts retire and die, equipment ages or becomes obsolete, and financial support erodes.” Bailey believes that, “from a purely technical standpoint, it would be most prudent for the U.S. Senate to reject the CTBT and to allocate funds for resumption of U.S. testing and for reconstruction of the U.S. nuclear weapons production infrastructure.” But she notes as well that “it may be politically desirable to undertake some limitations on testing.” 



**Trashing Government Intervention in Refuse**



One of the biggest environmental issues at the state and local levels is garbage—how to collect it, dispose of it, recycle it, and pay for doing so. In a new Cato study, “Time to Trash Government Intervention in Garbage Service” (Policy Analysis no. 331), Peter VanDoren, assistant director of environmental studies at Cato, challenges the reigning orthodoxy that the government must decide those questions for citizens. That belief, VanDoren points out, is grounded in the assumption that economies of scale and collection route density mean the government must have a monopoly on trash collection. VanDoren’s research on the economics of refuse markets reveals that government management of garbage service is unnecessary and counterproductive. He argues that homeowners should be allowed to choose among competing collection firms and that homeowners, not bureaucrats, should have the final say about what kind of service they want.



 _This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)



<em><a href=”/people/patrick-michaels”>Patrick J. Michaels</a> is Director of the Center for the Study of Science at the Cato Institute.</em>
"
"

How many hurricanes do you think will hit the East Coast of the United States in 2015? Will the Arctic ice sheet disappear next year? How fast will the U.S. economy grow? What will the level of the Dow Jones stock index be at the end of 2015? Which team will win the World Series?



Go back and look at predictions made by the experts, and then look at what really happened. The climate alarmists 15 or so years ago were forecasting catastrophic events by this time. Yet sea levels have not been rising any faster than they have been for centuries. The major climate models were projecting steady rises in global warming each year, yet average temperatures have not risen for 17 years. Al Gore and his alarmist crowd told us that the Arctic would be free of sea ice during the summer by now and that we would be having more and stronger tornadoes and hurricanes. The Arctic sea ice is still with us, and few ships dare sail there. Many tornado and hurricane records have been broken — not because there were more — but because there have been fewer. Florida has gone a record nine straight seasons without a significant hurricane.



None of the above disproves climate change, but it should caution those who have made many rash predictions. The economist‐​philosopher F.A. Hayek warned about “the limits of knowledge” and the “fatal conceit” exhibited by so many “experts.” The communists and socialists claimed that they could allocate resources and income better than markets. These false claims ultimately destroyed the lives of tens of millions and caused untold human misery. Despite the never‐​ending failures of socialist and other collectivist schemes (such as Obamacare), colleges, governments and the media are still filled with smug — but ignorant or uncaring — individuals (think Jonathan Gruber) who still think they are smarter than markets, and thus have the self‐​appointed right to control your life.



Economists have little to crow about when it comes to forecasting. Most of them missed calling the Great Recession. The Federal Reserve, which employs hundreds of economists, many from the best schools, kept predicting 4 percent‐​plus economic growth each year, after the recession bottomed in 2009. In fact, actual growth has been about half of what they predicted — but perhaps 2015 will be the year of 4 percent growth. Too many of my fellow economists, including many of those in the administration, get things wrong, in part, because they still use Keynesian economic models that treat increases in government spending as a positive rather than a negative, among other errors.



To make an accurate forecast, one needs to know what the Fed will do in regard to monetary policy and what Congress and the administration will do in terms of taxing, spending and regulation. One also needs to know what the economic policies of other countries will be — since the United States is not an island unto itself — and when wars and tsunamis will occur. The impossibility of knowing all of this does not mean that it is not useful to attempt to forecast, but merely that it is not scientific in the way that we can precisely predict the boiling point of water at a specific atmospheric pressure.



Forecasters need to have a good understanding of the major variables that might greatly affect their reasoning. The safest starting point for most forecasts is what happened in the previous period. Tomorrow’s weather is likely to be similar to today’s. One might begin an economic forecast by assuming next year is likely to look much like this year, then alter the forecast based on assumed changes in policy. For instance, if you assume the Republican‐​led Congress is likely to reduce government spending as a share of gross domestic product, and if you believe, as many of us do, that government spending (at the current high levels) is a drag on growth, then it is sensible to boost the growth estimate a bit — other things being equal. This same exercise needs to be repeated with each major variable — taxes, regulations, monetary actions, changes in oil prices — and what Russia’s Vladimir Putin might do next.
"
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"

The National Academy of Sciences (NAS) has just issued yet another report on global warming. A substantial part of it is based upon the “U.S. National Assessment” (USNA) of global warming, yet another government report that came out right before the last election. In turn, it was based, in large part, on computer models used in yet another government report on global warming, from the United Nations’ Intergovernmental Panel on Climate Change.



Together, the best I can tell, these were produced by a total of a couple thousand people. Together, they were dead wrong about the most fundamental aspect of climate change, namely how we are changing our atmosphere.



First, a little physics. It has been known since at least 1872 that carbon dioxide–a byproduct of combustion, or the meta‐​respiration of civilization, dependent upon your point of view–traps warming radiation. It has also long been known that its warming effect becomes less at increasingly high concentrations. As a result, a constant increase in atmospheric carbon dioxide results in less and less warming over time.



So, the only way to keep warming the atmosphere at a constant rate is to add carbon dioxide at an increasing, or exponential rate. This is what the U.N., the USNA, and the National Academy all assume … at least inasmuch as the Academy report states its parentage is the USNA, in its section titled “Consequences of Increased Climate Change”.



The fact is that carbon dioxide has not accumulated in the atmosphere at an exponential rate for the last quarter‐​century: This is obvious to anyone with an Internet connection (in order to download a graph of the carbon dioxide history), eyeballs and a ruler. You will see that the behavior of the last 25 years looks a lot more like a straight line than an upward‐​pointing curve. Those with statistical expertise could also enter the data into a program like Excel and see if drawing an up‐​curve through the data results in a significant improvement over a straight line. The answer, for the last 25 years, is no.



This can only mean one thing. The linear change in carbon dioxide for the last quarter‐​century will result in an inevitable and inexorable slowing of global warming in coming decades.



So why isn’t carbon dioxide increasing exponentially, even as the number of people are? Two reasons: We are becoming increasingly efficient, and the planet is getting greener.



We now produce a (deflated) dollar’s worth of stuff using about half as much energy as we used to. Neither the U.N. nor the EU, despite their blustering, forced us to do this. Instead, stockholders made it happen, demanding more output for less cost. There’s every reason to expect this behavior to continue.



The earth got greener because more carbon dioxide made the plants grow better, and a warming, primarily of the winter, lengthened the growing season. Will this greening stop, as some fear, when forests become mature and fall over? Not if they’re turned into houses, which last for hundreds of years. This is one very good argument for managed, as opposed to “natural” forestry.



How could the Academy, the National Assessment Team, and the United Nations fail to notice that they got the basic behavior of carbon dioxide (and therefore, future warming) wrong? Could thousands of scientists simply miss what anyone with a hard drive and a ruler can see? Of course not. But where would my profession be if we couldn’t scare you into funding us any more?



In a world where he who presents the scariest argument gets the most funding, everything is threatening and nothing is benign.



It’s not just in climate science, either. How about cancer? We spend just about as much money there as we do on global warming. The government regales us with impressively weak associations between diet, urban air, polar ozone depletion and death, when the lion’s share of cancer deaths would go away if people would simply choose not to smoke ciggie butts. Which causes more cancer–increasing ultraviolet radiation by 2 percent from ozone depletion (itself maybe too large an estimate) or going to the beach and taking off 98 percent of your clothes? But simple behavior changes cashier armies of regulators, who, thank you, would much rather be employed. So we tout the obscure while ignoring the obvious.



Which, sadly, is why thousands of the best minds in America aren’t eager to tell you that changes in atmospheric carbon dioxide have been so slow that global warming is likely to slow down in future decades. Exactly when, though, no one knows. Please pass the funding until I figure this out.
"
"
Share this...FacebookTwitterOther blogs have mentioned today a report from the German financial daily, Handelsblatt here, but didn’t provide many details, and so I’ve decided to shine a little more light on the article. It is indeed frightening.
The German government has been generously subsidising renewable energy sources for years now, and it’s going to cost the German consumer a bundle – and soon.
The big price driver is solar energy. Year after year more and more panels are getting installed on German roofs and far surpassing even the most optimistic projections. But that shouldn’t be a surprise because Germany’s Energy Feed in Act (EEG Gesetz) guarantees solar energy system operators a fixed tariff for 20 years, making solar energy systems extremely lucrative for those who have them.
According to the Rhine Westphalia Institute for Business Research (RWI) the net costs of all photovoltaic system installed between 2000 and 2010 add up to a whopping $107 billion, Subsidies for renewable energy are going out of control.
An open letter written by Johannes Lackmann, former director of the German Association of Renewable Energy, caused many to take a closer look. Lackmann warns:
Companies are positioning themselves on the same square as the old industries, who failed to modernise and keep up with the market demands because they came to rely on generous subsidies paid by governments to survive. The EEG Act must not be allowed to be misused as cushion to sleep on.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed the extreme comfort of the EEG subsidies will lead to an additional 9000 MW of solar energy capacity to be mounted on roofs this year alone in Germany. It’s a run-away train that will have serious consequences.
Recall that solar energy generators are guaranteed payment for 20 years. For systems going online this year, an average of almost $0.40/kw-hr is guaranteed by law until 2031. By comparison conventional energy is traded on the EEX  exchange for just over 6 cents. The huge difference is passed on to the German consumer.
According to the RWI the net costs for all photovoltaic systems installed between 2000 and 2010 over their 20 year operating life will add up to $107 billion. According to the Handelsblatt:
This sum is more than one quarter of the entire German annual federal budget. Yet the amount of solar energy as a share of the total energy produced is still puny despite the huge subsidies. It is only 1 percent.
Just the photovoltaic systems installed this year will lead to an additional $33 billion dollars in costs over their 20 year operating life. Electrical energy rates will climb 10% in 2011. Energy giant RWE just announced it will increase rates by 7.3% in August.
Sure Germany has cut back the subsidies – some, but the prices of solar panels and systems have fallen even faster, and so as a result they are even more lucrative. Lackmann thinks reductions in subsidies are long overdue, and the industry will not be doing itself a favour attempting to fight them off.

They don’t provide any incentive for investment in R&D. Leading German solar companies invest less than 2% of their sales turnover in R&D. This is far below what a company like Siemens invests in R&D.

Share this...FacebookTwitter "
"
NOTE: Mike alerted me in comments about this article he wrote along the lines of my story on Color and Temperature: Perception is everything. I thought this would be good to examine again.  This article below is re-posted from John Daly’s website, and was originally published July 7th, 2002. – Anthony

By: Michael Ronayne

In a story titled “Coloring Climate Change” by Nick Schulz, Tech Central Station reported that key documents, in a US government report titled “The National Assessment of the Potential Consequences of Climate Variability and Change“, were “doctored” to distort public perceptions of climate change. The report was published by the United States Global Change Research Program. According to their own web page, the USGCRP coordinates the research of ten Federal departments and agencies with active global change programs and provides liaison with the Executive Office of the President. The budget of the USGCRP in fiscal year 2002 was approximately $1.7 billion US dollars.
The National Assessment report has served as the basis for parts of the 2001 National Academy of Sciences’ report “Climate Change Science: An Analysis of Some Key Questions” prepared for President Bush on the state of climate science and, most recently, for the highly controversial “U.S. Climate Action Report – 2002“, covertly issued by climate alarmists within the Environmental Protection Agency, with the objective of embarrassing the Bush Presidency.
The TCS story displays two graphics, shown below. The graph on the left is the one which was circulated during the public comment period after the original draft was developed. It compares the Canadian Model with the Hadley Model for the lower 48 States for the summer months of June through August, over the next 100 years. The TCS story provides additional background on the two graphs and is highly commended to your attention. Then the disparity between the two models’ future forecasts, cast doubt on the predictive capacity of the Canadian and Hadley models, the USGCRP issued the final report on the right, with the color scale altered to obscure the differences between the two models.











Unfortunately for the USGCRP, the two models show the areas of warming and cooling to be occurring in widely different sections of the United States. The USGCRP’s solution to this conundrum was to alter the temperature color scale by eliminating yellow and green, and extending the color orange into negative temperature ranges as low as -1.0°F, thereby implying warming,  when in fact the models were showing no temperature change or cooling for some localities.






 




Above: When the “Draft” and “Final” copies of the USGCRP graphs are animated, employing a technique used elsewhere on this web site, the amateurish nature of the deception becomes painfully obvious.
Not only was the distorted temperature color scale used to obscure the next 100 years of temperature models, it was also used to change the perception of the United State’s         past climatic history. The page “Overview:  Looking at America’s Climate” contains a graphic titled “Temperature Change” (shown below), which attempts to minimize the significant cooling which occurred in the Southeastern United  States during the 20th Century. This is achieved by coloring even the zero or `no change’ temperatures in light orange, and blending colors in such a way as to make it almost impossible to differentiate anything between about 0° and 5°.  Not even the IPCC has as yet stooped to this level of deception. 

On the same web page, there is another graph titled “Summer Maximum and Winter Minimum Temperature Change” (shown         below), which contains the USGCRP’s final version of the Canadian and Hadley 21st Century Summer and Winter Models, again with a choice of color scheme which blends         everything from 0° to 5° into a deceptive spread of orange.  Even         areas which these models show will not change, are colored in orange.         What other purpose can this peculiar coloring scheme serve but to suggest         future warming         in areas where none is actually predicted by the model?
 
“The National Assessment of the Potential Consequences of Climate Variability and Change” report is comprised of three separate sections which represent themselves as addressing increasing levels of detail. The descriptions are those used by the USGCRP:
1. Overview Report:  Concise, well illustrated summary.
2. Foundation Report: Volume, more detailed than the Overview Report.
3. Background Information:  Learn more about the National Assessment.
The Overview Report is published in both HTML and PDF formats and contains all of the USGCRP graphs and most of the URLs, previously referenced. This report is clearly intended for the media and the general public. Its primary message is one of impending doom, associated with anthropogenic global warming.
I am not sure why the USGCRP expended the effort to create the Foundation Report. It has so many technical flaws, in terms of electronic publishing techniques, that anyone who attempted to read it, would be quickly discouraged from delving into its contents. The report is only published in two PDF formats. Each subsection of the report is comprised of two PDF files, one which is black and white, with extremely low resolution gray scale graphics. The second PDF file contains the color figures and graphs but only the text associated with each figure. As the figures associated with the text report are all but useless, because of the poor quality, the serious reader must have  two PDF files open and switch between both files to comprehend the report. What is interesting is that the PDF file titled “Potential Consequences of Climate Variability and Change“, which contained color figures, shows in Figure 13, the US temperatures using the altered color temperature scale, but in Figure 20, the Global temperatures are displayed using the  original color temperature scale found in the draft report. The only function of the altered color temperature scale is to obscure the differences between the Canadian and Hadley models for the 21st Century United States.  By contrast, the 21st Century Global graphs were not altered in this way.
In the Background Information section, things become interesting. On a deeply buried page at “VEMAP Trend Maps” the original high resolution images, on which the draft graphics were based, can still be found. The individual graphs are: “CGCM1 Maximum Temperature Trend (JJA)” and “HadCM2 Maximum Temperature Trend (JJA)“.
One could engage in endless speculation as to why the USGCRP went to the trouble of altering the first two sections yet failing to alter the third, which contained the most incriminating information. The two most likely explanations are: (1) the Background Information section was overlooked and (2) the USGCRP did not expect anyone to find the original graphs from the Canadian and Hadley Models. Also, on the “VEMAP Trend Maps” page the Canadian and Hadley Models are not compared side-by-side, so the inconsistencies between the models are not as obvious. 
Of course, the USGCRP may not even care if the real results from the Canadian and Hadley Models are found. As long as the media continues to endlessly report only the results from the first two sections, the voices of a few skeptics can be safely ignored. 
Last year in another story, a question was asked for which no reply has been forthcoming:   If the evidence for global warming is that compelling, why is it necessary for those who believe in global warming, to misrepresent data in this manner to support their cause?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e5e5da4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Sky note:
 After sunset on Sunday, Dec. 23rd, the full Moon and Mars will rise in the east less than 2 degrees apart. They’ll be two brightest objects in the evening sky, as Mars is very near it’s closest approach (opposition) to Earth, which happened just a couple of days ago. 
It will look something like this:

Note the image is not to scale. Mars is bigger than it will actually appear.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1dacdc0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On May 4, 2012, the Cato Institute honored economist Mao Yushi as the recipient of the 2012 Milton Friedman Prize for Advancing Liberty. One of the most outspoken activists for individual rights and free markets in China, Mao has emerged over the last several decades as an instrumental voice against the nation’s heavy‐​handed, authoritarian grip. Before the presentation of the award, attendees heard a keynote address by Chris Christie, the governor of New Jersey. Governor Christie stressed the significance of communicating the ideas of freedom, and how his experience in office holds lessons for the nation at large.



GOV. CHRIS CHRISTIE: It’s a pleasure to be here this evening — to leave all that’s exciting in New Jersey on a Friday night, come down here to this sleepy little hamlet, and speak before you all.



Back in 2008, I remember Barack Obama talking about the lack of hope around the country. And although he and I always defined the solutions to that problem differently, the environment in which I found myself shortly thereafter was not significantly different.



When I first took office in New Jersey in January 2010, optimism was a hard thing to find. In the eight years before I became governor, our state had raised taxes _115 times_. From 2000 to 2009, New Jersey had — literally — a zero job growth decade. In the four years before I became governor, $70 billion in wealth had left the state — not diminished wealth, _departed wealth_. Our unemployment rate was over 10 percent, with 115,000 private sector jobs lost during the four years of my predecessor.



New Jersey had the highest tax burden in the country, the worst climate for small business, and a bloated state government that contained the most public workers per square mile in the country — yeah, you can laugh unless you live there. And it only got worse. In my second week in office, my state treasurer told me that, in the subsequent five weeks, we had to find $2.2 billion in cuts from money that had already been appropriated.



We essentially had to impound the money back from certain departments just to meet payroll — all in what was the second wealthiest state per capita in America. If you need any greater example of what happens to an economy when a government overtaxes, overspends, overborrows, and overregulates, just visit New Jersey in January 2010.



So what did we do? Thanks to New Jersey’s unique constitutional structure, which allows spending to be cut by executive order, my staff and I sat in a room over the course of three weeks and went over all 2,400 line items in the state budget that I inherited. The result was finally cutting $2.2 billion. And the great thing about operating by executive order was that, at first, I didn’t have to tell anybody.



But, after delivering the news in my first speech before the joint session, you can imagine the reaction from the legislature.



Reporters descended upon the floor as the Democrats began calling me names: Julius Caesar, Napoleon Bonaparte — all of those great leaders of the past that I admire. And I realized something. The way I confronted my first substantial problem in office set the tone for my administration. I made clear from the first day that decades of fiscal irresponsibility were no longer going to be tolerated. As I said on the campaign trail, I was ready to go to Trenton and turn it upside down.



Last year we passed a $2.3 billion tax cut for businesses, with nearly 70,000 new private sector jobs created. We’ve cut spending in every department of our state government, from areas that folks told me were the third rails of politics. Given that I was still upright, I decided to go after public pensions and benefits next. And what happened? For the first time in 10 years, a majority of New Jerseyans recently polled believe the state is back on the right track. On election day in 2009, that number was 19 percent. Today, it’s 53 percent.



The American people are ready to hear the truth. They know our government is out of control. And the only thing they care more about than today is tomorrow — because tomorrow is about our children and grandchildren, and today is just about us.



The bottom line is we took action — we did it with solid principles and strong leadership — putting our state’s interests ahead of partisan ones. We turned Trenton upside down. And in the difficult times that America is in now, the only way to govern is by treating our citizens as adults — by telling them the truth about the depth of our challenges and the difficulty of the solutions.



When we fail to do this, we pay the price as a country many times over. The domestic price is obvious: growth slows, unemployment persists, and we make ourselves even more vulnerable to the unpredictable behavior of rightfully skittish markets.



But there’s also a foreign policy price to pay. To begin with, we diminish our ability to influence the thinking and ultimately the behavior of others. Democracy is the best protector of human dignity, liberty, and freedom — and history shows that mature democracies are less likely to resort to force against both their citizens and their neighbors. Yet, all across the world — in the Middle East and Asia and Africa and Latin America — people are debating their own political and economic futures. They’re looking for inspiration, and we have a stake in the outcome of those debates. There’s no better way to reinforce the likelihood that others in the world will opt for more open societies and marketbased economies than to demonstrate that our own system is working well.



At one time in our history, our greatness was a reflection of our country’s innovation, determination, ingenuity, and the strength of our democratic institutions. When there was a crisis in the world, Americans found a way to come together to help our allies and fight our enemies. When there was a crisis at home, we put aside parochialism and put the greater public interest first.



Today, our ability to effect change has been diminished because of our own inability and unwillingness to effectively deal with our problems. Now, I understand full well that succeeding at home and setting an example is not enough. But it’s a start. And I realize that what I’m calling for requires a lot of our elected officials and our people. I plead guilty to that. But I also plead guilty to being an optimist, because I believe in what this country and its citizens can accomplish _if they understand what’s being asked of them_.



We seem to have forgotten that this is a human business. Day after day, I’ve spent time sitting with colleagues on both sides of the aisle, convincing them of my intentions and letting them know that I don’t believe compromise is a dirty word. There’s always a boulevard between compromising your principles and getting everything you want. You should never compromise your principles.



But you also need to understand that you’re not always going to get everything you want. The job of a leader is to find your way onto the boulevard between the two without driving into the ditch of compromising what you believe. And trust me, if you can do this in New Jersey, you can do it anywhere. That’s where my optimism comes from. See, I’m not looking to be loved. I get plenty of love at home — and when you’re looking for love in this job, that’s when deficits get run up.



However, if you make people understand that you’re willing to say no, but you’re also always willing to listen — that you’re willing to stand hard on principles, but you’re also willing to compromise when those principles won’t be violated — then respect will come. It’s about being consistent. It’s about leading by example. It’s about standing up for the things that we believe in, instead of simply trying to figure out which way the wind’s blowing. There’s no need for varnish anymore. In fact, I don’t think we have the luxury to put it on. Liberty and freedom and the human spirit are the most powerful things in the world — and we need to say that directly to the American people. They’re ready to hear it.



I want to thank the Cato Institute for setting an example of why liberty and freedom are so important to the future greatness of America. But please never forget that it’s not going to come without a fight. We need to fight hard, even harder than we are now because the stakes are too great to do anything less. Only then can we allow the United States to, once again, export hope and liberty and freedom around the world, not just because those values are a part of our past, but because we will be acting to make them a bedrock of our future.



 **MAO YUSHI:** Ladies and gentleman, I bring you all my humble greetings from China. Tonight, we are here together in this “Shining City upon a Hill” to celebrate our common beliefs, our common hopes, and our common commitment to the values that make the Cato Institute so very special. Those Cato values bring us together today, united as common citizens of the world. I like to think of us all as “Cato citizens.” The values of which I speak, of course, are peace, free markets, limited government, and the preservation of individual liberty.



Tonight, we celebrate those timeless values — the common thread that runs through all great civilizations of the world from the beginning of humanity. Regardless of whether your traditions and cultural roots are in Africa, the early settlements along the Yellow River, the Tigris and Euphrates, the Nile, or the valleys and mountain ranges of Meso‐​America, these universal values are our common heritage. They touch each heart and resonate in the basic moral fiber of our souls.



I am personally honored and humbled to be recognized as the recipient of the Milton Friedman Prize for Advancing Liberty. I cannot express enough my grateful appreciation for this recognition and for Cato’s many decades of invaluable guidance on the long road to liberty in China.



Over the last 83 years, I’ve endured many threats and fearful nights, years of deprivation and political persecution. My family and friends, however, provided the love, the loyalty, the dignity, and the moral compass to continue that journey, regardless of the headwinds.



They helped me remember the lessons of our nation’s past heroes and heroines, as well as our moral responsibilities to the future generations. They provided the light in the storm so that we could stay the course. I could not be here today without them.



My family and I are honored tonight, but we realize that we are not here just as sons and daughters of China. We are also here on behalf of three additional constituencies who join us in _absentia_.



First, we stand in the shadows of earlier honorees from Estonia, the United Kingdom, Peru, Venezuela, and Iran. Each represents societies that have traveled on their own Homeric Odyssey, which Cato has appropriately recognized. We honor them individually and we salute their great peoples and cultures.



Secondly, we stand in the foreshadow of Cato honorees yet to emerge. To these future Cato citizens, let them know that we see their courage. We feel their heartbeats, their yearning, and their allegiance to the values of the Cato Institute. In the years to come, let everyone here be our proxy and congratulate them on their journey to a shared better tomorrow.



Thirdly, and most importantly, we receive this honor on behalf of two constituencies in China.



The first is the tens of thousands of grassroots organizations who currently work every day to serve the common citizen of China, who strive to build a better and more humane tomorrow. Countless scholars, workers, peasants, teachers, students, volunteers, and friends struggle against the common enemies of humanity: tyranny, poverty, disease, and war. They are the real honorees of tonight’s prize.



The second constituency we represent is the tens of millions of Chinese over the last century who have sacrificed their lives along the road to overthrow feudal dynasties, defeat warlordism, and defend liberty against foreign colonialism and imperialist invasion. They have proven countless times that freedom is more precious than life itself, and their struggle is also revered here tonight. This award is accepted on behalf of them, with the solemn promise that your torch will be carried by each succeeding generation with the same energy, faith, and devotion you brought to your endeavors. We do this to continue to brighten our country’s future and to deliver the inalienable and universal rights of all human beings to our descendants.



This glow — when combined with the lamps carried by fellow Cato citizens around the world — will become beacons of light from the “Shining City upon a Hill,” spreading Cato values to the dark corners of the world. In your hands, more than mine, will rest the final success and failure and hopes for liberty of our peoples.



Many have sacrificed for China’s people, for her dignity and her liberty. Despite my eight decades — my now weak hearing and failing sight — I still remember the names of those who sacrificed for our country. I see their faces. I hear their voices. I feel their souls. Tonight I speak for those who cannot be here. Their sacrifices have not been in vain, and I thank the Cato Institute for giving their lives renewed meaning.



China is a very old country. She is a noble and wise civilization, with a grand history of fine art, science, medicine, philosophy, exploration, hard work, tolerance, and openness. It is a society based on balance known as the Golden Mean, or the middle way. This theme runs through our great traditions of Daoism, Buddhism, and Confucianism. Our people have always sought balance between the needs of the collective and the individual — and against extreme government.



China understands the power of economic liberalism. Her people know that free markets and liberty nourish each other, acting together as a force for social progress. Our long history is full of examples of people rejecting the arbitrary power of the state, refusing to subordinate the rights of the individual, and recognizing that unchecked collectivism stifles human creativity and productivity.



Systemic checks and balances against unlimited government power, corruption, and improper privileges — mechanisms which include free markets, the rule of law, an independent judiciary, and the shift to a smaller, more accountable government — need to be built and put in place permanently.



These are so important because of China’s horrific past. This generation has personally experienced mass repression through the government apparatus, witnessing the more than 50 million Chinese who died directly as a result of the influence of the rule of man over the rule of law. Both our leaders and our people know what is at stake for China’s future. They are all personally invested in ensuring that Cato’s values — tailored to China’s realities — can help build a strong, prosperous, and harmonious country in the finest traditions of our ancestors.



In modern times, many countries have recalibrated their societies to fundamentally improve them with these balanced values. In the early 1970s, there were about 40 democracies in the world. As the 20th century ended, there were 120 diverse countries with some form of self‐​government crafted by the people — a largely peaceful trend that continues to this day. China needs to learn from these powerfully instructive experiences.



In order to preserve societal harmony and build China’s better tomorrow, the government should further extend liberty, freedom, and free markets — and reestablish the peaceful rise of “good neighbor” policy to preserve regional and world peace. If China’s leaders can implement the essential principles common to every successful society, she can further contribute to the world’s peace, prosperity, and harmony.



I remain optimistic that China’s government will hear her people’s desire to make this vision a reality. If one examines our country objectively, they will see that there is great reason to be hopeful given China’s tremendous progress already. Edmund Burke, the great British parliamentarian, once warned that “a state without the means of some change is without the means of its conservation.” China has been changing and progressing — a process which does not require disruptive instant transformation.



Our country has successfully raised more than 300 million people from poverty — a huge number, even in China. Knowingly or not, accomplishments like this have been achieved by the balanced implementation of Cato values in the context of the Chinese society. China’s successful evolution over her long history has been rooted in the balance between rights and responsibilities — a balance that is in focus now as we approach a transition in leadership at the end of this year.



Those of us in China know that the rights of the individual do not come from the generosity of the state. In fact, we have very limited expectations from government. We ask only that the system lives up to our constitution, abides by our laws, and complies with the international covenants of the world community. The people of China know that the fruits of society are not the sole prerogative of the powerful and privileged few. As the next generation steps forward to battle against tyranny, poverty, disease, and war itself, I believe that our journey into the future will be long, but successful. All big rivers come from small streams. Our efforts in China are but one small stream.



Tonight’s constituencies — from the people of China to the other tributaries inspired by the timeless wisdom of the Cato Institute — will join together as a mother river to nourish the human spirit and wash away the hardships of our imperfect world. Thank you once again for this great honor.
"
"

The Wall Street Journal is reporting that a bunch of venture capitalists are now backing Norway’s Think electric-car company. Their plan is to bring the company’s Think City car to the U.S. in 2009 and build it here as well.
I drive a 2002 Ford Think electric car, the open frame model. I’m pretty happy with it, at 3 cents a mile, and I’ve put about 300 miles on it around town since buying it 3 weeks ago. It has gotten a lot of attention in my hometown of Chico, and people are constantly asking me how much it cost and where could they get one? The town is blessed with many alternate back routes, so I don’t have to travel the main congested roads.
The U.S. version is expected to travel 110 miles on a single charge and kind of resembles Smart’s ForTwo. The company expects the car to be priced under $25,000. It’s looking for a site in the U.S. to build U.S.-spec models because it’s cheaper to build an entire line here than it is to ship from Europe, thanks to the weak dollar. Maybe Michigan politicians should be making some calls to Oslo.
The Think City is already in production in Europe, and the company is rushing to produce 10,000 units this year for sale there. One of the people behind the VC funding says they could sell 30,000 to 50,000 Think City cars in the U.S. See Norway’s Think to Produce, Sell Small Electric Cars in U.S. (from WSJ.com) 
There is another car that Think has in the pipeline, and it is pretty cool looking, see it below:



Its new concept, called “Ox”, looks to be a much more mainstream vehicle than any of the minicars the company sells overseas.
But the name needs to change, because I don’t want my friends teasing me that I’m driving an “Ox car”. I think they were shooting for some spin on “Oxygen” but missed the mark.
Roughly the size of a Scion xB, the front-wheel-drive Ox MPV will have a 60-kW electric motor and a range of 124 miles on a full charge. It can be charged via a normal household outlet. Charging the car to 80% will take just an hour using a special charger, while a full charge will take 12 hours. 
The company is planning to use either sodium or lithium-ion batteries, and there’s a strip of solar cells running down the center of the roof. The Ox is built on an interchangeable platform, so a coupe body style with a larger motor and batteries or a taxicab configuration could also be manufactured.
Unfortunately, the Ox looks to be a true concept, with no firm date on when we could expect to see it on the road. The other unfortunate part is that Think doesn’t have a presence in the U.S. General Electric recently invested $4 million into Think, though, so don’t give up hope of one day seeing the “Ox” on the street. More photos here.


Bring a production version of the Ox with a different name, though, and I’d expect people to line up.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ec9aba2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Now the nights are drawing in and the air is chill, many of us stop going out in the evening and retreat to our front rooms to spend cosy evenings curled up with a book or stretched out in front of the TV. By the start of November, even the most miserly of penny pinchers has typically caved in to the need to put the central heating on. Unfortunately, not everyone is lucky enough to be able to afford such a basic necessity as warming their own home. Traditionally a household has been considered to be “fuel poor” if required to spend more than 10% of their total income on energy to maintain an adequate standard of warmth. According to the government’s Annual Fuel Poverty report, 2m UK households are currently living in fuel poverty: 10.4% of the population. Unsurprisingly, unemployed are worst hit with almost a third recorded as fuel poor. The inability to afford energy bills killed a staggering 10,000 Britons in last year. While the UK’s rate of fuel poverty is unacceptably high, transport poverty should also be given attention. The 2m in fuel poverty is dwarfed by the 21m households that motoring lobby group the RAC reports spend more than 10% of their disposable income (as used to be the measure of fuel poverty) on transportation. According to the RAC, transport is the single biggest cost for the average household, at 14% of disposable income – with the majority of this being spent on owning and running a car. Those without a car make, on average, half the number of journeys. As yet transport poverty has not been widely discussed, little research exists and there is no commonly accepted definition. It may be the case that simply transferring home energy costs to transport outgoings is the most appropriate means for tracking this phenomenon or there may be more sophisticated ways to measure it.  Sustrans, the sustainable transport charity, combines three factors: time taken to access essential services; distance to the nearest bus or train station, and; family income. Using this rationale, they estimate half of the nation’s local authorities contain (mostly rural) areas with a high risk of transport poverty. Research from the Office for National Statistics shows that the UK’s poorest families (the lowest tenth of household incomes) spend 31% of their money on the purchase and operation of a car; a proportion that is rising each year as motoring costs increase.  Transport poverty is most pronounced in rural areas where owning a car is considered a necessity and not a luxury. These are areas of low population density, where jobs and services will typically be located a long way from homes. With generally inadequate public transport and long distances making walking or cycling impractical, cars can be the only option for those living in our villages and hamlets. For those living in rural parts of the UK, transport poverty is already a major issue. Research into commuting habits from the RAC show much more need for cars in rural areas. More than half of urban commuters walk or cycle rather than drive but just 31% of rural dwellers could do this. Public transport is easily available in cities and towns but rare elsewhere. And unfortunately for rural dwellers fuel is more expensive in the countryside, adding to an increased cost of living. There is, then, a rural transport problem that needs to be taken seriously. As fuel poverty begins to move onto the political agenda, we must hope that similar happens for transport poverty.  Our presumption for private vehicle ownership is clearly unsustainable in environmental terms: though more efficient petrol and diesel engines still belch out toxic fumes, while even electric cars largely rely on fossil fuel power stations.  Transport poverty shows that our obsession with owning and running cars is not socially or economically sustainable either. Those who are young, elderly, unemployed or in poorly paid work risk being locked out of ordinary society simply because they cannot afford to buy into the car system. Many of those that have enough money for a car will find it a burden that means they are priced out of other activities. In particular, rural communities risk decimation as people need to move away and what remains is a two tier society: the affluent that enjoy full access to 21st century society and an underclass that are left behind. We need to move beyond the current car system and perhaps rural car clubs with collective ownership and shared vehicles offer a way forward."
"
Share this...FacebookTwitterBy Prof. Fritz Vahrenholt at Die kalte Sonne
(Translated by P. Gosselin)
June 7, 2020
Dear ladies and gentlemen
First, the global mean temperature of satellite based measurements was surprisingly much higher in May 2020 than in April. In contrast, the global temperatures of the series of measurements on land and sea decreased. The difference can be explained by the fact that under warm El-Nino conditions the satellite measurements lag about 2-3 months behind the earth-based measurements.
From November 2019 to March 2020 a moderate El-Nino was observed, which has now been replaced by neutral conditions in the Pacific. Therefore, it is to be expected that also the satellite based measurements, which we use at this point, will show a decrease in temperatures within 2-3 months.
The average temperature increase since 1981 remained unchanged at 0.14 degrees Celsius per decade. The sunspot number of 0.2 corresponded to the expectations of the solar minimum.
The earth is greening


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In August 2019, I reported on a remarkable publication by the Max Planck Institute for Meteorology in Hamburg: “Our main finding,” said Aexander Winkler’s researchers at the time, “is that the effect of CO2 concentration on terrestrial photosynthesis is greater than previously thought and therefore has important implications for the future carbon cycle.
According to this, the CO2 attenuation effect of plants is 60% higher than the average of climate models had assumed.
“In the last two decades, an average of 310,000 km² of additional leaf and needle area – roughly the size of Poland or Germany – has been created every year,” the researchers say. I had shared this important finding with the members of the German Bundestag at the time, which led Stefan Rahmstorf to conclude that I was “trying to fool the German Bundestag“. This assessment was taken up by some media such as the TAZ and ultimately led to my dismissal as sole director of the German Wildlife Foundation.
New confirmation: CO2 uptake by plants is increasing
In April ,2020, a research group led by the Australian scientist Vanessa Haverd published a paper in Global Change Biology which more than confirmed the findings of the Max Planck Institute. The researchers describe that plants have absorbed 30% more CO2 since 1900. The previous estimates were 17%. In their calculation for a mild increase in CO2 in this century (IPCC scenario 2.6), the researchers led by Vanessa Haverd arrived at a net uptake of 528 billion tonnes of CO2 by plants by 2100, compared to the 238 billion tonnes of CO2 previously calculated by climate models.
According to Adam Riese, this is more than twice as much. By way of comparison: In scenario 2.6, a total of 1000 billion tonnes of CO2 (IPCC, Chapter 6, p. 468) will be emitted in this century. Today the plant world absorbs about 30% of the anthropogenic CO2 annually, the oceans another 24%.
In contrast, the statement of the IPCC in its last report from 2013 (p.26 of the Summary for Policymakers) is diametrically different: “Based on Earth system models, there is a high confidence that the feedback between climate development and the carbon cycle in the 21st century is positive. As a result, more of the anthropogenic CO2 emitted will remain in the atmosphere. Maybe I need to write to the German Bundestag again.


		jQuery(document).ready(function(){
			jQuery('#dd_702a1f6ffcce5ac21c82418cd887a235').on('change', function() {
			  jQuery('#amount_702a1f6ffcce5ac21c82418cd887a235').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRudolf Kipp of http://www.science-skeptical.de/blog/deutsche-klimaforscher-starten-neuen-rettungsversuch-fur-ein-globales-klimaabkommen-die-klima-kopfpauschale/002480/  reports on the latest German plan to save the planet: Peak and Trade. This is my summary in English.
Without a doubt the Copenhagen Climate Conference last December was a major flop. A treaty for reducing CO2 emissions could not find the support it needed. The conference was such a disaster that a climate agreement is all but ruled out for 2010 as well. So, what to do?
Policy Shift in Germany
The Potsdamer Institute for Climate Impact Research, which includes alarmists Stefan Rahmstorf and Hans Joachim Schellnhuber, has been the primary advisor for chancellor Angela Merkel, and have arguably lost much clout since the embarrassment that was Copenhagen.
To get things back on track, PIK has come up with a new position paper that outlines a whole new approach that could lead to a breakthrough in climate treaty negotiations. It’s dubbed New Strategies for Reaching the 2°C max Climate Target, a scheme which the PIK calls Peak and Trade. It calls for a so-called per capita climate quota, see here (Abstract written in English). http://www.pik-potsdam.de/research/publications/pikreports/.files/pr116.pdf
The PIK scientists call for allocating a quota of 5 tonnes of CO2 to every inhabitant on earth. Should an earthling exceed the quota, then payment would be made into a fund (a yet to be created World Climate Bank). If the quota is not reached, then money would come out of the fund as a reward.
This of course becomes very attractive for the poorest of countries, and very expensive for industrious nations. For example the average American emits 16.9 tonnes of CO2 – more than three times the quota. Yet, some European countries with high amounts of hydroelectric or nuclear power are well-positioned. For example Sweden and Switzerland emit on average 5.6 tonnes, the French 6.3 tonnes. The financial punishment of Peak & Trade would be mild and thus bearable.
The per capita climate quota is very attractive for developing countries
Before anyone writes this idea off as crazy, take a close look at the numbers and who the potential winners and losers would be. But first, where does the 5.1 tonnes of CO2 per earthling number come from? The authors of the paper simply have taken the projected 2015 CO2 emissions figure of 35 billion tonnes and divided it by the current world population of 6.9 billion. Copenhagen failed in part because of resistance from the developing countries, especially Brazil, China and India. They just didn’t see enough incentives. These countries however are well below the magic 5.1 tonne quota – especially Brazil and India, and so now they have ample reason to enthusiastically support Peak and Trade.  It’s reasonable to think this is the instrument to get them back on board.
The per capita climate quota is also an incentive for population growth (and poverty) 
Poor developing countries now stand to rake in the cash. The more inhabitants and the poorer the standard of living, the more money a country stands to pull in. That could be an irresistable incentive for many countries. The humanitarian problem with this German Meisterwerk of a plan now becomes clear: governments and regimes in poor countries would have no incentive to improve living standards for their citizens, and thus keep CO2 emissions at low levels. The funds earned by falling below the 5.1 tonne quota of course would never be seen by the poor citizens. Most likely these funds would flow into in the hands of corrupt government officials and into anonymous bank accounts in Switzerland.
Readers may think this is some far-fetched idea. It is – and that’s the danger. These things are often hatched by social engineering experimentalists here in Europe and they always seem to grow legs. Don’t underestimate it. Peak and Trade has to be killed.
Share this...FacebookTwitter "
"

Most economists once believed that monetary policy should aim at achieving full employment, but we now know that holding unemployment below its natural rate has dangerous consequences. Could it be that another supposed economic ideal — zero inflation — is similarly wrong‐​headed?





In his 1997 book _Less Than Zero_ , Cato’s George Selgin first made the case for allowing price levels to vary to reflect changes in productivity. Now, a new edition of _Less Than Zero_ from the Cato Institute updates this important and prescient argument for 2018.



  
In the introduction for this edition, Scott Sumner of the Mercatus Center at George Mason University makes the case that Selgin’s book was “ahead of its time” and that it is time to return to Selgin’s argument for a productivity norm, where prices would rise or fall inversely to changes in productivity.



Selgin himself, however, contends that his idea is not entirely innovative; over the course of his research, he found that similar arguments have been made by other early 20th‐​century economists he admired. “Eventually, it became clear to me that — far from being novel — my understanding of deflation had once been almost orthodox, having been shared by prominent economists of many different schools of thought, only to be flung aside in the wake of the Keynesian revolution,” he writes. Two decades after its first publication, the ideas in _Less Than Zero_ are no longer as radical as they once were, as more economists are arguing for nominal income targeting and speculating about the possibility of “good” deflation. With that new climate in mind, this edition revisits these important and thought‐​provoking ideas.
"
"Wolves, lions and other large carnivores rely on meat for sustenance and there are only so many wild animals to go round. Sometimes, dinner means cow or sheep. Farmers can use guard dogs or protective fencing to deter predators and protect livestock. But lethal methods such as hunting and trapping are also used to control wild carnivore numbers. As a livestock farmer in wolf country, it would be reasonable to assume that killing more predators would result in fewer attacks on your animals. However, a new study by Washington State University has turned this assumption on its head by discovering the opposite: the more wolves that are killed (up to a threshold of 25% of the population), the more the remainder preyed on local sheep and cows. Why is this? The researchers, Robert Wielgus and Kaylie Peebles, point to the nature of the species’ social systems: wolves live in family groups containing a breeding pair (also known as the alpha pair) along with related sub-adults, juveniles and pups. The alphas are the only breeders within the group as they limit reproduction by their subordinates.   Killing one of the alphas disrupts the pack and subordinate wolves, who often outnumber the breeders, are then free to reproduce. This could increase the number of breeding individuals in the area, thereby increasing the population of hungry wolves – maybe farmers who shoot wolves are inadvertently doing more towards conservation than they think! Conversely, as humans are more likely to shoot youngsters than adult breeding wolves, the alphas may be temporarily be in a more favourable situation. There would be less competition for food, fewer clashes with other wolves and less risk of the transmission of disease.  Again, this could result in short-term increases in attacks on livestock. Wolf packs also have an important educational role, as the experienced wolves pass on their knowledge. Killing them impairs this social learning. If the rest of the pack hasn’t learnt the skills necessary to take on bison or elk they may instead turn towards easier pickings on the farm. This same behaviour has been seen in lions and cougars (although has not been documented in many other carnivore species). It is interesting to note that this paradoxical finding is not just found in relation to wolves – lethal control of cougars (or mountain lions) also means the remainder kill more cows and sheep as younger, inexperienced cougars are more likely to attack livestock.  Coyotes also show increased litter sizes and more frequent breeding in populations that were lethally controlled. Culling programmes could have even exacerbated livestock attacks by taking out younger, less predatory coyotes.  Further, state-funded coyote removal campaigns have failed to reduce predation on sheep.  Lynx, too, do not significantly reduce livestock attacks until lethal control dramatically reduces total population numbers. It must be noted that other studies have shown that killing predators can sometimes reduce the numbers of livestock they themselves kill, but this is only temporary,  until new populations of predators establish themselves.   If we would like a world where neither livestock nor predators are killed, we are either going to have to take away all the predators or all the livestock. Clearly neither one of these options is viable so we must aim to reduce preying on farm animals to a tolerable level. Despite proof that changes in livestock husbandry reduces predation, farmers may still not want these creatures living near them as they may feel that the carnivores have “won” or taken over “their” land.   As such, despite scientific evidence showing that predators don’t kill that many cattle anyway, that lethal control usually doesn’t reduce attacks, and that non-lethal methods can almost eliminate attacks, this still may not be enough to sway farmers from their anti-predator mind-sets.   We must therefore start to think outside the box. Much of this conflict between humans and wild predators is not really about protecting livestock, but instead concerns a deeper historic and cultural aversion to wolves, lions and other scary carnivores. This won’t be fixed through simple technical solutions – and we now know it certainly won’t be fixed with a gun."
"
Share this...FacebookTwitterHere’s another example illustrating just how volatile and unreliable wind energy really is.
Wind energy proponents like to claim that although turbines installed on land don’t produce so optimally, the ones at sea are wonderful because the wind there is always blowing and so it all kind of evens out.
The chart below shows the output of all wind turbines installed in Germany, both on land and offshore, from the five major German grid operators:

The dark horizontal line denoting 60,000 MW represents the so-called installed total capacity. Readers will note that less than 10% of rated capacity often gets produced. Only rarely does an output of 33% (20 MW) ever get reached.


		jQuery(document).ready(function(){
			jQuery('#dd_47d74c3ef530a413e0a50449d409a405').on('change', function() {
			  jQuery('#amount_47d74c3ef530a413e0a50449d409a405').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Sahia got married when she was 15 years old – and saw her husband for the first time on their wedding day. Her sense of humour is apparent as soon as we meet: “He is a good husband. I was lucky, but at the same time he must have exaggerated his status a bit to my parents,” she says, laughing. “His market value was put up slightly over his actual value.” She has agreed to talk to us about the ways climate change and environmental stress are influencing people’s lives. She says that after their wedding, she moved in with her husband and father-in-law. Her new home was Singpur – a riverside village in central Bangladesh, only a couple of hundred kilometres from the capital, Dhaka. She had been living here for about a year when she witnessed her first home collapsing into the river.   One day we found cracks in the floor as the land had started to get pulled into the water. We realised that this was a bad sign. The cracks kept growing deeper and deeper for each day that passed. Riverbank erosion usually happens slowly, but occasionally a larger chunk of land suddenly falls into the water. This is what happened the day Sahia lost her first house. As soon as she noticed how deep the cracks were in the ground, she started carrying out their belongings to safety. A few hours later, the house was gone.  Sahia and her family moved in with her husband’s uncle for a while. But it was not long before his house was swallowed by the river, too. They moved into an abandoned house after this, but they are still living too close to the river, Sahia tells me in a worried voice. Sahia’s husband was a fisherman so the family got by on what he managed to catch. They were managing, until one day everything changed. My husband used to catch fish, but when the fish got some disease he had to stop fishing … Nowadays, there is hardly any fish left in the river … There used to be tons of fish, but when the fish started dying from that disease people stopped eating them … All the big and good fish disappeared. Once their main income stopped sustaining them, the family felt forced to change livelihood. They decided to migrate seasonally to work in a brick factory in Aliganj, further up the river. Nowadays, they leave Singpur for six months every year during the rainy season, to avoid being there when the village is flooded. The man who brought us there [to the brick factory] gave us some money, and with that money we managed to survive the six months in Singpur. What we earn we use to feed our family. They usually pay us on a weekly basis, around 2,000-4,000 taka [£20-40]. After feeding my family, consisting of six family members, we manage to save about 1,000 taka [£10] or so per week. That amount is our family’s whole savings per week. When the family return to Singpur, they live on their savings from the brick factory. The work is hard and dangerous. The children miss out on school to work in the factory and it is only a matter of time before someone ends up getting hurt or sick from the hard work. Their savings would not sustain the whole family through a crisis.  The shift of livelihood has not only been hard on Sahia physically. Before the factory work, she was a housewife like the rest of the women in Singpur village. Working outside of the house as a woman has brought social stigma on her. In this village, if you work outside you end up losing your honour… After observing us some people said: ‘The women out there are working! What do they know about work?’ We see those people, we hear them, but we do not fear their words anymore. We work to survive. I can see that she is watching me. My hair, which is tied up in a bun, and the clothes I am wearing. We are two women not that different from one another in a lot of ways, but we are living two completely different lives in very different parts of the world. She says: I am actually not that old. It is the hard work that made me look like this. Did you not see my husband? Nobody thinks I am his wife after meeting him. Her husband is young, in his late twenties or early thirties. When I first met Sahia I thought to myself that she must be about a decade older than me. It turns out that we are about the same age. She had to make the necessary sacrifices in her life to sustain her family.  When I return to the village a year later, I immediately head off to find her. I have thought about her throughout the year. I ask the people I met on my way there, but nobody seems to know anything. I reach the area where her house used to stand, but it is gone. It has fallen into the river. I ask her neighbours if they know where she is, where the family went, but nobody seems to know what has happened to her."
"
Share this...FacebookTwitter19 dead and up to 400 injured, many seriously.These are the latest gruesome numbers from yesterday’s Duisburg Love Parade, crowd-control disaster. It’s a classic case of what can go wrong when warnings are ignored or played down. City officials were warned that the location was seriously inadequate, but nobody wanted to be a party-pooper.
Germany’s techno-music Love Parade first started in 1989. Most have been held in Berlin, until the city got tired of the chaos and filth they left behind, and the event had lost money anyway.
Yet, everybody hates to see a good party end, and so other places were sought to host the million-plus visitor event. This year’s choice proved to be a disaster.
The catastrophe occurred at a tunnel under a street that served as the main entrance to the event. The following video vividly shows the catastrophe in motion. Especially interesting is the 0:31 mark of the clip. there you see the fully packed ramp leading down to the tunnel.
LOVE PARADE DISASTER VIDEO
Just the shear physics of the situation alone are staggering.
If you estimate 100,000 people on that sloped ramp, each with an average weight of 70 kg, you have a total weight of 7000 tons trying to move forward. If the ramp has a slope of 3°, then sine 3° times 7000 tones yields a gravity force vector of 366 tons pressing down against the wall. Not a good place to be. Granted that’s just a real rough calculation, but it gives you an idea.
A sure sign that the old rail yard location, where the event took place, was not going to work was its relatively small size. It had an area of 2.2 million square feet, meaning that the place was going to be overly packed with just half a million people. Organisers expected 1 million, 1.4 million showed up. 1.5 sq ft per person. Experts say the area was suitable for a maximum of only 300,000.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Throw in youth, alcohol and drugs and you have all the ingredients for a crowd control disaster.
The police had warned city officials that the risks were too high and had advised against approving the event. But reports say no top city official wanted to be the party-pooper. The Love Parade got the green light.
Now, after the catastrophe, with the nation and continent in shock, officials are scrambling and the finger-pointing has begun. According to German newspaper Bild:
Duisburg mayor Adolf Sauerland defended the safety concept of the Love Parade against criticism.
In his view the reasons for the tragedy were not due to a poor safety concept, but rather very likely had more to do with individual weak points.
Tell that to the prosecuting attorneys Herr Burgermeister.
The State’s Attorney has announced that an investigation for negligent homicide will be conducted, and all city and official documents relating to permitting and organisation of the event have been confiscated by police.
The Love Parade organisation has announced today that there will not be any more Love Parades in the future.
UPDATE – USEFUL LINKS:
http://www.thelocal.de/national/20100726-28737.html
http://www.spiegel.de/international/germany/0,1518,708462,00.html
Share this...FacebookTwitter "
"
In our last episode, we looked at a COOP station on a roof of a fire station operated by the NWS in San Diego. Moving north, we have another COOP station operated by the San Francisco/Monterey Weather Service office that is also on a rooftop. You can see the MMTS sensor in the photo provided.
This station, COOP number 04-1838 is in Cloverdale, CA is just a few feet away from a chimney flue and an exhaust stack of a diesel generator. Note also the rain gauge placement. This station, while not a USHCN station, is part of the “A” network, which does report climate for NCDC.

Photo from NWS SFO/Monterey
For those unfamiliar at spotting NOAA issued MMTS temperature sensors, its the post on the leftmost portion of the lower roof, directly beneath the satellite dish.
To see other stations, try my blogs “weather_stations” link under the categories at right. To see stations in your state see www.surfacestations.org and click on the online image gallery link. You may even wish to signup to help survey a station in your area.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea288d583',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Current Wisdom is a monthly series in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



The Current Wisdom only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



This year’s installment of the United Nations’ annual climate summit (technically known as the 16th meeting of the Conference of the Parties to the Framework Convention on Climate Change) has come and gone in Cancun. Nothing substantial came of it policy‐​wise; just the usual attempts by the developing world to shake down our already shaky economy in the name of climate change. News‐​wise probably the biggest story was that during the conference, Cancun broke an all time daily low temperature record. Last year’s confab in Copenhagen was pelted by snowstorms and subsumed in miserable cold. President Obama attended, failed to forge any meaningful agreement, and fled back to beat a rare Washington blizzard. He lost.



But surely as every holiday season now includes one of these enormous jamborees, dire climate stories appeared daily. Polar bear cubs are endangered! Glaciers are melting!!



Or so beat the largely overhyped drums, based upon this or that press release from Greenpeace or the World Wildlife Fund.



And, of course, no one bothered to mention a blockbuster paper appearing in Nature the day before the end of the Cancun confab, which reassures us that Greenland’s ice cap and glaciers are a lot more stable than alarmists would have us believe. That would include Al Gore, fond of his lurid maps showing the melting all of Greenland’s ice submerging Florida.



Ain’t gonna happen.



The disaster scenario goes like this: Summer temperatures in Greenland are warming, leading to increased melting and the formation of ephemeral lakes on the ice surface. This water eventually finds a crevasse and then a way down thousands of feet to the bottom of a glacier, where it lubricates the underlying surface, accelerating the seaward march of the ice. Increase the temperature even more and massive amounts deposit into the ocean by the year 2100, catastrophically raising sea levels.



According to Christian Schoof of the University of British Columbia (UBC), “The conventional view has been that meltwater permeates the ice from the surface and pools under the base of the ice sheet… .This water then serves as a lubricant between the glacier and the earth underneath it… .”



And, according to Schoof, that’s just not the way things work. A UBC press release about his Nature article noted that he found that “a steady meltwater supply from gradual warming may in fact slow down the glacier flow, while sudden water input could cause glaciers to speed up and spread.”



Indeed, Schoof finds that sudden water inputs, such as would occur with heavy rain, are responsible for glacial accelerations, but these last only one or a few days.



The bottom line? A warming climate has very little to do with accelerating ice flow, but weather events do.



How important is this? According to University of Leeds Professor Andrew Shepherd, who studies glaciers via satellite, “This study provides an elegant solution to one of the two key ice sheet instability problems” noted by the United Nations in their last (2007) climate compendium. “It turns out that, contrary to popular belief, Greenland ice sheet flow might not be accelerated by increased melting after all,” he added.



I’m not so sure that those who hold the “popular belief” can explain why Greenland’s ice didn’t melt away thousands of years ago. For millennia, after the end of the last ice age (approximately 11,000 years ago) strong evidence indicates that the Eurasian arctic averaged nearly 13°F warmer in July than it is now.



That’s because there are trees buried and preserved in the acidic Siberian tundra, and they can be carbon dated. Where there is no forest today — because it’s too cold in summer — there were trees, all the way to the Arctic Ocean and even on some of the remote Arctic islands that are bare today. And, back then, thanks to the remnants of continental ice, the Arctic Ocean was smaller and the North American and Eurasian landmasses extended further north.



That work was by Glen MacDonald, from UCLA’s Geography Department. In his landmark 2000 paper in Quaternary Research, he noted that the only way that the Arctic could become so warm is for there to be a massive incursion of warm water from the Atlantic Ocean. The only “gate” through which that can flow is the Greenland Strait, between Greenland and Scandinavia.



So, Greenland had to have been warmer for several millennia, too.



Now let’s do a little math to see if the “popular belief” about Greenland ever had any basis in reality.



In 2009 University of Copenhagen’s B. M. Vinther and 13 coauthors published the definitive history of Greenland climate back to the ice age, studying ice cores taken over the entire landmass. An exceedingly conservative interpretation of their results is that Greenland was 1.5°C (2.7°F) warmer for the period from 5,000‑9000 years ago, which is also the warm period in Eurasia that MacDonald detected. The integrated warming is given by multiplying the time (4,000 years) by the warming (1.5°), and works out (in Celsius) to 6,000 “degree‐​years.” 



Now let’s assume that our dreaded emissions of carbon dioxide spike the temperature there some 4°C. Since we cannot burn fossil fuel forever, let’s put this in over 200 years. That’s a pretty liberal estimate given that the temperature there still hasn’t exceeded values seen before in the 20th century. Anyway, we get 800 (4 x 200) degree‐​years.



If the ice didn’t come tumbling off Greenland after 6,000 degree‐​years, how is it going to do so after only 800? The integrated warming of Greenland in the post‐​ice‐​age warming (referred to as the “climatic optimum” in textbooks published prior to global warming hysteria) is over seven times what humans can accomplish in 200 years. Why do we even worry about this?



So we can all sleep a bit better. Florida will survive. And, we can also rest assured that the UN will continue its outrageous holiday parties, accomplishing nothing, but living large. Next year’s is in Durban, South Africa, yet another remote warm spot hours of Jet‐​A away.



 **References:**



MacDonald, G. M., et al., 2000. Holocene treeline history and climatic change across Northern Eurasia. Quaternary Research 53, 302–311.  
Schoof, C., 2010. Ice‐​sheet acceleration driven by melt supply variability. Nature 468, 803–805.  
Vinther, B.M., et al., 2009. Holocene thinning of the Greenland ice sheet. Nature 461, 385–388.
"
"

In the May/​June 1983 issue of _Regulation_ magazine, the economist Bruce Yandle set forth a new theory of government intervention. At the time Yandle was executive director of the Federal Trade Commission, and, as an economist, he had recently become interested in the demand for and supply of social regulations. Where exactly do these regulations come from?



As he read about historical efforts to control alcohol by banning Sunday sales, Yandle found the answer in an unlikely coalition. Churchgoing teetotalers endorsed the prohibition on moral grounds, while bootleggers supported the restrictions in order to limit competition. As Yandle discovered more and more examples of this alliance — from environmental policy to interstate trucking — he concluded that “durable social regulation evolves when it is demanded by both of two distinctly different groups.” He referred to these groups as “Bootleggers,” who have an economic interest in the regulation, and “Baptists,” who have a moral argument.



In a new book called _Bootleggers & Baptists_, Yandle — along with Adam Smith, director of the Center for Free Market Studies at Johnson and Wales — revisits an old theory with new perspective. The book explores a political dynamic connecting interest groups who, for very different reasons, spend time and resources seeking government favors. “At its very root, it is a story about the demand for and supply of politically provided pork,” Smith and Yandle write.



The authors begin by surveying the explosive growth in federal regulation during the 1970s and 1980s — when the theory was germinating — presenting an array of Bootlegger and Baptist stories stretching from Magna Carta to today’s energy industry. This thick layer of rules is at the center of the narrative. “When the pace of regulation accelerates,” they write, “Bootleggers and Baptists are sure to barbecue while the political fire pits are hot.”



Smith and Yandle then spend time deconstructing the two groups. They show how the ultimate results of noble‐​minded efforts to effect change in the public interest prove to be neither noble nor in the public interest. Instead, a publicspirited group wraps a selfinterested lobbying effort in a cloak of respectability. As a result, “once Bootleggers and Baptists are locked into a successful coalition, their structural incentives change, making political wealth extraction more attractive than private wealth generation — to society’s detriment.”



Consider, for instance, Obamacare. The implementation of health care reform provided some amount of access to a larger share of the population. Yet, it also cartelized 17 percent of the U.S. economy, guaranteeing an expanded market for the country’s biggest political players. “In each of these efforts, a vast Baptist choir sang the praises of government‐​assisted health care,” the authors write. “But lurking in the background — and sometimes in the back row of the choir — were pharmaceutical, insurance, and other health care Bootleggers ready to expand sales to the regulated sector.” As Smith and Yandle detail in full, the story behind the rise of Obamacare is more complex than it seems — and much more interesting.



In one example after another — from the Troubled Asset Relief Program to climate change — the two economists reveal the interaction of lofty values with narrow selfinterest. In short, politicians who deliver pork to Bootleggers can justify their actions by appealing to higher “Baptist” morality. This phenomenon is driven by forces deeply rooted in our DNA, they explain. What, then, is the endgame? Is this marriage of strange bedfellows here to stay? “We are all at least a little bit Bootlegger, a little bit Baptist,” Smith and Yandle conclude, “which means as long as we remain human, the story of Bootleggers and Baptists will continue.”
"
"Devoted followers of international wrangling on climate change will see much that they recognise in the five-page text emanating from the UN climate talks in Lima. The “parties” (countries) have long accepted the maximum 2°C warming target; that mitigation and adaptation must go hand-in-hand; and that past emissions of developed countries need to be accounted under “common but differentiated responsibilities.” Actual promises of financial assistance to developing nations show little sign of progressing to their US$100bn target, though, and emission pledges so far fall far short of levels consistent with 2°C of warming. The hard-fought document restates a commitment from all countries to raise their level of ambition in order to arrive at the next meeting in Paris in December 2015 with a convincing plan to resolve their differences once and for all. Radical approaches will be needed. Climate change is a particularly tough nut to crack: there is still deep uncertainty over its likely impact; there are sharply differing viewpoints and conflicts of interest; and there is no central authority to implement any agreed solutions. Social planners call these public policy issues that challenge conventional thinking “wicked” problems. Most awkward of all, climate and development are deeply interconnected, driving a deep rift between developed and developing blocs. Changes happening now are a result of the fossil-fuelled development of the richer countries, while issues of public health and energy security in poorer nations can seem remote from long-term climate change – at least until rising seas or diminishing harvests intervene.  The climate policies that animate negotiators each December must be thoroughly embedded in the everyday battle to escape poverty that still afflicts the greater part of humanity. Climate change is for life, not just for Christmas. For the third year running a UN climate conference notes with grave concern the significant gap between countries’ emissions pledges and the levels required to stay within the magic two degrees. However the lamentable lack of action is partly the result of a parallel gap in knowledge as politicians need to know how to reduce emissions without damping growth.  Climate change is pushing climate and socio-economic systems far from their equilibrium states and researchers are struggling to catch up. Traditional economic modelling relies on equilibrium assumptions of perfect markets and full employment, but the scale of transition required breaks many of these assumptions. Green growth means exploiting the resulting opportunities to mobilise underused resources.  Economists can’t solve the whole problem on their own. Neither can climate scientists, lawyers, sociologists, sustainability experts or people who work on the heath impacts of climate change. All these fields and others need to work together to come up with the best possible policies. But if politicians can’t, or won’t, find a solution to climate change then other actors are waiting to step in. What was once the exclusive realm of national negotiators now sees, for instance, groups of leading companies and major cities calling for coordinated action. Such widening of participation is one of three core suggestions to accelerate progress backed by the Yale Climate Dialogue prior to Lima.  As highlighted by Lord Stern, the key to success in Paris has less to do with wrangling over financial transfers between self-interested governments and more to do with steering the trillions of dollars spent annually on infrastructure and energy transitions in developing countries towards clean, low-carbon options.  There are plentiful ideas and initiatives to make this happen, but before a vision of a sustainable future can become an everyday reality, our leaders need solutions from the research community that reflect the true scale of the problem and its wicked, multi-faceted ways."
"Ryanair has been accused of greenwashing after the UK advertising watchdog banned an ad campaign claiming that the airline has the lowest carbon emissions of any major airline in Europe. The budget airline, which was named last year as one of Europe’s top 10 carbon emitters in an EU report, later ran a TV, press and radio campaign claiming it was “Europe’s lowest fares, lowest emissions airline”. The ads claim that Ryanair has the “lowest carbon emissions of any major airline”, based on CO2 emissions per passenger per kilometre flown, because it has the youngest fleet, highest proportion of seats filled on flights and newest, most fuel-efficient engines. However, one of the charts Ryanair presented to the Advertising Standards Authority to back up its claims was dated 2011, which the watchdog said was “of little value as substantiation for a comparison made in 2019”. The ASA added: “In addition, some well-known airlines did not appear on the chart, so it was not clear whether they had been measured.” The ASA also said that the ads failed to factor in seating density – the number of seats per plane – which it considered “significant information that consumers needed in order to understand the basis of the claim”. The ASA banned the ads ruling that they were misleading because the airline had failed to substantiate its environmental claims. “The ads must not appear again in their current forms,” the ASA said. “We told Ryanair to ensure that when making environmental claims they held adequate evidence to substantiate them and to ensure that the basis of those claims were made clear.” The environmental group Transport & Environment accused Ryanair of greenwashing instead of tackling its emissions. The airline ran the low-emissions ad campaign just over five months after it became the first non-coal company to be named in the EU top 10 carbon emitters list. “Ryanair should stop greenwashing and start doing something to tackle its sky-high emissions,” said Jo Dardenne, the aviation manager of T&E. Ryanair remained defiant, claiming it had abided by the UK advertising code. “Ryanair is both disappointed and surprised that the ASA has issued this ruling given that Ryanair fully complied with advertising regulations, engaging with regulators and providing documentation that fulfilled all the substantiations needed,” said a spokeswoman. The Ryanair boss, Michael O’Leary, has suggested shooting environmentalists and has repeatedly denied that the climate crisis is driven by carbon emissions, which aviation produces in abundance. Last year, Ryanair claimed it was already the greenest airline in terms of carbon emissions per passenger. The company has also pledged to be “plastic free” by 2023 and set up a voluntary carbon offset payment scheme for customers when booking."
"Climate change news can be incredibly depressing. In 2018 alone, The Conversation covered the loss of three trillion tonnes of ice in Antarctica; Brazil’s new president and why he will be disastrous for the Amazon rainforest; a rise in global CO₂ emissions; and a major IPCC report which warned we are unlikely to avoid 1.5℃ of warming.  Then there were the rogue hurricanes, intense heatwaves, massive wildfires and the possibility we are emitting our way towards a Hothouse Earth. Global warming has left some wintery animals with mismatched camouflage, and it may even cause a global beer shortage. But things cannot be entirely bad, can they? We asked some climate researchers to peer through the smog and highlight a few more positive stories from 2018. Rick Greenough, professor of energy systems, De Montfort University 2018 saw the largest annual increase in global renewable generation capacity ever, with new solar photovoltaic capacity outstripping additions in coal, natural gas and nuclear power combined. This is one of several hopeful signs that the “cleantech” sector is rising to the challenge of climate change. The UK, for instance, set new records for wind generation. And now that subsidy-free solar generation has proven possible, there are plans for the UK’s largest solar farm to provide the cheapest electricity on the grid, thanks to battery backup (crucial for intermittent renewable technology). Tesla, meanwhile, installed the world’s largest lithium battery in Australia and it is set to pay back a third of its cost within one year. Mike Wood, reader in applied ecology, University of Salford Three decades ago, the world experienced its worst nuclear accident to date. The damaged Chernobyl nuclear power plant released large quantities of radioactive material into the environment, necessitating evacuation of an area now known as the Chernobyl Exclusion Zone (CEZ). But forget the popularised imagery of a nuclear wasteland; Chernobyl is now home to an amazing diversity of wildlife, its forests are expanding and the future of this region is looking positive.  In the fight against climate change, there is a global need to reduce greenhouse gas emissions and to increase the removal and storage of carbon dioxide from the atmosphere (a process known as carbon sequestration). The ongoing expansion of Chernobyl’s forests means more atmospheric carbon is becoming incorporated into the trees. Additionally, the central part of the CEZ is now home to a major new solar farm development and wind farm development is being considered. Consequently, this post-accident landscape is now contributing to a sustainable future. Anna Pigott, researcher in environmental humanities, Swansea University The Extinction Rebellion direct action movement might not be the most obvious choice for positivity, what with its use of skull imagery and banners such as the one hung over Westminster Bridge in November reading: “Climate Change: We’re F****d”. But a closer look suggests that the movement’s acknowledgement of personal and collective despair in the face of environmental collapse might be a very positive move indeed.  As its co-founder Gail Bradbrook explains, “grief is welcome here – it is an emotional, physical, and spiritual necessity”. Poets and scholars alike have long spoken about how grief mobilises awareness and action, but rarely has this wisdom found its way into large environmental movements.  Pain usefully alerts us to problems that need our attention, and, in the case of climate change and species loss, our grief is a sign that we care deeply. Now is not the time to turn our back on such emotions. As the poet Mary Oliver has written: “You tell me your despair, yours, and I will tell you mine.” For many, the Extinction Rebellion movement has given them permission to grieve, and to share this grief with others. And this could be the most mobilising force for climate action yet.  Daniele Malerba, honorary research fellow, University of Manchester Expansion in the global economy may have peaked, according to the Organisation for Economic Cooperation and Development. The economic think-tank is worried by the slowdown, but it may actually be good news for the climate and possibly for society too. This is because less global economic growth means less production, less consumption – and lower emissions.  But any slowdown or eventual reversal in growth must happen in an equitable way to make sure that human well-being still increases. This is why an increasing number of researchers, politicians and citizens are advocating for degrowth. Degrowth addresses the issue technological improvements are not enough to avoid climate change and an alternative to capitalism is urgently needed. The recent protests in France show that environmental and social issues need to go hand-in-hand. And this is critical in a situation when populist movements are spreading. Degrowth is the solution. As Ghandi once said, we have enough for everybody’s needs, but not everybody’s greed.  Parakram Pyakurel, researcher, Warsash School of Maritime Science and Engineering, Solent University A lot still needs to be done to reduce global carbon dioxide emissions but not all is doom and gloom. For instance, the US, UK and Japan are among the countries whose total carbon emissions from energy fell in 2017 (the most recent year available), according to BP’s statistical review of world energy.  Interestingly, Ukraine showed the greatest reduction, with its 2017 energy emissions around 10% lower than in the previous year. This was thanks to a big fall in coal use, perhaps part of the country’s grand vision of a 2050 low emission development strategy, though it remains to be seen whether Kiev will take the strategy seriously in the long term.  Other nations that managed to reduce their energy emissions include South Africa, Argentina, Mexico and the United Arab Emirates. We’ll need to carefully monitor the statistics in upcoming years to see whether they continue on this path. Rory Telford and Stuart Galloway, Department of Engineering, University of Strathclyde Renewable generation technologies such as wind turbines or solar photovoltaics are now a familiar sight, but many may not realise that communities themselves are accelerating the transition towards low carbon energy. In Scotland, the government’s programme to support local involvement in renewable energy has been a success. An initial target of having 500MW of community and local owned energy was achieved early and with policy stability and continued effort the new 1GW target by 2020 also looks achievable. The Smart Fintry project based in Stirlingshire is an excellent example of a community approach to decentralised energy provision. The project balances local renewable electricity generation with community energy needs via dynamic energy management technology and an innovative tariff. This offers far greater flexibility to the network and cheaper energy for households.  Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
 
Given that the sun is so quiet lately (click image – no sunspots) and there is talk of an ebb in its next solar cycle 24, it bears looking into the details of our primary climate driver.
 The National Geographic Channel has a TV special on the sun, sunspots, climate, etc. They interviewed several people involved in that debate. It includes interviews with Judith Lean, Leif Svalgaard, and others.
It will be shown on the National Geographic Channel. It’s titled: Naked Science ‘Solar Force’.
It goes out on Tuesday 30th October 2007 at 9pm ET and again at midnight ET.
It also rebroadcasts on Thursday 1st November 2007 at 10pm ET.
TV listings can be found on http://channel.nationalgeographic.com/channel/ if you are interested.
UPDATE: description from the NGC website –
The suns energy seems to be constant, but this gigantic nuclear reactor is in a continual state of flux. National Geographic Channel (NGC) reveals the latest scientific information that is uncovering the hidden ways that fluctuations in the suns output influence our climate. See how a radical experiment supports the idea that the suns invisible cosmic rays may have a visible impact on our weather, and find out how a new NASA program could shed new light on how solar wind impacts Earth. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2f7f181',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe  green dream, with all its scenic beauty and nature conservation, has arrived in northern Germany. But now that green dream faces more obstacles. 

All is not so wunderbar when it comes to Germany’s wind power outlook.
Germany’s Renewable Energy Sources Act, passed in 2000, was intended to ensure the generation of “green” electricity. Operators of wind turbines were guaranteed subsidies for a period of twenty years  – with the hopes the technology would develop to such an extent that it would operate economically without subsidies.
20 years later, the wind turbines are still not competitive reports trendsderzukunft.de here.
Plagued by high costs
The first problem with the old turbines? The costs. They require comparatively frequent maintenance. “This drives up the costs, which is why operation is not economical in many cases.” reports trendsderzukunft.de. “A study has shown that at an electricity price of 3.375 cents euro per kilowatt-hour, only 23 percent of the old plants can be operated without subsidies.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Feed-in requirement running out
The second problem: “The Feed-in priority” law which forced power grid operators to purchase wind electricity. “However, it is unclear whether this regulation will continue to apply despite the expiration of subsidies,” says trendsderzukunft.de. “This question will probably have to be cleared up by the courts in the end. However, many operators will probably not wait for this and prefer to shut down the old wind turbines instead.”
Sites will have to be abandoned
Trendderzukunft.de. adds: “By 2025, there is a risk of losing 2,300 to 2,400 megawatts of capacity every year.”
Moreover, legal hurdles prevent repowering, which involves “replacing several old turbines with one new and larger one.” The problem, reports trendsderzukunft.de  is that at many existing locations “there is a height limit for wind turbines. The installation of the latest generation of wind turbines is therefore not possible there. However, smaller turbines are no longer available on the market. In many cases, therefore, the sites simply have to be abandoned.”
“Imminent catastrophe”
For that reason, around 1,000 of 1691 wind turbine sites are affected in Lower Saxony alone and are currently not available for so-called repowering. “Lower Saxony’s Minister of Energy and Environment, Olaf Lies (SPD), speaks of an imminent ‘catastrophe’ for wind power in Germany,” writes trendsderzukunft.de.


		jQuery(document).ready(function(){
			jQuery('#dd_4233aa9cba44a662eec750787b2ca3fe').on('change', function() {
			  jQuery('#amount_4233aa9cba44a662eec750787b2ca3fe').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Michael Gove has implicitly criticised the US and Brazilian presidents for their scepticism about the climate emergency, as he refused to comment on speculation that he could be put in charge of government preparations for UN climate crisis talks. Gove, a Cabinet Office minister with a wide-ranging government role, used a speech opening an event looking ahead to COP 26 to express strong views on the UK’s likely role at the summit it is hosting in Glasgow in November.  A series of leading voices in the climate crisis have said the UK seems to be floundering in its preparations for the event, with the perceived chaos and lack of focus exemplified by the sudden dismissal of the former energy minister Claire O’Neill as president of the negotiations. O’Neill later said that Boris Johnson had demonstrated a “huge lack of leadership and engagement” over the event, and that he did not understand the issue. David Cameron subsequently turned down an offer from the prime minister to take over, and the spotlight has since turned on Gove, a former environment secretary whose cabinet role was formerly focused on preparing for a possible no-deal Brexit. But asked after his speech to the Green Alliance conference in London if he would become the new president, Gove said: “I am very happy with the job that I have, and I think there are many, many, many talented people who could do the job of COP president better than I could.” Using his speech to call for concerted global action on the climate emergency at the summit, Gove noted the lack of efforts on the issue by President Trump and the Brazilian leader, Jair Bolsonaro. “I shan’t mention any world leaders by name in a critical fashion,” he began. “However, it’s important in the United States and in Brazil that we recognise that there will be people, at the state and at the city level, who can play a vital role in driving change that we all need to see.” Predicting that nonetheless COP 26 would be a success, Gove pointed to what he called “politically, a realisation of the scale of the challenge and the emergency” across the globe. He promised new green policies from the UK in areas such as transport, energy, and housebuilding, and said there was “a moral responsibility to lead here, as the first country in the world to industrialise”. He said: “We have a responsibility on the first in, first out basis to ensure that the country that pioneered the industrial revolution, and thus played the biggest role in powering the change in our climate that hydrocarbon extraction and burning for energy created, we have a responsibility to lead a green industrial revolution as well.” Gove also said the event in Glasgow should be the most transparent thus far held, with a mission to “invite citizens in” and livestream conversations. There was, however, no detail of new policies or initiatives, or any sign of when a new president would be appointed to get a grip on the preparations for the summit. Gove was briefly heckled when answering a question about what, for him, would constitute a successful summit. As he began by talking about “acceptance of the needs to act”, an audience member shouted that acceptance had been reached 20 years ago, and what was needed was action. Gove continued: “Acceptance of the needs to act leads to action that is irreversible, accelerating and increasing.”"
"
For now, we have about 1 year of significant cold phase tendency in the Pacific Decadal Oscillation (PDO), here is the last 108 years of the PDO index, plotted from monthly values:

Click for larger image – source Steven Hare, University of Washington
Compared to the negative magnitudes seen from 1946 to 1977, our current PDO phase shift magnitude is relatively mild. But that could change. Don J. Easterbrook, a retired professor from the Dept. of Geology, Western Washington University, in Bellingham, WA sends this analysis:
la-nina-and-pacific-decadal-oscillation-cool-the-pacific (PDF)
The announcement by NASA’s Jet Propulsion Laboratory that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase (Fig. 1) is right on schedule as predicted by past climate and PDO changes.
Global temperatures peaked in 1998 and have not been exceeded since then. Pacific Ocean temperatures began a cooling phase in 1999 that was briefly interrupted by El Nino and dramatic cooling in 2007-2008 appears to be a continuation of a global cooling trend set up by the PDO cool phase (Fig. 1) as predicted [shown in the figure below].

Thus, we seem to be headed toward several decades of global cooling, rather than the catastrophic global warming predicted by IPCC. 
If we are lucky, this PDO will be a short event. 2-4 years. If we are unlucky, and it is the “full Monty” phase switch at 20-30 years as Easterbrook suggests, we may be in for extended cooler times. This may result in some significant extended worldwide effects, notably on agriculture.
UPDATE! Professor Easterbrook adds in comments:
“The projected warming from ~2040 to ~2070 is NOT driven by CO2, it’s merely a continuation of warm/cool cycles over the past 500 years, long before man-made CO2 could have been a factor. We’ve been warming up from the Little Ice Age at rate of about 1 degree or so per century and the 2040-70 projection is simply a continuation of non-AGW cycles. 
An interesting question is the similarity between what we are seeing now with sun spots and global temperature and the drop into the Little Ice Age from the Medieval Warm Period. Could we be about to repeat that? Only time will tell–We might see a more pronounced cool period like the 1880 to 1910 cool cycle (when many temp records were set) or a milder cooling like the 1945-1977 cool cycle. In any case, the setting up of the cool phase of the PDO seems to suggest cooler times ahead, not the catastrophic warming predicted by IPCC and Al Gore.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f700a29',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterRecently Hollywood director James Cameron participated in a panel discussion together with economist Tom Friedman, actress Sigourney Weaver, and MSNBC’s Joe Scarborough. Cameron called Climate Change ‘As Great As The Threat’ U.S. Faced in World War II. To me, watching some Hollywood stars give advice on saving the planet is like watching lumberjacks talk about how to do heart surgery. In both cases the result is the same: the patient dies.
http://www.cnsnews.com/news/article/64516
Cameron’s humble abode
Climate protection starts at home. According to more than one source. Cameron’s humbe abode has 8,272 square feet and boasts 6 bedrooms, 7 bathrooms, pool, tennis court and inner courtyard fit for Hollywood royalty – perfect to sit around in with like-minded stars and worry about the “dying planet”, all the while the private jet sits on the tarmac getting refuelled and the chauffeured limousine waits outside.
http://realestock.com/joomla/blog/Inside-the-Homes-of-2010s-Oscar-Nominees.html (scroll down)
Making everyone miserable
Perhaps Cameron’s arrogance, hypocrisy and insistence the rest of the working masses sacrifice more while he continues to live high on the hog has something to do with a character disorder.  According to Wikipedia collaborator and author Orson Scott Card calls Cameron, who has been married five times, selfish and cruel. He said working with him was “hell on wheels”.  Card added, “He was very nice to me because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way”.
Cameron’s real estate visions: community of a half dozen estates
http://www.therealestatebloggers.com/2007/06/03/james-cameron-selling-730-acres-in-malibu-near-pepperdine-university/
Share this...FacebookTwitter "
"

Net primary production (NPP) represents the net carbon that is fixed (sequestered) by a given plant community or ecosystem. It is the combined product of climatic, geochemical, ecological, and human effects. In recent years, many have expressed concerns that global terrestrial NPP should be falling due to the many real (and imagined) assaults on Earth's vegetation that have occurred over the past several decades—including wildfires, disease, pest outbreaks, and deforestation, as well as overly-hyped changes in temperature and precipitation.   
  
The second “National Assessment” of the effects of climate change on the United States warns that rising temperatures will necessarily result in the reduced productivity of major crops, such as corn and soybeans, and that crops and livestock will be “increasingly challenged.” Looking to the future, the National Assessment suggests that the situation will only get worse, unless drastic steps are taken to reduce the ongoing rise in the air's CO2 content (e.g., scaling back on the use of fossil fuels that, when burned, produce water and CO2).   
  
But is this really the case? If growing crops are increasingly affected, damage should also be showing up in the global ecosystem. Is the productivity of the biosphere in decline?   
  
In a word, **_no!_** Observational data indicate that just the _opposite_ is occurring (see, for example, the many studies reviewed previously on this topic here). Rather than withering away, biospheric productivity is _increasing_ , thanks in large measure to the growth-enhancing, water-saving, and stress-ameliorating benefits of atmospheric CO2 enrichment.



The latest study to confirm as much comes from the research team of Li _et al_. (2017). Working with a total of 2,196 globally-distributed databases containing observations of NPP, as well as the five environmental variables thought to most impact NPP trends (precipitation, air temperature, leaf area index, fraction of photosynthetically active radiation, and atmospheric CO2 concentration), Li _et al_. analyzed the spatiotemporal patterns of global NPP over the past half century (1961–2010).   
  
Results of their analysis are depicted in the figure below, which shows that global NPP increased significantly from 54.95 Pg C yr-1 in 1961 to 66.75 Pg C yr-1 in 2010 (Figure 1a). That represents a linear increase of 21.5 percent in the last half-century. In quantifying the relative contribution of each of the five variables impacting NPP trends (Figure 1b), Li _et al_. report that “atmospheric CO2 concentration was found to be the dominant factor that controlled the interannual variability and to be the major contribution (45.3%) of global NPP.” Leaf area index, which is also enhanced by increasing atmospheric carbon dioxide, was the second most important factor, contributing an additional 21.8 percent, followed by climate change (precipitation and air temperature together) and the fraction of photosynthetically active radiation, which accounted for the remaining 18.3 and 14.6 percent increase in NPP, respectively. Li _et al_. also report that the vast majority of the observed rise in NPP occurred in the middle and high latitude regions, with 61.1 percent of the increase occurring between 30 and 60 degrees of latitude and 26.4 percent between 60 and 90 degrees of latitude of both hemispheres (see Figure 1c).   






**_Figure 1_** _. (A) Annual variations in global NPP between 1961 and 2010. (B) Changes in NPP in recent decades that resulted from multiple environmental factors including climate, leaf area index (LAI), fraction of photosynthetically active radiation (fPAR), and CO 2, and the relative contribution rate (%) of each factor during the study period. (C) Spatial distribution of the trend in NPP during the period 1961–2010. Source: Li _et al. _(2017)._



The observed increase in global NPP over the past five decades is quite an accomplishment for the terrestrial biosphere, especially when one considers all the negative stories—nary a day goes by without notice of some environmental disaster (human- or naturally-caused) occurring somewhere in the world and wreaking havoc on nature. Since 1980, the Earth has experienced three of the warmest decades in the modern instrumental temperature record, has weathered a handful of intense and persistent El Niño events, and suffered large-scale deforestation, ""unprecedented"" forest fires, disease and pest outbreaks, and episodes of persistent, widespread, and severe droughts and floods. Yet, despite each of these factors, and every other possible negative influence that has occurred over the past half century, terrestrial net primary productivity has increased by 21.5 percent! And it has done so largely because of the ongoing rise in atmospheric CO2. How ironic it is, therefore, that the supposed chief _culprit_ behind the many real (and imagined) assaults on Earth’s vegetation—rising atmospheric CO2—has been found to be the primary _cause_ of an ever-greener planet.   
  
**Reference**   
  
Li, P., Peng, C., Wang, M., Li, W., Zhao, P., Wang, K., Yang, Y. and Zhu, Q. (2017) ""Quantification of the response of global terrestrial net primary production to multifactor global change."" _Ecological Indicators_ **76** : 245–255.


"
"We are living on the planet of the chickens. The broiler (meat) chicken now outweighs all wild birds put together by three to one. It is the most numerous vertebrate (not just bird) species on land, with 23 billion alive at any one time. Across the world, chicken is the most commonly eaten meat. This has made it a vivid symbol of the Anthropocene – the proposed new geological epoch that marks the overwhelming impact of humans on the Earth’s surface geological processes. The modern bird is now so changed from its ancestors, that its distinctive bones will undoubtedly become fossilised markers of the time when humans reigned the planet.  In a recent study together with colleagues, published by Royal Society Open Science, we compared the bones of the modern meat chicken to the bones of their ancestors dating back to Roman times. Modern broiler chickens are radically different – they have a super-sized skeleton, distinct bone chemistry reflecting the homogeneity of their diet and significantly reduced genetic diversity. This is because a modern broiler is twice the size of a chicken from the medieval period and they have been bred for one thing: rapid weight gain.  The speed of growth accelerated in the second half of the 20th century, with the modern broiler putting on weight five times faster than meat chickens from the 1950s. The result is that at just five or six weeks old they are already slaughter ready. The evidence of this extraordinary growth is preserved in their bones, which are less dense and often deformed. Poignantly, these birds cannot even be “rescued” from their factory farms – the strain of their enormous body means that if left to live even for another month, many birds die from heart or respiratory failure.  The modern chicken only exists in its current form due to human intervention. We have altered their genes to mutate the receptor which regulates their metabolism, which means that the birds are always hungry and so will eat and grow more rapidly. Not only that, their entire life cycle is controlled by human technology. For example, the chickens are hatched in factories with computer-controlled temperature and humidity. From one day old, they live under electric lights to maximise the hours they can feed. Their slaughter by machine allows for thousands of birds to be processed every hour. Domesticated cows, pigs and sheep each number a billion or so, but it is chickens that are the most striking example of the modern biosphere. Their bones are scattered across landfill sites and farms worldwide and therefore have a good chance of being preserved in the rock record as symbols of how our planet and its biosphere has changed from its pre-human state to one dominated by humans and our domesticated animals.  While humans have been selectively breeding chickens since their domestication in south-east Asia around 6,000 years ago, the speed and scale of change in the 20th century is far beyond anything observed in the past. From the 1950s, the chicken population has risen in step with the rise in human population, as has our use of fossil fuels, plastics and other resources: now, this enfeebled and short-lived animal is more numerous than any bird species in Earth history. What does the future hold? Right now, chicken consumption is on the rise. The meat is cheap, and many are moving away from beef and pork in order to reduce their greenhouse gas emissions. Somehow we must adapt to a growing population in a world affected by climate change. But business as usual may be off the cards. In a surprising move, the world’s largest chicken producers – Tyson Foods and Perdue Farms – are now investing in plant-based proteins. Does this mean the era of chickens could be over in a (geological) instant?  Nevertheless, the record of this human-engineered bird will be forever set in stone. Any intelligent species which arises in the far future – hyper-evolved rats or octopuses, perhaps? – will have a puzzle on their hands (or tentacles) in trying to figure out how and why millions of these rapidly-evolved bones lie mixed with the technofossil debris of the huge petrified dumpsites we will leave behind. As these future explorers reconstruct this bird – a creature far more helpless than the dodo – they may well rumble it too as a technological construct."
nan
"A few years ago, I found myself in Gunaikurnai country, in a place more recently given the lusty name Paradise Beach. I was driven there by my own strong settler/coloniser impulse, after googling a list of “cheapest beachside addresses in Australia”. My notion was that, given the unlikeliness of my owning an everyday home, I could skip straight to “beach house”. I was looking to cut off a slice of so called Australia, put a fence around it, and indulge in endless summer. I booked an extraordinarily cheap Airbnb and decided to take a recce. I sped up the highway past the smoking Hazelwood power station and Loy Yang open-cut brown coalmine before turning onto a lonely, unpoliced road through vast swathes of clear-cut pine plantations and agricultural green. I hit Paradise singing country music, taking note of the free but nearly empty RV park, the foreclosed takeaway joint and general store selling nothing but booze and tackle. The action in town was clearly at the two real estate agents, side by side like gunslingers on a dusty main drag.  The whole area smacked of grand enterprise unfulfilled. Fibro shacks, flat pack houses and shanti lean-tos hustled together in the scrubby blocks on streets named Bondi and Clovelly for wealthier beaches. “It’s like the 1970s there,” I’d henceforth tell anyone willing to ignore my 1983 birthdate and nostalgically understand the 70s to mean rebellion, entitlement and AC/DC in any order or combination. My dating of Paradise Beach turned out to be, in part, accurate. In the 19th century the Gippsland Lakes were opened up for the purpose of transport and fishery. As a result freshwater environments turned brackish, and the periodically inundated land between the lakes and Bass Strait dried out, leaving stretches that could be occupied, European style, after centuries of Indigenous custodianship. In the mid-20th century some men from council, development and advertising got together to launch the Golden Beach Club Estate plan, and other assorted dreamland havens; marketing cheap subdivisions on “mile after mile of glittering golden sand” to new Australians looking to purchase a postcard-worthy plot. 1954 advertising copy heralds a holiday haven of fishing, duck shooting, and jaunts on the Tambo Princess cruiseliner. An offset-printed bathing beauty with perfect blonde pin curls and bedroom eyes preens on the shore of a future that would never be. “A New Gold Coast is born!” runs the slogan. Reader, it is not. The dream stalled. And by the 1970s concerns about building homes on a sandy wedge between the ocean and a wet place were conceded. Though many subdivisions were sold, only three areas were approved for decidedly un-Gold-Coastian development. Paradise Beach is, despite mid-century ambitions, a tiny, mostly unoccupied clutch of residential zoning near the middle of the fourth longest uninterrupted beach in the world. Often deserted and unswimmable, the ocean itself is commonly a rough, relentless, reminder that you are standing at the edge of the world. Sometimes though, it can be glassy, flat, grey or green and clotted with weed – a visual image of consciousness. The sand is indeed golden, the scrubland behind virtually indistinguishable mile by mile. I have walked along this looping landscape for so long that my brain clicks, the idea of change becomes wholly theoretical and time plunges into the swell, opening out to infinity. That first trip to Paradise Beach I finished writing a book, danced alone on the sand, read for hours, swam, yelled at the wind; felt wild, young and free. I know that it’s a well-wrung cliché for a writer to walk on a beach and think about infinity and maybe pick up some shells and marvel at all they cannot understand. But I’m trying to bounce between the poetic and the practical here in a way that’s productive. I bring you to this place and hand you this mixed bag of facts and emotions because I want to demonstrate how the places and times we find ourselves feeling in can connect to complicated histories and futures. The emergency of today is also the one we have been producing via colonialism and capitalism for hundreds of years. Writing in an emergency means attending to this complexity, becoming unable to render the world as only a backdrop for individual human drama and aspiration. As the summer of 2019 hit, we all watched places we love burn. Exhausted volunteers worked themselves to husks defending trees, houses and animals. Fire sliced up the map. Fence lines blurred and fell. I scrolled through images of eerie orange cities and koalas venturing onto the hot tar to drink from plastic bottles. I, like many, wondered if Australian summer would ever be the same. Perhaps in denial or determination, I checked the requisite disaster apps, packed up the station wagon and headed to Paradise Beach. But when I vaulted across the empty campground and onto the sand, the beach had changed. It was a hot midweek day, the first of the smoky ones for Melbourne, and there, in the middle of infinity, was an offshore drilling rig. I tried to ignore the thing. I walked along the beach waiting for my angst to melt and my brain to kick into infinity-mode, but it wouldn’t. The massive rig commanded my eye with its ominous steel limbs. It reminded me of all the others I’ve seen. Gas drilling rigs in the Arafura, the Montara oil spill in the Timor Sea, the flaming Deepwater Horizon in the Gulf of Mexico, all reaching back to 1989, when I sat baffled in front of the news watching people clean oil from seals as my mother attempted to explain how we extract oil and gas and burn it to make energy but it’s not safe, or forever, and sometimes things go very wrong. A rig. An anchor for anxiety and finite time. How many parents are attempting these explanations for their children now? The rig off Paradise Beach is an experimental driller for “Carbon Net”, a carbon capture and storage project capable of processing a promotional video extols, “the equivalent of CO2 emissions from around one million cars every year that it operates”. The comparison is misleading however, as Carbon Net will not capture emissions from the air but from high polluting industrial sites in the Latrobe Valley, piping them seaward to inject into layers of sandstone deep in Bass Strait. In this state of emergency, I know that decarbonisation should be our priority. I don’t know enough about CCS to rail against it here, though phrases like “clean coal” juxtaposed with “gradual or catastrophic leakage” pique the imagination in terrifying ways, threatening ugly texts and futures. I can understand why the residents of Paradise Beach have gone full Nimby on the project, which exemplifies ongoing exploitative relationships between urban, regional and rural areas and shows, for at least the second time in 75 years, and the third in 200, how one stretch of ocean can be sized up for an unproven and risky future. The greater argument here though, is not strictly against Carbon Net and CCS per se but against overinvestment in anything, whether material (like infrastructure) or immaterial (like rhetoric), that insists capitalism is an infinite jaunt along golden dunes. Anxiety about our future is not paranoia, but comes from a rising understanding that the way we live now is incompatible with climate action. The grief and pain we feel is our sense of responsibility smashing against our desire for tomorrow to be the same as, or better than, yesterday. We are at a moment in time that smacks of grand enterprise gone awry. We need to back out, decide what to relinquish (perhaps highly polluting industries and a colonial conception of home) instead of scrabbling to scaffold our out-of-date dreams of paradise. And the time to do this is now, was now in 1954, and 1970 and now and now and now – there is no other time. Writing probably doesn’t feel like the most crucial response here, and maybe it’s not. There is lots of other work to be done. But writing can help us see connections, record violence, build empathy, address possible futures. Writing in an emergency means pulling yourself back from the nostalgic deep dive. It means unwriting our lusty paradises, because they never were. As I drove home early through a smoke-hazy sunset, it occurred to me that a new kind of discipline might develop when you are on edge – I am a woman writer at the edge of this era of denial, of her emotional tether, of her store of patience with governance and systems of power. I am also a woman driven to the literal edge of a landmass by double-edged desires for escape and the illusion of home, looking out to imagined infinity and a very real drilling rig. Seeing rare Burrunan dolphins breaching blue waves, flocks of black and white cockatoos crisscrossing a luminous red sky and still only partially comprehending how much is at stake. Lucky Ticket by Joey Bui (Text Publishing) Songspirals by Gay’wu Group of Women (Allen & Unwin) See What You Made Me Do by Jess Hill (Black Inc) The House of Youssef by Yumna Kassab (Giramondo Publishing) Diving into Glass by Caro Llewellyn (Penguin Random House) When One Person Dies the Whole World is Over by Mandy Ord (Brow Books) There Was Still Love by Favel Parrett (Hachette Australia) Here Until August by Josephine Rowe (Black Inc) This is How We Change the Ending by Vikki Wakefield (Text Publishing) The Yield by Tara June Winch (Penguin Random House) The Weekend by Charlotte Wood (Allen & Unwin) Paper Emperors by Sally Young (NewSouth Publishing) The winner of the Stella prize will be announced in Sydney on Wednesday 8 April.  • Briohny Doyle is the author of The Island Will Sink and Adult Fantasy. She is a lecturer in creative writing at Deakin University"
"Jeremy Corbyn has accused Boris Johnson of “failing spectacularly” to measure up to the scale of the climate crisis, after the sacked president of COP 26 revealed the UK was miles behind in getting ready for the November summit. Speaking at prime minister’s questions, Corbyn raised the government’s failure to organise COP 26 properly, after Johnson’s team sacked Claire O’Neill as the summit’s president just days before its formal launch. Corbyn highlighted O’Neill’s criticisms that “there has been a huge lack of leadership and engagement from this government” over the climate crisis conference. But Johnson dismissed the attack, saying all that Corbyn would produce on tackling global heating was “a load of hot air”. “If you look at what this government is achieving and already has achieved on climate change, it is quite phenomenal,” Johnson claimed. The international COP 26 summit is due to take place in November in Glasgow but is without a leader after David Cameron and William Hague both turned down the vacant role of president. Corbyn noted that two former Conservative leaders had turned down the job, joking that “maybe it could be third time lucky”, as he suggested Sir Iain Duncan Smith for the role. Labour said its suggestion would be for Ed Miliband, the former energy secretary and ex-Labour leader, to take over the presidency as he had the experience. Corbyn’s spokesman said: “Ed Miliband is certainly someone who has a strong record and an entirely suitable person. The issue is not exactly who should take on the role but someone with credibility.” The Labour leader went on to criticise Johnson’s record on climate change, highlighting the comments from the prime minister that were sceptical of climate science until as recently as four years ago. “Considering his monumental failure in advance of COP 26, isn’t it really just a continuation of his climate change denial statements that he was regularly making up until 2015?” Corbyn said. Johnson said Corbyn was “talking absolute nonsense” and defended the government’s agenda, saying: “We lead the world in going for a zero-carbon approach. His own approach is utterly unclear and has been condemned by the GMB as a disaster for the UK economy. “He would confiscate people’s cars and prevent them from having foreign holidays.” Corbyn countered that the prime minister had a “very vivid imagination”, adding: “Unfortunately, his vivid imagination seems to have taken over from his memory because he might recall saying that climate change is a primitive fear without foundation.”"
"Unfortunately, it’s the 15th anniversary. … We want you to help, because in the years to come, we do not want to write another ad that says: “Unfortunately, it’s the 30th anniversary”. (Bhopal Medical Appeal advertisement, 1999) Unfortunately, it is indeed the 30th anniversary of the the Bhopal gas leak and it remains an ongoing disaster of epic proportions. The world’s worst industrial catastrophe was utterly preventable, stemming from serious negligence on the part of chemical manufacturers Union Carbide.  Despite widespread, creative and steadfast activism, the site of the chemical factory that released tons of lethal gases in December 1984 has not been cleaned up. Toxins continue to leak into the soil and groundwater supply up to 3km from the site of the abandoned factory, causing a third generation to be born with serious health problems and disabilities. This is often described as Bhopal’s “second disaster”. Bhopal is back in the news as the anniversary protests and commemorations make for arresting headlines, but campaigners in the city have been working tirelessly to provide healthcare without state support and to lobby for adequate compensation.  In an era of rapid-fire news reports, ceaseless disasters, and short-term memories, the question is how to retain focus on the effects of disasters that last for decades rather than months or years.  It’s an issue we’re grappling with at Leeds as part of a series of events entitled Reframing Disaster. As researchers working on Bhopal and other global catastrophes, we feel this struggle exposes the dark heart of globalisation and corporate abuse. At the same time, we wanted to show how survivors living in the poorest conditions, experiencing ongoing trauma and neglect, have created networks of solidarity, care, and hope. One of the things we’ve tried to do is put pressure on the idea of disaster as an “event” – in Bhopal’s case, something that happened on 2-3 December 1984 – and instead focus on the long-term nature of catastrophe. This is very effectively dramatised through the arts: creative writing, photography, music and film provide us with lasting reflections on the physical and emotional fallout of large-scale disasters. They go far beyond numbers and statistics and offer a basis for understanding the preconditions for recovery and healing. Contrast this with the technocratic focus in disaster research on mitigation, preparedness and response. Without attention to long-term recovery these sorts of accidents will continue to happen. To understand this dynamic, we need to look towards the profound forms of knowledge that the arts can bring to issues like trauma, injustice and exploitation in the wake of disasters. Take the world-renowned Indian photographer Raghu Rai, whose work provides vivid testimony of the disaster’s effects over time. As one of the first photographers on the spot in Bhopal in 1984, Rai documented the immediate aftermath of “that night”, and return visits over the past 30 years have built up a powerful visual archive of the local hardships. He is currently back in the city once again to document the anniversary commemorations. We’ve been fortunate to be able to collaborate with the Bhopal Medical Appeal to bring an exhibition of Rai’s work to Leeds, along with a presentation of Francesca Moore’s Bhopal: Facing 30 photography,movie screenings and performance poetry by Avaes Mohammad, a former chemist who won an Amnesty International award for his poem Bhopal. People are keen to understand why disasters like Bhopal last so long. After all, there are many Bhopals around the world – whether in the shape of ongoing catastrophes such as the radioactive Spanish town Palomares, the nuclear Pacific or Fukushima where the long-term effects are keeping people vulnerable, or in the shape of disasters waiting to happen.  Activism and the arts are at the forefront of exposing how power and privilege impact upon the poorest communities and continue to place them directly in harm’s way. We need to continue challenging the structural inequalities driving the rise and overwhelming scale of global disasters. The injustice still that surrounds Bhopal after 30 years shows exactly why."
nan
"The world is facing a series of interlinked emergencies that are threatening the existence of humans, because the sum of the effects of the crises is much greater than their individual impacts, according to a new global study. Climate breakdown and extreme weather, species loss, water scarcity and a food production crisis are all serious in themselves, but the combination of all five together is amplifying the risks of each, creating a perfect storm that threatens to engulf humanity unless swift action is taken.  The links among the crises are clear in many cases, but the methods the world has chosen to try to solve them do not take account of these connecting factors. For instance, extreme heatwaves can add to global heating, because they release vast amounts of stored carbon from affected ecosystems, in a feedback loop. It has been seen clearly in the Australian bushfires, which are already contributing significantly to the store of carbon in the atmosphere. The links do not stop there: as the heatwaves damage natural ecosystems, killing off wildlife and flora, they also lead to greater water scarcity, and in turn damage agriculture. These combined effects exacerbate the harm done to people struggling with food and water shortages, in a vicious cycle. Faced with these crises in nature individually, it could be possible to fix the problems causing them. But confronted with multiple interlinked emergencies that in combination amplify one another’s impacts, people are facing unprecedented dangers and many communities cannot cope. The report, which took the form of a survey of 222 leading scientists from 52 countries, conducted by the international sustainability network Future Earth, found that the responses to these emergencies by governments, civil society, business and institutions did not recognise their interlinked nature. Trying to solve the problems individually, without taking account of the “cascading” impacts, was likely to be ineffective, the scientists said. More than a third of the scientists surveyed said the five crisis types would worsen one another “in ways that might cascade to create global systemic collapse”. While the risks are amplified when they are connected, so too are the solutions, however. Whenever action is taken to remedy environmental problems, the benefits also cascade: for instance, nurturing wildlife and flora in a wetland can also reduce water pollution and soil erosion, and protect crops against storm damage, alleviating water scarcity and allowing for more food production. “Despite the ubiquity of connections [between these looming crises] many scientists and policymakers are embedded in institutions that are used to thinking and acting on isolated risks, one at a time,” the report says. “This needs to change, to thinking about risks as connected.” Amy Luers, the executive director of Future Earth, said: “2020 is a critical time to look at these issues. Our actions in the next decade will determine our collective future on Earth.” The authors of the report urged a change in the way risks were handled: “We call on the world’s academics, business leaders and policymakers to pay urgent attention to these five global risks, and to ensure they are treated as interacting systems, rather than addressed one at a time in isolation.” The report also warned of social problems that scientists identified as potential major risks for the future. These included the rise of populism and fake news, trends in migration and the rise of artificial intelligence."
"

Asked recently when the Senate might vote on cap‐​and‐​trade, Senate Majority Leader Harry Reid, D-NV, demurred, muttering about “a busy, busy time the rest of this year.” And yet last week, the Obama administration quietly moved forward with a plan to regulate power plants and other large stationary sources of greenhouse gases.



The Obama team appears to believe it has the authority to implement comprehensive climate change regulation, Congress be damned. Worse still, under current constitutional law–which has little to do with the actual Constitution–they’re probably right.



In a democratic country, you’d think that before the executive branch could regulate CO2–a ubiquitous substance essential to life–the legislature would have to vote on the issue. But you’d be wrong.



In 2007, the Supreme Court ruled that the 1970 Clean Air Act’s definition of air pollutant was broad enough to allow regulation of CO2 emissions from new cars, and that the EPA was required to regulate once it issued a finding that CO2 contributes to global warming. In fact, once the EPA rules that CO2 is a dangerous pollutant–as it did in April–regulation of industrial sources likely becomes mandatory as well.





As Obama’s popularity erodes, he may come to like the idea of being the ‘decider.’



But existing law still leaves the executive branch enormous discretionary power–and thus a hammer to hold over Congress’s head. A report issued in April by the New York University Law School argues that “if Congress fails to act, President Obama has the power under the Clean Air Act to adopt a cap‐​and‐​trade system.”



James Madison believed that there could be “no liberty where the legislative and executive powers are united in the same person.” And yet, here we are, with those powers united in the person of a president who has pledged to heal the planet and stop the oceans’ rise.



This constitutional nightmare is the culmination of a trend many years in the making. The first sentence of the Constitution’s first article says that “all legislative Powers herein granted” are vested in Congress.



The Supreme Court once took that language seriously, as when, in 1935, it struck down a key New Deal program for delegating legislative power to the executive. Yet the Court eventually made its peace with statutes that allow the executive branch to both make and enforce the law.



That paved the way for the modern administrative state, which looks a lot like the situation complained of in the Declaration of Independence, in which “a multitude of New Offices… harass our people and eat out their substance.”



After 9/11, the phrase “unitary executive theory” (UET) came to stand for the idea that the president can do whatever he pleases in the national security arena. But it originally stood for a humbler proposition: UET’s architects in the Reagan administration argued that the Constitution’s grant of executive power to the president meant that he controlled the executive branch, and could therefore rein in aggressive regulatory agencies.



In an era when Republicans held a virtual lock on the Electoral College, that idea had some appeal. But as Elena Kagan, now President Obama’s Solicitor General, pointed out in a 2001 Harvard Law Review article, there’s little reason to think that “presidential supervision of administration inherently cuts in a deregulatory direction.”



How far will Obama push in the other direction? He may be reluctant to stretch his authority as far as the law will allow, in a political climate where even green‐​leaning Democrats scream bloody murder every time gas prices rise.



But as Kagan notes, after the Democrats lost control of Congress in 1994, President Clinton used his regulatory authority unilaterally to show progress, pushing “a distinctly activist and pro‐​regulatory agenda.” As Obama’s popularity erodes, he may come to like the idea of being the “decider.”



Will liberals who decried George W. Bush’s unilateralism object to this staggering concentration of executive power? Don’t hold your breath.
"
"
Share this...FacebookTwitterHighly anomalous terrain (an active volcano), 40 years of cooling temperatures, and a CO2 record that dramatically contrasts with fluctuating values from forests and meadows reaching 600-900 ppm all beg the question: Is Mauna Loa’s CO2 record globally representative?
Mauna Loa is the Earth’s largest land volcano. It has erupted over 3 dozen times since 1843, making this terrestrial landscape extremely unusual relative to the rest of the globe’s terrain. (Forests, in contrast, cover over 30% of the Earth’s  land surface.)
Mauna Loa has been thought to be the world’s best location to monitor global CO2 levels since 1958.
While Mauna Loa CO2 levels show a rise of 338 ppm to 415 ppm since 1980, Mauna Loa temperatures (HCN) show a cooling trend during this same time period. The only warming period in the last 65 years occurred between about 1975 and 1985.

Image Source: oz4caster
Forest CO2 fluctuations
As mentioned above, forests are orders of magnitude more terrestrially representative than the highly anomalous site of the Earth’s largest volcano.
In forests or tree-covered areas, CO2 rises from around 300 ppm in the warmth of the afternoon (~3 p.m.) to over 600 ppm before sunrise (~4 a.m.), when it is cooler (Fennici, 1986, Hamacher et al., 1994). This massive fluctuation occurs daily and CO2 values average out to be far higher than the Mauna Loa record suggests.

Image Source: Fennici, 1986

Image Source: Hamacher et al., 1994
Meadow CO2 fluctuations
In open fields, or meadows, air CO2 can vary between 266 ppm and 1,430 ppm. The average variance is from 280 ppm to 980 ppm 2 meters above the soil (Szaran et al., 2005).
Interestingly, just as in forests, temperature drops of 4 to 5°C are associated with rising levels of CO2.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Szaran et al., 2005
CO2 beneath snow and ice
Modern CO2 concentrations beneath snowpack and ice range from 600 to 1800 ppm. These concentrations can fluctuate by as much as 200 ppm within a period of just 4 days (Massman and Frank, 2006).
If this kind of rapid and wide-ranging variability can be observed for modern conditions, our capacity to accurately assess the “global” CO2 concentration for ice and snow thousands of years old becomes all the more suspect.

Image Source: Massman and Frank, 2006
CO2 near cave entrances
Within caves, CO2 levels can reach as high as 30,000 ppm. Even in the open air <1 meter from the entrance to a cave, CO2 levels can reach 11,500 ppm (Cowan et al., 2013).
CO2 levels vary by 10s of 1000s of ppm from one cave to the next in the same geographical region.

Image Source: Cowan et al., 2013
Mauna Loa CO2 is globally representative?
Cave entrances should probably be considered no less terrestrially unusual than the site of the world largest active volcano. And certainly forests and meadows are far more representative of the Earth’s terrestrial landscape than the Mauna Loa site.
And yet it has been decided, via consensus, that the rarified air above a Hawaiian island in the middle of the Pacific correctly monitors the CO2 levels for the entire globe.
Why?


		jQuery(document).ready(function(){
			jQuery('#dd_ec2a615e01c9d5f09bc7e5a514a859b6').on('change', function() {
			  jQuery('#amount_ec2a615e01c9d5f09bc7e5a514a859b6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The security of the United States does not require nearly 1,600 nuclear weapons deployed on a triad of systems — bombers, land‐​based intercontinental ballistic missiles (ICBMs), and submarine‐ launched ballistic missiles (SLBMs) — to deliver them. As Cato’s Benjamin H. Friedman, Christopher A. Preble, and Matt Fay calculate in **“The End of Overkill? Reassessing U.S. Nuclear Weapons Policy”** (White Paper), a smaller arsenal deployed entirely on submarines would save roughly $20 billion annually while deterring attacks on the United States and its allies. The triad grew from the military services’ competition to meet the Soviet threat. The public rationale was based upon the notion of a second strike: a diversity of delivery systems ensured the nuclear arsenal’s survival against a Soviet preemptive attack. The more sophisticated rationale was a first strike: deterring Soviet aggression against European allies required the ability to preemptively destroy their nuclear forces. But, as the authors show, “U.S. power today makes the case for the triad more dubious.” No U.S. adversary has the capability to destroy all U.S. ballistic submarines, let alone all three legs, and there would be time to adjust if that changed. In fact, nuclear weapons are essentially irrelevant in actual U.S. wars, which are against insurgents and weak states without nuclear arsenals. Cases where the success of deterrence hinges on the U.S. capability to destroy enemy nuclear forces are far‐​fetched. “Even hawkish policies do not require a triad,” the authors write. At a time when austerity heightens competition for Pentagon resources, service leaders may see nuclear missions “as red‐​headed step‐​children that take from true sons.” That shift would facilitate major reductions in the nuclear arsenal, the elimination of at least one leg of the triad, and substantial savings.



 **Comparing Welfare to Work**  
In 1995 the Cato Institute published a groundbreaking study estimating the value of the full package of welfare benefits available to a typical recipient in each of the 50 states and the District of Columbia. It found that not only did the value of such benefits greatly exceed the poverty level but, because welfare benefits are tax‐​free, their dollar value was greater than the amount of take‐​home income a worker would receive from an entry‐​level job. Since then, many welfare programs have undergone significant change. In their new analysis, **“The Work versus Welfare Trade‐​Off: 2013”** (White Paper) Michael D. Tanner, senior fellow at the Institute, and Cato research assistant Charles Hughes examine the current system in the same manner. “Welfare benefits continue to outpace the income that most recipients can expect to earn from an entry‐​level job, and the balance between welfare and work may actually have grown worse in recent years,” they write. In fact, the current system provides such a high level of benefits that it acts as a disincentive for work. Welfare currently pays more than a minimumwage job in 35 states, even after accounting for the earned income tax credit, and in 13 states it pays more than $15 per hour. “If Congress and state legislatures are serious about reducing welfare dependence and rewarding work,” the authors conclude, “they should consider strengthening welfare work requirements, removing exemptions, and narrowing the definition of work.” Moreover, states should consider ways to shrink the gap between the value of welfare and work by reducing current benefit levels and tightening eligibility requirements.



 **Subsidizing the Risk of Terrorism**  
The terrorist attacks of September 11, 2001, inflicted enormous losses on the insurance industry and businesses. In the wake of these disruptions, the government enacted the Terrorism Risk Insurance Act of 2002 to create a “temporary” federal backstop against catastrophic losses. In effect, this program subsidized private risk with public funds through a cost‐​sharing program for which the government does not receive any compensation. But as Robert J. Rhee, professor of law at the University of Maryland, writes, “if there was some ambiguity about the program’s need before, there is none now.” In **“The Terrorism Risk Insurance Act: Time to End the Corporate Welfare”** (Policy Analysis no. 736), Rhee argues that terrorism risk is not more severe than other insurable risks such as natural catastrophes. A federal backstop stakes public money to protect the insurance industry, and subsidizes the terrorism risk insurance premiums for commercial policyholders. “The private market is capable of underwriting this risk,” he continues. Yet in response to effective lobbying by the insurance industry and business interests, Congress has twice extended the program. The program is now scheduled to sunset at the end of 2014, 12 years after this supposedly temporary program was instituted. Rhee argues that the program should sunset as scheduled in 2014, thus ending this form of corporate welfare. “After the fears of the unknown have subsided,” he concludes, “[the insurance market] can more rationally assess terrorism risk and price it.”



 **Driving Investment Policy**  
No country has been a stronger magnet for foreign direct investment than the United States. Valued at $3.5 trillion, the U.S. stock of inward foreign direct investment accounted for 17 percent of the world total in 2011, more than triple the share of the next largest destination. In **“Reversing Worrisome Trends: How to Attract and Retain Investment in a Competitive Global Economy”** (Policy Analysis no. 735), Daniel J. Ikenson, director of Cato’s Herbert A. Stiefel Center for Trade Policy Studies, notes that as the world’s largest economy, the United States has been able to attract the investment needed to undergird its position atop the global economic value chain. “But the past is not necessarily prologue,” he argues. Indeed, while the U.S. claim to 17 percent of the world’s stock of foreign direct investment is impressive, the share stood at 39 percent as recently as 1999. To a large extent, this trend reflects the emergence of new, viable destinations for investment resulting from inevitable demographic, economic, and political changes. “However, some of the decline is attributable to a deteriorating U.S. investment climate,” Ikenson writes. That environment conspires to deter inward investment and to encourage companies to offshore operations that could otherwise be performed competitively in the United States. Ikenson concludes that a proper accounting of these policies, followed by implementation of reforms to remedy shortcomings, will be necessary if the United States is going to compete effectively for the investment required to fuel economic growth and higher living standards.



 **Against Military Action in Syria**  
In the midst of growing public wariness about large‐​scale foreign interventions, the Obama administration has decided to arm the Syrian rebels. But according to Erica D. Borghard, a PhD candidate in political science at Columbia University, in **“Arms and Influence in Syria: The Pitfalls of Greater U.S. Involvement”** (Policy Analysis no. 734), those who call for increasing the scope of U.S. aid to the Syrian rebels are wrong on all counts. “There is a high risk that the decision to arm the Syrian rebels will drag the United States into a more extensive involvement later,” she writes — and this is the very scenario that the advocates for intervention claim they are trying to avoid. The unique characteristics of alliances between states and armed nonstate groups — in particular “their informal nature and secrecy about the existence of the alliance or its specific provisions” — create conditions for states to become locked into unpalatable obligations. That seems especially likely in this case. The Obama administration, therefore, should not have decided to arm the Syrian rebels. Looking ahead, Borghard writes, it is important for policymakers to understand the nature of alliances between states and armed nonstate groups even after the Syrian conflict is resolved. “Given that Americans are unwilling to support large‐​scale interventions in far‐​flung reaches of the globe, policymakers looking for military solutions to political problems may conclude that arming proxy groups may be an attractive policy choice,” she concludes. They should instead, however, avoid committing to conflicts that don’t threaten core national security interests.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
It’s well known that the United States suffered severe drought and record high temperatures back in the 1930s, which we know resulted in the famous Dust Bowl and economic hardship across North America.
But now, from a climate point of view, that period has become an embarrassment to the scientists who propose the anthropogenic global warming theory. Atmospheric CO2 concentrations were much lower back then, and so according to their theory,  it should have been cooler than it is today. But it wasn’t.
But instead of questioning CO2’s role in driving global temperature, NASA GISS has decided to just rewrite the historical data so that it fits their flakey theory. This of course is scientific fraud.
Today we examine NASA GISS data for the Wellsboro station in Pennsylvania. First we look at the Version 4 “unadjusted” annual mean temperature data and compare it to the new Version 4 “homogenized” data:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data: NASA GISS
Above we see how NASA scientists simply went back in the old datasets and simply rewrote them so that the hot years of the early 20th century are cooled tremendously – by over two degrees in many years.
Earlier Wellsboro saw a cooling trend. But now since NASA scientists have fiddled with the data, the trend has been forged to fit the AGW theory.
Next is a chart comparing version 3, which starts in 1883 and ends in 2019, to Version 4, which starts in 1882:

Version 4 data source: NASA GISS, Version 3 here.
The earlier Version 3 also showed cooling before NASA rewrote the data and wiped it out. The current Version 4 used to show cooling, but was that too was tampered with and now it shows strong warming.
Many would argue that this is not science, but outright Orwellian scientific fraud.


		jQuery(document).ready(function(){
			jQuery('#dd_03feef225a932a34440c9fc287be9307').on('change', function() {
			  jQuery('#amount_03feef225a932a34440c9fc287be9307').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAs the globe has warmed since the end of the Little Ice Age, alarms concerning retreating glaciers have been sounded worldwide. The reason for the warming remains hotly disputed: alarmists blame it on manmade CO2 while skeptics say natural factors are just as much at play, if not more so.

Image: Norwegian glace, for illustration purpose. Source: NASA/John Sonntag, public domain.
Very little retreat in Norway this past summer
Yesterday Norwegian NRK here reported “several of the largest glaciers have almost not shrunk” during this past summer.
“This year, several places in the country have almost not shrunk,” according to the Norwegian NVE.
Since 1962 experts have been monitoring the Nigardsbreen glacier, an arm of Jostedalsbreen located in Vestland county.  The summer of 2020 has seen the sixth slowest result in about half a century. “If we get more such summers to come, then the glacier front will grow forward again,” says Even Loe in Statkraft.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The glacier is named after the farm Nigard, which was crushed by the glacier in 1748. At that time the front of the glacier stopped about 4.5 km further ahead than it is today,” reports the NRK.
Experts attribute this past summer’s stagnation to “a good winter with a lot of snow.”
“The Nigardsbreen glacier has actually grown bigger.”
Glaciologist Hallgeir Elvehøywhich said the glacier retreated 4 meters, “something that is very small compared to previous ones.”
“The trend is largely the same elsewhere in the country,” he says.
Although many glaciers have decreased relatively little this year, the Norwegian experts still remain pessimistic about their future, should warming continue as the models project. “But in all the gloom, there is also a small glimmer of light, should the rainfall continue.”
“There is nothing in the way that the climate system can give us several years with so much snow, and then it will have an effect.”


		jQuery(document).ready(function(){
			jQuery('#dd_d715d3c01e279f99b82ff2c7fb93a8ba').on('change', function() {
			  jQuery('#amount_d715d3c01e279f99b82ff2c7fb93a8ba').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Advocates for robust American global leadership are having a bad decade. Donald Trump’s election was clearly a wake‐​up call to the foreign policy establishment in Washington. In contrast to decades of bipartisan consensus that the United States was the “indispensable nation,” Trump appears to be monumentally indifferent to America’s role in the world.



His tense relations with longstanding allies and his decisions to withdraw from the Trans‐​Pacific Partnership and the Paris climate treaty have moved critics like former national security adviser Susan Rice to argue that Trump is “undoing American leadership on the international stage.”



Fears about Trump, however, simply echo concerns voiced throughout the Obama administration. Critics point to Obama’s withdrawal from Iraq, his failure to intervene in the Syrian civil war, and his failure to check Russia over Crimea and Ukraine as evidence of unhealthy retrenchment resulting in “the desperation of our allies and the glee of our enemies.”





The United States, and the world, would be better off if America led less often and more thoughtfully.



The real issue, however, is not America’s failure to lead; it is the failure of American leadership. Since the end of the Cold War the United States has flexed its muscles repeatedly. The problem is that this has too often produced resentment, conflict and instability, precisely the opposite of what its proponents have promised. The fundamental reason for this failure is that American officials have too much faith in their power to dictate outcomes around the world, especially through the use of military force.



The past 15 years provide ample evidence of the perils of leadership. After the 9/11 attacks, the Bush administration launched a war on terrorism based on a strategy combining military intervention, regime change and nation building. The goal was to kill terrorists in the short run, destroy their organizations in the medium run, and over the long run to reshape the politics of nations to prevent terrorism from sprouting up in the first place.



The Obama administration mostly followed suit, scaling back in Iraq but pursuing regime change in Libya, surging in Afghanistan and expanding the drone war against terrorists in seven countries. Today the Trump administration has begun to escalate the fight against the Islamic State and al‐​Qaeda, empowering the Pentagon and the military to determine troop levels and make swifter battlefield decisions.



The problem in the Middle East hasn’t been the lack of leadership; the problem has been the failure to recognize that the American strategy has been a failure. Political leaders exaggerated the terrorist threat to the United States and then applied the wrong tools to the problem. Military intervention turned out to be great for getting rid of governments, but completely ineffectual at defeating terrorist organizations.



Since 2001 the number of terrorist groups and jihadist fighters has skyrocketed, al‐​Qaeda franchises continue to operate, and the invasion of Iraq inadvertently caused the chaos that helped the Islamic State take root. Everywhere the United States has intervened — whether by drone or by invasion — since 2001, in fact, is less stable and more violent today than it was before.



Nor has the nation‐​building game gone any better. The United States has spent billions of dollars on nation‐​building efforts in Iraq and Afghanistan rebuilding infrastructure, training the police and military troops, and providing internal defense against terrorists. The hard truth, however, is that neither country is a functioning democracy, neither is stable, and neither would last long without outside support.



Meanwhile, the failure of the war on terror has come with astronomical costs, both for the United States and for the Middle East. The United States has already spent trillions of dollars and seen 7,000 Americans killed in the fighting, while according to NGOs somewhere between 1.3 million and 2 million Iraqis, Afghans and Pakistanis have died. This doesn’t count those in Libya, Yemen, Syria or elsewhere whose deaths are a result of U.S. intervention and its consequences.



Sadly, despite this recent history, there is little sign that Washington is ready to recognize the limits of American leadership. Though the Trump administration may frustrate the foreign policy establishment on certain issues, it is clear that American reliance on military intervention in the Middle East is here to stay.



American leadership can indeed be a powerful influence for good, but the United States is neither all‐​powerful nor faultless. The United States, and the world, would be better off if America led less often and more thoughtfully.
"
"
Share this...FacebookTwitterThe Kyushu region of Japan has been getting lots of rain lately, which has led to flooding and 62 reported deaths because local officials failed to properly heed warnings to evacuate.
The New York Times, however, blames it all on the “collision” of “demographic change and global warming” instead of incompetence by local authorities.
NYT claims: “More torrential rains”
“In recent years, climate change has spurred more torrential rains in Japan, causing deadly flooding and mudslides in a nation with many rivers and mountains,” reports Motoko Rich of the New York Times (NYT).
Unfortunately the NYT neither provides data nor cites any study showing this to be the case.
So we look at Japan precipitation trends going back decades – something the NYT journalists obviously neglected to do themselves (or they did, but then decided not to bring it up).
JMA data tell a whole different story
Using data from the Japan Meteorological Agency (JMA), Japanese blogger Kirye first plotted the annual precipitation anomaly for Japan since 1898:

Data source: here
The plotted data above show how Japan’s precipitation has indeed trended downward somewhat since 1898, and not risen.
The most recent decade in Japan has seen precipitation levels similar to that of the 1950s, and the very early part of the 20th century. Nothing unusual is happening here.
Next we look at the JMA annual precipitation data for Hitoyoshi, Kumamoto Prefecture itself since 1948:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: here
Here we observe very little trend change at all. The recent rainfall in the region over the past decade has been similar to that seen in the 1950s. Extremes are no more intense or frequent today than they were in the past.
No July precipitation trend changes
Going a step further into greater detail, we look at the southern Japan region’s rainfall for July, going back to 1948.

Data source: here
Also the here we have the same story: no major trend changes to speak of. Very wet July months in fact were just as frequent and more intense in 1950s.
Decreasing hot days
Finally we look at the region’s number of days recording a temperature of 30°C or higher.

Data source here
Here as well the REAL trend is in fact bucking what the NYT always likes to suggest: increasingly more hot days because of global warming.
Incompetent authorities after all
Later in the article, Rich does ultimately get around to the real cause of the recent flooding deaths in Kyushu: “The Japanese government issues standardized evacuation protocols, but they do not take into account the unique characteristics or terrain in different parts of the country, said Professor Tsukahara of Kyushu University.”
But all in all, very shoddy journalism here by the New York Times. They would do their readers a service by checking the data instead of lazily repeating old, exaggerated narratives.


		jQuery(document).ready(function(){
			jQuery('#dd_50a36dd65648004a6024adc66f4c2a06').on('change', function() {
			  jQuery('#amount_50a36dd65648004a6024adc66f4c2a06').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOur friend “SnowFan” here looks at the claims that September 2020 was the warmest ever recorded. It turns out that other measurement advanced satellites don’t agree.
According to the much ballyhooed data, temperatures in Europe in September this year were on average 0.2 degrees Celsius higher than in the previous record September 2018. The service providing the data is part of the European earth observation program Copernicus.
But the satellite data from the UAH and RSS both agree that this is not really the case!

Above the global satellite data from UAH (left) and from RSS (right) in the tables clearly clearly show the monthly deviations from the WMO mean 1981-2010 (UAH) and from the climate mean 1979-1998 (RSS): September 2020 was not the warmest since satellite measurements began in 1979. At UAH, September 2019 was slightly warmer while at RSS even September 2017 was warmer.
Strong La Nina may be in the works


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Overall the globe’s surface continues to cool since the peak of the 2015/16 El Nino, and that cooling will very likely continue if NASA and the US National Weather Service projections are correct. Both Agencies see a significant La Nina in the pipeline for 2021.

The current ENSO forecasts of NASA (left) and NOAA (right) from October 6, 2020, predict an unusually strong La Niña in the equatorial Pacific with temperature deviations down to -3°C and with unusually long duration until the NH summer of 2021.
Such a strong event would certainly lead remarkable surface cooling. Source: BOM ENSO models with additions.


 


		jQuery(document).ready(function(){
			jQuery('#dd_e2c39a96e5018d087a7844243e936550').on('change', function() {
			  jQuery('#amount_e2c39a96e5018d087a7844243e936550').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"The equivalent of one truck of plastic waste is dumped into the ocean every minute, but what if it could be caught and removed before it drifted out to sea? One such solution, called the Recycled Park Project, is being floated in Rotterdam. Developed over the last five years, the idea is turning plastic waste into islands. The Recycled Island Foundation and the WHIM Architecture firm launched the Recycled Park Project in 2014 with the aim of catching plastic waste in Rotterdam’s New Meuse river before it enters the North Sea. Three floating litter traps with nets attached collect litter in the water while volunteers sweep the riverbank. The retrieved plastic is converted into hexagonal building blocks which have been used to build a floating island park in the river itself. The park is open to the community and filled with plants and benches, giving people a new green habitat to enjoy in the heart of the city. A 140 square metre prototype was opened to the public in July 2018. It’s hoped that five more plastic litter traps can be added to the river, creating an island of at least 190 square metres. If successful, similar islands could be built worldwide, with research ongoing in Indonesia. The River Meuse carries a huge amount of plastic waste which is exposed after high tide on the river banks. By removing plastic from the river, the more costly and difficult job of removing it from the North Sea is avoided. Despite the obvious benefits, however, retrieving plastic waste from the river is technically illegal. Waste in the New Meuse River still legally belongs to whoever discarded it, as EU law states that waste may not be abandoned and littering is a form of abandonment. So, the taking and using of this waste in theory amounts to stealing from its most recent owner. But since there’s no way of identifying to whom the litter belongs and with no way of identifying ownership, it’s socially acceptable for anyone to take and use plastic waste in rivers. In order to encourage more reuse and recycling, we need to start treating waste as our communal property. A communal approach would encourage a sense of responsibility for waste and ensure that the community reaps the benefits of any solution, such as a floating park. So how would communal approaches work in practice? There would need to be a clear stage at which waste becomes communal property. This could be, for example, by making contents in specific bins free for people to take and use. For households in the UK, the contents of a bin belongs to the householder until it is removed by the local authority.  


      Read more:
      A rubbish idea: how blockchains could tackle the world's waste problem


 It’s not free for others to take and use it, either before, during or after collection, including by the employees of the waste management services. In one legal case, waste management workers for a corporation were convicted of stealing goods from bins collected in the course of their duties. Once waste is collected, it becomes the property of the local authority responsible for managing the waste. Communal bins would provide an additional benefit if combined with rules that require the contents to stay within the community. Local dumps already exist, but much of this waste is currently being taken elsewhere for landfill or exported to other countries.  If at least some waste were to stay permanently in the communities in which it’s created, people would be faced by the vast amounts of waste they produce and could see the benefits of producing less of it. The current system of privatising waste is failing to curb the pollution crisis, but with a communal approach, people would have the right to launch creative solutions of their own. We need the freedom to imagine different ways of living with the plastic our modern world accumulates."
"The first International Polar Year, held over 1882–1883, was an important event for science. The year was the brainchild of Austrian explorer Karl Weyprecht who, after a few years on different research missions, realised that scientists were missing the big picture by not sharing information with each other.  In 1875, at the annual meeting of German Scientists and Physicians in Graz, Austria, he proposed the setting up of an observational network of research stations to monitor the Arctic climate. It was the beginning of collaborative research in the region. Today, data collected 134 years ago on temperature, air pressure, or wind speed is still freely available.  There have been two more International Polar Year events since that inaugural one, most recently in 2007–2008, along with numerous other collaborative expeditions and research missions aimed at understanding aspects of Arctic biology, ecology, climate or geology.  But these co-ordinated efforts are the exception rather than the rule. Instead, most research locations north of the Arctic Circle have developed via a range of particular historical contingencies (easy access by boat or road, for instance, or a stable and open political climate), many of which have had nothing to do with scientific considerations.  Since the Arctic covers some 14.5m square kilometers, and conducting research in remote locations is very expensive, time consuming and often dangerous, the result is an extremely uneven concentration of research effort. The region is warming faster than anywhere else on Earth and its polar bears and melting glaciers have become a key symbols of climate change. But the Arctic, it seems, is not as well researched as we think it is.  We wanted to put some hard numbers behind this opinion, and to explore what these geographic gaps may mean in terms of broader scientific understanding. That’s what inspired our research project, carried out with colleagues and published earlier in 2018 in Nature Ecology & Evolution. We looked at 1,840 published studies across the Arctic dating back to 1951.  These studies covered nine broad disciplines in the natural and physical sciences and contained 6,246 sampling locations. We also looked at a total of 58,215 citations that refer to the studies. Citations of a particular study signify the amount of times it has been mentioned in other studies and is one indicator of importance within its discipline.  We found that a third of all study citations originate from sites within 50km of two research stations: Toolik Lake in Alaska and Abisko in Sweden. Our results show that for two important variables, temperature and vegetation density, the present pattern of sampling locations in the Arctic represents the average conditions well, but does not represent extreme conditions that are widespread across the region. The focus on Scandinavia and Alaska means that results from these sites are extended to other locations. The assumption that conditions in two well-studied sites are representative across the Arctic results in the under-sampling of more remote locations. These include vast regions that are relatively colder and warming more rapidly such as Russia’s northern coastline or the thousands of islands that make up Canada’s Arctic Archipelago.  It’s a substantial unknown. Although other parts of Canada and Russia are reasonably well sampled, they too have received considerably less citations, which leads to the poorer dissemination of the knowledge created in these studies. In this way, our understanding of the impact of climate change on the Arctic is biased in favour of sites that are well connected and well resourced.  It is important to note that we are not discounting the wealth of Arctic research conducted in Scandinavia and Alaska. Research at those sites has been, and continues to be, valuable and has enhanced our understanding of Arctic processes within their vicinity. For example, a wetland near Abisko has been studied for three decades during which time there has been an increase in methane emissions as the permafrost (frozen ground) thaws. Similarly, the discovery that the transport of carbon from land to water was much larger in tundra ecosystems than previously thought was uncovered by researchers at Toolik Lake back in the early 1990s.  Regardless of discipline, when looking for scientific evidence, it is habitual to cite research from well known and established locations. One way we could make our understanding of the Arctic more representative is to diversify our research citations by citing research that has been conducted in under-cited or under-sampled locations. Another way is to prioritise research at those under-sampled areas and (try to) convince funding agencies that research in those areas will fill gaps in our knowledge and result in more representative information about the Arctic. We hope our work will aid other scientists in making that point."
nan
nan
"

With the newspapers full of crises, it can be hard to maintain a proper perspective on the progress humanity has made, and to remember that there are individuals striving every day to make the world a better place. In a recent interview, businessman and philanthropist Bill Gates discussed the improving state of humanity, and the work that he is doing through private charity to help those in need. He said,   




I think the idea that people are worried about problems, like climate change or terrorism or these challenges of the future, that’s okay. But boy, they really lose perspective of what’s happened over the last few hundred years. And how science and innovation have been a central factor of that. And I think that’s too bad, because people are lucky to live now. And they should see that progress is actually taking place faster during their lives than at any time in history.



One of the major initiatives of the Gates Foundation, for example, aims to eliminate polio. The data bear out how much progress has already been made towards that end:   






In 1980, about half of all children received the polio vaccine. Today, around 90% of children receive the vaccine, and eradication of the condition is in sight – just as people eradicated smallpox in 1979.   
  
  
Gates is also among the many caring individuals working to eliminate malaria and malnutrition, areas where humanity has already made great strides. Insecticide‐​treated mosquito nets, for example, protect more children from malaria in Sub‐​Saharan Africa:   






Malnutrition among children is also declining. In populous developing regions, such as East Asia and the Pacific, malnutrition affected about 20% of children in 1990. More must be done, but today malnutrition affects fewer than 6% of children in those areas.   






Even one child afflicted by polio, malaria, or malnutrition is too many, but the dramatic improvements the world has made on these fronts should be celebrated. Like Gates, while working to make the world better we must not lose a proper perspective on the progress humankind has already made.
"
"
NOTE: Earlier today I posted a paper from Joe D’Aleo on how he has found strong correlations between the oceans multidecadal oscillations, PDO and AMO, and surface temperature, followed by finding no strong correlation between CO2 and surface temperatures. See that article here:
Warming Trend: PDO And Solar Correlate Better Than CO2
Now within hours of that, Roy Spencer of the National Space Science and Technology Center at University of Alabama, Huntsville,  sends me and others this paper where he postulates that the ocean may be the main driver of CO2. 
In the flurry of emails that followed, Joe D’Aleo provided this graph of CO2 variations correlated by El Nino/La Nina /Volcanic event years which is relevant to the discussion. Additionally for my laymen readers, a graph of CO2 solubility in water versus temperature is also relevant and both are shown below:
   
Click for full size images
Additionally, I’d like to point out that former California State Climatologist Jim Goodridge posted a short essay on this blog, Atmospheric Carbon Dioxide Variation, that postulated something similar. 
UPDATE: This from Roy on Monday 1/28/08 see new post on C12 to C13 ratio here
I want to (1) clarify the major point of my post, and (2) report some new (C13/C12 isotope) results:
1.  The interannual relationship between SST and dCO2/dt is more than enough to explain the long term increase in CO2 since 1958.  I’m not claiming that ALL of the Mauna Loa increase is all natural…some of it HAS to be anthropogenic…. but this evidence suggests that SST-related effects could be a big part of the CO2 increase.
2.  NEW RESULTS: I’ve been analyzing the C13/C12 ratio data from Mauna Loa.  Just as others have found, the decrease in that ratio with time (over the 1990-2005 period anyway) is almost exactly what is expected from the depleted C13 source of fossil fuels.  But guess what? If you detrend the data, then the annual cycle and interannual variability shows the EXACT SAME SIGNATURE.  So, how can decreasing C13/C12 ratio be the signal of HUMAN emissions, when the NATURAL emissions have the same signal???
-Roy
Here is Roy Spencer’s essay, without any editing or commentary:

Atmospheric CO2 Increases:
Could the Ocean, Rather Than Mankind, Be the Reason?
by
Roy W. Spencer
1/25/2008


            This is probably the most provocative hypothesis I have ever (and will ever) advance:  The long-term increases in carbon dioxide concentration that have been observed at Mauna Loa since 1958 could be driven more than by the ocean than by mankind’s burning of fossil fuels.
            Most, if not all, experts in the global carbon cycle will at this point think I am totally off my rocker.  Not being an expert in the global carbon cycle, I am admittedly sticking my neck out here.  But, at a minimum, the results I will show make for a fascinating story – even if my hypothesis is wrong.  While the evidence I will show is admittedly empirical, I believe that a physically based case can be made to support it.
            But first, some acknowledgements. Even though I have been playing with the CO2 and global temperature data for about a year, it was the persistent queries from a Canadian engineer, Allan MacRae, who made me recently revisit this issue in more detail.  Also, the writings of Tom V. Segalstad, a Norwegian geochemist, were also a source of information and ideas about the carbon cycle.

            First, let’s start with what everyone knows: that atmospheric carbon dioxide concentrations, and global-averaged surface temperature, have risen since the Mauna Loa CO2 record began.  These are illustrated in the next two figures.

 

Both are on the increase, an empirical observation that is qualitatively consistent with the “consensus” view that increasing anthropogenic CO2 emissions are causing the warming.  Note also that they both have a “bend” in them that looks similar, which might also lead one to speculate that there is a physical connection between them.
Now, let’s ask: “What is the empirical evidence that CO2 is driving surface temperature, and not the other way around?”  If we ask that question, then we are no longer trying to explain the change in temperature with time (a heat budget issue), but instead we are dealing with what is causing the change in CO2 concentration with time (a carbon budget issue).  The distinction is important.  In mathematical terms, we need to analyze the sources and sinks contributing to dCO2/dt, not dT/dt.
So, let us look at the yearly CO2 input into the atmosphere based upon the Mauna Loa record, that is, the change in CO2 concentration with time (Fig. 3).

Here I have expressed the Mauna Loa CO2 concentration changes in million metric tons of carbon (mmtC) per year so that they can be compared to the human emissions, also shown in the graph.
Now, compare the surface temperature variations in Fig. 2 with the Mauna Loa-derived carbon emissions in Fig. 3.  They look pretty similar, don’t they?  In fact, the CO2 changes look a lot more like the temperature changes than the human emissions do.  The large interannual fluctuations in Mauna Loa-derived CO2 “emissions” roughly coincide with El Nino and La Nina events, which are also periods of globally-averaged warmth and coolness, respectively.  I’ll address the lag between them soon. 
Of some additional interest is the 1992 event.  In that case, cooling from Mt. Pinatubo has caused the surface cooling, and it coincides in a dip in the CO2 change rate at Mauna Loa.
These results beg the question: are surface temperature variations a surrogate for changes in CO2 sources and/or sinks?
First, let’s look at the strength of the trends in temperature and CO2-inferred “emissions”.  If we compare the slopes of the regression lines in Figs. 2 and 3, we get an increase of about 4300 mmt of carbon at Mauna Loa for every degree C. of surface warming.  Please remember that ratio (4,300 mmtC/deg. C), because we are now going to look at the same relationship for the interannual variability seen in Figs. 2 and 3.
In Fig. 4 I have detrended the time series in Figs. 2 and 3, and plotted the residuals against each other.  We see that the interannual temperature-versus-Mauna Loa-inferred emissions relationship has a regression slope of about 5,100 mmtC/deg. C. 
There is little evidence of any time lag between the two time series, give or take a couple of months.

So, what does this all show?  A comparison of the two slope relationships (5100 mmtC/yr for interannual variability, versus 4,700 mmtC/yr for the trends) shows, at least empirically, that whatever mechanism is causing El Nino and La Nina to modulate CO2 concentrations in the atmosphere is more than strong enough to explain the long-term increase in CO2 concentration at Mauna Loa.  So, at least based upon this empirical evidence, invoking mankind’s CO2 emissions is not even necessary. (I will address how this might happen physically, below).
In fact, if we look at several different temperature averaging areas (global, N. H. land, N.H. ocean, N.H. land + ocean, and S.H. ocean), the highest correlation occurs for the Southern Hemisphere ocean , and with a larger regression slope of 7,100 mmtC/deg. C.  This suggests that the oceans, rather than land, could be the main driver of the interannual fluctuations in CO2 emissions that are being picked up at Mauna Loa — especially the Southern Ocean.
Now, here’s where I’m really going to stick my neck out — into the mysterious discipline of the global carbon cycle.  My postulated physical explanation will involve both fast and slow processes of exchange of CO2 between the atmosphere and the surface. 
The evidence for rapid exchange of CO2 between the ocean and atmosphere comes from the fact that current carbon cycle flux estimates show that the annual CO2 exchange between surface and atmosphere amounts to 20% to 30% of the total amount in the atmosphere.  This means that most of the carbon in the atmosphere is recycled through the surface every five years or so.  From Segalstad’s writings, the rate of exchange could even be faster than this.  For instance, how do we know what the turbulent fluxes in and out of the wind-driven ocean are?  How would one measure such a thing locally, let alone globally?
Now, this globally averaged situation is made up of some regions emitting more CO2 than they absorb, and some regions absorbing more than they emit.  What if there is a region where there has been a long-term change in the net carbon flux that is at least as big as the human source? 
After all, the human source represents only 3% (or less) the size of the natural fluxes in and out of the surface.  This means that we would need to know the natural upward and downward fluxes to much better than 3% to say that humans are responsible for the current upward trend in atmospheric CO2.  Are measurements of the global carbon fluxes much better than 3% in accuracy??  I doubt it.
So, one possibility would be a long-term change in the El Nino / La Nina cycle, which would include fluctuations in the ocean upwelling areas off the west coasts of the continents.  Since these areas represent semi-direct connections to deep-ocean carbon storage, this could be one possible source of the extra carbon (or, maybe I should say a decreasing sink for atmospheric carbon?).   
Let’s say the oceans are producing an extra 1 unit of CO2, mankind is producing 1 unit, and nature is absorbing an extra 1.5 units.  Then we get the situation we have today, with CO2 rising at about 50% the rate of human emissions.
If nothing else, Fig. 3 illustrates how large the natural interannual changes in CO2 are compared to the human emissions.  In Fig. 5 we see that the yearly-average CO2 increase at Mauna Loa ends up being anywhere from 0% of the human source, to 130%.  
It seems to me that this is proof that natural net flux imbalances are at least as big as the human source.

Could the long-term increase in El Nino conditions observed in recent decades (and whatever change in the carbon budget of the ocean that entails) be more responsible for increasing CO2 concentrations than mankind?  At this point, I think that question is a valid one.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1663853',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"World leaders are gathering in Katowice, Poland, to negotiate the world’s response to climate change. The 24th Conference of the Parties (COP24) will last from December 3-14 and its primary aim is to reach agreement on how the Paris Agreement of 2015 will be implemented. In a year which saw record weather extremes and an extraordinary announcement from the UN that we have only 12 years to limit catastrophe, the need for meaningful progress has never been greater. To explain how the COP works and what it means for the fight against climate change, we asked our academic experts to share their views. The urgency to reach key milestones in the Paris Agreement and deal with climate change puts a lot of high expectations on COP24 – Federica Genovese, lecturer in government, University of Essex. Rulebook: this is the conference’s main goal – to establish consensus on how nations should implement the Paris Agreement and report their progress. Emissions targets: COP24 is expected to resolve how emissions will be regulated, although it’s unlikely that sanctions for countries failing to meet their targets will be agreed on. Finance: the rich countries need to find US$20 billion to fulfil their pledge of providing US$100 billion a year in funding to help poorer countries adapt to climate change by 2020. Agreeing when this will be paid is likely to be contentious.  Role of “big” states: the international political climate casts a long shadow over the talks. Domestic politics in the US, the UK, Russia and Brazil threaten to undermine climate change leadership among larger emitters at COP24.  How did we get here? 1997: Creation of Kyoto Protocol, which set binding emissions targets. It failed as the US did not ratify it. 2009: COP15 in Copenhagen failed to yield any agreement on binding commitments. 2013: COP19 in Warsaw failed to finalise any binding treaty. 2015: COP21 in Paris generated considerable optimism with agreement reached on a legally binding action plan. But two years later, US president Donald Trump announced his intention to withdraw the US from the Paris Agreement. We aren’t facing the end of the world as envisaged by many environmentalists in the late 1980s and early 1990s, but if we do nothing to mitigate climate change then billions of people will suffer – Mark Maslin, professor of Earth system science, University College London The world’s poorest and most vulnerable people are most at risk from the effects of climate change, with many having to migrate from sea level rise, crop failure and pollution. Sahia – a woman from Bangladesh – lost her home and her family’s livelihood. As global temperatures near 1.5°C above pre-industrial levels, the limit set in the 2015 Paris Agreement, scientists are increasingly anxious about how changes in the environment could work to accelerate the pace at which the rest of Earth is warming.  The Arctic is warming twice as fast as the rest of the planet and strange recent events here, such as heathland turning brown, could be a sign that previous natural stores of carbon are no longer working properly.  Methane released from Arctic permafrost and other rapid changes could take the matter of limiting greenhouse gas emissions out of our hands in the near future. A paper published in 2018 warned that runaway climate change could lead the planet into a “Hothouse Earth” state: A chain of self-reinforcing changes might potentially be initiated, eventually leading to very large climate warming and sea level rise – Richard Betts, professor of climatology, University of Exeter  Whatever is agreed at COP24 will be what is politically possible,
but experts urge us to bear in mind what the science demands to avoid the worst impacts of climate change and keeping global warming below or at 1.5°C. We’re failing to cut down our emissions, the technologies for NETs [Negative Emissions Technologies] don’t exist at any meaningful scale yet, and there are no political drivers in place to enforce their deployment. There is also a real risk of a dramatic rise in methane in the near future. COP24 will have to consider emergency plans – Hugh Hunt, reader in engineering, University of Cambridge. A more radical approach at COP24 could highlight the ample opportunity there is for slowing climate change by restoring habitats. For many countries, reforestation is a more immediate way to slash emissions and make society happier and healthier in the process. However, while the climate has changed radically since global warming was first declared a man-made phenomenon 30 years ago, international efforts to tackle it haven’t. Many experts argue that the involvement of commercial interests at COP24 limits what is possible for mitigating climate change. Ten years after the financial crisis, COP24 should not legitimise large financial investors as the architects of a transition where sustainability rhymes with profitability – Tomaso Ferrando, lecturer in law, University of Bristol Representatives from pension funds, asset managers and large banks will be lobbying world leaders to favour investments in infrastructure and energy production as part of the transition towards a low-carbon economy. Finance sector sees this transition as an opportunity to generate profit. If climate change is fought according to the rules of Wall Street, says Ferrando, people and projects will be supported only on the basis of whether they will make money. If COP24 can’t save us, what can? At COP24 environmental movements have an opportunity to use their platform to highlight the relationship between economic growth and environmental impact, and even to discuss radical alternative futures that are not dependent on a growth-based economy – Christine Corlet Walker, PhD researcher in ecological economics, University of Surrey To bring about radical action on the environment, many academics believe we need an equally radical social movement. They argue that protesters should seize the initiative to attack the root causes of climate change, such as economic growth. 2018 marks 30 years since climate change was first declared a man-made phenomenon, during a congressional committee in Washington DC. The testimony of NASA climatologist James Hansen was met with both concern and scepticism at the time, but the science is in: anthropogenic climate change is incontrovertible.   Climate change is happening and is being caused by humans. This is the academic consensus, backed by science. But for climate change deniers:  The 97% 97.5% of scientists who had published peer-reviewed research about climate change agreed with the consensus that global warming is human-caused (2010 study from Princeton University). 97.1% of relevant climate papers published over 21 years affirmed human-caused global warming (2013 study involving multiple institutions).  97% consensus in published climate research found to be robust and consistent with other surveys of climate scientists and peer-reviewed studies (2016 study involving multiple institutions). What do the other 3% think? There is no consistent theme among the reasoning of the other 3%. Some say “there is no warming”, others suggest the sun, cosmic rays or the oceans as a reason. Why do some still not believe in human-caused climate change? The fossil fuel industry has spent many millions of dollars on confusing the public about climate change. But the role of vested interests in climate science denial is only half the picture. The other significant player is political ideology – John Cook, research fellow in climate change communication, George Mason University An analysis by American professor Robert Brulle found that from 2003 to 2010, organisations promoting climate misinformation received more than US$900m of corporate funding a year. From 2008, funding through untraceable donor networks (so-called “dark money ATM”) increased. This allowed corporations to fund climate science denial while hiding their support. In 2016, an analysis of more than 40,000 texts from contrarian sources by Justin Farrell, another American professor, found that organisations who received corporate funding published more climate misinformation. At an individual level, however, there is considerable evidence that shows that political ideology is the biggest predictor of climate science denial. People who fear the solutions to climate change, such as increased regulation of industry, are more likely to deny that there is a problem in the first place. Consequently, groups promoting political ideology that opposes market regulation have been prolific sources of misinformation about climate change, as three American academics found.  Five Techniques used by climate change deniers to look out for: Fake experts: create the general impression of an ongoing debate by casting doubt on scientific consensus.  Logical fallacies: logically false arguments that lead to an invalid conclusion. These usually appear in myths, in the form of science misrepresentation or oversimplification. Impossible expectations: demand unrealistic standards of proof before acting on the science. Any uncertainty is highlighted to question the consensus. Cherry-picking: best described as wilfully ignoring a mountain of inconvenient evidence in favour of a small molehill that serves a desired purpose. Myth of global warming ‘pause’ Degrowth is the radical future the UK needs A plastics treaty could clean up our oceans"
"

The 11th biennial fiscal report card on the governors comes at a time when those leaders have struggled with a sluggish recovery, resulting budget deficits, unemployment, and other economic problems in their states. Many reform‐​minded governors elected in 2010 have championed tax reforms and spending restraint to get their states back on track. Other governors have expanded government with old‐​fashioned tax‐​andspend policies. In **“Fiscal Policy Report Card on America’s Governors: 2012”** (White Paper), Chris Edwards, director of tax policy studies at the Cato Institute, graded all of the states’ governors and awarded four of them A’s — Sam Brownback of Kansas, Rick Scott of Florida, Paul LePage of Maine, and Tom Corbett of Pennsylvania. Five governors were awarded F’s — Pat Quinn of Illinois, Dan Malloy of Connecticut, Mark Dayton of Minnesota, Neil Abercrombie of Hawaii, and Chris Gregoire of Washington. Edwards offers short analyses of each governor’s performance in addition to a letter grade. Many states are facing major fiscal problems in coming years. Rising debt and growing health and pension costs threaten tax increases down the road. At the same time, intense global economic competition makes it imperative that states improve their investment climates. To that end, some governors are pursuing broadbased tax reforms, such as cutting income tax rates and reducing property taxes on businesses. This report discusses those trends and examines the fiscal policy actions of each governor in an attempt to make their records more transparent.





**Amtrak’s Insatiable Appetite for Federal Funds**  
When Congress created Amtrak in 1970, passenger‐​rail advocates hoped that it would become an efficient and attractive mode of travel. But, in **“Stopping the Runaway Train: The Case for Privatizing Amtrak”** (Policy Analysis no. 712), Cato senior fellow Randal O’Toole shows that more than 40 years of operations have disappointed. Amtrak has become the highest‐​cost mode of intercity travel and remains an insignificant player in the nation’s transportation system. Nationally, average Amtrak fares are more than twice as much, per passenger mile, as airfares. Despite these high fares, perpassenger‐ mile subsidies to Amtrak are nearly 9 times as much as subsidies to airlines, and more than 20 times as much as those to driving. When fares and subsidies are combined, its costs per passenger mile are nearly four times as great as airline costs. “A close look at the data reveal that Amtrak has failed for two primary reasons,” O’Toole notes. First, passenger trains simply aren’t competitive in most markets. Second, government control of Amtrak has saddled it with numerous inefficiencies, including unsustainably expensive labor contracts. “No amount of reform will overcome the fundamental problem that, so long as Amtrak is politically funded, it will extend service to politically powerful states even if those states provide few riders,” O’Toole continues. The only real solution, he writes, is privatization. “Simple justice to Amtrak’s competitors as well as to taxpayers demands an end to those subsidies,” he concludes.



 **The Transparency Problem**  
“President Obama has committed to making his administration the most open and transparent in history,” the White​house​.gov website declared just minutes after the new president took office on January 20, 2009. Yet, it is now clear that government transparency has not improved materially since the beginning of President Obama’s administration. According to Jim Harper, Cato’s director of information policy studies, in **“Grading the Government’s Data Publication Practices”** (Policy Analysis no. 711), this is not due to lack of interest or effort. “Along with meeting political forces greater than his promises,” Harper writes, “the Obama transparency tailspin was a product of failure to apprehend what transparency is and how it is produced.” Starting from a low transparency baseline, this administration made extravagant promises and put significant effort into the project of government transparency. It has not been a success. House Republicans, who manage a far smaller segment of the government, started from a higher transparency baseline, made modest promises, and have taken limited steps to execute on those promises. President Obama lags behind House Republicans, but both have a long way to go. The solution, Harper argues, is to tackle the low‐​hanging fruit first. Establishing an authoritative list of programs and projects within the basic units of government, for instance, “is like creating a language, a simple but important language computers can use to assist Americans in their oversight of the federal government.” Harper goes on to lay out a variety of good data publication practices, which he insists can help to produce a more accountable, efficient, and responsive government.



 **The Race for Protection**  
The world is awash in trade‐​distorting subsidies. According to Scott Lincicome, a former employee of Cato’s Center for Trade Policy Studies and now an international trade attorney at White & Case, governments have adopted massive “stimulus” packages since the financial crisis of 2008 — handouts that have included taxpayer subsidies for various industries including agriculture, alternative energy, and automobiles. In **“Countervailing Calamity: How to Stop the Global Subsidies Race”** (Policy Analysis no. 710), Lincicome argues that these subsidies have, in turn, “distorted global markets, bred cronyism, and undermined free trade.” They have also encouraged copycat subsidization, which spawned an increase in litigation at the World Trade Organization and led to the frequent imposition of protectionist duties. Trade reform is badly needed. “Unfortunately,” Lincicome writes, “the U.S. government has little credibility on this issue: it is one of the world’s largest subsidizers, funneling billions of dollars annually to chosen industries, causing economic uncertainty, and creating breeding grounds for corruption.” Yet, ironically, with 59 currently active or pending national countervailing duty (CVD) measures affecting over $11 billion of imports, the U.S. government is also one of the most frequent users of antisubsidy disciplines. The only answer here, according to Lincicome, is true reform. By curtailing federal subsidies to favored industries and by changing CVD procedures to ensure that they serve the rule of international trade law — rather than protectionist objectives — “the U.S. government can reduce market distortions, restore some faith in free markets, and lead national and international subsidy reform initiatives,” he concludes.
"
"European polling on climate change denial puts Poland towards the top – or bottom, depending on which way you view it – of the leader board. Though the UN’s COP24 climate conference is currently being hosted in its southern city of Katowice, Poland itself has displayed little concern over global warming. Indeed, the EU’s largest coal producer often opposes any attempt to get it to cut its carbon emissions. What’s driving such potent scepticism? With minimal media coverage of climate change, climate impacts, or policy, Poland is an outlier in Europe. This is particularly surprising because EU climate policy, and the possibility of a stiff carbon tax in future, has significant long-term implications for the country’s economy. The issue partly dates back to the collapse of communism in Poland that began in 1989. The resulting industrial decline and overhaul of outdated and highly polluting sectors caused a rapid decrease in CO₂ emissions.  All this meant that when Poland joined the EU and signed up to the Kyoto Protocol it could easily meet its generous emissions targets as they were set relative to 1988, when its polluting industries were still in full swing. However, new EU climate and energy legislation will soon kick in and comparing Polish emissions with a more recent year will make things harder. In our academic research, we have looked at why there is so little coverage of climate issues in the Polish media.  In part, it’s a reflection of the prominence of climate deniers, both politicians and scientists, in the media. Many politicians in Poland have publicly announced scepticism, not only about climate policy but also about the scientific findings on climate change. It is also relatively easy for incompetent people to gain a sizeable platform. We found that, typically, denialist scientists featured in the Polish media are not climatologists, but rather medical scientists, geologists, economists, or engineers from the energy or mining sectors. There is also no publicly-owned media in Poland, except for public television and radio, which has been politicised by the coal-friendly ruling party. Commercial media competes for a small audience, and as a result is much more likely to touch on controversial points of view than to try to analyse them. There has also been little media coverage in Poland of the UN’s IPCC reports on the scientific consensus about climate change, and a complete absence of politicians promptly reacting to the reports. This lack of coverage can be partly explained by the relative scarcity of IPCC authors from Poland. But journalists and editors are unlikely to choose a topic that they know is of little interest to their audiences and it appears that many Poles believe the cure – climate change mitigation – could be worse for Poland than the disease. Much of this can be traced back to the influence of the coal lobby, which has been powerful ever since communist times when exports were a vital source of convertible (foreign) currency. Poland is today the largest coal producer in the EU, and around 60% of the country’s overall energy comes from coal. No wonder sentiment towards fossil fuel remains strong. This is why most Polish politicians will nominally support taking action on climate change, regardless of political orientation – but only as long as it does not mean moving away from coal. They frequently speak of the importance of coal to the economy and energy security, yet they conveniently ignore or downplay the coal-climate link. There is a strong “inconvenient truth” at work as burning coal is, globally, responsible for much of the ongoing climate change.  Even past increases in the price of domestic coal have not helped renewable energies, but rather resulted in Poland importing cheaper coal from abroad while propping up its own industry. Most Poles recognise the benefits of being in the EU and understand that Poland must play by the rules. Yet many other voices are demanding renegotiation of an EU climate and energy package that they say is harmful to their nation. Indeed, many perceived this an externally-driven policy problem, imposed from abroad. They expect more effective action from non-EU countries which emit most of the world’s carbon dioxide. Like other EU members, Poland would eventually like to decarbonise its energy sector. However, concerns remain over the abrupt introduction of a high carbon tax and the threat of “carbon leakage” where coal production and jobs shift eastwards from Poland to countries that are not obliged to reduce emissions under the Kyoto Protocol. For now, a switch from cheap coal to a more costly low-emission economy is politically unpalatable. Popular opinion and a wide range of politicians simply do not support the vision of leaving coal underground and paying more for energy. The country is still poorer than those in Western Europe, and the fear of energy price hikes is overwhelming. Don’t expect Poland or its media to embrace climate action any time soon."
nan
"

I’m taking a few days off blogging for friends and family. Seeing the pandemonium that has gripped the streets of my town while people scurry about trying to pull off that last minute shopping, and seeing my in-laws already getting into a small accident (even though they tried to avoid the traffic glut) and seeing local cycling enthusiast Ed McLaughlin get into a serious biking accident reminds me to remind you to drive, bike, and walk safely this holiday.
Watch for ice, it’s cold out there, don’t slip like the polar bear!
When I return I’ll have an update on my Stevenson Screen paint experiment.
In the meantime I wish each and every one of you a joyous Christmas holiday and I want to thank all of you for the help and support this year on this blog and in the www.surfacetstations.org project.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1cb16ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The past week saw two significant events in European science. You know about the first one: the triumphant Rosetta mission which landed a probe on a comet. But the other event was less publicised, and much less welcome. The European Commission has decided to scrap its chief scientific advisor (CSA) role. The current CSA, Professor Anne Glover, tweeted about the incredible achievement of the European Space Agency at the same time as her post was being axed.  I’m a chief scientific advisor myself, for the UK government’s Food Standards Agency, so I understand the demands of the role and the value of such advice. Professor Glover’s dignity and continued enthusiasm for European science highlights her integrity and professionalism, something she has brought to the role throughout her tenure. The removal of the CSA post was controversial. In the UK much of the media reaction framed it as an attack on science, or a triumph for the green campaigners who wrote to the EC about Glover’s support of genetically modified crops. But this oversimplifies the issues and doesn’t focus on what does now need addressing, in the wake of Juncker’s decision not to have a CSA – what now for science and evidence in European policy? Just days before the decision, I attended an event celebrating 50 years of UK government CSAs. Attendees heard about a battle during World War II between Lord Cherwell and Henry Tizard, both eminent scientists, over the effectiveness of bombing German civilians. Lord Cherwell said it was the best strategy, but Tizard thought Cherwell’s calculations were hopelessly optimistic – and Tizard turned out to be right. The politically favoured Cherwell had Curchill’s ear however, causing Tizard’s advice to be ignored, and Allied bombers were wasted on unproductive missions.  The government realised scientific advice must be politically independent. Eventually, the ad hoc system favoured by Churchill evolved into an official role that helped create clear blue water between science and politics. The position in Europe was created for similar reasons and thus it is strange how those opposed to it suggest the CSA was not fully capturing the breadth of scientific opinion on certain issues (especially GM crops), suggesting the green campaigners thought the role was political and aimed navigating between opposing views. Of course, this is precisely what it is not. CSAs do gather consensus and the certainty of the evidence – but only among scientists and the evidence they have generated. There needs to be a robust way to link the external world of science and scientific networks with the internal world of government and policy, which is why most UK government departments have a CSA. Of course, this is just one possible model and there are some objections – green campaigners fear too much power is concentrated with one person, for instance – but CSAs essentially work. They are an important and useful way to ensure the government looks at things through a scientific lens.   The Food Standards Agency is currently considering the safety of what some term “risky” foods such as raw drinking milk. As the CSA, I can offer the scientific evidence relating to such food: how to manage the risks, whether to regulate or offer information on best practice for producers and consumers. Of course, my authority only extends so far – I’m a chief adviser, not a chief executive. At the 50th anniversary meeting former CSA Lord May put it well: “Science frames a stage for democratic policy making. It must provide the evidence, not the answer”, he explained. “When a problem arises, it’s imperative to group the best available people to get knowledge and emphasise openness and uncertainties.” This, in a nutshell, is what a CSA is for, and it directly counters the arguments used by people who dislike the concept. The European Commission must address all sorts of complex questions from climate change to space exploration or disease control, and this means different bodies of evidence need to be linked by what Geoff Mulgan calls systems thinking – and this means an independent expert. People who draft policy look through many lenses and science is only one, albeit an important one. EC president Jean-Claude Juncker needs to quickly identify how he will have the clearest and most accessible advice to assist him in making decisions shaped and informed by science. He isn’t off to a great start – the commission CSA’s office will be replaced by the European Political Strategy Centre which has no scientific advisory role. Many of the attacks on Juncker’s decision to abolish the post go too far, but I do wonder and worry how he will receive the words of scientific wisdom in a trusted and consensual way. Europe deserves the best possible link between science and government."
"

Here’s something you don’t see every day; an exploding comet.  In fact the last time this comet did this was in 1892.
Comet 17P/Holmes is now larger than Jupiter. Astronomer Eric Allen of Quebec’s Observatoire du Cégep de Trois-Rivières combined images he captured on three consecutive nights (Oct. 25, 26 and 27) and placed them beside a picture of Jupiter scaled to the same distance as the comet, as shown above. More at www.spaceweather.com
The comet is visible to the naked eye, and looks even better through a telescope. This would be a good excuse to go visit the Chico Community Observatory in upper Bidwell Park Sunday night and have them swing the telescope by for a look. Or if you want to spot it yourself, here is a sky map.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea306c1e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The new Liberal MP Katie Allen has canvassed the need for the government to appoint a dedicated climate change minister, as the Coalition splits on how best to “evolve” its policy response. Allen, who was elected for the seat of Higgins in May, is one of a number of Liberal MPs who have been arguing internally for the government to increase its ambition on climate change against resistance from conservatives in the Coalition.  Allen’s suggestion for a climate change portfolio was made in a group chat of MPs in a discussion about how the government’s policies could be better communicated, sources have confirmed. She did not advocate in the group that the government needed to do more, but suggested there was frustration that voters were not hearing the government’s message. She argued that the “significant investment” being made by the government did not appear to be acknowledged in the community. Allen proposed that her fellow Victorian MP Tim Wilson would be a good advocate to communicate the government’s policies. Both Allen and Wilson declined to comment on the discussion. Wilson was one of the MPs who spoke in Tuesday’s partyroom in favour of more ambitious climate action and has also urged the government to consider nuclear power. His call for more action was backed up by Allen, along with the member for Reid, Fiona Martin, and the North Sydney MP, Trent Zimmerman, who argued that the government needed to ensure it respected divergent views on the issue, and acknowledged the strength of feeling for more action in inner city seats. Nationals MPs George Christensen, Barnaby Joyce and David Gillespie argued that the government should not succumb to pressure from inner city voters to do more on climate, with Joyce saying it was a “hobby horse” that was being pursued through the tragedy of bushfires. On Wednesday morning, following the partyroom stoush, Morrison said he would not “be bullied” into more action on climate change. “We listen to Australians right across the country. Not just in the inner city,” Morrison told Channel Nine. “It’s important to listen to everybody but take people forward on practical, balanced action that doesn’t go and write people’s jobs off, or industries off,” Morrison said. “It’s about technology, not taxation. So we won’t be bullied into higher taxes or higher electricity prices. What we’ll do is take practical action that deals with these challenges.” Morrison also dismissed overnight calls from the UK prime minister, Boris Johnson, for other countries to join Britain in striving for net-zero carbon emissions by 2050. “I would never make a commitment like that if I couldn’t tell the Australian people what it would cost them,” Morrison said. As the Coalition seeks to reposition on climate, Morrison faced questions in Parliament about the government’s response to the bushfires. In response to a question from the Warringah MP, Zali Steggall, asking for a bipartisan approach to climate change in the wake of the crisis, Morrison said Australia was already acting. “Emissions reduction is important. We’re acting on that reduction. I can tell you also resilience to climate is also important,” Morrison said. “Hazard reduction is important, if not more important, than emissions reduction when it comes to protecting people from fire and hotter, drier, longer summers in the future. “Also, in a country ravaged by drought, and the impacts that we have experienced, and that drought continues, building dams is climate action now.” The opposition leader, Anthony Albanese, asked Morrison why the government had not called a meeting of state governments when the bushfire crisis began – a request made in writing to the prime minister by Labor on 22 November. Morrison said the government had responded in an “unprecedented” manner to the fires, and it was “disappointing” that Labor was trying to “politicise the bushfires”. Earlier, Liberal MP John Alexander used his condolence speech on the bushfires to call climate change “the elephant in the room”. “Today is a day for commemoration, not politics. But one thing I would like to say is the need to recognise that these fires are not a warning about climate change – they are climate change,” he said. “The leader of the opposition said that ‘this is not normal’. I fear this is actually the new normal. “In focusing on saving this country for our grandchildren, we risk forgetting we need to save our neighbours. We must obviously mitigate future risks and change our ways, but we also must adapt because these longer, hotter summers will be our new normal.” The challenge facing Morrison to balance the Coalition’s competing constituencies was further highlighted on Wednesday, when the new Nationals party deputy leader, David Littleproud, argued that the government should look to retrofit coal-fired power stations to reduce emissions. Party leader Michael McCormack, who survived a leadership challenge on Tuesday, said the government was taking action on emissions reduction in a “responsible way”. “We don’t want to be sending industry offshore. And there is a future for coal in this country.” But despite the divergent views, home affairs minister Peter Dutton rejected suggestions the party was divided. “I don’t see huge points of difference in our party room,” he told the ABC. “Obviously, as we’ve all pointed out, we’re experiencing hotter weather, longer summers, but did the bushfires start in some of these regions because of climate change? No. It started because somebody lit a match. There are 250 people as I understand it, or more, that have been charged with arson. That’s not climate change.” There is no evidence that anything like that number of people have been arrested over alleged arson in connection with this summer’s bushfires, let alone charged. As the political debate over climate action continues to rage, the RBA governor, Philip Lowe, told the National Press Club that the cost to the economy of bushfires and drought saying they were “a stark reminder that the economic effects of these climate events are material”."
"

As the noted social critic H. L. Mencken once declared, “The whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



In _A Dangerous World? Threat Perception and U.S. National Security_ , a new book edited by Christopher A. Preble and John Mueller of the Cato Institute, a number of scholars ask to what degree the United States is threatened, examining not only the multiplicity of supposed dangers, but also the wisdom or folly behind the measures that have been proposed to deal with them. Paul Pillar, a visiting professor at Georgetown University’s Center for Security Studies, observes that “substate” threats — from terrorism to crime — pale in comparison to traditional threats posed by states. He urges readers to reexamine their preconceived notions of what is required to keep the United States both safe and free. Noting a disconnect between the severity of threats and how much alarm they generate, he voices dismay at the tendency to identify monsters abroad and “conceive of America’s place in the world largely as one of confrontation against them.” In the end, he cautions that the capacity of the United States to curb substate conflict is usually very limited. In fact, intervention itself can be counterproductive.



Eugene Gholz, associate professor of political science at the University of Texas at Austin, explores the economic effects of warfare. Although war itself has many highly undesirable effects, he finds that overseas tensions do not necessarily harm the U.S. economy. Daniel Drezner, professor of international politics at Tufts University, finds some merit in the global public good provided by overwhelming U.S. primacy. However, it is not obvious that military power is the primary driver of the benefit, while sustaining it comes at great cost.



Other contributors include Peter Andreas of Brown University on “Transnational Crime as a Security Threat,” Martin Libicki of the RAND Corporation on “Dealing with Cyberattacks,” and Mark G. Stewart of the University of Newcastle on “Climate Change and National Security.” Although the world will never be free from dangers, we should aspire to understand them clearly. By chipping away at the common perception that the world is getting more dangerous each day, the contributors to this volume attempt to tame the tendency to overreact.
"
"
Former Virginia State Climatologist Patrick J. Michaels wrote an op-ed about his paper with Ross McKitrick from Canada’s University of Guelph in an American Spectator column today about the surface temperature record. This paragraph really caught my eye: “Weather equipment is very high-maintenance. The standard temperature shelter is painted white. If the paint wears or discolors, the shelter absorbs more of the sun’s heat and the thermometer inside will read artificially high. But keeping temperature stations well painted probably isn’t the highest priority in a poor country.”
The Stevenson Screen experiment that I had setup this summer is living proof of this.
Compare the photo of the whitewash paint screen on 7/13/07 when it was new with one taken today on 12/27/07. No wonder the NWS dumped whitewash as the spec in the 70’s in favor of latex paint. Notice that the Latex painted shelter still looks good today while the Whitewashed shelter is already deteriorating.

Click image for larger view

Click image for larger view

Whitewashed Screen on 7/13/07

Whitewashed Screen on 12/27/07
The whitewash coating I used was from a formula and method provided to me by a chemist at the US Lime Corporation, who is an expert on whitewash. He said the formula was true to historical records of the time when whitewash was used on the shelters. I was amazed to find that after just a few short months, my whitewash coating had lost about 40-50% of it’s surface area. Perhaps there was a mistake in the formula, or perhaps whitewash really is this bad at withstanding weathering.
In any event the statement of Patrick Michaels “Weather equipment is very high-maintenance. The standard temperature shelter is painted white. If the paint wears or discolors, the shelter absorbs more of the sun’s heat and the thermometer inside will read artificially high.” seems like a realistic statement in light of the photos above. The magnitude of the effect in the surface temperature record has yet to be determined, but it seems clear that shelter maintenance, or lack thereof, is a significant micro-site bias factor that has not been adequately investigated nor accounted for in the historical temperature record.
I’ll have more on this experiment soon including temperature time series graphs showing the difference between bare wood, latex painted, and whitewashed shelters.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1bd3f8a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Note: I don’t normally allow the discussion of things related to Nazi Germany here, including discouraging the use of the word “denier” due to it’s “Holocaust Denier” connotations. But this full page ad in the Sunday papers in Britain, touting “climate crime” and “climate cops” is just a bit over the top, and deserves some attention. It is particularly relevant since the sponsoring website climatecops.com has a teachers section, and we’ve just seen some sensibility from Schwarzenegger in Sacramento on this very issue. I find this method of indoctrinating school children to normal everyday living being harmful to the earth with the “climate crime” connotation as distasteful and wrong headed. I have no problems with energy conservation, in fact I encourage it. But combining  such advice with a “climate cop” idea is the wrong way to get the message across. Can you imagine what sort of reaction the neighbors will have to the kids hanging this door hanger on their front door? Will the result of this now be hiding your electric dryer behind false walls so the kids and neighbors don’t see it?
At the very least, npower could have chosen a different color scheme: red, black and white are the same three colors used in the flag of Nazi Germany What were they thinking? – Anthony
Reposted from the website EU referendum:

Can I be the only one more than a little disturbed by the latest campaign to be fronted by energy company npower?
Launched today with large colour ads in the Sundays, it appeals directly to children, urging them to enlist as “climate cops”, to root out “climate crimes“, and thus “save the planet”.
In a luridly-designed website, mimicking the style of “yoof” cartoons, it offers a bundle of downloads, including a pack of “climate crime cards“, urging its recruits to spy on families, friends and relatives, inviting each of them to build up a “climate crime case file” in order to help them ensure their putative criminals do not “commit those crimes again (or else)!”
Quite what the “or else!” should be is not specified, but since the “climate cops” are being encouraged to keep detailed written records (for those who can read and write), there is nothing to stop these being submitted to the “Climate Cops HQ” for further sanctions, the repeat offenders being sent to re-education camps. And for those “climate cops” that successfully perform the “missions” set (or turn in their own parents), there is the reward of “training” in the “Climate Cop Academy”.
In a system which has echoes of Hitler’s Deutsches Jungvolk movement, and the Communist regime Pioneers, perhaps successful graduates can work up to becoming block wardens, then street and district “climate crime Führers”, building a network of spies and informers.
How nicely this ties in with James Hansen’s call to put the chief executives of large fossil fuel companies on trial for high crimes against humanity and nature, accusing them of actively spreading doubt about global warming.
No doubt, with a willing band of “climate cops”, the prosecutors can spread their nets wider, reaching into the homes of all climate change deniers, until the insidious virus of doubt is exterminated (final solution, anyone?). Then we can all march on the sunlit uplands of a “carbon-free” planet – to the tune of Ode to Joy no doubt.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9df59f46',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFrom environmental heroes – to national shame
The whole publicity stunt was probably supposed to go something like as follows: To protest against forest clearance to make way for a new stretch of autobahn, a group of 9 masked German tree-huggers would rappel from a speedway overpass and hang a banner demanding that the planned deforestation be stopped. And for their courageous activism, they’d surely make the regional news – maybe even the national news – and draw needed attention to man’s ruthless destruction of nature. Of course they’d be adored by the public as environmental heroes. They’d maybe even hold press conferences – and be surrounded by TV cameras and mikes.
Except for the national attention, things didn’t quite work out that way for the group of German radical environmentalist tree-huggers after tragedy struck.
According German daily Bild here, they ended up causing an 8-kilometer traffic jam and one “horror accident” as a 29-year old driver suffered serious injuries and had to be airlifted to a trauma center.
One German national daily labelled the activists as environmental idiots.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image cropped from Bild online here: 
According to Bild, the “anti-autobahn activists” blocked the A3 autobahn near Idstein as two of them “rappelled off an overpass” and hung a banner across it. Their publicity stunt caused the fast moving traffic to jam to a sudden halt, causing an 8-kilomter traffic jam. The 29-year old driver mentioned above failed to noticed the end of the traffic jam in time and crashed into the rear of a truck.
“A 29-year-old drove into the end of the traffic jam and his Skoda crashed into the back of a truck – the man was seriously injured,” online Bild reports.
“Horror crash because of this idiot,” was the headline in Bild’s hard copy print edition this morning, with an arrow pointing at a young female activist hanging from the overpass. Another Bild photo showed authorities dragging off one of the protesters.
The online Bild here also reports that one protester is still being detained by the police. But: “Six have been released.”
“A protest that endangers human life has no legitimacy,” said Home Secretary of Hesse, Peter Beuth. “Anyone who endangers his fellow citizen has to be punished severely.”


		jQuery(document).ready(function(){
			jQuery('#dd_0755afe815319cbec0ef6652a11c042f').on('change', function() {
			  jQuery('#amount_0755afe815319cbec0ef6652a11c042f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
It’s been awhile since I updated this series, and its not for lack of material. But I got busy with the UCAR conference, publishing a slide show, and other things. But this morning, über volunteer Don Kostuch sent me a note on his latest survey in Titusville, FL near Cape Canaveral and KSC. I’d like to point out that Don has traveled further and surveyed more stations in the USA than anyone. He is a surveying machine. He wrote this in his email to me:
  “On your scale of 1 to 5, this is an 8. Peace, Don Kostuch”
Ok in the past we have seen stations on rooftops, at sewage treatment plants, over concrete, next to air conditioners, next to diesel generators, with nearby parking, excessive nighttime humidity, and at non-standard observing heights.
Imagine a USHCN station that embraces all of that. I give you the Titusville, FL USHCN station:



Ever thorough, Don also provided photographs of the Climate Reference Network site, just 7 miles east at KSC, which demonstrates the correct environment for measurement of near surface air temperature:

Now I know there will be the usual critics who will jump in and say “This can be adjusted for!”. Ok here is your chance, show me the equations to untangle Titusville’s temperature record from microsite bias. Personally, it looks FUBAR to me.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3cc7da6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Ask people to describe what they associate with butterflies, and you will probably get an image of a sunny summer’s day, with a beautiful peacock drifting gently on the cooling breeze.  Ask the same question but for moths, and you are more likely to be told about holes in a favourite woollen jumper, or something small and brown beating itself to death against a bathroom light fitting. We don’t view moths with the same affection as their day-flying cousins, and the irrational fear of moths is even common enough to have a name: mottephobia. To my mind, this is unfair. Let’s start with the clothes-eating accusation. According to the charity Butterfly Conservation (which also protects moths), there are around 2,500 species of moths found in the UK; of these, only two will attack clothes. They prefer dirty items in undisturbed places: that means that if your jumper has been attacked, you probably weren’t wearing it often enough anyway.  In fact, there are also moth species that are crop pests, such as the diamondback moth, and some which produce irritant hairs as caterpillars – the non-native oak processionary moth currently infesting London is one such example. But again, these examples are very much in the minority among the total diversity of moths, and there are very many more reasons to love moths than to hate them. Many moths are just as beautiful as butterflies; some even look like butterflies! A particular favourite of mine is the brimstone moth: exactly like the brimstone butterfly, it is so-called for its vivid sulphur-yellow colour. Some are superbly camouflaged, like the buff-tip – easily mistaken for a broken twig of silver birch. And some have attractive names to match their appearance, such as the fabulous Merveille du Jour (“marvel of the day”), pictured at the top of this article. But beyond their beauty, moths also perform vital roles in the natural communities to which they belong. Along with colleagues, I recently examined the scientific literature to establish how important moths are to flowering plants as providers of pollination services. Most moth species that feed as adults do so by drinking nectar, and in doing so accidentally carry pollen between flowers.  We found examples of moths serving as important pollinators across many important habitats and in every continent except Antarctica. Several studies suggested that moths were the second most important pollinators in the area surveyed, behind only bees. In the UK, moths might be pollinating wildflowers near you including honeysuckle, bramble, white campion, wild carrot, thistle and ragwort. In North America, moths pollinate many types of cactus, and milkweed (beloved of caterpillars of the monarch butterfly). Globally, they are also well-known pollinators of many orchid and lily species. If you’re a bird or a bat, a moth makes for a tasty treat. Researchers from the University of Bristol have examined the diet of two species of long-eared bat in England, revealing that their main food source was large-bodied moths from the Noctuidae (or owlet moth) family. Whereas bats (and the eerie nightjar) eat adult moths, moth caterpillars provide a vital food source in the spring for young birds including blue tits, which can munch their way through 35 billion caterpillars in the UK every year.  Especially important in this regard are those moth species that are active as adults through the winter, including the aptly named winter moth; these species reach the caterpillar stage of their life cycle at exactly the right time of year for busy blue tit parents to exploit. And did you know that one species of moth has been domesticated into a valuable commercial livestock species, at the centre of a multi-billion dollar global industry? When spinning a cocoon in which to undergo the transformation to an adult moth, the caterpillars of the domesticated silkmoth Bombyx mori produce lustrous threads several hundred metres in length. These threads, once unravelled, are spun into the fine textile silk. And if none of this has convinced you, know this: butterflies are actually just a minor subset of moths!"
"Will Boris Johnson please listen to his own father, rather than Jeremy Corbyn’s climate sceptic brother, on the subject of climate change? It may go against the prime minister’s instincts, but it is the best hope for Britain to live up to its responsibilities in a crucial year for our species. Johnson cannot do this on his own. That much was clear this week during the shambolic London launch of the COP 26 UN climate summit, which will take place in Glasgow in November. This will be the most important international conference in five years and as host the UK will play a leading role in deciding whether it ends in success or failure.  The event was overshadowed by the sacking of the UK’s COP 26 president Claire O’Neill, who responded with a scathing takedown of a premier who she described as “not getting” climate change, failing to devote sufficient resources to summit preparations, and being fundamentally untrustworthy. The firing was not all bad news. O’Neill lacked the political heft and diplomatic skill needed to forge a global consensus on this most divisive of issues. If a more senior politician is appointed in her place, it would show the UK is taking preparations more seriously. The fact that the prime minister is fronting the launch demonstrates an investment of political capital that is commensurate with the challenge. He also said many of the right things, most concretely by confirming the UK’s commitment to go net-zero on carbon emissions by 2050. “Of course it’s expensive, of course it’s difficult, it will require thought and change and action, people will say it’s impossible and it can’t be done, and my message to you all this morning is that they are wrong,” he declared. “I hope it will be a defining year of action for our country, and indeed for our planet, on tackling climate change but also on protecting the natural world.” But can he be trusted and can he deliver? On that question, almost all the evidence – from parliamentary voting records and donations from climate sceptics to his newspaper columns and performance as mayor – says no. Although Johnson has repeatedly called for reduced emissions, he scored zero out of 100 in the Guardian’s climate scorecards last year based on his rejection of actions that would achieve this in five key parliamentary votes. This includes opposition to onshore wind subsidies, emissions-based vehicle taxes and carbon capture and storage. He was also shown to have declared donations of £5,000 from Michael Hintze and £25,000 from Terence Mordaunt (via First Corporate Shipping), who fund the climate science sceptic Global Warming Policy Foundation. The Conservative party says the Guardian cherry-picked votes and failed to recognise Johnson’s support for the net zero emissions target. The Guardian stands by its methodology, which can be read here. Other broader data sources confirm the prime minister’s woeful record on parliament. The parliamentary watchdog They Work for You notes that Johnson has “almost always voted against measures to prevent climate change” with one vote in favour, eight against and six absences (the latter largely due to his period as mayor of London). The gulf between words and deeds was also evident when Johnson was foreign minister, when he oversaw a 60% cut in the UK’s team of climate attaches across the world from 165 to 65, just as the UK was supposed to be delivering the Paris agreement. He then tried to hush up what happened, according to the former chief scientist David King, who claimed Johnson also ignored his advice to be more publicly supportive of climate change and misled the public about King’s agreement to chair an inquiry into building an airport in the Thames estuary. While mayor of London, his record was mixed. On the positive side, he initiated a trial of fully electric buses, raised diesel charges, improved cycling networks and encouraged tree planting and zero-emission taxis. But he also held back a report on the impact of air pollution on deprived schools and curtailed a western extension of the congestion charge zone, which could have reduced traffic and improved air quality. His most famous green boast was that he would lie down in front of bulldozers to prevent a third runway being built at Heathrow airport. But was absent during the 2018 vote on this issue, and strongly supported his own pet airport project – a four-runway mega-terminal in the Thames estuary that never got off the ground. Johnson’s newspaper columns provide copious proof that he does not “get” climate change. In 2012, he described wind farms as “white satanic mills”, supported shale gas fracking and urged readers to “ignore doom merchants” who warn of the dangers of emissions. On three occasions, he blithely – and completely wrongly – claimed global heating was primarily caused by solar activity. “Whatever is happening to the weather at the moment, it is nothing to do with the conventional doctrine of climate change,” he wrote in 2015, citing the debunked claims of the climate sceptic Piers Corbyn – the brother of the Labour leader Jeremy Corbyn. He was scoring points against a political opponent by dismissing the consensus view among scientists that the climate crisis is driven by human activity. The prime minister’s father, Stanley Johnson, is, by contrast, a committed environmentalist who has campaigned for faster action on climate change. At an Extinction Rebellion demonstration last year he declared himself proud to wear the badge of “non cooperative crusties”, which was the criticism levelled by his son at protesters. His influence is apparent in Boris Johnson’s endorsement of various wildlife campaigns, which he presents in colonial fashion as the fault of other nations. A quick glance at his backlist of articles reveals attacks on Japan for whaling, China for the slaughter of pangolins, and a belief that African elephants and cheetahs need to be saved by the UK. On the basis of his diplomatic clumsiness, poor grasp of science and voting record, Johnson appears to be the worst man for the job of making COP 26 a success. But, beyond his dad, there are two other reasons to hope that he might yet turn the pig’s ear of preparations into a silk purse of an event. If the democrat in him – and, perhaps more importantly in his adviser, Dominic Cummings – decides strong climate action is the will of the electorate (as polls overwhelmingly suggest), then he could demonstrate this by pushing it through with the same determination as Brexit. And if the patriot in him makes up his mind to make the UK a global leader in solving the greatest challenge of our age, then he will recognise the need to appoint a strong candidate to replace O’Neill. In this week’s speech, he alluded to this in an encouraging recognition of the UK’s responsibility to decarbonise. “I think it’s quite proper that we should, we were the first, after all, to industrialise. Look at historic emissions of the UK, we have a responsibility to our planet to lead in this way and to do this,” he said. Sadly it seems more likely, given his own history and his squabbles with Nicola Sturgeon over the hosting arrangements, that Johnson will put political partisanship first and bow to pressure from the US to downplay climate concerns in return for a trade deal, and from his donors and supporters to weaken EU environmental standards so the UK economy is more competitive after Brexit. Father will not approve. But Corbyn’s brother, at least, will have reason to smile. • This article was amended on 11 February 2020. A previous version said that, as Mayor of London, Boris Johnson had electrified the city’s bus fleet; it now clarifies that he initiated a trial of fully electric buses."
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
As urban expansion continues worldwide, it wouldn’t surprise anyone that cities would see a growing number of hot days as asphalt, concrete, steel and automobiles act as heat sinks that absorb the summer sun’s energy, a phenomenon known as the urban heat island effect (UHI).
Indeed this has been the case for many German cities over the past decades. Though he climate in Europe has changed over the past 30 years, that change is likely due to natural cyclic pattern changes.
Tokyo not seeing hotter summer highs
But even with a stronger UHI and supposed climate warming, Japan’s sprawling megalopolis of Tokyo has not seen an increase in the number of hot days (days where the thermometer climb to 30°C or higher), as the following chart clearly depicts:

Chart shows the number of days each year (May 1st to October 31st) where the temperature rose to 30°C or more in Tokyo. Data source: Japan Meteorological Agency (JMA). 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Clearly the number of hot days has little to do with CO2 emissions.
Tokyo October not warming
Also we’ve got the mean temperature data for October 2020 for Tokyo:

Data source: JMA
Above the chart shows that the mean October temperature in the city of Tokyo has not risen in almost 3 decades. In fact it has trended downward a bit, though statistically insignificantly.
So if anyone is claiming Tokyo is getting hotter days and hotter in general, then they either don’t know what they are talking about or they are misleading us.


		jQuery(document).ready(function(){
			jQuery('#dd_a5ebd9bed6ff70015a66e5fd318aaf1e').on('change', function() {
			  jQuery('#amount_a5ebd9bed6ff70015a66e5fd318aaf1e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Binge-watching Netflix doesn’t just fry your brain; it may also be frying the planet. The streaming service’s global energy consumption increased by 84% in 2019 to a total of 451,000 megawatt hours – enough to power 40,000 average US homes for a year. Netflix disclosed these figures in its inaugural environmental, social and governance report, noting it matched 100% of its 2019 non-renewable power use “with renewable energy certificates and carbon offsets”. While these may help the brand, they don’t address the inconvenient fact that our love of streaming has unfortunate side-effects – most of which we are only starting to comprehend.  Digital technology has ushered in an age of inconspicuous consumption. It is easy to understand the environmental impact of buying “stuff” or flying across the Atlantic. It is harder to wrap your head around how much energy it takes to fly data across the web. We may feel that we are consuming less thanks to the internet, but digital technologies account for more carbon emissions than the aerospace industry, according to a study by the Shift Project, a Paris-based thinktank. Transmitting and viewing online video accounts for a large portion for this, generating nearly 1% of global emissions. Similarly, a study from the universities of Glasgow and Oslo found that streaming music has led to “significantly higher carbon emissions than at any previous point in the history of music”. Being a conscientious consumer does not mean you have to turn off your wifi or chill with the Netflix. But we should think more critically about our data consumption. Apple already delivers screen-time reports; perhaps tech services should start providing us with carbon counts. Or maybe Netflix should implement carbon warnings. Caution: this program contains nudity, graphic language and a hell of a lot of energy. •Arwa Mahdawi is a Guardian columnist"
"
Share this...FacebookTwitterMelting mountain snow in the Canadian Mackenzie Mountains has uncovered ancient weapons used by early hunters. In the Canadian Mackenzie Mountains scientists have found weapons up to 2400 years old, reports Tom Andrews of the Prince of Wales Northern Heritage Centre in Yellowknife and his colleagues in a press release from the Arctic Institute of North America.
http://www.arctic.ucalgary.ca/main/documents/media_release_pdfs/Melting%20ice%20reveals%20ancient%20artifacts.pdf
 Scientists suspect that hunters followed herds escaping mosquitoes and heat during the hot summers. Caribou was an important food source.
 The results of their findings have been extraordinary. Andrews and his team have found 2400-year-old spear throwing tools, a 1000-year-old ground squirrel snare, and bows and arrows dating back 850 years. Biologists involved in the project are examining caribou dung for plant remains, insect parts, pollen and caribou parasites. It is very likely that snow and ice today still covers more ancient relicts.
 The findings and their dates of origin undercut warmists’ claims that the Medieval Warm Period did not exist, or was localised in Europe, and that today’s warm period is unprecedented. The age of the found artefacts correspond to the Roman Warm Period and the Medieval Warm Period.  Ancient artefacts recovered in the Alps tell the same story.

H/T: http://klimakatastrophe.wordpress.com/
Share this...FacebookTwitter "
"
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
NOTE: This essay represents a collaboration over a period of a week via email between myself and Basil Copeland. Basil did the statistical heavy lifting and the majority of writing, while I provided suggestions, reviews, some ideas, editing, and of course this forum. Basil deserves all our thanks for his labor. This is part one of a two part series.  -Anthony

Evidence of a Significant Solar Imprint in Annual Globally Averaged Temperature TrendsBy Basil Copeland and Anthony Watts 
It is very unlikely that the 20th-century warming can be explained by natural causes. The late 20th century has been unusually warm.
 
So begins the IPCC AR4 WG1 response to Frequently Asked Question 9.2 (Can the Warming of the 20th Century be Explained by Natural Variability?).  Chapter 3 of the WG1 report begins:
Global mean surface temperatures have risen by 0.74°C ± 0.18°C when estimated by a linear trend over the last 100 years (1906-2005). The rate of warming over the last 50 years is almost double that over the last 100 years (0.13°C ± 0.03°C vs. 0.07°C ± 0.02°C per decade).
Was the warming of the late 20th century really that unusual?  In recent posts Anthony has noted the substantial anecdotal evidence for a period of unusual warming in the earlier half of the 20th century.  The representation by the IPCC of global trends over the past 100 years seems almost designed to hide the fact that during the early decades of the 20th century, well before the recent acceleration in anthropogenic CO2 emissions beginning in the middle of the 20th century, global temperature increased at rates comparable to the rate of increase at the end of the 20th century.
I recently began looking at the longer term globally averaged temperature series to see what they show with respect to how late 20th century warming compared to warming earlier in the 20th century.  In what follows, I’m presenting just part of the current research I’m currently undertaking.  At times, I may overlook details or a context, or skip some things, for the sake of brevity.  For example, I’m looking at two long-term series of globally averaged annual temperature trends, HadCRUTv3 and GHCN-ERSSTv2.  Most of what I present here will be based on HadCRUTv3, though the principal findings will hold true for GHCN-ERSSTv2.
I began by smoothing the data with a Hodrick-Prescott (HP) filter with lambda=100.  (More on the value of lambda later.) The results are presented in Figure 1.

Figure 1 – click for a larger image
The figure shows the actual data time series, a cyclical pattern in the data that is removed by the HP filter, and a smoothed long term low frequency trend that results from filtering out the short term higher frequency cyclical component. Hodrick-Prescott is designed to distinguish short term cyclical activity from longer term processes.
For those with an electrical engineering background, you could think of it much like a bandpass filter which also has uses in meteorology:
Outside of electronics and signal processing, one example of the use of band-pass filters is in the atmospheric sciences. It is common to band-pass filter recent meteorological data with a period range of, for example, 3 to 10 days, so that only cyclones remain as fluctuations in the data fields.
(Note: For those that wish to try out the HP filter, a freeware Excel plugin exists for it which you can download here)
When applied to globally averaged temperature, it works to extract the longer term trend from variations in temperature that are of short term duration.  It is somewhat like a filter that filters out “noise,” but in this case the short term cyclical variations in the data are not noise, but are themselves oscillations of a shorter term that may have a basis in physical processes.
For example, in Figure 1, in the cyclical component shown at the bottom of the figure, we can clearly see evidence of the 1998 Super El Niño.  While not the current focus, I believe that analysis of the cyclical component may show significant correlations with known shorter term oscillations in globally averaged temperature, and that this may be a fruitful area for further research on the usefulness of Hodrick-Prescott filtering for the study of global or regional variations in temperature.
My original interest was in comparing rates of change between the smoothed series during the 1920’s and 1930’s with the rates of change during the 1980’s and 1990’s.  Without getting into details (ask questions in comments if you have them), using HadCRUTv3 the rate of change during the early part of the 20th century was almost identical to the rate of change at the end of the century. Could there be some sense in which the warming at the end of the 20th century was a repeat of the pattern seen in the earlier part of the century?  Since the rate of increase in greenhouse gas emissions was much lower in the earlier part of the century, what could possibly explain why temperatures increased for so long during that period at a rate comparable to that experienced during the recent warming?
As I examined the data in more detail, I was surprised by what I found.  When working with a smoothed but non-linear “trend” like that shown in Figure 1, we compute the first differences of the series to calculate the average rate of change over any given period of time.  A priori, there was no reason to anticipate a particular pattern in time (or “secular pattern”) to the differenced series.  But I found one, and it was immediately obvious that I was looking at a secular pattern that had peaks closely matching the 22 year Hale solar cycle.  The resulting pattern in the first differences is presented in Figure 2, with annotations showing how the peaks in the pattern correspond to peaks in the 22 year Hale cycle.
Besides the obvious correspondence in the peaks of the first differences in the smoothed series to peaks of the 22 year Hale solar cycle, there is a kind of “sinus rhythm” in the pattern that appears to correspond, roughly, to three Hale cycles, or 66 years.  Beginning in 1876/1870, the rate of change begins a long decline from a peak of about +0.011 (since these are annual rates of change, a decadal equivalent would be 10 times this, or +0.11C/decade) into negative territory where it bottoms out about -0.013, before reversing and climbing back to the next peak in 1896/1893.  A similar sinusoidal pattern, descending down into negative annual rates of change before climbing back to the next peak, is evident from 1896/1893 to 1914/1917.  Then the pattern breaks, and in the third Hale cycle of the triplet, the trough between the 1914/1917 peak and the 1936/1937 peak is very shallow, with annual rates of change never falling below +0.012, let alone into the negative territory seen after the previous two peaks.  This same basic pattern is repeated for the next three cycles: two sinusoidal cycles that descend into negative territory, followed by a third cycle with a shallow trough and rates of change that never descend below +0.012.  The shallow troughs of the cycles from 1914/1917 to 1936/1937, and 1979/1979 to 1997/2000, correspond to the rapid warming of the 1920’s and 1930’s, and then again to the rapid warming of the 1980’s and 1990’s.
While not as well known as the 22 year Hale cycle, or the 11 year Schwabe cycle, there is support in the climate science literature for something on the order of a 66 year climate cycle.  Schlesinger and Ramankutty (1994) found evidence of a 65-70 year climate cycle in a number of temperature records, which they attributed to a 50-88 year cycle in the NAO.  Interestingly, they sought to infer from this that these oscillations were obscuring the effect of AGW.  But that probably misconstrues the significance of the mid 20th century cooling phase.  In any case, the evidence for a climate cycle on the order of 65-70 years extends well into the past.  Kerr (2000) links the AMO to paleoclimate proxies indicating a periodicity on the order of 70 years.  What I think they may be missing is that this longer term cycle shows evidence of being modulated by bidecadal rhythms.  When the AMO is filtered using HP filtering, it shows major peaks in 1926 and 1997, a period of 71 years.  But there are smaller peaks at 1951 and 1979, indicating that shorter periods of 25, 28, and 18 years, or roughly bidecadal oscillations.  There is a growing body of literature pointing to bidecadal periodicity in climate records that point to a solar origin.  See, for instance, Rasporov, et al, (2004).  A 65-70 year climate cycle may simply be a terrestrial driven harmonic of bidecadal rhythms that are solar in origin.
In terms of the underlying rates of change, the warming of the late 20th century appears to be no more “unusual” than the warming during the 1920’s and 1930’s.  Both appear to have their origin in a solar cycle phenomenon in which the sinusoidal pattern in the underlying smoothed trend is modulated so that annual rates of change remain strongly positive for the duration of the third cycle, with the source of this third cycle modulation perhaps related to long term trends in oceanic oscillations.  It is purely speculative, of course, but if this 66 year pattern (3 Hale cycles) repeats itself, we should see a long descent into negative territory where the underlying smoothed trend has a negative rate of change, i.e. a period of cooling like that experienced in the late 1800’s and then again midway through the 20th century.

Figure 2 – click for a larger image
Figure 2 uses a default value of lambda (the parameter that determines how much smoothing results from Hodrick-Prescott filtering) that is 100 times the square of the data frequency, which for annual data would be 100.  This is conventional, and is consistent with the lambda used for quarterly data in the seminal research on this technique by Hodrick and Prescott.  I’m aware, though, of arguments for using a much lower lambda, which would result in much less smoothing.
In Part 2, we will look at the effect of filtering with a lower value of lambda.  The results are interesting, and surprising.
Part 2 is now online here
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0602b82',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
November 30th marks the official end of hurricane season. Below is some good news, courtesy of Ryan Maue at Florida State University COAPS :
The 2007 Atlantic Hurricane season did not meet the hyperactive expectations of the storm pontificators. This is good news, just like it was last year. With the breathless media coverage prior to the 2006 and 2007 seasons predicting a catastrophic swarm of hurricanes potentially enhanced by global warming a la Katrina, there is currently plenty of twisting in the wind to explain away the hyperbolic projections. The predominant refrain mentions something about “being lucky” and having “escaped” the storms, and “just wait for next year”.
Before we prepare for the obvious impending onslaught of the next “above-average” hurricane season, let’s review some very positive aspects of what 2007 offered:

The 2007 Atlantic Hurricane season was below-normal and tied for 2002 as the most inactive since the El Nino depressed 1997 season in terms of storm energy. Note: Hurricane Energy is measured through the Accumulated Cyclone Energy (ACE) index
The North Atlantic was not the only ocean that experienced quiet tropical cyclone activity. The Northern Hemisphere as a whole is historically inactive. How inactive? One has to go back to 1977 to find lower levels of cyclone energy as measured by the ACE hurricane energy metric. Even more astounding, 2007 will be the 4th slowest year in the past half-century (since 1958) .
Fewest Northern Hemisphere Hurricane Days since 1977. 3rd Lowest since 1958 (behind 1977 and 1973). See the Hurricane Days Graphic below.
When combined, the 2006 and 2007 Atlantic Hurricane Seasons are the least active since 1993 and 1994. When compared with the active period of 1995-2005 average, 2006 and 2007 hurricane energy was less than half of that previous 10 year average. The most recent active period of Atlantic hurricane activity began in 1995, but has been decidedly less active during the previous two seasons.
When combined, the Eastern Pacific and the North Atlantic, which typically play opposite tunes when it comes to yearly activity (b/c of El Nino), brushed climatology aside and together managed the lowest output since 1977. In fact, the average lifespan of the 2007 Atlantic storms was the shortest since 1977 at just over two days. This means that the storms were weak and short-lived, with a few obvious exceptions.

Hurricane Days by Year

2007 Departure from ACE and Climatic norms:


Basin
Current ACE
Climo ACE
% Departure


Northern Hemisphere
373.4
525.2
-28.9%


North Atlantic
67.7
93.8
-27.8%


Western Pacific
209.2
286.8
-27.1%


Eastern Pacific
52.2
131.2
-60.2% 




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea25e7b10',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"One of the common problems we have in wildlife biology is knowing how many of an endangered species there are in a particular area. For example, how many orangutans are there in a particular national park in Sumatra? Traditionally this problem was solved by biologist doing on-the-ground surveys: literally walking through the forests counting the number of orangutans and their nests (a proxy of their presence). Tough terrain and the difficulty in seeing the animals high up in the rainforest’s canopy makes this very time consuming. To solve this problem Serge Wich, a primate expert at Liverpool’s John Moores University, came up with the idea of using drones to survey for orangutans and their nests. As commercial quality drones were too expensive for day-to-day use in conservation biology projects, he and his colleagues set about building their own drone and developing appropriate software.  Now from their site you can download instructions to build a high-quality drone for around US$2,000. Conservation drones have proved to be a huge success. In just a few hours they are able to survey huge areas of the rainforest ––something in the past that took weeks or months to do by biologists walking on foot and straining their necks as they looked constantly upwards.  This means that we are now able to better track changes in population sizes.   However, all these drones generate many hours of video, which needs to be analysed. It is important that the time gained by the use of drones is not lost through the need to manually observe the video recorded. Thus, the development of video recognition software to count objects such as orangutans and their nests is well underway. Drones can also be used to protect animals.  Kenya announced this year that it would be using drones to assist its national park rangers in the fight against poaching. Using drones it is possible to monitor remote areas of national parks where herds of elephants or groups of rhinos may fall victim to the poacher’s bullet.   Drones are also useful in forests where they can detect the campfires of poachers or illegal loggers and gather video evidence. Poachers can’t just shoot the drones down either – shooting a drone with an AK47, the poacher’s weapon of choice, is difficult as drones are small, speedy (60kph) and fly at several hundred feet. Drones open up all kinds of interesting areas of research for wildlife biologists.  For example, the HD cameras on a drone means you can obtain images of rainforests that would make most spy satellite images look like a highly pixelated 1970s computer screen. Plus due to the low cost of flying the drones it would be possible to constantly monitor the target environment – something that can be done with satellite images, but at a high cost. The possible applications for drones in conservation biology are almost endless. I would like to use them to film and follow secretive species such as the puma from the Brazilian savannahs (cerrado).  They could also be used to pick up data from remote sensing stations in environments where there is no telecommunication signal. For example, images from camera traps could be uploaded to a drone as it circles overhead – reducing the need to walk for days to download such images. This technology is not without its problems; at the moment low-cost drones are not able to stay in the air for much longer than 45 minutes. But advances in battery technology and the use of solar panels means this issue should disappear in the not too distant future."
"Commuting is always a hot topic. This week, I was invited onto a BBC regional radio station to talk about why we find commuting so stressful: a caller had gone on a rant to producers about local roadworks that were making his daily drive hellish.  The irate caller was typical – 91% of UK workers commute and most find it an ordeal. We Brits do love a moan, so what better to complain about? In this light, the chancellor’s Autumn statement announcement of £15 billion for road schemes up and down England promises an invitation for even greater commuter disgruntlement. While George Osborne says his investment will ease congestion and link up major urban areas, the very act of commuting is actually bad for us. By encouraging more commuting, and especially by further deepening the hold of the car system with the presumption for private ownership, we will just see more of the same: a legion of stressed out commuters miserably trudging to work and home again throughout the week. We are commuting further than ever before, an average of more than nine miles, a trend that reflects the pressure to find and hold down a job in times of austerity. This has predictably negative consequences. Recent research into commuting has shown it makes us unhappy and anxious while lowering our sense of self-worth and fundamentally reducing levels of life satisfaction. The commuting that more than 80% of workers tolerate at an average of an hour a day adversely affects both our physical and mental health. Commuting increases incidences of back, joint and neck pain, with two-thirds of drivers blaming their daily travel for such ailments.  As well as suffering from higher levels of insomnia, commuters are less likely to take regular exercise and more likely to forego wholesome home-cooked food in favour of ready meals and take-away.  A study in California found commuting to be the most significant lifestyle factor behind obesity – the amount of miles travelled directly correlating to weight gain. When we are out of shape, our self-image suffers and there is a strong relationship between obesity and poor mental health. Commuting also tends to make us more isolated. Every 10 minutes of commuting is said to reduce social capital – the networks of friends and acquaintances we can develop – by 10%. We have fewer people to turn to unburden ourselves. This is the loneliness of the crowds. While commuting inevitably means being surrounded by others, they are typically strangers at best – or, more likely, rivals to compete with and be antagonised by. Indeed research suggests  a rise in commuting by car has increased social atomisation and supplanted the idea of community with a heightened level of detached individualism.  Commuting is not only implicated in a decline of civic spirit but can even be attributed as a major cause of marriage break-up with those travelling more than three quarters of an hour to get to work 40% more likely to divorce their partner. The lack of control in commuting is another stress factor, as traffic jams and unpredictable weather mean we are constantly on edge. Some of the worst effects can be found in women. While women tend to work shorter hours and commute less, they are unfavourably impacted by the health issues surrounding commuting. It has been suggested that this trend may result from the generally weaker occupational position women experience but it seems more likely to result from their having to take on greater responsibility for day-to-day household tasks such as childcare and housework.  In particular, this can be found in a practice labelled “trip chaining” as women tend to make more interim stops along the route of their commute, picking up children from schools or purchasing goods at the shops meaning that they have less flexibility and are under more pressure to squeeze in extra activities. A recent study found walking or cycling to work improves mental well-being, as well as the obvious physical benefits. Those who get to work under their own steam are able concentrate better and felt under less strain than when travelling by car.  Even opting for public transport made commuters feel better than driving so the message seems to be to get out of the car if you want to feel better.  Of course, the happiest people are actually those who work at home so, with advances in telecommuting and flexi-time, not going into the office at all would be the ideal. But for those who must commute, the government’s pre-election inducement to make this easier to do by car might seem like good news but, really, will only tie drivers into a practice that is slowly killing them."
"

“War is God’s way of teaching Americans geography,” the 19th century American writer Ambrose Bierce sagely observed, and the war in Iraq is no exception. After weeks of intense coverage, one fact is plain: The people of Iraq should be among the richest in the world. 



Iraq has been cursed by brutal politics, but it has been abundantly blessed by geography. Two great rivers bound a fertile plain in a subtropical climate. It possesses a seaport and ready access to major markets. As the repository for many of the world’s greatest archeological sites, it should be a tourist Mecca. And, of course, the country sits on top of the world’s second largest known oil reserves. 



Among his many crimes, Saddam Hussein squandered and stifled the potential wealth of his country by his warmongering and official thievery. The economic policy of his Baathist Party was a kind of thuggish socialism: government control of prices and industry, ubiquitous rationing, arbitrary confiscation of private wealth, and little trade other than arms and oil. 



Saddam’s misrule has left Iraq by far the poorest country in the Persian Gulf region. Its per capita gross domestic product is the equivalent of $2,500 a year, far lower than the per capita GDP of Qatar ($21,200), Kuwait ($15,000), Saudi Arabia ($10,600) or even Iran ($7,000). Iraq’s imports are a fraction of what they were in the 1980s, when it citizens were buying almost $1 billion worth of U.S. farm products a year. 



America’s well‐​earned victory on the battlefield will be in jeopardy if the people of Iraq cannot enjoy the material fruits of their new‐​found freedom. To safeguard our investment of blood and treasure, the United States and Great Britain should ensure that any new Iraqi government protects the economic as well as the political and civil freedom of its citizens. 



An essential part of any plan to establish freedom in Iraq should be a commitment to a free market and the institutions that support it, including a commitment to free trade. Iraqis must enjoy a secure right to property, a stable currency, decontrolled prices, the rule of law and contract, and the freedom to engage in business, at home and through international trade. 



Post‐​war reconstruction of Europe provides a model, although not in a way most people believe. Yes, Marshall Plan aid played a role in reviving Western Europe, but the real story was the continent’s turn toward markets and free trade. In June 1948, Germany’s Ludwig Erhard abruptly repealed price controls and issued a new currency. Tax and tariff cuts soon followed. As one historian noted: “The spirit of the country changed overnight. The gray, hungry, dead‐​looking figures wandering about the streets in their everlasting search for food came to life.” Iraq needs an Arab Ludwig Erhard. 



Study after study has confirmed that nations relatively open to trade grow faster and achieve higher incomes than those that are relatively closed. The model for Iraq’s new leaders should be Ireland, Chile, the tigers of East Asia, and other countries that have achieved sustained growth through expanding trade—not the stagnant and increasingly isolated countries of the Arab Middle East. 



With the Saddam regime now history, UN sanctions should be lifted immediately. If France and Russia insist on keeping sanctions in place to protect the UN’s bureaucratic control over Iraqi oil, the United States should ignore the sanctions and unilaterally allow Americans to trade with the people of Iraq. U.S. markets should be opened to goods made by Iraqis, especially import‐​sensitive textiles, apparel, and farm products. Beyond stimulating growth, trade with Iraq would bring humanitarian relief, cement ties between our two countries, and send a positive signal that Iraq is open to foreign investment. 



Much has been written about the need for political reform in the Arab world, but it also desperately needs economic reform. The Arab world’s share of global trade and foreign investment has been declining in the last two decades. Outside of oil, Arab countries export little that the rest of the world is willing to buy. With a few exceptions, barriers to trade and foreign investment remain high. There are more McDonald’s franchises in the tiny Netherlands than in all the Arab world. 



A vibrant Iraqi economy would give hope to a new generation of Arabs to reclaim their rightful place in the world of trade, science and ideas. An educated, hopeful middle class would in turn create more fertile soil for limited and representative government. But if a post‐​Saddam Iraq fails to prosper, its people will grow frustrated and blame the market and the West for their troubles, creating fertile soil for terrorism. 



The technology, dynamism, and openness of our own market economy helped us win this war; if spread to Iraq, those same market forces can help us win the peace.
"
"Not only is air pollution bad for our lungs and heart, it turns out it could actually be making us less intelligent, too. A recent study found that in elderly people living in China, long-term exposure to air pollution may hinder cognitive performance (things like our ability to pay attention, to recall past knowledge and generate new information) in verbal and maths tests. As people age, the link between air pollution and their mental decline becomes stronger. The study also found men and less educated people were especially at risk, though the reason why is currently unknown.  We already have compelling evidence that air pollution – especially the tiniest, invisible particulates in pollution – damages the brain in both humans and animals. Traffic pollution is associated with dementia, delinquent behaviour in adolescents, and stunted brain development in children who attend highly polluted schools.  


      Read more:
      London air pollution is restricting children's lung development – new research


 In animals, mice exposed to urban air pollution for four months showed reduced brain function and inflammatory responses in major brain regions. This meant the brain tissues changed in response to the harmful stimuli produced by the pollution. We don’t yet know which aspects of the air pollution particulate “cocktail” (such as the size, number or composition of particles) contribute most to reported brain deterioration. However, there’s evidence that nanoscale pollution particles might be one cause.  These particles are around 2,000 times smaller than the diameter of a human hair, and can be moved around the body via the bloodstream after being inhaled. They may even reach the brain directly through the olfactory nerves that give the brain information about smell. This would let the particles bypass the blood-brain barrier, which normally protects the brain from harmful things circulating in the bloodstream. Postmortem brain samples from people exposed to high levels of air pollution while living in Mexico City and Manchester, UK, displayed the typical signs of Alzheimer’s disease. These included clumps of abnormal protein fragments (plaques) between nerve cells, inflammation, and an abundance of metal-rich nanoparticles (including iron, copper, nickel, platinum, and cobalt) in the brain.  The metal-rich nanoparticles found in these brain samples are similar to those found everywhere in urban air pollution, which form from burning oil and other fuel, and wear in engines and brakes. These toxic nanoparticles are often associated with other hazardous compounds, including polyaromatic hydrocarbons that occur naturally in fossil fuels, and can cause kidney and liver damage, and cancer. Repeatedly inhaling nanoparticles found in air pollution may have a number of negative effects on the brain, including chronic inflammation of the brain’s nerve cells. When we inhale air pollution, it may activate the brain’s immune cells, the microglia. Breathing air pollution may constantly activate the killing response in immune cells, which can allow dangerous molecules, known as reactive oxygen species, to form more often. High levels of these molecules could cause cell damage and cell death. The presence of iron found in air pollution may speed up this process. Iron-rich (magnetite) nanoparticles are directly associated with plaques in the brain. Magnetite nanoparticles can also increase the toxicity of the abnormal proteins found at the centre of the plaques. Postmortem analysis of brains from Alzheimer’s and Parkinson’s disease patients shows that microglial activation is common in these neurodegenerative diseases.  


      Read more:
      Your exposure to air pollution could be much higher than your neighbour's – here's why


 The latest study of the link between air pollution and declining intelligence, alongside the evidence we already have for the link between air pollution and dementia, makes the case for cutting down air pollution even more compelling. A combination of changes to vehicle technology, regulation and policy could provide a practical way to reduce the health burden of air pollution globally.  However, there are some things we can do to protect ourselves. Driving less and walking or cycling more can reduce pollution. If you have to use a car, driving smoothly without fierce acceleration or braking, and avoiding travel during rush hours, can reduce emissions. Keeping windows closed and recirculating air in the car might help to reduce pollution exposure during traffic jams as well.  But young children are among the most vulnerable because their brains are still developing. Many schools are located close to major roads, so substantially reducing air pollution is necessary. Planting specific tree species that are good at capturing particulates along roads or around schools could help.  Indoor pollution can also cause health problems, so ventilation is needed while cooking. Open fires (both indoors and outdoors) are a significant source of particulate pollution, with woodburning stoves producing a large percentage of outdoor air pollution in the winter. Using dry, well-seasoned wood, and an efficient ecodesign-rated stove is essential if you don’t want to pollute the atmosphere around your home. If you live in a naturally-ventilated house next to a busy road, using living spaces at the back of the house or upstairs will reduce your pollution exposure daily.  Finally, what’s good for your heart is good for your brain. Keeping your brain active and stimulated, eating a good diet rich in antioxidants, and keeping fit and active can all build up resilience. But as we don’t yet know exactly the mechanisms by which pollution causes damage to our brains – and how, if possible, their effects might be reversed – the best way we can protect ourselves is to reduce or avoid pollution exposure as much as possible."
"
Share this...FacebookTwitterA “potential connection” between anthropogenic global warming and the frequency or intensity of wildfires in California has yet to emerge in the trend observations.
Scientists have found a “lack of correlation between late summer/autumn wildfires” and “summer precipitation or temperature” in coastal California. In fact, “there is no long-term trend in the number of fires over coastal California” in the last 50 years (Mass and Ovens, 2019). 
Image Source: Mass and Ovens, 2019
Fire history in the Western USA has recently declined to the lowest point in the last 1,400+ years (Marlon et al., 2012).

Image Source: Marlon et al., 2012
As CO2 concentrations have risen from 300 ppm to 400 ppm (1900 to 2007), the decline in global burned area has been significant (Yang et al., 2014).

Image Source: Yang et al., 2014
The falling trends in global-scale wildfires can even be dectected over the short-term 2001-2016 period (Earl and Simmonds, 2018).

Image Source: Earl and Simmons, 2018
Share this...FacebookTwitter "
nan
"

In February 2012 Gen. Martin Dempsey, the chairman of the Joint Chiefs of Staff, declared, “I can’t impress upon you [enough] that in my personal military judgment, formed over 38 years, we are living in the most dangerous time in my lifetime, right now.”



One year later, he upped the ante: “I will personally attest to the fact that [the world is] more dangerous than it has ever been.” But General Dempsey is hardly alone. Dire warnings about our uniquely dangerous world are ubiquitous. Director of National Intelligence James Clapper testified in early 2014 that he had “not experienced a time when we’ve been beset by more crises and threats around the globe.”



Members of Congress agree. Sen. John McCain (R-AZ), born before World War II, explained in July 2014 that the world is “in greater turmoil than at any time in my lifetime.”



Is it? Do we actually live in a uniquely dangerous world? And, if we do not, why do we believe that we do?



In his magisterial study of the decline in violence worldwide, Harvard’s Steven Pinker posits that “we may be living in the most peaceable era in our species’ existence,” even as he concedes that most people don’t believe it.



If our perceptions aren’t entirely accurate, if the world isn’t, in fact, more dangerous than a decade ago, or a century ago, we could blame our 24/7 media. After all, reporters don’t write about the planes that land safely; the 11 o’clock news never leads with the murder that didn’t happen. Likewise, the stories about the personal information not stolen by identity thieves, the wars that aren’t fought, and the trade and commerce that flows uninterrupted, are rarely told. Moreover, we lack perspective. There is little focus on the threats that no longer threaten. Few talk about the dangers no longer looming. It is rare, even, to find people putting today’s threats in context with the recent past. Or the distant past. Few even bother to ponder the question.



However, is not an easy task. From wars between states to wars within them, from crime and terrorism to climate change and cyber‐​mischief, we are beset by a seemingly endless array of threats and dangers. How canone compare them to past threats, especially given that the judgments of what should or should not frighten us are inherently subjective?



It remains true that the only existential threat to the United States comes from a prospective thermonuclear war — the stuff of countless novels and Hollywood films during the dark days of the Cold War. Who is to say that this event, which has never occurred, is, or should be, more frightening than the very real acts of violence that do take place every day? And does society benefit if our fears of very low probability, high‐​impact events (e.g., global thermonuclear war) were merely supplanted by fears of slightly higher probability, low‐​impact ones? Some might say that it is better to be safe than sorry. That we should worry about all potential threats. By this logic, it is better to fear things that aren’t real than to take too lightly those that are.



Perhaps the tendency to take seriously even seemingly modest dangers has been programmed into our DNA, a product of thousands of years of natural selection. Our distant ancestors who correctly perceived a fourlegged creature charging at them from a distance to be a dangerous predator had time to either flee or defend themselves, and thus lived to procreate. By contrast, their threatdeflating neighbors, who believed the approaching beast to be harmless, realized their error too late and were mauled to death.



But while we have learned to take threats seriously, we are also taught to differentiate the real from the imaginary. Fallacious claims of impending danger will erode one’s credibility, to the point that the congenital fearmonger is no longer taken seriously. The parable warns of the dangers of crying “wolf” when there are no wolves, but it doesn’t teach us to stay silent when we see one. In the parable, the wolf eventually does come, and the dishonest boy is eaten. The moral of the story is not that all dangers are inflated, but rather that the phony ones should not be.



In truth, we should be on the lookout for both kinds of errors. The business world punishes both the imprudent optimist as well as the too‐​gloomy pessimist. The financial analyst who rated all tech startups as “strong buys” in 2000 or the housing speculator who bought multiple condominiums in Miami in 2007 could rightly be cast as too optimistic. On the other hand, extreme risk aversion can blind us to possibilities. And excessive fear can be harmful to both our physical health and emotional well‐​being. The National Institute of Mental Health explains that “excessive, irrational fear and dread” are key symptoms for one of several anxiety disorders, which according to one estimate, afflict 18 percent of Americans.



 **FEAR IS THE HEALTH OF THE STATE**  
But there is a political harm as well. Individual liberty is often threatened during periods of heightened fear and anxiety, a fact that informed the very structure of the U.S. government. James Madison, in making the case for restraining the new government’s war‐​making powers, warned the delegates to the Constitutional Convention in Philadelphia: “The means of defence against foreign danger, have been always the instruments of tyranny at home.”



He went on: “Among the Romans it was a standing maxim to excite a war, whenever a revolt was apprehended. Throughout all Europe, the armies kept up under the pretext of defending, have enslaved the people.” A decade later, Madison returned to this theme in a letter to Thomas Jefferson. Madison knew that there was already some demand for a standing military, and that a few would use fear of foreign threats to whip up public sentiment in favor of a more powerful state. Indeed, Madison postulated “a universal truth that the loss of liberty at home is to be charged to provisions against danger real or pretended from abroad.”



Others since then have stumbled upon similar ideas about popular notions of threats, and of how the fear of threats has been used to grow the power of government. For example, the noted writer, social critic and satirist H.L. Mencken declared “the whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



Madison and Mencken’s warnings remain relevant today. Recall how in November 2008 incoming Obama chief of staff Rahm Emanuel called for swift government action to deal with what he said was an urgent threat. “You don’t ever want a crisis to go to waste,” Emanuel explained in an interview, “it’s an opportunity to do important things that you would otherwise avoid.”



While Emanuel was talking about an economic crisis, an increasingly powerful state can be used in many different ways, regardless of whether it was precipitated by fears of foreign or domestic threats. The same sorts of powers that allowed the Justice Department to go after suspected terrorists allowed the IRS to harass suspected tea partiers.



 **NEW TECHNOLOGIES, NEW FEARS**  
Because inaccurate or misleading characterizations of threats pave the way for the growth of government, it is crucial to understand their true nature.



Thus, is the world more dangerous than ever before? In a word, no. Americans, especially, enjoy a measure of security that our ancestors would envy, and that our contemporaries do envy.



This is not to say that there are no dangers in the world today, as the residents of Tel Aviv and Gaza City will attest. Nor can we say that circumstances will not change for the worse in the future. No one would have predicted that a single act of violence in late June 1914 would precipitate a series of events that culminated in the First World War. Could the 21st‐​century successors to Gavrilo Princip deploy cyber‐​weapons to wound, or even kill, their chosen targets? Could they do more than simply kill a single head of state, but do grievous harm to millions? Or could their actions, as Princip’s did, lead to a major war, which in the nuclear era would result in the deaths of hundreds of millions?



The possibilities cannot be ruled out. But, for now, they are only that: possibilities, and unlikely ones at that. Since their inception, nuclear weapons have been the one true weapon of mass destruction. And following the 9/11 attacks, many believed that they would inevitably fall into the hands of terrorists or other nonstate actors inclined to use them. Still others worry that more nation‐​states will acquire them. The fear of proliferation is not new. In either case caution is warranted, but excessive fear is not. Few countries have ever seriously aspired to possess such weapons, and many of those who did eventually gave up. In a few cases, countries actually turned over their weapons entirely. In fact, for nearly every country in the world, nuclear weapons are more trouble than they are worth.



Terrorists and nonstate actors have, so far at least, come to a similar conclusion. Contrary to the apocalyptic predictions immediately after 9/11, al Qaeda and others have relied exclusively on conventional weapons–chiefly bombs and bullets–to terrorize their victims. They seem to be heeding the advice found in a memo on an al Qaeda laptop seized in Pakistan in 2004: “Make use of that which is available … rather than waste valuable time becoming despondent over that which is not within your reach.”



 **NATION-STATES VS. NONSTATES**  
What of the more traditional threats posed by states? While Vladimir Putin seems to be trying to restore Russia to its place at the top of the enemies list, China is the one country with sufficient size and potential wealth to directly challenge the United States in the future. But it is premature, to say the least, to assume that a war between China and the United States is inevitable. To be sure, U.S. treaty commitments to some of China’s neighbors risk drawing the United States into vexing territorial disputes, and China has been developing military capabilities that could significantly raise the costs for the United States if it chose to back its allies’ claims by force. For now, however, all parties have many reasons to try to resolve these claims peacefully — including the fact that China is the leading trading partner throughout the region. Similarly, the United States and China have many reasons to work together to address other problems beyond the Asia Pacific region.



Many of those common dangers emanate from nonstate and substate actors, from terrorists and insurgents to revolutionaries and rebels. And while these threats are real, they pale in comparison to what states used to do to one another on a regular basis.



Terrorism is, in fact, far less dangerous than widely believed. Consider, for example, that a total of 19 Americans have been killed in four separate terrorist incidents carried out by Islamist extremists on American soil since 9/11. For reference, 50 people were killed in just three separate incidents during a 14- month span in 2012 and 2013 (Aurora, Colorado; Newtown, Connecticut; and the Washington Navy Yard). Excluding U.S. military personnel, fewer Americans have been killed by terrorism globally since 2002 than have died from allergic reactions to peanuts (an average of 50–100 per year).



Although a relatively small number of people are killed or injured by terrorism every year, many people worry that new technology will allow nonstate actors to inflict harm in other ways, say, for example, by attacking the Internet, or company or individual computers connected to it. States, too, are known to have used so‐​called cyberweapons, or have aspired to do so. Thus, numerous U.S. officials have warned that cyberattacks are the single greatest threat to national security. Here again, however, some skepticism is in order. Hackers and criminals are adept at exploiting the vulnerabilities in computer networks, but it is extremely difficult to carry out a major attack with far‐​reaching consequences.



Even an attack that managed, somehow, to crash the banking system completely would be unlikely to undermine confidence in the wider economy, let alone trigger a recession. As the RAND Corporation’s Martin Libicki points out, NASDAQ’s three‐​hour shutdown on August 22, 2013, didn’t spark a wave of panic selling. “It would require data corruption (e.g., depositors’ accounts being zeroed out) rather than a temporary disruption,” Libicki explains, “before an attack would likely cause depositors to question whether their deposits are safe.”



The greater threat may come from measures taken to prevent attacks if they significantly impede legitimate transactions in cyberspace. Similarly, poorly conceived or badly executed policies after the fact might cause more harm than the original incident that precipitated them. The attribution problem compounds the risk. The difficulty in tracking possible cyberattacks to their true source, and in ensuring that the punishment or retaliation is directed at the perpetrators, places a high premium on measured, targeted responses. Maintaining that standard will help ensure a safer world.



Another factor that can explain why the world is becoming less, rather than more, dangerous, is evolving social norms. Harvard’s Pinker notes a perceptible shift in public attitudes toward violence, and documents an associated decline in violent crimes of all types. The rate of such crimes, including murder, rape, and assault, are at or near all‐​time lows. Within the United States, for example, the homicide rate (homicides per 100,000 residents) fell by nearly half (49 percent) in the 20‐​year period from 1992 to 2011.



 **SUPPOSED THREATS TO GLOBAL STABILITY AND ECONOMIC PROSPERITY**  
Still others worry not so much about physical security, but rather about our prosperity and way of life. They fear that a war could cripple the international economy, or that the mere threat of war could disrupt global trade and commerce, including the world’s oil supplies. This concern, at least today, is the main justification for the U.S. military’s forward presence around the world, a posture oriented around stopping possible threats before they materialize.



But the patterns of global trade are far more resilient than the pessimists envision. War between major trading partners is highly unlikely, and, even if it were to occur, trade flows between nonbelligerents would not be disrupted, or not for very long. Indeed, Eugene Gholz shows that when countries shift resources to the purchase of military goods, the result is similar to other consumption binges. During wartime, neutral nations may well benefit economically, as they become a safe haven for investment diverted from warzones, and as they are able to buy certain goods at low cost.



While war itself has many horrific effects, the costs that Americans pay to stop all wars are unlikely to be outweighed by the benefits. There are other risks associated with maintaining a forward military posture. Current U.S. strategy encourages other countries to free‐​ride on the security guarantees provided by the U.S. military, imposing an unnecessary — and ultimately counterproductive — burden, on U.S. taxpayers. Exaggerated fears of distant conflicts could even prompt the United States to fight wars that pose no direct threat to U.S. security and to spend too much on the military, which, in turn, weakens the overall U.S. economy.



 **PUTTING TODAY’S THREATS IN PERSPECTIVE**  
Although the world will never be free from dangers, we should aspire to understand them clearly. Maintaining perspective isn’t easy when we are bombarded with images of fighting from Eastern Ukraine or Gaza. But, in many instances, we are today merely seeing what has always existed beyond our field of vision. Tragic, even horrifying, stories of human suffering do not portend that we are living in a more dangerous world. In most respects, we are living longer, better lives. Our chances of suffering a violent or premature death are very low, and still declining. And our prosperity and broader wellbeing are protected by a dynamic and resilient international economy, and by the spread of powerful ideas that have reduced poverty and disease.



A better understanding of what actually threatens us will help us tame our tendency to overreact. An honest assessment of the threat environment — problems that lurk today and on the horizon — will allow us to redirect some of the money that goes to the Pentagon and military contractors back to the taxpayers and private entrepreneurs. And, recalling Madison and Mencken’s warnings about how and why states exaggerate threats to grow their power, a more accurate assessment of the world’s dangers will ultimately help us to preserve our liberty.
"
"

There’s a common misconception that people who are in favor of a free market are also in favor of everything that big business does. Nothing could be further from the truth.



As a believer in the pursuit of self‐​interest in a competitive capitalist system, I can’t blame a businessman who goes to Washington and tries to get special privileges for his company. He has been hired by the stockholders to make as much money for them as he can within the rules of the game. And if the rules of the game are that you go to Washington to get a special privilege, I can’t blame him for doing that. Blame the rest of us for being so foolish as to let him get away with it.



I do blame businessmen when, in their political activities, individual businessmen and their organizations take positions that are not in their own self‐​interest and that have the effect of undermining support for free private enterprise. In that respect, businessmen tend to be schizophrenic. When it comes to their own businesses, they look a long time ahead, thinking of what the business is going to be like 5 to 10 years from now. But when they get into the public sphere and start going into the problems of politics, they tend to be very shortsighted.



The most obvious example is protectionism. Can you name any major American industry that has really benefited from tariffs and protection? Alexander Hamilton, in his famous report on manufactures, praised Adam Smith to the sky while at the same time arguing that the United States was a special case in that it had infant industries that needed to be protected, including steel. Steel is still being protected 200 years later.



Commercial banking is another example. At the end of World War II commercial banking accounted for roughly half of the capital market. Today it accounts for about one‐​fifth. Why has it deteriorated? Why is the international financial market in London, not in New York? 



The answer is the long‐​term effect of the of the banking industry’s insistence on special government favors. In the early days, under what was known as Regulation Q, the government set a limit on the interest rates that banks could pay, including a rate of zero on demand deposits. The government‐​imposed interest rate of zero on demand deposits encouraged the emergence of money market funds and the growth of substitutes for and alternatives to banks. The banking industry consistently supported fixed exchange rates. When the dollar got into trouble, President Johnson introduced restrictions on foreign lending and an interest‐​equalization tax. The result was to drive the commercial banking industry to London. Both of those measures reduced the commercial banking industry from the predominant supplier of credit to a minor player. Again, a policy that was very shortsighted.



The easiest shot of all is the way in which corporations make contributions. The oil industry contributes to conservation organizations that are trying to sharply reduce the use of oil. The nuclear industry contributes to organizations that support nonnuclear energy. Recently, Capital Research Center analyzed grants from major corporations to public policy organizations and found that the major corporations made $3 in grants to the nonprofit left for every dollar they gave to the nonprofit right.



Why hasn’t the corporate world followed the excellent example that was set by Warren Buffett? From his earliest days, in sending a dividend check to his stockholders, he said, “We are prepared to distribute X dollars on your behalf for each share of stock to charity, to some organization. Let us know to whom you would like it sent, and we will send it on your behalf.” 



Why should corporations decide the charitable purposes that should be supported by the income of their stockholders? Why shouldn’t each stockholder decide that? And why is the business community in general so insistent on supporting its own enemies? 



Now consider education. As you know, I have long been in favor of trying to privatize schooling through a voucher system. One strong argument in favor of privatization has to do with the values instilled by our public education system.



Any institution will tend to express its own values and its own ideas. Our public education system is a socialist institution. A socialist institution will teach socialist values, not the principles of private enterprise. That wasn’t so bad when elementary and secondary education was more dispersed, so there could be more local control. When I graduated from high school there were 150,000 school districts in the United States. Today there are fewer than 15,000 and the population is twice as large.



What has been the business community’s attitude toward education? Members of the business community have been well aware that schools instill values that are unsympathetic to a free private enterprise system. They are also aware that it’s difficult to get employees with the appropriate skills. But have they been trying to promote a private enterprise education industry? Not at all. Their major activity has been to assign some of their employees to teach in public schools and to contribute computers and other items to public schools. I can’t blame an individual for what he does, but I think it’s tragic that Walter Annenberg contributed hundreds of millions of dollars for government schools, for public schools, not for private schools. I have not seen any movement in the business community in general, until very recently, to try to promote an educational system under which the customer, namely the parent and the child, has a real choice about the schooling the child gets.



Now we come to Silicon Valley and Microsoft. I am not going to argue about the technical aspects of whether Microsoft is guilty or not under the antitrust laws. My own views about the antitrust laws have changed greatly over time. When I started in this business, as a believer in competition, I was a great supporter of antitrust laws; I thought enforcing them was one of the few desirable things that the government could do to promote more competition. But as I watched what actually happened, I saw that, instead of promoting competition, antitrust laws tended to do exactly the opposite, because they tended, like so many government activities, to be taken over by the people they were supposed to regulate and control. And so over time I have gradually come to the conclusion that antitrust laws do far more harm than good and that we would be better off if we didn’t have them at all, if we could get rid of them. But we do have them.



Under the circumstances, given that we do have antitrust laws, is it really in the self‐​interest of Silicon Valley to set the government on Microsoft? Your industry, the computer industry, moves so much more rapidly than the legal process, that by the time this suit is over, who knows what the shape of the industry will be. Never mind the fact that the human energy and the money that will be spent in hiring my fellow economists, as well as in other ways, would be much more productively employed in improving your products. It’s a waste! But beyond that, you will rue the day when you called in the government. From now on the computer industry, which has been very fortunate in that it has been relatively free of government intrusion, will experience a continuous increase in government regulation. Antitrust very quickly becomes regulation. Here again is a case that seems to me to illustrate the suicidal impulse of the business community.



Now I come to the hard part: Why is there that suicidal impulse? Why do business people behave that way? I hope many of you in this room will think about it and try to come up with an answer. I will give you the few suggestions that I have, but none of them seems to me an adequate explanation. One reason was stated more than a century ago by a remarkable man, Gen. Francis A. Walker, a professor at Yale and subsequently president of M.I.T. He wrote:  




When it comes to economics, everybody is an expert who almost always gets it wrong—and business executives are no exception.



Schumpeter gave a very different explanation for this phenomenon. He argued that, within large corporations, the people in charge develop essentially bureaucratic‐​socialist attitudes and institutions. Belief in entrepreneurship and private enterprise tends to be replaced by a bureaucratic approach, leading to the emergence of a socialist system. I don’t believe that’s true. In a competitive society there is enough pressure around to prevent that from happening. But that would be an explanation.



The general climate of opinion, which treats government action as an all‐​purpose cure for every ill, is probably a more important factor. However, over the past 40 years, the climate of opinion has been changing. It is no longer taken for granted, as it used to be, that if there is a problem the way to solve it is to get the government involved. We have been winning the war of ideas even though we have been losing the war in practice. Governments today are far bigger and more intrusive than they were 40 or 50 years ago, at the same time that—partly as effect—the climate of opinion is much less favorable to government control than it was then. But I still don’t think that is an adequate explanation, so I confess that I have no good explanation. Yet I think the phenomenon calls for an explanation and that it’s in your self‐​interest to find one and change the pattern of business behavior in order to get rid of what is a clear suicidal impulse.



 _This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)
"
"BBC Studios has announced a documentary series about the teenage climate activist Greta Thunberg. The new show will follow Thunberg’s “international crusade” against the climate emergency, focusing on her campaign work as well as her “journey into adulthood”. It will also see Thunberg meet with scientists, politicians and businesspeople to explore the evidence around rising global temperatures. In a statement, the executive producer, Rob Liddell, said: “Climate change is probably the most important issue of our lives so it feels timely to make an authoritative series that explores the facts and science behind this complex subject. To be able to do this with Greta is an extraordinary privilege, getting an inside view on what it’s like being a global icon and one of the most famous faces on the planet.” Stockholm-born Thunberg came to prominence after organising a school strike against the climate crisis in 2018, under the banner Fridays for Future. She has since gained global recognition, and has addressed the United Nations, been nominated for the Nobel peace prize in 2019 and 2020, and been named Time magazine’s person of the year 2019. Thunberg’s activism has prompted criticism from seeming climate science deniers, among them Donald Trump, who described her as having an “anger management problem” in December.  The 17-year-old also remains at the forefront of climate activism. Writing in the Guardian last month, she urged world leaders attending the World Economic Forum in Davos to abandon investments in fossil fuels, describing it as “madness”. Thunberg went on to describe the climate crisis as “extremely complicated, and this is an emergency. In an emergency you step out of your comfort zone and make decisions that may not be very comfortable or pleasant. And let’s be clear – there is nothing easy, comfortable or pleasant about the climate and environmental emergency.” The announcement of the BBC series follows news that the US broadcaster Hulu is making a documentary about Thunberg with the working title Greta."
"

Would a liability insurance mandate for firearm owners provide an effective means of gun control? In the latest issue of _Regulation_ , Stephen G. Gillis and Nelson Lund examine this alternative, which rests on the principle of competitive pressure.



Insurance companies would have an incentive to keep premiums for low‐​risk gun owners low, while charging higher premiums to those who are more likely to cause injury to others. “The benefits to public safety would be modest,” the authors ultimately conclude, “but such a regulation would be preferable to many politically popular gun control proposals that would be ineffective, unconstitutional, or both.”



Timothy D. Lytton next considers our inadequate system of food regulation — pointing in particular to misrepresentative food labeling and outbreaks of illness. These problems, he notes, underscore “the shortcomings of government food regulation and the inadequacy of industry self‐​regulation.”



Yet, there is one niche market in the industry that points to an alternative solution. “The success of kosher food certification offers a model of independent, private certification that could improve food safety and labeling,” Lytton writes, “and point the way toward regulatory reform in other areas such as finance and health care.”



Patrick J. Michaels and Paul C. Knappenberger ask why climate change assessments overlook the differences between the models being used and the empirical data. Jagadeesh Gokhale considers a new approach to Social Security disability insurance reform, noting that more of the disabled would return to work if they faced better incentives.



Other contributors include Ike Brannon, who tackles the economics of sports stadiums in “Could Dan Snyder End Publicly Financed Stadiums?” and M. Todd Henderson, who considers ways to improve corporate governance in “Reconceptualizing Corporate Boards.”



The Fall 2013 issue features book reviews on the causes of and response to the financial crisis, what ended history’s great empires from ancient Rome to modern America, and why state‐​led humanitarian efforts typically fall short of their stated goals. It wraps up with editor Peter Van Doren’s survey of recent academic papers, as well as a final word from Tim Rowland on the demise of automotive dealerships.
"
"
Share this...FacebookTwitter

Climate alarmism dissenters getting increasingly vocal.
Yesterday I reported on how science editor Axel Bojanowski at German national daily DIE WELT had written a commentary on the deceptive use of a faulty climate hockey stick by ZDF German broadcasting.
Naturally whenever anything of the sort happens here in Germany, attack dog Stefan Rahmstorf of the alarmist Potsdam Institute rushes out to discredit the dissenter and defend the beloved but flawed chart.
Rahmstorf attacks DIE WELT’s Axel Bojanowski
And Rahmstorf again made the mistake of attacking the messenger of the news (Axel Bojanowski). It was probably the only option left for the ever haughty Rahmstorf, because the diligent DIE WELT editor based his stinging hockey stick commentary on statements made by four experts. Rahmstorf likely had no desire going after the four distinguished colleagues. So it was probably easier and safer for him to just take shots at DIE WELT and editor Bojanowski.
What follows is part of the Twitter exchange between Rahmstorf and Bojanowski, English translation follows:


Hallo Herr Rahmstorf, schade, dass Sie einen Fehler nicht zugeben können, sondern einen drauf setzen müssen.
Ich zitiere in meinem Artikel vier Experten mit Kommentaren zu Ihrer nicht in der Fachliteratur publizierten Kurve, alle fällen ein höchst kritisches Urteil⬇️    1/🧵 pic.twitter.com/pjd2Xl83Zf
— Axel Bojanowski (@Axel_Bojanowski) August 7, 2020


Rahmstorf writes to Bojanowski (see above):
Needless to say – the hockey stick curve is well confirmed by hundreds of scientists after more than two decades of further research – also by the latest data shown above. Its authors have received many awards.
And I’m going swimming now!

Bojanowski reply (in English):

Hello Mr. Rahmstorf, it is a pity you cannot admit a mistake, but even have to put one on top.
In my article, I quote four experts with comments on your curve, which has not been published in the technical literature, all of whom make a highly critical judgment.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Mann jumps in
Even Michael Mann, the creator of the false hockey stick, jumped in as well in typical hothead fashion, offering to “fix” Bojanowaski’s Twitter account:



Like Rahmstorf, Mann too avoided criticizing the four scientists underpinning Bojanowski’s comment, instead resorted to throwing insults and name-calling.
Kachelmann: “potsdumb unscientific nonsense of Rahmstorf”
Moreover, warmist (but non-alarmist) high-profile Swiss meteorologist Jörg Kachelmann got into the fracas, taking a hard shot as well, but at the alarmist camp, tweeting (English below):


Ein #Thread von @Axel_Bojanowski zum potsdämlich-unwissenschaftlichen Unsinn von @rahmstorf und dessen Pressesprecher @terliwetter, der den Sender @zdf als Outlet für die regelmässige Absonderung von Stuss missbraucht.
Wie lange noch? Keine journalistischen Standards mehr @zdf? https://t.co/eqyvz01pXc
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) August 8, 2020

Kachelmann’s tweet above in English:
A #thread from @Axel_Bojanowski on the pots-dumb unscientific nonsense of @Rahmstorf
and his press spokesman @terliwetter, who misuse broadcaster @zdf as an outlet for the regular secretion of bullshit.
How much longer? No more journalistic standards @zdf?”
In summary, there seems to be some progress being made on how science gets conducted in Germany. Increasingly dissenters are seeing victories and the public is growing weary of all the arrogance from certain scientists – especially in the fields of climate, COVID-19 and energy.
But it remains to be seen whether or not this long overdue trend gathers steam.


		jQuery(document).ready(function(){
			jQuery('#dd_fff92674c5c3098e9bc708715a5b22b9').on('change', function() {
			  jQuery('#amount_fff92674c5c3098e9bc708715a5b22b9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"

Most every paper in the country is trumpeting today that China has finally agreed to limit its emissions of carbon dioxide, gutting the principal objection of people opposed to unilateral and expensive reductions in ours.   
  
  
Too bad it’s not true.   
  
  
According to the official pronouncement, all China said was that they “intend” to cap their emissions “around 2030”. Anything new here? In November, 2009, prior to the (failed) UN climate fest in Copenhagen, they announced their “intention” to reduce their emissions per unit economic output (called “carbon intensity”) by 40–45% by 2020. Since then, things haven’t appreciably changed—so they now have five years to execute this huge drop, which isn’t going to happen.   
  
  
The road to global warming is paved with China’s good “intentions”.   
  
  
We also note that they “intend” to derive 20 per cent of their energy from non‐​carbon based sources by 2030. No doubt working late into last night (as did we; this story broke at 10:30), the estimable Roger Pielke, Jr., has already calculated that this means that the Chinese will have to put the equivalent of one nuclear power plant _per week_ on line between now and then. As Roger wryly noted, “some people take it seriously”.   
  
  
Don’t. But we should take seriously President Obama’s announcement that the US will double its scheduled emissions reductions by 2025. Thanks to the 2007 Supreme Court (5–4) decision that incredulously said that the 1992 Clean Air Act Amendments gave the President the power to command and control virtually our entire energy economy, he indeed can do what he just said.   
  
  
It would take an act of Congress to prevent him, an act that would most certainly be vetoed, without the necessary two‐​thirds majority to override.   
  
  
One might think that he would care about what the voters think—but that’s not the case. A careful read of election returns reveals that the cap‐​and‐​trade, and not health care, cost his party control of the House in 2010, and, in 2014, the epicenter of electoral carnage was in the coal mining regions of Kentucky and West Virginia, costing his party the Senate.   
  
  
While China has good “intentions” we get real “unemployment”. Such a deal!
"
"
This is environmentalism jumping the shark:

Click image above to play the game
I don’t know where to begin, except to say that when we see things like this, we should complain loudly and incessantly. The Australian Broadcasting Corporation has crossed a line beyond science, beyond decency, and beyond rational thought.
This is what you get after pressing “start”:
 
The screen above says: When you’re done, click on the (skull and crossbones) to find out what age you should die at so you don’t use more than your fair share of Earth’s resources!
Hat tip to CallonJim who writes:
This “kids” games at the Australian Broadcasting Corporation. Tell’s kids depending on their magical “carbon footprint” how long they should live?
The actual title is “Professor Schpinkee’s Greenhouse Calculator – find out when you should die!”
The thing I find amazing is the average foot print is 24.6 tonnes of CO2, which calculates out to 9.3 years old! Where it tells the child “YOU SHOULD DIE AT THE AGE 9.3!” Guess what age this kids games is marketed to? That’s right, 9 year olds.
What is most disgusting about this is that ABC ignores their own published Code of Practice
In section 2.12 they talk about content for children:
2.12 Content for Children. In providing enjoyable and enriching content for children, the ABC does not wish to conceal the real world from them. It can be important for the media, especially television, to help children understand and deal with situations which may include violence and danger. Special care should be taken to ensure that content which children are likely to watch or access unsupervised should not be harmful or disturbing to them.
I venture that any child who takes this carbon footprint test “unsupervised” without mommy and daddy around, and who may be old enough to read, but not old enough to understand he/she is being brainwashed by an agenda, would be “disturbed” find they should die at age nine, since just clicking through with default choices gives you that age.
Here is where you can contact the ABC and give them an inbox full of your opinion. This kind of propaganda needs to be removed.
http://www.abc.net.au/contact/contactabc.htm
UPDATE: There is a row developing in the Austrailian press over this.
UPDATE2: The New York Post highlights this site on June 1st with the headline “Enviro Mental Institution“


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9eb64813',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSomething is rotten with the GHE
By Erich Schaffer
Introduction
The greenhouse effect (GHE) is a well established theory which most people consider a solid fact, even those who are otherwise “critical” over global warming. On the other side there are some voices who “deny” the GHE with flatearther-like arguments, which seemingly only adds to the credibility of the theory. This is a very odd situation, since the are huge issues with the GHE hidden in plain sight.
“Without GHGs, the Earth would be a frozen planet with a temperature of only -18°C, or 255°K”. This definition is all too familiar to us all and the experts naming it are legion. The 255°K isthe result of a (relatively) simple formula.
(342 x ((1-0.3) / 1)  / 5.67e-8) ^0.25 = 255
342W/m2 is the amount of solar radiation (the exact number may vary), 5.67e-8 is the Stefan-Boltzmann constant and ^0.25 (or the 4th root) represents the Stefan-Boltzmann law according to which radiation is a function of temperature to the power of 4.
Black body assumption trouble
The interesting part however is (1-0.3 / 1). 0.3 is the albedo of Earth and 1-0.3 os thus the absorbtivity, which is the share of solar radiation the Earth absorbs (~70%). The 1 below the comma, which is usually omitted, represents emissivity, which is the share of LWIR emitted by the Earth relative to a perfect black body of the same temperature. In other words, it is being assumed Earth would be emitting just like a perfect black body if it were not for GHGs. And that is where the trouble starts.
The basic problem
Quite obviously there are two factors that “violate” the assumption named above.

The surface of the Earth, mainly consisting of water, is not a perfect emitter, pretty much like any real surface. Although it is not the scope of this article, it can be shown there is a significant deviation from 1 (in the 0.91 to 0.94 range). One needs to look up Fresnel equations, the refractive index of water and so on to sort out this subject.
Clouds interfere massively with LWIR emissions. Actually this is common wisdom, as “clear nights are cold nights” and most people have made the according experience. Even the IPCC states clouds would block a 50W/m2 of SW radiation, retain a 30W/m2 of LWIR and thus have a net CRE (Cloud Radiative Effect) of -20W/m2 [1]. Of course those -50W/m2 of SW CRE are already included in the formula above (part of the 30% albedo), while the 30W/m2 of LW CRE are not.

For this reason we need to make some minor corrections to the GHE as presented above. Basically Earth receives some 240 W/m2 of solar radiation (= 0.7 x 342) and is meant to emit some 390 W/m2 at 288°K at the surface. Next to a temperature of 33°K, the GHE would thus amount to about 150W/m2 respectively (=390-240).
Since in reality the surface is not a perfect emitter, the 390W/m2 are totally inaccurate. In fact it is easy to call it “fake science” whenever someone claims this number, or an even higher one. Rather we need to reduce this figure by at least 20 W/m2 to allow for a realistic surface emissivity. Next we need to allow for the 30 W/m2 that clouds provide and thus our GHE shrinks from 150 W/m2 to a maximum of only 100 W/m2 (150 – 20 – 30).
For the sake of clarity we should rename the GHE to GHGE (greenhouse gas effect) as this is the pivotal question. How much do GHGs warm up the planet? It is important so see that GHGs were attributed with their specific role in a kind of “diagnosis of exclusion”. If it were not for GHGs, what would be the temperature of Earth? Any delta to the observed temperature can then be attributed to GHGs.
Such a “diagnosis of exclusion” is always prone to failure, be it in the medical field or anywhere else. Essentially a large number of variables need to be taken into account and the slightest mistake in the process, will necessarily cause a faulty outcome. For that reason it should be considered an approach of last resort, maybe helpful to treat a patient or solve a criminal case. As a starting point in physics it is a no-go, and as we can see, it delivers wrong results. But maybe that is the reason why it was chosen in the first place. Faulty approaches give a certain freedom of creativity.
GHGE being notoriously exaggerated
Still, we have not broken any barriers so far. Yes, the GHGE is notoriously being exaggerated and anyone who claims Earth would be 255°K cold if it were not for GHGs, is either incompetent, or simply lying. You cannot excuse such a claim as “simplification”, since exaggerating the GHGE by some 50% at least is certainly beyond negligible.
On the other side, this does not deny the global warming narrative at all. One might consider downgrading climate sensitivity a bit, which would only result in climate models better matching reality. Even then, this will only put things on a healthier and more appropriate basis, eventually supporting the theory of CO2 induced global warming.
Digging deeper
So far I have not introduced anything substantially new, but only pointed out to what is known and yet constantly forgotten. Especially the CRE in its quoted magnitude is pretty much an undisputed fact of science. Although I do not know exactly what the origins of these estimates were, experts like Veerabhadran Ramanathan already zeroed in on it in the 1970s. Satellite driven projects like ERBE or CERES later confirmed and specified those estimates.
The net CRE of -20W/m2 thus can be found in the IPCC reports, NASA gives detailed satellite data on it, and even “sceptics” like Richard Lindzen name and endorse it[2]. Such a solid agreement is not just good for my argumentation above, it is also great for the GHGE itself. In fact the negative CRE is pretty much a conditio sine qua non. If clouds were not cooling the planet, the scope for GHGs might become marginal.
There are indeed some issues with the CRE I need to talk about and things are not nearly as settled as I just suggested.

Whatever experts name a net CRE of about -20 W/m2, they refer to the same sources, which are ERBE and CERES satellite data.
This are not satellite data at all, but models which are getting fed with some satellite data, among others.
These models were largely developed by the same people who predicted the negative CRE in the first place. They might not even have a (significant) GHGE if the result would not turn out how it did.
A closer look on these model results show totally inconsistent outcomes over time. Regions with massively negative local CREs turned into having positive CREs, and vice verse.[3]
The only thing which really held constant over time was the overall negative CRE of the named magnitude. Of course, that is a precondition to the GHGE and cannot be put into question, if “climate science” wants to have an agenda.

There is yet another side to it. Obviously the net CRE is the sum SW and LW CREs, which can easily be formulated as CREsw + CRElw = CREnet. Since the CRElw is what is being forgotten so notoriously (as it diminishes the GHGE), we could assume there might be a motivated tendency to minimize the CRElw. Given the logical restrictions, this can be achieved by making the CREnet as negative, and the CREsw as small as possible. In other words, there is a trinity of issues with the CRE.

The -50 W/m2 of SW CRE. This figure is pretty low as compared conventional wisdom, according to which clouds make up for about 2/3s of the albedo, or almost -70W/m2.
The net CRE of some -20 W/m2. We are going to have a look into this hereafter.
The LW CRE of +30 W/m2 which is reducing the GHGE as shown above, but for some strange reason tends to be “forgotten”.

Putting things to the test
Since the net negative CRE is “confirmed” by nothing but models of dubious nature, since logic might suggest the opposite (to cut a long story short) and the whole GHGE theory totally depends on it, this question made a perfectly legit target for fact checking. It is the one pivotal question it all boils down to. Is the CRE negative indeed and how could we possibly put it to the test?
As on my previous works in forensic science it seemed mandatory to pass by any conventional approach subject to predictable restrictions. Rather you will have to go beyond the understanding of those who might conspire so that their possible defences turn futile. And of course this would require brute force of intellect, creativity and a bit of luck to find an appropriate leverage.
At least the latter turns out to be a friendly gift by the NOAA. Under the title “QCLCD ASCII Files” the NOAA provided all METAR data from US weather stations[4]. Regrettably they pulled these valuable data from their site soon after I downloaded it, and the alternative “Global-Hourly Files” is not quite working[5].
The METAR data, as far I understand, are taken at airports and contain, next to usual meteorologic data, cloud conditions originally meant to assist aircraft operating around these airports. The data are anything but perfect for our scope and are subject to a couple of restrictions. As a rule, cloud condition is only reported up to 12,000 ft, yet individual exceptions may occur. Then this cloud condition is reported in 5 different “flavours”, which are CLR, FEW, SCT, BKN and OVC, or combinations of which. For our purpose any combination will be reduced to the maximum cloud condition.
Even if this is not an ideal data pool, it meets a lot of necessary requirements. First it is a totally independent data source, which has never been meant to be used for climate research. Second these data have been collected by many people, who may have made individual mistakes in the process, but were certainly not systemically biased. Third these data are thus “democratic” in nature, not controlled by the bottle neck of a few experts. Eventually, and that is the most important point, we need no models here, but we can look straight onto the empiric evidence.
The Result


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




First I need to tell how such basic research gives you amazing insights otherwise not available anywhere.
You will not get to see what you like to, or expect to see, but what there is. Just like Christopher Columbus searching for India and finding America, you will have to take things for face value. Analyzing the data back and forth, using different perspectives, this was not a simple look up to confirm a certain expectation, but rather a process of continuous learning. Accordingly there are lots of results giving excellent insights into the nature of clouds, or their impacts on climate respectively.

This graph was taken from Harvard’s educational site [6] on the subject. Here, like in later iterations of the ERBE / CERES modelling, the northern Pacific is meant to be one of the areas with a massively negative CRE, which are of special interest to me.
Since the Aleutian islands are US territory, my NOAA data set included 10 stations located right there.

For the years 2016 and 2017, these stations report about 325,000 valid datasets, with almost 60% of which being overcast. So it is indeed a very cloudy region.

Once we resolve the cloudiness/temperature correlation by season, we find a very typical outcome. OVC skies are correlated with lower temperatures in spring and early summer, and with higher temperatures throughout the rest of the year. This pattern has been seen in all subsets featuring distinct seasons and is due to surface temperatures lagging behind solar intensity.

It is an analogy to the day/night cycle, where clouds hold down day time temperatures, while keeping nights relatively warm. It is about the relation of incoming SW to outgoing LW radiation. As clouds interfere with both radiative fluxes, their primary effect will be relative to which of these fluxes is stronger. In spring surface temperatures lag behind solar intensity, LW emissions will be relatively weak and thus clouds are cooling. In autumn this relation naturally reverses and then clouds are warming.
Note: These “tidal effects” are a direct representation of the LW CRE. Although it goes way beyond the scope of this article, such data are very helpful in assessing the actual magnitude of the LW CRE.
A huge surprise
Finally, if we add up the above results and look at the annual average (thus seasonally adjusted), we are in for a huge surprise (or possibly no more at this point). The correlation between clouds and temperature is strictly positive. The more clouds, the warmer it is, and that is in a region where models suggest a massively negative CRE.
Obviously something is totally wrong here.

I am confident the METAR data are correct, and I am certain my analysis is correct, since I have gone over it many times and the outcome is consistent with all the different perspectives. Instead, the ERBE/CERES models are wrong when compared to empiric evidence a.k.a. “reality”. That is not much of a surprise given a track record of inconsistent results.
And as much as the Bering Sea looks like “a perfect match” to fact check these models, the problems go far beyond the region. No matter where ever I looked, a negative CRE could not be found.
Just the tip of the ice berg
Yet this is just the tip of the iceberg. Of course you need to check for biases to see how much a correlation also means causation, and there are a few. Humidity, as much as it may serve as an indicator for the assumed GHG vapour, is indeed correlated with cloudiness (78% rel. humidity with CLR, 85% with OVC), but this delta is a) influenced by rain and b) too small to explain what we see.
More importantly, this analysis is all about low clouds up to 12.000 ft and it is undisputed the net CRE turns more positive the higher up clouds are. Then there is the subject of rain chill, which makes clouds look statistically colder than they are. Finally temperatures are sluggish relative to ever changing cloud conditions and we would certainly see a larger delta in temperatures if respective cloud conditions were permanent.
Systemic GHE failure
Unlike what I named before, this is not just a scratch on the GHE theory, but systemic failure. If clouds warm the planet indeed, and all the evidence points this way, the very foundation of the theory is getting annihilated. Not that GHGs might not play a certain role in Earth’s climate, but the size of the GHGE will be only a fraction of 33°K, and one that is yet to be precisely determined.
[1] 5th AR of the IPCC, page 580
[2] https://wattsupwiththat.com/2020/06/29/weekly-climate-and-energy-news-roundup-414/
[3] https://www.researchgate.net/figure/Comparison-of-annual-mean-SW-LW-and-net-CRE-of-E55H20-E61H22-and-E63H23-to-CERES-40_fig2_335351575
[4] https://www.ncdc.noaa.gov/data-access/quick-links
[5] Documentation does not fit the data format, for some reason I am unable to locate temperature readings, the format itself is hard to read, and finally for some reason these data, station by station, do not correspond to those of the “QCLCD ASCII Files”
[6] https://www.seas.harvard.edu/climate/eli/research/equable/ccf.html


		jQuery(document).ready(function(){
			jQuery('#dd_c8e1fc71e470391995075f099bc5906a').on('change', function() {
			  jQuery('#amount_c8e1fc71e470391995075f099bc5906a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

August, 2005 — Hurricane Katrina blows into the Gulf of Mexico and blasts New Orleans to smithereens. Environmentalists quickly blame the storm on global warming — or at the very least, claim that warming will inevitably lead to more Katrina‐​like hurricanes. Although there is no clear scientific consensus on what impact a warming world might have on the frequency of big Gulf hurricanes, it’s enough to move public opinion significantly on the question of whether federal, state, and local governments ought to do something about climate change.   
  
  
May, 2006 — Al Gore’s _An Inconvenient Truth_ opens in New York and Los Angeles. The companion book becomes the #1 paperback non‐​fiction book on the New York Times bestseller list in July. The movie goes on to become the fourth highest grossing documentary in U.S. history and wins an Academy Award.   
  
  
July, 2007 — _Live Earth_ concerts to save the planet feature 150 top musical acts in 11 cities around the world. While it’s unclear how many people actually watched those concerts, _Live Earth_ set a record for on‐​line entertainment with over 15 million video streams during the live concerts alone.   
  
  
October, 2007 — Al Gore and the Intergovernmental Panel on Climate Change win the Nobel Peace Prize.   
  
  
March, 2008 — The Heartland Institute sponsors a conference in New York City to showcase scientific skepticism about the seriousness of climate change. The event is received with uncharacteristically loud derision by the mainstream media.   
  
  
Now, with all of that in mind, wouldn’t you think that the public would be growing more — not less — worried about climate change? You might, but you would be wrong. According to today’s _Energy & Environment Daily _(subscription required), a new poll conducted by Princeton Survey Research Associates and released by the John Brademas Center for the Study of Congress at New York University finds that Americans are less worried about climate change than they were a couple of years ago.   
  
  
_E &E Daily_ reports that the survey’s margin of error was +/- 3 percent. Here are the highlights:   
  
  
The percentage of Americans who said global warming requires immediate attention declined from 77 in 2006 to 69 percent today.   
  
  
The percentage of Americans who said they were “very worried” about global warming increased from 31 percent in 2006 to 39 percent in 2008. But that’s misleading; everyone gets “more worried” about everything in a presidential election year. What’s striking to me is that the rise in the number of those “very worried” about global warming was less than the rise in the number of those “very worried” about the four other issues surveyed by Brademas Center (Medicare, Social Security, and energy).   
  
  
The declining number of those who said they were “somewhat worried” about global warming more than offset the increase of those who reported being “very worried.”   
  
  
There are several possible explanations for this data. My guess is that it’s a little of each of the following.   
  
  
Explanation #1 – The public has only limited patience for “end of the world” prognostications. If the world isn’t visibly ending from whatever boogey man is said to menace said world, most of us begin to lose interest. We’re all well aware that Earth has been sentenced to doom hundreds of times over by activists of various stripes but has somehow gained a reprieve time and time again.   
  
  
Explanation #2 – The time horizon of most voters is very, very short. Getting people to voluntarily sacrifice for “the grandkids” or whomever is a near‐​impossible task. It would probably take a Katrina‐​a‐​year … and even then, that might not be enough. The mathematical certainty regarding the economic train wreck about to be visited upon “the grandkids” as a consequence of the trillions of dollars of unfunded liabilities for present federal health care and retirement programs does not engender sacrifice. It engenders shrugs and accelerated wealth transfers from the future to the present.   
  
  
Explanation #3 – Global warming, if it plays out as the IPCC suspects, will be a slow‐​moving event. Panic over climate change has to compete with panic over Islamic terrorism, panic over housing markets, panic over globalization, panic over energy prices, panic over immigration, and episodic panic over dozens of other (usually dubious) worries. Simply put, global warming has a hard time competing with all of the other items on the policy agenda.   
  
  
So conservatives, take heart. Enviros, take a valium.
"
"
Share this...FacebookTwitterBy Kirye
September mean temperature in Sweden has not been warming like alarmists said it certainly would.
Looking at data from the Japan Meteorological Agency (JMA) for the 6 stations with data going back 22 years, we see that 3 of 6 stations have seen cooling September mean temperature over the past 22 years:

Data: JMA.
Europe on the path to societal/energy suicide?
On another note FORBES reported on Prof. Fritz Vahrenholt, the leading German climate science skeptic who I had reported on recently. Here’s an excerpt:
Let us move on to our second piece of evidence, this time from the other side of the “climate emergency” aisle.  Professor Fritz Vahrenholt is a giant among environmental circles in Germany. (The country is well known as the world’s leading champion for all things environmental and for pushing Europe to “net zero emissions by 2050”.) Prof. Vahrenholt holds a doctorate in chemistry and started his professional career at the Federal Environmental Agency in Berlin (responsible for the chemical industry) before joining the Hessian Ministry of the Environment. From 1984 until 1990 he served as state secretary for environment, from 1991 till 1997 as minister for energy and environment in the state of Hamburg.
One day before the publication of the Boston Review article on October 5th, Prof Vahrenholt stated baldly in a German TV interview that climate science was “politicized”, “exaggerated”, and filled with “fantasy” and “fairy tales”. He pronounced that “The [Paris] Accord is already dead. Putin says it’s nonsense. […] The Americans are out. The Chinese don’t have to do anything. It’s all concentrated on a handful of European countries. The European Commission in massively on it. And I predict that they will reach the targets only if they destroy the European industries.” He lambasted Germany as a country “in denial when it comes to the broader global debate taking place on climate science”. He went on to characterize Europe’s recent push for even stricter emissions reduction targets to madness akin to Soviet central planning that is doomed to fail spectacularly.”

Read entire FORBES article here.



		jQuery(document).ready(function(){
			jQuery('#dd_f520153282479aa8bdd5967e8ebb475b').on('change', function() {
			  jQuery('#amount_f520153282479aa8bdd5967e8ebb475b').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Donald Trump’s cabinet of generals and billionaires looks poised to gain another controversial member: Rex Tillerson. The CEO of ExxonMobil is now the secretary of state nominee. His nomination raises several red flags, but the central one is: conflicts of interest. In that, he’s a perfect match for the Trump administration.



Tillerson faces a difficult confirmation process for a variety of reasons, including congressional Democrats’ determination to grill him on climate change issues. Yet some of the key issues cited by the media are actually less problematic than they may seem.



For one thing, while he has no governmental foreign policy experience, Tillerson has spent years running a multinational corporation so vast that various observers have likened it to a privately owned state. ExxonMobil, the world’s eighth‐​largest company, operates on six continents, and as CEO, Tillerson has successfully negotiated a number of complex and high‐​profile international deals.





Donald Trump’s secretary of state pick faces a dizzying level of conflicts of interest. These could pit national security against his personal gain.



While competitor BP was being pushed out of its Russian holdings by the government, for example, he struck an effective compromise with the energy giant Rosneft, allowing ExxonMobil to maintain its presence. Likewise, Tillerson’s effective negotiation with Venezuela meant that the company was one of the few to receive compensation — a whopping $900m — for Hugo Chavez’s expropriation of its assets.



Nor is it clear that Tillerson really is — as Senator Marco Rubio and others have argued — a close friend of Vladimir Putin. The Russian Order of Friendship, which Tillerson received in 2013, has been given to numerous Americans in recent years, and contacts with senior Russian officials are an unfortunate but necessary part of doing business in that corrupt country.



Indeed, in other circumstances, his strong working relationship with the Russian government could be highly beneficial, particularly after eight years of a strained personal relationship between US and Russian leaders.



Unfortunately, the nomination raises other problems. For one thing, it’s not actually clear what Tillerson’s views on foreign policy are. Despite concerns over other candidates for secretary of state — such as John Bolton and Mitt Romney — at least it was clear what kind of foreign policy approach they favored. We don’t know if Tillerson is hawkish or not, nor do we know his views on US alliances or international institutions.



The profit motive underlying Tillerson’s previous experience is also fundamentally different from national security interests. Tillerson may have good relations with some world leaders, but given the nature of the oil industry, many of these are poor, corrupt and authoritarian states; ExxonMobil’s recent agreements include such paragons of corruption as Russia, Guyana, Angola and Nigeria.



As a result, the relationships that Tillerson built during his time at ExxonMobil have the potential to be a double‐​edged sword. How long will his good relations with many of these leaders last when he is required to criticize their human rights records as secretary of state, or to oppose them in negotiations on security issues?



By far the biggest problem with Tillerson’s nomination is the serious conflict of interest created by his long history with ExxonMobil. Even if he cuts all direct ties to the company, he will still have a strong financial interest in its success. According to one estimate, he will still hold stock totaling around $218m, and a pension plan worth $70m. And there are many areas where Exxon’s profit motive conflicts with the US national interest.



The most obvious of these is undoubtedly US sanctions on Russia. ExxonMobil has already lost as much as $1bn from sanctions, and Tillerson has been a frequent visitor to the White House and the Department of the Treasury to lobby against them. But there are plenty of other potential clashes: a recent Exxon deal in Mexico would seem to conflict with Trump’s confrontational approach to that country, and the company last year directed a lobbying firm to monitor US energy‐​related sanctions on Iran for opportunities.



In these and many other cases, Tillerson’s personal financial interests will conflict with his duties as secretary of state. He’s certainly not alone in Trump’s cabinet in having such conflicts: the family of Elaine Chao, nominee for secretary of transportation, owns a shipping firm, and the president‐​elect has come under increasing scrutiny for his refusal to divest himself of involvement in his business empire.



Yet while Tillerson may be an excellent negotiator and a good fit for Trump’s nontraditional cabinet of businessmen and generals, these conflicts of interest call into question whether he can fulfill its duties effectively. The most important question remains unanswered: whose interests will he serve as secretary of state?
"
"

Here’s a roundup of bloggers who are writing about Cato research and commentary: 



Are you blogging about Cato, but not on the list? Drop us a line and let us know!
"
"
Share this...FacebookTwitterSuper Pugh
Faster than a speeding bullet…more powerful than a locomotive…able to leap tall buildings in a single bound! Look, it’s a bird! No it’s a plane! It’s Super-Pugh!
When evil threatens the planet Earth, then it’s Super Pugh to the rescue.
Now Super Pugh is in Nepal, where he just completed a 1 km swim across an icy 2″C lake to save the planet from, yes, you guessed it, manmade global warming read here.
One or two summers ago, publicity-monger and Mr Save-The-Earth Lewis Gordon Pugh attempted canoeing to the North Pole to draw attention to the problem of AGW (not to himself). He didn’t make it, probably too much kryptonite up there.
But now, in Nepal, according to his Hero Website, this was a “Swim for Peace. It is a plea to every nation, to do everything it can, to put a stop to climate change.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Who knows, maybe Super Pugh will get the Nobel Peace Prize this year.
How did he get to Nepal? Like everywhere else his expeditions take him, it’s up up and away – in a jet! (Surely he offsets his super carbon footprints).
Action hero Pugh is concerned that AGW is not being taken seriously enough. According to the Daily Planet, er, the Beeb: 
He urged governments to make tackling climate change a priority and said he was disappointed the issue did not feature more prominently in the UK election. 
It’s not easy being an action hero when nobody cares. Finally, here’s a video of Super Pugh in action!
My hero.
Share this...FacebookTwitter "
nan
"Controlling the epidemic of tuberculosis in English cattle is a hugely controversial issue. The role badgers play in that epidemic and how to prevent their infection spreading to cattle is also hotly contested.  The recent Bovine TB Strategy Review, known as the “Godfray Review”, described culling badgers as having only a “relatively modest benefit” in reducing the incidence of TB in cattle and described the use of “non-lethal” control of infection in badgers as “highly desirable”.   Culling badgers has been largely unpopular with the public, and since the report suggests it may not be an effective long-term solution, what are the non-lethal alternatives, and do they work?   There are three main approaches to reducing the risk that TB in badgers poses to cattle. We can reduce the number of badgers, reduce contact between badgers and cattle, or reduce infection among badgers.   The idea behind culling is simply to reduce badger numbers so that the amount of badger-to-badger and badger-to-cattle transmission falls, ideally until the infection dies out among the badgers.  The largest trial of badger culling, the Randomised Badger Culling Trial, finished around ten years ago and found that culling caused both a reduction in badger numbers and of TB in cattle. But it also found that a high proportion of the remaining badgers had TB. So once culling stops, a regrowing badger population that’s still infected might pose an even greater risk to cattle. 


      Read more:
      Badger cull didn't kill enough badgers to be effective


 Early trials have suggested another way to reduce badger populations: immunocontraception. This involves vaccinating badgers against their own reproductive hormones so they can’t get pregnant. It may prove more socially acceptable than culling.  But rolling this out is some way off. As it reduces the future birth rate rather than increasing the death rate, contraception will also take longer to reduce badger populations than culling.  Keeping cattle and badgers apart with physical barriers around farm buildings such as badger-proof gates and electric fences – an approach known as “biosecurity” – is relatively simple but can be expensive.  How the disease is transmitted in open environments, through slurry and soil movement or on contaminated farm vehicles, people and wildlife, is less well understood. However, improved biosecurity will also help control other infectious diseases. The TB Advisory Service, which provides free advice to farmers, emphasises integrating TB and other disease control programmes. Badger vaccination requires first trapping the badgers, as it uses the same injectable vaccine, BCG, as is used in humans. It is undertaken by trained and licensed volunteers in England, thereby reducing the cost.  Oral vaccines would be easier to use but are not yet licensed. Small numbers of trials have demonstrated that vaccination reduces both the number of badgers infected and the degree of disease in badgers with only partial protection. However, unlike for culling, there have been no large-scale trials of the effect that vaccinating badgers has on TB in cattle. Most badger vaccination in England has been undertaken in areas in front of the expanding cattle epidemic, where the badgers are believed to be free of bovine TB. This is because vaccination will protect against infection in susceptible badgers but will not stop the development of the disease in those already infected. But vaccination may still be effective at the population level among infected badgers by reducing transmission.  A recent study in Ireland found that vaccination reduced transmission enough that TB should die out among the badgers if reinfection from cattle is prevented. It will be interesting to see if vaccination on the edge of the English cattle epidemic has any effect on the expansion of the epidemic in either cattle or badgers. Determining this, however, will require more surveillance of TB in badgers than at present. In Ireland, the follow-up strategy to culling badgers appears to be vaccination, while a “test-vaccinate-remove” (TVR) approach, in which individual badgers are caught, tested and either vaccinated or killed, is being trialled in Northern Ireland and Wales.  TVR is as labour-intensive as vaccination but might reduce the numbers of infected as well as susceptible badgers. Unfortunately, the current diagnostic tests for TB in live badgers are not very sensitive, so some infected badgers will be left in the population. The Godfray Review discusses how combined approaches to controlling TB might have bigger effects than any single approach. Vaccination plus immunocontraception, or short culls plus vaccination, all combined with better biosecurity may be the answer.  But this would require a more joined-up approach than currently exists and an agile policy, responsive to local differences and capable of changing rapidly as new evidence becomes available. And, of course, whatever approach is used to control TB in badgers, reintroduction of infection is possible if the disease is not better controlled in cattle."
"
Share this...FacebookTwitterIn search of particles
Envirozealots are now moving against street sweepers, burning firewood and wood floors, claiming they emit dangerous aerosols. Expect microscopic aerosols to become the next environmental catastrophe.  
The Swiss online news magazine Die Weltwoche has a report by journalist Alex Reichmuth called Environmental Protection Ad Absurdum (in German). 
Environmental protection in Switzerland, like much of Europe,  has fallen into the hands of envirozealots. European ministries of environment are increasingly becoming armies of white-gloved snoopers in search of single molecules of contaminants. And the envirowacko journalists are chiming in, of course.  
In Switzerland the latest environmental catastrophe are airborne microscopic aerosols ( now joining biodiversity, ocean acidification, water consumption and climate change). It’s gotten so bad that now even environmental groups are now getting annoyed.
For example since 1988  it has been a tradition for environmental awareness group Alps Initiative to light a bonfire every August to remind people to protect the Alps from air pollution. This year, however, the event has been banned by the local environmental authorities. The reason, reports Reichmuth:  
The bonfire would harm the climate and pollute the air with microscopic aerosols.  
The Alps Initiative reacted:  
This is making a mountain out of a molehill.  


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed it is. But modern environmentalism has always been about making molehills into mountains, hasn’t it? Just look how life-giving CO2 has been made the culprit for the coming global Armageddon.  
Reichmuth serves up more ad absurdum cases.  
Another example: residents in small villages in Graubünden and in Tessin have been discovered to be suffering from microscopic aerosols emitted by homes burning wood in fireplaces in the wintertime. Yes, it’s about time to close up them romantic fireplaces in Swiss chalets.  
Even street sweepers are now deemed a microscopic-aerosol producing problem. A local newspaper wrote:  
When sucking up dirt, dangerous fine particles are emitted into the air by the sweeper’s air exhaust. And depending on the manufacturer, at alarming rates!  
Wait, it gets worse! That beautiful flooring you have in the rooms in your home? It may be emitting fine aerosols that are dangerous to your health too. Reichmuth writes:  
Anyone with wood or wood laminate floors is living dangerously. According to a German study, rooms with smooth floors produce concentrations of microscopic aerosols that are considerably higher than rooms with carpeting. Concentrations on average were even higher than Swiss daily limits. Taking into account all the victims who have died as a result of the aerosols, then we have to call manufacturers of natural wood floors and wood laminate mass murderers.  
Sounds loony, but let it be a warning of what can happen if you don’t stand up and push this movement back. Although Cap & Trade is in a coma in the US, waiting to wake up after the November elections, the EPA is waiting to swoop down and run every aspect of your lives.
Share this...FacebookTwitter "
"

When the Federal Reserve was created in 1913, its powers were limited and the United States was still on the gold standard. Today the Fed has virtually unlimited power and the dollar has no backing. Limited, constitutional government requires a rules‐​based, free‐​market monetary system with a stable‐​valued dollar. “For this reason,” F. A. Hayek wrote in 1960, “all those who wish to stop the drift toward increasing government control should concentrate their effort on monetary policy.”



To that end, the Cato Institute has launched the Center for Monetary and Financial Alternatives. By leveraging the Institute’s reputation for objective research and sound analysis, the Center will make a comprehensive economic, political, and philosophical case for reform by exploring alternative monetary arrangements. “We’ve assembled a group of scholars who will challenge the Federal Reserve in a way it hasn’t been challenged in 100 years,” Cato president John Allison says. Ultimately, the goal is to build the intellectual foundation for a freemarket banking system.



In November, following the announcement of the Center, the Institute held its 32nd Annual Monetary Conference, bringing together leading scholars and advocates for reform in order to examine the case for sound money. The event was directed, as always, by Cato vice president for monetary studies, James A. Dorn.



In his keynote address, James Grant, the founder and editor of _Grant’s Interest Rate Observer_ , declared that the need for sound money is clear and urgent. “Money is as old as the hills, and credit — the promise to pay money — is as old as trust,” he said. “Yet we still search for an answer.” Grant went on to explain that the notion of sound money is neither clear nor urgent to those who own so much of the other kind.



“I will count us victorious when the name of the chairman of the Federal Reserve Board is just as obscure as that of the chairman of the Weights and Measures Division of the Department of Commerce,” he said. “Come to think of it, the monetary millennium will arrive when the dollar reverts to a tangible weight or measure.”



Throughout the day, panelists discussed a wide range of topics — from the bitcoin revolution and the future of cryptocurrencies to the role of gold in a decentralized monetary regime — before considering the path toward fundamental reform. Judy Shelton, codirector of the Atlas Network’s Sound Money Project, explained that money is supposed to be a tool for measuring value, not a means for implementing economic and social policy. “This monetary anti‐​system we have today is anathema to free trade, to the ideals of Bretton Woods,” she said. “If America still believes in the power of free markets and the potential of free people, we need to fix what broke.”



Gerald P. O’Driscoll Jr., senior fellow at the Cato Institute, proposed the formation of a committee for monetary reform. “To get from talk to action, I propose that those committed to actual monetary reform plan to meet regularly,” he said, “not to discuss current policy but to devise a concrete plan for monetary reform.” Norbert Michel, research fellow at the Heritage Foundation, offered several near‐​term solutions — including reversing quantitative easing and removing the Fed’s regulatory role — that would complement any structural changes that came about.



In his luncheon address, Patrick Byrne, the CEO and chairman of Over​stock​.com, said that the vulnerabilities of the current system stem in part from the vulnerabilities of regulators. “They can be captured by the very same people they’re supposed to go after,” he said. Yet the intellectual climate has never been more open to a critical analysis of existing institutions, both here and abroad.
"
"

NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
Part II
By Basil Copeland and Anthony Watts
 
 
In Part I, we presented evidence of a noticeable periodicity in globally averaged temperatures when filtered with Hodrick-Prescott smoothing. Using a default value of lambda of 100, we saw a bidecadal pattern in the rate of change in the smoothed temperature series that appears closely related to 22 year Hale solar cycles. There was also evidence of a longer climate cycle of ~66 years, or three Hale solar cycles, corresponding to slightly higher peaks of cycles 11 to 17 and 17 to 23 shown in Figure 4B. But how much of this is attributable to value of lambda (λ). Here is where lambda (λ) is used in the Hodrick-Prescott filter equation:

The first term of the equation is the sum of the squared deviations dt = yt − τt which penalizes the cyclical component. The second term is a multiple λ of the sum of the squares of the trend component’s second differences. This second term penalizes variations in the growth rate of the trend component. The larger the value of λ, the higher is the penalty. 
For the layman reader, this equation is much like a tunable bandpass filter used in radio communications, where lambda (λ) is the tuning knob used to determine the what band of frequencies are passed and which are excluded. The low frequency component of the HadCRUT surface data (the multidecadal trend) looks almost like a DC signal with a complex AC wave superimposed on it. Tuning the waves with a period we wish to see is the basis for use of this filter in this excercise.
Given an appropriately chosen, positive value of λ, the low frequency trend component will minimize. This can be seen in Figure 2 presented in part I, where the value of lambda was set to 100. 

Figure 2 – click for a larger image
A lower value of lambda would result in much less smoothing. To test the sensitivity of the findings reported in Part I, we refiltered with a lambda of 7. The results are shown in Figures 3 and 4.

Figure 3 – click for a larger image 
As expected, the smoothed trend line, represented by the blue line in the upper panel of Figure 3, is no longer as smooth as the trend in the upper panel of Figure 1 from Part I. And when we look at the first differences of the less smoothed trend line, shown in Figure 4, they too are no longer as smooth as in Figure 2 from Part I. Nevertheless, in Figure 4, the correlation to the 22 year Hale cycle peaks is still there, and we can now see the 11 year Schwabe cycle as well. 


Figure 4 – click for a larger image 
The strong degree of correspondence between the solar cycle peaks and the peak rate of change in the smoothed temperature trend from HadCRUT surface temperature data is seen in Figure 5. 

Figure 5 – click for a larger image
The pattern in Figure 4, while not as eye-catching, perhaps, as the pattern in Figure 2 is still quite revealing. There is a notable tendency for amplitude of the peak rate of change to alternate between even and odd numbered solar cycles, being higher with the odd numbered solar cycles, and lower in even numbered cycles. This is consistent with a known feature of the Hale cycle in which the 22 year cycle is composed of alternating 11 year phases, referred to as parallel and antiparallel phases, with transitions occurring near solar peaks. 
Even cycles lead to an open heliosphere where GCR reaches the earth more easily. Mavromichalaki, et. al. (1997), and Orgutsov, et al. (2003) contend that during solar cycles with positive polarity, the GCR flux is doubled. This strongly implicates Galactic Cosmic Ray (GCR) flux in modulating global temperature trends. The lower peak amplitudes for even solar cycles and the higher peak amplitudes for odd solar cycles shown in Figure 4 appears to directly confirm the kind of influence on terrestrial climate postulated by Svensmark in Influence of Cosmic Rays on Earth’s Climate (1998)From the pattern indicated in Figure 4, the implication is that the “warming” of the late 20th century was not so much warming as it was less cooling than in each preceding solar cycle, perhaps relating to the rise in geomagnetic activity. 
It is thus notable that at the end of the chart, the rate of change after the peak associated with solar cycle 23 is already in the negative range, and is below the troughs of the preceding two solar cycles. Again, it is purely speculative at this point, but the implication is that the underlying rate of change in globally averaged temperature trends is moderating, and that the core rate of change has turned negative.It is important to understand that the smoothed series, and the implied rates of change from the first differences, in figures 2 and 4, even if they could be projected, are not indications of what the global temperature trend will be. 
There is a cyclical component to the change in global temperature that will impose itself over the underlying trend. The cyclical component is probably dominated by terrestrial dynamics, while the smoothed series seems to be evidence of a solar connection. So it is possible for the underlying trend to be declining, or even negative, while actual global temperature increases because of positive cyclical factors. But by design, there is no trend in the cyclical component, so that over time, if the trends indicated in Figures 2 and 4 hold, global warming will moderate, and we may be entering a phase of global cooling.
Some are probably wondering which view of the historical correspondence between globally averaged temperatures and solar cycles is the “correct” one: Figure 2 or 4? 
Such a question misconstrues the role of lambda in filtering the data. Here lambda is somewhat like the magnification factor “X” in a telescope or microscope. A low lambda (less smoothing) allows us to “focus in” on the data, and see something we might miss with a high lambda (more smoothing). A high lambda, precisely because it filters out more, is like a macroscopic view which by filtering out lower level patterns in the data, reveals larger, longer lived processes more clearly. Both approaches yield valuable insights. In Figure 2, we don’t see the influence of the Schwabe cycle, just the Hale cycle. In Figure 4, were it not for what we see in Figure 2, we’d probably miss some similarities between solar cycles 15, 16, and 17 and solar cycles 21, 22, and 23.In either case, we are seeing strong evidence of a solar imprint in the globally averaged temperature trend, when filtered to remove short term periodicities, and then differenced to reveal secular trends in the rate of change in the underlying long term tend in globally averaged temperatures. 
At one level we see clear evidence of bidecadal oscillations associated with the Hale cycle, and which appear to corroborate the role of GCR’s in modulating terrestrial climate. At the other, in figure 4B, we see a longer periodicity on the order of 60 to 70 years, correspondingly closely to three bidecadal oscillations. If this longer pattern holds, we have just come out of the peak of the longer cycle, and can expect globally average temperature trends to moderate, and increased likelihood of a cooling phase similar that experienced during the mid 20th century. 
In Lockwood and Fröhlich 2007 they state: “Our results show that the observed rapid rise in global mean temperatures seen after 1985 cannot be ascribed to solar variability, whichever of the mechanisms is invoked and no matter how much the solar variation is amplified.” . Yet, as Figure 5 demonstrates, there is a strong correlation between the solar cycle peaks and the peak rate of change in the smoothed surface temperature trend.
The periodicity revealed in the data, along with the strong correlation of solar cycles to HadCRUT surface data, suggests that the rapid increase in globally averaged temperatures in the second half of 20th century was not unusual, but part of a ~66 year climate cycle that has a long history of influencing terrestrial climate. While the longer cycle itself may be strongly influenced by long term oceanic oscillations, it is ultimately related to bidecadal oscillations that have an origin in impact of solar activity on terrestrial climate.
 
UPDATE: We have had about half a dozen people replicate from HadCRUT data the signal shown in figure 4 using FFT and traditional filters, and we thank everyone for doing that. We are currently working on a new approach to the correlations shown in figure 5, which can yield different results using alternate statistical methods. A central issue is how to correctly identify the peak of the solar cycle, and we are looking at that more closely. As it stands now, while the Hodrick-Prescott filtering works well and those results in figures 2,3, and 4 have been replicated by others, but the correlation shown in figure 5 is in question when a Rayleigh method is applied, and thus figure 5 is likely incorrect since it does not hold up under that and other statistical tests. There is also an error in the data point for cycle 11. I thank Tamino for pointing these issues out to us. 
We are continuing to look at different methods of demonstrating a correlation. Please watch for future posts on the subject.
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
References: 
 
Demetrescu, C., and V. Dobrica (2008), Signature of Hale and Gleissberg solar cycles in the geomagnetic activity, Journal of Geophysical Research, 113, A02103, doi:10.1029/2007JA012570.
Hadley Climate Research Unit Temperature (HadCRUT) monthly averaged global temperature data set (description of columns here) 
J. Javaraiah, Indian Institute of Astrophysics, 22 Year Periodicity in the Solar Differential Rotation, Journal of Astrophysics and Astronomy. (2000) 21, 167-170
Katsakina, et al., On periodicities in long term climatic variations near 68° N, 30° E, Advances in Geoscience, August 7, 2007
Kim, Hyeongwoo, Auburn University, “Hodrick-Prescott Filter” March 12, 2004 
M. Lockwood and C. Fröhlich, Recent oppositely directed trends in solar climate forcings and the global mean surface air temperature, Proceedings of the Royal Society of Astronomy doi:10.1098/rspa.2007.1880; 2007, 10th July
Mavromichalaki, et. al. 1997 Simulated effects at neutron monitor energies: evidence for a 22-year cosmic-ray variation, Astronomy and Astrophysics. 330, 764-772 (1998) 
Mavromichalaki H, Belehaki A, Rafios X, et al. Hale-cycle effects in cosmic-ray intensity during the last four cycles ASTROPHYS SPACE SCI 246 (1): 7-14 1997.
Nivaor Rodolfo Rigozo, Solar and climate signal records in tree ring width
from Chile (AD 1587–1994), Planetary and Space Science 55 (2007) 158–164
Ogurtsov, et al., ON THE CONNECTION BETWEEN THE SOLAR CYCLE LENGTH AND TERRESTRIAL CLIMATE, Geophysical Research Abstracts, Vol. 5, 03762, 2003 
Royal Observatory Of Belgium, Solar Influences Data Analysis Center, monthly and monthly smoothed sunspot number. (Description of data here)
Svensmark, Henrik, Danish Metorological Institute, Influence of Cosmic Rays on Earth’s Climate, Physical Review Letters 15th Oct. 98
Wikipedia, Hodrick-Prescott Filter January 20, 2008


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9feb48b0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Electric eels can incapacitate prey by producing a stunning 660-volt zap of electricity, but what’s really shocking is how they use that power. The mechanism of the eel’s attack was a mystery, but an experimental study published in the journal Science reveals the shocking predatory power of these fish. Scientists have discovered that electric eels use shocks to “remote control” their prey, causing involuntary spasms that reveal the target’s location and prevent its escape. Electric eels – Electrophorus electricus, technically fish rather than true eels – are one of several species of electrical fish but they are the most powerful, with around 80% of their bodies composed of specialised cells that effectively work as biological batteries. They breathe air and surface regularly to do so. A special mucous membrane in their mouths can absorb oxygen from the air, helping them to survive during the dry season. Electric eels live in muddy freshwater in and around the Amazon and Orinoco rivers and their vision is poor. To sense their environment, eels and other electrical fish therefore send out low-voltage pulses of power which they use like a radar. Electrical eels, however, have evolved a high-voltage charge to defend themselves and to capture prey.  We know how the charges work from the eel’s side of things – they start their attack when the battery cells discharge a high-frequency burst of high-voltage pulses. At 660v, these pulses have several times the power of standard mains electricity. However, until now little was known about what the eel’s electrical zaps actually did to their prey. Recent experiments have demonstrated that electric eels have evolved a precise remote control mechanism that takes advantage of their prey’s nervous system. An eel’s electrical discharges can efficiently induce muscle contractions in fish, as they mimic the electrical pulses that the fish’s own neurons send to stimulate muscle movement. The eel effectively hijacks the target’s neural pathways. Escape is prevented by further high-frequency volleys of electricity. These trigger massive, involuntary, whole-body muscle contractions in the prey. This shocking predatory strike ensures detected prey can be immobilised and seized. What’s more, there’s no safe hiding place from this electrical predator. It can emit bursts of charges that cause a massive involuntary twitch in concealed prey (think of how your arm jerks if you brush against an electric cattle-fence).  These charges ask the question: “Anything alive and tasty out there?” The induced response provides the answer, and it also reveals the prey’s location. Nature is yet to come up with a response that allow animals to fight back against the electrical discharges of electric eels. In South America electric eels are known as poraquê – the one who puts you to sleep, and their oil is used to treat rheumatism, osteoporosis, and insect and snake bites. Early attempts to understand electricity made use of electric eels. The great electrical pioneer Michael Faraday, for instance, spent a year experimenting on a less potent relative species that he had shipped over from Guyana specially.  Electric eels can also provide some festive cheer and get people all charged up for Christmas. Aquariums including ones in Japan, the US and Canada have been shocking visitors with their Christmas displays – using electric eels to power the illuminations on their Christmas trees."
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"
Share this...FacebookTwitterBy Prof. Fritz Vahrenholt
The global mean temperature of the satellite-based measurements remained almost unchanged in August compared to July. The deviation from the 30-year average (1981 to 2010) was 0.43 degrees Celsius.

Temperature measurements on land and in the sea continue to decrease, as the graph of the JRC analysis shows, especially in the southern hemisphere (blue).

Approaching La Nina
The research institutes predict with high probability a La Nina in the Pacific Ocean next winter. Therefore, a further decrease in global temperatures is expected until next spring. The following diagram shows the incipient cooling effect in the Pacific.

Share this...FacebookTwitter "
"
One of the things that happens when your work becomes well known is that people send you things to look at. Such is the case for today’s subject. Here we have a NOAA COOP station which is on the side of a mountain, well away from large cities. Only problem is, they put it right next to a parking lot.
A reader of this blog, Brad Herrick, sent me these photos of the Mt. Charleston weather station on State Route 157 west of Las Vegas.  For those that don’t know, Mt. Charleston is the large mountain to the west that overlooks Las Vegas. NOAA lists it on it’s COOP-A list, meaning that it reports for the climatic database. It’s been in operation since 1949. Its been moved 3 times, but all within about 1/2 mile as the fire station changed and grew.
According to NOAA’s MMS database, here is the description: Elevation, 7600 feet. NV DIV OF FORESTRY FIRE STN KYLE CYN OUTSIDE AND 30 MI NW PO AT LAS VEGAS NV. Topographic Details: RUGGED DEEP CANYON .25 – .5 MILE WIDE, RISING TO PEAKS 3000-5000 FEET HIGHER TO NORTH, SOUTH AND WEST A DISTANCE OF 2 TO 4 MILES. Lat/Lon 36.2597, -115.6452 , COOP ID 265400
Seems pretty rural, with a mental image of “way up in the mountains” if you were researching this station. By James Hansen’s figuring, it would also be a “lights=0” station since I doubt there is municipal street lighting for this area.
It’s certainly well enough away from the super sized Las Vegas concrete and asphalt heat island.
Here is the view from Google Earth:

click for larger view
Except for a few houses, it certainly looks “rural”.  Any researcher at NCDC or maybe a university that might use this station in some research report would certainly think this station was well away from the building/concrete/asphalt influence of bustling Las Vegas wouldn’t they?
But then we see this:

and this:

and this:

click for larger images
Unfortunately, I don’t have a time series temperature graph of this station to show you since I haven’t found a place at NCDC yet to graph COOP stations that are COOP-A. If anybody knows of such a link, please let me know. 
There’s nothing like convenient parking to convert a rural station to urban. But lets not forget the maintenance of the Stevenson Screen roof (see pic #2 -large), hillside, shade bushes, fire station building revisions, and portable storage unit. When did all that happen? We have no idea.
Surely, it’s easy to disentangle all that from the temperature record. Quick! Somebody create an adjustment equation.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1307288',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



 _The Current Wisdom_ only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper.



The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.



Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year‐​by‐​year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas” — there really is not much of it around.



The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.



Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).



So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now — assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.



The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed



At any rate, improvements to the Japanese‐​developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.



The high resolution version (MIROC3.2(hires)) was quite a doozy — responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise.



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse‐​gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.



The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high‐​resolution at 4.3°C (7.7°F) and the medium‐​resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2).



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).



Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.



The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) — more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.



And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme — which produces clouds which compare more favorably with satellite observations — projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2‐​induced warming, low clouds are now projected to retard it.



Here is how Watanabe _et al_. describe their results:



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing… .



MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).… This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes — still among the most uncertain of climate processes — is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.



Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!



 **References:**



Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report [4], available at http://​www​.ipcc​.ch.  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312–6335.  
[1] Watanabe et al. report that the sensitivity of MIROC3.2 (medres) is 3.6°C (6.5°), which is less that what was reported in the 2007 IPCC report. So 25% is likely a conservative estimate of the reduction in warming.  
[2] Whether enhanced cloudiness enhances or cancels carbon‐​dioxide warming is one of the core issues in the climate debate, and is clearly not “settled” science.  
[3] Degrees Kelvin (K) are the same as degrees Celsius (C) when looking at relative, rather than absolute temperatures.
"
"Westminster’s energy strategy to “keep the lights on” by relying on new nuclear build is looking increasingly like a recipe for economic ruin and political disarray. George Osborne, the chancellor, confirmed in this week’s Autumn Statement a co-operation agreement with a Franco-Japanese consortium to build a new plant at Moorfield in Cumbria as part of his national infrastructure plan.  There is already such an agreement in place for another plant at Wylfa Newydd in Wales, and of course a full deal agreed with the Franco-Chinese project to build Hinkley Point C in Somerset – the first new station in the UK in a generation. Yet that latter project’s huge estimated cost increase illustrates exactly what is wrong with nuclear – and why global sentiment has swung against it as the real costs become clearer.  Westminster’s claim that Hinkley Point C would cost £16 billion has been countered by experts at the EU who have placed the cost at nearer £25 billion (and note the original estimate was £10 billion). The deal involves paying twice the current price for electricity, with UK taxpayers and electricity consumers locked into a binding contract for an extraordinary 35 years.  The European Commission raised concerns that Westminster had breached state aid rules in the subsidies being offered to finance the project. Energy secretary Ed Davey’s huge sigh of relief in October, when the EC controversially gave the green light for the project, may be premature: it will be challenged by the Austrian government in the EU courts. Even if these obstacles can be surmounted, the financial risks to these kinds of projects are simply huge. Severe delays to new-build stations at Olkiluoto in Finland and Flamanville in France demonstrate a systemic problem. The Level-7 nuclear incidents at Fukushima and Chernobyl are evidence of the safety issues that are forever present – and then there is the insoluble problem of nuclear waste and the astronomical eventual decommissioning costs.  According to government figures, the waste problem at Sellafield alone hit nearly £68 billion — and that was almost two years ago. Little wonder that there is both local opposition to Hinkley Point and a fair amount of concern about nuclear power across the British public.     Areva’s failing financial performance must also be of significant concern to the project. Its share price plummeted on November 19 and is yet to recover amid a slew of profit warnings and multi-billion-euro debts.    To blame for the company’s predicament are its exposure to the French and Finnish nuclear projects, Japan’s reluctance to resume its nuclear programme and reticence in other countries, not least Germany. If this leads to a restructuring at the company, it brings into question the future of Hinkley Point. While the project is being led by EDF, Areva is providing its European Pressurised Reactor technology and has a 10% equity stake in it. Without the company’s injection of billions of pounds and its technological know-how, the project has to be in jeopardy.  Another issue is who should provide these projects. With Areva and EDF both under French state control, critics have said that the project amounts to the UK treasury writing a “blank cheque” to the French government. The same could be said of China General Nuclear Corporation and China National Nuclear Corporation, who came onboard last year.  EDF is also reputedly planning to hand over an additional and significant financial stake in Hinkley Point to other foreign corporations. Saudi Electric is reportedly in talks, while the Qataris have confirmed an interest too. British involvement in the project has been non-existent since the UK’s Centrica left a cavernous hole in the project by ceasing involvement in early 2013. Should de facto control of such an important element of our national electricity security be placed in the hands of foreign corporations?  Taken together, these sets of very deep concerns mean that nuclear can only be an option of last resort. To the astonishment of many, the Telegraph recently reported that the former UK chief scientist and nuclear “salesman”, has arrived at the same conclusion. Given this analysis, the Scottish government would appear more than justified in using its extensive planning powers to reject new nuclear build. In light of this and the fragility of future fracking prospects, Westminster would be wise to rethink its national energy policy and give more and not less support to onshore and offshore wind and other marine renewables.  To get an alternative viewpoint on nuclear power, now read this piece."
"In a shallow pool amid a mossy landscape is a trap, a tiny triggered vacuum that sucks in unexpected prey at great speed, absorbs what it needs, then ejects the empty husk of its victim. If you’ve sunk and splashed your way through a peat bog in summer, you may have caught a glimpse of the plant’s more alluring feature, the showy yellow flowers that wave above the water. Bladderworts are free-floating aquatic plants that sink back in winter to tight buds, washed along in the currents of wilder weather. They are not alone in their bizarre eating habits. There are sundews whose hundreds of pin-shaped tentacles wrap their sticky digestive juices around their prey, and butterworts, which possess the strongest glue in nature to trap hapless insects wandering over them, among the heathers and layers of sphagnum moss that make up peatland.  Peat bogs are incredible ancient landscapes; layers of dead plants, only partially decomposed due to the anaerobic conditions, turn into a spongy black soil. A single bog can be more than 1,000 years old, and it takes that time again to develop a deposit of a metre or so of new peat. It’s renewable, but not on a timescale we understand. With nutrients in short supply, everything here grows very slowly; it takes sphagnum moss a quarter of a century to grow 2.5cm. Those carnivorous plants grab at any nutrients they can to stay alive, hence trapping insects to feed themselves. Peatlands are considered the most efficient carbon sinks on Earth. The plants that grow in them capture the carbon released by the peat, maintaining an equilibrium that we cannot afford to lose. Extracted and degraded peat bogs do the opposite: they release a lot of carbon dioxide. It goes without saying that we can’t afford to destroy them while the world burns. About 10 years ago, the Department for Environment, Food and Rural Affairs (Defra) promised that it would phase out peat use in amateur horticulture by 2020. This was a voluntary scheme, and it’s predictably failed. Walk into nearly any garden centre and there’s plenty of peat on offer, from composts to growing media for houseplants, bedding plants and so on. Peat use in horticulture is by no means the only issue: there’s burning peat for fuel, and the vast quantities used in mushroom growing. But this is the bit you can influence. Stop buying peat-based compost (the ingredients are on the back), buy plants from the many good folk who already grow peat-free, and ask your local garden centre to commit to being #peatfree. You have power – use it to preserve peat."
nan
"
I happened across a NOAA internal training manual a couple of weeks ago that contained a photo of a USHCN official climate station that I thought I’d never get a photo of.  The Baltimore Customs House.
 
Baltimore USHCN station circa 1990’s photo courtesy NOAA, click for more images
What is interesting about this station, is that it is a rooftop station, like we’ve seen in San Francisco, Eureka, and many other US cities. Rooftop stations are suspected to impart a warm bias to the surface temperature records, for obvious reasons. The NWS/NOAA has been reluctant to change these stations to ground-level, wanting to keep a continuous record. The Baltimore USHCN station closed in 1999 and has not been replaced at this location.

From this single photo, and with the help of Google Earth and Microsoft Live Earth, I was able to complete this station survey, post mortem, giving it a CRN 5 rating. Below is one of the aerial photo links:
http://maps.live.com/default.aspx?v=2&cp=qjf9bk8mg9ks&style=o&lvl=2&tilt=-90&dir=0&alt=-1000&scene=7161999&encType=1
The Baltimore customs house is centered in the picture, you can see the old platform for the USHCN station is still there.

Earlier this year, www.Surfacestations.org volunteer John Goetz located an early historical photo of the Baltimore station, seen below:
 
But, this NOAA internal training manual not only cited this example photographically, they also did a correlation study that proved that the rooftop placement was actually warmer. 
Here is what NOAA said about the Baltimore USHCN station in their training manual:
Instrument exposure standards are really compromised with rooftop locations (figure 11).

Figure 11: An Early Rooftop Meteorological Station.
While the number of NOAA rooftop climate stations has remained at about 40 for the last decade, the number of private rooftop stations has grown during that period into the thousands. Rooftop exposures have an advantage of increased instrument security and good exposure for wind sensors (standard height is about 33 feet). However, there are also drawbacks. Access for maintenance can be difficult and exposure for precipitation and temperature instrumentation is clearly non-compliant, being elevated to high above the ground. Additionally the instrument exposure is usually over environmentally nonrepresentative surfaces (metal, black tar, shingle, stone etc.), while at the same time being close to a wide variety of roof surfaces which are subject to change.

No argument there. The trade off is security versus representivity of climate. But climate usually loses in a rooftop station instance.
They go on to say:
The unrepresentative-ness of rooftop temperature and precipitation data was discovered long ago after studies quantified the biases. The late Professor Helmut Landsburg, considered the “father of climatology”, stated in his 1942 “bible of climatology” textbook, “Physical Climatology” that:

“Climate derived from records of roof stations may by no means be representative of those at the ground level.”

In another published paper 28 years later, Professor Landsburg again reiterated his concern about rooftop exposures with respect to the urban warming issue: “They [rooftop stations] are certainly of little value in a full assessment of the climatic changes brought about by urbanization.”
That leads one to wonder why they kept these stations at all, let alone appoint them as “High Quality” USHCN stations for use in climate research. The Baltimore Custom House is also a GISS station. They also write:
Rooftops make good observation sites if you live work, play, or grow your food on a roof. Unfortunately, few people do any of the above. Rooftop exposures have been shown to exhibit biases towards warm temperatures (both maximum and minimum) and lower precipitation when compared to ground based stations. The warm temperature biases likely result from extreme daytime heating of artificial rooftop surfaces, reduced cooling of the roof at night, and from heat flow from within the building, especially in winter. The biases can be substantial. One limited study indicates 5 to 10 degrees on summer days with bright sun and light winds. Biases have been found to vary significantly, depending on many factors (location on the roof, color of roof, type of roof surface (rock, metal, etc.) time of year, etc when compared to standard ground-based sensors. On the flip side, if a station has been on a non-changing roof for decades, the site may have good continuity (value) for tracking climate change and variability. For some climate applications, consistency with a long record can be more important than accuracy with a shorter record. 
The best of both worlds is to have an exposure compliant, long-term station.
But the thing that really hit me was the data they compiled, comparing to other nearby stations, and thus proving the case for rooftop bias with this station:

They cite the table with:
The table to its right summarizes a comparison of 12 months of overlapping data that was collected on the rooftop and at the new relocated site (for data continuity), relocated several blocks away at ground level with other nearby standard, ground based stations. A combination of the rooftop and downtown urban siting explain the regular occurrence of extremely warm temperatures. Compared to nearby ground-level instruments and nearby airports and surrounding COOPs, it is clear that a strong warm bias exists, partially because of the rooftop location.
Maximum and minimum temperatures are elevated, especially in the summer. The number of 80 plus minimum temperatures during the one-year of data overlap was 13 on the roof and zero at three surrounding LCD airports, the close by ground-based inner Baltimore harbor site, and all 10 COOPs in the same NCDC climate zone. Eighty-degree minimum are luckily, an extremely rare occurrence in the mid-Atlantic region at standard ground-based stations, urban or otherwise. Temperatures can be elevated on roofs due to the higher solar radiation absorption and re-radiation associated with many roof surfaces including black tar, shingles, stone, and metal. During the colder months, ongoing upward heat transfer through the roof from the heated interior of the building also can contribute to the warm bias although stronger winter winds tend to create better mixing and minimize this impact.

The table shows that the rooftop station has Tmax >90°F more than twice as often  as other stations and a Tmax >100°F  13 times where no nearby station achieved it. Similarly we have this station recording a Tmin >80°F where no other stations did.
Yet amazingly, knowing all this, stations like this, and stations that have instrumental biases such as Tucson, with its parking lot placement (USHCN) and HO83 problems(GISS) still remain as part of the USHCN and GISS datasets. The official all-time high temperature record of Tucson of 117°F still stands, set by a known faulty HO83 thermometer.
In the case of Baltimore, the question is, in the plot below what really has been measured? Is it city growth, building energy use/dissipation, rooftop albedo variations, nearby building changes, or climate change? Given that it is impossible to disentangle all these things, the data, in my opinion, should be deemed compromised and discarded.
         
GISTEMP Plot of Baltimore City USHCN station #180470
In any other line of scientific study or in engineering, data that has been so badly compromised would likely be forced out by peer review, or the researchers themselves once the errors were discovered. Yet here we are today, keeping this station record for use in climatological study.
Reference: NOAA Professional Competency Unit 6 (PCU6) manual (PDF)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1894ab0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Mysterious, seemingly coordinated, drones have appeared in the past month over a number of nuclear power stations in France. We don’t know what these flights are for or who is behind them. But perhaps the most crucial question they raise is whether this now widespread technology poses a threat to nuclear facilities. Drone flights were first reported over at least 13 nuclear facilities in October.  The flights have taken place mostly at night, involving drones of different sizes and capability, from smaller models that would need to be operated within the immediate vicinity to larger ones around two meters in size, which could be controlled from kilometres away. Flights have been carried out both in isolation and concurrently, with drones flown simultaneously over nuclear facilities hundreds of miles apart. It is difficult to assess the risk posed by the recent drone flights as at this point it is unclear who is behind them and crucially what their intentions and capabilities are.  The fights are in breach of the 2.5km no-fly zones, which protect the air space around French nuclear power plants. However, nuclear operator EDF has been quick to down-play the potential threat noting, “these objects are not capable of damaging anything if they fall, nor is any object they might drop.”  Since the 1980s, French nuclear regulation has considered the risk of light aircraft crash, dictating that certain nuclear power plants had to be able to withstand a collision by an aircraft of 5.7 tonnes (the size of a small private jet). Following 9/11 the French authorities will undoubtedly have revisited the issue. However drones themselves are relatively light and slow: a nuclear plant should easily brush off the impact of a crash.  More worryingly, drones could be used to carry explosives for detonation close to the reactor or other sensitive parts of a nuclear site, although there have been no reports to date that these drones have been carrying a malicious cargo.  Although this may seem alarming, the use of explosives to cause a radiological release or disruptive plant operations will have been assessed by nuclear operators when designing onsite security systems.  A typical reactor design includes a high strength steel pressure vessel at least a foot thick to contain the reaction and a concrete containment structure several feet thick. While this is mostly in place for safety reasons, it also brings benefits in terms of security. That said this specific attack route – involving drones as a delivery vehicle – may not have been considered as the wide availability of this technology is a recent phenomenon. If so France and other countries will no doubt be in the process of updating their Design Basis Threat, a restricted document that outlines the capabilities and intentions of potential adversaries and serves as a guide against which physical protection systems are designed and evaluated. Drones could also have other malicious uses. When mounted with small cameras, they could be used to conduct reconnaissance or to test security provisions before carrying out a follow-up attack by other means. Or they could be potentially used to drop equipment onsite to help out a malicious insider. A recent case in Belgium involving the sabotage of non-nuclear systems of a power station by an employee highlights that insiders can pose a real threat.   The malicious use of drones – although unlikely – is certainly a threat worthy of consideration. However, the actors and intentions behind the French cases still remains a mystery.  A group of three model aeroplane enthusiasts were arrested and questioned by French authorities in early November. They were allegedly about to launch a basic drone (costing around €100) in the vicinity of a power plant. However, following their arrest the mysterious flights have continued, with reports suggesting those arrested were either copycats or just pursuing their hobby in an unfortunate location. More likely than a malicious group being behind the flights is that they are the work of anti-nuclear pressure groups. There have been a number of incidents in recent years involving protesters breaking in to nuclear facilities to highlight inadequate security and nuclear “risks”. These include a break in by more than 60 people earlier this year at a facility in France, a break in at a Swiss plant in March, and three break ins to Swedish nuclear plants over two years up to 2012.  In 2012 a Greenpeace activist flew into the secure area surrounding a French reactor using a motorised paraglider.  A spokesperson for Greenpeace – which has denied any connection to the recent drone flights – stated at the time that “we wanted to illustrate an external danger, like a fall of an aeroplane” onto nuclear facilities. Perhaps the use of drones represents the work of another group of activists seeking to highlight the risk that readily available drone-technology could pose to nuclear facilities.   The drone flights, as with other breaches of security  involving terrorists, spies or protesters, can have important implications for the nuclear industry. While a radiation release caused by a malicious act at a nuclear facility is unlikely, incursions and other breaches of security can impact in other ways. Events leading to outage or plant shutdown can be hugely costly. The sabotage at the Belgian power plant, for example, along with the unrelated shutdown of two other reactors is costing the operator €40m a month.  More broadly, the reputation of operators, regulators and the whole industry is at stake when security weaknesses are highlighted. It is for this reason that – whoever is behind the mysterious flights – it is in the interests of the nuclear industry and the French authorities to get to the bottom of the issue as soon as possible."
"The Paris Agreement of 2015 has a central aim to keep global temperature rise this century well below 2°C above pre-industrial levels and to “pursue efforts” to limit the temperature increase even further to 1.5°C. This is an ambitious aim – global temperatures are rapidly approaching the 1.5°C target and the 2°C limit is not far away. The path to 1.5°C requires that the world achieve zero emissions before 2050. It is imperative, therefore, that we stop burning fossil fuels, known as mitigation. However, our present trajectory suggests we’re not on track. COP24 can’t take its eye off this ball –- there is no long-term plan that doesn’t include zero fossil-carbon emissions. The scientific consensus is that we need to reach “net zero” CO₂ emissions by 2050. But to tack closer to a scenario of 1.5°C warming, COP24 should set this target for 2035. The United Nations, in the IPCC Special Report on Global Warming of 1.5ºC has accepted that there isn’t any obvious pathway to zero emissions in such a short time frame, so they have pegged their hopes on NETs – Negative Emissions Technologies. These approaches include carbon capture and storage (CCS), which involves sucking CO₂ from the air and storing it deep underground.  Carbon removal along these lines is the second imperative for COP24 in Katowice. Globally we emit around 40 billion tonnes of CO₂ annually, so net zero CO₂ by 2050 will require CO₂ removal of this scale, starting immediately. 


      Read more:
      Explainer: what is carbon capture and storage?


 But CO₂ isn’t the only problem. We emit other greenhouse gases such as methane, nitrous oxide and chlorofluorocarbons (CFCs) which all contribute to climate change. Methane is on the rise and is 84 times more potent as a greenhouse gas than CO₂. It comes from cows, and it leaks from oil wells and coal mines as “fugitive methane”. It is also seeping out of the melting permafrost in the Arctic. This is a worrying form of “positive feedback” where global warming causes the further release of gases that cause further warming.  Nitrous Oxide, which is 300 times more potent than CO₂, is rising too, caused by modern agriculture. And the concentration of refrigerant gases, such as CFCs, which are thousands of times more potent than CO₂, is not falling as fast as we’d hoped. So COP24 has a third imperative, to prevent the rise of non-CO₂ greenhouse gases. If we can stabilise non-CO₂ greenhouse emissions at present day levels we’ll be doing well, but concentrations are rising fast. All of this is going to be hard work. We’re failing to cut down our emissions, the technologies for NETs don’t exist at any meaningful scale, yet and there are no political drivers in place to enforce their deployment. There is also a real risk of a dramatic rise in methane in the near future. COP24 will have to consider emergency plans. One such plan is very controversial. There are so-called “geoengineering” technologies which can be used to cause changes in global temperatures. One of these is Solar Radiation Management (SRM), which involves injecting tiny aerosol particles high in the atmosphere where they reflect sunlight into space. We know from the eruption of Mount Pinatubo in 1991 that stratospheric aerosols caused a cooling of around 1°C over a year. The northern winter of 1992 saw a dramatic increase in sea ice and a stalling of glacial melting. SRM technologies exist and the first sun-dimming experiments are underway. There is a realistic possibility that deploying SRM can buy us some time to enact the essential measures needed to stop warming at or before 1.5°C. The discussions at COP24 must keep all options on the table, and as unpalatable as geoengineering technologies might seem, their deployment may prove to be unavoidable. The indicators are all in the danger zone. We are seeing increasing Arctic temperatures, rapid loss of Arctic sea ice, reduced Arctic reflectivity, rapidly melting ice shelves and methane release from permafrost. These are leading to rapidly rising sea levels, coastal flooding and storm surges, increased hurricane and storm activity, dry and hot conditions conducive to wildfires, and drought and crop failure. The urgency for decisive action is the imperative for COP24. The UN must press on with four major strands for meeting the Paris 1.5°C target: Reduce fossil carbon emissions. Remove carbon from the atmosphere (NETs). Halt the rise of emissions of non-CO₂ greenhouses cases (Methane, Nitrous oxide, CFCs). Investigate techniques for geoengineering, including Solar Radiation Management. All four of these must proceed simultaneously and in parallel. COP24 must make this perfectly clear. There is utmost urgency and no time to “wait and see”."
"A series of detailed maps have laid bare the scale of possible forest fires, floods, droughts and deluges that Europe could face by the end of the century without urgent action to adapt to and confront global heating. An average one-metre rise in sea levels by the end of the century – without any flood prevention action – would mean 90% of the surface of Hull would be under water, according to the European Environment Agency.  English cities including Norwich, Margate, Southend-on-Sea, Runcorn and Blackpool could also experience flooding covering more than 40% of the urban area. Across the North Sea, Dutch cities including the Hague, Rotterdam and Leiden were predicted to face severe floods from an average one metre sea-level rise, which is forecast if emissions rise enough to cause an increase in global temperature of 4C–6C above pre-industrial levels. The model does not account for the Netherlands’ extensive flood-prevention measures, although many other countries have not taken such action. Meanwhile, large areas of Spain, Portugal and France would be grappling with desertification, with the worst-affected zones experiencing a two and half-fold increase in droughts under the worst-case scenario. Hotter summers increase the risk of forest fires, which hit record levels in Sweden in 2018. If emissions exceed 4C, France, southern Germany, the Balkans and the Arctic Circle could experience a greatly increased fire risk. However, the absolute fire danger would remain highest in southern European countries, which are already prone to blazes. Further north, winters are becoming wetter. Failure to limit global heating below 2C could mean a swath of central and eastern Europe, from Bratislava in the west to Yaroslavl in the east, will be in line for sharp increase in “heavy rain events” during autumn and winter by the end of the century. In some areas of central and eastern Europe there is predicted to be a 35% increase in heavy rain events, meaning torrential downpours would be more frequent. While the climate data has been published before, this is the first time the EU-agency has presented it using detailed maps on one site. Users can zoom in on small areas, for example, to discover that one-third of the London borough of Hammersmith and Fulham could be exposed to flooding by 2071. The Copenhagen-based agency hopes the maps will reach decision-makers in governments and EU institutions, who would not usually read a lengthy EEA report on the impact of the climate emergency. “It’s very urgent and we need to act now,” said Blaž Kurnik, an EEA expert in climate change impacts and adaptation. Even if countries succeed in restricting global temperature rise, existing CO2 in the atmosphere would still have an impact, he said. “The number of extreme events and sea level rise will still continue to increase for the next decades to a century,” Kurnik said. “Sea level rise, especially, can be problematic, because it is still increasing because of past emissions and the current concentration of greenhouse gases.” The agency wants governments to focus on adapting to unavoidable global heating. “Adaptation is crucial in the next decades of the century. Even if we are able to increase the temperature by 2C, adaptation is crucial for the next decades.” The EEA has concluded it is possible to limit the rise in global temperatures to 2C above pre-industrial levels, as long as greenhouse gas concentrations peak during the next 15 to 29 years. Meeting a more demanding 1.5C limit requires concentrations to peak in the next three to 13 years. Under both scenarios, there is a 50% chance of overshooting the temperature. • This article was amended on 10 February and 14 May 2020. An earlier version said that the EEA concluded “it is possible to keep global temperatures 2C below pre-industrial levels, as long as emissions peak during the next 15 to 29 years”. That meant to say greenhouse gas concentrations, not emissions; and the 2C referred to a rise in temperature above pre-industrial levels, not the temperature below pre-industrial levels. This article was further amended because an earlier version omitted “enough to cause an increase in global temperature of” from the sentence: “… an average one metre sea-level rise, which is forecast if emissions rise enough to cause an increase in global temperature of 4C–6C above pre-industrial levels”. This has been corrected."
"When the broadcaster and naturalist David Attenborough launched the latest UN climate talks, COP24, he called for ordinary people to get involved, add their voice, and “take their seat”. A series of #takeyourseat videos were published, featuring people around the world discussing what climate change meant for them. This was a sign of something new at this COP. On Friday, its closing day, a Swedish school girl called Greta Thunberg called for a global climate strike and urged policymakers to hear the voice of the youth. Our children will suffer the consequences of the past actions of their elders and their current lack of climate action. So was the “people’s voice” truly included in the climate talks, or was it still business as usual? One way we can assess this is by looking at some of the pledges to emerge from the conference. In the final days of COP24, for instance, EU members and scores of developing countries pledged to toughen their existing commitments to cut emissions through enhanced Nationally Determined Contributions (NDCs). The NDCs are simply the post-2020 climate actions that nations intend to implement to meet the long-term goal set by the Paris Agreement. Yet, as the talks were taking place, references to human rights in the planning of NDCs were removed by negotiators, including US and Saudi Arabia. This is worrying for climate justice and human rights in general. And it’s particularly worrying for indigenous people, who have hands-on knowledge of the functioning of the natural world and who are often put in jeopardy for the benefit of private interests extracting natural resources. One problem with the power of financial resources is that they give a fake sense of legitimacy to the use of scarce natural resources. Because I pay, runs the argument, I can use it, and the act of paying detaches me from the consequences of my consumption. Hence, there is an urgent need for policymakers and those involved in financing climate to get rid of that sense of legitimacy.  Another element of the pledge is to bring in rapid change by broadening the “coalitions between governments and non-party stakeholders”. But who are the stakeholders? Who appoints them? Who are they accountable to and can we be sure they are genuinely committed to fighting climate change? Undermining the voice of the people is a key element in the failure of top-down policies to bring successful, rapid, and sustainable positive climate actions.  One way to push for more action on climate change is to empower women. And indeed, over the two weeks of COP24 meetings, we did hear the voice of women such as Joanna Sustento, a climate activist from the Philippines. Sustento lost many family members to Typhoon Haiyan in 2013, and is now fighting to hold 47 fossil fuel companies responsible for nearly a quarter of all greenhouse gases, accountable for deaths related to climate change.  We saw at the meetings the commitments of major organisations and development banks to divest from fossil fuel. But whether it will turn into further green washing, feeding speculative bubbles in financial markets again depends on that sense of legitimacy. We also heard the voice of Greta Thunberg, who created a global climate strike despite being herself at the margins of society because she is young, female and has Asperger syndrome. The elite of this world, flying high on corporate benefits and air miles, would do well to listen to the children that are now crying out for action, and to the little voice of the child they once were."
"
My good friend Jim Goodridge, former state climatologist for California, came to visit yesterday to offer some help on my upcoming trip, as well as to talk shop a bit about the state of affairs on climate change.
He had previously authored a paper that I had hoped to present on his behalf at ICCC, but unfortunately it got excluded from the schedule by an omission. Yesterday he decided to rework that paper to bring out it’s strongest point.
One of the best and simplest ways of seeing the solar connection is to look at accumulated departure. Here is Jim’s essay on the subject:
Solar – Global Warming Connection
Jim Goodridge
State Climatologist (Retired)
jdgoodridge – (at) – sbcglobal dot net
March 22, 2008
Solar irradiance has been monitored from satellites for three sunspot cycles. The sunspot numbers and solar irradiance were shown to be highly correlated. Since sunspot numbers have been increasing since 1935 the irradiance must also be increasing.
The sun was once considered to be constant in its output, hence the term “Solar Constant”. Recent observations suggest that the sun is a variable star. Observations of solar irradiance have been made with great precision from orbiting satellites since about 1978. These observations are from Wikipeda: http://en.wikipedia.org/wiki/Solar_variation
They clearly indicate that the solar irradiance varies with the historic sunspot numbers:

Click for a larger graph

Click for a larger graph:
Using this relationship, 307 years of solar irradiance is easily inferred.
Sunspot numbers since 1700 were plotted as accumulated departure from average in order to compare them with weather variables. The sunspot number index indicates a declining trend for the 1700 to 1935 period and an increase from 1935 to 2008. The eleven-year cycle is clearly visible.

An increase in sunspot activity, and by inference, irradiance since 1935 is plainly indicated.
Moderators note: And I want to also call attention to these graphs, which shows the change in solar irradiance since 1611 and Geomagnetic activity over the last 150 years:


Clearly, solar geomagnetic activity has been on the rise. There will be more interesting posts on sunpots coming in the next week or two, stay tuned -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0810d7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Humans have cut down half the trees on Earth since the dawn of agriculture – over 3 trillion of them. This huge loss holds the potential for massive reforestation today, which would protect local environments from soil loss, flash flooding and desertification and take up large quantities of atmospheric carbon dioxide. Despite these advantages, reforestation gets very little attention in our fight against climate change. Britain is ideal for tree growth due to its mild winters, plentiful rainfall, fertile soil and hill-sheltered topography, and it has growth rates higher than mainland Europe and Scandinavia.  Britain, without human interference, would be covered by mature oak woodland with some hazel, birch and pine further north. But England’s forest cover dropped as low as 5% after World War I, prompting the government to set up the Forestry Commission, which has helped increase forest cover over the last 80 years to nearly 10%. Overall, UK forest cover is now 13%, nearly 3.17m hectares. This is one of the lowest levels in Europe, which has a forest cover averaging 35%. Japan has a similar population density to the UK, but has maintained 67% forest cover. The UK government already has plans to create new habitats on 500,000 hectares of land by 2043, restoring forests, meadows and wetlands on about 2% of the UK’s area to mitigate and adapt to climate change. Each hectare that is rewilded will absorb the carbon emissions of 30 London buses or 90 cars for a year. If done correctly, this could take up a third of the annual carbon emissions of the UK.  This 2% target lacks ambition considering the government’s own Independent Panel on Forestry concluded in 2012 that the forested portion of England should rise to 15% by 2060 – a return to its Medieval level. This would mean reversing the decline in forest planting that has occurred over the last 30 years. The Climate Change Committee recommends that the UK should have a 2050 zero carbon emissions target. If reforestation occurred across the whole of the UK, increasing the total forest cover to 18% and if all bogs, grasslands, arable and horticultural lands were managed in the best way possible, then these could account for one quarter of the required cuts by 2050. If this mandate moves to a more stringent “zero carbon emissions” target for 2050, as suggested by the recent IPCC 1.5˚C report, then managing the British landscape could provide 25% of the solution.  Managing the natural landscape also has other benefits, from stabilising soils and preventing floods to ameliorating heatwaves and preserving biodiversity.  Using plants to combat climate change is an idea with plenty of supporters elsewhere in the world. Globally, mass forest restoration is already underway, with commitments across 43 countries to restore 292m hectares of degraded land to forest, ten times the area of the UK. The IPCC special report on keeping global warming below 1.5°C makes a compelling case for agriculture and forestry taking a leading role in absorbing and mitigating carbon pollution. Their recommendations include converting up to 8m km2 of pasture and up to 5m km2 of non-pasture agricultural land used for food and feed crops into at least 7m km2 used for energy crops. These energy crops will produce solid and liquid fuel that will replace fossil fuels. They also recommend aggressive global reforestation targets. One suggestion is to take the 1m km2 of forest that has been lost over the last decade and replacing it with 10 million km2 of forest by 2050. We can reduce carbon emissions from human land-use by restoring ecosystems, farming in a more sustainable way and encouraging a shift to less resource-intensive diets – a vegan diet emits a third of the carbon emissions of a meat dominated diet. Massive reforestation is not a pipe dream – there are excellent examples of it being done in the recent past. In the late 1990s environmental deterioration in China became critical, with vast areas resembling the dust bowl of the American Mid-west in the 1930s. Six large forest programmes were introduced during the late 1990s and early 2000s, targeting over 100m hectares of land for reforestation.  Grain for Green is the largest and best known of these programmes. It reduced soil erosion and desertification and stabilised local rainfall patterns. The ongoing programme also helped to alleviate poverty, as payments are made directly to farmers who set aside their land for reforestation. It has also reduced China’s grain surplus, which was depressing prices, and helped to rebalance the inequality between the eastern and western provinces. The Grain for Green programme  shows that widespread reforestation can have a very positive affect on the economy as well as the environment. There is ample opportunity and land on which to reforest. Despite the rising population of the Earth, which is projected to hit 10 billion by 2050, there is a net migration from rural areas into urban areas. Outside of cities, the world is getting wilder as people move into dense urban populations. This opens even more areas that could be considered for rewilding.  Global reforestation should be considered an essential tool to combat climate change at COP24. For the UK delegation, some ambition to revive the country’s beautiful forests is long overdue."
"
Share this...FacebookTwitterA 2006 peer-reviewed scientific paper (Inglesby et al., 2006) on pandemic mitigation policy said the efficacy of 3-feet social distancing is unknown, surgical masks do little to prevent virus droplet inhalation, and closing schools, restaurants, stores…have seriously adverse community consequences.
The paper refers to what policymakers should recommend if there is an influenza pandemic.
Keep in mind that the flu kills about 500,000 people a year worldwide and there have been some years when it has reached pandemic proportions, killing over a million people (i.e., 1957-’58, 1968-’69).
In 1918 the flu pandemic killed 50 million out of 500 million infections worldwide. At the time, that was one-third of the world’s population.
And yet at no time have we ever responded to a pandemic the way we have with COVID-19.

Image Source: Inglesby et al., 2006
Share this...FacebookTwitter "
"It wouldn’t be Christmas without a tree, but which is more sustainable – a real tree or a plastic one? You might expect anything plastic to be the least environmentally friendly option. It’s true that manufacturing plastic trees consumes a lot of energy, and so does shipping them, to the UK from where they’re commonly manufactured in China, for example. Although you can use a plastic tree for many years, most aren’t recyclable and ultimately still end up in landfill. However, real trees aren’t necessarily the greener option. The UK uses as many as 8m natural Christmas trees during the festive period each year and sadly, about 7m of these are discarded. The other million are mainly used as compost, though many people avoid this based on the assumption that the low pH of pine needles (between 3.2 and 3.8) will make the soil acidic.  Christmas trees such as the Norway spruce and Nordman fir have hundreds of thousands of pine needles which take a long time to decompose compared to other tree leaves. When they rot, they emit huge quantities of greenhouse gases. According to The Carbon Trust, the carbon footprint of a 2m-tall real Christmas tree is equivalent to 16kg of CO₂ if it ends up in landfill. That’s 100,000 tonnes of greenhouse gases from the 7m trees that end up languishing in landfills every year. A better solution would be to reuse the pine needles and the trees. My research at the University of Sheffield has been investigating whether there are useful products that we can get from pine needles and how to produce these. Like most plant biomass, 85% of a pine needle is a structurally complex polymer known as lignocellulose, which is rich in carbohydrate and aromatic compounds. The structural rigidity of lignocellulose makes it unattractive and useless in most industrial processes because of the high energy intensity needed to break it down. 


      Read more:
      Now that Christmas is over, what are you going to do with your tree?


 My research is focused on how the complex structure of this polymer can be broken down into simple industrial chemical feedstocks of high value and low molecular weight, such as sugars, organic acids and phenolics – chemicals which are important raw materials in industrial manufacturing. By a process called liquefaction which uses moderate temperatures and environmentally-friendly solvents like glycerol or water, the pine needles are converted into a liquid with a solid byproduct called bio-char. The warm solvent  helps to break down the complex chemical structure of pine needles into smaller chemical molecules, which make up the liquid. This liquid product typically results in glucose, acetic acid and phenol. Glucose is used in the production of sweeteners for food, acetic acid in making paint, adhesives and even vinegar, and phenol in the manufacture of medicines. None of the products from this process are wasted – even the bio-char can be used as a catalyst for other chemical reactions. The tree doesn’t need to be fresh either, as the process applied in this work can effectively handle both dry and wet biomass, eliminating the need for an expensive drying process. This is a key advantage of the liquefaction technique over traditional technologies such as combustion and gasification, whose efficiency depends on the moisture content of the biomass. This method also works well with other forms of biomass waste and can be used for any species of pine. An industry built on this process could convert much of the available biomass waste from food crops and forestry management into vital products. Aside from converting biomass waste into precious materials, this process adds value to otherwise less useful solvents such as crude glycerol – an unwanted byproduct from the biodiesel manufacturing industry. Using glycerol increases how much of the biomass waste can be converted to liquid product compared to the commonly used water process, known as hydrothermal liquefaction. More than 90% of pine needle mass is converted in the presence of glycerol compared to only 60% with water. The benefits of this research are huge. It can help reduce carbon emissions by decreasing dependence on imported artificial Christmas trees and limiting the amount of biomass sent to landfill. If commercially feasible, this could make industrial processes more sustainable by creating new products from something that was previously considered waste. Long after the festive period is over we could continue using this method to recycle forest and agricultural waste on a much larger scale, bringing greater benefits throughout the year."
"The prime minister, Scott Morrison, faces a fresh internal row over climate change policy, with MPs clashing over the issue in the first Coalition party room meeting of the year. After Morrison declared at the National Press Club last week that the government needed to focus on “practical” climate change action with a focus on mitigation and resilience, MPs debated how the government should reposition its policy response.  The debate followed a spill in the Nationals partyroom on Tuesday morning in which former leader Barnaby Joyce failed to topple Michael McCormack after arguing the government should do more to support coal-fired power and warning the party was at risk of losing its voice within the Coalition. In the party room meeting, Joyce also argued against “reactionary” climate policies in response to the bushfire crisis, accusing people of using the tragedy to push the “hobby horse” of climate action. He was backed by NSW National MP David Gillespie who suggested the party’s constituents did not want more action on climate policy, and it was not an issue being raised by voters. But several MPs spoke in favour of more ambitious climate action. The Liberal MP for the seat of Higgins, Katie Allen, expressed support for the “ultimate ambition of carbon neutrality” and an embrace of renewable technology to achieve that goal. Allen’s call for more action was backed by the North Sydney MP Trent Zimmerman, the member for Reid, Fiona Martin, and the Goldstein MP, Tim Wilson, who all supported the embrace of new renewable technologies to cut emissions. Sources say Wilson and Zimmerman also spoke in favour of nuclear power, commending MP Ted O’Brien for his parliamentary inquiry into the issue, which was tabled in December, and advocating for it to be further explored. Zimmerman spoke about the need to do more to address voter concern in seats such as his, pointing to the loss of the Liberal-held seat of Warringah to independent Zali Steggall at the election to highlight the strength of feeling on the issue. He is understood to have argued that the Coalition was not just a coalition of parties, but a coalition of seats with divergent views on the vexed policy matter, and that these different views needed to be respected. In response, the Queensland Nationals MP George Christensen argued his colleagues were sounding like the Greens, and the government was only in power because it won seats in Queensland based on its support for the coal sector and for its support for a new coal-fired power station in the state. This claim was rejected by some MPs. But Christensen’s view was also expressed by Matt Canavan, who addressed the party room after resigning his position from cabinet, telling colleagues the party had become the party for workers. “We have become the party of workers, workers in coalmines, workers in shipyards and workers in factories,” Canavan said. “We represent those people by fighting for their jobs, and defending their jobs.” Canavan is now calling for new coal-fired power stations to be built across the country. In an earlier meeting of just Liberal MPs, Queenslander Andrew Laming slapped down colleagues for publicly denying the science of climate change, saying it was akin to doubting “the science of the planes you fly on” coming to Canberra. He argued that while the party’s MPs should debate the policy response, it was an “impossible position” to be debating science that already underpinned the government’s emission reduction targets. The NSW senator Jim Molan was criticised for saying on ABC’s Q&A program on Monday night that climate change may not be caused by humans and declaring he was “not relying on evidence” for his climate position. Conservative Craig Kelly also came under fire for telling the BBC there was no link between Australia’s bushfire crisis and climate change. Speaking after Tuesday’s meeting, Wilson argued that most MPs agreed with Morrison that the government needed to “evolve” its policies to create the jobs of the 21st century. “We are progressively going to be cutting emissions over the cycle, and that is where most MPs are, which is taking a pragmatic, sensible approach that takes the whole of the Australian community with us,” Wilson told the ABC. Addressing the party room, Morrison urged both the Nationals and Liberals to work together, saying the Coalition had a strong track record of delivering for Australians. “We come to this joint Coalition party room to do things together,” Morrison said. Morrison faces a difficult task to reposition the Coalition on climate change after assuring voters that it wanted to “evolve” its policies and do more to reduce emissions. In January the issue was also discussed in cabinet, where Morrison’s senior ministers agreed not to bolster the government’s current emissions reduction target of 26-28% of 2005 levels by 2030. Speaking in parliament on Tuesday on the government’s response to the summer bushfires, Morrison said there was a need to “heed the lessons” of a natural disaster of such magnitude. “These fires have been fuelled by one of the worst droughts on record, changing in our climate and a build-up in fuel amongst other factors,” he said. “Our summers are getting longer, drier and hotter, that’s what climate change does, and that requires a new responsiveness, resilience and a reinvigorated focus on adaptation.”"
"
Share this...FacebookTwitterThe June SEARCH report of September sea ice outlook here shows the predictions of 16 research institutes for 2010:
According to the report:
 A quick calculation (leaving out the outliers of 3.2 million in 2009 and 1.0 million in 2010) shows an average prediction of 4.72 in June 2009, and 5.05 million this year.
On average their predictions for 2010 are 300,000 km² more than last year’s average prediction. This is probably due to the embarassment they had to endure last year, when ice melt was far less than all the institutes had predicted. Last year sea ice area bottomed out at 5.4 million km², see below:
As you can see from the above chart, they all fell short. This year they probably thought twice about making headline-grabbing claims like they did last year.
Share this...FacebookTwitter "
"If you live in a poorly insulated home, and many of us do, you could spend thousands this winter on energy bills. But our ancestors had many ways to keep snug at little or no cost. Now, thanks to modern infrared cameras and advances in environmental physics, we can understand how these methods work and measure how effective they are. The key to understanding how to keep warm is the fact you lose more heat by radiation to your surroundings than you do by convection to the air. This is why your house feels so cold when you get back from a winter break, even after you’ve turned on the central heating; though the air quickly warms up, the walls take far longer to do so and may continue to make you shiver for up to a day.  In the same way, in poorly insulated houses the inside of the external walls can be several degrees colder than the air and the internal walls, making you feel chilly.  Fortunately, there are five simple ways to overcome this and minimise your energy bills. During the day, your windows let in more radiant energy than gets out; sunlight can enter through the glass, but the window is opaque to the infrared radiation trying to escape. At night, however, single-glazed windows can get extremely cold – in my Victorian house which we try and keep at a room temperature of 20°C, an infrared camera showed internal window temperatures of as low as 7°C on a frosty night.  Even double-glazed windows aren’t great insulators and can fall to around 14°C. This results in energy losses of 50-100 watts per square metre, equivalent to running an old-fashioned light bulb.  The best way to prevent this heat loss is to close your curtains and lower your blinds immediately after dusk. They provide an extra barrier to radiant heat loss, add insulation and reduce draughts. My cheap blinds raise the internal surface temperature to 16°C and thick curtains raise it virtually to room temperature, minimising heat loss and making the room feel cosier. Solid brick or stone walls are better insulators than glass, but they still get cold and let out lots of heat. In my house the external walls fell to 16-17°C, 3-4°C cooler than the air in the room, even though they were made of 50cm thick sandstone.  Fortunately you can significantly reduce energy losses by covering them with picture or mirrors. Even a simple poster adds an extra layer of insulting air, raising internal surface temperatures by around 1°C and cutting lost energy by a quarter. Framed pictures or mirrors are better, if more expensive. Not being a Russian oligarch or a medieval baron I don’t have any carpets or tapestries to hang on my walls, but these would be even more effective.  Best of all are bookshelves. My partner is an avid collector and her old books make superb insulators. The spines of the volumes in our book-lined study are raised almost to room temperature, making it snug and warm. Thermally at least, printed books are far superior to their electronic counterparts. Doors can let in draughts, and being thin and sometimes glazed can be very poor insulators, falling to 10-15°C on cold nights. Covering your door and the surrounding wall with a thick lined door curtain can eliminate pretty much all the heat loss. Even if you can’t reduce all the heat loss from your outer walls you can still shield yourself from the cold. Our ancestors used to draw up wooden screens behind themselves and huddle up to the fire. Being at room temperature, the screens kept their backs warm, while radiation from the fire heated up their front. You could do the same, and you could even protect your face from the damaging effects of a roaring fire by using miniature fire screens, just like Georgian ladies. How warm you feel in a room depends on where you are, even though air temperature is the same throughout. You will feel warmer if you position yourself closer to the inside of the house because the cold external walls are further away. So try and place your furniture next to an internal wall.  If your desk is up against an external wall so you can look out of the window your legs will tend to get cold, though you can reduce this effect by leaning a cardboard sheet against the wall. If the head of your bed is next to a cold external wall you will be prone to getting a stiff neck, though you can counter this somewhat by using a solid headboard. The best solution, of course, is a four-poster bed, but most bedrooms just aren’t big enough. So knowing something about how heat moves can help you brave the cold winter. My experience has also shown that investigating the thermal properties of your house with an infrared camera will keep your kids amused for hours."
"
This thread debates the Miskolczi semi-transparent atmosphere model. 
The link with the easiest introduction to the subject is http://hps.elte.hu/zagoni/Proofs_of_the_Miskolczi_theory.htm


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ea1d30e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGünther Aigner released a German video with the title “Die Alpengletscher im Klimawandel: Status quo“ (The Alps glaciers in climate change: status quo).
Hat-tip: Die kalte Sonne
Today global warming alarmists insist blaming climate change on man-made CO2 emissions. Yet, everywhere we look it’s difficult to find any correlation between CO2 and warming. Pre-industrial history shows that changes in CO2 in fact followed temperature changes.
Today we look at some climate charts of the European upper Ostalpen to look for hints what may be behind the warming since the late 20th century. We know glaciers there have been receding over the recent decades.
First is a mean temperature chart of the region for the May to September period going back 133 years:

Chart cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
Plotted are data from the Austrian ZAMG and the Swiss MeteoSchwiez, 5-year smoothed (green) and the linear trend (black). Clearly there’s been a long-term warming., but the vast share of the warming occurred since the late 1970s, after a 30-year period of cooling (since the early 1940s).
What could have happened since the early 1980s?
Of course CO2 emissions rose since 1980, and summer temperatures high in the Ostalpen rose. But is there something else?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One thing that could cause summertime temperatures to rise and glaciers to melt is sunshine. So Aigner plotted the number of sunshine hours for the May-September period for each year and produced the following chart:

Source: Cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
As the above chart shows, the sun has shined considerably more over the recent decades than it used to in the 1970s, or late 19th century. Today the region sees about a whopping 200 hours more sunshine than 120 years ago! More sunshine would mean more warmth.
The plot of sunshine hours indeed looks awfully similar to the plot depicting mean temperatures.
Aigner in the video then superimposed to two plots going back 50 years. Here’s how they compare:

Number of sunshine hours (orange) compared to the mean temperature in degrees Celsius (red). Each curve is 10-year smoothed. Source: Cropped from video “Die Alpengletscher im Klimawandel: Status quo“, by Günther Aigner
The region receives over 100 hours of sunshine more today on average than it did 40 years ago. It shouldn’t be a surprise that the Alps have warmed and glaciers receded.
The real question is why is the region less cloudy today?


		jQuery(document).ready(function(){
			jQuery('#dd_cf0de1bdc5883a7b54f7c33f21051906').on('change', function() {
			  jQuery('#amount_cf0de1bdc5883a7b54f7c33f21051906').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

What explains the enormous differences in income per capita that exist across the world today? The question has been posed many times over. The gaps in prosperity that surround us in the modern age are much wider than those that motivated Adam Smith to write _The Wealth of Nations_ in 1776, which of course is where the modern discipline of economics began.



Whereas back then the gap between the richest and poorest nations was 4- or 5- fold, today it is over 40‐​fold. Why is it, then, that certain nations are distinguished from others in terms of wealth and poverty, health and sickness, food and famine? Theories abound. If you turn to the popular media — or even some respectable journals such as Science and Nature — you will most likely come across articles that argue that geographic factors are what explain these differences. Climate, soil quality, disease, and the environment have all been put forth as the determining elements of prosperity. Yet, when you look at the evidence, these geographic factors don’t seem to be all that important. The same countries that are very rich today were once poorer than others with the same soil quality, for instance.



An even more popular explanation is the importance of cultural factors. You will hear, for example, that it is the difference between Catholics and Protestants (as Max Weber argued), or perhaps between Christians and Muslims and Judeo‐​Christians that leads to economic differences. Others have focused on Asian versus non‐​Asian values, or differing social attitudes toward work. The significance of cultural factors is a popular explanation for the differences that exist between North America and the Iberian cultures of Latin America, as well.



Popular among academics and journalists is the notion that “enlightened leadership” is what matters — meaning that either leaders or their advisers have the right ideas about what drives prosperity. It’s no surprise that this has some appeal to economists, who, of course, are in the business of developing the best micro‐ and macroeconomic policies — ones believed to be so critical to a nation’s ultimate success.



However, once again, these all seem to have relatively little explanatory power. Remember that it was only four decades ago that many scholars were talking about the deleterious effects of Confucian values — the same cultural traits that are now touted as the foundation upon which Chinese growth has been built. And while economic policies that condemn nations to poverty abound, it will soon become clear that those policies are not adopted by mistake. They are adopted by design. It is not in the ignorance of leaders, in other words, that we should look for the causes of poverty. It is in their incentives. Let me explain.



 **INSTITUTIONS: INCLUSIVE VERSUS EXTRACTIVE**  
Our theory rests upon the nature of institutions — meaning the rules, both formal and informal, that govern our economic and political life. It should not come as any surprise that there are certain sets of economic institutions — property rights, enforcement of contracts, and so on — that create incentives for investment and innovation. Those institutions that create a level playing field through which a nation can best deploy its talents are referred to as “inclusive economic institutions.”



Inclusive economic institutions, however, are the exception rather than the rule. That holds true throughout history as well as around the world today. Instead, many nations today and in the past operate under extractive institutions, which do not create property rights, generate law and order, create secure contract environments, or reward innovation. They certainly do not create a level playing field, and therefore they do not encourage sustained economic growth.



As I have already mentioned, however, these extractive institutions do not develop by mistake. They are designed by the politically powerful to extract resources from the mass of society for the benefit of the few. Such institutions are in turn sustained by extractive political institutions, which concentrate power and opportunity in the hands of an elite. This elite, in essence, designs, maintains, and benefits from these extractive institutions.



So the question is: Why do these extractive institutions emerge and persist? This is where politics enters into the equation. When extractive political institutions concentrate power in the hands of the few, those groups that monopolize political power can maintain these institutions in spite of the fact that they fail to create incentives for economic growth. Let me offer an example.



 **CASE STUDY: SOUTH AMERICA**  
There is no better laboratory that demonstrates how extractive institutions emerge and persist than the New World. The Americas provide a brilliant example for understanding how different institutions form, how they become supported within different political frameworks, and how that, in turn, leads to huge economic divergences.



The economic and political institutions in the New World have been largely shaped by their colonization experience starting at the beginning of the 16th century. While the tales of Francisco Pizarro and Hernán Cortés are quite familiar, I’d like to start with Juan Díaz de Solís — a Spaniard who in 1516 initiated the colonization of the southern cone of South America, in what is today Argentina and Uruguay. Under de Solís’s leadership, three ships and a crew of 70 men founded the city of Buenos Aires, meaning “good airs.” Argentina and Uruguay have very fertile lands, with a climate that would later become the basis of nearly a century of very high income per capita because of the productivity of these areas.



The colonization of these areas itself, however, was a total failure — and the reason was that the Spaniards arrived with a given model of colonization. This model was to find gold and silver and, perhaps most importantly, to capture and enslave the Indians so that they could work for them. Unfortunately, from the colonists’ point of view, the native populations of the area, known as the Charrúas and the Querandí, consisted of small bands of mobile huntergatherers.



Their sparse population density made it difficult for the Spaniards to capture them. They also did not have an established hierarchy, which made it difficult to coerce them into working. Instead, the Indians fought back — capturing de Solís and clubbing him to death before he could make it into the history books as one of the famous conquistadors. For those that remained, there were not enough Indians to act as workhorses, and one by one the Spaniards began to die as starvation set in.



The rest of the crew moved up the perimeter to what is now known as Asunción, Paraguay. There the conquistadors encountered another band of Indians, who on the surface looked similar to the Charrúas and the Querandí. The Guaraní, however, were a little different. They were more densely settled and already sedentary. They had also established a hierarchical society with an elite class of princes and princesses, while the rest of the population worked for the benefit of the elite.



The conquistadors immediately took over this hierarchy, setting themselves up as the elite. Some of them married the princesses. They put the Guaraní to work producing food, and ultimately the remainder of de Solís’s original crew led a successful colonization effort that survived for many centuries to come.



The institutions established among the Guaraní were the same types of institutions that were established throughout other parts of Latin America: forced labor institutions with land grants for the elite Spaniards. The Indians were forced to work for whatever wages the elites would pay them. They were under constant coercive pressure — forced not only to work but also to buy what the elites offered up for sale. It is no surprise that these economic institutions did not promote economic growth. Yet it’s also no surprise that the political institutions underpinning this system persisted — establishing and continuously recreating a ruling class of elites that did not encourage economic development in Latin America.



Yet, the question still remains: Could it have been geography, culture, or enlightened leadership — rather than institutional factors — that played a critical role in the distinct fates of the two teams of explorers?



 **CASE STUDY: NORTH AMERICA**  
Roughly a thousand miles north, at the beginning of the 17th century, the model of the Virginia Company — made up of the elite captains and aristocrats who were sent to North America — was actually remarkably similar to the model of the conquistadors. The Virginia Company also wanted gold. They also thought that they would be able to capture the Indians and put them to work. But unfortunately for them, the situation they encountered was also quite similar to what the conquistadors witnessed in Argentina and Uruguay.



The joint stock companies found a sparsely populated, very mobile band of Indians who were, once again, unwilling to work in order to provide food for the settlers. The settlers therefore went through a period of starvation. However, while the Spaniards had the option of moving up north, the captains of the Virginia Company did not have this option. No such civilization existed.



They therefore came up with a second strategy. Without the ability to enslave the Indians and put them to work, they decided to import their own lower strata of society, which they brought to the New World under a system of indentured servitude. To give you a sense of this, let me quote directly from the laws of the Jamestown colony, promulgated by the governor Sir Thomas Gates and his deputy Sir Thomas Dale:



No man or woman shall run away from the colony to the Indians upon pain of death. Anyone who robs a garden, public or private or a vineyard or who steals ears of corn shall be punished with death. No member of the colony will sell or give any commodity of this country to a captain, mariner, master, or sailor to transport out of the colony or for his own private use upon pain of death.



Two things become immediately apparent in reading these laws. First, contrary to the image that English colonies sometimes garner, the Jamestown colony that the Virginia Company was chartered to establish was not a happy, consensual place. Pretty much anything the settlers could do would be punished by death. Second, the company encountered real problems that were cause for concern — namely, that it was extraordinarily difficult to prevent the settlers they brought to form the lower strata of society from running away or engaging in outside trade. The Virginia Company therefore fought to enforce this system for a few more years, but in the end they decided that there was no practical way to inject this lower stratum into their society.



Finally, they devised a third strategy — a very radical one in which the only option left was to offer economic incentives to the settlers. This led to what is known as the headright system, which was established in Jamestown in 1618. In essence, each settler was given a legal grant of land, which they were then required to work in exchange for secure property rights to that plot. But there was still one problem. How could the settlers be sure that they had secure rights to that property, particularly in an environment in which a stolen ear of corn was punishable by death?



The very next year, in order to make these economic incentives credible, the General Assembly offered the settlers political rights as well. This, in effect, allowed them to advance above the lower strata of society, to a position in which they would be making their own decisions through more inclusive political institutions.



 **LESSONS**  
These historical examples illustrate several important lessons. The first is that there is clear positive feedback between inclusive economic and political institutions. Inclusive economic institutions are not only more conducive to economic growth than extractive ones. They are also supported by, and support, inclusive political institutions, which distribute political power widely, while still achieving some amount of political centralization so as to establish law and order, the foundations of secure property rights, and an inclusive market economy.



Second, this example illustrates that none of the alternative theories has much explanatory power. The large disparities in prosperity that exist around us today formed mostly in the 19th and early 20th centuries. But why did they form? The examples we have considered give us several insights.



It wasn’t geography that caused the divergence between South and North America. If anything, much of South America had higher agricultural productivity, supporting a greater population density, at the time of colonization. But South America ended up poorer than North America. This reversal cannot be accounted for by the impact of geographic factors. It wasn’t some sort of culture either. In fact, it’s remarkable how similar the objectives and chosen methods of the Spanish and English colonialists were. Even if their religion and culture were different, they were after the same thing and they had the same way of going about getting it. But the conditions on the ground meant that the Spanish could achieve their goals and the English could not. And the divergence wasn’t related to enlightened leadership. If anything, the Spanish leaders were more successful because they could achieve what they wanted. The Virginia Company, Sir Thomas Dale, and Sir Thomas Gates could not.



Instead, the root cause of the divergence between South and North America is in the different economic and political institutions that developed in these territories. Because the Spanish were successful in setting up extractive institutions to enrich themselves and their king, the long‐​run economic development of most of their empire was hampered. Because the English failed in setting up similar extractive institutions — and instead inclusive institutions started developing there — the United States would be much better placed to take advantage of new technologies and economic opportunities come the 19th century.



The history of the Americas is illustrative because it shows how the trajectory of institutions and economic development depends on whether elites bent on setting up extractive institutions succeed or fail. But the Americas are not fully representative of the rest of the world. In many other parts of the world, extractive institutions are not so much imposed from the outside, but are created by domestic elites. The crucial part of the story, therefore — which _Why Nations Fail_ tries to explain in detail — is the process of institutional change.



 **CONCLUSION**  
A key lesson of the framework we present in Why Nations Fail is the importance of politics. Of course, it is economic institutions that determine economic incentives and the resulting allocation of resources, investment, and innovation. But it is politics that shapes how economic institutions work and how they have evolved. Most societies suffering under extractive economic institutions do so because political power is concentrated in the hands of an elite ruling under extractive political institutions.



The recent events in the Middle East and North Africa also highlight the role of politics. The Arab Spring has shaken not only Tunisia, where it started, but Egypt, Libya, Yemen, Bahrain, and Syria, even if the governments in the latter two countries are still holding onto power. The roots of discontent in these countries are economic and social, but those are in turn shaped by political factors. The general population has been repressed and excluded from political power for generations. The protesters in Tahrir Square in Egypt understood this and this is why they demanded not just handouts or concessions from the existing regime, but fundamental political change.



This all implies a simple but critical conclusion: You can’t succeed economically if you don’t get your politics right. And that’s where the difficulty lies, because there is no formula for getting politics right. This is illustrated, for example, by the challenges lying ahead for the Middle East and North Africa — in particular, Egypt and Tunisia. Do we expect democracy or extremism to triumph in Egypt? Have the events in Tahrir Square changed the nature of politics irrevocably or will a similar economic and political structure reemerge under a different guise? Have they opened the way to a new authoritarian regime under the auspices of the Muslim Brotherhood? Central though these questions are for understanding the economic trajectory of the region, unequivocal answers are not possible. It’s only the details of politics and how the contingent path of history will play out that will determine how successful politically and thus economically these nations will be.
"
"

Until now, most of the surface temperature measurement stations I’ve highlighted as substandard locations for measuring temperature accurately have been in the USA. Today, courtesy of Geoff Sherrington, we are treated to the sight of the main Australian historic site, Melbourne metropolitan, near LaTrobe St, Melbourne. He reports it has max-min temp records daily since 1855 to late 2007.
Yet look at the pictures, this station is only 2 meters from a sidewalk, and a couple of meters more from a major street intersection and voluminous traffic. Hardly the best place to measure temperature. This site demonstrates the growing trend of climate monitoring stations that have been gradually surrounded by increasingly closer urban influences, and demonstrates that the problem is not unique to the USA.
Here are some additional pictures, click for large versions.


And a satellite image of downtown Melbourne showing the intersection is available at Windows Live Maps
UPDATE: Kristen Brynes has offered a couple of photos she had available taken from different angles of the same site, see them below. Thanks Kristen.


Additionally, the Lat/Lon of this station is:
-37.8075, 144.9700
A PDF document from Australias BOM lists the METADATA for this site and is available here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea334d832',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"We know the world is warming and that, unless things drastically change, we will keep emitting more carbon. We know the two are linked. But exactly how much warmer will it become as we emit more carbon? It’s one of the most important questions we all face. It lies at the heart of the Intergovernmental Panel on Climate Change’s 5th report on climate science, which appeared in 2013. The lead author of the report, Professor Tom Stocker, identified the most important finding as the analysis of the link between cumulative carbon emissions and global mean surface warming. This question of how much warmer it will get as we emit carbon is usually only understood using highly complex climate models, including many physical, chemical and biological processes. These climate models are like extensions of weather forecast models, but projecting over the next century, rather than the next week. The climate projections are calculated for various “emissions scenarios” – defined rates of carbon emissions for every year, ranging from a best case scenario where emissions are reduced to a worst case where nothing is done and emissions keep increasing. Analysis of the climate models found a deceptively simple result: climate warming depends almost linearly on how much carbon we emit, rather than the details of the particular emission scenario. This is because the most important factor behind global warming is the total amount of carbon emitted since the pre-industrial age, not the details of the rate of carbon emitted in every particular year.  You can see this response on the graph above for how temperature has increased  versus carbon emissions since 1870. The different coloured lines show different emission scenarios with the precise timing of how much carbon is emitted over the next 100 years. However the red, orange, light and dark blue lines for the different emissions all nearly lie on top of each other in the graph. Therefore, to know how warm it will be in, say, 2100 all you need to know is the total amount of carbon emitted up to then, rather than the details of the emission scenario. In order to understand why climate warming links so simply to how much carbon we emit, we have gone back to basics, drawing on simple climate theory to understand the climate response. Our research is published in the latest edition of the journal Nature Geoscience. We derived a relatively simple equation, using global heat and carbon budgets to connect climate warming to how much carbon has been emitted since the pre-industrial era. The equation includes crucially two competing factors: how the ocean takes up heat and how the climate system takes up carbon from the atmosphere. We found that the simple link between warming and carbon emissions emerges due to how the ocean takes up heat and carbon. There is a reduction in surface warming over time from the way the ocean soaks up carbon dioxide, which is almost cancelled by the increase in surface warming from the way the ocean soaks up heat. This link means that whatever warming we experience, it will stay around for many centuries after carbon emissions are stopped. We found from our simple theory that we will experience around 1°C of warming for every million-million tonnes of carbon emitted, with an uncertainty of around half a degree. This warming range from our simple theory agrees well with earlier findings from complex climate models, thus highlighting the crucial role of the ocean in connecting warming to emissions. But our results could be amplified by other climate forcing, such as release of other greenhouse gases, including methane from direct emissions, marine hydrates or permafrost."
"A “sustainability charge” on meat to cover its environmental damage could raise billions to help farmers and consumers produce and eat better food, according to a report. The levy, which would increase the price of a steak by about 25%, would be phased in over the next decade. The report focuses on EU countries and was produced for the Tapp Coalition of health, environment and animal welfare organisations. It says “fair pricing” for meat should be included in the forthcoming European “green new deal” and so-called farm to fork strategy.  The report, produced by environmental research group CE Delft, analysed the costs of greenhouse gas emissions, other air and water pollution, and losses of wildlife associated with livestock production. It estimated that covering these costs would increase the price of beef by €0.47 (40p) per 100g. This would increase the cost of a 227g supermarket steak in the UK by about 25%. The levy on pork and chicken would be lower owing to their smaller environmental impact, at €0.36 (31p)/100g and €0.17 (14p)/100g respectively. The report suggests such charges could reduce consumption of beef in the EU by 67%, pork by 57% and chicken by 30% by 2030. As well as reducing emissions by 120m tonnes a year, the charges would raise €32bn a year for EU member states, according to the report. The Tapp Coalition said about half of this should be given to help farmers move their production away from meat, which could increase individual farm incomes by thousands of euros per year. The rest should be used to reduce the cost of fruit and vegetables, support poorer families and help developing countries deal with the climate crisis. Jeroom Remmers, a Tapp Coalition director, said: “Europeans eat roughly 50% more meat than is recommended in dietary health guidelines. [So] we could also save billions of euros every year in lower healthcare costs.” In November, three European health associations wrote to Frans Timmermans, the senior European commissioner leading the green new deal initiative. They said: “Numerous studies in recent years have shown that a shift to healthy, more plant-rich diets can deliver important health, environmental and economic benefits.” A carbon tax on high-impact food is also backed by a second report, from the Behavioural Insights Team (Bit), a social purpose company part-owned by the UK government. It further suggests making plant-based food the default choice at catered events or on flights. Recent research has shown that a huge reduction in meat-eating in rich nations is essential to tackle the climate emergency. Other work indicates that avoiding meat and dairy products is the single biggest way to reduce your environmental impact on the planet. “Including the environmental cost of animal protein in the price is a crucial element of meeting EU targets for climate, biodiversity, public health, and animal welfare,” said Prof Pier Vellinga at Wageningen University in the Netherlands and chair of the Tapp Coalition. The report from Bit, also known as the “nudge unit” set up by David Cameron in 2010, examines how governments, the food industry and campaign groups can help shift diets away from meat. As well as supporting a carbon tax on high-impact food, it says governments could lead by example, by “removing or reducing unsustainable foods from public canteens in hospitals, schools and government offices”. It also says practical cooking skills could be taught in schools and colleges. Food companies could make plant-based products the default choice, for example at catered events or on flights, the Bit report said. It also suggested marketing plant-based food as “delicious, normal, and satisfying, not as light, abstemious, or overtly healthy or vegetarian”. Another proposal is placing veggie burgers alongside their meat counterparts instead of separating them on menus or in supermarket aisles. The report said campaign groups could reduce the perceived complexity of sustainable eating by promoting clear rules of thumb, such as “red meat’s a treat”. Toby Park, the head of energy and sustainability at Bit, said: “Governments, industry and consumers around the world are more aware than ever of the need to live within our planet’s means. “While some of the solutions will come from technical advancements, there is huge potential and need to reduce our environmental impacts with some simple behaviour changes.” In the UK, the National Farmers’ Union says agriculture can become climate neutral by 2040 without cutting beef production. Instead, it says three-quarters of farming emissions can be offset by growing fuel for power stations and then capturing and burying the carbon dioxide."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




The figure below is a portion of a screen capture from the “Heat-Related Deaths” section of the EPA’s new “Climate Change Indicators” website. It is labeled “Deaths Classified as ‘Heat-Related’ in the United States, 1979–2010.”   
  
We don’t know anyone who could look at this chart and not be left with the strong impression that heat-related deaths in the United States are on the rise—apparently confirming the president’s concern about climate change and underscoring his desire to do something about it.   






  
  
But notice the asterisk at the bottom of the box. Here's the text associated with it:   




Between 1998 and 1999, the World Health Organization revised the international codes used to classify causes of death. As a result, data from earlier than 1999 cannot easily be compared with data from 1999 and later.



In other words, you shouldn’t plot pre- and post-1999 data on the same chart because the data are not comparable, lest you mislead the uninitiated reader. The EPA ignores its own warning and instead plots the two sets of not-easily-compared data side by side on the same chart, ensuring that they are compared!   
  
Such an analysis would probably grade out as an F in an undergraduate paper, but perhaps the EPA is suffering from a bit of “Noble Cause Corruption.” After all, they are trying to save us from certain death.   
  
The proper way to view the EPA chart is to put your hand over the data points on the right-hand side of the chart (1999 and onwards) and then over the data points on the left-hand side of the chart (pre-1999 data). In doing so, you’ll see that during both periods the rate of heat-related mortality does not rise.   
  
For those who want a clearer image of the truth when it comes to the effect of global warming on trends in heat-related mortality across the United States, see the figure below, taken from a brand new study by Jennifer Bobb from the Harvard School for Public Health and colleagues. The graph shows the number of heat-related deaths (for every thousand overall deaths) that result from the daily temperature being 10°F above normal, from 1987 to 2005. The trend is strongly downward—in other words, fewer deaths are associated with heat.   






_Temporal trends, from 1987 to 2005, in the excess number of deaths (per 1,000 deaths) attributable to each 10°F increase in the same day’s summer temperature, nationally in the United States (excerpted from Bobb et al., 2014)._   
  
Or as Bobb and colleagues put it:   




This study provides strong evidence that acute (e.g., same-day) heat-related mortality risk has declined over time in the US, even in more recent years. This evidence complements findings from US studies using earlier data from the 1960s through mid-1990s on community-specific mortality rates, as well as European studies that found temporal declines in heat-related mortality risk, and supports the hypothesis that the population is continually adapting to heat. [citations removed]



Not only is the risk from extreme heat declining, but so too are the actual numbers of people dying from extreme heat both in the United States and abroad (when properly standardized for demographic changes and population growth).   
  
This is opposite from what the EPA chart leads you to believe.   
  
The only way for the EPA to be so out of touch with the prevailing science is to be so on purpose.   
  
We can’t help but to think that purpose will be revealed today.   
  
**Reference:**   
  
Bobb, J.F., R.D. Peng, M.L. Bell, and F. Dominici, 2014. Heat-related mortality and adaptation in the United States, _Environmental Health Perspectives_ , http://dx.doi.org/10.1289/ehp.1307392


"
"Americans elected Donald Trump, who insisted climate change was a hoax – so it’s no surprise that since taking office he’s been all-in for the fossil fuel industry. There’s no sense despairing; the energy is better spent fighting to remove him from office. Canada, on the other hand, elected a government that believes the climate crisis is real and dangerous – and with good reason, since the nation’s Arctic territories give it a front-row seat to the fastest warming on Earth. Yet the country’s leaders seem likely in the next few weeks to approve a vast new tar sands mine which will pour carbon into the atmosphere through the 2060s. They know – yet they can’t bring themselves to act on the knowledge. Now that is cause for despair.  The Teck mine would be the biggest tar sands mine yet: 113 square miles of petroleum mining, located just 16 miles from the border of Wood Buffalo national park. A federal panel approved the mine despite conceding that it would likely be harmful to the environment and to the land culture of Indigenous people. These giant tar sands mines (easily visible on Google Earth) are already among the biggest scars humans have ever carved on the planet’s surface. But Canadian authorities ruled that the mine was nonetheless in the “public interest”. Here’s how Justin Trudeau, recently re-elected as Canada’s prime minister, put it in a speech to cheering Texas oilmen a couple of years ago: “No country would find 173 billion barrels of oil in the ground and leave them there.” That is to say, Canada, which is 0.5% of the planet’s population, plans to use up nearly a third of the planet’s remaining carbon budget. Ottawa hides all this behind a series of pledges about “net-zero emissions by 2050” and so on, but they are empty promises. In the here-and-now they can’t rein themselves in. There’s oil in the ground and it must come out. This is painfully hard to watch because it comes as the planet has supposedly reached a turning point. A series of remarkable young people (including Canadians such as Autumn Peltier) have captured the imagination of people around the world; scientists have issued ever sterner warnings; and the images of climate destruction show up in every newspaper. Canadians can see the Australian blazes on television; they should bring back memories of the devastating forest fires that forced the evacuation of Fort McMurray, in the heart of the tar sands complex, less than four years ago. The only rational response would be to immediately stop the expansion of new fossil fuel projects. It’s true that we can’t get off oil and gas immediately; for the moment, oil wells continue to pump. But the Teck Frontier proposal is predicated on the idea that we’ll still need vast quantities of oil in 2066, when Greta Thunberg is about to hit retirement age. If an alcoholic assured you he was taking his condition very seriously, but also laying in a 40-year store of bourbon, you’d be entitled to doubt his sincerity, or at least to note his confusion. Oil has addled the Canadian ability to do basic math: more does not equal less, and 2066 is not any time soon. An emergency means you act now. In fairness, Canada has company here. For every territory making a sincere effort to kick fossil fuels (California, Scotland) there are other capitals just as paralyzed as Ottawa. Australia’s fires creep ever closer to the seat of government in Canberra, yet the prime minister, Scott Morrison, can’t seem to imagine any future for his nation other than mining more coal. Australia and Canada are both rich nations, their people highly educated, but they seem unable to control the zombie momentum of fossil fuels. There’s obviously something hideous about watching the Trumps and the Putins of the world gleefully shred our future. But it’s disturbing in a different way to watch leaders pretend to care – a kind of gaslighting that can reduce you to numb nihilism. Trudeau, for all his charms, doesn’t get to have it both ways: if you can’t bring yourself to stop a brand-new tar sands mine then you’re not a climate leader. Bill McKibben is an author and Schumann distinguished scholar in environmental studies at Middlebury College, Vermont. His most recent book is Falter: Has the Human Game Begun to Play Itself Out?"
"

No doubt about it, it’s been a good month for tornadoes even by the “spinny” standards of May, when most twisters occur. Even more predictable than the development of severe storms in spring, however, is the phenomenon of people trying to tie such bad weather to global warming. Witness Tom Toles’s cartoon in the May 7 Washington Post, which intoned, “These superpowerful tornadoes are the kind of storm we’re likely to see more of with global climate change.” Who’d he get that from, Al Gore?



It has become Standard Operating Procedure in climate change hype to never bother with inconvenient facts. Tons of tornado data are only a few mouse‐​clicks away. And they show that Toles was dead wrong in his implication that the recent storms show any link to the slight warming of the atmosphere that has occurred in recent decades. In fact, just the opposite may be occurring despite a perception of increased storminess.



Two interesting facts: The number of reported tornadoes has increased for decades while the number of deaths has dropped.



What’s going on is called “radar.” Thanks to an awful 1953 tornado in Worcester, Massachusetts (far from the Oklahoma and Texas “tornado alley”), the Weather Bureau (today’s National Weather Service) went on a crash program to develop a national network of weather radar. Spearheaded by David Atlas and Ted Fujita (whose “F‐​scale” rates tornado severity on a 1–5 basis, as is done for hurricanes), meteorologists soon learned that when the radar paints a thunderstorm that looks more like a comma than a blob, there’s often a tornado buried in the curliest point.



It took several years for the original radars, known as WSR-57’s, to cover the country. But by 1970 the job was nearly complete. As more radars came online, more and more tornadoes were reported. It’s interesting that as this network stabilized, from 1970 through 1990, so did the number or tornadoes.



Beginning in 1988, a new network began to take shape that was even better at detecting potential twisters. Instead of painting a picture of a thunderstorm, the new machines, called Doppler radars and designated as WSD-88’s, actually measure the change in a storm’s velocity by tracking the movement of raindrops. When those drops start to rotate, it’s not long before there’s a tornado warning. The rotation fields often develop before the comma shape, which means more tornado warnings. This gets people’s attention, and saves more and more lives. Not surprisingly, the number of tornadoes increased again, in the 1990’s this time proportional to the number of WSD-88’s, which now blanket the nation. By the beginning of this century, with the new network now in place, the number has stabilized again.



Any reporter (or cartoonist) doing his homework might have asked if indeed the number of big storms (categories 3–5 on the Fujita scale) is increasing. The fact is that the vast majority of tornadoes are in the “weenier” classes. Only about 5 percent reach category 3 or higher. (The severity data is at  http://​www​.spc​.noaa​.gov/​a​r​c​h​i​v​e​/​t​o​r​n​a​does/. Click on, graph it up, and you’ll see that the number of severe tornadoes is dropping.)



Where does the notion that tornadoes must increase because of global warming come from? Another panel in the Post’s cartoon reads: “With energy added to the atmosphere, more frequent and intense storms are a probable outcome.”



Perhaps a refresher course in high school earth science might be in order here. Tornadoes occur because a portion of a normally quiescent thunderstorm begins to spin. That spinning is done in large part by a dip in the strong westerly winds (“jet stream” in common parlance) that sometimes penetrates the U.S. when thunderstorms are common. The jet stream is the result of the temperature contrast between the poles and the tropics. Global warming reduces this contrast (warming the poles much more than the tropics) and reduces the spin. That means fewer tornadoes, not more.



Obviously, it’s a lot hotter in June, July, and August than it is in the peak of the tornado season in May. So much for the hot‐​air‐​tornado link. And why are there so many tornadoes in Mississippi in February? 



Rather, the key ingredient that spins garden‐​variety thunderstorms into killer tornadoes, the jet stream, is missing during the hottest part of the year, having migrated north to Canada for the summer. Warm it up and the migration will start earlier, it will move further north.



That may explain why the number of severe tornadoes is declining. They may be running out of spin, unlike stories attempting to relate these destructive storms to global climate change. 
"
"
Share this...FacebookTwitterLike in many countries in Europe, politcal parties in Germany, whether right or left, are big boosters of re-engineering society in order to save it from the fantasized self-inflicted climate catastrophe. People who speak up face risk feeling the wrath of the many climate-doctrine-following drones and zombies. And as the level of absurdity reaches intolerable levels, people are indeed speaking  up.
One such person is mayor Hans-Martin Moll of the town of Zell am Harmersbach in Germany. He has written a letter addressed to Tanya Gönner, Minister of Environment in the state of Baden Wuerttemberg and a member of the conservative CDU party.  The European Institute For Climate and Energy (EIKE) features Moll’s letter here in German.
Mr Moll, who is also a CDU member,  has become very concerned about the CDU’s aimless drift, led by Angela Merkel, in the direction of “green illusions” over the last years. Chancellor Merkel is advised by alarmists like Hans Joachim Schellnhuber and Stefan Rahmstorf.
What takes the cake for Mr Moll is Tanya Gönner’s declaration that Germany’s EEG Act is a complete success, and that it ought to be continued. The EEG Act forces power companies to buy renewable energy from anyone who produces it at fixed, exhorbitant prices that are guaranteed for years. (More info on the EEG Act here). Moll writes:
Producing power with coal or nuclear reactors costs between 2.5 and 4 cents per kwh. The EEG forces the consumers and the economy to pay 43 cents per kwh for photovoltaic power, or about 15 times more than the reliable, steady supply, conventional power.
And to make this hugely subsidised power of any use, billions of euros more are needed for expanding the power grid, for adding necessary over-capacity, and for “imaginary storage technologies”, which are physcially and geographically completely illusionary.
 
You call this a success story? I call it a political swindle of the citizens. Only in a communist centrally planned economy has such a thing ever been done.
Consumers and the economy had to fork out already 12 billion euros in 2009 for a completely useless and ideological nonsense. This EEG Act which you call a success story will cost hundreds of billions of euros.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moll does not mince any words. Indeed the amount of CO2 that Germany may save by 2020 will be offset by Chinese economic growth within just a few months. It amounts to nothing. But it is a very expensive nothng.  CO2 reductions in Germany will have zero impact on the climate, assuming that added CO2 has a noticeable impact on climate. Moll writes:
With this kind of politics, the only thing that is sustainable is the harm done to the consumer, the economy and the jobs for our future generations.
This swindle must not only be reduced, it has to be eliminated completely. The same is true for wind energy.
Energy policy is going precisely in the Green parties’ direction. Their target is not the environment, rather it is the dismantling of industry.
Mr Moll concludes with:
I do hope the conservative CDU party will wake up soon, recognise this huge error, and that it will endeavour to pursue a real energy policy that is based on natural science and common sense.
I couldn’t agree more with Mr Moll. As people start speaking up, other people will start listening.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterUlli Kulke of the German online Die Welt national newspaper has written a piece: How Sceptics Are To Be Converted. He reports on the recent Global Media Forum held by German public broadcaster Deutsche Welle dubbed “The Heat Is On – Climate Change and the Media”, see here for background and here. According to Kulke the real objective of the forum:
The media are to warn the public of the dangers of climate change even more effectively and powerfully than before, and of course to make it even more clear that it’s the fault of man.
One well-attended workshop was: How To Deal With Climate Scepticism. Its own stated objective:
This workshop aims to point out what journalists must know about climate change policy, whom to trust and when to question their own professional procedures.
and warned:
Falling back on a “neutral” journalistic position can mean playing into the hands of the skeptics at the expense of the basis of life.
According to the workshop’s moderator, Bernhard Pötter of the newspaper Tageszeitung,
For journalists, climate change is the most important topic of the 21st century.
The “How To Deal With Climate Scepticism” workshop was designed to provide assistance to frustrated editors, authors and other journalists on how to best deal with the unwanted confrontation with a climate sceptic.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Oreskes’s Propaganda
One notable speaker at the workshop was Naomi Oreskes, who, according to Kulke, requests journalists eliminate the use of the word “scepticism” from their reporting. Kulke reports on Oreskes:
‘Scepticism” is too positive, and is indeed even a virtue in science. It’s better to use the word “contrarian’, which one can translate as ‘adversary’ or ‘dissenter’, says Oreskes. “Also it’s a no-no to use the term climate debate’.
‘It’s no wonder,’ complained Oreskes, ‘that people think science is still debating climate change when everywhere in newspapers one reads about a ‘debate’. Debate has long been in the history books. Climate change is a scientifically proven fact.’ It’s important for journalists to stress that the debate is over.
Ulli Kulke wonders what newspapers Oreskes could be possibly reading out in California, which would lead her to conclude the press is playing down climate change. Kulke writes:
In the years leading up to and after the last IPCC assessment report in 2007, the press and television reported daily on the coming end of the world in America and Europe.
But this has changed over the last half-year. Inconsistencies, cover-ups, big blunders and, most of all, exaggerations by climate scientists have been exposed. Some have admitted their errors. Even plots by scientists against their sceptic colleagues came to light. As a result the media have toned down their alarmism a little. And one even gets the impression that, since Climategate, journalistic principles have made a comeback. But some people have got a problem with that.
Like Oreskes.
Much to her chagrin, parts of the German press, such as Ulli Kulke, are not ready to abandon the principles of journalism. That’s good news.
Expect scepticism contrarianism to grow in Europe.
Share this...FacebookTwitter "
"We live in a time when our climate is warming more rapidly than ever before.  Rising temperature and associated changes in weather are driving shifts in the distributions of species on Earth.  Some are thriving in these new climate conditions and have even moved into new regions that were historically inhospitable. One concern for us humans is how harmful species – diseases or pests – are responding to a changing climate. An example is the malaria parasite, which is transmitted to humans by tropical mosquitos.  In the western highlands of Colombia, malaria is slowly creeping upwards in elevation as the climate warms. But these diseases don’t just directly harm humans. Some species of wildlife are being infected by diseases that are thriving in a warmer climate? Should we be concerned? In the spring of 2008 I first started noticing that large ochre sea stars (or starfish) known as Pisaster were literally turning to ooze on the western coast of Vancouver Island – both on beaches and in experimental aquariums.   It was a gruesome sight. Symptoms of wasting started with lesions or bulging of the tissues, and in some cases, the arms even dropped off.  I tried anti-fungal and bacteria agents, and even tea tree oil, yet the only way that I found to control the disease was to move infected animals to cooler temperatures. What struck me at the time was how rapidly wasting spread among my animals, with death the ultimate outcome, all in the span of few days. In particular, I found that outbreaks occurred when animals were exposed to temperatures that were warmer than they were used to.  In a scientific study where I reported these results with my collaborators, I suggested that this “sea star wasting disease” could be triggered by changing climate conditions and even lead to large-scale outbreaks. Starting in early summer of 2013 starfish starting dying in massive numbers. Dozens of sea star species were affected and millions of individuals were wiped out along the entire Pacific coast of North America – an exceptional event that captured international media attention and puzzled scientists.   A new study published in the Proceedings of the National Academy of Science has exposed the culprit. The authors present compelling evidence that a virus (Parvoviridae) is responsible for the wasting disease – they were able to infect healthy sea stars with the virus, which then leads to wasting symptoms.  They also found that the virus is common in the sea star’s environment, and is even associated with urchins, distant cousins of sea stars.  But perhaps more intriguing, the authors identify the virus in museum specimens dating back to 1942.   This paints the picture of a virus that can spread outside its host, that is not particularly picky about which host it infects, and that has been living with sea stars for over seven decades without causing the large-scale mass mortality of the past two years.   While implicating a virus as the likely cause of wasting disease is certainly a commendable achievement, we still don’t know why such a large number of sea stars over such a wide geographic area have succumbed to this disease. Has the virus changed to be more virulent, or deadly? Perhaps something in the environment has shifted, increasing the prevalence of this disease agent or its spread within wild populations. Is this disease outbreak somehow related to climate change? While we can’t answer these questions now, the scope and scale of the wasting outbreak in sea stars is unprecedented and suggests a need to pay attention to wildlife diseases – especially because many species exposed to warmer temperatures than they are adapted to are weakened and become more vulnerable to disease.   It is important to take heed of the speed and aggressiveness with which such a disease can spread and dramatically alter ecosystems as well as the potential to impact humans more directly.  Marine diseases can infect species that we depend upon for food or other resources, or even cause illness in humans such as has been observed with the Vibrio virus, which is spreading north through Europe.   Increasing outbreaks of wasting disease in sea stars will probably lead to dramatic changes in our shorelines.  Sea stars feed on algae and other marine invertebrates, helping shape the ecosystem in which they live by what they eat.   This role for sea stars was made famous by Pisaster, the original keystone species, so named because it feeds on mussels along the shoreline, preventing the mussels from taking over and creating habitats for many different species.  Populations of these sea stars have vanished in the past year and we are left to wonder about the ecological outcomes. Knowing what agent is responsible for this disease may lead to strategies to mitigate its further spread, and hopefully, to reverse the decline of starfish in the northeast Pacific. As well, scientists and managers may be able to learn how best to both detect and respond to future marine diseases."
"The circular economy is typically seen as the progressive alternative to our wasteful linear economy, where raw materials are used to make the products that feed today’s rampant consumerist hunger, which are then thrown away. The idea of the circular economy only took off in the 1980s, but this doesn’t mean that the practices at the core of a circular economy, such as repairing, recycling, refurbishing, or repurposing, are equally novel. All of these strategies have the aim of keeping materials in use – whether as objects or as their raw components – for as long as possible. And all are hardly revolutionary. The repurposing of objects and materials may be as old as tool use itself. In Palaeolithic times, smaller flint tools were made from old hand-axes. People in the Neolithic period had no problem reusing standing stones to construct their tombs, such as seen in Locmariaquer in France. Even ceramics, made from clay and therefore available in abundance, were frequently recycled. Old pottery was often ground down to powder and used in the clay for new pots. On Minoan Crete, this ceramic powder, known as grog, was also used to manufacture the mudbricks from which houses were built.   At the Bronze Age site in Hungary where I excavate, spindle whorls made from broken pot fragments turn up regularly. Large stones at this site pose an interpretative dilemma because of their continuous reuse and repurposing, from grindstone to anvil and doorstep to wall support. In fact, up until the 20th century, repair, reuse, and repurposing were common ways of dealing with material culture. The dominance of the wasteful linear economy is a real historical anomaly in terms of resource use. But we should be careful not to fall into the trap of the “noble savage”. Our ancestors were no ecological saints. They polluted their surroundings through mining, burned down entire forests, and they too created massive amounts of waste. Just look at Monte Testaccio, a large artificial hill in Rome made up entirely out of broken amphorae.  When things are in abundance, people easily accept a wasteful and exploitative attitude. But for most of the past, most things were not in abundance, and so a core practice of a circular economy was adopted. This did not happen due to ideological motivation, but out of necessity.  Archaeologists typically don’t use the terminology of the circular economy, and describe the above examples simply, as reuse. This might partly explain why the deep roots of core practices of the circular economy are not discussed more widely. The same is also true of recycling.   When one adopts a very broad definition of recycling (thinking of it, for example, as the use of previously discarded artefacts), the origins of this practice can be traced all the way back to the Palaeolithic period. But let’s focus here on the understanding of recycling as is employed today. This is a practice in which waste (used objects) is completely converted, becoming the raw material of new products. This practice of complete transformation also entered the repertoire of human behaviour far earlier than you may think. It became the core practice of an economy as long ago as the Bronze Age. From about 2500BC, prehistoric people started to combine copper and tin on a regular basis, making metal known as bronze. The mass adoption of this artificial material caused significant shifts. Societies reoriented themselves economically because making bronze meant moving materials over long distances. Connecting sources with end users led to an intensification of trade. For these reasons, the Bronze Age is considered to be a formative epoch in the formation of Europe, in which we witness the emergence of pan-European exchange networks and large-scale trade. Bronze also made people think in new ways. The process of metalworking differs markedly from other, earlier, crafts. Wood and stone carving involve the removal of material, which is why they are known as reductive technologies. Basketry, weaving, and pottery, meanwhile, are additive technologies. Bronze is different in that it is a transformative technology. The raw material is melted down to a liquid state and poured into a mould. Moulds were the very first blueprints, documenting the design of an object to be produced – and reproduced. This may not sound very exciting to us now but for the prehistoric people involved this must have a been a groundbreaking way of working materials.  Just imagine, if your stone axe broke, you could repurpose the pieces, but you would not be able to remake that axe. In contrast, if your bronze axe broke, you could remelt it and produce the same axe with the same quality, again. Recycling, as a core economic practice, was invented in the Bronze Age.   Bronze was not the first metal to be used in such a way; the origins of metal use start with pure copper being hammered into shape. But it is only at the beginning of the Bronze Age that recycling starts to take place on a large scale.  From the Middle Bronze Age onwards, all over Europe, bronze was being recycled. We know this because archaeologists have analysed the metal composition of hundreds of objects, showing the depletion of certain elements, as a result of frequent recycling.  In addition, “old” metal was traded. A shipwreck discovered off the coast of Dover carried a large amount of French bronze objects dated to 1100BC, destined to be recycled in the UK.  As a political term, we might want to keep the circular economy in the present, but the practices that are part of it have long been part of human existence. In this respect, the Bronze Age could be seen as the first example of a circular economy in practice. Bronze was a main material of this period, and its economy revolved around recycling. Recognise this, and we start seeing that it is not the circular economy that is novel. Rather, it is the linear, and wasteful economy that is the anomaly. The beauty of this is that we can put the past to good use. The core values of a circular economy are rooted in our past and in this manner, they can help shape and inspire a modern craftsmanship that fundamentally should revolve around sustainability and durability."
"Few things have loomed larger in Brexit imaginary than the stupendous trade deals the UK will get as soon as it frees itself from the thicket of European Union regulation. The government hopes to hash out deals with both the EU and the United States by the end of the year, when the transition period ends and Britain is no longer bound to Europe’s rules. It’s an ambitious goal, but not impossible. President Trump is “bullish” on a UK deal, the American ambassador Woody Johnson recently said, adding that Trump, like the prime minister, wants to “get it done”. They make it sound so easy. And it’s true that Donald Trump has so far favoured quick and dirty deals with individual countries over complex, multinational agreements. But trade negotiations will also mean facing the world’s largest economy at the bargaining table alone, with little leverage and a ticking clock. The US will push aggressively against anything that blocks American companies from doing business here, and there are few things on which the two countries are more divided than climate policy.  The UK has ambitious plans to decarbonise by 2050, while the US has little policy and no targets to speak of. So the government will struggle to come to an agreement without sacrificing its already fragile climate commitments. Trade deals are increasingly about the alignment (a very EU-ey word) of standards and regulations to allow foreign goods and services into our borders. And it won’t surprise anyone to learn that alignment with the US rarely leads to stronger regulation; instead, trade agreements often act as a powerful and undemocratic tool to erode or supersede existing protections. The common example is the fear of poorly checked, chlorine-washed American chicken being sold in the UK. But the gulf between the US and UK on climate is even larger than on food standards. Documents from a 2018 meeting, leaked in November, set out a harsh starting position: the US will not discuss the climate crisis or include the term in any deal, which means it won’t consider existing climate policies legitimate grounds for opposition to any of its demands. This suggests the American negotiators will push to weaken existing low-carbon standards, or worse – given the UK has many climate targets but little actual concrete policy as yet – lock in clauses favourable to the fossil fuel industry that can’t be easily overturned by domestic legislation. This would effectively head off climate policy before it can be written. A similar thing played out before negotiations when the UK was part of the EU: in the now stalled Transatlantic Trade and Investment Partnership (TTIP), the US pushed for wording that would prevent initiatives to help renewables compete with fossil fuels. That demand is likely to be repeated, with the American fossil fuel industry keen to get fracked gas as well as oil from tar sands into the UK. The government’s recent fracking ban could also come under pressure from US companies looking to operate here. In practically all the sectors identified by the committee on climate change as requiring decarbonisation policies – agriculture, power generation, transport – there will be a push to lock in market mechanisms, enshrine “competition”, and head off attempts at regulation. Perhaps most worryingly, the US has also indicated it will push for any agreement to include an investor-state dispute settlement – a controversial mechanism that gives foreign companies access to a supranational tribunal where they can sue entire countries within the bounds of the trade agreement but outside their own legal system. These have been used by companies to recoup “lost profits” when countries try to introduce environmental legislation: for instance, the Canadian province of Quebec was sued by an oil company under the North American Free Trade Agreement after it banned fracking in 2011. With an ISDS, a bad deal for climate would not only harm existing standards, it could also prevent climate policies being made in the future, either because they contradict the agreement or because they could open the government to trade disputes. Climate policy wouldn’t cover a product quite as viscerally unappealing as chlorinated chicken, or an institution as beloved as the NHS, both of which have focused public anger on the usually arcane process of trade negotiation. The risk with climate is that a trade deal would involve death by a thousand cuts for regulation, with changes across the entire economy – none big enough to contest on their own. It doesn’t have to work this way: the EU is currently proposing a “carbon border tax” that would force trading partners to comprehensively consider the climate impact of all their products and services, an example of how trade policy can be used to strengthen global climate efforts. But Liz Truss, the minister in charge of negotiations, and Dominic Raab, the foreign secretary, are deregulators of the highest order. Both were co-authors of the 2012 book Britannia Unchained, which criticised the UK for a “bloated state, high taxes and excessive regulation”. They maintain that the government is committed to its existing climate targets, but it’s unlikely they’ll actually back the protections we need in order to meet them. • Stephen Buranyi is a writer specialising in science and the environment"
"
Share this...FacebookTwitterThe online German Der Spiegel reported yesterday here that the EU Commission wants to accelerate cuts in CO2 emissions, but industry and government officials are saying “no!”. With economies gripped by hardship and overall growing public scepticism (see here), calls for even more draconian measures to curb CO2 emissions are ringing hollow.
German Minister of Economics Rainer Bruederle says it’s time to take a break from efforts to protect the climate:
It accomplishes nothing for environmental protection when Europe goes it alone and jobs are sent to other regions of the world.
Werner Schnappauf, Director of the German Association of Industry adds:
As long as there is no international and legally binding climate protection treaty,  industry rejects increasing the climate reduction target from 20% to 30%. There are only disadvantages for both the climate and economy if Europe rushes and goes it alone.
      The EU wants to ratchet up the target from 20% to 30% less CO2 emissions than 1990 by 2020. Other leading German government officials think they can both appease the climate gods by making human sacrifices at the Altar of Climate, and at the same time boost the economy. German Minister of the Environment Norbert Roettgen and other EU environment ministers have said they want to go ahead and require the 30% target, with or without an international treaty.
It’s good for the environment and also for the incentive to innovate, from which the German industry would greatly profit.
      According to the EU Commission, a CO2 reduction of 30% by 2020 would lead to a 0.54% drop in GDP. Can’t these people think of ways to make our lives easier for a change?
Share this...FacebookTwitter "
"Some 12m hectares of the UK is currently covered by agricultural grasslands which support a national lamb and beef industry worth approximately £3.7 billion. However, proposals have been made that this landscape should undergo radical changes to aid the country’s climate change commitments. A controversial government advisory report recently produced by the independent Committee on Climate Change calls for UK lamb and beef production to be reduced by up to 50%. It claims that by replacing grazing land with forestry the UK will be able to substantially decrease its greenhouse gas (GHG) emissions. The National Farmers Union has responded to the report stating that it has no plans to reduce livestock numbers. Lamb and beef production is an important part of the UK’s cultural heritage, and is vital for supporting rural communities. The lamb and beef industry also provides the country with a supply of high-welfare locally sourced meat. In fact, the UK is the top lamb producer and the third largest beef producer in the EU. And in 2016, the UK was 76% self-sufficient in terms of its own food production. But lamb and beef production is also the greatest contributor to agricultural GHG emissions – the CCC report states that, in 2016, lamb, beef and dairy production combined contributed to around 58% of UK agricultural emissions.  Sheep and cattle grazing is also an integral part of how upland landscapes are currently managed. This is particularly true for Scotland, where managing the upland landscape is important for supporting other industries, such as game bird production. These upland systems have great potential for afforestation – the planting of trees in previously unforested areas – though this doesn’t necessarily have to result in a decrease in livestock numbers.  Planting trees is a crucial step in the fight against climate change. Trees act as a carbon sink for CO2 and also provide a source of different biofuels products. Previous planting schemes have seen success, for example, between 1990 and 2010 the area of the UK covered by woodland increased from 2.6 to 2.8 million hectares. But grazing land need not be taken away for the sake of this environmental initiative. Afforestation plans can be sensitive to the aforementioned socioeconomic and cultural factors if a balanced approach is taken. So what can be done? Agroforestry might be a way to meet the Committee on Climate Change’s recommendation to release between three to seven million hectares of grassland for afforestation without affecting the UK’s food supply.  Under agroforestry schemes, new woodlands are grown and existing trees are cultivated on farmlands. The aim is to optimise farming systems by incorporating woodland into them rather than replacing grazing land with trees. Planting trees and hedgerows improves grass growth, protects against flooding and topsoil erosion, increases farmland biodiversity and provides a source of natural shelter for livestock. And if the trees are used for biofuel or timber they can provide additional farm income.  Agroforestry schemes can improve animal welfare too. The 2018 lambing season resulted in an unprecedented lamb mortality rate. But it has been shown that, by providing a source of natural shelter, lamb mortality rates can be reduced by up to 50% during inclement weather. Projects like this are already in place, for example the Welsh government’s Glastir scheme. Launched in 2012, this pan-Wales sustainable land management scheme rewards farmers financially for adhering to environmental guidelines. Though it must be noted that while Glastir has proven more effective than previous agri-environmental schemes, it has been criticised for its lack of measureable outcomes and its limited uptake by Welsh farmers. With Brexit looming, now is the perfect time for agricultural reform as the country revisits current land use policies. As an industry that is currently so reliant on EU subsidies, there is a strong incentive to optimise production methods. Government discussions are already well under way over how to bring together the agriculture and forestry sectors in order to better manage pastoral landscapes. If agroforestry is incorporated in to these new agricultural policies and subsidy schemes there will be huge benefits for farmers, conservationists, the general public and the livestock they rely on."
nan
"
Share this...FacebookTwitterFrom about 80,000 to 20,000 years ago, Greenland temperatures abruptly warmed by about 10°C in just a few decades on at least 20 occasions. And then, about 870 to 1,500 years later, CO2 rose. 
Li and Born (2019) document 8-16°C climate warmings (Dansgaard-Oeschger events) in Greenland that extended to both hemispheres between about 80 and 20 thousand years ago. (Though global in scope, temperature changes were less pronounced outside Greenland.)
These abrupt warmings occurred within decades (or less). It has been suggested the warm-ups may have required no external forcing, as they’re considered an “unforced oscillation”.

Image Source: Li and Born (2019)
A new study (Shin et al., 2020) suggests the about 1,000 years after these warming events occurred, CO2 concentrations rose.
Despite the millennial-scale duration of this lag relative to the decadal-scale temperature changes, there are many who believe CO2 changes are a driver of warming.
“However, the CO2 decrease did not always start at exactly the same time as the onset of the DO warming, and the lag itself varied. For example, during Marine Isotope Stage (MIS) 3, atmospheric CO2 maxima lagged behind abrupt temperature change in Greenland by 870±90 yrs. During MIS 5, the lag of atmospheric CO2 maxima with respect to abrupt temperature warming in the NH was only about 250±190 yrs (Bereiter et al., 2012). … During MIS 6d which corresponds to CDM 6d.1 and 6d.2, CO2 concentrations show a much slower increase over a duration of ~3.3 kyr. Here, CO2 lags behind the onset of the NH abrupt warming by 1,500±280 yrs and 1,300±450 yrs, respectively (1,400± 375 yrs on average).”

Image Source: Shin et al., 2020
Share this...FacebookTwitter "
"

There is a lot not to like about the Quadrennial Defense Review, which comes out today (the _National Journal_ posted a leaked copy Friday). Like past QDRs, this one uses vague, trendy ideas about international relations to inflate threats and justify our massive defense budget. As usual, we hear the evidence‐​free claims that non‐​state actors are getting more powerful and that the world is getting more complex and unpredictable (“change continues to accelerate”). I believe that states are hanging onto or even gaining power relative to other sorts of social organizations and that the world is no less predictable than it was in 1900 or 1950. The QDR also says that climate change is a national security problem. That’s a popular line, which as near as I can tell is a marketing gimmick. Then there the usual tripe about how great our alliances are, how strategic every country with a Marine in it is, how terrific interagency cooperation is, and so forth.   
  
  
The good news is that it doesn’t really matter. Newspapers confuse the QDR with law, but it is closer to PR. It’s like a particularly important speech. It sells what Secretary of Defense is selling and justifies what the Department of Defense does. Because it comes in part from agencies it is supposed to guide, it rationalizes rather than leads. Because it is largely a consensus document, it says only what half of the Pentagon can agree on—various strains of mush. Can anyone explain what past QDR’s have accomplished? I think nothing. Sure, there are interesting tidbits about forces structure plans, but these are in the budget documents too. At best it causes DoD to justify itself, giving us analysts something to argue about.   
  
  
The administration’s proposed defense budget, also being released today, matters much more to policy. It reveals more about the nation’s defense strategy than the vacuous documents that purport to do so.   
  
  
Policy types love strategy documents because they are mostly technocratic idealists. They want government polices to be made by rational processes that reveal national interests, which are then laid out in plans like the QDR. They want policy to be like science. But democratic government is the push and pull of competing ideologies and interests. Public plans or strategies are part of that process. Congress should thank DoD for these mind‐​numbing 120 pages, throw them away, and focus on the budget.
"
"

_Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Global warming buffs have been fond of claiming that the roaring winds of Typhoon Haiyan were the highest ever measured in a landfalling tropical cyclone, and that therefore (?) this is a result of climate change. In reality, it’s unclear whether or not it holds the modern record for the strongest surface wind at landfall.   
  
This won’t be known until there is a thorough examination of its debris field.   
  
The storm of record is 1969 Hurricane Camille, which I rode out in an oceanfront laboratory about 25 miles east of the eye. There’s a variety of evidence arguing that Camille is going to be able to retain her crown.   
  
The lowest pressure in Haiyan was 895 millibars, or 26.42 inches of mercury. To give an idea, the needle on your grandmonther’s dial barometer would have to turn two complete counterclockwise circles to get there. While there have been four storms in the Atlantic in the modern era that have been as strong or a bit stronger, the western Pacific sees one of these approximately every two years or so.   
  
Camille’s lowest pressure was a bit higher, at 905 mb (26.72 inches). At first blush it would therefore seem Haiyan would win the blowhard award hands down, but Hayian had a very large eye around which its winds swirled, while Camille’s was one of the smallest ever measured. At times in its brief life, Camille’s was so small that the hurricane hunter aircraft could not safely complete a 360 degree turn without brushing through the devastating innermost cloud band, something you just don’t want to be near in a turning aircraft. In fact, the last aircraft to get into Camille, which measured 190mph sustained winds, lost an engine in the severe turbulence and fortunately was able to limp home.   
  
Haiyan’s estimated 195mph winds were derived from satellite data, rather than being directly sensed by an aircraft. But winds over the open ocean are always greater than those at landfall because of friction, and the five mph difference between the two storms is physically meaningless. 



The chance that an onshore anemometer (wind-speed and direction sensor) will survive such a storm isn’t very high, so the winds are inferred by scientists and engineers from the texture and distribution of what’s left behind.   
  
Every year, our National Hurricane Center summarizes the Atlantic hurricane season in painstaking detail in article published in the prestigious journal _Monthly Weather Review_. Describing Camille’s destruction, it said:   




Maximum winds near the coastline could not be measured, but from an appraisal of splintering of structures within a few hundred yards of the coast, velocities probably approached 175 k[nots]. 



That’s 201 mph.(Higher winds have been measured on small islands. With Haiyan and Camille, we are talking about storms running into large landmasses, where friction takes place.)   
  
Camille killed 143 along the Gulf Coast, while Haiyan’s toll is currently estimated to be more than 2,500.   
  
The difference, which is more than an order of magnitude, is largely (but not completely) due to poverty. Despite experiencing roughly five landfalling tropical cyclones per year, Philippine infrastructure simply isn’t as sound as it is in wealthier countries. As a grim example, a number of Haiyan’s casualties actually occurred in government-designated shelters that collapsed in the roaring eyewall.   
  
In addition, the transportation infrastructure simply couldn’t handle a mass evacuation. If a similar situation applied to the U.S. Gulf Coast, Camille would have killed thousands at landfall, a fact noted in the Hurricane Center’s report on the 1969 season. Where Haiyan hit in the Philippines, there simply weren’t any roads capable of evacuating the citizens of Tacloban City safely inland, forcing them to ride it out dangerously close to the invading ocean and exposed to winds that pulverized most structures.   
  
So, while we really don’t know which storm had higher winds, we do know that more affluent societies are much less affected by even the strongest storms. As Indur Goklany, (who writes frequently for Cato) has pointed out, if left to develop, the entire world will be much more resilient to climate change than it would be if the ineffective policies to “stop” it slowed economic growth.


"
"
Share this...FacebookTwitterI must admit I’m in a bit of shock reading this stuff, especially full police report about Al Gore’s alleged behaviour. I’m not going to draw any conclusion right now. It’s all too stunning, if it’s true.
The other climate blogs have not written a word about it so far. Maybe they want to be extra careful, which is understandable. But then again, Drudge plasters the story as its big headline for the day. Matt Drudge has been in business for years, and surely he’s done his homework. Maybe it’s to encourage other victims to speak out, if there are any.
I read the entire police report and it is shocking – really. Will other women come forward? Is it all a hoax? We’re not talking about Mike Tyson or Kobe Bryant here. We’re talking about the former VP of the USA and the prophet of AGW.
Incredible.
UPDATE; Germany’s top tabloid Bild reports here (German).
Share this...FacebookTwitter "
"

A couple of comments have mentioned the global “turn off your lights” night. Lubos Motl at the Reference Frame has a suggestion
Earth Hour: turn your lights on at 8 p.m.
Tonight, at 8 p.m. local time, you should turn on all the light bulbs you have for 60 minutes (it will only cost you 3 cents per light bulb in average for the whole hour) to fight global obscurantism. You should look how many lights are on around. Every light bulb you see will be a sign of the audacity of hope, as Jeremiah Wright would say.”15 years ago, I would have done this. Now, I plan to turn all my lights on as my silent form of protest against the likes of Gore and his Enron like carbon credit scheme. I’m going to “Watts Up” my house!
If you want to learn about the event, here is the web page:
http://www2.earthhourus.org/
Of course if you are simply interested in saving money and using less electricity (something I’m for, especially here in California since the state has hamstrung itself for future power generation) then get one of these:

I have several. They work great. And, buying one via this link sends some help back to me for keeping my www.surfacestations.org effort running.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea021377a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterAs recently as 2000 to 1000 years ago, spanning the Roman to Medieval Warm Periods, East Antarctica was 5-6°C warmer than it is today. The consequent ice melt resulted in >60 meters higher water levels in East Antarctica’s lakes.
East Antarctica has been rapidly cooling in recent decades, with magnitudes reaching -0.7°C to -2.0°C per decade since the mid-1980s (Obryk et al., 2020).

Image Source: Obryk et al., 2020
A new study (Myers et al., 2020) reports that until about 15,000 years ago and throughout the Last Glacial Maximum, East Antarctica was 4-9°C colder than it is today.
Antarctica then abruptly warmed 15°C within centuries. From 12,000 to 6,000 years before present, East Antarctica was about 5°C warmer than it is today.

Image Source: Myers et al., 2020
And then as recently as 2,000 to 1,000 years ago, East Antarctica was so warm (~6°C warmer than present) that its lakes were filled with 60 to 80 meters more meltwater than exists in lake basins today.
“Resistivity data suggests that active permafrost formation has been ongoing since the onset of lake drainage, and that as recently as 1,000 – 1,500 yr BP, lake levels were over 60 m higher than present. This coincides with a warmer than modern paleoclimate throughout the Holocene inferred by the nearby Taylor Dome ice core record. …  Stable isotope records from Taylor Dome (located roughly 100 km west of the MDVs) indicate mean annual air temperatures ca. 4-9 °C lower than modern during the LGM (Steig et al., 2000).”
“Between 12,000–6,000 yr BP, Taylor Dome ice core record indicates that regional temperatures were up to 5 °C warmer than modern conditions (Fig. 2) (Steig et al., 2000).”
“Permafrost age calculations indicate late Holocene lake level high-stands (up to ~81 masl, 63 m higher than modern Lake Fryxell) roughly 1.5 to 1 ka BP that would have filled both Lake Fryxell and Lake Hoare basins (Fig. 3b). …  Taylor Dome ice core records show a highly variable Holocene, with short lived peaks up to + 6 °C above modern temperatures between 1-2 ka BP (Steig et al., 2000).”
“Lake levels were higher potentially during and after the LGM when an ice dam blocked the mouth of TV, allowing for lake levels to increase by over 280 m compared to modern level. Taylor Dome ice core records indicate an abrupt warming of >15 °C from 15 – 12 ka BP, (Steig et al., 2000), which may have coincided with the maximum lake level of GLW.”
“Short lived changes in temperature such as a 6 °C increase in the late Holocene could have resulted in anywhere between 60 to 80 m of lake level rise and subsequent drawdown.”
This substantial regional warmth can also be verified by the 1,000-year-old elephant seal remains that document a time when Antarctica was sea ice free 2,400 kilometers south of where sea ice free conditions occur today (Koch et al., 2019). Elephant seals require sea ice free conditions to breed, and the same locations where they used to breed during the Medieval Warm Period are today buried in sea ice.

Image Source: Koch et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_9e221820cc7def01ad3e9985c5250b19').on('change', function() {
			  jQuery('#amount_9e221820cc7def01ad3e9985c5250b19').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Since this blog has main focused on air temperature measurement, and has not done any discussion of manual measurement techniques of Sea Surface Temperature measurements, I thought it would be good to first review some of the instrumentation used.
Sea Surface Temperature Measurement Instruments:
Standard Thermometer

Measures: Temperature in degrees, typically used in the bucket thermometer
Operates: At any depth by cable or line or by hand
Notes: Mercury in original thermometer has been replaced in many standard
thermometers by less toxic materials

Bucket Thermometer

Measures: Water temperature near the surface
Operates: At the surface by hand or line
Notes: Typically lowered about 1 meter into the water, left there for one
minute, and then retrieved deck side for reading.

Reversing Thermometer (for Nansen Bottle)

Measures: Water temperature at a specific predetermined depth
Operates: Only when turned 180 degrees (the mercury breaks in the special loop
and will not get back together until reset) Temperature at depth can be recorded
with a 180 degree flip (as is done with the Nansen Bottle) and there will be no
change on the way up.
Notes: A Nansen bottle
is a device for obtaining samples of seawater at a specific depth. It was
designed in 1910 by the early explorer and oceanographer Fridtjof Nansen.
The bottle, a metal or plastic cylinder, is lowered on a cable into the sea, and
when it has reached the required depth, a brass weight called a “messenger” is
dropped down the cable. When the weight reaches the bottle, the impact tips the
bottle upside down and trips a spring-loaded valve at the end, trapping the
water sample inside. The bottle and sample are then retrieved by hauling in the
cable.

Bathythermograph (BT)

Measures: Water temperature over a range of depth
Operates: Over any depth with a cable or line by hand or with a hydraulic winch
Notes: This model records the information inside and is retrieved however there
are expendable models (XBTs) that free fall on a copper line and transmit the
temperature and depth information through the copper wire before dropping to the
bottom


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ef4fb93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Supreme Court’s decision in _King v. Burwell_ upheld President Obama’s massive power grab, allowing him to tax, borrow, and spend $700 billion without congressional approval. This establishes a precedent that could let any president modify, amend, or suspend any enacted law at his or her whim.



As it stands, Obamacare will continue to disrupt coverage for sick Americans until Congress repeals it and replaces it with reforms that make health care better, more affordable, and more secure. Despite the ruling, Obamacare remains unpopular with the American public and the battle to set in place a health care system that works for all Americans is far from over. At a Capitol Hill Briefing in July, Michael Cannon, director of health policy studies at Cato, and Ilya Shapiro, a senior fellow in constitutional studies at the Institute, came together to discuss the impact of _King v. Burwell_ on health care reform, the separation of powers, and the rule of law.



 **MICHAEL CANNON** : The ink wasn’t yet dry on the ruling in _King v. Burwell_ before supporters declared that the debate over repealing Obamacare is over. Well, that debate has been declared over so many times at this point that I’ve lost count. But I believe that Obamacare supporters dodged a bullet with this ruling. The way the Affordable Care Act (ACA) was written, approved by both chambers of Congress, and signed into law by the president, it gave states the power to block major provisions of the law. States can block the subsidies that are supposed to flow through the health insurance exchanges. They can block the employer mandate and, to a large extent, the individual mandate. The law as written gave states these powers.



Obamacare supporters dodged a bullet because 34 or 38 states — depending on how you count — did not establish health insurance exchanges, which they needed to do in order for those provisions to take effect. They effectively exercised the vetoes that Congress gave them over portions of the ACA. And eliminating those subsidies would have revealed the full cost of this coverage to enrollees in Health‐ Care​.gov. That is what the Obama administration and its supporters fear. (I do not consider them ACA supporters, by the way, because they do not really support that law as written.) Nevertheless, the repeal debate is not over and there are a lot of reasons why. First, more than six years after the first draft of Obamacare was introduced in the House, it remains unpopular. In fact, it is as unpopular now as it was when it was enacted more than five years ago. And this is a year and a half into implementation, a year and a half after people have been receiving the benefits under this law, as rewritten by the Obama administration and the Supreme Court. And a lot of the costs of the law have not even taken effect yet: the Cadillac tax, some premium hikes on the horizon, the fact that some of the temporary programs designed to mitigate adverse selection will expire. This law has been unpopular for six solid years, and you know what? Unpopular laws, in a democracy, are always up for repeal. Second, the Obamacare repeal debate is going to keep going on because Obamacare hurts the sick. Yes, it does insure more people by throwing lots and lots of money at health insurance. But it also threw millions out of their health care plans in 2013 — including cancer patients and others with severe illnesses — leaving them with inferior coverage. It threatened to throw millions more out of their plans again until the Supreme Court amended the law in this latest ruling. And it is going to continue to threaten coverage for cancer patients and others as long as that law remains on the books.



It is not just opponents of the law that are noticing this. If you look at the January 29 issue of the _New England Journal of Medicine_ , the lead article is about how Obamacare is pushing insurers into a race to the bottom by jettisoning coverage for HIV patients. There are other studies that have found the same thing happening with other high‐​cost chronic conditions, like mental illness, diabetes, rheumatoid arthritis, and cancer. This has happened in other markets with Obamacarelike bans on discrimination on the basis of preexisting conditions. There are a number of examples where community rating rules have encouraged insurers to avoid, mistreat, and ultimately dump the sick. And it does not matter that it is only happening with a few insurers now. The _New England Journal of Medicine_ article said that a quarter of the insurance plans that they studied displayed these characteristics. But eventually all insurers will have to follow suit. Supporters of the law will likely blame this on greedy insurance companies. But the truth is that it is Obamacare that forces these companies into this race to the bottom, even when insurers are not trying to discriminate against the sick.



Another reason the repeal debate will continue is because Obamacare, and the way it has been implemented, demeans voters. And they feel it. They sense that it demeans them. The way that the Obamacare’s architects designed, sold, enacted, and implemented this law has been an ongoing string of insults to the intelligence, the compassion, the dignity, and the sense of fair play of Americans who oppose this policy. Obamacare’s architects have lied to the public. They have called voters stupid. They have called opponents evil. President Obama and the Supreme Court have rewritten the ACA now in so many ways that it disempowered and disenfranchised Obamacare opponents.



Finally, this debate is going to continue because there are ways to provide more secure health coverage to the sick. No one wants the pre‐​Obamacare world where government had already been making health insurance less secure. But in spite of the tax exclusion for employer‐​sponsored insurance and all the other things that the government has done to cripple private health insurance markets, they were still innovating to develop products that made health insurance more secure.



One example is guaranteed renewability. Another example is preexisting condition insurance, an innovation that was happening right underneath Congress’s nose as they were debating the Affordable Care Act. It was first introduced in late 2008, and UnitedHealthcare was getting regulatory approvals in early 2009. What was this product? It was basically an insurance product where, at a cost of just 20 percent of what you would pay for health insurance, you can buy the option to purchase that health insurance plan at any time, no matter how sick you get. Even if you develop a preexisting condition, you pay the same rate as everyone else. This was available in 2008. There were further innovations that markets were likely to develop that encouraged insurers to compete to cover the sick rather than to avoid them.



But Obamacare destroyed these innovations. When you think about it, if Obamacare were such an improvement over the status quo ante as it existed before the law, then why do 65 percent of Health​Care​.gov enrollees want the freedom to purchase their pre‐​Obamacare plans? Obviously something is amiss there. Unfortunately, these innovations are not coming back until we get rid of Obamacare, specifically its community rating price controls, and that’s why the debate is going to keep happening.



Now, what should Congress do in the wake of _King v. Burwell_? It should stay focused on what it has always been focused on with regard to Obamacare — which is repealing it. We are not going to get lower costs and more secure health insurance for the sick until this law is off the books. Repeal is actually more important after _King v. Burwell_. With that ruling, the president and the Supreme Court just created entitlements and imposed taxes on 70 million Americans — employers and individuals — that no Congress ever approved and from which Congress specifically exempted those 70 million employers and individuals. The fact that tens of millions of Americans are currently subject to taxes that Congress never approved makes it all the more important that Congress repeal Obamacare. _King v. Burwell_ shows that what we are living under right now is an illegitimate law.



I like to say that the Affordable Care Act is imperfect. But, gosh, it is a lot better than what we’ve got.



 **ILYA SHAPIRO** : Look, I’m a simple constitutional lawyer. Essentially everything I know about health care I’ve learned through litigating two cases, _NFIB v. Sebelius_ three years ago and now _King v. Burwell_. Originally, when we were planning this forum, we thought the case could go either way. It was really a 50/50 toss‐​up, and I thought I could add some value by explaining the nuanced rules of decision: for instance, what does this mean for health care or other types of regulatory policy going forward? Given the Court’s opinion, however, all I can say is that there’s really very little law, as it were, in the majority opinion. It’s as if the whole opinion just said “Affirmed.” The reasons don’t matter because they don’t make any sense whatsoever. Words have whatever meaning the writer wants them to have — which is a far cry from the following judicial opinion: “It is not our job to protect the people from the consequences of their political choices.” Now who wrote that? Is that from Justice Antonin Scalia’s dissent here in _King v. Burwell_? No. Is it from Justice Anthony Kennedy’s (combined with Scalia’s) dissent in _NFIB v. Sebelius_? No. It was Chief Justice John Roberts in the majority opinion in _NFIB_.



What’s going on here is an unholy confluence of liberal judicial activism and conservative judicial passivism that has found a perfect home with John Roberts. It’s not that the Court is liberal. Nor is John Roberts “evolving,” as so many justices have in the past. What’s going on here is that, well, Obamacare is special. As Scalia pointed out in his dissent, all the normal rules of constitutional — and now statutory — interpretation go out the window when it comes to the Affordable Care Act. Mind you, it wasn’t a matter of enforcing the text of the Affordable Care Act. It was rewriting the text in a different way to do what John Roberts thought in his infinite wisdom would cause the least disruption in public policy or the health care system. And it shows why we don’t want judges making these kinds of extra‐​legal determinations, because again this is not a liberal decision. In _NFIB_ , the individualmandate case, Justice Ruth Bader Ginsburg’s partial concurrence/​partial dissent said that there are no constitutional limits on federal power. That’s the liberal position.



Similarly here, the liberal position would have been to say that the IRS gets to do whatever it wants, applying what is known as _Chevron_ deference. _Chevron_ is a legal doctrine named after a case from more than 30 years ago, which says that when a law is ambiguous, courts are to defer to an agency’s interpretation of that law, unless that agency is being arbitrary and capricious. So even if courts might disagree with the agency’s determination, as long as it’s not completely crazy they’ll defer to it. In other words, A, B, C, D, and E are all somewhat plausible interpretations. One might be better; one might be worse. But as long as the agency doesn’t go for X, Y, or Z — which are all completely out of left field — they’ll be okay. Roberts specifically said in _King v. Burwell_ said that that was not what he was doing. This was not an administrative‐ law, agency‐​deference case. And that was backed up in June in the _Michigan v. EPA_ case, where Justice Clarence Thomas wrote a concurring opinion on the need to narrow _Chevron_. Justice Scalia’s majority opinion there said that the agency’s determination was indeed unreasonable and therefore sent it back to the drawing board.



But that doesn’t mean, I don’t think, that in some future instance _Chevron_ is going to be narrowed. I don’t know if the Court has the five votes necessary to do that. Some people are saying the fact that _Chevron_ wasn’t expanded is the silver lining in _King v. Burwell_. It sort of is. But again, this is kind of a sui generis opinion, good for the Affordable Care Act only. I’m sure some lower‐​court judges will use it to buttress some future fanciful statutory interpretations — to say that A is equivalent to Not‐​A, as it is here — that “exchange established by the state” means “exchange not established by the state.” Possibly. But really the way that it’s written, John Roberts’s goal in _King_ , as it was in _NFIB_ , was to achieve a certain result without really changing legal doctrine, without expanding federal power.



Let me extrapolate from that. Looking forward, there’s a lesson we can draw to avoid that unholy alliance of liberal activism — rewriting the law — with conservative passivisim — restraining and bending over backwards to let Congress do whatever it wants. The way we avoid that is to learn the lessons of history.



In the late 1930s and early 1940s, the Supreme Court started going off the rails and eviscerating the doctrine of limited and enumerated powers. It began eviscerating federalism and bifurcating (even trifurcating) our rights, such that some rights are more equal than others. Then, from the 1950s to the 1970s, when the Warren and Burger Courts continued these “evolving” notions of what the Constitution means, the conservative response to that sort of activism was not: “You’re wrong. Here’s the correct theory of constitutional or statutory interpretation.” Instead and alas, the response was: “Why are you not deferring to the political branches? You are unelected judges. You should be restrained. You should be deferring. You should be sitting on your hands.” And that’s why we have what we have now.



The answer is to appoint judges who are actually committed to judging. We should be fighting for judges who have a proven track record of saying what the Constitution actually is — of engaging with the law — rather than trying to bend over backwards and defer to agencies or to Congress. The judicial branch is a branch of the federal government for a reason. It’s there to check and balance the others.



John Roberts’s background was too smooth in many ways. He checked all the right boxes and excelled at the legal craft, but he never identified as an originalist or movement conservative. It’s clear that he’s a Republican. But it’s never been clear that he was committed to any particular theory of judicial interpretation. So congratulations to him, but future Republican administrations will have to be more careful about what kind of conservative they want on the Supreme Court: someone who focuses on restraint (a judicial mode) or someone who has a particular substantive theory of jurisprudence.



Those of you, especially here on Capitol Hill, who are going to the barricades to fight for a proper judiciary, make sure you’re fighting for the right people so that it’s worth the effort. Someone who has displayed loyalty to generic conservatism or some kind of “Red Team, Blue Team” fight is not enough. We need judges who actually are willing to make the difficult “balls and strikes” calls — as John Roberts said at his confirmation hearings — rather than kick the plate a little bit, squint, and call it a strike because Congress “intended” it to be a strike. This is not going to change overnight. It’s not a matter of winning the White House. It’s a matter of picking the right judges and understanding the climate of ideas such that the proper judicial philosophy isn’t being conservative or minimalist or incrementalist or restrained. It’s about judging in a particular way and applying the standard tools of statutory and constitutional interpretation regardless of where the political chips may fall.
"
"It is often said that humans have caused the Earth to warm at an unprecedented rate. However researchers have discovered another period, some 55m years ago, when massive volcanic eruptions pumped so much carbon into the atmosphere that the planet warmed at what geologists would think of as breakneck speed.  The good news is that most plants and animals survived the warm spell. The planet has experienced several mass extinctions – and this wasn’t one of them. But there’s a catch: even after carbon levels returned to their previous levels, the climate took 200,000 years to return to normal. Geologists have a name for this earlier period of sudden warming: the Palaeocene-Eocene Thermal Maximum. The PETM, as we’ll call it, occurred 55.5–55.3 million years ago. According to new research published in the journal Nature Geoscience it involved global warming of between 5 and 8°C over a period of 200,000 years. The massive carbon injections responsible for the PETM probably originated in volcanic eruptions in the North Atlantic and the burning of organic-rich rocks through which lava passed, which further triggered the melting of frozen methane stored at the bottom of the deepest oceans. There are obvious analogies between the PETM and the present-day situation, even despite the lack of fossil-fuel burning humans 55m years ago. The study shows the PETM was caused by annual carbon emissions of at least 900 million tons (900 MT) over the 200,000 years. That is ten times less than the 9500 MT carbon humans are releasing into the atmosphere every year. Surely cause for concern?  However, it is a little misleading to suggest emissions of just 10% of current levels resulted in warming of 5°C or more. It is possible to zoom out too far, even when assessing climate change. Considering that CO2 only sticks around in the atmosphere for 1,000 years at most, to achieve as much as 8°C warming the bulk of PETM carbon must have been delivered to the atmosphere in a very short time, during which the long-term average was greatly exceeded.  The researchers identify two such “pulses”, each lasting no more than 1,500 years. Volcanic eruptions are not predictable – and certainly not consistent – but they would fit the profile of these “pulses”. It is these massive, rapid injections of carbon into the atmosphere that overwhelmed Earth’s otherwise extremely efficient oceanic carbon sink: it takes time to deal with such blows.  The study suggests that it took 200,000 years before Earth returned to normal (probably hindered by volcanic eruptions) – a duration that suggests Earth will not recover from its current stresses any time soon. An intriguing aspect of the PETM warming is that there was no mass extinction. Perhaps ecosystems were resilient having evolved in the aftermath of the great extinction of the dinosaurs some 10m years earlier. Perhaps the warming didn’t last long enough. Perhaps warming just made things “nice”.  But global warming hasn’t usually been particularly good news for the planet’s inhabitants. When massive volcanic eruptions in present-day Siberia generated a comparable increase in global temperature 250m years ago, it caused the greatest crisis in Earth’s history. Around 95% of the planet’s species were wiped out in what is known as the Permian-Triassic extinction, or the Great Dying. Earth was definitely not “nice” and remained inhospitably hot for millions of years. In comparison, the PETM looks like a tea party. Is humankind off the hook? After all, we are only emitting a bit of carbon each year, nothing like the massive doses administered by enormous eruptions in geological history. No. The excellent high-resolution palaeoclimate records now emerging indicate that global warming has precedent in the rock record, and it always takes Earth a long time to recover.  The impact of 5-8°C global warming today is hard to define. It is beyond the worst-case scenarios of most climate models – and yet it is not beyond the bounds of possibility. Let’s do some rough calculations – and simplify things by talking about CO2 alone: today, Earth’s atmosphere contains at least 3000 gigatons of CO2. Humans inject a further 30000 megatons every year (1% of the mass in the atmosphere). Volcanoes add only a fraction of the human contribution.  By contrast, the total CO2 release from the Siberian Traps and the rocks burned by its lava is estimated at 30,000 to 100,000 gigatons. That’s ten to thirty times the total amount of carbon currently in the atmosphere. At current rates it will take humans just 1000 - 3000 years to produce this amount (it took the Siberian Traps much longer) and by the year 3014 Earth could be facing another catastrophe. Does that sound a long time away? For a geologist, these are incredibly short timescales. Our saving grace appears to be that we are coming out of an ice-age with an atmosphere relatively low in CO2, and that we are going to run out of fossil fuels before the above scenario can happen. However, geologists have shown that massive injections of carbon to the atmosphere can change the climate quite rapidly, and we are already well along the road. We are now seeing major changes to our weather and time will tell if this is the manifestation of longer term climate change.  Undoubtedly Earth’s climate will change as we continue to emit. Our generation might not see the impact of that change, but we need to decide what we want for the future of Earth. It is time to learn from Earth’s past – episodes such as the PETM, and the Permian-Triassic – as we look to its future."
"
Share this...FacebookTwitterDr Oleg Pokrovsky has kindly taken the time to provide further information on his recent remarks, which have been widely quoted. We thank him for doing so. He writes as follows and includes a link to a ppt. presentation (see below):
Dear Colleagues,
Thank you for discussion of my conclusions presented at recent IPY conference
occurred in AARI (St.Petersburg, Russia).
My vision of future climate is based on comprehensive analysis of climate index series analysis, which permits to reveal fundamental quasi-periodical oscillations in most components of climate system:
-Solar activity
-SST of ocean (AMO and PDO)
-Surface air temperature
– Surface irradiance
-Precipitations
-Ice extent in Russian Arctic Seas
I found that that those are in strong coherence when inter-annual climate noise was removed in each of them
My motivation might be illustrated by a set of figures presented at recent Arctic Frontiers Conference (Tromso, Norway)
http://www.arctic-frontiers.com/index.php?option=com_docman&task=doc_download&gid=242&Itemid=155
Please keep your comments focused on the contents Dr Pokrovsky’s presentation.
Share this...FacebookTwitter "
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"A volcanic eruption in Iceland caused massive disruption throughout Europe in 2010. A huge ash cloud grounded more than 100,000 flights and delayed 10m passengers, costing the aviation industry more than £2 billion. This wasn’t a freak event. New evidence shows such ash clouds are more common than we thought, and they can even cross the Atlantic from volcanic hot-spots in North America. We need to be wary as another major ash cloud could arrive at any time. In fact, the ash has barely settled from Alaska’s latest major eruption. Given volcanoes erupt all the time it seems odd that the Iceland incident came as such a shock. Perhaps there is a failure to appreciate that volcanic eruptions often occur in cycles with busy periods followed by intervals of relative quiet during which time these events pass out of social memory. Looking back through history one can see that 2010 was by no means unique. The Icelandic volcanoes Katla and Hekla, for example, produced large ash plumes in 1947 and 1918, but both were modest by comparison with the massive Asjka eruption of 1875 which blanketed much of Scandinavia in ash. We should remember that intercontinental plane travel has only existed for around 50 years, with budget airlines allowing mass air travel only within the past few decades. Flying has changed from being the reserve of the wealthy to a regular travel expectation for the majority.  The industry was lucky to evolve in what was a relatively quiet period between major ash producing eruptions in Iceland. A few years ago we were involved in a project to reconstruct past environmental changes along North America’s east coast. We found a number of ash layers throughout the sediments covering the past several thousand years.  By analysing the elements in the ash’s glass particles we are able to obtain a chemical “fingerprint” unique to that ash layer. These “fingerprints” can then be compared with samples from elsewhere. When an ash layer is identified, it provides a means of joining and aligning the environmental histories of different areas where it occurs. They are very precise time markers in the sediment because they are deposited over a very short period of time (days to weeks). The majority of the dozen or so ash layers we found during this study were from well-known eruptions in North American volcanic regions such as the Aleutian Islands off Alaska or the Cascade Mountains near Portland. One layer however stood out. It presented us with a puzzle: we had found a chemical match between an ash layer from Alaska and a layer which occurs throughout Europe, which was always presumed to come from Iceland. Using the ages of the eruptions was no help as they both occurred at approximately the same time. In North America, we know this as the White River Ash, which erupted from Bona-Churchill massif in Alaska. The European layer is called the AD860B (named after the approximate date of the layer).   We suspected both derived from the same eruption. But this would imply that ash would be capable of travelling from Alaska, over North America, and out across the Atlantic to Europe – a  total distance of 7,000km. One might expect this of past mega-eruptions such as Toba on Sumatra which blasted ash as far as Lake Malawi in eastern Africa around 75,000 years ago.  However, the White River Ash was by no means a mega-sized event. Although it was large – approximately ten times larger than the 1990 eruption of Pinatubo – it was also half the size of the 1815 eruption of Tambora. In the long run we could expect an eruption the size of White River somewhere in the world every 100-200 years.  We collected samples of both the White River Ash and AD860B from both sides of the Atlantic and re-examined them in detail: there were no appreciable differences between the Alaskan and European ash deposits. As an added bonus the ash has also been found deep in the Greenland ice. This allowed us to count the annual ice layers as one would for tree rings to obtain a new age for the eruption of around AD 847.  It is unlikely that we stumbled upon the only time North American ash that made it to Europe, and we fully expect more such layers will be found to correspond with the many large eruptions that have occurred in North America. If it happened at least once before, we need to be aware of the risk that it will happen again. The White River Ash/AD860B layer covered a third of the globe’s circumference at approximately 60°N. This coincides with a number of trans-Atlantic flight paths and would pose an obvious hazard when any of North America’s plentiful volcanoes have a White River Ash-type eruption. Findings such as ours should provide additionally useful data for the airline industry when calculating the risk likelihood associated with future volcanic eruptions and how to improve resilience against them."
"Christmas songs are in the air and supermarket aisles are fit to burst – the festive season is upon us. But many may view the shiny, plastic-wrapped goods on the shelves through a different lens this year thanks to growing concern over plastic pollution.  Each year, the UK produces over 2.3m tonnes of plastic packaging and over 4.7m tonnes of paper and cardboard, with only 45% of plastic being recovered and recycled. Waste production peaks over the Christmas holidays, with a 30% increase in plastic use, such as packaging and plastic products, and Britons bin the equivalent of 108m rolls of wrapping paper. A survey last year found that 84% of consumers were worried about the amount of plastic packaging used on gifts at Christmas, with 22% of those surveyed stating there was too much waste generated by their households to recycle all of it and 37% confused about what can be recycled. It’s easy to forget the implications of plastic once our waste has been binned, but significant volumes of the plastic waste we generate this Christmas will remain in the environment for hundreds of years. However, the Plastics Collaboratory, a joint research effort by academics in different fields at the University of Hull, have come up with 12 ways you can reduce your plastic use this Christmas. Over one billion Christmas cards are sold every year in the UK, but many are not recycled. Cards are often embellished with plastic and glitter, which makes them non-recyclable. You can tear off parts that can’t be recycled, but making your own cards is much more cost effective, personal and fully recyclable. All you need is recycled plain card, ink stamps, crayons and your imagination. Although most plastic-based decorations are used over several years, many (such as tinsel) degrade over time, releasing plastic fragments. An alternative is to use string and natural materials, such as pine cones and fruit, which look good and reduce plastic at the same time. Plastic-based floral foam is often used as a backing to hold christmas wreaths together, which crumbles into small microplastics over time. Make sure you buy wreaths that include natural moss. Or, you can make your own wreath, which can be used for years, using natural materials. You’d have to use an artificial plastic tree for over ten years to reduce the carbon footprint to that of a real tree. But the best option of all is to buy a potted tree which you can plant out and re-pot every year. In the UK, consumers buy an estimated 370m mince pies over the holidays yet consume only 80% of them – 74m go uneaten! Around 62m plastic trays are needed to hold all the pies that are sold. The best solution is to make your own. Secret Santa is a great tradition, but often results in people trying to buy silly gifts that are cheap and tacky and often made of plastic that is quickly thrown away. A nice alternative could be a refillable gift stored in a glass jar, such as hot chocolate or pancake mix. Enough wrapping paper to wrap around the equator nine times is sold each year in the UK. Unfortunately, much of this paper contains glitter and plastic films which make them non-recyclable. You can test this using the scrunch test – if the paper has no glitter on it and scrunches up you can recycle it. But an easier alternative is making and decorating your own, with recycled brown paper and string, which is fully recyclable. “Reindeer food” has become a popular craft activity for families at Christmas, often made from porridge oats and glitter that is spread in the garden for Santa’s reindeers. Glitter is technically a microplastic and cannot be recycled, but there are natural alternatives such as the mineral mica which has the same effect without the environmental impact. Gone are the days of the small fabric stockings that dangle at the end of the bed. Now it’s all about huge synthetic stockings that brim with presents and flashing lights. By using felt, old pillow cases or even old socks you can recreate your own plastic-free personal stockings which children will cherish for years to come. The majority of Christmas crackers are laden with teeny, tiny plastic gifts. With over 150m crackers  pulled each year, that’s a lot of plastic waste with a few hours of lifespan before the rubbish bin. Make your own crackers and fill them with homemade hats, dad jokes and tiny games such as bingo or origami frog racing for a fun and unique alternative. Christmas dinner comes with a huge amount of plastic packaging. Even when you attempt to be good and buy loose vegetables at the supermarket, most retailers still only have single use plastic bags to hand. Worry not – all you need is a lightweight material, such as an old pillow case and you can make your own veggie bags that you can use again and again. You have 12 months to get ready for next year. Instead of buying an advent calendar wrapped in plastic and with chocolate in plastic moulding, why not make your own? One design features 24 matchboxes filled with chocolate and wrapped in card."
"

In a recent speech to the Washington‐​based think tank Resources for the Future, EPA Administrator Gina McCarthy promoted the White House’s new Clean Power Plan by: (a) appealing to science and disallowing any debate about it; (b) making statements unsupported by the science; (c) praising the economic analysis behind the plan; and (d) announcing rules that economic analysis says won’t work and will cost too much.



In other words, it was business as usual in the world of climate policy.



She started her speech by saying that scientists are as sure that humans cause climate change as they are that smoking causes cancer, and “we are way past any further discussion or debate.…don’t debate about climate change any longer because it is our moral responsibility to act.”



From there she focused on the harms from extreme weather events, attributing the California drought to carbon dioxide emissions, as well as increased storms, wildfires and floods. She said anthropogenic climate change (i.e. global warming) leads to more extreme heat and, amazingly enough, more extreme cold. And she linked weather‐​related economic threats facing families and small businesses to anthropogenic climate change.



Now whatever you do, don’t question the science. For many years we have been told to rely exclusively on the UN Intergovernmental Panel on Climate Change (IPCC) for official truth on all climate topics. There are, of course, lots of reasons to mistrust the IPCC, including its past blunders, the conduct of its disgraced and discredited chair Rajendra Pachauri, and its clique‐​like report‐​writing process. But for now, let’s play the game and turn to the IPCC.





A growing body of economic analysis over the years has indicated that the models overstate the potential savings from energy efficiency programs.



In 2012 the IPCC published a Special Report on Extreme Weather (SREX), gathering up the available knowledge on storms, droughts, etc, and their possible connection to climate change generally and carbon dioxide emissions specifically.



Contrary to McCarthy’s claim, the SREX singled out the U.S. as a region where “droughts have become less frequent, less intense or shorter.” Worldwide there is only “limited to medium” regional evidence regarding changes in floods because the records are sparse and the effects are confounded with changes in land use and engineering. “Furthermore,” they said, “there is low agreement in this evidence, and thus overall low confidence at the global scale regarding even the sign of these changes.”



Does this sound like the level of confidence associated with the link between smoking and cancer?



Overall the IPCC’s attribution of a causal link between extreme weather and carbon dioxide emissions was the limpest possible: “There is evidence that some extremes have changed as a result of anthropogenic influences, including increases in atmospheric concentrations of greenhouse gases.” But, they went on, there is only low confidence in attribution of tropical cyclone activity to anthropogenic influences, and “Attribution of single extreme events to anthropogenic climate change is challenging” — UN speak for “we’d go further if we could but even we can’t torque the evidence that far.”



They also made it clear that economic vulnerability to weather is a function of a nation’s wealth, adding “Increasing exposure of people and economic assets has been the major cause of long‐​term increases in economic losses from weather‐ and climate‐​related disasters (high confidence). Long‐​term trends in economic disaster losses adjusted for wealth and population increases have not been attributed to climate change, but a role for climate change has not been excluded.”



Do doctors say “trends in lung cancer have not been attributed to cigarette smoking, but a role for tobacco has not been excluded”? Of course not. McCarthy’s invocation of scientific certainty and prohibition on further debate was mere demagoguery.



After boasting about the extensive research and consultation that went into the rule, McCarthy then said that it will reduce household utility costs, a prediction based on engineering studies behind the energy efficiency rules in the Clean Power Plan. But do these programs really save households money?



A growing body of economic analysis over the years has indicated that the models overstate the potential savings from energy efficiency programs. New evidence from a large‐​scale randomized field experiment has confirmed this. Conducted by a team of economists from Berkeley and MIT, the study tracked more than 30,000 households in the federal Weatherization Assistance Program. Participants in the program went through household energy audits using a government‐​approved engineering model to estimate the savings from undertaking a fully subsidized efficiency upgrade.



By comparing before‐ and after‐​data, and comparing against households that did not undergo weatherization, the authors showed that the engineering models were way off, exaggerating the energy savings 2.5-fold, with the result that the renovations cost twice the value of the subsequent energy savings. Even taking account of social and environmental benefits the rate of return on the program was about -9.5 percent annually, in other words the costs greatly exceeded the benefits.



The authors also found that the implicit cost of carbon dioxide emission reductions in the program were about $330 per tonne, roughly ten times the administration’s own estimate of the Social Cost of Carbon. In other words, taking the administration’s science and economics at face value, a major component of their climate plan costs $330 per tonne for emission cuts they themselves value at $38 per tonne.



McCarthy told her audience not to question her science and to respect the research behind their climate policy. If you doubt her analysis, you will definitely find the policy plan misguided. The problem is that, even if you accept her science and economics, it’s still misguided.
"
"
Share this...FacebookTwitterA few days ago I wrote a post about how Reuters wasted no time blaming the outbreak of a fungus disease in the U.S. Northwest and British Columbia on climate change: http://pgosselin.wordpress.com/2010/04/23/were-to-blame/.
Of course this was just another example of wreckless media speculation and wishful thinking by those who simply can’t wait for the coming manmade climate catastrophe. They really are pretty desparate. The claim has already been completely debunked at World Climate Report: http://www.worldclimatereport.com/.
Share this...FacebookTwitter "
"

A plurality of likely voters now say they disagree with Vice President Gore on what is clearly his innermost, core belief. As a result, Washington insiders have advised Bush to capitulate to Gore. 



Last Earth Day, Gore re‐​released his book “Earth in the Balance,” which declares that fighting global warming should be the “central organizing principal for civilization,” and that the price of energy should be increased. Gore says he “wouldn’t change a thing” about the original (1993) edition. 



In order to create the legal framework for his program of global salvation, he told ABC’s This Week that he would “build support” for the Kyoto Protocol on global warming, which coerces dramatic reductions in energy use through higher prices, before submitting it to the Senate for ratification.



According to a recent Zogby/​Reuters poll, so far he has failed. By a margin of 46 percent to 42 percent, people support Bush’s position over Gore’s. Specifically, in the second debate, Bush said, “I’ll tell you one thing I am not going to do. I’m not going to let the United States carry the burden for cleaning up the world’s air, like the Kyoto treaty would have done.” 



Not only did Bush carry the day, it looks as if Kyoto causes heartburn for a lot of Democrats. Of people identifying themselves as Democrats, 68 percent agreed with Gore. That means one‐​third of the party faithful either agreed with Bush or had no opinion, while only 15 percent of Republicans favored Gore’s position. 



Sensing defeat, Gore recently retreated from his “Earth in the Balance” position on energy taxes, which are the fastest (and most economically disruptive) way to discourage fuel use. In the second debate, he said “I’m not in favor of energy taxes.”



So, why hasn’t Bush belled Gore’s cat? Not only does Bush enjoy popular support, he has caught Gore in another clear misstatement. The answer is that the Bush campaign itself is conflicted about climate change. Tucked away in his energy policy white paper released last month is a statement about limiting carbon dioxide emissions, the main greenhouse warming gas. That position potentially puts him to the left of Gore on global warming. 



In many ways, Bush is handling environmental policy a lot like his father did. In 1992, President Bush went to Rio de Janeiro to sign the original U.N. global warming treaty, against the advice of many but riding a crest of popularity. Five months later he was beaten by Clinton and Gore, the latter of whom heckled him in Rio. 



Why do Bushes do this? While Gore governs, campaigns, lives and breaths confrontation, Bushes make compromises and “bring people together.” They’re “kindler and gentler” and “compassionate.” 



In Washington, the people you “bring together” are involved in Washington’s primary industry: government. And so when either Bush sought Washington advice on climate change, he ran into people who can always be depended on to recommend federal programs to “do something.” For global warming, this means placing some type of restriction on greenhouse gas emissions. “Nothing” is not an acceptable answer in governmentville.



There are neither term limits nor elections for lobbyists, and they will do anything to stay here. The food is good (they’re not paying), the wine is cheap (D.C. has the lowest liquor taxes around), and the power is even more intoxicating. Keeping those perks means defining anything as a problem, requiring a solution from the federal government. 



Thus it was the electric utility lobby that encouraged Bush to include emissions restrictions in his environmental proposals. But the electric utility lobby will pursue its regulatory interests regardless of who is elected. With the prospect of a Bush victory, the lobby just made up a set of proposals, in order to have a reason for existence come inauguration time. 



Do they care what voter opinion is? Look at the Zogby poll and decide for yourself. Have they rewarded Bush for his reluctance to go along with Kyoto? No, because they’d be out of a job. Instead, they advise Bush to capitulate on his opponent’s most heartfelt opinion, in spite of evidence that Bush articulates the most popular position. They fear that Bush might actually win and make them irrelevant.
"
"It was late October when Adrian Sparks caught sight of the first smoke rising from the hilly horizon. Within days the haze evolved into drift smoke, which grew thicker as the mountain behind the Mount Pleasant winery in the Hunter Valley caught fire. “It was full on,” Adrian says. “There was smoke all through November and December. A clear day would still be hazy. At its worst, some days our eyes would sting. We’d be coughing. You’d have to stay inside with the doors shut and the air conditioning going. It was like an apocalypse..”  Though the winery suffered no fire damage, the blanket of smoke that was its legacy has caused nightmares for it and the broader Hunter Valley wine industry, thanks to what is known as “smoke taint”. Within the Hunter at least, the taint is forcing growers to confront the possibility that an entire year’s harvest will be dumped, with some vineyards choosing not to produce a 2020 vintage at all. The phenomenon occurs when smoke binds to the skin of grapes, ruining the taste of wine made from the fruit. For an industry where perception equals success, the reputational damage caused by selling a vintage affected by smoke taint can be lethal.  Sparks had seen the effects of smoke on wine grapes before. Years earlier he encountered the problem while working as a winemaker in the Yarra Valley, around the time of the Black Saturday bushfires. “It wasn’t as bad in 2009,” he says. “This is first time ever I’ve seen a company pull the pin on a vintage. I’ve been with the company for 20-odd years and I’ve never pulled the pin on an entire vintage.” On 14 January the winery that ordinarily produces 30,000 cases of wine in a year decided it wouldn’t take the risk and scrapped its 2020 vintage entirely. Mount Pleasant wasn’t alone. While vineyards further away from the fires escaped the worst, among the first to speak publicly about the issue was Bruce Tyrrell. Tyrrell’s Wines – the family have operated in the Hunter Valley since 1858 – ordinarily harvests 1,200 tonnes of grapes but this year lost 80% of its crop. “We didn’t have any immediate fire, we just had the smoke hanging around,” Tyrrell says. “We made the decision early, we weren’t going to take the risk with the brand. If a sommelier at a restaurant in New York opens a bottle of ours in 2030 and that wine has smoke taint, I’ve lost a whole lot of work. “We’ve worked too long, too hard to build the reputation to get where we are to let it go in five minutes. We’ve been here for 160 years and I’d like to see the family here in another 160 years.” Brokenwood Wines, Meerea Park Wines and Davis Wine Group have all made the similar difficult decisions about their harvests with some being left unpicked. Christina Tulloch, the chief executive of Tulloch Wines and president of the Hunter Valley Wine and Tourism Association, says the full economic cost is yet to be known. The community is already hurting after the bushfires cost it $42m in lost tourism. “That’s the economic loss based purely on visitation – people visiting cellar doors,” she says. “It is still too early to put a figure on the loss in production as we would normally be in the middle of vintage. We are hearing reports of between 50 to 90% of crop loss due to smoke taint. “Overall, we’re saying the loss will be more likely to be around 80 to 90% in reduction of tonnage that is brought into wineries in the 2020 year.” A similar story is playing out elsewhere. When bushfires tore through the Adelaide Hills before Christmas, a third of the wine-producing region was hit hard, as were grape growers on Kangaroo Island off the coast. Those who weren’t directly affected by the fires themselves watched the smoke linger over their fruit. Anita Poddar, of Wine Australia, still says she is hopeful the worst may be avoided. Out of 64 wine-producing regions which make up the $6.25bn industry, just 1% have been affected by the fires. With authorities still assessing the direct and indirect damage, there is a chance some regions may escape unharmed. “At this stage it is still too early to tell what the exact situation is,” Poddar says. “We started doing research on smoke taint in 2003. What happens is when there’s fresh, heavy smoke, it lands on the outside of the grape, and specific compounds get into the skin, not the flesh. It’s a one-season thing – next season the vines are fine.” With most winemakers now focused on the short-term work of getting through the year, few are asking whether this might represent a new normal. Alisdair Tulloch, of Keith Tulloch Wine, says the threat posed by bushfires and smoke taint are indicative of a larger problem. “You need to pull the camera back further than the bushfires themselves and the way they are influenced by climate change to look at the broader picture of grape growing in general,” he says. “Grape growing has been showing the fingerprints of climate change since the 1980s when the harvesting dates began to move forward. “Last year was both the hottest and driest year on record, according to the Bureau of Meteorology, which confirms that this drought is anything but normal.” Like chocolate and coffee crops elsewhere, wine grapes the world over require specific microclimates, while growers need predictable weather to make production decisions. Thanks to the climate emergency this is changing right across the globe. Within Australia, seasons are arriving earlier and weather has grown unpredictable. Rain falls in large quantities or not at all, while drier conditions are making it harder to grow certain varieties. These effects were predicted in a landmark study on the impact of climate change on the Australian wine industry published in 2011. A team at Australia’s scientific research agency CSIRO forecast temperatures would rise between 0.3C and 1.7C by 2030. Events have since borne out the predictions, with shorter winters and earlier harvests as fruit ripens earlier. All of which has underscored the need to act, according to Alisdair Tulloch, who says problems affecting viticulture are true for all agricultural sectors. “Our small family vineyard has been carbon neutral since 2017,” Tulloch says. “We’ve been certified as carbon neutral by the Australian government since March 2019. “Meanwhile, other industries are free to pollute and pump as much of these greenhouse gases into the atmosphere, while agriculture and the wine industry is picking up the bill. “If we can do it, why can’t they?” "
"
Guest post by John Goetz

I keep an active watch of the news for progress being made in the areas of renewable and alternative energy sources. One area that has caught my eye is algal fuel (biofuel produced by algae). One company that has been in the news lately is Sapphire Energy, which claims to be able to produce ASTM compliant 91-octane biogasoline. Sapphire Energy says their technology “requires only sunlight, CO2 and non-potable water – and can be produced at massive scale on non-arable land”.
I am not trying to pick on any one solution or Sapphire Energy in particular. I simply wondered how massive a scale of CO2 and non-arable land is needed to make a noticeable dent in our gasoline demand.
First, how much CO2 do we need? The IPCC guidelines for calculating emissions require that an oxidation factor of 0.99 be applied to gasoline’s carbon content to account for a small portion of the fuel that is not oxidized into CO2. To calculate the CO2 emissions from a gallon of fuel, the carbon emissions are multiplied by the ratio of the molecular weight of CO2 to the molecular weight of carbon, or 44/12. Thus, the IPCC says the CO2 emissions from a gallon of gasoline = 2,421 grams x 0.99 x (44/12) = 8,788 grams = 8.8 kg/gallon = 19.4 pounds/gallon.
Now let’s assume Sapphire Energy simply reverses the process and consumes the CO2 to produce gasoline. In other words, we take 19.4 pounds of CO2 out of the atmosphere for every gallon of gasoline we produce. This seems like is a nice “carbon neutral” process.
What is the cubic volume of atmosphere required to make 1 gallon of gas? Let’s assume for the moment an efficiency factor of 100%, meaning our process will consume 100% of the atmospheric CO2 it is fed. This is unrealistic, but it is unrealistic on the “optimistic” side. According to the EPA, one cubic meter of CO2 gas weighs 0.2294 lbs. At an atmospheric concentration level of 385ppm, one cubic meter of atmosphere contains 0.000088319 lbs of CO2. Thus, 19.4 / .000088319 = 219658 cubic meters (yes, I am ignoring the atmospheric density gradient as one moves from the ground upward, but hang with me). This equates to roughly 4553 gallons of gasoline per cubic kilometer of air.
According to the US Energy Information Administration, US gasoline consumption is currently averaging (4-week rolling) 9.027 million barrels of gasoline per day, or about 379 million gallons (42 gallons per barrel). Thus, to completely replace US gasoline consumption, Sapphire Energy would need to “scrub”, at 100% efficiency, just over 83000 cubic kilometers of air per day. Certainly there is plenty of air available – this volume represents less than 0.02% of the volume of air in the first 1 km of atmosphere. Nevertheless, it is an enormous  amount to process each day.
Of course, Sapphire Energy’s near-term goals are much more modest. As CEO Jason Pyle told Biomass Magazine, “the company is currently deploying a three-year pilot process with the goal of opening a 153 MMgy (10,000 barrel per day) production facility by 2011 at a site yet to be determined.” Using my fuzzy math above, that equates to a minimum of 92 cubic kilometers of air a day. Still seems like a lot.
So where will all of the CO2 come from?
Presumably the answer is coal-fired power plants. But let’s see if that makes sense. According to Science Daily, the top twelve CO2-emitting power plants in the US have total emissions of 236.8 million tons annually, or 1.3 billion pounds per day. Now, if that can be converted completely to gasoline, it would amount to 67 million gallons per day, or roughly 1/6 of the daily gasoline consumption.
(Science Daily refers to the twelve as the “dirty dozen,” which I found somewhat humorous given that CO2 is colorless and odorless, and is presumably needed to sustain some forms of life. But then again, so is dirt.)
Sounds great, except that a lot of land is needed to grow all that algae. According to Wikipedia, between 5,000 and 20,000 gallons of biodiesel can be produced per acre from algae per year. Assume for the moment that biogasoline can be produced at the same rate per acre. If we attempted to produce 67 million gallons of gasoline from our “dirty-dozen” every day, we would need between 1.2M and 4.9M acres of land to do this on. The low-end of the scale puts the area needed at more than that of Rhode Island. The high-end adds in Connecticut.
I kind of doubt there is that much land around each of the dirty dozen facilities. This means the gas would have to be sent by pipeline to a giant algae field. Given our ability to pipe oil and natural gas all over the place, sending CO2 across the country via pipeline is probably doable. There may also be plenty of unused or abandoned land (think abandoned oil fields) available to produce the gasoline. Nevertheless, the production scale and transportation logistics required to make this a viable alternative do indeed look massive.
So while the technology holds promise at the micro-scale, it remains to be seen what can actually be done at a scale that matters.
Talk among yourselves.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e6e362c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Jouwatch
(Translated/edited by P. Gosselin)
Since everyone is preoccupied with Corona, hardly anyone notices what is being decided to continue destroying Germany:
The German government now wants to make the use of renewable energies a question of national security. “The use of renewable energies for electricity generation is in the public interest and serves public security,” says the draft of the new German Renewable Energy Sources Act, on which the newspaper “Welt am Sonntag” reported.
From the point of view of experts, the decision is of enormous significance.
It concerns a energy-political turning point, say legal experts of energy law at the law firm of Luther, Gernot, Engel, reports Die Welt am Sonntag.
In the controversy over the building  of wind parks, for example, the reference to “public security” may fundamentally impact court rulings. In court proceedings in connection with the expansion of bioenergy, wind and solar power, the reference to “public safety” could restrict the impact rulings by judges, business representatives fear, according to the “Welt am Sonntag”.
The new norm threatens to become a basis for far-reaching state intervention.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The federal government confirmed to Die Welt am Sonntag that the new state consecrations for eco-energy should make it easier to enforce building applications. “The regulation stipulates an overriding public interest in electricity generation from renewable energies as well as a public security interest,” the Federal Ministry of Economics announced in response to an inquiry by the newspaper.
The specification is important for discretionary and public interest rulings by authorities and institutions.
Latest government power-grab
If this law passes, and it will pass because there is no real opposition apart from the AfD party, the path is cleared for Germany. Then wind turbines will be forced to be built directly next to residential areas, and ownership rights will be undermined.
That is the revolution from above. That is energy fascism. Resistance must be stirred up here – and it fatally reminds us of the power grabbing in these times of Corona!
The massively green electricity damaged Wattenrat East Friesian comments on this new underhanded approach as follows:
The renewable energy industry is insatiable, ideologically consolidated and closely linked to politics – and above all very inventive,
if it concerns the preservation of its ecclesiastical  income, which is paid by all current customers through the EEG green energy feed in act to the tune of double-digit billions annually.
Now the use of renewable energies is even supposed to “serve public safety”, says the draft of the new Renewable Energy Sources Act. This would make wind farm sites easier to implement. This is incredibly brazen and wrong because the renewable energies (wind and sun) only work depending on the weather. Especially wind power plants endanger the security of supply due to the erratic feed-in through grid instability unstable power grids, are therefore a public safety risk.”


		jQuery(document).ready(function(){
			jQuery('#dd_cd206d699e3eb9d49f563e7d100375a7').on('change', function() {
			  jQuery('#amount_cd206d699e3eb9d49f563e7d100375a7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
NOTE: This post is the second in the series from Dr. Roy Spencer of the National Space Science and Technology Center at University of Alabama, Huntsville. The first, made last Friday, was called Atmospheric CO2 Increases: Could the Ocean, Rather Than Mankind, Be the Reason?
Due to the high interest and debate his first post has generated, Dr. Spencer asked me to make this second one, and I’m happy to oblige. 
Here is part2 of Dr. Spencer’s essay on CO2 without any editing or commentary on my part.
(Side note: Previously, I erroneously reported that Dr. Spencer was out of the country. Not so. That was my mistake and a confusion with an email autoresponse from another person named “Roy”. Hence this new update.)

More CO2 Peculiarities: The C13/C12 Isotope Ratio

Roy W. Spencer
January 28, 2008

In my previous post, I showed evidence for the possibility that there is a natural component to the rise in concentration of CO2 in the atmosphere.  Briefly, the inter-annual co-variability in Southern Hemisphere SST and Mauna Loa CO2 was more than large enough to explain the long-term trend in CO2.  Of course, some portion of the Mauna Loa increase must be anthropogenic, but it is not clear that it is entirely so.
Well, now I’m going to provide what appears to be further evidence that there could be a substantial natural source of the long-term increase in CO2.
One of the purported signatures of anthropogenic CO2 is the carbon isotope ratio, C13/C12.   The “natural” C13 content of CO2 is just over 1.1%.  In contrast, the C13 content of the CO2 produced by burning of fossil fuels is claimed to be slightly smaller – just under 1.1%.
The concentration of C13 isn’t reported directly, it is given as “dC13”, which is computed as:
“dC13 = 1000* {([C13/C12]sample / [C13/C12]std ) – 1
The plot of the monthly averages of this index from Mauna Loa is shown in Fig. 1.

Now, as we burn fossil fuels, the ratio of C13 to C12 is going down.  From what I can find digging around on the Internet, some people think this is the signature of anthropogenic emissions.  But if you examine the above equation, you will see that the C13 index that is reported can go down not only from decreasing C13 content, but also from an increasing C12 content (the other 98.9% of the CO2).
If we convert the data in Fig. 1 into C13 content, we find that the C13 content of the atmosphere is increasing (Fig. 2).

So, as the CO2 content of the atmosphere has increased, so has the C13 content…which, of course, makes sense when one realizes that fossil-fuel CO2 has only very slightly less C13 than “natural” CO2 (about 2.6% less in relative terms).  If you add more CO2, whether from a natural or anthropogenic source, you are going to add more C13.
The question is: how does the rate of increase in C13 compare to the CO2 increase from natural versus anthropogenic sources?
First, lets look at the C13 versus C12 for the linear trend portion of these data (Fig. 3).

The slope of this line (1.0952%) represents the ratio of C13 variability to C12 variability associated with the trend signals.  When we compare this to what is to be expected from pure fossil CO2 (1.0945%), it is very close indeed: 97.5% of the way from “natural” C13 content (1.12372%) to the fossil content.
At this point, one might say, “There it is!  The anthropogenic signal!”.  But, alas, the story doesn’t end there.
If we remove the trend from the data to look at the inter-annual signals in CO2 and C13, we get the curves shown in Figures 4 and 5.


Note the strong similarity – the C13 variations very closely follow the C12 variations, which again (as in my previous post) are related to SST variations (e.g. the strong signal during the 1997-98 El Nino event).
Now, when we look at the ratio of these inter-annual signals like we did from the trends in Fig. 3, we get the relationship seen in Fig. 6.

Significantly, note that the ratio of C13 variability to CO2 variability is EXACTLY THE SAME as that seen in the trends!
BOTTOM LINE: If the C13/C12 relationship during NATURAL inter-annual variability is the same as that found for the trends, how can people claim that the trend signal is MANMADE??


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1489aab',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterCoastal history analyses increasingly suggest sea levels are lower today than at any time in the last 7000 years – even lower than the 1600s to 1800s.
Recently we compared cartology from the 17th to 19th centuries to direct aerial images of coastal positions today. Rather surprisingly, there seemed to be more land area below sea level a few hundred years ago.
For example, an 1802 nautical map of New York City and Long Island shows there may have been more open waters in this region during the Little Ice Age than in 2019.

Image Source: Amazon.com
Shoreline analysis from India also suggests the coasts were further inland during the 1600s than they are today (Mörner, 2017).

Image Source: Mörner, 2017
In another new study, the borehole sea level history for the Italian port city of Salerno reveals the coast was hundreds of meters further inland compared to today’s 7000 years ago. Even 300 years ago the coast was still much further inland (Amato et al., 2020).

Image Source: Amato et al., 2020
Citing previous studies, another new paper has today’s sea levels about 2 to 3 meters lower than they were 4000 to 5000 years ago along the coasts of Brazil (Martins et al., 2020). And, again, today’s relative sea levels seem to be the lowest of the record – lower than the Little Ice Age.

Image Source: Martins et al., 2020
Share this...FacebookTwitter "
"

A new USC study shows that Deep-sea temperatures rose 1,300 years before atmospheric CO2 rose, ruling out the greenhouse gas as driver of meltdown, says a study in Science.
Carbon dioxide did not cause the end of the last ice age, a new study in Science suggests, contrary to past inferences from ice core records. “There has been this continual reference to the correspondence between CO2 and climate change as reflected in ice core records as justification for the role of CO2 in climate change,” said USC geologist Lowell Stott, lead author of the study, slated for advance online publication Sept. 27 in Science Express. “You can no longer argue that CO2 alone caused the end of the ice ages.” Deep-sea temperatures warmed about 1,300 years before the tropical surface ocean and well before the rise in atmospheric CO2, the study found.
The finding suggests the rise in greenhouse gas was likely a result of warming and may have accelerated the meltdown – but was not its main cause. The study does not question the fact that CO2 plays a key role in climate. I don’t want anyone to leave thinking that this is evidence that CO2 doesn’t affect climate,” Stott cautioned. “It does, but the important point is that CO2 is not the beginning and end of climate change.” While an increase in atmospheric CO2 and the end of the ice ages occurred at roughly the same time, scientists have debated whether CO2 caused the warming or was released later by an already warming sea.
The best estimate from other studies of when CO2 began to rise is no earlier than 18,000 years ago. Yet this study shows that the deep sea, which reflects oceanic temperature trends, started warming about 19,000 years ago. “What this means is that a lot of energy went into the ocean long before the rise in atmospheric CO2,” Stott said. But where did this energy come from” Evidence pointed southward. Water’s salinity and temperature are properties that can be used to trace its origin – and the warming deep water appeared to come from the Antarctic Ocean, the scientists wrote. This water then was transported northward over 1,000 years via well-known deep-sea currents, a conclusion supported by carbon-dating evidence. In addition, the researchers noted that deep-sea temperature increases coincided with the retreat of Antarctic sea ice, both occurring 19,000 years ago, before the northern hemisphere’s ice retreat began.
Finally, Stott and colleagues found a correlation between melting Antarctic sea ice and increased springtime solar radiation over Antarctica, suggesting this might be the energy source. As the sun pumped in heat, the warming accelerated because of sea-ice albedo feedbacks, in which retreating ice exposes ocean water that reflects less light and absorbs more heat, much like a dark T-shirt on a hot day.  “The climate dynamic is much more complex than simply saying that CO2 rises and the temperature warms,” Stott said. The complexities “have to be understood in order to appreciate how the climate system has changed in the past and how it will change in the future.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea37da97f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe recall last year how projections of new lows in Arctic sea ice extents were boldly made, and eagerly publicised by the catastrophe-obsessed  media. It made for good headlines, but they were all wrong. See here June Outlook Report.

This year Germany’s Alfred Wegener Institute AWI here and the KlimaCampus of the University of  Hamburg are taking another shot at it, along with a dozen or so other fortune tellers. The AWI press release states:
The projection of the KlimaCampus of the University of Hamburg is 4.7 million sq km, which is more negative than the 5.2 million sq km made by the AWI scientists. Yet both research centres do not exclude a record low of 4.3 million set in 2007 being reached.
Decisive factors like ice thickness and how the rest of the summer develops are unknown and do not allow for accurate projections. Yet the AWI is projecting a sea level that is almost a million sq km over 2007. They know that the ice in the central Arctic is thick, but probably don’t want to say it publicly. Indeed, most of the sea ice fortune tellers are projecting 2010 to finish higher than 2008.
Both teams used different methods for their projections. Prof. Rüdiger Gerdes and his team at the AWI worked out a model together with the scientific companies OASys and FastOpt that uses oceanic drift buoys and satellite data for measuring the movement of ice. The projection will be revised each month during the summer. Dr. Gerdes says:
Currently we calculate with 80% probability that the sea ice extent will be between 4.7 million and 5.7 million sq km in September. The projections will become more precise as summer progresses.
Meanwhile the KlimaCampus-Team of Prof. Lars Kaleschke takes satellite photos of the Arctic sea ice for each day of 2010 and compares them to the same day of each year from 2003 to 2009.
The number and size of the ice-free areas are indicators for subsequent ice developments. These dark spots store more solar energy in early summer and thus enhance ice melt during the polar summer, as the sun does not disappear until September.
Share this...FacebookTwitter "
"

Just released: new DOE figures showing that the US has reduced CO2 production 1.5% last year in 2006, even without the US signing on to Kyoto.  You can read the full report here (Adobe PDF file).
 Here are some of the numbers for 2006:

• Total U.S. greenhouse gas emissions in 2006 were 1.5 percent below the 2005 total—the first annual drop since 2001 and only the third since 1990.
• The total emissions reduction, from 7,181.4 million metric tons carbon dioxide equivalent (MMTCO2e) in 2005 to 7,075.6 MMTCO2e in 2006, was largely a result of reductions in carbon dioxide (CO2) emissions. There were smaller reductions in emissions of methane (CH4) and man-made gases with high global warming potentials (high-GWP gases)
• U.S. carbon dioxide emissions in 2006 were 110.6 million metric tons (MMT) below their 2005 level of 6,045.0 MMT, due to favorable weather conditions; higher energy prices; a decline in the carbon intensity of electric power generation that resulted from increased use of natural gas, the least carbon intensive fossil fuel; and greater reliance on non-fossil energy sources.
Despite my stance on the measurement and interpretation errors associated with the surface temperature record, I’ve always felt that reducing pollution is a good thing. At the same time I’ve always felt that our environmental movement is too often focused on panic driven ideas.
Coupled with the news about the 2007 hurricane season being very low in my post below, I believe we’ve seen evidence that things aren’t all they are claimed to be, particularly by Gore. I think the best approach overall is to not panic, and to work on alternate energy solutions and better efficiency as a way to wean ourselves from foreign oil. The key here is slow change. It took us 100 years to get to this point, it will probably take us at least half that to reverse the trends in a sensible way with new technology.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2506bff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Biodiversity hotspots that have given species a safe haven from changing climates for millions of years will come under threat from human-driven global heating, a new study has found. Species that have evolved in tropical regions such Australia’s wet tropics, the Guinean forests of Western Africa and the Andes Mountains will come under increasing stress as the planet warms, the study finds.  Experts reacted to the study with shock, saying it was depressing that some of the Earth’s “most critical real-estate for saving nature” could be under threat. Published in the journal Nature Climate Change, the study modelled global land and ocean temperatures and rainfall for the past 21,000 years. The study then examined the impact of adding greenhouse gases to the atmosphere under two scenarios – one considered to represent very high emissions and another with much lower levels of emissions. Places that are rich in biodiversity tend to overlap with places in the tropics that have experienced relatively stable climates in the past, providing a refuge for species when other regions have warmed, the paper explains. Assoc Prof Damien Fordham, a global change ecologist at the University of Adelaide and a co-author of the research, told Guardian Australia this had allowed ancient species to survive, causing a “stacking” of biodiversity. He said: “We had hoped that what we would find was that these places would continue to have stable climates. But what’s extremely worrying is that we see a shift from stable to unstable.” He said while the study looked closely at the past 21,000 years, the areas impacted were known to have supported stable climates for millions of years. Fordham said species in these areas tended to be adapted to survive within very narrow temperature and climate boundaries. But human-caused climate change would happen too fast for species to evolve or move, so even small shifts would have large impacts. “Here we see really strong direct evidence of how climate change could have really adverse affects in the tropics that harbour the highest places for biodiversity on the planet.” He said the higher and lower emissions scenarios used to project changes to the end of this century showed similar impacts. He added: “We see this as a further reason for action on climate change, and also in considering these hotspots in our plans for climate change mitigation and adaptation.” Global expert on conservation management Prof Hugh Possingham, of the University of Queensland and chief scientist at the Nature Conservancy, said the study was “depressing”. He said there had been a common assumption that places that had acted as “climate refugia” of the past would carry out the same function in the future. He told Guardian Australia: “If this is true, and they do provide compelling arguments, it is more bad news for nature. “What I would like to see is some work on picking landscape scale actions that could ameliorate these predicted effects. “What actions could we conceivably take that would facilitate climate adaptation at these critical locations? Habitat restoration? Reducing human pressures like hunting?” Prof Bill Laurance, director of James Cook University’s centre for tropical environmental and sustainability science, who was not involved in the study, said it was “the scariest paper that I’ve read in the last couple of years”. He said: “The idea that our global biodiversity hotspots – Earth’s most critical real-estate for saving nature – will be intensely vulnerable to future climate change is enough to scare the bejesus out of anyone with a lick of common sense. “These tiny vestiges of native vegetation – essentially living arks of unique species – are going to be repeatedly body-slammed by highly fluctuating temperatures as this century progresses, according to this study. Wildfires, killer droughts, intense storms, flooding rains, the list of calamities goes on.” But he said it was important that people did not fall into the trap of feeling helpless, adding: “This work is alarming but it’s akin to being warned that there’s a massive sinkhole in the highway ahead. “We have a chance to dodge it if we start changing direction now.”"
nan
"

Well, Paul Krugman sure smeared me in his May 29 column (sub. req’d.) where he accused me of “fraud pure and simple” in congressional testimony eight (!) years ago.   
  
  
Krugman’s screed was just another salvo in the current global warming charm offensive, coinciding with Al Gore’s screeching movie, demonstrations against Max Mayfield, director of the National Hurricane Center, because he had the audacity to NOT blame last year’s Hurricane Katrina on global warming (which would have been “fraud pure and simple”), and multiple smearings of any climate scientist who dares to speak out against the current hysteria.   
  
  
Krugman was incensed with my July 27, 1998 testimony before the House Committee on Small Business. In it, my purpose was to demonstrate that commonly held assumptions about climate change can be violated in a very few short years.   
  
  
One of those is that greenhouse gas concentrations, mainly carbon dioxide, would continue on a constant exponential growth curve. NASA scientist James Hansen had a model that did just this, published in 1988, and referred to in his June 23, 1988 Senate testimony as a “Business as Usual” (BAU) scenario.   
  
  
BAU generally assumes no significant legislation and no major technological changes. It’s pretty safe to say that this was what happened in the succeeding ten years.   
  
  
He had two other scenarios that were different, one that gradually reduced emissions, and one that stopped the growth of atmospheric carbon dioxide in 2000. But those weren’t germane to my discussion. Somehow, Krugman labelled my not referring to them as “fraud.”   
  
  
The BAU scenario produced a whopping surface temperature rise of 0.45 degrees Celsius in the short period from 1988 through 1997, the last year for which there was annual data published by the United Nations’ Intergovernmental Panel on Climate Change at the time of my testimony. The observed rise was 0.11 degrees.   
  
  
I cited the reasons for this. In fact, the rate of carbon dioxide increase in the atmosphere was quite constant–rather than itself increasing like compound interest–during the period. Ten years later, Hansen published a paper in which he hypothesized that “apparently the rate of uptake by carbon dioxide sinks, either the ocean, or more likely the forests and soils, has increased.” This was not assumed in any of his scenarios. In fact, the general hypothesis has been that, as the planet warms, the ocean takes up carbon dioxide at a slower rate.   
  
  
Then, contrary to everyone’s expectation, the second most‐​important global warming emission, methane, simply stopped increasing. Some years have shown an actual drop in its atmospheric concentration. To this day, no one knows why.   
  
  
There’s also the nagging possibility that we haven’t yet figured out the true “sensitivity” of surface temperature to changes in carbon dioxide. Scientifically, that’s a chilling possibility.   
  
  
On May 30, Roger Pielke, Jr., a highly esteemed researcher at University of Colorado’s Center for Science and Technology Policy Research, examined Hansen’s scenarios. Of the two “lower” ones, he concluded, “Neither is particularly accurate or realistic. _Any conclusion that Hansen’s 1988 prediction got things right, necessarily must conclude that it got things right for the wrong reason_.” (italics in original)   
  
  
That’s precisely the keynote of my testimony eight years ago: in climate science, what you think is obviously true can literally change overnight, like the assumption of continued exponential growth of carbon dioxide, or how the earth responds.
"
"Like many young people, Joe Brindle, 17, is scared for the future because of the climate crisis. He is, he says, “angry about the injustice that is allowing the most vulnerable people in the world to suffer from the actions of the richest and most powerful”. So Brindle, who is preparing for his A-levels in Devizes, Wiltshire, decided to do something. On top of his studies, he founded a campaign group, Teach the Future, which has spent the last few months formulating legislation entitled the climate emergency education bill. The latest version has just arrived in his inbox: it has been written by a professional parliamentary draftsperson, paid for by crowdfunding. “We didn’t want our demands to be half met, so we thought we’d show them exactly what we want,” says Brindle. Hiring an experienced drafter was a nifty move to quash any notion that young people’s ideas are unworthy of serious consideration. Brindle hopes the bill will be taken forward by the government, “or it could be a private members bill”.  On 26 February Brindle and his fellow campaigners will gather in parliament’s biggest committee room to launch their bill, sponsored by Nadia Whittome, the UK’s youngest MP. Greta Thunberg, the climate activist, in her blistering speech to the United Nations last September, said that as young people begin to understand adults’ betrayal of the planet, “the eyes of all future generations” will be watching. Now young people in the UK are demanding that the government address the climate emergency through radical reform of what – and how – pupils learn. But is the government listening? Brindle founded Teach the Future to campaign for a sustainable education system, after being inspired by the global school strikes that began in 2018. Its six demands are uncompromising, he says. “We feel the education system is wasting our time, because we’re facing the biggest issue of our time, and our education isn’t even touching on it.” As well as the proposed new act, Teach the Future is calling for a government review into how the education system is preparing students for the climate emergency and the ecological crisis. It wants teacher training to assess a minimum standard of knowledge about climate change and its impact, and a national fund to help young people’s voices be heard. It calls for all new state-funded educational buildings to have a zero-carbon footprint from 2022, with the entire education sector becoming net-zero by 2030, and a youth climate endowment fund to support young people’s projects and ideas. Last week the prime minister said the UK would lead the world in cutting carbon emissions, but was accused by Claire O’Neill, a former energy minister, of a “huge lack of leadership and engagement”. So, can our education system – and the Department for Education – step up?  Many teachers have their own ideas. In Leeds, Matt Carmichael, a long-time environmental campaigner, and English and drama teacher, who has spoken at events run by the Extinction Rebellion Educators group (XR Educators), is dedicating up to three days a week unpaid to develop an action plan for schools. This will not only mean changing what is taught, Carmichael says, but recognising that our education system itself has an impact on the planet through carbon emissions, building projects and procurement practices. Carmichael’s five action points include changing how buildings are heated; preparing schools for extreme weather; and addressing the mental health implications for students of fully comprehending the climate emergency. “What happens if our temperature levels are smashed by five degrees as happened in France? Then there’s flooding, and high winds. I don’t think schools would have a clue how to make children safe. We’re going to have to think about it.” While some schools are adapting their curriculum content, many others are “burying their heads in the sand as they always did”, he says. Everyone is operating “in the most astonishing radio silence” from the Department for Education, he adds. “There’s no guidance, no staff training and no accountability structure, so schools have no idea what the government thinks they are responsible for doing about climate change. It seems business as usual is fine by the DfE, but that’s not acceptable to a lot of students, parents and teachers.” The DfE says it understands the importance of students learning about climate change and “relevant topics are included in the national curriculum for both primary and secondary schools”. However, when asked if there were plans for new guidance, a spokesperson could offer nothing specific. Frances Bowman, a sixth form art and design teacher in London and a member of XR Educators, says children need to be supported through the “fear, guilt, anger and sadness” many feel. Sadness, she says is the one that students keep bringing up, “because there’s so much being lost. And the kids feel that more acutely, I sense, than the adults do.” Bowman says Extinction Rebellion has inspired her to reassess her responsibilities as an educator. “I feel quite cautious about the protest part of it, but there is an urgency to it,” she says – and the curriculum needs to take account of that. “The climate crisis means things are going to change, and yet we teach as if it’s all going to just go on as it is.” Some organisations are creating resources. A free “climate curriculum”, launched by the social enterprise ThoughtBox Education, and a climate learning week, created by London’s City and Islington college and the University and College Union, are two such projects. But a more comprehensive, deeper and radical approach is urgently needed, says Dr d’Reen Struthers, lecturer at the Institute of Education at University College London. She is campaigning for new thinking about the ethos in schools. “It means rethinking our content-heavy curriculum of information pupils need to regurgitate, and instead helping them learn how to question the insidious agendas that are all about money being made, which have led to this ecological crisis.” Brindle’s plan, too, calls for a fundamental change. “Some people have been pushing a natural history GCSE as the solution to climate education, but I think this furthers us from the solution,” he says. “Rather than pushing this aside so that only a handful of students learn about it, we should be making it a key aspect of all parts of education. “We don’t just want future ecologists to understand sustainability. We want bankers, builders and everyone else to consider it in everything they do.”"
"Sometime in the next year or two, Nigeria will become the seventh country to reach a population of 200m or more. It is still growing considerably faster than all other nations towards the top of the list and, by 2050, the UN expects Nigeria to have the world’s third-largest population. Keeping all those mouths fed will be a huge challenge, not least because millions of Nigerians depend on fish from the Atlantic coastline, mostly caught by small-scale artisanal fishermen. And those livelihoods are now threatened by climate change, pollution and illegal fishing. Firstly, climate change: as oceans warm, habitats will be degraded and biodiversity will be lost. Many fish will migrate towards the poles to follow the cooler seas, making fishing at high latitudes more productive while tropical fisheries suffer. Nigeria, just above the equator, will be hit particularly hard. According to the World Bank, under a high CO₂ emissions scenario there will be a 53% reduction in the country’s fish resources by 2050.  Nigeria did adopt a climate change strategy as part of the UN policy process which culminated in the Paris Agreement, but it does not seem to have been widely implemented. Neither is there any evidence of an adaptation policy that would enable vulnerable coastal communities to become more resilient in the face of climate change.   Second, pollution by oil companies also threatens the livelihood of more than 6.5m people in the Niger Delta area. The 2011 Bonga oilfield spillage, a facility owned by Shell Nigeria, is just one recent example of the inadequacies of existing environmental regulations. The spill discharged an estimated 40,000 barrels of crude oil that then spread along the Atlantic coast for 185 km. Nearly 30,000 fishermen were forced to abandon their trade.  According to the Niger Delta Artisanal Fisher Association, not only have communities not been compensated, they have also been hit by reduced populations of bonga fish, a common species in the area.  As a relatively poor country with relatively rich seas, Nigeria is also vulnerable to illegal fishing by foreign vessels, predominantly from China. No wonder: the government’s fisheries department does not have any patrol boats to monitor licensed vessels and, when I interviewed a representative from the department, they claimed there has been no budgetary allocation for “monitoring, control and surveillance” in more than 15 years. In March 2018, the Nigerian navy noted that the country loses an estimated US$70 million each year due to illegal fishing.  These three threats add up to looming disaster: according to an ODI report, half of the fish species in waters off West Africa are already overexploited.  With fewer fish to catch, people who depend on the oceans for their livelihoods are seeking more creative ways to make ends meet. This has included fishing across borders, which has the potential to provoke conflict between Nigeria and its neighbours – in one 2017 incident, Cameroonian forces are alleged to have killed 97 Nigerians who they claimed had not paid a fishing levy.   With no clear climate change mitigation and adaptation strategy, weak fisheries and environmental management policies, there is growing potential for unrest. Coastal communities are attempting to build their resilience without institutional support, yet may undermine the stability of Nigeria and neighbouring countries. To see that depleting fisheries have the potential to undermine the limited stability currently enjoyed in the country, one only has to look as far as north-central Nigeria. There, a conflict between nomadic pastoralists and resident farmers due to lack of land for grazing has resulted in more than 1,000 deaths, and the displacement of millions of people.  To avoid a similar resource conflict along its coasts, Nigeria needs a national plan of action for how to help people who are left vulnerable by its depleting fisheries. This would first require robust monitoring and control mechanisms to cut out illegal fishing and ensure the country’s remaining resources are exploited sustainably. Environmental agencies also need to be better equipped to enforce existing regulations, including ensuring that oil companies clean up their spillages to the level that is seen in developed countries.  Finally, having a nicely written climate change mitigation policy is great, but its time for Nigeria to walk the walk. It must invest in climate adaptation strategies that would empower coastal communities and better prepare them to cope with the impact of depleting fisheries."
"

Thomas Friedman of the _New York Times_ has a column today provocatively titled “Why I Am Pro‐​Life.” Of course he doesn’t mean that he wants the government to protect life in utero. Instead he turns to a standard Democratic theme: How can you say you’re “pro‐​life” and oppose welfare, environmental regulation, and every other government program? Friedman doesn’t miss a beat: “common‐​sense gun control…the Environmental Protection Agency, which ensures clean air and clean water, prevents childhood asthma, preserves biodiversity and combats climate change that could disrupt every life on the planet.… programs like Head Start that provide basic education, health and nutrition for the most disadvantaged children.…”   
  
  
But then he takes it a breathtaking step further: 



the most “pro‐​life” politician in America is New York City Mayor Michael Bloomberg. While he supports a woman’s right to choose, he has also used his position to promote a whole set of policies that enhance everyone’s quality of life — from his ban on smoking in bars and city parks to reduce cancer, to his ban on the sale in New York City of giant sugary drinks to combat obesity and diabetes, to his requirement for posting calorie counts on menus in chain restaurants, to his push to reinstate the expired federal ban on assault weapons and other forms of common‐​sense gun control, to his support for early childhood education, to his support for mitigating disruptive climate change.



Thomas Friedman’s vision of “pro‐​life” policies is, in every case, a network of bans and mandates forcing us to live our lives in ways that are pleasing to him and Mayor Bloomberg. No “life, liberty, and the pursuit of happiness” for him. No, his pro‐​life vision is Ira Levin’s dystopia in _This Perfect Day_ , a world in which the state takes care of our every need.   
  
  
When Hayek, in his essay “Why I Am Not a Conservative,” wrote about “the party of life,” he described it as “the party that favors free growth and spontaneous evolution.” Not Tom Friedman’s party! And certainly also not the party that seeks to ban drugs, gay marriage, and the discussion of evolution in science class. In her book _The Future and Its Enemies_, Virginia Postrel wrote at length about “the party of life,” and she didn’t have in mind Friedman’s crabbed view of a government that “protects life” by snuffing out liberty.   
  
  
Some years ago I wrote a column titled “Pro‐​Life,” and I too had the Hayekian, not the Bloomberg‐​Friedman, view of life and liberty in mind. But long before that, as usual, Alexis de Tocqueville, in “What Sort of Despotism Democratic Nations Have to Fear,” warned us that one day Thomas Friedman and Michael Bloomberg would come for our liberties: 



Above this race of men stands an immense and tutelary power, which takes upon itself alone to secure their gratifications and to watch over their fate. That power is absolute, minute, regular, provident, and mild. It would be like the authority of a parent if, like that authority, its object was to prepare men for manhood; but it seeks, on the contrary, to keep them in perpetual childhood: it is well content that the people should rejoice, provided they think of nothing but rejoicing. For their happiness such a government willingly labors, but it chooses to be the sole agent and the only arbiter of that happiness; it provides for their security, foresees and supplies their necessities, facilitates their pleasures, manages their principal concerns, directs their industry, regulates the descent of property, and subdivides their inheritances: what remains, but to spare them all the care of thinking and all the trouble of living?   
  
  
Thus it every day renders the exercise of the free agency of man less useful and less frequent; it circumscribes the will within a narrower range and gradually robs a man of all the uses of himself. The principle of equality has prepared men for these things;it has predisposed men to endure them and often to look on them as benefits.   
  
  
After having thus successively taken each member of the community in its powerful grasp and fashioned him at will, the supreme power then extends its arm over the whole community. It covers the surface of society with a network of small complicated rules, minute and uniform, through which the most original minds and the most energetic characters cannot penetrate, to rise above the crowd. The will of man is not shattered, but softened, bent, and guided; men are seldom forced by it to act, but they are constantly restrained from acting. Such a power does not destroy, but it prevents existence; it does not tyrannize, but it compresses, enervates, extinguishes, and stupefies a people, till each nation is reduced to nothing better than a flock of timid and industrious animals, of which the government is the shepherd.
"
"It was not long ago that, if you wanted to reduce the impact of your consumer choices on the environment, your only option was to use your own shopping bag. These days, the eco-minded shopper is overwhelmed with “green” choices. With the rise of reusable pads and menstrual cups, your period can now be plastic-free. Cosmetics increasingly come in glass and aluminium containers. Even hosiery brands are swapping nylon for more eco-friendly material. Given the devastating toll of consumer waste on the health of the planet, you may find this visible drive towards sustainability on supermarket shelves cheering. But if you are a man, you may not have even noticed it: most eco-friendly products are marketed to women.  There is an obvious (and depressing) reason for this: women are not only more powerful consumers, but also disproportionately responsible, still, for the domestic sphere. The result of this is what the market research firm Mintel has termed an “eco gender gap”, where green branding might as well be pink. In a 2018 report by Mintel on the subject, Jack Duckett, a senior consumer lifestyles analyst, said women “still tend to take charge of the running of the household”, with laundry, cleaning and recycling falling under that banner. But “with eco-friendly campaigns and product claims largely aimed at female audiences”, advertisers run the risk of communicating the message that sustainability is women’s work. The idea is already insidious due to the persistent portrayal of women as caregivers – even of the planet. Janet K Swim, a professor of psychology at Pennsylvania State University who has done extensive research into the social consequences of environmentally friendly behaviour, points to a political cartoon showing Theodore Roosevelt, the US president from 1901 to 1909, wearing an apron, “trying to mock him as feminine” for his conservation policies. While it is true that women are more likely than men to be green, in the past that gender gap has been attributed to personality differences. Research from the mid-90s to early 00s pointed to women’s greater tendency to be prosocial, altruistic and empathetic; to display a stronger ethic of care; and to assume a future-focused perspective. “Research suggests that women have higher levels of socialisation to care about others and be socially responsible, which then leads them to care about environmental problems and be willing to adopt environmental behaviours,” says Rachel Howell, a lecturer in sustainable development (a subject that is, she notes, studied overwhelmingly by women at undergraduate level) at the University of Edinburgh. Whether women are born caring about the planet or learn to do so, there is evidence to suggest that femininity and “greenness” have come to be cognitively linked (by men and women) – and that this, as absurd as it may sound, is partly what puts off men from doing their bit. In a study published last year in the journal Sex Roles, Swim and her fellow researchers at Penn State found that men could be disinclined to carry a reusable shopping bag – or recycle, or any environmentally friendly activity that had been gendered as feminine – for fear of being perceived as gay or effeminate. (The same concern has been well established as a factor in men’s reluctance to adopt vegetarian or vegan diets.) Similarly, a 2016 paper in the Journal of Consumer Research found that “men may be motivated to avoid or even oppose green behaviours in order to safeguard their gender identity” and that their participation could be encouraged by weakening the association between femininity and sustainability, such as “by using masculine rather than conventional green branding”. Plastic Freedom and Package Free Shop, two popular zero-waste online retailers, say they are careful to use gender-neutral marketing – but both say about 90% of their customers are women. Lauren Singer, the founder of Package Free Shop, which sells items such as bamboo cutlery and eco cleaning products, suggests that the imbalance presents women with an opportunity to lead. “Let’s be the ones that absorb the responsibility of being the stewards and educators of sustainability,” she says. But with analysis in 2016 from the Office for National Statistics showing that women carry out an average of 60% more unpaid work than men, Howell’s suggestion may hold more appeal: to close the eco gender gap, close the associated gap “in terms of who does the laundry, who does the grocery shopping, who does the cleaning at home”. Also, get rid of “false ideas of what a sustainable product must look like”, she says, pointing out that men may carry a rucksack or reusable sports bottle without it being sold or perceived as green. The British men’s skincare company Bulldog stands out for making sustainability a cornerstone of its brand. Its plastic tubes are derived from sugar cane, not fossil fuel; it sells bamboo-handled razors and shower gel in cardboard packs; and its Original Moisturiser is certified carbon-neutral. “We’ve never thought that there’s really a gender perspective on the idea of sustainability,” says Simon Duffy, the brand’s founder. “It’s a crazy idea to me, that that is considered more feminine than masculine. Everybody should be focused on this.” For female beauty brands, sustainability may well be a way of standing out in a crowded market, says Duffy (for male beauty, “the challenge is getting men to use these types of products at all”) – but commitment should not be only skin deep. “We’re trying to make it inherent to everything we do,” he says, “because it’s the right thing to do. It doesn’t have to be a marketing story. Companies that try to use that as a tactic to sell more products are missing the point.” It is a reminder that, for a company to be truly green, its attention to sustainability should extend to every level of its business – not just the one that draws consumers. Areeba Hamid, a senior campaigner at Greenpeace, says the impact of individual choices – even giving up meat or air travel on a large scale – is negligible: “If the corporations keep drilling for oil and gas, that is not going to amount to anything.” Howell says: “While individual action is important, the individualisation of responsibility can go too far. We have got to look at the whole and try to do something at a societal level.” Even arguments about meaningful action on the climate crisis are are split down gender lines. Another study by Swim, published in the journal Global Environmental Change last year, showed that men preferred arguments that centred on science and business and tended to “attribute negative feminine traits” to men who argued on the basis of ethics and environmental justice – as women typically did. Women tend to have less trust in institutions, Howell says, which may mean they have less faith in the ability of science, technology and the government to address the issues that face us. Men, however, having been historically well served by the status quo, “are much more inclined to believe that, if they accept there is a problem, then somebody or some technology will sort it all out – that we don’t need to change our lifestyle”. Misogyny has been shown to be a factor in climate denial. A 2014 paper in the International Journal for Masculinity Studies found that: “For climate sceptics, it was not the environment that was threatened; it was a certain kind of modern industrial society built and dominated by their form of masculinity.” As Martin Gelin wrote last year in the New Republic, the highest-profile climate campaigners in the world today are two young women: Greta Thunberg and Alexandria Ocasio-Cortez. Those shouting them down are primarily older conservative men. “Occasionally, I feel quite angry,” says Howell, “because a lot of the problems have historically been created more by men, because they have more power, but it sometimes seems that women are getting more desperate about trying to solve them – and maybe have less power to do it.” But the world is changing. Millennials and Generation Z (those born between the early 80s and the mid-00s) have been shown to align broadly on the climate crisis, according to recent data compiled in the US by the Pew Research Center; even young Republicans are more likely to say that governments need to do more. “If you look at the youth climate movement, and Greta, there are a lot of young men as well as women,” says Hamid. “I think this is generational.” Duffy says that brands are changing, too. More than two-thirds (68%) of Europeans rate being environmentally friendly as more important to them than it was five years ago, according to a European Consumer Packaging Perceptions survey from 2018. For 19- to 29-year-olds, it was 80%. While some companies will change their ways only when forced, Duffy looks to Adidas’s range of trainers made from upcycled ocean plastic, and the footwear company Allbirds, as examples of those with “sustainable stories”, irrespective of the gender of their target market. “It’s certainly seeping in,” he says. “If you’re a man and you’re not picking up that this is important, something is wrong.” Additional reporting by Ruby Abbiss"
nan
"
Share this...FacebookTwitterThe German version of RIA Novosti reports that Russia hopes to gain more precise weather forecasts, new findings on global warming and improved exploration of new oil and gas reserves from its planned, new Arktika Satellite system. http://de.rian.ru/science/20100429/126119398.html
The Arktika System, which is made up of 5 satellites,  is a whole new instrument that will deliver absolutely new data on climate change says Alexander Bedrizki, Climate Appointee of the Russian President.  The project will allow continous observation of the Arctic 24 hours per day and be able to measure water temperature and ice thickness. The project will also have economic value because the Arctic holds huge oil and gas reserves. The project will also enable commerical flights to pass over the Arctic.
Alexander Frolow Director of the Russian Weather Service hopes to generate more accurate weather forecasts and to better assess events such as the recent Iceland volcano eruption which was above 60° north latitude.  Current satellite systems were not able to accurately track the cloud of ash from afar.
Russian aerospace company Lawotschkin will begin work on the project this year. Two communications satellites, two waether satellites and a radar satellite for measring ice  and exploring natural resources will be developed and launched into space.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterYet another Russian scientist believes the Arctic is set for cooling and thus increasing sea ice, this reported in the German version of the Russian online news RIA NOVOSTI (see links below). Scientist Vladimir Sokolov says:
The warming that occurred in the Arctic has swung back to cooling and sea ice that melted over the past years is recovering.
http://de.rian.ru/science/20100427/126088756.html, The English version is here: http://en.rian.ru/russia/20100427/158771845.html
Arctic sea ice reached a historic minimum in 2007 when it shrank to 4.28 million sq km. But the trend now appears to have reversed. According to the weather observation administration Roshydromet, it has grown by a fifth reaching 5.2 million square km in 2009, Sokolov said at the Petersburg Research Insttitute for the Arctic and Antarctic on Tuesday.
Sokolov calls predictions of continued shrinking Arctic ice “incorrect”.
He says the cooling is due to the polar night and the associated missing sunlight, and this as a result will lead to ice formation. Some scientists are warning that politicians and corporations who promise lucrative oil and gas projects in the Arctic may have made dramatic miscalculations. The researchers say that no warming will take place, instead cooling will impact the earth over the next decades.
Share this...FacebookTwitter "
"

Today’s question at “ _Politico_ Arena”:   
  
  
**“Have the greens failed?”**   
  
  
My response:   
  
  
If the greens have failed, it’s not for lack of trying. For years now, in everything from pre‐​school programs to “educational” ads aimed at adults, they’ve been “greenwashing” our brains. In September the _Wall Street Journal_ reported that the EPA was focusing on children: “Partnering with the Parent Teacher Organization, the agency earlier this month launched a cross‐​country tour of 6,000 schools to teach students about climate change and energy efficiency.”   
  
  
Yet for all that effort, the public isn’t buying. As _Politico_ notes this morning: “The Pew Research Center found that by last January, global warming ‘ranked at the bottom of the public’s list of policy priorities for the president and Congress this year.’ ” And “Independent voters and Republicans ranked it last on a list of 20 priorities, while Democrats ranked it 16th.” Meanwhile, “other polling suggests Americans are growing more skeptical of the science behind climate change, with those who blame human activity for global warming — 36 percent — falling 11 percentage points this year, according to Pew.” And that was _before_ “Climategate” came to light.   
  
  
At bottom, the greens face three basic problems. First, by no means is the science of global warming “settled” — if anything, the fraud Climategate surfaced has settled _that_ question. Second, even if global warming were a settled science, the contribution of human activity is anything but certain. And finally, most important, even if the answers to those two questions were clear, the costs — or benefits — of global warming are unknown, _but the costs of the proposals promoted by the greens are astronomical._   
  
  
So how do they respond to all of this? _Politico_ cites Greenpeace executive director Phil Radford: “ ‘Obama’s problem is not his position on the climate issue but, rather, his will,’ says Radford. ‘The question is how much the president will lead.’ Americans have ‘overlearned’ the lessons of Kyoto, where President Bill Clinton agreed to a treaty that he never submitted for ratification because it faced near‐​unanimous rejection in the Senate, Radford said. ‘They’re using that as a reason to hide behind Congress instead of to lead Congress.’ ”   
  
  
There you have it. It’s all a matter of will — indeed, of belief. The president needs simply to will this through, the people (and Congress) be damned. We, the anointed, know what’s right, what needs to be done. Is it any wonder that the greens are failing, at least where the people can still be heard?
"
"
Share this...FacebookTwitterReconstructions of past temperatures show much colder periods with higher CO2 levels or as-warm or warmer periods with much lower CO2 levels.
A new study (Paus, 2020) indicates modern July temperatures center around 7.5 to 8°C in the Scandes Mountains (Norway). Today’s CO2 atmospheric concentration has reached 410 ppm. 
During the latter stages of the last ice age (19,000 to 17,000 years ago), Late Glacial (LG) CO2 fluttered near 200 ppm. But with the discovery of temperature-sensitive tree species in the area it can be affirmed that July temperatures were also “at least 7-8°C” in the Scandes at this time. Despite more than a doubling of CO2, there has been no consequent summer temperature increase in this remote location.

Image Source: Paus, 2020
In another new study from Eastern Europe, Blagoveshchenskaya (2020) has determined January temperatures were almost 11°C warmer than the Little Ice Age (700 to 300 yrs ago) and 4°C warmer than today from about 6,000 to 4,500 years ago.
CO2 levels were 270 ppm when this region was 11°C warmer and 275 ppm when 11°C colder. It is 4°C colder today, at 410 ppm, than it was when CO2 was 270 ppm.
So, once again, reconstructions of Holocene temperatures do not support the narrative that CO2 and temperature changes are correlated.

Image Source: Blagoveshchenskaya, 2020


		jQuery(document).ready(function(){
			jQuery('#dd_674ed8c89aead8cfe23d2bf9d8a55622').on('change', function() {
			  jQuery('#amount_674ed8c89aead8cfe23d2bf9d8a55622').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I am Paul C. Knappenberger, Assistant Director of the Center for the Study of Science at the Cato Institute, a nonprofit, non‐​partisan public policy research institute located here in Washington DC, and Cato is my sole source of employment income. Before I begin my testimony, I would like to make clear that my comments are solely my own and do not represent any official position of the Cato Institute.



For the past 25 years, I've conducted research on topics of climate and climate change including hurricanes, heat-related mortality, and temperature trends as well as worked to quantify the projections of human-caused climate change.



This last topic, specifically how it relates to the proposed Keystone XL pipeline, will be the subject of my testimony.



When I refer to climate change in these remarks, I am specifically referring to that climate change which may occur as a result of human emissions of greenhouse gases, primarily carbon dioxide. Climate change may (and does) occur from other influences as well, both human and natural. But the primary concern raised over the Keystone XL pipeline involves the carbon dioxide emissions resulting from the burning of the oil that the pipeline will carry. So it is the potential climate change from these emissions that will be focus of my testimony.



In its Draft Environmental Impacts Statement (DEIS), the State Department has done a good job in quantifying the extra emissions that result from the extraction, transportation, refining, and eventual end use of the oil which will be transported by the Keystone XL pipeline. They find, and I think that there is broad agreement on this point, that a barrel of oil produced from the Canadian tar sands has about a 17 percent carbon dioxide emissions premium compared to the average barrel of oil finding its way into the U.S. market.



The emissions premium primarily arises from the relatively energy-intensive manner in which tar sands oil is currently extracted. In the DEIS, the State Department points out that this emissions premium may well shrink over time as new extraction methodologies are developed, as extraction in other regions, such as Saudi Arabia, becomes more energy intensive, or depending on the type of oil that is ultimately displaced by the oil carried by the Keystone XL pipeline.



The disagreement between the State Department, the Environmental Protection Agency, and several environmental groups, involves how many new carbon dioxide emissions this current 17 percent per barrel premium results in when applied to the 830,000 barrels of oil that the Keystone XL pipeline will carry each day when operating at full capacity.



The State Department concludes that the demand for the tar sands oil is great enough that it will come to market whether or not the Keystone XL pipeline is ever built. It thus finds very few additional carbon dioxide emissions resulting from the pipeline project—somewhere in the range of an additional 0.1 to 5.3 million metric tons of carbon dioxide emissions per year over the case where the pipeline is not built.



The EPA contends that the State Department is too quick to come to such a conclusion. The EPA suggests that without the pipeline, much of that oil will remain in the ground. Therefore, if the pipeline were to be built, oil would be produced from the tar sands to meet its capacity. While this won't result in more oil being used in the U.S., it will result in a 17 percent carbon dioxide emissions premium applied to the 830,000 barrels per day of delivered oil. The EPA cites an extra 18.7 million metric tons of carbon dioxide emissions per year over the situation of no pipeline.



Several environmental organizations take the view that while the Keystone XL pipeline may not increase the amount of oil used in the U.S., the oil that it displaces from the U.S. market will be consumed by other countries as the global demand for oil continues to grow. Thus, they calculate the full emissions from the 830,000 barrels per day plus the 17 percent emissions premium, and arrive at an additional 181 million metric tons of carbon dioxide emissions per year resulting from the existence of the Keystone XL pipeline.In terms of carbon dioxide emissions, these differences may appear large and contentious, and, in fact, much of the protestation involving the Keystone XL pipeline focuses on these emissions numbers.



But, these protests are largely misplaced.



It is very important to keep in mind that the end game is climate change and the potential need of climate change mitigation. _Carbon dioxide emissions are not climate change_. They influence climate change, but they are not a measure of it.



Therefore, before any type of assessment as to the potential climate impact of the Keystone XL pipeline can be made, it is essential to translate the additional carbon dioxide emissions that may result from it into climate units—such as the global average temperature. In other words, how much global warming will the Keystone XL pipeline produce?



Isn't that what everyone wants to know?



Why is it then, that such numbers are never given?



It is not as if there is no good way of calculating them—that is precisely what climate models are designed to do. These complex computer programs emulate the earth's climate system and allow researchers to change various influences upon it—such as adding additional carbon dioxide emissions—and seeing what the end effect is. These climate models produce the projections of future climate change from human activities that we are all familiar with using precisely this methodology.



General circulation climate models are very complex and computational expensive to run (both in time and money) and as a result have not been used to generate the global temperature effects of the Keystone XL pipeline.



However, in lieu of running a full climate model, climate model emulators have been developed which can run on a desktop computer. One such program is MAGICC, the Model for the Assessment of Greenhouse-gas Induced Climate Change. MAGICC is a climate model simulator developed by scientists at the U.S. National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency and other organizations.



MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models that is designed to produce an output that emulates the output one gets from much more complex climate models. MAGICC can produce in seconds, on a personal computer, results that complex climate models take weeks to produce running on the world's fastest supercomputers. MAGICC doesn't provide the breadth of output or level of detail that fully resolved climate models do, but instead simulates the general, broader aspects of climate change such as the global average temperature.



Moreover, MAGICC was developed, according to MAGICC's website, ""to compare the global-mean temperature and sea level implications of two different emissions scenarios."" So, using MAGICC to compare the climate change that is projected to result from the different Keystone XL pipeline carbon dioxide emissions scenarios fits precisely into the program's designed purpose.



Using MAGICC, I (and anyone else) can calculate the potential impact of the Keystone XL pipeline on the global average temperature based on the various carbon dioxide emissions estimates, and produce results very similar to ones that would be achieved by using a full climate model. In the base case, I run MAGICC using a mid-range, business-as-usual future emissions scenario as defined by the IPCC (SRES A1B). To examine the climate change impact using the EPA's Keystone XL carbon dioxide emissions scenario, I add 18.7 million metric tons per year to the global carbon dioxide mission total each year beginning in the year 2010 and continuing through the year 2100. To assess impact of the emissions scenario preferred by some environmental organizations, I add 181 million metric tons of additional carbon dioxide emissions to the global total beginning in 2010 and extending through 2100.



When running MAGICC as described, I find that no matter how the additional carbon dioxide emissions are calculated, the Keystone XL pipeline has an exceedingly and inconsequentially small impact on projected the course of global temperature.



In the case of the State Department's analysis, as there are very few additional carbon dioxide emissions, there is essentially no associated change in the global climate. The change in global average temperature resulting from the EPA's additional 18.7 million metric tons of carbon dioxide emissions per year from the Keystone XL pipeline, would be about 0.00001°C per year—that is one one-hundred thousandths of a degree. The 181 million metric tons per year from the assumption that all Keystone XL oil is additional oil in the global supply would result in about 0.0001°C of annual warming—one ten-thousandths of a degree.



In other words, if the Keystone XL pipeline were to operate at full capacity until the end of this century, it would, worst case, raise the global average surface temperature by about 1/100th of a degree Celsius. So after nearly 100 years of full operation, _the Keystone XL's impact on the climate would be inconsequential and unmeasurable_.



And even these tiny numbers are probably overestimates. In calculating them, I used the MAGICC default value for the magnitude of the earth's equilibrium climate sensitivity. A value, 3°C, that was based on the assessment of the equilibrium climate sensitivity given by the Intergovernmental Panel on Climate Change (IPCC). The equilibrium climate sensitivity is the amount that the earth's surface temperature will rise from a doubling of the pre-industrial atmospheric concentration of carbon dioxide. As such, it is probably the most important factor in determining whether or not we need to ""do something"" to attempt to mitigate future climate change. The lower the climate sensitivity, the less the temperature rise from human carbon dioxide emissions, and the lower the urgency to try to reduce them. If the sensitivity is low enough, carbon dioxide emissions confer a net benefit.



And despite common claims that the ""science is settled"" when it comes to global warming, we are still learning more and more about the earth complex climate system—and the more we learn, the less responsive it seems that the earth's average temperature is to human carbon dioxide emissions.



For example, the observed lack of statistically significant temperature rise over the past 16 years (and counting), is strong indication that climate models have a tendency to overestimate the amount of warming resulting from human greenhouse gas emissions (Figure 1).





Figure 1. Current (ending in December 2012) trends in three observed global surface temperature records of length 5 to 15 years (colored lines) set against the probability (gay lines) derived from the complete collection of climate model runs used in the IPCC Fourth Assessment Report under the SRES A1B emissions scenario (Knappenberger and Michaels., 2013).



I was involved in research that we published more than a decade ago pointing out that global temperatures were not rising as fast as climate model expectations (Michaels et al., 2002), and increasingly, there is a growing acknowledgement of this fact.





Figure 2. Climate sensitivity estimates from new research published since 2010 (colored, compared with the range given in the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4) (black). The arrows indicate the 5 to 95% confidence bounds for each estimate along with the best estimate (median of each probability density function; or the mean of multiple estimates; colored vertical line). Ring et al. (2012) present four estimates of the climate sensitivity and the red box encompasses those estimates. The right-hand side of the IPCC AR4 range is dotted to indicate that the IPCC does not actually state the value for the upper 95% confidence bound of their estimate and the left-hand arrow only extends to the 10% lower bound as the 5% lower bound is not given. The light grey vertical bar is the mean of the 14 best estimates from the new findings. The IPCC's ""best estimate"" (3.0°C) is 50% greater than the mean of recent estimates (2.0°C).



Over the past three years, a collection of findings in the peer-reviewed scientific literature has suggested that the IPCC's best estimate of the equilibrium climate sensitivity is likely too high by nearly 50 percent. Instead of the IPCC's 3.0°C, the new findings are indicating a value close to 2.0°C (see Figure 2).



Rerunning the MAGICC climate model simulator with an equilibrium climate sensitivity setting of 2.0°C, instead of the 3.0°C default value, drops the calculated warming impact from the Keystone XL pipeline by about 30 percent.



It is this information, not the information on carbon dioxide emissions that is required to properly assess the climate change aspect of the environmental impact of the Keystone XL pipeline.



In these terms, the difference between the State Department's Environmental Impact Statement and those of its critics all but vanish.



No matter whose carbon dioxide emissions estimate is used to calculate it, the climate impact of the oil carried by the Keystone XL pipeline is too small to measure or carry any physical significance.  
In deciding the fate of the Keystone XL pipeline, it is important not to let symbolism cloud these facts.



 **References:**



Aldrin, M., et al., 2012. Bayesian estimation of climate sensitivity based on a simple climate model fitted to observations of hemispheric temperature and global ocean heat content. _Environmetrics_ , doi: 10.1002/env.2140.



Annan, J.D., and J.C Hargreaves, 2011. On the generation and interpretation of probabilistic estimates of climate sensitivity. _Climatic Change_ , **104** , 324-436.



Hargreaves, J.C., et al., 2012. Can the Last Glacial Maximum constrain climate sensitivity? _Geophysical Research Letters_ , **39** , L24702, doi: 10.1029/2012GL053872



Intergovernmental Panel on Climate Change, 2007. Climate Change 2007: _The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change_. Solomon, S., et al. (eds). Cambridge University Press, Cambridge, 996pp.



Knappenberger, P.C., and P.J. Michaels, 2013. Policy Implications of Climate Models on the Verge of Failure. American Geophysical Union Science Policy Conference. Washington, DC, June 24-26, 2013, submitted.



Lewis, N. 2013. An objective Bayesian, improved approach for applying optimal fingerprint techniques to estimate climate sensitivity. Journal of Climate, doi: 10.1175/JCLI-D-12-00473.1.



Lindzen, R.S., and Y-S. Choi, 2011. On the observational determination of climate sensitivity and its implications. _Asia-Pacific Journal of Atmospheric Science_ , **47** , 377-390.



Michaels, P.J., Knappenberger, P.C., Frauenfeld, O.W., and R.E. Davis. 2002. Revised 21st century temperature projections, _Climate Research_ , **23** , 1-9.



Ring, M.J., et al., 2012. Causes of the global warming observed since the 19th century. _Atmospheric and Climate Sciences_ , **2** , 401-415, doi: 10.4236/acs.2012.24035.



Schmittner, A., et al. 2011. Climate sensitivity estimated from temperature reconstructions of the Last Glacial Maximum. _Science_ , **334** , 1385-1388, doi: 10.1126/science.1203513.



Wigley, T.M.L., et al. MAGICC/SCENGEN v5.3. Model for the Assessment of Greenhouse-gas Induced Climate Change/A Regional Climate Scenario Generator. http://www.cgd.ucar.edu/cas/wigley/magicc/



van Hateren, J.H., 2012. A fractal climate response function can simulate global average temperature trends of the modern era and the past millennium. _Climate Dynamics_ , doi: 10.1007/s00382-012-1375-3.
"
"

Florida State University’s COAPS (Center for Ocean-Atmospheric Prediction Studies) says that hurricane season 2007, which ends November 30th, is looking well below normal, in fact they are calling it “historic inactivity”.
According to COAPS: “Unless a dramatic and perhaps historical flurry of activity occurs in the next 11 weeks (ACE is based on calendar year, not traditional June-November hurricane season) , 2007 will rank as a historically inactive Tropical Cyclone year for the entire Northern Hemisphere. During the past 30 years, only 1977, 1981, and 1983 have had less activity to date (Jan-December). For the period of June 1 – October 19, 2007, only 1977 experienced LESS tropical cyclone activity.”
ACE Departure from Climatology thru October 24th, 2007 
Northern Hemisphere  -31% **** 316 (458) (Historic inactivity, 16% of season to go)
North Atlantic  -28% **** 63 (87) (Bill Gray wants 4 more (huh?, Season 91% over)
Eastern Pacific  -59% **** 52.2 (128) (Kiko helping out a little, Season 95% over)
Western Pacific  -25% **** 179 (237) (Still 21% of yearly activity to go)
PDI Departure from Climatology thru October 24th 2007
(PDI = Power Dissipation Index)
Northern Hemisphere  -24% **** 29687 (39101)
North Atlantic  -8% **** 6533 (7095) Effects of the Category 5’s
Eastern Pacific  -63% **** 3875 (10510) Includes Kiko
Western Pacific  -18.3% **** 17189 (21037) Includes Kajiki
Here are the named storms so far and their PDI:
Andrea 2.3 (Subtropical)
Barry 3.4
Chantal 2.5
Dean 386
Erin 1.3 (weak weak weak)
Felix 215
Gabrielle 4.0
Humberto 8.2
Ingrid 2.8
Jerry 2.4
Karen 17.2
Lorenzo 6.7
Melissa 1.9
There are some caveats:
Climatology based upon ACE (Bell et al. 2000) from 1970-2006 for each basin. ACE is not a perfect metric and does not account for storm size. Northern Hemisphere includes Northern Indian Ocean after 1976, which accounts for less than 3% of the yearly total. Data quality is a tremendous issue. The NHC declared extratropical observations were not included, which can account for up to 20% a year in additional ACE. The JTWC only started keeping track of EX phases in 2004, so there are literally 1,000 observations since the 1950s that are likely extratropical in the database (as phished out from the JMA database).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea324212a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Last year, Chairman of the Joint Chiefs of Staff Gen. Martin Dempsey contended that “we are living in the most dangerous time in my lifetime, right now.” This year, he was more assertive, stating that the world is “more dangerous than it has ever been.”



Is this accurate? At “Dangerous World? Threat Perception and U.S. National Security,” a Cato Institute Conference held in October, experts on international security assessed the supposed dangers to American security, examining the most frequently referenced threats, including wars between nations and civil wars within nations.



Historically, states have posed the greatest threats to international security. The first two panels discussed whether this is still the case, exploring the dangers not only from traditional nation‐​states but also from sub‐​state actors.



“The U.S. government has overreacted to terrorism relative to its direct physical costs,” Max Abrahms, assistant professor at Northeastern University, said. But the policy community has also inflated the risk that terrorism would spread throughout the world. “Just as the direct costs of terrorism have been overstated, so too has the political value.”



With a lack of credible state rivals since the end of the Cold War, fears have arisen in response to less traditional dangers, including cyberwar, climate change, and general instability. Mark G. Stewart, director of the Centre for Infrastructure Performance and Reliability at the University of Newcastle, subjected the worst‐​case scenarios of global warming to cost‐​benefit analyses. “My answer is that the impact of climate change on national security is manageable,” he concluded. “Change is going to be gradual—not abrupt—and there will be plenty of time to adapt.”



Given that many of these threats have been inflated, the question remains whether the global order depends on a single power enforcing the rules. In the final panel, scholars considered whether the United States must prevent general lawlessness in order to maintain our relative prosperity. Eugene Gholz, associate professor of political science at the University of Texas at Austin, challenged this thesis by focusing on how costly it is to fight wars.



“The claim that the global economy would become unhinged if the United States was not providing primacy and tamping down conflict around the world is just not true,” he concluded. In the end, many of those scholars that disagreed with the Institute’s positions nevertheless praised its scholarship—even on issues as divisive as foreign policy. “Cato scholars are very strong and clear advocates of the view that the U.S. should retrench,” said Stephen Brooks, associate professor of government at Dartmouth College. “In my view, this comprehensive version of retrenchment … is the one which is most interesting and most compelling as an alternative to the current U.S. grand strategy.”
"
"In February 2018, the Bolivian authorities captured two Chinese citizens in a poultry store in the city of Santa Cruz de la Sierra. They were apprehended in possession of 185 jaguar fangs, three feline skins, a 22-caliber pistol, a large sum of money, and body parts of many other animal species, including rattlesnakes, marsh deer, giant armadillos and jaguars. All were to be shipped to Chinese medical markets, where, according to experts in the animal illegal trade, the jaguar – a wild feline native to the Americas – is in more and more demand as a replacement for Chinese medicines derived from their “original” source – the Asian tiger. This story highlights something not often discussed when it comes to the illegal wildlife trade for traditional medicines – that traditions are a human product, a result of entangled exchanges, often driven by commerce. Thanks to these processes, new species that are strangers to Chinese lands can become staples of the black medical market as substitutes to the (dwindling) traditional ones.  The jaguar is the largest native feline species of the New World, the third largest in the world and the only extant member of the genus Panthera native to the Americas. Jaguars have already been long exploited for the fur industry, both at home and abroad. Alexander von Humboldt recorded that late in the 18th century, 4,000 jaguars were killed in the Spanish colonies annually and 2,000 were exported every year from Buenos Aires for use in the fur industry. But the historical sources do not mention any medical use of jaguar body parts. Today, the species is considered “near threatened” and the trade in jaguars and their body parts is prohibited. Numbers are, however, declining: due to the emergence of a new black market for jaguar parts located on the other side of the world. 


      Read more:
      Captive breeding has a dark side – as disturbing Czech discovery of trafficked tiger body parts highlights


 This story of substitution and surrogates in the wildlife trade has a long history, especially when it comes to medicines derived from animal parts. As the Spanish military engineer Félix de Azara put it early in the 19th century: “Humans make remote beasts bind together.” One such story that I’m currently researching revolves around the hooves of a species of large mammals. Early in the 1580s, the Milanese physician Apollonius Menabenus, former doctor of John III Vasa of Sweden, published a treatise discussing the  virtues of the elk. This “great beast”, he wrote, was a creature that suffered from epilepsy and cured itself by putting the hoof of the left hind foot into its ear.  This peculiar pairing of affliction and cure had been seen before. A similar habit had long ago been noted in the ass by Roman scholars Dioscorides and Pliny. So the fact that Swedish naturalist Olaus Magnus, one of the most important early modern authorities on this animal, described the elk as a wild ass or onager in his History of the Northern Peoples (1555) can probably explain this new belief about the elk.  This could have stimulated Menabenus to write about the elk, an animal that in Sweden had been placed under the king`s protection. Aware of the potential commercial interest in Swedish exotic and curious objects, Menabenus promoted the medical use of the so-called “nail of the great beast”. The currency of this strange tale by no means ended in Northern Europe. Late in the 18th century, the Welsh traveller, naturalist and antiquarian, Thomas Pennant, devoted a long description in his Arctic Zoology to the elk and the moose, the largest extant species in the deer family. According to Pennant, North American natives used the elk hoof in the same way it was used in Old World pharmacopeias:  The opinion of this animal’s being subject to the epilepsy seems to have been universal, as well as the cure it finds by scratching its ear with the hind hoof till it draws blood. That hoof has been used on Indian medicine for the falling-sickness; they apply it to the heart of the afflicted, make him hold it in his left hand, and rub his ear with it. On the other side of the Americas and almost at the same time, Félix de Azara attributed the same property to the hooves of the Paraguayan tapir, a large herbivorous mammal, with a short, prehensile snout, that inhabits forest regions of South America. The animal was called “gran bestia” by the Spaniards and “anta” by the Portuguese. Since then, every time the tapir was described, in no matter which region of South America, the medical virtues of the its hoof reappeared over and over again as a local, native tradition. Referring either to the moose, elks or tapirs, all the way across the Americas and Europe, the sources exhibit a recurring belief that the hooves of large mammals, often simply called “the great beast”, were valuable in treatment of human epilepsy. Pennant, the Welsh naturalist, considered this association to be a kind of universal belief or a remarkable parallel to the Old World. He saw the pattern as evidence of an underlying ancestral unity of humans that causes them to react similarly to new and evolving situations. 


      Read more:
      Pangolin: illegal medicine trade threatens these scaly mammals with extinction


 But the contemporary trade in jaguar’s fangs shows us that these reactions could have more mundane causes. The nail of the “great beast” and the jaguar´s fang have something in common. Both illustrate how the commerce in animal products transfers names and virtues across species, contributing to a human association between species that come from alien worlds. It was commerce, not universal truth, that lead to the search for surrogates that could act as the “great beast” on both sides of the Atlantic, sealing the destiny of the local animal species selected to be the source of the curing hoof.  Jaguar fangs, for now, are not used in Bolivia as medicine; their sale appears strongly linked to the community of Chinese workers who have arrived in the country in the last decade. But the story of the nail of the great beast makes it plausible that in the near future it will be considered part of the country’s traditions. We’ll have to stay tuned."
"It’s amazing what you can get away with in a hi-vis vest. On Thursday 30 January, pedestrians in some suburbs of Melbourne, Sydney and Brisbane would have walked straight past activist artists (vandals, some critics might call them) removing advertising from bus shelters and inserting artwork protesting the Morrison government and its handling of Australia’s bushfire crisis.  “Bushfire Brandalism”, the action is being called. Sydney-based artist Scott Marsh chuckles about the hi-vis vests: “It’s the cloak of invisibility.” Marsh, known internationally for his political murals, contributed a portrait of Scott Morrison with the words “climate denial” emblazoned across his forehead. A QR code on each poster links people to a bushfire-related charity of the artist’s choice. It also gives the collective an idea of how and where people are connecting with the campaign. Unfortunately, many of the works get taken down as fast as they go up. The collective said on Monday they installed 78 posters last week, describing it as “the nation’s largest unsanctioned outdoor art exhibition”. But only a few posters remain. The practice of “brandalism” – also known as “subvertising” or “anti-advertising” – isn’t new. In 1979, Billboard Utilising Graffitists Against Unhealthy Promotions (colloquially known as “Bugger Up”) was formed in Sydney and spread to other Australian cities, targeting cigarette and alcohol advertising. British artists the KLF have been associated with the practice since the 90s, and in 2015, more than 80 artists took over 600 Parisian bus shelters during the COP21 UN climate change conference. Forty-one artists are involved in this latest Australian iteration, including Georgia Hill, Tom Gerrard, Sarah McCloskey, Ghostpatrol, Callum Preston and E.L.K, as well as anonymous artists. In one poster, a Caramello Koala has burst and is melting above the words “Save an Aussie icon”. In another, Blinky Bill runs from an encroaching wall of flames. The collective launched three weeks prior to the posters going up, via a group chat of artists on Instagram. They were dismayed at what they saw as biased bushfire coverage and at the misinformation being shared by some media – particularly the Murdoch-owned press. “It felt like taking over public space was an appropriate angle to tackle it,” says one organiser who doesn’t want to be named. “There was a lot of discussion about the divided attitudes and experiences of people living in the city versus the regional areas – but we were limited in time and resources, and also those type of ‘adshell’ spaces only really exist in the city centre.” Many of the artists involved aren’t known for being political in their work. The same can’t be said for Marsh, who has for years responded to policies he takes issue with – starting with Sydney’s lockout laws, when he was an art student living behind the Coke sign in Kings Cross. “People were losing their jobs and it didn’t make any sense at all – [there was] dodgy stuff when you started looking into it,” he says. “I went to the Keep Sydney Open march, and then had the idea of doing a mural of Casino Mike [Baird, the then-NSW premier].” Then there was his mural of Alan Jones with a ball gag in his mouth; and one lampooning Israel Folau for the crowdfunding campaign he launched after Rugby Australia stripped him of his contract following homophobic social media posts (Folau is depicted begging next to a Lamborghini). Marsh’s murals are frequently defaced or painted over, such as his short-lived “Merry Crisis” Scott Morrison mural that lasted three days (he turned it into a merchandise line and raised $131,424.50 for Australian fire brigades) and a saintly George Michael portrait, which got painted over in black ink before locals reclaimed it with pro-marriage equality messages. Marsh’s depiction of Cardinal George Pell and Tony Abbott – A Happy Ending – was painted over within 24 hours by three men who labelled it “pornography”. On one occasion, Marsh even obliterated his own mural – Kanye Loves Kanye – after he reportedly sold a one-off print of it for $100,000. “The interactive stuff’s more fun than murals,” Marsh says. “I did a Fraser Anning Easter egg hunt last year.” (Those who found the eggs were encouraged to throw them at the Fraser bunny.) Before last year’s federal election, he made posters of  Clive Palmer  falling asleep during question time “and invited people to draw dicks on him”. Sometimes Marsh really goes the extra mile. Not content with the mural of Cardinal Pell in his prison greens that he put up in Sydney 50 metres away from St Mary’s Cathedral, Marsh flew to Rome on his own coin and did the same near the Vatican. When he has to act fast, he has a printed paper version of the mural, pastes it up and finishes the job with paint, but in any case, passersby were surprisingly supportive. The last of the 78 Bushfire Brandalism posters have now gone up, but for Marsh, tackling the issue of climate change is far from over. He continues to create works relating to the Adani coalmine, not least because of all the family holidays he spent at the Great Barrier Reef. “It’s really hit home for me,” he says. “I’m frustrated with the lack of action. I got distracted for a while, but you can smell it in the air that now is the time to really push on climate change. If nothing happens now, it’s never going to fucking happen.” "
"For a month or two every summer, beaches across the south-eastern Mediterranean might be packed but the inviting seas remain suspiciously empty. It’s jellyfish season, and the “nomads” are out in force. Bathers avoid the sea while the swarm is offshore; their stings are painful and may last weeks, and one is likely to feel a burning sensation even without touching the creature.  Yet the nomad jellyfish (Rhopilema nomadica) is but one of more than 350 marine invasive alien species that have traversed the Suez Canal from the Red Sea into the Mediterranean. People long dreamed of joining the two seas, and these ideas gained force as Europe’s maritime trade with the East became increasingly profitable. Indeed, one of Napoleon’s aims in his Egyptian campaign was to “…cut the Isthmus of Suez … for the French Republic”. In 1854 the French vice-consul to Egypt, Ferdinand de Lesseps, was given the go-ahead. Private investors in France put up half the capital needed and much of the rest was invested by Egypt’s ruler, Mohammed Said, who also supplied the bulk of the workforce – 20,000 conscripted fellahin (peasants) and prisoners.  That canal, completed in 1869, was 8m deep, 58-90m wide and 160km long. The canal was deepened and widened several times until it reached its present size: 24m deep and 313m wide. The Suez Canal is now one of the primary pathways for invasions by alien marine species globally – and its impact on the Mediterranean has been particularly harmful.  The invasion was foretold. Even before the original Suez Canal was excavated Leon Vaillant, a French zoologist, argued that the canal would allow organisms to “emigrate”. He called for a survey of the pre-canal ecosystem – what today would be considered a “baseline study”. That never took place. Today, massive jellyfish swarms clog intake pipes at desalination and power plants. They spoil fishing harvests and nets, stinging tourists and causing millions of euros in economic damage to the marine tourism industry.  The silver-cheeked toadfish (Lagocephalus sceleratus), a poisonous pufferfish from the Indo-Pacific Ocean, has become highly abundant in fishing catches right across the Mediterranean over the past decade and has now made it as far as Crimea. It poses severe health hazards: between 2005 and 2008, 13 people were treated for poisoning in Israel alone and hospitalisations occurred in Egypt, Lebanon, Cyprus, Turkey and Greece.  Alien invasive rabbitfish have created extensive barren areas along rocky shores. They’re algae-eaters with few competitors. Their overgrazing has dramatically reduced the habitat available for other creatures, with knock-on effects right through the food web. In August this year Egypt announced plans to further enlarge the Suez Canal, doubling its capacity by creating a new waterway parallel to the current channel. This should mean more invasions from the Red Sea. The Mediterranean is a sensitive and unusual environment – a sea full of unique wildlife. Therefore risky projects of this magnitude must, at minimum, be subject to a transparent and scientifically sound Environmental Impact Assessment (EIA) and risk analysis.  This in turn should lead to control and mitigation measures. One of the possible choices may be the introduction of locks and a salinity barrier – the original Suez Canal passed through a highly salty artificial lake which kept some organisms from crossing. Yet the publicly available information on the enlargement of the canal makes no reference to an EIA.  Marine biologists are apprehensive, and a group of 18 of us have expressed our concerns in the journal Biological Invasions. We fear an increased influx of additional marine invasions in the Mediterranean Sea.  We recognise Suez enlargement is inevitable, as global trade and shipping are vital to society. Yet the existing international agreements also urge for sustainable shipping that minimises unwanted impacts and long term consequences. There are means available to limit the introduction of invasive species, which can be carried out at the early stages of a project but which become increasingly expensive as the project progresses.  The ecological and economic cost of inaction may be substantial, potentially leading to irreparable, large-scale ecosystem damage."
"Is solar power the technology of the future? It is certainly the fastest-growing energy generation technology in the UK. By the early 2020s, according to a new report, it will be cost-competitive with gas and coal power. If so, the goal of having unsubsidised renewable energy is in sight.  The report, by Berlin-based think tank Thema1, concludes that this is possible without radical technology improvements or similar step changes. This somewhat disagrees with similar studies, which tend to point to the next big thing as being just around the corner.  There are lots of exciting developments in the laboratories but to make a real difference they need time – more than the 10-year time frame in Thema1’s forecasts, so their report is right not to factor them in. The majority of new technologies focus on the photovoltaic (PV) module itself, promising higher power output per unit (by using graphene or nanotechnologies) or much reduced production costs (using novel materials like organic solar cells).  Higher rates of converting light into electricity (“efficiencies”) are always welcome in new PV devices, but their viability depends on the production costs. It is possible today to produce cells that can convert up to 46% of the sun’s power into electricity, but costs render these commercially unfeasible. The incumbent technology, wafer-based silicon PV modules, converts about 22% of sunlight at a fraction of the cost.   On the other hand, there is a lot of excitement around technologies such as organic solar cells that are less efficient but have much reduced costs. But this approach tends to shift the balance of costs from the module to the other system components such as mounting structures and can make the system more expensive. To be commercially viable, these devices need a minimum efficiency of about 10%-12%. This recently led to the demise of virtually all thin-film silicon manufacturers, for example, which struggled to get the double-digit efficiencies in cost-effective production times.  Market shares of regular and thin-film PV The reality is that the road from laboratory cell to a full-size module is surprisingly difficult and slow. This can be seen when looking at current polysilicon thin-film technologies and how long it took them to come to their current competitive position. There is no reason to believe that other technologies will be much luckier.   Having said this, the Thema1 report is right to say that PV can achieve the costs required to survive without subsidies without any step change in technology. All it needs is the political will. If governments offer sufficient subsidies in the short term, solar will cut costs just by doing things better. This was the underlying idea of solar subsidies all around the world in recent years. Yet Thema1 suggests that all we now need to do is incrementally reduce these subsidies and by 2020 will have learned how to do things at the market price. This is not completely impossible, but there are some major caveats.  The reductions to UK subsidies of recent years are in fact one of the biggest issues in the industry at present. There were step cuts in funding that incentivised developers to rush through solar projects before cut-off dates, which resulted in installation gluts. This has been detrimental for the quality of installations, resulting in higher operation and maintenance costs and thus higher energy costs.  Governments might argue that subsidy reduction has happened each year and is therefore foreseeable, However, this ignores the fact that these “cliffs” result in a rushed building phase to meet the deadlines. Reductions typically occur in April. This means most building happens in the first quarter of the year, when the weather affects ground conditions and can drive up costs. Changing this hard funding cliff to a softer decline and shifting the timing to later in the year may actually make a noticeable difference in system costs.   UK renewables subsidies 2014/15 The cost of connections is another major issue in the UK, especially with larger developments. The connection cost is sometimes nearly as expensive as the system itself – clearly rendering the investment impossible. This may be down to weaknesses in the grid and should be addressed on a national scale. All new technologies for producing electricity have required major grid investment, so saying such moves are too expensive for solar is a bit of a smoke screen.  The most mentioned solution to making solar more competitive is to make it possible to store the electricity to get around the problem that the amount of solar energy varies during days and seasons. But this is potentially not required in the medium term. One reason is that people make the mistake of looking at technologies in isolation. There have been studies in Germany that indicate that this variability can be offset by using wind and solar together, for example. One would need to look at the combinations for the UK to see if this is true in this country as well. It is also worth pointing out that subsidies are paid to renewable electricity irrespective of the time of generation. If they were somewhat redistributed to include a timing element, it could be a way of cutting the price of PV energy without improving the technology. You can also maximise the amount of generation by shifting the orientation of the panels.  In short, the factor that has the power to make or break solar power is the political support. Combine such changes with the fact that PV still does have amazing cost-saving potential through technological progress and you have a future that could still be very sunny indeed. It is no exaggeration to say that incentive-free solar really could be on the horizon."
"
Share this...FacebookTwitterThe COVID 19-pandemic has led the German public to grow weary and distrustful of the “follow the science” mantra and to realize that it means losing liberty and fundamental rights.
Political shifts and public opinion changes are afoot in Germany as libertarians and conservatives become every more disenchanted with the restrictions.

Tens of thousands gathered in Berlin in August 1st to protest government restrictions. Image cropped here, Michael Ballweg.
Tide change in Berlin 
Last weekend tens of thousands (estimates range from 10,000 to over 1,000,000) demonstrated in Berlin to protest government restrictions aimed at limiting the spread of the COVID-19 virus. The protest was also about the limitation of free speech and the right to assemble.
Desperate, ruling politicians and their allied mainstream media responded by defaming the protesters as nutcases rather than treating them as respectable working citizens and taking their grievances seriously. The gross mistreatment by politicians and media only confirmed to many demonstrators that they’ve gone too far.
Citizens demonstrating in Stuttgart
This weekend German citizens have turned out by the thousands in Stuttgart and other cities to continue their protests and demand their freedom and rights back. A large part of the public refuses to “just believe” the government experts. In their view the science is not settled at all, and there’s no reason to continue refraining from their liberties.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




German Research Institute reverses
Another sign of changing opinion tides and distrust was the recent reversal by the German Research Foundation (DFG) which earlier had taken down a climate science critical comment by high profile satirist Dieter Nuhr from its website. In his comment Nuhr warned its reckless to “follow the science” because the science behind climate and COVID-19 is far from settled.
Under mass public pressure, the DLG reinstated Nuhr’s comment.
State now considered “a threat to civil liberty”
Moreover, the German Tagespost here warns German leaders to be wary of the current trend, and sees the political right in Germany to be transitioning away from statist traditions.
“At the latest since Hegel and in accordance with Prussian and Protestant traditions, the state had a promising, even metaphysical sound. That is currently changing,” the Tagespost comments.
US right-wing liberal thinking gaining influence
“While Chancellor Merkel was accused of state failure in the migration crisis, and thus still demanded a strong state that is able to protect its borders, the state itself is now increasingly becoming a problem. As in the USA, right-wing liberal thinking is gaining influence. The state is considered a threat to civil liberty.” adds the Tagespost. “Corona is currently whirling the relationship of the Germans to the state in a colorful confusion.”
The next major demonstration is planned for August 29 in Berlin.


		jQuery(document).ready(function(){
			jQuery('#dd_8e919b1a54fb1ca36f88be78165d8b92').on('change', function() {
			  jQuery('#amount_8e919b1a54fb1ca36f88be78165d8b92').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Picture this, a cute baby penguin, blown down a gully during a fierce storm, with no escape. You’ve been filming the natural world for weeks, following this individual. You’ve invested in it, become interested in it, attached to it – would you be able to let it die, or would you want to save it? Well, while filming an episode of Dynasties, a BBC crew decided to intervene, causing controversy in the process. As noted by presenter David Attenborough at the end of the episode, intervention by film crews is “rare”. Indeed, the goal for any nature documentary crew is to capture the living world without succumbing to emotional distress, and thus the urge to alter the things they film. So, was saving the penguin the right thing to do?  There are many arguments against what the BBC crew did: first, death is a natural process in the animal kingdom. Without dead animals many species would starve to death. In fact, many animals have evolved to feed off other dead animals – these are the scavengers, the ones that clean up the mess – and carcasses can attract thousands of animals. For instance, carcasses of whales may support fauna such as crabs, lobsters, sharks and fish, for up to 80 years. Death is also needed in order for species to evolve and become adapted to their environment: evolution via natural selection, otherwise known as “survival of the fittest”. This is where individuals that are more suited to take advantage of the resources in the environment – be they food, shelter or mates – are more likely to survive. They are also more likely to survive long enough to reproduce, hence they will pass their advantageous genes on to their offspring and, over time, we end up with a population that is perfectly adapted to their environment. Once we start intervening with this natural process we are potentially augmenting a “survival of the not-so-fit”, and often “survival of the weakest”.  However, intervention doesn’t always take the form of a benevolent documentary crew. Often, animals are saved by well-meaning people – those who want to nurture animals and provide them with the best life possible. Every spring the Royal Society for the Protection of Birds (RSPB) are inundated with calls from members of the public who have found “abandoned” birds and tried to save them. However, most of the birds haven’t been abandoned, as they are typically only a short distance from their parents. Unfortunately, well-meaning people often “rescue” animals which, in reality, means that they will either be destined to a life in captivity or will be returned to the wild without the necessary skills to survive, so will probably perish.  Born free, the true story of Elsa the lioness who was rescued by George and Joy Adamson, documents how Elsa was raised by the Adamsons until she became too big and caused chaos. By this time, Elsa had been in captivity for so long that she didn’t display the normal behaviour needed to survive in the wild – she couldn’t hunt and didn’t know how to behave around other lions – she had to learn how to be a wild lion again. The film portrays the emotional attachment and inner turmoil that Joy has towards Elsa. For example, “I know what is good for her but I don’t want to let her go”, is seen in many humans, putting their own compassion over what might be best for animals. However, to denounce the direct action of humans in this way would seem to ignore the alterations humans have already made to our environment. By destroying habitats, exploiting species, polluting the planet and introducing non-native species, humans have already caused destruction to the planet, and are the cause of the current extinction crisis. This could see much-loved species such as lions go extinct in the wild through human-induced threats including persecution and trophy hunting. Given our profound impact on wild populations, it is fitting that we help as many animals as possible. One way to do this responsibly would be to undertake more effective habitat management or educate people on aspects of conservation.  For example, Attenborough released a three-part documentary, State of the Planet, in 2000, but it didn’t attract the usual number of viewers. The danger is that repetition may make people numb to the issues, or simply stop watching. Although, more recent attempts have been far more successful. The final episode of the Blue Planet II highlighted the current plastic problem and people took notice. Indeed, the emotional response that such documentaries elicit, and that is inherent in most of us, can be beneficial. However, its benefits are often limited as the biggest conservation draws are the “flagship species”, those that are cute, cuddly, charismatic and pull on our heartstrings. So, what about the penguins? If these were animals being killed by a natural predator, it is hard to justify why you would intervene and prioritise one species of animal over another. But, if these were animals dying from a human-induced threat, surely we have a responsibility to help. Regardless of whether the storm that stranded the penguins was natural or not, I would have found it hard to watch the penguins perish."
"For several years now climatologists have puzzled over an apparent conundrum: why is Antarctic sea ice continuing to expand, albeit at the relatively slow rate of about one to two percent per decade, while Arctic sea ice has been declining rapidly (by some 13% per decade in late summer)? Just a few weeks ago the Antarctic saw a third consecutive record year of sea ice coverage. The two previous records were set in 2012 and 2013.  To help get to the bottom of this mystery, one team of scientists have enlisted an underwater robot to help measure the thickness of the ice. Their vehicle, known as SeaBED, has an upwards looking sonar which maps the underside of ice floes and provides novel, highly-detailed three-dimensional maps of Antarctic sea ice. The researchers present their findings in the journal Nature Geoscience. Measuring a total of ten ice floes covering more than 500,000 square metres, they found mean ice thicknesses of 1.4 to 5.5 metres. In some places the ice was up to 16 metres thick. This is much thicker than has been gauged by previous more limited field-based (mainly ship-based) measurements, possibly because ships tend to avoid the areas of thicker sea ice, so there may very well be a sampling selection bias.  Satellites would ideally be able to assess ice thickness over a much wider area. However, although they have had some success in the Arctic, at the other end of the world satellites are severely hampered by our poor knowledge of how much snow there is on top of any given area of Antarctic sea ice. The researchers report that the ice they measured was in its first year of growth. This is important because it is the multi-year sea ice (ice that survives more than one summer melt season) which is more susceptible to thickness growth through deformation and ridging.  Previous estimates of mean thickness for first-year Antarctic sea-ice – which date back to at least 1986 – suggest it is no more than around a metre thick on average. It has long been recognised however that much thicker multi-year ice floes exist – especially near the coast and the Antarctic Peninsula, where sea-ice ridges can be the size of a house. Although the new study is important, especially from an innovation/technological point of view, I would like to see this kind of analysis repeated across a range of different areas, and if possible seasons and years. At the end of winter around 20 million square kilometres of sea around the Antarctic is covered by ice – an area larger than Russia. The surveyed zone is tiny in comparison. If the results are confirmed by future work, they suggest Antarctic sea-ice may be more resilient towards climate warming than has previously been appreciated.  Also, changes in the sea ice will in turn affect land-based glacial ice and free-floating ice-shelves if the sea-ice suddenly gets removed (or is thicker than realised). This is especially important in regions next to the strongly warming Antarctic Peninsula where some ice shelves have dramatically broken away into the ocean. However, it will be some time yet before we know answers to the crucial question of what has caused recent sea-ice growth in Antarctica: is it changes in ocean currents, maybe related to an increase in fresher, colder sub-surface meltwaters running off from the great continental ice sheets? Nevertheless thicker Antarctic sea-ice cover has profound implications. Ice thickness closely controls the exchange of energy between the ocean beneath and the air above – without ice cover, too much heat will leave the oceans and join the atmosphere. Although ice is a very effective insulator, as soon as it reaches a few tens of centimetres thickness, equally important are holes in the ice cover. These holes, knows as leads and polynyas (leads are long rectilinear channels in the ice, while “polynya” comes from the Russian from “open” and is a larger, lake-like opening) act as natural vents or chimneys, releasing hundreds of watts  of heat per square metre into the overlying atmosphere.  Any changes in the distribution of ice thickness can dramatically affect the points where these features form and their persistence. Thus it is fair to say that Antarctic sea ice thickness plays a pivotal role in what we call sea ice-climate feedbacks, where levels of ice cover are strongly linked to ongoing global climate change and vice versa. It is therefore crucial for scientists modelling sea ice behaviour to have a good knowledge of thickness distribution to feed their models. This study represents a significant step forward."
"Boris Johnson has promised “urgent action” on the climate crisis, taking personal leadership of this year’s UN climate talks after a blistering attack by the sacked former minister who was to lead them. “Unless we take urgent action, we will get 3C hotter,” the prime minister told a gathering of climate experts, business leaders and civil society groups at the Science Museum in London on Tuesday morning. “As a country, as a society, as a planet and as a species, we must now act.” He called on all governments to follow the lead of the UK in setting a target of net zero emissions by 2050, promised support for “our Chinese friends” in their efforts to tackle species loss and environmental degradation, and announced he would bring forward the phaseout of diesel and petrol cars in the UK from 2040 to 2035. In a boost to Johnson as social media buzzed with criticism of his handling of the climate talks so far, Sir David Attenborough signalled his strong public support at the launch. “Unless we do something, [the climate crisis] becomes insoluble. That’s why Glasgow is extremely important,” said Attenborough. “It is up to us to organise the nations of the world to do something about it. That it is why it is so encouraging to know that the present government has devoted this year to it. It’s a huge encouragement to those of us who have been worrying about this problem for a very long time to know that now this government is going to do something about it.” The UK will host this year’s crunch UN talks on the climate crisis, known as COP 26, scheduled for Glasgow this November. But the launch of the government’s strategy got off to a troubled start, as the intended president of the talks, the former energy minister Claire O’Neill, was abruptly sacked late on Friday. For almost three decades, world governments have met every year to forge a global response to the climate emergency. Under the 1992 United Nations Framework Convention on Climate Change, every country on earth is treaty-bound to “avoid dangerous climate change”, and find ways to reduce greenhouse gas emissions globally in an equitable way. Cop stands for conference of the parties under the UNFCCC. The UK will host Cop26 this November in Glasgow. In the Paris agreement of 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, these targets are insufficient, and if allowed to stand would lead to an estimated 3C of heating, which scientists say would spell disaster. For that reason, the Cop26 talks in Glasgow are viewed as the last chance for global cooperation on the emergency, with countries expected to come with tough new targets on emissions. The negotiations will be led by environment ministers and civil servants, aided by UN officials. Nearly every country is expected to send a voting representative at the level of environment secretary or equivalent, and the big economies will have extensive delegations. Each of the 196 nations on earth, bar a few failed states, is a signatory to the UNFCCC foundation treaty. The Cops, for all their flaws, are the only forum on the climate crisis in which the opinions and concerns of the poorest country carry equal weight to that of the biggest economies, such as the US and China. Agreement can only come by consensus, which gives Cop decisions global authority. Fiona Harvey Environment correspondent That leaves a vacuum that will not be filled, perhaps for as long as a fortnight, as the appointment depends on a wider cabinet reshuffle. Former prime minister David Cameron was approached but declined the role. Possible other candidates are understood to include Michael Gove, a serving cabinet minister, and the former Tory party leaders William Hague and Michael Howard, both now in the Lords. O’Neill made a damaging intervention with a swingeing attack on the prime minister on BBC Radio 4’s Today programme on Tuesday morning, just before the launch. She accused Johnson of not understanding the climate emergency, of a lack of commitment and of being untrustworthy. Whitehall sources have been briefing that O’Neill had rows with her staff and gave contradictory opinions on the COP 26 talks when meeting other governments. O’Neill rebuffed these accusations in a letter to Johnson, saying: “No 10 is rumoured to be behind the media briefings put out to support your decision, which variously contained awful, false and distorted defamatory allegations. To take two examples: ‘bullying allegations’ were referred to, when you are aware that there was a single historical complaint, which was fully investigated by the Cabinet Office and found to be entirely without merit. Equally, reports of ‘problems on international engagements’ stemmed from a single blogpost which I believe can be completely rebutted by the emails.” High-ranking officials and people familiar with O’Neill’s work as COP 26 president in the few months since her appointment have said her performance has been mixed, with some criticising her for falling out with senior officials. Others have said she made a good impression on other governments at key meetings. Time is now running out, several people familiar with the talks warned. “We are certainly far behind where we should be now” in getting key countries on board, said one longtime observer of the climate negotiations. The mission for COP26 is to forge a new global consensus on the climate crisis. At Paris in 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, those national commitments are insufficient to meet the Paris goal, and if allowed to stand they would lead to an estimated 3C of heating, which scientists say would spell disaster. The world’s leading scientists, the Intergovernmental Panel on Climate Change, have warned that the world must drastically change direction in the next decade to have a hope of meeting the Paris goals. For that reason, this year’s talks are viewed as the last chance for global cooperation on the emergency. Countries are expected to come to Glasgow with stringent new targets on emissions. Johnson’s launch of the UK’s COP 26 strategy was intended to draw a line under the O’Neill row and move forward to an all-out diplomatic strategy to forge agreement among 196 nations, some of which are reluctant to come up with new targets and some of which are avowed opponents of the Paris accord. However, without a replacement for O’Neill, it looks hard to persuade people that the UK has really started on what is seen as one of the biggest international challenges this year. As one prominent attendee at the launch said: “We are getting no direction. That’s what’s missing here.”"
"
Share this...FacebookTwitterIn my recent post here I wrote about a ZDF story on an Expert Assessment Report, led by Prof. Dr. Kai Konrad of the Max Planck Institute and a team of finance researchers, on Europe’s and Germany’s climate policy. The report is titled:
Climate Policy Between Emissions Prevention and Adaptation
Expert Assessment By The Scientific Advisory Board Of The Federal Ministry of Finance
Note: The report itself is not a product of the Max Planck Institute, as some have mistakenly believed. The lead author is Dr. Kai Konrad of the Max Plank Institute, who is also vice chairman of the Finance Ministry’s Scientific Advisory Board, the actual producer of the assessment report. The members of the Scientific Advisory Board participating in the expert assessment are listed below at the end of this post.
You’ll recall the assessment report was so damning that the Finance Ministry took it down from its website. When you read the following summary and conclusion you’ll see how it completely contradicts the government’s current policy, which is to prevent CO2 emissions and to subsidise alternative energy. This is a finding that was embarrassing for the government.
Note that the authors of the assessment report take the position that CO2 is bad for the climate, i.e. the more CO2 that is produced, the worse the climate will become. They are finance experts after all, and not climate experts – obviously.
I’ve translated the all-important Part 4, Summary and Conclusion (bold print is my emphasis), which is as follows:
4. Summary and Conclusion
Economic and political action on global warming can be categorised under two kinds of measures: 1) measures that aim to slow down global warming (prevention) and 2) measures that aim to react to global warming (adaptation).
With adaptation measures, the beneficiary and the cost-bearer are the same. Decisions concerning many adaptation measures can thus be decided by the private economy. In the cases where this is not possible, the extent of adaptation measures can be handled by the local, regional or national politics.
But when it comes to measures for preventing CO2 emissions, the circle of beneficiary and the cost bearer splits apart. A meaningful reduction in emissions through uncoordinated, single country initiatives cannot be achieved. Effective emissions reduction with respect to global climate protection can be accomplished only through global coordination. In the past, global coordination has proven to be difficult and hardly successful. Despite various international attempts and considerable use of resources on the part of some countries, a worldwide climate policy has not been reached.
The theory of international public good offers an economic explanation as to why the international climate policy has not reached its ambitious goals up to now. That’s why suspicions that the current efforts will not lead to any success are being confirmed.
This assessment yields the following results:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




• The uncoordinated, single-country go-it-alone approach leads to unachievable emissions reductions. Many polluters hardly participate in avoiding emissions. It has to be expected that only the more populated, economically strongest, environmentally aware and climatically threatened countries will make any notable efforts to undertake emissions reductions.
• Efforts by single countries to act as a leader in climate protection and to influence climate policy by imposing emissions reductions on itself can cause other countries to slack off in their own climate-policy efforts rather than intensifying them. As a result, taking a leadership role in climate policy leads to, as a rule, higher costs in that country without assuring any decisive improvement in the global climate.
• Special efforts and leadership initiatives made by individual countries also do not necessarily improve the situation for a global climate agreement, but rather can actually imperil an agreement. Diminishment of remaining benefits arising from worldwide climate agreements make the realisation of an agreement more improbable.
• Also unfavourable are agreements among groups nations of the international community of nations. Such agreements greatly burden  the participating countries economically, and serve to benefit the countries that do not participate. Despite the high costs, the positive climate effects of such group-nation agreements can end up being very small. Moreover, coalitions of nations can actually worsen the chances of an international worldwide climate treaty.
However, in no way do these arguments speak against continuing international negotiations. Effective international climate agreements are urgently needed. The arguments listed above do, however, speak against going it alone nationally, taking a leadership role, in preventing CO2 emissions.
When it comes to implementing measures for adaptation to climate change, there are no problems like those listed above. Measures for adapting to climate change do not have the problems that measures for prevention have. Adapting to climatically related environmental changes do not have the “free-rider” problem, where one incurs the costs and the other reaps the benefits. The circle of beneficiary and cost-bearer are mutual when it comes to adaptation measures. The strategy of adaptation thus offers opportunities for a unilateral, cost-effective national climate policy in a wide variety of impact areas (e.g. against flooding or storm damage). At the same time, such a policy augments the chances of an international emissions limitation.
• The adaptation strategy leads to an immediate climate cost reduction in one’s own country, independent of  international agreements.
• If a country invests in national adaptation measures, it also improves its bargaining strength in negotiations for a climate treaty.
• When all countries take up adaptation strategies, it results in – when compared to an ideal, worldwide combination of both instruments – a strain that in the end favours adaptation instead of prevention. The economic-political result would be worse than the one from a non-existing prosperity-maximizing world government, but better than the result that would arise from foregoing an adaptation strategy.
• Without adaptation measures, more prevention measures would have to be undertaken due to reasons of precaution and in view of the uncertainty of climate impacts from irreversible CO2 emissions. Adaptation buys governments time to more precisely research climate impacts.
The way for some especially motivated industrial countries to use comprehensive unilateral early contributions and subsides for alternative energy is misguided with regards to a timely, binding and adequately scaled climate policy. Even worse, it is to be feared that this policy not only has been and is very expensive for Germany and Europe, but also that it is an obstacle to reaching an effective worldwide climate policy. In view of the fact that emissions reduction is an internationally public good and in view of strategy effects, the Advisory Board recommends options for adaptation to climate change be examined and pursued more vigorously by single countries than in the past. The strategy of adaptation does not only ensure immediate adaptation to climate change, but also increases the chances for an effective international agreement to reducing emissions.
Directory of members of the Scientific Advisory Board at the Federal Ministry of Finance
Prof. Dr. Clemens Fuest (chairman)
Prof. Dr. Kai A. Konrad (vice chairman)
Prof. Dr. Dieter Brümmerhoff
Prof. Dr. Thiess Büttner
Prof. Dr. Werner Ehrlicher
Prof. Dr. Lars P. Feld
Prof. Dr. Lutz Fischer
Prof. Dr. Heinz Grossekettler
Prof. Dr. Günter Hedtkamp
Prof. Dr. Klaus-Dirk Henke
Prof. Dr. Johanna Hey
Prof. Dr. Bernd Friedrich Huber
Prof. Dr. Wolfgang Kitterer
Prof. Dr. Gerold Krause-Junk
Prof. Dr. Alois Oberhauser
Prof. Dr. Rolf Peffekoven
Prof. Dr. Dieter Pohmer
Prof. Dr. Helga Pollak
Prof. Dr. Wolfram F. Richter
Prof. Dr. Ulrich Schreiber
Prof. Dr. Hartmut Söhn
Prof. Dr. Christoph Spengel
Prof. Dr. Klaus Stern
Prof. Dr. Marcel Thum
Prof. Dr. Alfons Weichenrieder
Prof. Dr. Dietmar Wellisch
Prof. Dr. Wolfgang Wiegard
Prof. Dr. Berthold Wigger
Prof. Dr. Horst Zimmermann
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterSnowFan here reports on the latest winter forecasts for the 2020/21 Europe winter. History and statistics show Europe could be in for a frosty winter. 
Currently a significant La Nina is shaping up, and history shows that these events in the Pacific have an impact on Europe’s winters:

The NOAA reanalysis above shows the temperature deviations (left) and for precipitation (right) from the WMO average 1981-2010 during the six La Niña years of winter in Europe. Large parts of Europe have average temperatures and precipitation is distributed differently, with Germany being slightly drier overall than the WMO average. Is a 2020/21 winter in Germany under La Niña conditions shaping up to have average temperatures and slightly less humidity?
Strong winter-solar correlation
A more important factor determining winter in Europe may be solar activity. Data from the German DWD national weather service since 1954 show a remarkable higher frequency of cold winters in times of low solar activity, such as we are now in the midst of.
The following chart shows the December-January-February cold temperature anomalies occurring in the times of low solar activity (circled):

After the current minimum of solar activity in December 2019, statistically it leads us to expect a crisp winter 2020/21 – not only in Germany. Source: DWD time series with supplements.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It could look like the chart below because the NOAA reanalysis shows the high statistical probability of cold winters in Europe during the weakest solar cycles after 1948. What follows is a chart showing the temperature anomalies for the winters occurring at times of low solar activity:

The winters in Europe since 1948 with the weakest solar activity so far have all been significantly colder than the 1981-2010 WMO climate average. The solar activity cycle ending in December 2019 was one of the weakest cycles ever since observations began. Source: NOAA reanalysis and long-term weather. 
But with the different statistical approaches, one thing already seems to be clear: The winter 2020/21 will probably not be particularly mild in Europe…
Also IRI expects cool 2020/21 winter
Last message: IRI continues to expect a rather supercooled winter 2020/21 in Central Europe with slightly more precipitation than average.

Like in September 2020, the IRI October forecast of Columbia University in New York also predicts a slightly cooler winter 2020/21 with slightly increased precipitation over Central Europe. Source: IRI Seasonal Forecast

Thanks to SnowFan for this report.


		jQuery(document).ready(function(){
			jQuery('#dd_61345aa3ea6c80009ddf1466cb136d50').on('change', function() {
			  jQuery('#amount_61345aa3ea6c80009ddf1466cb136d50').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEarly in 2011, NTZ readers and I entered a climate bet with Rob Honeycutt and climate warming dogmatist Dana Nuccitelli. The bet, which I dubbed the Honeycutt Climate Bet for Charity, was whether globally the 2011-2020 decade would be warmer or cooler than the previous 2001-2010 decade.
Myself and and a number of NTZ followers that bet the 2011-2020 decade would see no warming, or even cooling. Conversely Messieurs Honeycutt and Nuccitelli claimed global warming would continue, due to manmade greenhouse gas emissions, of course. To decide the winner of the bet, it was agreed that the RSS and UAH datasets would be used.
Now we are at the end of 2020, and the data suffices to declare a winner. The following chart is the latest UAH from Dr. Roy Spencer:

Chart: Dr. Roy Spencer.
As we can see, before 2016 the global mean temperature had been cooling since 1998, thus establishing what came to be known as “the hiatus”. The early half of the 2011-2020 decade had been running a bit cooler than the previous decade.
But then came the monster 2015/16 El Nino, a natural event occurring at the equatorial Pacific and it saved the warmists from losing the bet:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




That ENSO event was the strongest in memory, and thus drove global surface temperatures to a new level. As we can see from the UAH chart above, the overall current decade indeed did become warmer than the previous 2001-2010 decade. I haven’t crunched the numbers, but I think no one will dispute it.
Congratulations to the warmists on winning the bet, all thanks to the natural factors that coolists keep arguing in favor of.
Of course, the bet winners will boast and insist it’s all due to human activity. But it isn’t. Such a claim is Dominion-voting-machine science. The one deciding factor was the powerful mid-decade El Nino event, which is natural and has little to do with people burning fossil fuels.
Personally the results do not change my skeptic view of CO2’s role in climate at all. In fact the results only reinforce my view because it’s crystal clear that the 2016- 2020 warming was due to the El Nino, a natural factor, and not CO2.
200 euros for SOS Kinderdorf e.V. 
Those participating in the bet of course must honor their bets and pay the pledged amounts to a charity helping needy children. I myself will be paying to SOS Kinderdorf e.V., a Germany-based charity set up to aid needy kids, 200 euros (ca. 230 USD). I’ll be posting proof of payment in the days ahead, as soon as the bank statement comes in).
I’ll also be sending an e-mail to the other bet participants and asking them to pay up. Hopefully they are all still with us.
I don’t ever lose my climate bets. But as you can see, there’s always a first time.


		jQuery(document).ready(function(){
			jQuery('#dd_ec8e161b5077dbcad2b1a7613e27689f').on('change', function() {
			  jQuery('#amount_ec8e161b5077dbcad2b1a7613e27689f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Super Typhoon Haiyan left a path of tremendous devastation as it traversed the Philippines. Virtually all Category 5 hurricanes that make landfall do the same, with the number of casualties modulated by poverty, preparation, and preparedness (which are two different concepts).



While they are not infrequent over the open ocean, Category 5 storms don’t hit land very often. There’s some evidence that hurricanes of this intensity are unstable, and the eyewall structure required to maintain such winds often undergoes a natural change that reduces them to Category 4, or even three.





The best defense against Category 5 storms is resilient infrastructure and preparedness—characteristics surely better achieved through a free‐​market, than global governance.



But some of them do devastate a coastline before this occurs. That includes the current record‐​holding storm for (estimated) maximum wind, 1969 Hurricane Camille, which one of us (Michaels) rode out in an oceanfront laboratory not far from the eastern eyewall. Haiyan may well beat Camille’s records, but we won’t know until damage surveys are completed.



If Category 5 landfalls were more common we’d probably be more cautions as to how and where (and if) we decided to construct coastal communities. It’s a fact that buldings can be hardened to withstand some pretty severe winds. Hong Kong is going to get a Category 5 one day, and the high‐​rise building codes there are extremely stringent. It’s a matter of cost and benefit, like so many other things.



Nowadays, in the aftermath of every weather‐​related disaster, proponents of restricting fossil fuel use in the name of halting climate change are quick to place the blame for the tragedy on human‐​caused climate change (i.e., industrialized nations like the U.S.). The calls to “do something” amplify.



This is happening right now in Warsaw, at the latest (19th) in a long string of U.N.-organized Climate Change Conferences aimed at getting countries to agree to some sort of action aimed at mitigating climate change.



On the conference’s opening day, an envoy form the Philippines, Yeb Sano, gave an emotional address to the delegates in which he vowed to stop eating until something was accomplished.



“I will now commence a voluntary fasting for the climate. This means I will voluntarily refrain from eating food during this (conference) until a meaningful outcome is in sight.”



Adding,



“We can fix this. We can stop this madness. Right now, right here.”



Sano got a tear-filled standing ovation.



While the outpouring of sympathy was certainly deserved, an outpouring of action on climate change is certainly not. A story from the _Associated Press_ covering the events at the conference summed up the science on anthropogenic climate change and tropical cyclones pretty accurately:



Scientists say single weather events cannot conclusively be linked to global warming. Also, the link between man-made warming and hurricane activity is unclear, though rising sea levels are expected to make low-lying nations more vulnerable to storm surges.



In other words, limitations, even strict ones, on anthropogenic emissions of carbon dioxide and other greenhouse gases—the very thing that Sano seeks—will have no detectable (at least based on our current scientific understanding) impact on the characteristics of future tropical cyclones, such as Haiyan, or Sandy, or Katrina, or any other infamous storm. And as for sea level rise, projections are far more lurid than observations.



The hard numbers (from Ryan Maue’s excellent compilation) show that global tropical cyclone activity for the last 40+ years—during the time of decent observations and the time with the greatest potential human impact from greenhouse gas emissions—while showing decadal ups and downs, show little overall change. In fact, global cyclone activity has been below average for the past 5 years.





_Figure 1. Global cyclone activity as measured by the ACE ACE -2.14% (accumulated cyclone energy) index since 1972 (top). The Northern hemisphere ACE index is denoted by the lower (black) points, and the Southern Hemisphere ACE is the area in between the two curves (from Ryan Maue)._



The science on tropical cyclones is complicated and ultimately unclear in terms of the influence of greenhouse gas emissions, but is quite clear when it comes to the influence of demographics and wealth vs. climate change—the former grossly dominates the latter when it comes to future tropical cyclone disasters. So, no matter what this year’s U.N. climate confab does (forecast: nothing significant), it will not result in any meaningful changes to damages from future tropical cyclones.



Category 5 storms like Haiyan, Andrew, and Camille will always pose a threat to coastal communities (and beyond) in tropical cyclone-prone areas of the globe. The best defense against them is resilient infrastructure and preparedness—characteristics surely better achieved through a free-market, than global governance. But no matter what actions are taken, more Category 5 monster storms are coming. When they arrive, the news ought to focus on where they hit, not _that_ they hit.
"
"

While doom and gloom predictions continue about CO2 induced global warming, saying that it now is the largest driver of climate, overwhelming any influences of the suns variation, there appear to be other things happening. There are forecasts emerging for a wet and cold winter.
Lets review. We have a longer than normal solar minimum occurring, and we have a strong La Niña developing too. We have colder water in the Pacific.
Here is a animated view of the growing La Niña. Watch the animation, note the exapnding La Niña off the west coast of South America, note also the expanding pool of cooler water developing the Gulf of Alaska. This will be a key formation point for cold wet storms.
And there are other signs too. Acorns. Have you noticed this year we have an overabundance of acorns? I was walking in Bidwell Park a couple of weeks ago and the ground was covered with them, and they were still raining down like hailstones. I’ve never seen anything like it. This has been what biologists call a “mast year” for valley oaks.
While this may sound a bit like an “Old Farmers Almanac” moment, but I have a theory for it.
Trees are directly in touch with the sun, more so than other living things in the biosphere. Our “valiant” dendroclimatologists, like Michael Mann, point to tree rings as a proxy for earths climate. That may be true, but I think in addition to “treemometers” they also act as helioproxies too.
In a nutshell (ahem); I think it’s highly likely that trees have evolved survival strategies that are based on detecting changes in the sun’s output. It stands to reason that over the billion plus of years that plant life has been on earth and the millions of solar cycles they’ve been through, that they can detect changes in their primary energy source, the sun, and adapt accordingly. Producing abundant acorns could well be such a survival strategy.
We have strong signs of a solar cycle that is late and well below average, a near record low hurricane season, and a strong La Niña emerging.  Now we have valley oaks producing acorns like there is no tomorrow. Maybe we should heed the trees.
h/t Russ Steele at NCwatch for the animation for forecast links


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea315f833',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea35292fa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
You may recall the previous post where Basil Copeland and I looked at correlations between HadCRUT global temperature anomaly and sunspot numbers. This is similar, but looks at the Pacific Decadal Oscillation (PDO) and uses the same Hodrick-Prescott (HPT) filter as before on the HadCRUT global temperature anomaly data and the PDO Index.

click for a larger image –
NOTE: the purple line is a monthly warming rate, to get decadal values, multiply by 120
This graphic provides some context to what may be happening with the PDO. In the upper panel we’ve plotted the PDO (in red), a smoothed PDO (in light blue), and our analysis of the bidecadal variation in warming rates.
From the PDO data itself, it is just too soon to be able to tell whether the current cool phase is just one of the shorter cycles, or whether it is the beginning of a longer term cycle like we saw back in the 1950’s and 1960’s. It is tempting, when looking at the warming rate cycles, to believe that we’ve just come out of a 60-66 year “Kerr” climate cycle, and are on the cusp of a cool phase like we see for the 1950’s and 1960’s.
But if you look closely at the end of the purple curve for our warming rate cycle, it seems to be about ready to turn back up. Now we do not want to put too much stock in the end values of a series that has been smoothed with HP filtering. So it could still be on a downward trend.
Then, to make it all the more interesting, we have solar cycle 23 lingering on. Considering that also, confidence is higher that we will continue to see a relative respite in the rate of warming and that we’re not likely to see our warming rate cycle jump back to where it was during solar cycles 22-23. But whether we see a full blown interlude between two strong warming trends, like we saw during the 1950’s and 1960’s, remains to be seen.
In other words, as we saw with Easterbrook’s analysis, we can be reasonably confident in projecting at least no further warming for a while. For that to happen, the purple warming rate curve must not only turn back upwards, it must rise into the region of positive values, and continue to rise for several years. If solar cycle 24 turns out to be a weak solar cycle, and there are historical precedents for cycle length suggesting it is likely to be weak, that probably isn’t happening.
I’ll have more on solar cycles 23 and 24 coming up in the next day or so.
So, in summary; probably no net warming for awhile, and maybe a period of extended cooling as in the mid 20th century. It all depends on whether this current PDO shift is a short term or longer term event such as we saw in the mid 20th century.
This is inline with the article in today’s UK Telegraph, saying: 
“Global warming will stop until at least 2015 because of natural variations in the climate, scientists have said. Researchers studying long-term changes in sea temperatures said they now expect a “lull” for up to a decade while natural variations in climate cancel out the increases caused by man-made greenhouse gas emissions. 
The average temperature of the sea around Europe and North America is expected to cool slightly over the decade while the tropical Pacific remains unchanged. This would mean that the 0.3°C global average temperature rise which has been predicted for the next decade by the UN’s Intergovernmental Panel on Climate Change may not happen, according to the paper published in the scientific journal Nature.”
There’s a similar article in Yahoo News.
The paper by Keenlyside et al entitled “Advancing decadal-scale climate prediction in the North Atlantic sector” from the Nature website


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f4de349',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

According to Sir David Attenborough, the famous British broadcaster and naturalist, “humans are threatening their own existence and that of other species by using up the world’s resources.” In a recent interview, Attenborough said that “the only way to save the planet from famine and species extinction is to limit human population growth.”   




We are a plague on the Earth,” he continued. “It’s coming home to roost over the next 50 years or so. It’s not just climate change; it’s sheer space, places to grow food for this enormous horde. Either we limit our population growth or the natural world will do it for us, and the natural world is doing it for us right now… We keep putting on programmes about famine in Ethiopia; that’s what’s happening. Too many people there.



In 2006, Sir David Attenborough was voted Britain’s greatest living icon. Popularity, however, is no substitute for wisdom. As I have explained in a previous blog post, “[The] rate of global population growth has slowed. And it’s expected to keep slowing. Indeed, according to experts’ best estimates, the total population of Earth will stop growing within the lifespan of people alive today. And then it will fall… the long‐​dreaded resource shortage may turn out not to be a problem at all.”   
  
  
Some of the reasons why Attenborough is as mistaken about the “over‐​population problem” today as Paul Ehrlich was when he published his infamous The Population Bomb in 1968, include:   




What is to be said about Attenborough’s take on the famine in Ethiopia? In a word: embarrassing.   
  
  
To start with, population density in Monaco is 17,676 people per square kilometer. It is 79 people per square kilometer in Ethiopia. Monaco is one of the richest countries in the world and Ethiopia one of the poorest. If anything, there is an inverse relationship between population density and poverty. Some of the world’s most populated places (Hong Kong, Singapore, The Netherlands, etc.) are very rich, while some of the least heavily populated countries (Central African Republic, Chad, the two Congos, etc.) are very poor.   
  
  
The real reasons for Ethiopian famines are altogether different. First, Ethiopia was a Marxist dictatorship and like many Marxist dictatorships (USSR, PRC and Cambodia), it experienced both economic collapse and civil war. Second, Ethiopia has almost no economic freedom. All land, to give one example, is owned by the state – and the state can take it away. As a consequence, farmers have little incentive to make long term plans and undertake necessary investment, and agricultural production suffers.   
  
  
Attenborough is, in many ways, a great man and I love watching his programs. But, he thinks he knows more than he does. A little intellectual humility would not be amiss.   
  
  

"
"
U.S. Senate Report: Over 400 Prominent Scientists Disputed Man-Made Global Warming Claims in 2007 
from this link:
http://epw.senate.gov/public/index.cfm?FuseAction=Minority.SenateReport
The variety and reach of this is quite large. This is a report gathered from many independent publications, it is not one of those “Internet Petitions” which can be easily loaded up with fake names by those that seek to minimize it.
I think the gist of this is that the pronouncements of “the science is settled”, the “debate is over” and “scientific consensus” may be a bit premature.
Of course I’m sure we’ll have those that will denounce this for a variety of the usual reasons, such as the favorite “they are all employed or supported by the fossil fuel industry”.  But given the diversity on this list that will be pretty hard to prove.
For those interested in my work on the www.surfacestations.org project, this set of preliminary data posted here on 460 out of 1221 USHCN climate stations in the continental USA pretty well sums it up:

 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea203e59c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFormer Tropical Storm Edouard brings bitter temperature drop to Germany
By Kalte Sonne
(Translated/edited and image added  by P. Gosselin)
Stupidity clicks well in Germany. Alarmist messages about tropical storm Edouard now running through the Internet on numerous websites have been no better in quality than the earlier reports of an impending summer of heat shocks (The opposite has been true so far this summer).
On the other hand, reports like those by Fabian Ruhnau of Kachelmannwetter are beneficial, which assess former tropical strom Edouard somewhat differently, without neglecting the powerful thunderstorms:
“On Friday, a small, but rather weather-intensive low-pressure system will sweep over Northern Germany. It is ‘formerly EDOUARD’, the low was once a tropical storm, but is now just a normal low. In the run-up to the cold front, hot air reaches the south and southeast, where powerful thunderstorms can form. During the weekend the weather will calm down and get colder again.”

Image cropped from kachelmannwetter.com 
The kneejeck digital excitement and constant alarms are clearly a sign of our times. But a very dangerous one, because permanent alarm dulls the senses.
 
Share this...FacebookTwitter "
"
The Dr. Roger Pielke Sr. weblog today includes a letter from Dr. Joanne Simpson, recently retired.  He calls her “among the most preeminent scientists of the last 100 years”. It seems that she really spoke her mind on the subject of climate models and the problems of the changing measurement environment around climate monitoring stations.
The full letter is here on that weblog.
Excerpt: 

Since I am no longer affiliated with any organization nor receive any funding, I can speak quite frankly. […] The main basis of the claim that man’s release of greenhouse gases is the cause of the warming is based almost entirely upon climate models. 
We all know the frailty of models concerning the air-surface system. We only need to watch the weather forecasts. […] The term “global warming” itself is very vague. Where and what scales of response are measurable? One distinguished scientist has shown that many aspects of climate change are regional, some of the most harmful caused by changes in human land use. 
No one seems to have properly factored in population growth and land use, particularly in tropical and coastal areas. 
[…] But as a scientist I remain skeptical. I decided to keep quiet in this controversy until I had a positive contribution to make. […] Both sides (of climate debate) are now hurling personal epithets at each other, a very bad development in Earth sciences. 

I agree, enough of this sniping. 
Witness the cordial exchange I have with Atmoz, a graduate student at the University of Arizona in Tucson. We see things differently, each of us has made some good analyses and each of us has made some mistakes, but we don’t insult each other over it.
Though I do wish he and others would remove the cloaks of anonymity. Science has never been advanced by an anonymous person, there’s always a real person with a name at the center of discovery and progress.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0e96a5c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
The week was productive, 21 USHCN stations visited, 20 surveyed, one dropped due to access problems (Southport, NC which turned out to be at an Army Depot). My trip odometer said 1828 miles when I turned in the car in Nashville tonight.
Here is the map of my travels this week:

Click for an interactive map
The highlight of the week was of course my 2 day visit to NCDC and the survey of the new CRN station west of Asheville. Another fun moment in the trip came when I visited the Lewisburg, TN Agricultural Experiment Station. It was quite a pretty setting for a station:

While I was doing the survey, and looking for the MMTS which wasn’t near the Stevenson Screen but was indicated by the NCDC equipment log, a farm cat came by to say hello. He was quite the talker. He gave me the grand tour and followed me while I was looking around.

I asked him: “hey Kitty, have ya seen Hansen’s Bulldog around” ? He answered simply “meow” and then took off to the cattle barn. I kid you not.
Interesting thing about this trip, I identified two stations that have undergone undocumented station moves in the last year, which look like good test cases for detecting undocumented changes points via the new USHCN2 methodology. More on that later.
Footnote: While this is a lot of miles, it’s nothing compared to the mileage that Don Kostuch, Eric Gamberg, Russ Steele, and others have put in over the life of this project. I wish to thank them too.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fbaff9e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Please click the picture then continue reading.
This is the city. Los Angeles, California. I study weather stations here. I carry a thermometer. My name’s Anthony. The story you are about to see is true; the names have been changed to protect the innocent.
The day was Monday, March 24th, four days after the vernal equinox. It started out like any other day, with a bad cup of coffee and a stack of reports on scumbags you normally wouldn’t give the time of day to. But then, just as I was about to down that last gulp of coffee, a tip came in on the email hotline. It was Goetz, and his side kick Foutch.  They said there has been a heist of a weather station on the southeast side. It had been moved, and then it was dumped mysteriously on the campus of USC.
9:15AM Goetz and Foutch told me they had picked up the trail of the weather station the night before. They knew it had been bagged, and that some g-men were hopping mad about it. The g-men had written a report on the crime. In it, they claimed that because of the heist, which had been orchestrated by some other g-men at NOAA, the great City of Los Angeles had been denied it’s due: A new rainfall record year of 2004-2005. Worse than that, the temperature of the city was going down.
I’d heard about this station. It was ugly, it was dirty, it was perched on a rooftop, and it was on the wrong side of town, out by the City Department of Water and Power, just south of the Santa Ana freeway. It hung out with utility trucks and those little red street racers the punks around here drive. There was only one single photo of it. It wasn’t the kind of pristine weather station you’d take home to introduce to your mother.
10:05 AM I knew this was going to be a tough case to crack without hard as nails proof, so I decided to setup surveillance. I called in a favor from a chopper pilot named Barney that I used to share a beat with. I asked him to get aerial photos, lots of them. He asked why. I told him it was because nobody would believe that a City of Los Angeles official weather station had been on a rooftop of a parking garage and now was a shell of it’s former self sitting over at the USC campus.
I told him that when they dumped it in a cool park at USC, they killed the heart and soul of the city’s temperature record with it. And worse, they not only moved the station, but they replaced the man who had sweated and toiled on the rooftop in the hot LA smog and sun to get that weather data with one of those sissy robot contraptions. They call it an ASOS, and it has a sleek look about it, but it could never do a man’s job.
12:01 PM So Barney sets me up with the aerial surveillance from this morning. He sends the photos. I took them down to the lunch counter of the corner drugstore to develop them on my laptop. I had a cup of coffee while I did that. It cost 25 cents, and included pie.
The first aerial photo was a little fuzzy, it was hard to make out the station:

Click for a live link
But I found it, and marked it with an arrow. It wasn’t a pretty sight, right in the middle of acres of blacktop and automobiles. I kept reminding myself I’d seen worse, like in Tucson, and down the street from that Ace hardware store parking lot in Lampasas, Texas. But still, it ate at me.
12:15 PM I finished the pie, and asked for refill on the coffee. The waitress looked at the first photo and just shook her head. Barney had made several passes from several angles, and he snapped one good photo of it that hit me between the eyes like the butt end of a .38 special. There it was, our beloved City of Angels Weather Station. It made me sick just to look at it. What kind of people would do something like this?

Click for larger image
But that wasn’t all, Barney got a picture from another angle out of the archives, and it showed the station even closer looking east. It was even uglier than the other photo. Just thinking about the albedo of the parking lot in the hot LA summer made my skin crawl.

1:05 PM Barney said he had other photos, but he couldn’t get them to me now. So he put them in a file, on something called a web server. And gave me something called a link. He said any citizen of our fair city who wanted to see the terrible place where they put the City of Angels weather station could click the link and look at the photos from all angles. Good man that Barney.
The photos were good, but not good enough. I knew that these photos would eventually be seen by Judge Rabett. Rabett has told us before that pictures don’t matter in his court, so I knew this wouldn’t be enough. I had to prove the connection to the single photo taken by the g-men for their report.
2:15 PM I had figured out a way to show that the single picture taken by the g-men in their report matched the aerial photos Barney took. To do that, I used the photo lab. The guy there is named Gimp, he walks with a limp from an old command line of fire injury. But he does good work. With Gimp’s help I was able to match the camera angle of the single land photo taken by the g-men with one of the aerial photos:

Click for a larger image
3:03 PM I’d finished up the aerial surveillance work of the original scene of the crime, but I still had to get photos of the place where the body of the weather station had been dumped in the park. All I had to go on was the single photo of the park taken by the g-men for their report. It sure looked like a nice cool park and final resting place. It had a little wrought iron fence around it and reminded me of a cemetery – a cemetery where the weather goes to die. It looked good, too good. I had a hunch it wasn’t as good as it looked.
3:05 PM I called up Barney, and asked about the aerial photos where they dumped the weather station; he said he had it covered. He said to check the file he left on the webserver for the street address where the park was.
3:15 PM I was running out of time, I had to get this wrapped up today. The webserver was slow, some punks were using it for a joyride. But I finally managed to open the file. and get the street address. It was out on South Vermont Avenue.
3:30 PM The aerial photos of the campus of USC where they had dumped the weather station proved my hunch was right. The picture the g-men took made it look like a perfect little park-like setting but in reality, it was just another cruddy location surrounded by acres of concrete and asphalt. The place where they dumped the station was only a few yards from the street:

Click for a live link

Source: https://www.bing.com/maps?v=2&cp=pp3hv95484k5&style=o&lvl=2&tilt=-90&dir=0&alt=-1000&scene=6986505&encType=1
The little bit of grass and the fact that it was closer to the beach made it a little cooler. The tennis courts probably didn’t help either.
Barney also left links for the close up aerial surveillance photos he’d done. When I pulled up the one looking West, it hit me. I knew why they had dumped the weather station there. There was a parking garage just across the street. It must have felt like home.

click for a larger image

4:00 PM It was getting late, I had figured out where the original crime had occurred, and where they dumped the body of the weather station. Now all I had to do was find it’s data and I was ready to close this case.
4:15 PM I found the data in a webserver called GISTEMP. Somebody had already plotted it. Sure enough, there it was, the smoking gun. The temperature had dropped about 1.5°C when they pulled this caper in 1999. The continuity of the record had been ruined and there was now a big step function in the data that hadn’t been removed by the g-men at NCDC.
No wonder the g-men who wrote the original report were so hopping mad about it.
Since I couldn’t undo the plot, I called in Gimp again. With his help I was able to separate the time-line into red and blue segments to show where in the time-line the data had been taken from:

Click to see original graph.
5:00 PM Quitting time. I had wrapped up this investigation into the sordid story of crime against temperature in the City of Angels and gotten all the documentation together to present for the court of public opinion. I’m feeling good, I’ve served the public interest. Thats’ my job. I think I’m going to go blow another quarter on pie and coffee.
9:30 AM Tuesday Foutch reports that he’s located the entire history of the station, which can be viewed here:
http://mrcc.sws.uiuc.edu/FORTS/histories/CA_Los_Angeles_Conner.pdf
The story you have just seen is true; the names were changed to protect the incompetent.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea071e2ea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Seven years ago, Rand Simberg, an adjunct scholar at the Competitive Enterprise Institute (CEI), wrote a blog post criticizing the work of Dr. Michael E. Mann, a climatologist at Penn State University and a noted global warming doomsayer. Mann helped create the famous “hockey stick graph,” which shows temperatures spiking in the 20th century, and he was implicated in the famous “Climategate” scandal, when hackers obtained emails from the Climate Research Unit at the University of East Anglia. One of those emails described “Mike’s nature trick,” referring to the splicing together of different temperature data to “hide the decline” in global temperatures. The emails created enough controversy that investigations into Mann’s work were launched by Penn State and the National Science Foundation, both of which cleared him of any wrongdoing.



Simberg wrote that many in the skeptic community regarded the Penn State investigation as a “whitewash” because “university circled the wagons and narrowed the focus of its own investigation to declare him ethical.” He then used unfortunate language to compare the investigation to the then‐​ongoing Jerry Sandusky scandal, writing that Mann could be said to be “Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science.” He raised the Sandusky comparison because Penn State’s investigation had concluded that “in order to avoid the consequences of bad publicity” the university’s top officials had “repeatedly concealed critical facts relating to Sandusky’s child abuse from authorities.” Simberg asked if the university could be expected to “do any less to hide academic and scientific misconduct, with so much at stake” in terms of reputation and funding.



Michael Mann sued CEI, Simberg, National Review, and Mark Steyn for defamation (Mark Steyn had linked and quoted Simberg’s post in a National Review post), claiming that the accusations of scientific misconduct and data manipulation and molestation were false statements of fact (rather than opinion) that defamed his reputation as a scientist. Surprisingly, the DC Court of Appeals, which is like the state supreme court for DC, allowed the case to go forward despite the clear and disturbing First Amendment implications. Now the case is on petition to the Supreme Court. Cato has filed for the fourth time in this case, now joined by the Individual Rights Foundation and the Reason Foundation, and we’ve asked the Court to stop this dangerous case from going forward.



Defamation is one of the categories of speech unprotected by the First Amendment, and it is very important that courts keep those categories narrow or a lot of protected speech could be censored or chilled. When speech is about an ongoing debate of significant public concern, as is the case here, courts should be wary of those who want to use defamation law to shut down public debate. Mann, who has described climate change “deniers” as “shills for the fossil fuel industry,” was exonerated by the investigations into his conduct, but Simberg disagreed. The DC Court of Appeals, however, put undue weight on the investigations into Mann and other climate researchers. In the court’s view, the investigations showed not only that the allegation of “scientific misconduct” was “capable of being proved true or false, but the evidence of record is that it actually has been proved to be false by four separate investigations.”



Questioning an investigation that purports to exonerate a controversial figure should not give rise to an actionable defamation claim. Calling O.J. Simpson a murderer is not defamation because he was acquitted by a jury. Saying that someone who was exonerated by the Warren Commission did in fact have role in the Kennedy assassination is not defamation either. If this case is allowed to stand if will be the law in the District of Columbia, the pulsing heart of our political discourse. Those who arrive at conclusions contrary to official reports or investigations will be too easily subject to possible defamation suits. The Supreme Court should take the case to ensure that people can’t use courts to shut down public debate.  

"
"
Its all quiet on the solar front. Too quiet. It has now been almost 2 and a half months since the last counted cycle 24 sunspot has been seen on April 13th, 2008. There was a tiny cycle 24 “sunspeck” that appeared briefly on May 13th, but according to solar physicist Leif Svalgaard, that one never was assigned a number and did not “count”. It is just barely discernable on this large image from that day.

The sun today: spotless
NASA’s David Hathaway updated his solar cycle prediction page on June 4th. The start of cycle 24 keeps getting pushed forward while the ramp up line starts to look steeper into 2009.

Click for full sized image
The most recent forecast ( June 27th, 2008 ) from the Space Weather Prediction Center says little that would suggest our spotless streak would end any time soon:
Solar Activity Forecast: Solar activity is expected to be very
low. 
Analysis of Solar Active Regions and Activity from 26/2100Z
to 27/2100Z: Solar activity was very low. No flares occurred during
the past 24 hours and the solar disk remains spotless.

 So when will solar cycle 24 really get going? It seems even the best minds of science don’t know for certain. A NOAA press release issued last year in April 2007 calls for Cycle 24 to be up to a year late, but they can’t decide on the intensity of SC24. That argument is ongoing.
Meanwhile the NOAA SEC Solar Cycle Progression Page looks pretty flat in all metrics charted. 
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e4ce774',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Cato Institute will sponsor a week‐​long seminar near San Diego from August 1 to 7, 1999, as part of its Cato University program. Cato Sponsors will be invited to participate in the program featuring lectures and discussions on American history, law, economics, and philosophy.



Cato University allows busy adults to explore the fundamental ideas of liberty and limited government. In addition to the seminars, there is a separate 12‐​month home‐​study course that uses audiotapes, books, and an integrated study guide.



Faculty at the week‐​long program will include Alan Charles Kors, professor of history at the University of Pennsylvania and coauthor of The Shadow University; Randy Barnett, professor of law at Boston University and author of The Structure of Liberty; Don Boudreaux, president of the Foundation for Economic Education; and Tom G. Palmer, director of Cato University.



Guest lecturers will include historian Paula Baker of the University of Pittsburgh; psychologist Nathan‐​iel Branden, author of Taking Responsibility; and philosopher Christina Hoff Sommers, author of _Who Stole Feminism?_ Cato’s Edward H. Crane, David Boaz, Ted Galen Carpenter, and Robert Levy will also speak. 



In announcing the August seminar, Palmer said, “This program gives you the chance to recapture the intense intellectual atmosphere of your college days, in a climate where the lecturers and other participants share your fundamental ideas about freedom and justice. The schedule of lectures and discussions is designed to impart a great deal of information and analysis and encourage spirited discussion about the implications of the basic ideas.”



Cato University will be held at the beautiful Rancho Bernardo Inn, about 20 minutes from downtown San Diego. It will begin with dinner on Sunday, August 1, and conclude with lunch on Saturday, August 7. The cost, which includes all lectures and discussions, all meals, six nights in the inn, and a set of readings, is $1,500. Some scholarships are available for full‐​time students.



All Sponsors will receive a seminar brochure soon. Check the Web site at www​.cato​-uni​ver​si​ty​.org for more information or to register online, or call 202–789-5296. 



_This article originally appeared in the March/​April 1999 edition of_ Cato Policy Report.   
Full Issue in PDF  (16 pp., 317 Kb)
"
"

My coffee buddy, Butte County Sheriff Perry Reniff helps Alexis Dominguez exit the helicopter (Photo: Bill Husa, Chico Enterprise Record)
 Today was a good day. No, strike that, today was a GREAT day!
The saga of the Dominguez family lost in the snow looking for a Christmas tree hit home with me in a big way, because I had people from all over asking me what the weather was going to do to the search and rescue effort. I was the bearer of bad news, which I hated, because the winter storm bearing down made survival even less likely.
(Note: for national/international readers of this blog, this story unfolded in my home city and county)
Mountain weather is unforgiving. Fortunately, they knew what to do. They improvised a snow cave, wrote “HELP” in the snow, and stayed put until rescuers could find them. When they did, the relief was nation-wide.
Yes, its the best Christmas present anybody could ever have.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea213c461',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWind energy investment plummeted in the third quarter of this year in Germany, reports the online IWR here.
“The nine-month figures now available make it clearer that this trend will continue in 2020,” writes the IWR.
Issued construction licenses drop 70 percent in 3 years
Installation of wind energy in Germany peaked in 2017 (see bar chart here), but has since fallen sharply after the German federal government enacted new rules and regulations against their construction.
According to Clean Energy Wire here,
An analysis by energy industry lobby group BDEW found that the falling number of permits issued for onshore wind turbines was the main factor behind the decline, with issued licenses dropping by 70 percent over three years. About 11 GW, roughly 2,000 turbines, were stuck in bureaucratic procedures as of mid-2019.”
In the third quarter of 2020, only 85 turbines with a total capacity of 293 MW were installed. Germany has approximately 30,000 turbines operating.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the first nine months of 2020, only a total of 306 new wind turbines  (1,104 MW) were added.
The IWR forecasts 1200 MW of new onshore wind energy capacity to be added for the full year 2020. In 2019 the figure was slightly lower at 1,078 MW. The slight increase, the IWR reports “is not sufficient to compensate for the decline in offshore wind energy.”
“Overall, it is thereforeexpected that the total increase in new wind power capacity in the current year will again be significantly weaker than in the already weak previous year 2019.”
Green energy expansion “being totaled”, a “farce” 
Green energy lobbyist Volker Quaschning at Twitter sees Germany’s green energy expansion as being “totaled” and the country’s promises of climate protection as “a farce”

Totalschaden mit Ansage: Ausbau der #Windkraft bricht noch weiter ein. So werden alle deutschen #Klimaschutz-Versprechen zur Farce. Lieber @peteraltmaier, @BMWi_Bund: Wann sorgen Sie endlich dafür, dass wenigstens Ihre Ausbauziele eingehalten werden? https://t.co/5JhhFYoUml
— Volker Quaschning (@VQuaschning) October 6, 2020



		jQuery(document).ready(function(){
			jQuery('#dd_a568152ecfe8f688295399cd73262f09').on('change', function() {
			  jQuery('#amount_a568152ecfe8f688295399cd73262f09').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Today we look at October mean temperatures for the emerald island country of Ireland, the Scandinavian country of Sweden and Finland.
Global warming alarmists claim that the globe is warming, which intuitively would tell us summers should be getting longer, which in turn would mean the start of fall is getting pushed back. In such a case, September and October temperatures should be warming, but they are not!
Cooling Ireland
First we plot the mean temperature for 7 stations in Ireland for the month of October, for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years:

Data source: JMA
Seven of 7 stations in Ireland have seen a strong cooling trend for October since 1995.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Nothing unusual happening in Greta’s Sweden
Next we look at the October trends for 6 stations in climate activist Greta Thunberg’s home country of Sweden.

Data Source: JMA. 
Three of 6 stations in Sweden show no warming trend for October. Greta can begin to calm down and stop worrying herself to death about climate doom. As is the case for all the charts shown here, we plot the stations for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years
Stable climate in Finland
Finally we look at Sweden’s Nordic neighbor of Finland. Here as well there’s little indication of a widespread warming.
Data source: JMA.
Three of 6 stations in Finland in fact showed a modest cooling trend for October over the past quarter century. There’s nothing to be alarmed about.


		jQuery(document).ready(function(){
			jQuery('#dd_8dedc33634186ee6cf8964dc52c4f36c').on('change', function() {
			  jQuery('#amount_8dedc33634186ee6cf8964dc52c4f36c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

After the Second World War, the entrepreneur virtually disappeared from economic analysis (Baumol 1968). This neglect followed from the emerging models of general equilibrium that formed one aspect of the core of economic theory.1 By assumption, the Walrasian auctioneer knew the appropriate prices necessary to equate quantity supplied with quantity demanded in each market. In addition, the auctioneer knew when and by how much to adjust prices when an exogenous factor changed such as income or production technology. Trade only occurred at equilibrium prices so that markets cleared. No market participant chose or changed prices; it occurred exogenously.



Kenneth Arrow recognized the lack of real world mechanisms to determine and adjust prices in competitive markets. He identified a logical gap in the perfectly competitive model. He wrote that “there is no place for a rational decision with respect to prices as there is with respect to quantities” (Arrow 1959: 42). Prices exist independent of consumer and firm behavior. A complete model would have to provide a solution to the conundrum.2



Israel Kirzner (1967, 1971, 1973) responded to Arrow’s challenge. He argued for the reintroduction of the entrepreneur into economic analysis in order to explain how markets work. He offered a novel interpretation of the role of the entrepreneur in explaining how markets adjust to changes in conditions. Entrepreneurs recognized profit opportunities that no one else had. What appeared as an equilibrium price was not; it was a disequilibrium price that once recognized would yield profits. The fundamental aspect of entrepreneurship is alertness. Entrepreneurs, wrote Kirzner (1997: 72), notice “hitherto unnoticed profit opportunities” that arise from disequilibrium prices.



Kirzner did not devote many pages in his writings to discussing economic development. He focused on microeconomic processes, especially those pertaining to the mechanisms necessary to attain or approach equilibrium states. He devoted less time to understanding the role of entrepreneurship in explaining the differences in income per capita around the world. However, early in the development of his theory of entrepreneurship, he did critically examine the role of the entrepreneur in explaining comparative economic development.



In “Entrepreneurship and the Market Approach to Development,” Kirzner (1971) offered a definition of the entrepreneur that predated his more well‐​known 1997 definition. He argued that development economics in the 1960s, outside of the Schumpeterian variety, did not include the entrepreneur and, as a result, explanations regarding the differences in income around the world were not adequate. Like general equilibrium models, models of economic development lacked an endogenous source of innovation, invention, and resource reallocation.



Kirzner’s critique of development economics included a citation to P.T. Bauer and Basil Yamey’s 1957 book, _The Economics of Under‐​Developed Countries_. He argued that their contribution missed the central feature of entrepreneurship—namely, alertness to new opportunities to make a profit. I disagree. Indeed, Bauer and Yamey (1957: 106) identified the same aspect of entrepreneurship that Kirzner would later stress—”hitherto unsuspected opportunities for profitable economic activity.” Even though Kirzner (2005: 465) referred to the Bauer‐​Yamey book as a “classic” in development economics, he failed to fully recognize the pioneering contributions they made to understanding the true nature of entrepreneurship.



Neoclassical economics, as characterized by Arrow‐​Debreu general equilibrium, does not explain how prices emerged from the trading process. Prices existed prior to exchanges. The Walrasian auctioneer knew the necessary information regarding consumers’ preferences and information as well as the cost curves facing the firms. After collecting all the information, the auctioneer identified the vector of prices necessary to clear markets. Trade only occurred after prices were announced. Arrow identified this puzzling aspect of neoclassical theory; it lacked a theory of how prices change in a competitive market: “Each individual participant in the economy is supposed to take prices as given and determine his choices as to purchases and sales accordingly; there is no one left over whose job its is to make a decision on the price” (Arrow 1959: 43). Prices existed independent of the decisions of individuals. No one within the model set or changed prices.



Interest in the entrepreneur increased in development eco­nomics in the early 1960s as alternatives to general equilibrium theorizing appeared. Irma Adelman (2001) argued that entrepreneurship became the central variable in development policy from 1958 to 1965, while McClelland (1961), Hagen (1963), Baumol (1968), and Leibenstein (1968) each offered their own attempt to include the entrepreneur. Peter Kilby (1971) identified no less than 13 aspects of entrepreneurship related to economic development. Entrepreneurship appeared on the intellectual agenda but its essential component—alertness—did not.



Kirzner (1971) developed the arguments that would later appear in _Competition and Entrepreneurship_ (1973). He criticized development and growth economics for misunderstanding the role of the entrepreneur:



In a footnote following the above quote, Kirzner cites Bauer and Yamey (1957). He includes them in the group of economists who have discussed entrepreneurship but failed to address its central feature of alertness—perceiving hitherto unnoticed profit opportunities. He should not have, because Bauer and Yamey identified the central aspect of entrepreneurship that Kirzner stressed. They, too, recognized and discussed entrepreneurial alertness.



Kirzner goes on to argue that:



Kirzner recognized the importance of the entrepreneur in explaining economic growth and development as did many others in the 1960s. Unlike the others, Kirzner identified an aspect of entrepreneurship they did not. In order for economies to grow, someone had to grasp “the knowledge which might otherwise remain unexploited” (Kirzner 1971: 197). But Kirzner was not the first to recognize the importance of alertness.



Although much of development economics in the 1950s and 1960s neglected the entrepreneur, the contributions of P. T. Bauer and Basil Yamey did not. Rather, they emphasized the importance of the entrepreneur in their earliest writings. Bauer clearly identified entrepreneurship as a vital but neglected aspect of orthodox development economics. His early studies on trade in West Africa (Bauer 1954) provided ample evidence that entrepreneurship was omnipresent and was vital in understanding how economies evolve from low to high levels of income per capita. For example, Bauer (1954: 30) wrote that the trader‐​entrepreneur (as he referred to entrepreneurs) in Nigeria and the Gold Coast exhibited the following characteristics: “exceptional effort, foresight, resourcefulness, thrift and the ability to perceive economic opportunity.” Trader‐​entrepreneurs, at least those in Nigeria and the Gold Coast, perceived profit opportunities. They were alert. Entrepreneurs recognized the gains that emerged from changes in relative scarcities, new information, new ideas, or serendipity.



In other writings, Bauer continued to argue that the trader‐​­entrepreneurs existed throughout the developing world. His fieldwork in sub‐​Saharan Africa provided plenty of evidence. According to Bauer ([1963] 1972: 347), “The prominence of foreigners in African commerce reflects technical and administrative skills, thrift, [and] the ability to perceive and take advantage of economic opportunity.” Once again, entrepreneurs perceive economic opportunity when it arises. They do not simply respond to a given set of prices, production techniques, or information. Entrepreneurs engage in more than arbitrage. They recognize profit opportunities no one else had and develop new means to attain their goals.



Bauer and Yamey (1957) offered a comprehensive discussion of the source of economic development that extended beyond conventional models at the time that stressed capital formation or the rate of savings. They stressed a number of factors including the quality of public institutions and policies, the importance of international and intranational trade, values, and attitudes. More importantly, central to their argument, they stressed entrepreneurial alertness and its perception of “hitherto unsuspected opportunities.”



Bauer and Yamey began their discussion of the entrepreneur by noting that entrepreneurship occurs quietly through small changes that raise productivity. Better knowledge of prices and costs allow the entrepreneur to make profits. Knowledge of productivity increasing techniques also represents an aspect of entrepreneurship. But they extended entrepreneurship beyond greater knowledge of existing conditions. In some cases, it leads to significant changes. In particular, “Innovation and the exercise of entrepreneurship in the sense of creating or taking advantage of _hitherto unsuspected opportunities for profitable economic activity_ are often dramatic in their impact,” wrote Bauer and Yamey (1957: 102, emphasis added). They went on to argue that “the ability of individuals to perceive new opportunities for profit and the ability and willingness to exploit them are indeed crucial in economic development” (ibid.). From these passages, it is clear that Bauer and Yamey held views similar to those of Kirzner regarding the central features of entrepreneurship.



Bauer and Yamey (1957: 102) pointed to how entrepreneurs help generate new ideas and new techniques that foster economic development:



Kirzner’s contribution to the theory of entrepreneurship clearly has its antecedents in the works of Bauer and Yamey. Yet, their contribution appears to have been forgotten. There is no mention of their pioneering work on entrepreneurship in Kirzner’s seminal book, _Competition and Entrepreneurship_ (1973); nor in Kirzner’s 1997 _Journal of Economic Literature_ article, which is a survey of his theory of entrepreneurship. Neverthelss, in Kirzner’s (2005) contribution to a conference volume in honor of Bauer after his death, he called Bauer and Yamey’s 1957 book a “classic.”



Bauer and Yamey’s (1957) identification of entrepreneurship with “hitherto unsuspected opportunities for profitable economic activity” prior to Kirzner does not imply that he did not make a scientific contribution to the theory of entrepreneurship. Originality is only one aspect of scientific progress. As George Stigler (1955: 294) noted, “Scientific originality in its important role should be measured against the knowledge of a man’s contemporaries. If he opens their eyes to new ideas or to new perspectives on old ideas, he is important in the scientifically important sense.”



Even though Bauer and Yamey identified an aspect of entrepreneurship that had eluded development economists and the profession more broadly, Kirzner’s discussion opened the eyes of others. For example, Schultz (1975, 1990) developed an alternative theory of entrepreneurship that emphasized the role of human capital partially in response to Kirzner in order to better understand the process of economic development. Moreover, Baumol (1990) differentiated between productive and unproductive entrepreneurship, and differentiated his approach from Kirzner’s. Although, Bauer and Yamey emphasized the role of entrepreneurship in explaining the process of economic development, Kirzner brought a new perspective to an old idea.



Adelman, I. (2001) “Fallacies in Development Theory and Their Implications for Policy.” In G. Meier and J. Stiglitz (eds.), _Frontiers of Development Economics: The Future in Perspective_ , 103–34. New York: Oxford University Press.



Arrow, K. J. (1959) “Toward a Theory of Price Adjustment.” In M. Abramovitz et al. (eds.), _The Allocation of Economic Resources: Essays in Honor of Bernard Francis Haley_ , 41–51. Stanford, Calif.: Stanford University Press.



Bauer, P. T. (1954) _West African Trade: A Study of Competition, Oligopoly and Monopoly in a Changing Economy_. Cambridge, UK: Cambridge University Press.



__________ ([1963] 1972) “The Study of Underdeveloped Economies.” In _Dissent on Development_ , chap. 8. Cambridge, Mass.: Harvard University Press.



Bauer, P. T., and Yamey, B. (1957) _The Economics of Under‐​Developed Countries._ Chicago: University of Chicago Press.



Baumol, W. (1968) “Entrepreneurship in Economic Theory.” _American Economic Review_ 58: 64–71.



__________ (1990) “Entrepreneurship: Productive, Unproductive, and Destructive.” _Journal of Political Economy_ 98: 893–921.



Fisher, F. M. (1983) _Disequilibrium Foundations of Equilibrium Economics_. New York: Cambridge University Press.



Gintis, H. (2007) “The Dynamics of General Equilibrium.” _The Economic Journal_ 117 (October): 1280–1309.



Hagen, E. (1963) “How Economic Growth Begins: A Theory of Social Change.” _Journal of Social Issues_ 19 (1): 20–34.



Kilby, P. (1971) “Hunting the Huffalump.” In P. Kilby (ed.), _Entrepreneurship and Economic Development_ , 1–40. New York: Free Press.



Kirzner, I. M. (1967) “Methodological Individualism, Market Equilibrium, and Market Process.” _Il Politico_ 32: 787–99.



__________ (1971) “Entrepreneurship and the Market Approach to Development.” In F. A. Hayek et al. (eds.) _Toward Liberty: Essays in Honor of Ludwig von Mises_ , Vol. 2, 194–208. Menlo Park, Calif.: Institute for Humane Studies.



__________ (1973) _Competition and Entrepreneurship_. Chicago: University of Chicago Press.



__________ (1997) “Entrepreneurial Discovery and the Competitive Market Process: An Austrian Approach.” _Journal of Economic Literature_ 35 (1): 60–85.



__________ (2005) “Human Attitudes and Economic Growth.” _Cato Journal 25_ (3): 465–69.



Leibenstein, H. (1968) “Entrepreneurship and Development.” _American Economic Review_ 58: 72–83.



McClelland, D. (1961) _The Achieving Society_. Princeton, N.J.: Van Nostrand.



Schultz, T. (1975) “The Value of the Ability to Deal with Disequilibria.” _Journal of Economic Literature_ 13: 827–46.



__________ (1990) _Restoring Economic Equilibrium: Human Capital in the Modernizing Economy._ Williston, Vt.: Basil Blackwell.



Stigler, G. (1955). “The Nature and Role of Originality in Scientific Progress.” _Economica_ 22: 293–302.



Yates, A. (2000) “The Knowledge Problem, Entrepreneurial Discovery, and Austrian Market Process Theory.” _Journal of Economic Theory_ 91: 59–85.



J. Robert Subrick is Associate Professor of Economics at James Madison University.



ShowHide

Endnotes



1 Keynesian macroeconomics formed another prominent aspect of economic theory. It too lacked any use for an entrepreneur as it focused on the movement of statistical aggregates with little concern about the underlying microeconomic processes.



2 Few have followed up on Arrow’s concerns. See Fisher (1983), Yates (2000), and Gintis (2007) for exceptions.
"
nan
"Dozens of activists have coated themselves in plaster and are trying to occupy the British Museum overnight in a bid to pressure the institution to cut ties with oil corporation BP. About 60 protesters were taking part in the defiant act of impromptu sculpture making as the museum in London attempted to close its doors at 5pm on Saturday. The action, entitled Monument, is the first of its kind to be conducted by the theatrical protest group BP or not BP? “We are imagining a world in which the British Museum has stopped celebrating those causing the climate crisis and is instead allying itself with those who have, currently are, and will be putting their bodies on the line in the fight for climate justice,” said a plaque describing the protest. The civil disobedience campaign began on Friday night when protesters dressed as ancient Greek warriors smuggled a 13-foot Trojan horse into the museum’s foyer. The protest was in response to an exhibition, Troy: Myth and Reality, which is described as “supported by BP”. Two guards slept inside its belly overnight, despite a downpour at around 4am, to prevent it being removed. The group estimated that as many as 1,500 supporters took part in the action on Saturday. Protesters occupied 11 of the museum’s rooms, which became host to spoken word performances, singalongs and talks by campaigners from West Papua and Senegal. Origami swans were scattered throughout the museum adorned with messages including: “Choose sponsors who care about our future”. Protesters finished the day by tearing apart paper versions of BP’s logo near the museum’s main entrance. “We feel that the museum’s attitudes towards climate change and colonialism are not what they should be in the 21st century,” said the group’s Jess Worth. Suzanne Savage, from Malvern, Worcestershire, was among a gathering of fee-paying British Museum members who unfurled a banner emblazoned with the words “BP must fall”. She said: “I have been a longstanding member of the British Museum. I very much support the art but we do not want this noble institution’s reputation being sullied by sponsorship from a dirty oil company, which is one of the biggest polluters in the world.” Hartwig Fischer, the director of the British Museum, said: “The museum is a public space where people can come to debate and we respect other people’s right to express their views. “We share the concerns for the challenges that we all face together as a result of climate change. We address these issues in an innovative way through significant exhibitions and public programming. “The British Museum offers for millions of people an extraordinary opportunity to engage with the cultures and histories of humankind. Without external support and sponsorship this would not be possible. “Removing this opportunity from the public is not a contribution to solving the climate crisis.” The protest is the latest move in activists’ campaign to end fossil fuel sponsorship of the UK’s leading cultural venues. In October, the Royal Shakespeare Company ditched its sponsorship deal with BP, after a campaign from artists, environmentalists and members of the public."
"Few people are familiar with the pangolin. It is a shy creature, about as big as a medium-sized dog, and its diet consists of ants and termites. Most distinctively, it has armoured plates and will curl into a ball to resist predators. The eight different species of pangolin live across most of Africa, India, southern China and Southeast Asia. Despite their recognition as an endangered species and the supposed protection that brings, pangolin are now the most trafficked wild mammal species in the world. Their scales have been used for millennia in Asian medicine. When roasted, they  are alleged to detoxify and drain pus, relieve palsy, and stimulate lactation. The animals are either traded alive (involving appalling cruelty) or killed and their scales removed to meet culinary and medicinal demand in East and Southeast Asia. In parts of Africa, scales are also used as a kind of traditional medicine known as “muti”, and African pangolins are also being increasingly exploited to satisfy growing demands from Asia. In July 2014 customs officials in Vietnam seized an astonishing 1.4 tonnes of dried pangolin scales from a cargo ship arriving from Sierra Leone. This was just one instance among many. Over the past decade thousands of pangolins and dozens of tonnes of scales have been seized in China and Vietnam. And what gets confiscated is but a fraction of what slips through unnoticed.  Each pangolin yields only about half a kilo of scales; the arithmetic of pending extinction is thus simple for a species that bears only one offspring per year. Logbooks apprehended in 2009 from one trafficking syndicate in the Borneo revealed 22,000 pangolins were killed over a 21 month period. This illegal trade continues unabated; online reports expose an extensive international trade network, with seven countries involved in 15 pangolin trafficking incidents investigated between August and October 2013. This multi-nationalism makes enforcement extremely challenging, with huge borders and vast coastlines to monitor across Asia, necessitating a greater exchange of information and intelligence between national enforcement agencies. International wildlife protection is simply not keeping pace with the resolve and devious methods of illegal traders, such as posting parcels of pangolin scales by mail (last year Beijing Customs uncovered more than one tonne of posted scales). Furthermore, illegal traders exploit corruption at border controls to facilitate the illegal supply chain. This is despite global legislation and national laws (notably in China and Vietnam), which carry severe punitive sentences, aimed at stemming cross-border trade. The illegal trade in fauna and flora (excluding fisheries and timber) is big business, worth somewhere between US$7 and US$23 billion each year according to the UN and Interpol. Of this total, however, the sale of valuable commodities such as ivory, rhino horn and tiger accounted for only around US$75m in 2010, despite  getting the most attention in Asia. Of course ivory poaching is awful, but it is important to recognise the sheer numbers involved when we talk about trade in other species. That 1.4 tonne shipment of pangolin scales represented around 3000 dead animals. Cutting out the supply is one thing, but reducing consumer demand is also essential. Lamentably, even the government-backed Chinese Pharmacopoeia Commission supports the medical value of pangolin scale. Ordinary citizens, often sick or elderly, searching for a traditional remedy are thus mislead and become unwitting participants in this species’ demise.  While other wildlife products such as ivory and rhino horn are prohibitively expensive for most Chinese consumers, and symbolise elite status within society, pangolin is more affordable and easily attainable. Its price has been increasingly recently, yet it is still a price consumers seem willing to pay. Clearly, people in China and across Southeast Asia aren’t going to be priced out of the pangolin market any time soon so it is imperative modern medicine is adopted and that we achieve a step-change in  public understanding of illegal trade and conservation. The exact extent to which this illegal exploitation affects wild pangolin populations remains largely unknown – although it would seem totally unsustainable. Further ecological research is crucial to establish how many pangolins are out there in the wild and how best to protect them. The Scaling up pangolin conservation action plan published by the International Union for Conservation of Nature requires urgent implementation, which means reducing consumer demand, strengthening protections in wild pangolin strongholds, help for communities to move away from poaching, and stronger, better enforced laws prohibiting the pangolin trade."
"
According to my stat counter, the Projects Page tab above has been getting a regular stream of interest, so it has been updated with relevant content as of today 10/22/07


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3457f2b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"It is the middle of the 2040s. After years of warning, scientists have just confirmed that the tipping point for the West Antarctic Ice Sheet has been triggered. Governments around the world are horrified as the news filters through. The days when they could reverse climate change by cutting carbon emissions to reduce global temperatures have just come to an end. There is now no way of stopping global sea levels from rising by over four metres. Nations and islands around the world are about to be lost forever.  It’s not real life – not yet anyway – but a computer game being played by a group of delegates and other attendees at the UN COP24 Climate Change conference in Katowice, Poland. Earth Remembers plays a bit like a 30-person version of Football Manager. Closely mirroring the real-life world climate change negotiations that take place every six months, each affected player represents a different country and has to negotiate with everyone else about how they’ll spend their national budget.  The game is the brainchild of a collaboration between undergraduates and academics from Glasgow Caledonian University, Utrecht University and the Purdue Climate Change Research Centre in Indiana. It uses an IPCC climate model to simulate the effects of each decision on carbon emissions, national GDP and the global temperature. Each turn skips forward five years, enabling players to “live” through the plausible future scenarios of 2033, 2038, 2043 and onwards out to 2118 to see how their decisions affect the world. The idea of using computer games, known as applied games, to help educate people goes back decades. Some might remember the “edutainment wave” of the 1990s, typified by games like Math Blaster (1994), where players had to do sums to release a tractor beam to pick up space rubbish.  Not only were such titles usually mediocre, the whole approach saw games as a kind of spoonful of sugar to make the nasty medicine of learning go down. It was bad teaching hiding inside bad games, and players were not usually fooled for long.  Fortunately, modern applied games are designed with a much better understanding of what makes games special. One of the best is the space exploration simulator Kerbal Space Program (2011). The object is to create a space programme for a race of little green humanoids called Kerbals. Players are encouraged to experiment and be creative. Once they have built the necessary equipment, they fly it with a simulator based on a realistic physics model.  The game wasn’t originally designed for educational purposes. But after NASA decided to back it, the developers created a version aimed at classrooms. Only by grappling with real-world trade offs between rocket components, life-support systems, fuel and weight, can players get the Kerbals to their nearest moon, the “Mun”, and then back in one piece.  Earth Remembers was designed with the same sense of jeopardy in mind. Should a country’s budget be spent on researching new green technologies that could, for example, extract carbon from the atmosphere and sink it in the ground? Should it be spent reducing greenhouse-gas emissions with investments in green energy sources or on adapting to the impacts of climate change? When events trigger in the game, each player/nation is given an event card that describes what has happened to their country. In one game, for example, the US was told that Miami’s coastline had seen significant flooding as a result of the ice sheet melt and that millions of residents were going to have to be moved further inland at great cost to the country. Meanwhile, other nations were coming to terms with news that certain industries had been dealt a massive blow.  The whole point is to use narrative storytelling and imagination to make the human and economic cost of these events more concrete. As the Dutch psychologist Nico Frijda convincingly argued, people have emotional reactions to what is apparently real – not to what actually is real. Games give us a platform to create just such an apparent reality.  The final part of our game shifts 100 years into the future, and players are presented with scenarios derived from the decisions they have made. The Fijian people no longer reside in Fiji, for instance, but in a part of Australia renamed by one participant “Fijiland”. This prompted discussions about the challenges of identity loss, whether this community should be a separate nation, and who was responsible for this situation.  Our trip to Katowice was the second time we’d simulated the game, having previously demonstrated it to negotiators at the climate change talks in Bonn, Germany in the spring. When the West Antarctic Ice Sheet collapsed in one of the games in Katowice, the sense of alarm in the room was palpable. We got great feedback from players about the way the game made tipping points like this one much more real. Negotiators will hopefully have kept this in mind when it came to the real-life discussions.  Next for us is to demonstrate Earth Remembers for scientists and policy specialists in Washington, DC in a few days’ time. We are also hoping to develop further enhancements to the game. This is where the potential of applied games lies: in mixing together the scientific and the artistic; the rational and the emotional. Do this well and you have the potential to create meaningful change – and maybe even help avert disaster in the process."
nan
"

La Scala to stage Gore’s ‘Inconvenient Truth’
MILAN, Italy (AP) — First it was the film and the book. Now the next stop for Al Gore’s “An Inconvenient Truth” is opera.
La Scala officials say the Italian composer Giorgio Battistelli has been commissioned to produce an opera on the international multiformat hit for the 2011 season at the Milan opera house. The composer is currently artistic director of the Arena in Verona.
Bring your marshmallows.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f207d1f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe pitfalls of planned economies…

Power shortages, power surpluses, seas of milk and mountains of butter
By Reinhard Storz
(Translated/ edited by P. Gosselin)
Some of the older generation will still remember the time when the press, radio and television talked about a sea of milk and a mountain of butter. Politicians had meant well and, in order to aid the farmers, decided on adequate prices for milk. This planned economy led to an ever increasing quantity of milk in West Germany. All the talk was about a sea of milk. There were numerous ideas on what to do with the surplus milk. One should give milk to the school children for school breaks etc. Surplus milk was processed into milk powder and butter. The resulting butter was stored in cold stores until there was no room left. Finally, a part of the butter that could not be sold in Germany was sold to the USSR for a fraction of the market price, and another part was ultimately given away to Chile.
Politicians learned back then that this was the wrong way, and so introduced a milk quota more than 30 years ago to promote agriculture. As a result the sea of milk evaporated and the mountain of butter disappeared. Today, due to the planned economy with solar and wind power, we have similar conditions as we did with milk and butter. When the wind blows strongly and the sun shines, coal, gas and nuclear power plants are throttled down. Nevertheless, we still have a surplus of green electricity at times. This cannot be accommodated by consumers in Germany. To get this problem under control one has two options.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




1. You search for customers in neighboring countries. That frequently succeeds. However, they are usually not willing to pay for the surplus electricity. The power is thus sometimes given away, often money has to be paid on top so that the power gets accepted.
2. Wind turbines are turned off if the grid gets overloaded. The operators of the wind parks, however, are still reimbursed for the price of the unproduced electricity. These costs are also passed on to us, the electricity consumers.
But there are also times when the wind does not blow and the sun does not shine. There you could very well use the electricity that was previously available in abundance. A program for even more wind turbines or solar roofs will not help. A tenfold amount of solar surfaces does not supply electricity at night and a tenfold amount of wind turbines has no use during calm periods. In nature, fluctuations are nothing unusual. We remember the story of the Pharaoh and the 7 fat people and the lean years. Surplus food was already stored for bad times thousands of years ago.
To even out the fluctuations in solar and wind power, electricity storage is therefore urgently needed. Politicians to plan it out in such a way that additional wind turbines and solar areas are only approved if they are equipped with appropriate storage capacity. The goal of the Energiewende (transition to green energies) is to replace electricity from coal-fired power plants with electricity from the sun and wind. But it must then also be available around the clock. This is not possible with the sporadically available electricity from wind turbines and solar systems.  This electricity, disparagingly referred to by some people as inferior, fidgety electricity, must be made permanently available by means of electricity storage systems. Only in this way can a transition to green energies succeed.
Therefore, demonstrations should first take place to demand the introduction of sufficient electricity storage. Until this is achieved, there will remain a need for coal-fired power plants. Anyone who wants to abolish coal-fired electricity without a secure and affordable alternative supply, is just sawing off the branch we are all sitting on. But we should not give up hope. Like the politicians who finally got the mountain of butter mountain and the sea of milk under control with the milk quota, they will also bring the necessary power storage facilities on the way as soon as possible.
Pumped storage facilities for such amounts of electricity are likely to be ruled out due to technical and economic reasons. The same is true with regards to compressed air storage, which has a lower efficiency than pumped storage plants. Battery storage is unaffordable at today’s costs. The only option is to produce hydrogen as an energy storage from surplus electricity.


		jQuery(document).ready(function(){
			jQuery('#dd_a4404f563bedf5c83941b0e5d9152640').on('change', function() {
			  jQuery('#amount_a4404f563bedf5c83941b0e5d9152640').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

In September 2005, the Danish newspaper _Jyllands‐​Posten_ printed a dozen cartoons — prompted by recent examples of self‐​censorship by the European media — related to Islam, one of which depicted the Muslim prophet Muhammad with a bomb wrapped in his turban. Their publication quickly spiraled into a violent international uproar, as Muslims around the world erupted in protest and the paper’s culture editor was branded by some as “the Danish Satan.” In _The Tyranny of Silence_ , a book published by the Cato Institute in November, Flemming Rose grapples with the difficult issues surrounding his decision to run those cartoons. At Cato’s 27th Annual Benefactor Summit in Naples, Florida, Rose spoke about the lessons he learned in the process of reconciling the tension between respect for cultural diversity and the protection of democratic freedom.



When I was writing my book back in 2009, I interviewed Salman Rushdie and he said something very important to me. I’d been having difficulty coming to terms with the fact that others were telling my story without, I felt, knowing who I was. Rushdie observed that from childhood, we use storytelling as a way of defining and understanding ourselves. It’s a phenomenon that derives from a language instinct that is universal and innate in human nature. It’s in fact one of the things that makes us different from other creatures. Any attempt to restrict that impulse and put limits on speech therefore isn’t just a violation of our political rights. It’s an act of violence against human nature, an existential assault that turns people into something they are not. What differentiates open and closed societies is the right to tell and retell our own and other people’s stories. In a democracy, no one can claim that exclusive right, be it an oppressive state or a minority.



Rushdie told me that the conflict over the right to tell a certain story was at the center of his own controversy. He said: “This goes back to the question of what sort of society we want. If you wish to live in an open society, it follows that people will talk about things in different ways, and some of them will cause offense and anger. From the moment you begin to talk about limiting and controlling certain expressions, you step into a world where freedom no longer reigns, and from that moment on, you are only discussing what level of un‐​freedom you want to accept. You have already accepted the principle of not being free.”



Rushdie’s words came just at the right time for me. They opened my eyes and helped me define my own project. Even though the Muhammad cartoons were conceived in a Danish and European context, the debate is global. It touches on issues fundamental to any kind of society: freedom of speech and of religion, tolerance and intolerance, immigration and integration, Islam and Europe, majorities and minorities, and globalization, to name but a few. And what I realized is that we are all entitled to tell whatever story we wish.



That insight is very fundamental. It goes to the heart of the relationship between the person who speaks and those who hear — between individuals and communities — and to what extent individuals, groups, and institutions have a right to determine speech limits. In the U.S. constitutional system, there is more focus on the speaker, on the individual. You have the right to autonomy. In much of the rest of the world, including the European Union, it’s the other way around. The community — those on the receiving end — have broad powers to determine what an individual is allowed to say.



And this difference in approach has had farreaching consequences for the concept of tolerance. Originally, tolerance implied one’s ability to bear what one couldn’t stand. Freedom of speech meant freedom for the speech we hate — that’s how Justice Oliver Wendell Holmes put it. But because the receiver of speech has so much discretion in many parts of the world, tolerance has been turned on its head. That’s a very dangerous development.



There are two factors that are driving the challenges of free speech in a globalized world. The first is migration: the fact that people are moving across borders in numbers never before seen in human history Every society is getting more and more diverse in terms of culture, ethnicity, and religion, which means that it’s a lot easier to get offended by what people around you say because we are increasingly exposed to different ways of living. How do we negotiate the right to freedom of expression and freedom of speech in this increasingly multicultural world?



The other factor that is driving this process is the digitization of communication technologies. Now when something is being published in one place, it is immediately published everywhere. And when information travels, context is lost. This creates a huge space for misunderstanding, not to mention outright manipulation. That is something I experienced personally during the cartoon crisis.



But migration and digitization also means that all of us are being impacted by what’s going on outside our own country. You have competing approaches to free speech that are beginning to clash. The disappearance of borders and the spread of technology means that there is a need for universal standards no matter where you live. To a certain extent this goes on within the United Nations. But often things seem to be moving in the opposite direction.



More and more countries are passing laws that fragment and undermine any universal standard, a point stressed by Miklós Haraszti, the former representative on freedom of the media for the Organization for Security and Co‐​operation in Europe. Haraszti writes that “the very notion of an international standard for limits on free speech become obsolete if the fragmentation into separate content‐​oriented, historically based, culturally defined, politically shaped, country‐​specific approaches to speech restriction becomes accepted.”



In other words, no international advocacy for free speech is possible without a shared assumption that only incitement of actual crimes should be illegal. Otherwise offensive speech should be countered by speech, not courts. Unfortunately, this fragmentation of the international standard, to a certain extent, started in Europe with the passing of Holocaust denial laws. And one of the big surprises I experienced writing my book was to find out that the vast majority of these laws were in fact passed after the fall of the Berlin Wall. This indicated to me that they were not passed right after the Holocaust to prevent incitement to violence, but for other reasons. It’s important to note that the horror of the Holocaust serves as the founding narrative legitimizing European integration, and it is the key motivation for hate‐​speech laws on the continent.



The Council of Europe Commissioner for Human Rights has called for all 47 member states to pass laws against Holocaust denial, based on a widely accepted interpretation of what led to the Holocaust. It says that anti‐ Semitic hate speech was the decisive trigger — that evil words beget evil deeds — and that if only the Weimar government had clamped down on verbal persecution of the Jews in the years prior to Hitler’s rise to power, then the Holocaust may never have happened.



In my research, I looked into what actually happened in the Weimar Republic and found that, contrary to what most people think, Germany did have hate‐​speech laws that were applied quite frequently. The assertion that Nazi propaganda played a significant role in mobilizing anti‐​Jewish sentiment is irrefutable. But to claim that the Holocaust could have been prevented if only anti‐​Semitic speech had been banned has little basis in reality. Leading Nazis, including Joseph Goebbels, Theodor Fritsch, and Julius Streicher, were all prosecuted for anti‐​Semitic speech. And rather than deterring them, the many court cases served as effective pubicrelations machinery for the Nazis, affording them a level of attention that they never would have received in a climate of a free and open debate.



In the decade from 1923 to 1933, the Nazi propaganda magazine _Der Stürmer_ — of which Streicher was the executive publisher — was confiscated or had its editors taken to court no fewer than 36 times. The more charges Streicher faced, the more the admiration of his supporters grew. In fact, the courts became an important platform for Streicher’s campaign against the Jews.



Alan Borovoy, general counsel of the Canadian Civil Liberties Foundation, points out that cases were regularly brought against individuals on account of anti‐​Semitic speech in the years leading up to Hitler’s takeover of power in 1933. “Remarkably, pre‐​Hitler Germany had laws very much like the Canadian anti‐​hate law,” he writes. “Moreover, those laws were enforced with some vigour. During the 15 years before Hitler came to power, there were more than 200 prosecutions based on anti‐​Semitic speech… As subsequent history so painfully testifies, this type of legislation proved ineffectual on the one occasion when there was a real argument for it.”



The same can be said about Yugoslavia. Before the 1990s, Yugoslavia had rather tough laws criminalizing incitement to national, racial, or religious hate. In fact, people were being put in jail for telling an ethic joke. Obviously, these laws did little to help prevent the ethnic violence that we saw in the wars following the disintegration of Yugoslavia. Nevertheless, the dominant view in Europe is that too much freedom of expression will destroy the peace. In that sense, the EU is driven by a vision of what I call a benign utopia, one that aims to eliminate hate and create an insult‐​free public space. This became particularly evident in 2012, when it was awarded the Nobel Peace Prize. In receiving the prize, the leaders of the EU made no reference to the close relationship between freedom and peace. Instead they focused on the EU’s efforts to avoid division and create a continent without conflict.



I believe that Europe would do itself a great service if the narrative about the Holocaust was integrated into a broader anti‐​totalitarian framework. Hate speech wasn’t the trigger for mass murder during World War II. It was the clash between two totalitarian powers in the center of Europe — the Nazi regime and the Soviet regime — that was the primary cause. And if that’s the case, it means that the destruction of Jews in Europe was closely connected to the destruction of freedom. Moving forward, it would mean that the struggle against evil doesn’t require less freedom, but in fact, quite the contrary.



It seems there are two available responses to threats against free speech. One option is, basically, “If you accept my taboos, I’ll accept yours.” If one group wants protection against insult, then all groups should be so protected. If denying the Holocaust or the crimes of communism is against the law, then publishing cartoons depicting the Muslim prophet should also be forbidden. But that option can quickly spiral out of control: before we know it, hardly anything may be said.



The second option is to say that, in a democracy, there is no “right not to be offended.” Since we are all different, the challenge is then to formulate minimum constraints on freedom of speech that will allow us to coexist in peace. A society comprising many different cultures should have greater freedom of expression than a society that is significantly more homogeneous.



That premise seems obvious to me, yet the opposite conviction is widely held, and that is where the tyranny of silence lurks. At present, the tendency in Europe is to deal with increasing diversity by constraining freedom of speech, whereas the United States maintains a long tradition of leading off in the other direction. And it appears that the United States will increasingly stand alone with its tradition of upholding near‐​absolute freedom of expression.



My personal view is that the Americans are right. Freedom and tolerance are, to me, two sides of the same coin, and both are under pressure. As noted earlier, the world is undergoing rapid change. Taking offense has never been easier, or indeed more popular: many have developed sensitivity so exquisite that it has become excessive.



It almost tempts one to ask Europe’s welfare states to spend some money, not on “sensitivity training” — learning what not to say — but on insensitivity training: learning how to tolerate. For if freedom and tolerance are to have a chance of surviving in the new world, we all need to develop thicker skin.
"
"
Share this...FacebookTwitterA new assumption about carbon budgets reveals climate scientists have been vastly underestimating (by a factor of 2) the amount of carbon absorbed by the ocean for decades. Every past carbon budget estimate has been twice as wrong as the current estimate.
When it comes to the ocean heat fluxes and source vs. sink carbon budget estimates, climate scientists have been providing little more than educated guesses for decades.
For example, climate models have long suggested the ocean heat fluxes may only vary around 1 W/m². But “objective” analyses of oceanic latent heat flux (LHF) using different assumptions (equations) reveals fluxes were likely closer to 10 W/m² during 1981-2005 (Yu and Weller, 2007). So our modeled guesses were off by a factor of 10 compared to newer analyses.

Image Source: Yu and Weller, 2007
Ocean carbon sink processes not understood and driven by natural variability
McKinley et al. (2017) analyzed ocean carbon sink estimates and was willing to admit that due to a lack of observation, we lack a “detailed, quantitative, and mechanistic understanding of how the ocean carbon sink works…”
In addition, because internal variability in oceanic carbon uptake is so massive and largely unobserved, we cannot yet detect an anthropogenic influence.
McKinley and co-authors go so far as to acknowledge the “change in CO2 flux over 10 years (1995-2005)…is due almost entirely to the internal variability” because in most ocean regions “the forced [human-induced] trends in CO2 flux are too small to be statistically significant” and the “variability in CO2 flux is large and sufficient to prevent detection of anthropogenic trends in ocean carbon uptake on decadal timescales.”

Image Source: McKinley et al. (2017)
The Southern Ocean absorbs more than 10 times less carbon than previously thought


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Southern Ocean is where the largest portion of anthropogenic carbon (from our emissions) is said to be absorbed, or Earth’s largest oceanic CO2 sink.
Just 2 years ago, a carbon uptake analysis (Gray et al, 2018 with a Physics Today press release) that utilized estimates from biochemical floats instead of estimates from ships suggested the exact opposite of what had been previously thought. Instead of absorbing close to 1 petagram of carbon (PgC) per year, the Southern Ocean is barely even a carbon sink at all – just 0.08 PgC of yearly absorption. In fact, large regions of the Southern Ocean near Antarctica are a net source of CO2 to the atmosphere.
In other words, when estimates are float-based rather than ship-based, one estimate is more than 10 times different than the other.

Image Source: Physics Today
The global ocean absorbs 2 times more CO2 than previously thought
And now a new study (Buesseler et al., 2020) has scientists insisting that all of our previous estimates of global ocean carbon uptake are substantially wrong because we’ve been measuring from a fixed depth rather than varying depths.
Previously scientists had been using flux estimates from the “canonical fixed 150-m depth.” The new-and-improved way to assess carbon uptake is from varying but often much shallower depths: the euphotic zone (Ez). This is the section of the upper ocean layer that sunlight is able to penetrate, and it can “vary from less than 20 m to almost 200 m” in depth.
When we use the Ez to estimate carbon absorption versus export, the absorption changes from 2.8 petagrams of carbon (PgC) per year to 5.7. So the global ocean sink can be more than doubled just by varying the depth of measurement rather than using a fixed depth.
So, up to this point, scientists’ past guesses about ocean carbon uptake have been emphatically wrong. We can be assured, though, that our current guesses are anywhere from 2 to 10 times less wrong than the last ones.

Image Source: Buesseler et al., 2020 and press release
Share this...FacebookTwitter "
"Where the Iron Curtain once divided Europe with barbed wire, a network of wilderness with bears, wolves and lynx now thrives. Commemorating 100 years since the end of World War I, people wear poppies to evoke the vast fields of red flowers which grew over the carnage of Europe’s battlefields. Once human conflict has ended, the return of nature to barren landscapes becomes a potent symbol of peace. These tragedies, which force people away from a place, can help ecosystems replenish in their absence. Though rewilding is typically considered an active decision, like the reintroduction of wolves to Yellowstone National Park, abandoned rural land often returns to wilderness of its own accord. Today, as people vacate rural settlements for life in cities, accidental rewilding has meant large predators returning to areas of Europe, long after they were almost made extinct.  Sudden changes, such as the the Chernobyl nuclear power plant disaster in 1986, result in wildlife recolonising exclusion zones in previously developed areas. Warfare can also result in human exclusion, which might benefit wildlife under specific conditions. Isolation and abandonment can generate wild population increases and recoveries, which has been observed in both terrestrial and aquatic ecosystems. Fish populations in the North Atlantic benefited from World War II as fishing fleets were drastically reduced. Fishing vessels were requisitioned by the navy, seamen were drafted and the risks of fishing due to enemy strikes or subsurface mining deterred fishermen from venturing out to sea. As a result, the war essentially created vast “marine protected areas” for several years in the Atlantic Ocean. After the war, armed with faster and bigger trawlers with new technology, fishermen reported bonanza catches. A more gruesome result of World War II allowed opportunistic species such as the oceanic whitetip shark to flourish, as human casualties at sea proved a rich and plentiful food source. Warship wrecks also became artificial reefs on the seabed which still contribute to the abundance of marine life today. The 52 captured German warships that were sunk during World War I between the Orkney mainland and the South Isles, off the north coast of Scotland, are now thriving marine habitats. Exclusion areas, or “no mans lands”, which remain after fighting has ended may also help terrestrial ecosystems recuperate by creating de facto wildlife reserves. Formerly endangered species, such as the Persian leopard, have re-established their populations in the rugged northern Iran-Iraq frontier. An uneasy post-war settlement can create hard borders with vast areas forbidden to human entry. The Korean Demilitarised Zone is a 4km by 250km strip of land that has separated the two Koreas since 1953. For humans it is one of the most dangerous places on Earth, with hundreds of thousands of soldiers patrolling its edges. For wildlife however, it’s one of the safest areas in the region.  Today, the zone is home to thousands of species that are extinct or endangered elsewhere on the Korean peninsula, such as the long-tailed goral. Miraculously, even habitats scarred by the most horrific weaponry can thrive as places where human access is excluded or heavily regulated. Areas previously used for nuclear testing, such as the Marshall Islands in the Pacific Ocean have been recolonised by coral and fish, which seem to be thriving in the crater of Bikini Atoll, declared a nuclear wasteland after nuclear bomb tests in the 1940s and 50s. For all the quirks caused by abandonment, warfare overwhelmingly harms human communities and ecosystems with equal fervour. A review of the impact of human conflict on ecosystems in Africa showed an overall decrease in wildlife between 1946 and 2010.
In war’s aftermath, natural populations were slow to recover or stopped altogether as economic hardship meant conservation fell by the wayside. Humans often continue to avoid a “no mans land” because of the presence of land mines. But these don’t differentiate between soldiers and wildlife, particularly large mammals. It’s believed that residual explosives in conflict zones have helped push some endangered species closer to extinction.  However, where possible, accidental rewilding caused by war can help reconcile people after the fighting ends by installing nature where war had brought isolation. There is hope that should Korea reunify, a permanently protected area could be established within the current demilitarised zone boundaries, allowing ecotourism and education to replace enmity. Such an initiative has already succeeded elsewhere in the world. The European Green Belt is the name for the corridor of wilderness which runs along the former Iron Curtain, which once divided the continent. Started in the 1970s, this project has sprawled along the border of 24 states and today is the longest and largest ecological network of its kind in the world. Here, ponds have replaced exploded land mine craters and forests and insect populations have grown in the absence of farming and pesticide use. Where war isolates and restricts human movement, nature does seem to thrive. If, as a human species, we aim for a peaceful world without war, we must strive to limit our own intrusions on the natural paradises that ironically human warfare creates and nurture a positive legacy from a tragic history."
"New leaders have emerged over the past few years who have changed the highest levels of political and diplomatic discourse. After divisive elections, both the US and Brazil are among the countries to have succumbed to the “strongman”. The modern strongman is a leader who rules by force and bluster, and who shuns cooperation in favour of an isolationist and protectionist foreign policy. This attitude is usually reflected in a very dim view of climate action. The US president, Donald Trump, and Brazil’s president-elect, Jair Bolsonaro, are both doubtful about climate science and have both suggested they would eventually like to withdraw from the Paris Agreement on climate change. At the latest COP24 climate talks in Poland, the US hosted a pro-coal side event, while Brazil has pulled out of hosting next year’s talks, COP25. Yet the two countries are a vital part of any fight against global warming. The US remains the world’s largest economy, and is the largest consumer and one of the largest producers of fossil fuels. Brazil also has a huge population and its wealth of natural resources – including the world’s largest rainforest, the Amazon –  are under threat.  In this context, with these leaders, what hope is there for the future of climate treaties and conferences?  Yet despite Trump, Bolsonaro and a few other awkward parties, delegates at COP24 have reached agreement on most of the “rulebook” for implementing the Paris Agreement on climate change. It is important to remember that Paris had 197 signatories with 184 countries ratifying it. This agreement is about more than just a couple of nations. Countries, it seems, are still working together. A significant reason for this is money – and, as the old adage goes: “money makes the world go round”. Many nations are still feeling the effects of the 2007-2009 financial crisis, and money remains in short supply. Any chance to create jobs and new investment will be welcomed by governments across the world, and investing in clean energy is one solution. Political leaders everywhere, including strongmen, are still motivated by money. For example, in many countries, public health has became a key climate change issue. Just look at China, where leader Xi Jinping shares some characteristics with classic strongmen yet his government has realised that fossil fuel use comes at a growing cost to public health, tourism and labour productivity. The Lancet estimates the welfare costs from pollution to be equivalent to 6.2% of global GDP – and these costs are rising. One way of negating the effects is to adopt a more climate change-friendly economic policy as China has been developing – for example, improving regulation, introducing environmental taxes and developing sustainable development zones. One key issue to emerge from the Paris Agreement was the idea of a “just transition”, which seeks to ensure that moving from a society based around fossil fuels to a new low-carbon economy is done fairly and with adequate support for workers. The Paris Agreement refers to: Taking into account the imperatives of a Just Transition of the workforce and the creation of decent work and quality jobs in accordance with nationally defined development priorities. This issue of a just transition remained relatively dormant but has risen to prominence over the past year. It was highlighted in the communiqué produced from the recent G7 talks in June this year. And notably, at the start of COP24, in a country dominated by coal, the Polish prime minister even announced a declaration on support for workers in rapidly-changing industries. There is no doubt that strongmen can distract from the global movement on climate change. But ultimately the movement has already gathered sufficient pace for continued cooperation by other countries, as evidenced by the numbers signed up to Paris. It is unfortunate that certain leaders will slow progress on climate action, perhaps particularly in their own countries, but nevertheless investment data shows that more and more low-carbon energy projects are receiving financial support globally. In following data trends, it is also clear that the cost to public health systems, the loss of worker productivity and the loss of tourism all from pollution is becoming more costly to societies than to reorient themselves to a low-carbon economy. In particular countries are acting on energy infrastructure issues and are developing strategies to deliver new low-carbon infrastructure.  Finally, there is recognition through the just transition movement that the labour force of our current fossil fuel-dominated economies needs to be transitioned to a labour force that builds a low-carbon economy – currently Scotland, Ireland, Germany and several states in the US and Australia are establishing commissions to deliver this policy. Money talks and, with finance now being geared towards low-carbon investments, it will supersede even the interests of media-savvy strongmen."
"
Share this...FacebookTwitterIn an interview with flagship daily Frankfurter Allgemeine Zeitung (FAZ here), Max-Planck Institute for Meteorology (MPIM) Director Dr. Jochen Marotzke said predicting how many degrees of warming we need to prepare for was like reading tea leaves and that he is not worried about “climate tipping points”. 
He also spoke of the wide disagreement among climate models.

Max-Planck Institute for Meteorology (MPIM) Director Dr. Jochen Marotzke told the FAZ he doesn’t worry about climate “tipping points”, but worries about panic. Image: MPIM. 
He told the FAZ that the worst case scenarios put out by some models were useful for the purpose of risk assessment, i.e. scenarios that are unlikely but cannot be ruled out. “In the latest generation of models, there are some models that are much more sensitive to greenhouse gases than previous models in terms of their temperature increase,” he said.
Five degrees “very very unlikely”
When asked about the results of the French model released earlier this year, which assumes five degrees of warming for a doubling of atmospheric CO2, Marotzke expressed his amazement, telling the FAZ what he thought of the French scientists: “My God, what are you doing? Because it is very, very unlikely that the true climate is as sensitive as these new models show.”
“The issue of climate sensitivity is extremely complex. Therefore, the results of a model should first be treated with caution,” Marotzke said.
When asked why the French model produced such a high warming for a doubling of CO”, Marotzke said he didn’t know why: “No one understands why they published it without first reflecting. The British did it differently, they said the new value is a mystery to us. They first want to investigate what the reason is and whether the warming rate is realistic.”
No worries about climate tipping points
Later in the interview, the FAZ touched on the so-called “tipping points in the climate system”, which are “threshold values that set irreversible processes in motion that, once started, can no longer be stopped.” Possible tipping points named by some scientists include the Greenland Ice Sheet, Gulf Stream, West Antarctica:, coral reefs, Amazon dying etc.
On whether they could happen, Marotzke views it as “conceivable” and that it “cannot be ruled out” and with “almost all of them we don’t know where we stand.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




When asked which one is most worrying, he replied: “None”.
“I don’t see any risk with Greenland”
And not even the melting of the Greenland ice sheet worries the MPIM Director. He told the FAZ:  “It’s gonna take so long – a couple thousand years. I don’t see any risk with Greenland.”
Arctic not a tipping element
On the subject of the Arctic, Marotzke says he is “quite sure that it is not a tipping point” – and that the ice albedo feedback “is not the dominant effect”.
“The ice comes back every year – in winter, said Marotzke, who has been Director at the MPIM in Hamburg since 2003. “When the temperature goes down again, the sea ice will come back.”
No worries about thawing permafrost
He is also not worried about the permafrost thawing, saying the contribution to warming “is relatively small.”
“Besides, even if the permafrost thaws, it is uncertain how much of the methane actually reaches the atmosphere,” said Marotzke. “Methane can be converted by bacteria to CO₂. I am not worried about methane.”
Worries “panic will backfire”
When asked about what he is worried about, he replies: “That the panic will backfire.” Marotzke warns against spreading panic: ” It can become incendiary. The question is, at what point do the risks of climate protection measures exceed the risks of climate change? Panic does not help here, only relatively sober analysis and weighing up – and a democratic discussion will help.”
Hat-tip: Die kalte Sonne. 


		jQuery(document).ready(function(){
			jQuery('#dd_1ad68e0372d18ffba913ecc7236487c6').on('change', function() {
			  jQuery('#amount_1ad68e0372d18ffba913ecc7236487c6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"A British “poo bus” went into service last week, powered by biomethane energy derived from human waste at a sewage plant.  For those of us who follow these matters – and my academic works include Geographies of Shit: Spatial and temporal variations in attitudes towards human waste – this was an exciting moment, a rare piece of good PR for human waste. After all, most societies strongly associate it with a sense of disgust. Poo threatens the health of around 2.5 billion people … and it smells bad.  Yet it also represents an important resource, used in lots of different ways throughout history. Though the “poo bus” has captured the imagination there are many other uses for human waste. Urine is particularly versatile. In Medieval Europe, it was widely used to clean clothes while the Romans used it for tanning leather and cleaning wool.  It also makes an excellent agricultural fertiliser. Before the 19th century realisation that human waste was a health risk, sewage was routinely transported from British towns to villages for use as manure.  However, most of the health risks can be eliminated if urine (harmless if unpleasant) and feces (full of diseases) are separated at source through some form of urine diversion toilet. Such strategies make sound environmental and economic sense given the urine produced annually by each adult contains enough plant nutrients to grow 250kg of grain, enough to feed them for a year. China has a long history of using such toilets to collect urine for use as a fertiliser. In some regions of Sweden these toilets are now mandatory, improving environmental quality as well as creating significant savings on fertiliser costs for farmers.  Although harvesting biogas from human waste is not a new concept (Assyrians were using it to warm their bath water back in the 10th century BC), the potential to simultaneously manage waste and generate power has attracted increasing attention in recent decades. Modern waste treatment leaves behind sewage sludge that has traditionally been difficult to dispose of. However when the sludge is fed into a large vat, essentially like a stomach, and left to digest (an anaerobic digestion plant) it can produce valuable biogas and nutrient-rich digestate.  Biogas can be used directly as a fuel, cleaned up to create bio-methane or fed through a combined heat and power unit to generate electricity. The digestate can be used as a fertiliser or soil conditioner, helping in the process to reduce methane emissions, enhance plant growth and sequester carbon through photosynthesis. In rural China especially, low-tech biogas sanitation systems play an important role in killing pathogens while providing clean cooking fuel and fertiliser from the digestate.  Sweden and Germany are particularly big anaerobic digestion users. In Germany, sewage plants can sell their excess energy back to the national grid. Attractive tariffs designed to promote renewable energy have even meant many plants have started to “feed” their anaerobic digestion units with purpose-grown energy.  Though the UK’s biogas industry lags behind that of countries like Sweden and Germany, some sewage works are already releasing biogas into the national grid. With each adult producing around 30kg of dried sewage each year, there is lots of growth potential. If all of the UK’s sewage plants adopted this technology, around 350,000 homes could be supplied with gas derived from human waste. The environmental benefits of poo-powered travel are clear: bio-methane produces 95% less CO2 and 80% less nitrous oxide than diesel as well as having no particulate emissions. In the UK, there is enough bio-methane to fuel half the country’s large trucks.  Four years ago engineers developed a VW Beetle fuelled by bio-methane gas generated at the Avonmouth sewage plant near Bristol. This same sewage plant is now powering the “poo bus” and it could do even more. Avonmouth produces around 17m cubic meters of bio-methane each year which, if exported to the grid, could meet the gas needs of 8,300 homes.  But Sweden, again, is a leader here. Their transport policy has prioritised the development of bio-methane for trucks and buses; an initiative that has helped to clean up the air and meet renewable energy targets.  At a smaller and more experimental scale, meanwhile, researchers at the Bristol Robotics Laboratory have succeeded in charging a mobile phone using electricity generated from urine. Using a microbial fuel stack, they have succeeded in taking advantage of the metabolism of live micro-organisms to create electricity from convert organic matter – in this case urine.  Other research teams working on similar “pee conversion” technologies have succeeded in generating electricity, clean water and hydrogen from human waste.  If such technologies can be made to work on a bigger scale, the future for renewable power looks not only bright … but yellow."
"
As you may or may not know, this blog has taken off with big traffic increases as of late.
While the traffic has been an indicator of success, unfortunately, keeping that success gets to be more and more time consuming. This blog platform is hosted on Moveable Type version 3.2, which is about as close to being crippled as blog software can get. For example. it doesn’t even have a spell checker. The spam comment filter doesn’t work much anymore, and email notifications are also broken. The host has promised for months now to upgrade the platform, but so far has been unable to do so.
Working with MT in it’s current state requires a lot of extra effort compared to other software, and I find myself spending an inordinate amount of time just dealing with the limits of Moveable Type and trying to work around them. It has become quite frustrating as I want to offer better quality content and find myself unable to easily do so. I can’t even put in fully rich HTML into MT because of the way it deals with formatting. I’ve tried several add on programs to aid in blog generation, all of which have been thwarted by the MT platforms non standards compliance. Of course, some of my more snarky readers would likely suggest that standards “don’t matter” and that any problems with the content can be “adjusted” 😉
So the question is this, should I:
1) Close this blog and give up blogging altogether on this platform
2) Move someplace else and link back to this location
3) both
I welcome any input.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3b81b69',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels and Paul C. “Chip” Knappenberger, from Cato’s Center for the Study of Science, review interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
  
  
With all the stern talk about global warming and widespread concern over climate change, you would think that we humans would have a propensity for cooler temperatures. Everywhere you look, the misery that rising temperatures (and the associated evils) will supposedly heap upon us seems to dominate reports about the coming climate. But do patterns of population movement really support the idea that we prefer cooler locations?   
  
**Increased Mobility**   
  
Since 1900, the population of the United States increased from about 76 million people to about 309 million people in 2010. Accompanying that population growth were major advances in technology and industry, including vast improvements in our nation’s system of transportation. As planes, trains, and automobiles replaced the horse and buggy, Americans became more mobile, and where we live was no longer connected primarily with proximity to where we were born. Instead, we became much freer to choose our place of residence based on considerations other than ease of getting there.   
  
Where has our new-found freedom of mobility led us? Figure 1 shows the rate of population change from 1900 to 2010 for each of the contiguous 48 states. Notice the increases in states with warm climates such as Florida, Texas, and California, and also in states with big industry (that is, jobs), such as New York, Michigan, and Ohio for example.   
  
  






  




_Figure 1. The state-by-state population trend (people/year) from 1900 to 2010 (data fromU.S. Census Bureau)._



Which states are people less likely to choose to live in? States such as North Dakota, South Dakota, Montana, Maine, Vermont—all of which have harsh climates and low temperatures.   
  
Comparing a map of the change in population (Figure 1) with a map depicting the average temperature of each state (Figure 2) reveals a pretty strong indication that people seem to be seeking out warmer states.   
  
  






**  
**_Figure 2. The state-by-state average annual temperature for the period 1900-2010 (statewide temperture data available from the U.S.National Climatic Data Center)._   
  
**Experiential Temperature**   
  
Another way of looking at human temperature preferences is to calculate what we’ll call the “average experiential temperature”—that is, the annual temperature that the average person living in the lower 48 states experiences each year. We can calculate this value by first multiplying the average temperature in each state during a particular year by the state’s population in the same year. Then we sum this product across the 48 contiguous states, and finally divide this sum by the total population of the country. In other words, the temperature in states with larger populations weigh more heavily on the national composite experiential temperature than does the temperature in those states with sparser populations. As the population of the country redistributes itself over time, we can track how the average person’s climate changes.   
  
When we do that for each year from 1900 to 2013, we get the result shown in Figure 3—a steadily rising temperature. In fact, the average experiential temperature has risen by a total of about 3.85ºF over the course of the last 114 years (a rate of 0.34ºF per decade).   








_Figure 3. The average experiential temperature of the population of the United States, 1900 to 2013._



But the history of experiential temperatures alone can’t tell us whether the increase has been unwillingly forced upon us by a large-scale warming of the climate from, say, an enhanced greenhouse effect, or whether the change results from Americans seeking out warmer locales on their own accord.   
  
**U.S.** **Average Temperature**   
  
To answer this question, we must calculate the area-weighted average temperature of the United States—that is, the combination of the yearly average temperature within each state weighted by that state’s total area. In this case, it is the size of the state, rather than the size of its population, that matters—the bigger the state, the bigger its contribution to the nationwide average.   
  
The result of this calculation is a quite different looking temperature history. In Figure 4, we included the annual U.S. average temperature history along with the annual U.S. “experiential” temperature from Figure 3. We see that, while the United States actual temperature has fluctuated a bit, experiencing warm decades such as the 1930s and 1990s and cold ones such as the 1910s and 1970s, it has increased only slightly during the 20th century—about 0.90ºF (a rate of 0.08ºF/decade).   






_Figure 4. Average temperature of the United States, 1900 to 2013._



For what it’s worth, when you calculate the national temperature this way (using the state-by-state temperature data from the National Climatic Data Center, NCDC), you get a heckuva lot less warming than is in the “official” NCDC record put out by the U.S. Department of Commerce. The difference lies in the “adjustments” plastered on to the original data. Both records are adjusted for a bias known as “time of day” when the previous 24-hour highs and lows recorded. It’s complicated, but it also does slightly alter the data.   
  
But the official version is additionally massaged more than—well, we can’t say in polite company. A laundry list can be found here. The sum of all of those adjustments is to put about twice as much warming in the record as is in our state-averaged plot.   
  
**Seeking the Heat**   
  
Although there has been a slight warm-up of the actual temperature, that rise is nowhere near the increase in the _experiential_ temperature. In fact, the average experiential temperature has climbed at a rate more than four times that of the U. S. average temperature—which is the experiential temperature had the population distribution not changed at all. That means that Americans have actively been moving to warmer climates. And there is every indication that they are continuing to do so, as evidenced by the strong rise in experiential temperatures during the past 20 or 30 years.   
  
While climatologists have not generally appreciated this fact, it has been long recognized and appreciated by sociologists. As both people's mobility and their ability to select the climate they prefer have increased throughout this past century, the core of the U.S. population has moved southward—into warmer climates. The overall migration of people into the southern ""Sunbelt"" states has created a temperature change over time for the ""average American"" that far outstrips the most pessimistic measurements of global warming for the past century, and rivals the projections for the next!   
  
Apparently, people--or Americans at least--seem to prefer a warmer climate to a cooler one. Next time climate prognosticators warn of the perils of rising temperatures, remember this: when given the means and a choice, some (or rather, most) like it hot!   
  
_(Special thanks to Robert C. Balling Jr. and Randy Cerveny, who assisted with early versions of this research.)_


"
"

In my recent op-ed for _The Hill_ examining the Obama administration’s estimation of the social cost of carbon (SCC)—a measure of how much future damage is purportedly going to be caused by each ton of carbon dioxide that is emitted through human activities—I identified two major problems with their measure.   
  
First, the administration’s SCC was based on an estimate of _global_ rather than domestic damages from anthropogenic climate change—an odd scope for a measure designed to be incorporated in the cost/benefit analysis of U.S. rules and regulations governing domestic activities (such as the energy efficiency of microwave ovens sold in the United States). In fact, Office and Management and Budget (OMB) guidelines state that   




Your analysis should focus on benefits and costs that accrue to citizens and residents of the United States. Where you choose to evaluate a regulation that is likely to have effects beyond the borders of the United States, these effects should be reported separately.



Instead of “reporting separately,” the administration’s SCC embodies “effects beyond the borders of the United States.”   
  
Second, the administration recently revised (upwards) its initial calculation of the SCC. In doing so, it included updates to its underlying economic/climate-change/damage models, but it did not include any updates to the characteristics of the equilibrium climate sensitivity used by the models. Since the equilibrium climate sensitivity is the _key factor_ in how much climate change will result from a given amount of anthropogenic carbon dioxide emissions, and since there is mounting scientific evidence that the equilibrium climate sensitivity is better constrained and lower than that used in the initial analysis, there is no defensible reason why the new science was not included in the administration’s revised SCC calculation.   
  
So that’s two strikes against it.   




What I didn’t go into in my op-ed, because it is a rather complicated topic, is the choice of discount rate used in the administration’s SCC analysis. The discount rate, generally put, reflects how much you are willing to pay now to avert future damages. The lower the discount rate, the more costly (in today’s dollars) future damages become. The same OMB guidelines mentioned above also cover the selection of the discount rate to use in cost/benefit analysis. The OMB guidance is that as a default an analysis should use a 7 percent discount rate as the base case, and to show the sensitivity of the results to the discount rate assumption, the analysis also should include the results of using a 3 percent discount rate.   
  
The administration ignored that guideline as well. Instead it opted to determine the SCC using discount rates of 2.5, 3, and 5 percent, and didn't include results for a 7 percent rate—results that would have indicated a _substantially_ reduced cost of future damages.   
  
With now three strikes against it, the administration’s determination of the social cost of carbon should be tossed out.   
  
The door to doing so has just been opened slightly with the announcement that the Department of Energy is opening for public comment a Petition for Reconsideration of its use of the administration’s newly figured and newly increased SCC in its above-mentioned microwave oven energy efficiency rule. We’ll see what becomes of that.   
  
In the meantime, here's an example of how the SCC is currently being used and abused in the justification of new regulations. Economist Robert Murphy (who recently testified to Congress as to the problems with the administration’s SCC methodology) posted this gem from the Environmental Protection Agency discussions of a proposed new rule regulating discharges from steam electric power plants (and how the rule may impact carbon dioxide emissions from the plants):   






Murphy comments:   




As the above table shows, EPA is being a dutiful federal agency, following Executive Branch guidelines on how to calculate costs and benefits—it reports its findings using both a 3 percent and a 7 percent discount rate. Yet as the footnote explains, when reporting the benefits of reducing CO2 emissions, the EPA actually _can’t_ use a 7 percent discount rate, because an estimate of the SCC (social cost of carbon) for a 7 percent rate is “not available.” _Why_ is it not available? Because the [administration’s] Working Group explicitly ignored the OMB guidelines, and only reported the figures for 3 percent and 5 percent.   
  
We thus have an absurd situation, in which EPA and other regulatory agencies will be following the rules and calculating benefits and costs at both the 3 percent and 7 percent discount rates. Yet, when they express the “social benefits” of reducing greenhouse gas emissions at the 7 percent rate, they are actually going to plug in the wrong number, and explain in a footnote why they are doing so. To repeat, this is important, because the “right” number would show that there are virtually _no_ “social benefits” from reducing greenhouse gas emissions.   
  
As I have explained elsewhere, there are far more problems to the Obama Administration’s computer-model-case against carbon, than just the choice of discount rate. Yet the knots into which the federal government has tied itself, in order to avoid revealing the truth about the actual economic literature, is quite revealing—not to mention hilarious.



The administration’s SCC is a devious tool designed to justify more and more expensive rules and regulations impacting virtually every aspects of our lives, and it is developed by violating federal guidelines and ignoring the best science.   
  
All around bad news.


"
"The gilets jaunes movement, by its own description, is motivated by broad discontent over shrinking incomes and rising living costs in France. However, the demonstrators in yellow jackets have rallied around one particular government policy: a looming hike in fuel taxes.  Since 2014, domestic excise taxes on energy products have been linked to carbon content, with more carbon-intensive fuels taxed at a higher rate. Tax rates have also been scheduled to increase on an annual basis. For consumers, this has translated into gradually rising prices for fossil fuels such as petrol, diesel, natural gas, and heating oil. The objective of these rate hikes, according to the French government, is to reduce reliance on imported energy, reduce greenhouse gas emissions and yield tax revenue to cut payroll taxes and stimulate employment. The problem, however, is that such taxes tend to be unpopular. While the recent protests in France stand out for their intensity, they join a growing record of political turmoil spurred by carbon and energy taxes in different parts of the world.  In 2017, Mexicans took to the streets to express outrage at fuel tax increases. During the recent US midterm elections, residents of Washington state again rejected a ballot measure aimed at introducing a carbon fee. North of the border, several Canadian provinces have started legal proceedings against a federal carbon pricing framework. Germany and Ireland recently backed away from carbon tax expansions.  Research has long shown how difficult it is to win popular support for carbon and energy taxes. One reason is that these impose an explicit and immediate cost on emitters, disproportionately affecting politically influential industries such as fossil fuel companies, which tend to use their influence to resist or weaken pricing policies. At the same time, they only promise diffuse future benefits for the broader population.  Renewable energy subsidies are a costlier way of achieving emission reductions overall but they spread that cost across the broader public. The financial benefit, meanwhile, is highly concentrated for subsidy recipients, creating strong supportive constituencies. Surveys confirm that voters prefer financial aid policies over taxes, suggesting that public support for climate action is broad but shallow. Addressing climate change enjoys widespread approval – until climate action comes with a tangible price tag, that is. Unlike other climate policies such as subsidies, carbon taxes make that price tag visible, and that is a big reason for their low popularity. Some commentators have therefore suggested a sequenced approach in which less contentious policies pave the way for gradual introduction of a robust carbon price. Still, the fact that other policies don’t carry an explicit price tag doesn’t mean they don’t also impose a burden on the economy and on consumers. In the long run, the cost of doing nothing will almost certainly outweigh the cost of climate action – but the latter is real and will be allocated unevenly, creating winners and losers. That explains a growing preoccupation with the distribution of climate policy impacts. The idea of a “just transition” is an overriding theme at this year’s climate summit in Poland. It asks us to consider how we can move society to a low-carbon world without leaving anyone behind. The idea was enshrined in a declaration signed by ministers attending COP24, calling for greater consideration of the social consequences of a low-carbon transition. The French could well have used this as a guide in its response to public backlash against its energy tax plans. The demand for a just transition includes gaining social approval for climate policy by compensating, training and supporting people likely to be impacted by it. The French government assigned revenues from its energy tax increase to the general budget and earmarked parts of it for a business tax credit meant to stimulate employment and competitiveness. For taxpayers, the benefits remained obscure, and credibility of the carbon tax suffered. Instead, the government could have returned the revenue directly to taxpayers in the form of uniform or targeted cash transfers. Not only would that have made the use of revenue more transparent, it would have counteracted the general tendency of carbon and energy taxes to be regressive, that is, to affect low-income households disproportionately. Dedicating a share of revenue to helping disadvantaged communities could further reduce the perceived inequities that sparked unrest in the first place. Other places have shown that it can be done. A carbon tax introduced in British Columbia in 2008 faced initial opposition, but smart investment of tax revenue – including an annual Climate Action Tax Credit for every citizen – and a robust communications strategy have since won it broad support. People need access to affordable low-carbon options if a carbon price is to be effective, which underscores the need for adequate investment in innovation and infrastructure, such as public transit or electric vehicle charging stations.  The gilets jaunes movement reminds us that we still have much to learn about how to craft climate policies that are both environmentally ambitious and politically durable. We would do well to heed this insight: time is not on our side, and we cannot afford more setbacks."
"

On Sunday I posted about the USHCN climate station of record in Tucumcari, NM highlighting its positive points since it has all the hallmarks of a well sited station with a long and uninterrupted record. But something was odd with the temperature record that didn’t quite make sense at first glance.
I also cross posted my report on Climate Audit since I always value additional input from the community there.
I noted that while this station is in fact well sited, and rates a CRN2, it has some oddities with it’s temperature record around the year 2000, something that looked like a step function to me.

Click for larger graph from NASA GISTEMP
Of course, even though this a truly rural station, 3 miles from the outskirts of town, Hansen and GISS apply adjustments to it anyway, which is part of the flawed “nightlights” algorithm incorrectly flagging this station for adjustment. Even though the adjustment makes the present cooler, it still seems misplaced given the station quality and history. Steve McIntyre said it best:
Here we have a rural station where there doesn’t seem to be any reason to adjust the temperature for population growth/UHI. But in this case, Hansen adjusts Tucumcari as though it were a city. Why is he even adjusting Tucumcari at all? (The “reason” is that its lights value removes it from the rural classification and it goes into the adjustment pool.) While Hansen sometimes seemingly cools rural stations in the past, for the GISS dset2 version here, he warms the past of the station (cools the present).
It’s more that this is a case of another unjustified adjustment by the “adjuster in chief” showing once again that the Hansen adjustments do not do what they are supposed to do – and the best that can be hoped from the Hansen adjustment program by users of this dataset is that the adjustments overall end up being pointless and random, rather than pointless and biased.
The adjustment by GISS looks like this:

Ok adjustments aside, the fact that there is a step at 2000 that remained unexplained until commenters on CA started looking at the data themselves. DaleC provided this graph:

Click for original image
Note the arrow that I place at the year 2000. Notice anything?
The annual average minimum temperature has exceeded 45°F and maintained the rise since 2000. For the first time in the station history going back to 1905, the minimum temperature has gone above that 45°F mark and stayed there. Yes there have been some previous brief excursions above 45°F, but none appear to have lasted more than 2 years. Note that average annual maximum temperatures did not increase during the same period.
What could cause that? We can rule out adjustments, since this is GHCN data before Hansen gets his adjuster mitts on it. We can rule out location change or equipment change, since according to NCDC metadata the station has been in the same place since at least 1946 and possibly longer. It still uses mercury max/min thermometers, so there’s no MMTS next to a building or parking lot to blame.
So what is left? Something around the station in the measurement environment that affects the nighttime readings. I recalled seeing this before. And back in early 2007, I had posted a story about a paper from Dr. John Christy of UAH where he studied a number of stations in the San Joaquin Valley in California because they had exhibited this same symptom:
The culprit? Irrigation. See my story and Christy’s press release
Christy remarks: “Another factor is the dry air, something common to all deserts. Water vapor is a powerful greenhouse gas. Desert air lacks water vapor. The air turns cold at night because it doesn’t retain much warmth from the daytime and it can’t trap what little heat might rise from the ground at night.”
Evaporation from irrigated fields adds water vapor to the air — a process that cools summer days but traps heat rising from the damp soil at night.
“If there is anything I’ve learned in Alabama, it is that humidity can make summer nights very warm,” said Christy, a Fresno, Calif., native who has lived in Alabama since 1987.
Once I mentioned this as a possibility to explain the increase in nighttime temperatures, it didn’t take Steve McIntyre long to find some anecdotal evidence that correlated:

http://cahe.nmsu.edu/news/1997/043097_irrigation_tour.html
A few years ago when cattle prices were high, we saw a tremendous increase in the number of irrigated grass acres in Quay County,” said Jeff Bader, Quay County Extension program director. “One reason was our limited water situation for irrigation, and high cattle prices made it look very attractive.”
When cattle prices dropped again, interest in irrigated pastures declined, but now the cattle market is improving, he said. Producers never really lost concern about irrigated pasture because it fits into the management scheme for water conservation so well in Quay County.
“Irrigated pastures fill a niche in this area because of their ability to produce under varying levels of irrigation,” said Rex Kirksey, superintendent of NMSU’s Agricultural Science Center in Tucumcari. “Pastures remain a viable option in many situations where irrigation water is too limited or unpredictable for corn or alfalfa production.”
What is interesting is that the director of the Ag Science Center, Rex Kirksey, is also the person that took these photos for the station survey. There’s quite a large water project in the area, called unsurprisingly, the Tucumcari Project.







Conchas Dam and Lake




Apparently they have quite a problem with water “disappearing” there as outlined in this report:
“The principal problem has been the loss of over half of the District’s surface water supply in the canal and distribution system that carries Canadian River water from Conchas Reservoir to the irrigated farms in the District.”
“The District’s report concluded that seepage losses from the system’s canals become greater, in quantity, each year.”
From that report I obtained a the study area map, and located where the town and the USHCN station is. Unsurprisingly, the USHCN station is situated close to the canals of the water project, and prevailing winds in the area tend to be from south to southwest:

With increased irrigation to pastureland for cattle, and a leaky canal system that loses half it’s volumes and demands ever more water to meet customer deliveries, it seems plausible that the Tucumcari area is becoming more humid, and with the increased humidity, per Christy, increased night time temperatures.
The studies of Roger Pielke Sr. show that land-use changes are an important factor in local and regional climate change. This effects of the changes in agriculture and irrigation on the local measurement of climate in Tucumcari might very well make a good case study.
I had hoped that the automated weather station that sets next to the Stevenson Screen might have humidity data that I could track, but alas it does not.

There is a fairly complete record though of temperature and precipitation at this link from the Western Regional Climate Center.
UPDATE 7/1/08 One of our commenters “AnonyMoose” has brought this well done historical weather and climate report by NMSU superintendent Rex Kirksey to our attention:
http://tucumcarisc.nmsu.edu/documents/rr751.pdf
This merits some further study for the data it contains. I’m committed to other projects today, so if anyone wants to have a go, be my guest and I’ll post it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e08f0a0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
_\---_   
  
Methane is all the rage. Why? Because 1) it is a powerful greenhouse gas, that molecule for molecule, is some 25 times as potent as carbon dioxide (when it comes to warming the lower atmosphere), 2) it plays a feature role in a climate scare story in which climate change warms the Arctic, releasing methane stored there in the (once) frozen ground, which leads to more warming and more methane release, _ad apocalypse_ , and 3) methane emissions are also be linked to fossil fuel extraction (especially fracking operations). An alarmist trifecta!   
  
Turns out, though, that these favored horses aren’t running as advertised.



While methane is a more powerful greenhouse gas in our atmosphere than carbon dioxide, its lifetime there is much shorter, even as the UN’s Intergovernmental Panel on Climate Change can’t quite say how long the CO2 residence time actually is. This means that it is harder to build-up methane in the atmosphere and that methane releases are more a short-term issue than a long-term one. If the methane releases are addressed, their climate influence is quickly reduced.   
  
This is why methane emissions from fracking operations—mainly through leaks in the wells or in the natural gas delivery systems—really aren’t that big of a deal. If they can be identified, they can be fixed and the climate impact ends. Further, identifying such leaks are in the fracking industry’s best interest, because, in many cases, they represent lost profits. And while the industry says it has good control of the situation, the EPA isn’t so sure and has proposed regulations aimed at reducing methane emissions from new and existing fossil fuel enterprises. The recent scientific literature is somewhat split on who is right. A major paper recently published in _Science_ magazine seemed to finger Asian agriculture as the primary suspect for recent increases in global methane emissions, while a couple of other recent studies seemed to suggest U.S. fracking operations as the cause (we reviewed those findings here).   
  
And as to the runaway positive feedback loop in the Arctic, a new paper basically scratches that pony.   
  
A research team led by University of Colorado’s Colm Sweeney set out to investigate the strength of the positive feedback between methane releases from Arctic soil and temperature (as permafrost thaws, it releases methane). To do this, they examined data on methane concentrations collected from a sampling station in Barrow, Alaska over the period 1986 through 2014. In addition to methane concentration, the dataset also included temperature and wind measurements. They found that when the wind was blowing in from over the ocean, the methane concentration of the air is relatively low, but when the wind blew from the land, methane concentration rose--at least during the summer/fall months, when the ground is free from snow and temperature is above freezing. When the researchers plotted the methane concentration (from winds blowing over land) with daily temperatures, they found a strong relationship. For every 1°C of temperature increase, the methane concentration increased by 5 ± 3.6 ppb (parts per billion)—indicating that higher daily temperatures promoted more soil methane release. However (and here is where things get real interesting), when the researchers plotted the change in methane concentration over the entire 29-yr period of record, despite an overall temperature increase in Barrow of 3.5°C, the average methane concentration increased by only about 4 ppm—yielding a statistically insignificant change of 1.1 ± 1.8 ppm/°C. The Sweeney and colleagues wrote:   




The small temperature response suggests that there are other processes at play in regulating the long-term [methane] emissions in the North Slope besides those observed in the short term.



As for what this means for the methane/temperature feedback loop during a warming climate, the authors summarize [references omitted]:   




The short- and long-term surface air temperature sensitivity based on the 29 years of observed enhancements of CH4 [methane] in air masses coming from the North Slope provides an important basis for estimating the CH4 emission response to changing air temperatures in Arctic tundra. By 2080, autumn (and winter) temperatures in the Arctic are expected to change by an additional 3 to 6°C. Based on the long-term temperature sensitivity estimate made in this study, increases in the average enhancements on the North Slope will be only between -2 and 17 ppb (3 to 6°C x 1.1 ± 1.8 ppb of CH4/°C). Based on the short-term relationship calculated, the enhancements may be as large as 30 ppb. These two estimates translate to a -3 – 45% change in the mean (~65 ppb) CH4 enhancement observed at [Barrow] from July through December. Applying this enhancement to an Arctic-wide natural emissions rate estimate of 19 Tg/yr estimated during the 1990s and implies that tundra-based emissions might increase to as much as 28 Tg/yr by 2080. This amount represents a small increase (1.5%) relative to the global CH4 emissions of 553 Tg/yr that have been estimated based on atmospheric inversions.



In other words, even if the poorly understood long-term processes aren’t sustained, the short term methane/temperature relationship itself doesn’t lead to climate catastrophe.   
  
The favorite thoroughbreds of the methane scare are proving to be little more than a bunch of claimers.   
  
  
  
**Reference:**   
  
Sweeney, C., et al., 2016. No significant increase in long-term CH4 emissions on North Slope of Alaska despite significant increase in air temperature. _Geophysical Research Letters_ , doi: 10.1002/GRL.54541.   
  
__


"
"
Share this...FacebookTwitterA new international agreement to replace the Kyoto environmental protocol will not be signed in 2010, the Russian presidential advisor on climate change has said. Read more:
http://en.rian.ru/Environment/20100426/158743966.html
On April 16, Russian President Medvedev said:
All countries, including developed and developing economies, should reach an agreement, or, if we do not agree on this [the common terms of carbon emissions reduction], Russia will not prolong its participation in the Kyoto agreement – you cannot have it both ways.
http://en.beta.rian.ru/Environment/20100416/158607110.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMany of us have made complaints about how the MSM is biased and reluctant to cover topics like Climategate and other scandals. Well take heart! People are waking up to the fact that they are being denied information and facts, and as a result they’re turning their backs on the main stream print media. Sure a part of it has to do with bad economic times (can’t blame that on Bush anymore), but another factor is that the internet offers an alternative. Here are the gory numbers for newspaper circulation in the US. Only the conservative Wall Street Journal has made gains. The rest are bleeding massively.
The Wall Street Journal 2,092,523 +0.5%
USA Today 1,826,622 -13.58%
New York Times 951,063 -8.47%
Los Angeles Times 616,606 -14.74%
Washington Post 578,482 -13.06%
Dallas Morning News 260,659 -21.47%
San Francisco Chronicle 241,330 -22.68%
The Star-Ledger, Newark, N.J. 236,017 -17.79%
Read the complete story here: http://www.editorandpublisher.com/eandp/news/article_display.jsp?vnu_content_id=1004086334
I’ve always encouraged tree-hugging subscribers to heed the advice from newspapers to cut CO2 emissions. And what better way to begin than to scale back the energy-guzzling industry of newspaper production, distribution and newspaper disposal? This can be done by cancelling subscriptions. This also rescues a lot of trees. So hats off to the tree-huggers out there who choose to save a tree by ending a subscription.
Share this...FacebookTwitter "
"
I mentioned this a couple of posts back, exploding Comet 17P/Holmes is one of the strangest things in the sky ever but it gets stranger…Last night, astrophotographer Alan Friedman of Buffalo, NY, took a close-up picture of the comet’s core. “A strong deconvolution filter followed by multiple passes of unsharp mask and gaussian blur reveals startling new structure in comet 17P/Holmes.” i.e. after processing the image with Photoshop Here’ what popped out:

Linus would be impressed. The structure is due to outgassing from fissures and crevices in the comet’s core, but it is just a little too coincidentally spooky. Of course we’ve always seen things in the sky that mirror our minds, from constellations to UFO’s,  so why not Halloweeen?
Would you like to see the comet masquerading as the great pumpkin?? Look north after sunset for an expanding fuzzball in the constellation Perseus: sky map. Comet Holmes is about as bright as the stars of the Big Dipper, easy to see with the unaided eye and for backyard telescopes. The Chico Observatory in upper Bidwell Park will also be looking at this object.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2d50172',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
After a near brush with death, GOES 12 is back up and running, our full disk image  is now 100% The loop may take some time to get synced but images are being produced from GOES 12 correctly now.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea231e8fe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The _Washington Post_ has an article today on the battle over the Keystone XL pipeline. There is a sense of urgency on both sides as the decision on the project is expected to be fast approaching.   
  
  
The _Post_ features arguments from pipeline proponents that the project will provide an economic boost to the state of Nebraska, and from pipeline opponents that the oil carried though it will lead to more carbon dioxide emissions than previously thought, thus upping the impact on global warming and climate change.   
  
  
But the numbers being tossed about don’t tell the whole story.   
  
  
First, a look at the new economic claims. An analysis from the Consumer Energy Alliance concludes that during the two year construction phase of the pipeline, the economic activity in Nebraska will increase by a bit more than $400 million per year—generating directly or indirectly, about 5,500 new jobs. Sounds impressive, but this boost is short‐​lived. After that, for the next 15 years, the economic input drops down to about $67 million/​yr, supporting about 300 jobs. A net positive, but not as much as many proponents claim.   
  
  
The climate claims are even less significant. In its new report, Oil Change International asserts that the current estimates of the well‐​to‐​wheel (WTW) carbon dioxide emissions from oil extracted from the Alberta tar sands have been underestimated. They claim that the State Department failed to fully include carbon dioxide emissions from the burning of the petroleum coke that is produced as a side product of producing oil from the tar sands. This “petcoke” can be burned like coal, and in fact, is cheaper and more energy dense than coal, so it is often preferable. According to Oil Change International, including the petcoke in the calculation would increase the WTW carbon dioxide emissions by about 13 percent.   
  
  
There are several things wrong with the Oil Change International analysis. First is that the State Department actually did include a considerable discussion of the influence of the treatment of petcoke in its assessment. It concluded, just like Oil Change International, that if the petcoke is burned, it increases the total wells‐​to‐​wheels carbon dioxide emissions of Canadian tar sands oil by the same 13 percent. But what the State Department points out, and which Oil Change International plays down, is that the burning of petcoke to produce energy by and large displaces the use of coal for the same purpose. So instead of the total emissions, what is important is the _incremental_ carbon dioxide emissions produced from using petcoke instead of coal. And when that number is used, the WTW emissions increase by less than 1 percent—which is why the State Department concluded that the fate of the petcoke really wasn’t all that significant in the overall WTW emissions calculation.   
  
  
But whether consideration of petcoke increases the WTW carbon dioxide emissions of the tar sands oil by 1%, 13%, or any number in between, really doesn’t matter anyway in terms of its impact of global warming. For as I have shown previously, the global warming potential of the Keystone XL pipeline oil is only about 0.00001°C/yr. Increase that by 13% and you basically get the same environmentally insignificant number. In fact, you’d have to increase it _by several orders of magnitude_ before it is even worth paying attention to.   
  
  
The war over the pipeline will probably rage on until (and even after) a decision is reached in a couple of months. Hopefully, emotion will play a role secondary to facts.
"
"
Share this...FacebookTwitterStar in Easy Rider
Share this...FacebookTwitter "
"What is a healthy room temperature?  On releasing its Cold Weather Plan for 2014, Public Health England has recently revised its recommended minimum levels to keep in good health.  No longer, they say, do living rooms need to be kept at 21°C and bedrooms at 18°C, as used to be advised. Now all rooms can be kept at 18°C with “minimal risk” to the health of “a sedentary person wearing suitable clothing”. It’s a rethink of just three degrees but, as with outdoor climate change, a few degrees of indoor climate change can make a significant difference. Such a review was long overdue. The World Health Organisation advice that standards were based on was more than 30 years old, and the accumulation of evidence over time (although limited) warranted another look. Under 18°C evidence shows that the health  risks increase, particularly for those with cardio-respiratory disease, and for older people who spend a lot of time at home and may not perceive cold temperatures due to physiological changes that occur with ageing. But the evidence that 21°C was a significant threshold, even for those most vulnerable, was found wanting. So good scientific governance.   But it is also a recognition that room temperature can have an indirect impact on health. If people keep their living rooms at 21°C because they feel they ought to, they will spend more on their energy, potentially getting into financial debt and other forms of hardship that have health and well-being implications. Higher room temperatures also more mean more energy demand and more carbon emissions, so the longer term health effects of climate change also need to be factored in. All sound and balanced thinking. Keeping warm and healthy depends on many other factors. Our bodily temperature is also the result of the clothes we wear, what we eat and drink, how active we are, all in a systemic interaction. So Public Health England says that younger people who are healthy and active and able to “layer up” should be fine with lower than 18°C. And overnight certainly the 18°C threshold is less important for healthy people if they have sufficient bedding, clothing and use thermal blankets when necessary. “Layering up” advice is territory where others have come unstuck. In an ongoing analysis of newspaper reporting of energy issues we are finding a pretty universal castigation of any suggestion made by politicians and energy companies that people should wear more clothes or put another blanket on the bed. Who are the prosperous and profiting to tell others how to live?  The messenger matters, clearly.  Public Health England should get an easier ride, but the organisation still concedes, “we know that people have strong feelings about their homes and don’t want to be told what to do in them”. Such are the political sensitivities nowadays around even common sense health advice entering the domestic sphere. The rethink on the “need” for room temperatures could have implications for government policy which currently defines being fuel poor as the energy required keep the main living area at 21°C (along with “required” energy for other purposes such as lighting and hot water).  Should this threshold now shift to 18°C? Maybe, but careful consideration is needed. The guidance notes that older or less healthy people can benefit in medical terms from a “slightly” higher room temperature.  If fuel poverty policy is about protecting the most vulnerable under all circumstances, then 18°C may well not be sufficient. In other respects the advice might not have got the balance quite right. They recommend keeping a bedroom temperature of 18°C overnight for people older than 65 or with pre-existing medical conditions, as this “may be beneficial” to protect their health. The health evidence here is not so strong (hence the “may”), and if taken seriously would mean keeping home heating systems on overnight in winter. Research shows this is far from standard practice for most households, and particularly for people on low incomes, so would imply increased energy costs of the form that elsewhere Public Health England says it is keen to avoid. There are also questions about energy rights, or energy justice more broadly. The new guidance says the 18°C threshold is relevant for everyone. Does it then follow on health grounds – as well as on the grounds of meeting what has become a basic human need – that there should be no longer be any disconnections by electricity and gas suppliers?   This was one of the demands of the Energy Bill of Rights recently launched by Fuel Poverty Action at the House of Commons. There are already regulatory and best practice provisions that limit the circumstances and times of year during which certain categories of households can be disconnected by energy suppliers. But if all people need to be able to achieve 18°C (or near to this) at home to minimise health risks, then shouldn’t the “right to not be cut off” be universally applied?   Since 1999, home water disconnections have been banned on health grounds. If “room temperature” is truly vital to health, maybe it is time we banned energy disconnections as well."
nan
"Lehman Brothers filed for bankruptcy on September 15, 2008. The investment bank’s collapse was the drop that made the bucket of global finance overflow, starting a decade of foreclosures, bailouts and austerity. The resulting tsunami hit the global economy and public sector, discrediting finance and its attempts to extract large rents from every aspect of the economy, including housing and food. An alternative was urgently needed. Ten years later, private finance and large investors will play a central role at the COP24 in Katowice, Poland, and in the full implementation of the 2015 Paris Agreement. Representatives from pension funds, insurance funds, asset managers and large banks will attend the meeting and lobby governments, cities and other banks to favour investments in infrastructure, energy production, agriculture and the transition towards a low-carbon economy.  There is a US$2.5 trillion gap in development aid which needs to be filled if poor countries can adequately mitigate the effects of climate change. With little enthusiasm among rich countries to stump up, the role of private finance is inevitable. Policy makers trust financial capital as our best hope of securing investment to avoid the catastrophic warming beyond 1.5°C. This has been the case for a while – the first announcement came at the UN Climate Summit in 2014, when a press release on the UN website said the investment community and financial institutions would “mobilise hundreds of billions of dollars for financing low-carbon and climate resilient pathways”. Since then, networks that stress the role of private finance in rescuing the planet have multiplied, including the Climate Finance session at the Sustainable Innovation Forum, which will also take place in Katowice, on December 9-10 2018. It is difficult to ignore that a strong reliance on private finance means putting the future of Earth in the hands of individuals and institutions that brought the global economy to the verge of collapse. It may be partially true that some are divesting from fossil fuels and funnelling their money into better projects. But before we pin our hopes on finance to solve climate change, there are some things we need to ask ourselves. How did we get to a point in history where it is taken for granted that public money alone can never be sufficient to finance our transition from fossil fuels? Is it an objective condition with no clear causation and responsibility, or something else?  What about the fact that global military spending in 2017 reached US$1.7 trillion while poor countries promised funding for climate change adaptation and mitigation in 2015 are still waiting? 


      Read more:
      COP24: climate protesters must get radical and challenge economic growth


 What about the cost of bailouts to the financial sector, which in the UK alone has been estimated at US$850 billion? As Michael Lewis noted in his boomerang theory, states that have propped up financiers with public money are now asking those same financiers to step in and do the job that states should do. And this leads to the second consideration. Climate change is historically, politically and socially complex. Although sustainable finance is not presented as the sole solution, analysing its role produces a series of strategic short circuits.  


      Read more:
      Climate change and migration in Bangladesh – one woman's perspective


 It oversimplifies and depoliticises the response to climate change. It legitimises the idea that sustainability can be achieved within continuous growth and expansion, which are essential to the survival of the financial sector.  It rewrites the way we think about our planet in the vocabulary of finance and its obsession for a return on investment. It marginalises any claim to address climate change based on present and historical injustices, redistribution and bottom-up projects organised by ordinary people. It accepts that the financial way of defining sustainability and its achievements are inherently aligned with the rights, interests and needs of people and the planet. Finance may be a partner in the fight against climate change, but it is certainly not a partner motivated by altruism. It’s motivated by generating profit from the transition. It is therefore unsurprising that energy generation, railways, water management and other forms of climate mitigation have been identified as priorities for sustainable finance. Wall Street can find large returns by investing in the transition to “greener” infrastructure, including the not-so-green Chinese green belt and road and dams like the Belo Monte, a project that originally applied for carbon credits and was labelled as a sustainable investment. Green bonds can help cities finance projects to reduce their environmental impact or adapt to climate change.  However, if money is the driver, we should not expect private investors to have any interest in projects that won’t generate a sufficient return, but would benefit people or cities that cannot pay for the service or for the debt, or that would protect vulnerable people from climate change. If climate change is fought according to the rules of Wall Street, people and projects will be supported only on the basis of whether they will make money. Ten years ago, the world saw that finance had permeated every aspect of the global economy. Back then, it was clear that financial interests could not build a better and different world. Ten years later, COP24 should not legitimise large financial investors as the architects of a transition where sustainability rhymes with profitability."
nan
"
Share this...FacebookTwitterH/T: Benny Peiser
This is the final day of the Deutsche Welle’s Global Media Forum, this year’s conference is titled “The Heat is On – Climate Change and the Media”. If any conclusion can be drawn, it is that elite warmists are extremely frustrated. Read here.
Bob Ward:
British journalists don’t know difference between fact and fiction.
Peiser’s GWPF report reads: “But he also concedes that there have been grave mistakes made by researchers”. And Ward called for scientists to handle their findings and knowledge responsibly. Ward goes on to say:
The IPCC is too slow in correcting the faults.
Naomi Oreskes, non-consensus denialist:
The statements from scientists are so greatly disconnected from the media in the USA because the journalists unknowingly and inaccurately repeat what was said.
…so-called climate skeptics are nothing but “contrarians” and can’t be taken seriously because their critique isn’t scientifically based.
Can you hear their teeth gnashing?
Share this...FacebookTwitter "
"”Unprecedented” is the word that keeps being tied to the apocalyptic weather Australia has faced over the past few months. Bushfires have always been a reality in Australia, but never recorded on this scale with such widespread damage. It’s estimated that more than 60,000 sq km have been scorched in New South Wales and Victoria alone. Days of smoke have shrouded Sydney, Canberra and Melbourne. And after the fires, flooding at the weekend in NSW and parts of Queensland left thousands without power and dozens of schools closed on Monday.  While the country is still grappling with the economic reality and human devastation caused by the fires, it’s easy to think the worst of this disaster is over. But unfortunately other extreme weather may yet occur this summer and these will also require safety preparations and rapid responses. Last year was the driest and hottest year on record in Australia. Some parts of the country have had several years of drought in a row. But all droughts end eventually. At the weekend devastating storms swept through eastern NSW, causing flooding, power outages and commuter chaos. The Bureau of Meteorology says 391.6mm of rain fell over Sydney in the past four days, the most since 414.2mm fell from 2 to 5 February 1990. Historically Australian continental-scale droughts are often broken by widespread heavy rain, leading to an increased risk of flooding, including potentially lethal flash floods. The flood risk from the heavy rains is exacerbated by the bare soil and lack of vegetation caused by the drought and by bushfires that destroy forest and grassland. When a decade-long drought ended in 2009, what followed were two extremely wet years with serious flooding. Flooding also brings the risk that ash might contaminate water supplies. The heavy rain falling on bare soil can also lead to serious erosion. The onset of the tropical wet season over northern Australia has been very much delayed, as predicted in the middle of last year by the Bureau of Meteorology. Most of the Australian tropics have had well below average rainfall in the past few months, and some areas had their lowest November-January rainfall. As well, the tropical cyclone season was late, also predicted by the bureau months ago. In recent weeks there has been some cyclone activity and some rain. But the wait is still on for widespread tropical rains and for more cyclones to cross the coast as Damien did at the weekend. Although rain brought by cyclones are often welcome, these systems can also leave serious damage. We are at the riskiest time for heatwaves in southern Australia. The risk usually peaks around the middle to the end of summer. Weather conducive to increased bushfire risk also usually peaks in February for southern states. So although media, community and political attention have focused on the horrendous bushfires we have already suffered, we should not overlook the likelihood of other extreme weather, including cyclones, floods and heatwaves, or think that the bushfire risk is over for the year. It is important to remain vigilant for all weather extremes. Although the fires have tragically claimed many lives, many others have been saved by the improved firefighting and warning systems, supported by improved weather and climate forecast systems to emergency services and the media by the Bureau of Meteorology. Temperature forecasts five days ahead are better than one-day-ahead forecasts were 50 years ago. Rainfall forecasts have also improved dramatically. As well, scientific seasonal climate forecasts of rain, drought and seasonal cyclones – something not even dreamed of 50 years ago – are provided routinely by the bureau. These longer forecasts allowed emergency services to better prepare for this horrible summer, and the detailed shorter-range forecasts of weather conducive to fire spread has helped fire agencies target warnings and provide resources to threatened areas. But one challenge as the summer continues will be the need for continued communication between forecasters, emergency services and the public about predicted extreme weather. For some, “warning fatigue” may set in and announcements of dangerous heatwaves or floods might be ignored. The warnings must continue to be disseminated to at-risk populations and local authorities must strive to ensure they are acted on. And meteorologists must keep up their efforts to improve forecasts on all timescales. Global warming is already lengthening the fire season and making heatwaves more intense, more frequent, and longer. It is also increasing the likelihood of heavy rains and flash floods while simultaneously making droughts worse in some areas. The occurrence of devastating bushfires has been increasing in the past few decades, despite better forecasts and improved firefighting technology and organisation. The intrinsic link between these weather extremes and climate change means we need to address the wider issue: what can we do to slow the rate of global warming. Australians pride themselves on winning against the odds and adapting to extreme weather. How often have you heard a politician say Australia has always been a land of droughts and flooding rains, or that we have always had heatwaves and bushfires? Australian rainfall is indeed more variable than most parts of the world, so the reality is that we do face extreme weather, year after year. But another reality is that daytime temperatures across Australia have increased by more than a degree just in the past 25 years. In the 1990s on average about 5% of the country each year had annual average daytime temperatures in the hottest 10% of historical temperatures. But over the past five years, on average, more than half the country has experienced such hot temperatures each year. From 5% to over 50% in the lifetime of generation Z. Such strong warming must affect not just all the weather extremes – droughts, floods, heatwaves, bushfires, cyclones and storms – but also their impacts on humans, animals and the bush. And there is no reason to expect this warming to slow without concerted political action. So we need an increased focus on how to deal with these amplified risks from climate and weather extremes – how to adapt – at the same time that our politicians lead global action to slow the rate of warming. We don’t have much time. We need to adapt, to manage the unavoidable, and slow greenhouse gas emissions to avoid the unmanageable. • Neville Nicholls is an emeritus professor at Monash University. He spent 35 years with the Bureau of Meteorology, with his research focusing on how and why the climate is changing"
nan
"
There is a lot of intense interest out there in watching TS Noel to see if this disorganized system turns into a Hurricane when it hits the warmer waters of the Gulf Stream. Click on the image for an animated version.

Then, place your bets.
The image is from my company, IntelliWeather, and updates every half hour.
UPDATE: Noel did in fact become a hurricane, and now has 80 mph sustained winds, but it appears to be headed out to sea, and into cooler waters where it will likely dissipate.
from NHC advisory 22: …HURRICANE NOEL EXPECTED TO BECOME A LARGE AND POWERFUL EXTRATROPICAL STORM OVER THE OPEN ATLANTIC ON FRIDAY… NOEL IS MOVING TOWARD THE NORTH-NORTHEAST NEAR 20 MPH…32 KM/HR…AND THIS MOTION IS EXPECTED TO CONTINUE DURING THE NEXT 24 HOURS. ON THIS TRACK…NOEL WILL CONTINUE TO MOVE AWAY FROM THE BAHAMAS.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2e89e8d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On January 10, CBS News began a series of six, count ‘em, separate global warming scare stories. On January 27, President Clinton called global warming “the greatest environmental challenge of the new century.” On February 1, Roger Ballentine, Clinton’s deputy assistant for environmental initiatives, sent a “Dear Interested Party” letter elaborating on the president’s position. On February 9, Bill Stevens, the New York Times global warming reporter (and advocate of Clinton/​Gore policies; see his new book The Change in the Weather) called, saying he’s writing a new, comprehensive feature article on the subject. On April 22, Earth Day, presidential candidate Al Gore will release a new edition of his 1992 bestseller, Earth in the Balance. 



Is there a pattern here? 



After avoiding the issue like the political plague that it is, the Gore campaign has decided to go into high dudgeon over climate change. 



The Gore team is banking on some type of national weather disaster this summer. They hope to call attention to global climate change and their belief that uncaring Republicans refuse to pass the Kyoto Protocol on global warming. This U.N. document will cost the country a fortune and has the potential to relegate an amazing percentage of our land — the United Nations calls it “Kyoto lands” — to their watchful eyes. They are about to release a report that puts just about all U.S. forested land in this category, as well as much of our farmland. That’s easily half the country. 



This makes it a good idea to examine what is coming out of the White House as it ramps up the weather horror machine. On February 1, Deputy Assistant Ballentine wrote, “You may have noticed the steady stream of new scientific studies suggesting that global warming is . . . occurring more rapidly than previously thought.” 



What “steady stream?” Fact: Of hundreds of global warming papers that appeared last year, only one, published in Geophysical Research Letters, says this, and it does so by using 16 months of data to forecast the next 100 years. The only “steady stream” it has created is a torrent of scientific criticism. 



Rather, the balance of scientific evidence, according to the U.N. Intergovernmental Panel on Climate Change is to the contrary. In its last comprehensive report, the United Nations stated, “When increases in greenhouse gases only are taken into account, most [climate prediction models] produce a greater mean warming than has been observed.” In other words, the computer models that gave rise to the initial concern predicted too much warming. 



From Mr. Ballentine: “Reports by … the National Climatic Data Center … found that since 1976 the planet has been warming at a rate of 0.35 degrees Fahrenheit per decade.” 



Fact: Integrated over the troposphere–the earth’s active weather zone — the planetary warming since 1976 has been a mere 0.07ºF/decade, or far beneath normal background fluctuations in this region. According to NASA scientist John Christy, writing in Nature magazine, the originally forecast tropospheric warming rate was around 0.70ºF/decade. This is 10 times what has been observed. Because those models, in the United Nation’s words “produce[d] a greater mean warming than has been observed,” the integrated tropospheric warming forecast was lowered, by 1997, to 0.4ºF/decade. This is still an egregious error, and a new report by the National Research Council has finally admitted that it casts serious doubt on current computer forecasts of global warming. 



More Facts: It is seriously misleading to report the temperature of “the planet” in disregard of the distribution of observed surface warming. Had Mr. Ballentine consulted the latest issue of Climate Research, he would have seen that by far the greatest warming is occurring in the coldest winter air masses of Siberia and northwestern North America. Northern Hemisphere cold‐​season warming outside those regions averages one‐​tenth of what is being observed within them, which is below normal variability. 



Still More Facts: The very air masses that are warming are those under which winter mortality is four times greater than summer mortality. Furthermore, in almost every year that surface temperatures have warmed, global food production has risen. This results from improved technology, benign weather and the same carbon dioxide that makes the coldest air of winter less deadly. Finally, as surface temperatures have warmed, we have witnessed the greatest democratization of wealth and expansion of longevity in human history. 



None of this matters when the hype is on, and Gore knows a lot about American weather. He has been told, for sure, that conditions in the industrial Midwest, Texas and Southern California are fairly dry, predisposing the region to a very mediagenic drought, just in time for the nominating conventions. 



And Gore surely has been told that the way the federal government measures moisture status — please sit down — puts an average of 20 percent of the Electoral College in drought each summer. This year, thanks to where the dry conditions are, it’s closer to one‐​third, or a mere New York+Florida from putting Gore in the White House.
"
"
From the days of Wine and Gore department: The Second annual Climate Change and Wine Conference is scheduled for Feburary 15th and 16th in Madrid, Spain. Al Gore will be the featured speaker.
http://www.climatechangeandwine.com/eng/index.php
SALUD!
Wine snobbery and climate change together.  What’s not to like?
No word yet on whether California wineries will be attending.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1a1dcff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The mascots for the Rio de Janeiro 2016 Olympics and Paralympics have been announced. The yellow chap above is the Olympic mascot, apparently an amalgam of all Brazilian animals, which surprisingly only includes monkeys, cats and birds. The green and blue fellow, the Paralympic mascot, is supposed to represent all of Brazil’s plant species. I’m not using their names as they don’t have them yet – the mascots are waiting for the results of a public competition.  The name choices on offer are not great.  Two are from Brazilian slang meaning great (Oba and Eba), two are from a native language and have meanings related to dance (Tiba Tuque and Esquidim) and the final two choices are after two of the founders of bossa nova music (Tom and Vincius). It’s a real shame that neither of the mascots has the chance of being given a female name. Over the past year Brazilian conservationists have been lobbying for various different species to be the Olympic mascot. Of course this is motivated by the financial funds likely to be directed towards the winner. Proposed species included primates (muriquis and golden lion tamarins), cats (ocelot) and birds (hummingbirds).  All of these would be appropriate and worthy choices. The golden lion tamarin is one of the true symbols of conservation biology – I learnt about its reintroduction back into the wild as an undergraduate student more than 25 year ago. It is also a natural Carioca: a species that once lived within the boundaries of Rio de Janeiro.   The muriqui, while not a Caricoa, can be found in the mountains which form a backdrop to the city.  Ocelots are medium sized cats, which once lived in present-day Rio.  And hummingbirds are also Cariocas.  All species are threatened with extinction in the region. When you talk about Brazilian wildlife most people think of the Amazon. It may therefore come as a surprise to find out that the Amazon basin is not actually considered a true biodiversity hotspot.  These hotspots, 35 in total around the world, are defined by the large number of species they contain and the high degree of threat from human activities.  Brazil has two: the Cerrado (savannahs) and the Atlantic forest. The latter extends along much of Brazil’s south-eastern coast and once covered the whole of the city of Rio de Janeiro. It is ranked the fourth most important biodiversity hotspot in the world, but it is being chopped down faster than ever. Despite conservation biologists having their own preference for the mascot, everyone agreed these games are an opportunity to highlight the importance of the Atlantic forest to the world.  Biologists were thus dismayed to see two mascots who appear uncannily like Pokémon characters. This has caused some derision among Brazil’s conservationists. For the football World Cup held in Brazil earlier this year the organisers went with Fuleco, a three-banded armadillo – cuddly but endangered. However they didn’t back things up with sufficient conservation support or even make it too clear what Fuleco was supposed to represent. I had hoped that the Rio Olympics would not make the same mistakes, and it seems they haven’t. However they appear to have avoided this simply by inventing cartoon characters.  If a mascot is invented – if there is no “real animal” to preserve – then no-one can complain about lack of support for its conservation. To me this odd hybrid-creature smacks of trying to appease the biologists lobbying for real species.   It should be pointed out that while Pokémon was popular with children its premise as a game was about capturing, collecting and training wild creatures. Given that animal trafficking is a major problem in Brazil, the Pokémon-like mascots are particularly inappropriate. As a father of two small children whose life is plagued by cartoons on the TV I understand the need for these mascots to appeal to kids.  But actually the real species proposed were all very cute.  What I object to more is the unashamed anthropomorphising of the mascots and giving them super powers – the ability to stretch.  Children’s TV series such as Dora the Explorer and Go Diego Go were perfectly entertaining and educational without the lead characters needing to have abilities of the Fantastic Four. I hope that some of the lessons from Fuleco have been learnt, and that the organisers of the Rio Olympics will grasp the opportunity to promote the plight of the Atlantic Forest and its amazing wildlife."
"Like all great pledges, it is just the right side of implausible. The UK aviation sector this week committed to making flying “net zero” by 2050, wiping out its carbon emissions despite taking more than 100 million extra passengers into the air each year. At a celebratory event in London, the bosses of airports, airlines and aircraft manufacturers queued to scrawl their signatures on a giant Net Zero pledge card from the Sustainable Aviation [SA] campaign. Greenpeace described it as greenwash. But aviation figures insist the ambition is genuine. John Holland-Kaye, the Heathrow airport chief executive, says: “I imagine it’s like it is for alcoholics. The first step is to admit we have a problem – and then do something about it. I’m not sure what the 10 points on the AA [Alcoholics Anonymous] programme are, but you can find the equivalent in the SA roadmap.” The steps certainly involve a leap of faith, in future fuels and aircraft technology, and human capacity to endlessly offset emissions. Just how does a growing industry plan to reduce its annual footprint from just over 36m tonnes of CO2 to net zero in three decades’ time? Aircraft such as Boeing’s 787 or Airbus’s A350 already emit significantly less than the older jets they are replacing, through lighter materials and more efficient engines. Sustainable Aviation anticipates that emissions will drop by about 30% on routes operated by Boeing 747 jumbos as they are phased out within a decade. The next iteration of the manufacturers’ short-haul workhorses are supposed to be 10%-15% more efficient – demonstrated with the A320neo superseding A320s, although the disastrous introduction of the grounded Boeing 737 Max has delayed airline plans to replace older 737s. Altogether different types of propulsion will be in operation by the 2030s, manufacturers believe. But while new electric planes could revolutionise regional flights, and hybrid-electric could manage short-haul flying, their contribution to cutting UK emissions – about 65% of which hail from flights of more than 1,000 miles – will be comparatively minor by 2050, SA admits. “We’re not going to be electrifying a London-Singapore A380 for a long time, if ever,” says Paul Stein, chief technology officer of aircraft engine maker Rolls-Royce. “But sustainable fuels can work on the engines we have today, and the concept has been proved.” Creating the necessary volume is the tricky bit. Hopes have been pinned on new schemes, such as the new British Airways-backed plant to create jet fuel from household waste. Stein says Rolls-Royce has been exploring whether a small modular nuclear reactor could be used in a synthetic fuel plant, making hydrocarbons in a way that would reduce CO2 to balance the emissions produced in flight. “Right now we’re in analysis phase – understanding the economics … But what we’re going to see over the next 10 to 20 years is a lot of innovation and energy directed at creating sustainable aviation fuel.” Progress in coordinating European airspace and air traffic control has been slow. But with better use of airspace, and coordinated flight paths, planes could be guided more efficiently from take-off to landing. The UK is in the midst of redesigning its airspace to help create better paths. More efficient operations, such as eliminating stacking, would help aircraft carry and burn less fuel. Environmental groups question the adequacy of offsetting – but the roadmap relies on it to account for more than a third of aviation’s projected emissions. Jonathan Counsell, head of sustainability at British Airways’ owner IAG, says: “We think that we can get to net zero without it – but not by 2050. This is a transitional measure.” The scepticism about offsetting means, Counsell admits, that any schemes have to be high-quality, independently verified and end in actual, additional carbon removal. For now, offsetting means reforestation or restoration of peatland bogs that absorb CO2 from the atmosphere; later, potentially, investment in new carbon capture and storage technologies. In an industry run by accountants, some jiggery-pokery was perhaps inevitable: emissions are “saved” as the 70% rise in passengers by 2050 is, in fact, slightly lower than the Department of Transport’s growth forecast growth. “We will have to pay more to fly,” says Holland-Kaye. Sustainable fuels are more expensive, and the carbon price is expected to rise tenfold, he says. The price of all the offsetting that the industry will rely on will be passed on to consumers through higher fares, constraining demand. So can this work? The industry urgently wants to present an alternative vision to the growing backlash against flying. Stein says: “Flying connects the world – it is a force for good in transporting people and goods, making sure our cultures have great levels of understanding.” As an engineer, he says, he is optimistic that emissions can be brought down sooner than the roadmap pledge: “We have the engineering tools to do it; we need the will, and government help.”"
"
Share this...FacebookTwitterYou would think that with all the added wind and solar energy in Germany, along with all the conventional power plants on standby, all totaling up to huge unneeded capacity, there would be no need to import any power at all. Well, think again.

Photo: P. Gosselin
The German epochtimes.de here reports that German imports of electricity in fact: “rose by 43.3 percent to 25.7 billion kilowatt hours in the first half of 2020 compared with the first half of 2019.”
The epochtimes.de explains further:
One reason for this was the declining share of domestic feed-in from base-load-capable, mostly conventionally operated power plants, which mainly use coal, nuclear energy and natural gas. As a result, electricity was imported to cover the demand for electricity, especially when there was no wind or darkness. The main import country for electricity was France with 8.7 billion kilowatt hours.
Overall, however, more electricity was still exported from Germany.”
What the article does not mention, however, is the reason for the rise in export from Germany. On windy and sunshine-plenty days, Germany produces more electricity than needed, and so is forced to dump the excess power into neighboring foreign markets – often at negative prices. The negative prices, in combination with the mandatory feed-in tariffs and excess production capacity, all means higher costs for consumers.
Little wonder that at close to 35 US cents per kwh, Germany’s electricity prices are among the highest in the world.


		jQuery(document).ready(function(){
			jQuery('#dd_f32389d0d82421273a7d3eb7eaea0392').on('change', function() {
			  jQuery('#amount_f32389d0d82421273a7d3eb7eaea0392').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The President on Tuesday signed the continuing resolution that funds the government through September and (gasp) keeps the sequester cuts intact. Now that it appears sequestration isn’t going away (and yet the earth continues to spin merrily on its axis), the focus should be on how this small step might be extended.



Unfortunately, the reaction to Paul Ryan’s relatively modest budget indicates the fight for smaller government will continue to be an uphill battle in the current political climate. The Ryan budget has brought predictable condemnation from the political left. Sen. Harry Reid called it “extreme,” and the New York Times called it “the worst of the Ryan budgets.” The plan’s sin: restraining the growth of federal spending to 3.4 percent instead of 5 percent.





Serious policymakers need to start explaining to the American people how the federal government doing less will do more to enhance their personal and economic well‐​being.



While there are some of us that don’t feel the Ryan budget goes nearly far enough, it was never going to become law as is with a Democratic White House and Senate. But here’s the important question: does sequestration and Ryan’s follow‐​up give proponents of limited government a reason to be optimistic?



Currently, the answer is no.



Sequestration has yet to cause a public revolt and the markets have treated it with indifference throughout. Although the cuts that happened under sequestration are hardly an occasion for a victory lap, they are a small and welcome bit of evidence that government can spend less without society as we know it coming to an end.



Sequestration reduces federal spending by $44 billion this year, which is a relatively small sum considering that total spending will be around $3.5 trillion. The budget deficit alone is projected to be around $850 billion. That means to balance the budget this year, the spending cuts would have to be almost 20 times larger. However, sequestration barely scratches entitlement programs, which dominate the federal budget and are the source of our long‐​term fiscal problems. And because it doesn’t actually terminate any agencies or programs, spending can be restored in the future.



So in the big picture, sequestration hasn’t changed all that much. Federal spending is still on a dangerous upward trajectory. Unfortunately, while there is much talk about the need to reform the welfare state in order to make it more affordable, the underlying desirability of our centralized system of cradle‐​to‐​grave entitlement programs remains virtually unchallenged on Capitol Hill. And while the Pentagon’s bloated budget is being challenged, only a handful of policymakers are questioning the underlying desirability of the United State’s global military footprint.



The size and scope of the federal government needs to be dramatically reduced. Republicans are commonly understood to be in favor of limited government, but their track record suggests otherwise. Federal spending went through the roof under Republican rule in the previous decade. After reclaiming the House in 2010, Republicans positioned themselves as the frugal alternative to the debt‐​happy Obama administration. Unfortunately, the manner in which Republicans handled sequestration indicates that they are still unwilling or incapable of making a principled argument for smaller government.



Ever the defenders of the warfare state, Republicans bemoaned the sequestration cuts made to the Pentagon’s budget. Mirroring the administration’s orchestrated hysteria over cuts to domestic programs, some Republicans even claimed that the cuts would “gut” the military — a specious assertion considering that military spending under sequestration would be higher in real dollars than peak Cold War spending.



So if sequestration doesn’t do a whole lot to shrink the size and scope of government, what about Mr. Ryan’s proposal? In his budget, Ryan calls for ending Obamacare, but that wouldn’t end the federal government’s involvement in health care. Ryan says that higher education subsidies should be capped, but that wouldn’t end the federal government’s involvement in education. How the federal government delivers the goods might change, but a more efficient government isn’t the same as limited government. And if the goal is limited government, as Republicans often claim, then there has to be actual limits on what the government is involved in.



The tough reality is that the average voter is content to spend other people’s money on programs that they benefit from. And every government program is backed by a special interest that will fight tooth‐​and‐​nail to protect their share of Uncle Sam’s loot.



That’s an obviously difficult dynamic to overcome. But if progress is to be made, serious policymakers need to start explaining to the American people how the federal government doing less will do more to enhance their personal and economic well‐​being. That means making the case for limited government. Until that happens, the question of whether or not proponents of limited government should be optimistic will remain no.
"
"

Foreign policy has been a contentious issue for libertarians since September 11, 2001. There have been countless harangues in Washington bars and policy salons over the past five years about libertarianism and the Iraq War, and the topic has been so divisive for libertarians that even Rose and Milton Friedman disagreed. She was in favor and he against, with Rose noting later: “This is the first thing to come along in our lives, of the deep things, that we don’t agree on.We have disagreed on little things … but big issues, this is the first one!”



Why has the war — and post‐​9/​11 foreign policy generally — been so controversial for libertarians? And now, more than six years after 9/11 and more than five years into the war in Iraq, what can libertarian insights tell us about how we got here and what to do next?



To try to answer these questions, we should begin with some libertarian starting points about government and then review the debate over the Iraq war and foreign policy more generally in the wake of 9/11.



Then we can consider where to go from here, and what a counterterrorism policy that paid heed to libertarian insights would look like.



 **Government: Dangerous at Home, Beneficent Abroad?**



Nation‐​states are self‐​interested collective organizations, both at home and abroad. As public choice economists tell us, the first interests the state looks after are the state’s — not the people’s. Quite often, the state’s interests are served by war.



War historically has been the most effective generator of big government. As Bruce D. Porter observed in his book _War and the Rise of the State_ , the nonmilitary sectors of the federal government grew at a faster pace during World War II than they did under the New Deal. War creates the perfect climate for the collectivist mentality, as well as ready‐​made occasions and arguments for expanding the power of the national state.



In the international arena, it is important to note that security — the first‐​order concern of any state — is ultimately contingent on a state’s ability to defend itself. Decisions about national policies are based on how threatening a state views the international environment. Overall, security is scarce, and history tells us that states are competitive and leery of any state that grows too powerful and/​or throws its weight around. The concentration of military power in the hands of one actor in the international system can cause fear, particularly if that state appears intent on overturning the existing balance. It was for that reason that Thomas Jefferson wrote in 1815 of his desire that nations “which are overgrown may not advance beyond safe measures of power, [and] that a salutary balance may be ever maintained among nations.”



In recent years the United States has upset the world’s balance. Countries assess threats on the basis of capabilities and intentions, and the U.S. government at present appears to have enough of both to alarm other governments. Washington spends roughly as much on its military as does the rest of the world combined, and political leaders in both parties argue that we need a military significantly bigger. At the same time, in addition to the attack on Iraq, American leaders have begun to openly discuss their intentions of unraveling the international order. During a June 2007 speech to the Economic Club of New York, Secretary of State Condoleezza Rice argued that “America has always been, and will always be, not a status quo power, but a revolutionary power.” Thus we should not be surprised when we encounter fear and distrust from Berlin to Beijing.



What is most peculiar about this state of affairs is that the United States sits unchallenged atop the international order, with an unparalleled ability to shape it and with any potential peer competitor several decades away. This state of affairs is hugely beneficial to us; imperfect though it is, the United States should be working to _preserve_ , not overturn, the existing international order. But some observers, including a few libertarians, seem to have concluded that the threat from terrorism is so great that the United States must embark on radical social engineering projects abroad to combat it.



 **What Changed after 9/11 — and What Didn’t**



Despite the preeminent position of the United States in the international order, many American political leaders and thinkers — including some libertarians — embraced aggressively interventionist foreign policies after 9/11. The threat of international terrorism, primarily from al Qaeda, was broadened to include the nation‐​state of Iraq. President George W. Bush argued that an effective strategy for fighting terrorism must include regime change in Iraq in order to transform the social and political culture of the Middle East.



Most libertarians questioned those moves. Some embraced them.



Perhaps the most prominent libertarian to advance these ideas has been Randy Barnett, a nonresident senior fellow of the Cato Institute and professor of law at Georgetown University. Barnett published an op‐​ed in the _Wall Street Journal_ in July 2007 criticizing noninterventionist libertarians for failing to understand that “libertarian first principles … tell us little about what constitutes appropriate and effective self‐​defense after an attack.” He argued that libertarians can and should think of the attack against Iraq as appropriate self‐​defense in response to 9/11. Further, he argued that libertarians should favor “a strategy of fomenting democratic regimes in the Middle East.”



Such radical government programs could only be endorsed by a libertarian _in extremis_. But there was never reason to believe Iraq was either responsible for 9/11 or plotting the next one. The Iraqi government was not involved in 9/11, and attacking it devoted scarce resources to the wrong target. The appropriate response to the newly prominent threat of nonstate terrorism was to concern ourselves more with nonstate terrorist groups, which do not have return addresses and frequently cannot be deterred. To lump in states — whose relations with each other were largely unchanged by 9/11 — with such groups is to confuse different types of problems.



Barnett himself wrote in his 1998 book _The Structure of Liberty_ that libertarian conceptions of self defense are limited to _imminent_ attacks, a limitation that Barnett deemed “well‐​founded … because of the enormous knowledge problem that would be confronted if we were to permit selfdefense actions prior to a threat becoming imminent.” Barnett warned readers further that “every erroneous and unjust use of violence threatens to induce resentment, bitterness and the desire on the part of those against whom violence is used to rectify this injustice by responding violently, thereby setting off a cascade of violence.”



One could apply those insights to the war in Iraq. The U.S. government attacked Saddam’s regime in the absence of any imminent threat, and it seems that we indeed induced a significant amount of resentment, bitterness, and desire for vengeance by starting the war. (The debate over whether the intelligence supporting the case for war resulted from governmental incompetence or malfeasance — and neither explanation should confound a libertarian — is irrelevant.)



In his _Wall Street Journal_ article, Barnett admits supporting the war even though he believed that it would go poorly. He concedes that “to a libertarian, any effort at nation building seems to be just another form of central planning which, however well‐​motivated, is fraught with unintended consequences and the danger of blowback” and that he is “disappointed, though hardly shocked, that the war was so badly executed.” A critic of the decision to go to war might then ask why one should support a war you expect to go badly. And given that the objective of the war was a massive social engineering project unprecedented in scope — the destruction and reformation of a regional order — how could libertarians have envisioned it going any other way than poorly?



Indeed, how is it simultaneously possible to oppose government involvement in education or health care on the grounds of the inherent lack of necessary knowledge, but believe that the federal government could invade Iraq and then unravel and reweave the fabric of a thousands‐​year‐​old society whose language we do not speak and whose tribal and confessional allegiances we do not understand? Following the insights of thinkers such as F. A. Hayek, libertarians are deeply skeptical that governments could collect and sort enough data to plan government health care or education effectively. Surely those difficulties are compounded when the goals are even more ambitious and the policies are conducted in foreign countries wracked by sectarian conflicts.



The _Atlantic’s_ Matthew Yglesias observed the debate among libertarians over the war and judged that “the notion that anything even remotely resembling libertarianism could underwrite an effort to conscript huge quantities of resources from the American public and deploy them in an attempt to wholly remake the social and political order in a foreign country is too absurd to merit a rebuttal… . It’s coercion, it’s planning, it’s every non‐​libertarian thing under the sun.”



The policies that libertarian hawks have supported have cost more than half a trillion dollars and four thousand American lives — greater than cost of the 9/11 attacks themselves. (Libertarians also should not ignore the violations of individual rights that occurred in the form of the hundred thousand or so Iraqis who perished as a result of our political science experiment in their country.) Government power, unchecked by prudence or other constraints, can do great harm not only to foreign targets, but also to the very citizens that the government is charged with protecting. To craft an effective response to the terrorist threat, it is necessary to dispassionately assess the nature and scope of the threat.



 **Getting Threat Assessment and Response Right**



The very real problem of terrorism can be handled without massive nation‐​building projects in the Middle East. In fact, the biggest successes in fighting terrorism since 9/11 have been achieved through cooperation with foreign intelligence services and police agencies. Precious few meaningful victories against terrorism, by contrast, can be ascribed to the government’s tinkering with Iraq.



My colleague Benjamin Friedman observes that even in 2001, the flu killed more than 10 times as many Americans as did terrorism. Certainly past performance is no guarantee of future results, and one can conceive of improbable scenarios that would radically expand the destructive capacity of terrorists (their acquisition of a nuclear weapon, say). But to date, the government’s nation‐​building‐​as‐​counterterrorism approach has been more destructive and wasteful than terrorism itself and has done little to diminish the problem. In fact, there is ample evidence that terrorists realize that the best way to inflict harm on America is to trick us into responding in ways that harm ourselves.



Osama bin Laden boasted in 2004 that it is “easy for us to provoke and bait this administration.” Describing his desire to “bleed America to the point of bankruptcy,” bin Laden remarked, “All that we have to do is to send two mujahedeen to the furthest point east to raise a piece of cloth on which is written ‘al Qaeda,’ in order to make generals race there to cause America to suffer human, economic and political losses.”



Instead of allowing ourselves to be goaded into self‐​destructive responses, we should review our diagnosis, our prescription, and our prognosis. In pursuing an accurate diagnosis, we must confront a painful truth that study after study has revealed: U.S. foreign policy plays a significant role in public opinion in the Islamic world — and as a result, represents a big part of our terrorism problem. As a 2006 Government Accountability Office report noted, “U.S. foreign policy is the major root cause behind anti‐​American sentiments among Muslim populations and … this point needs to be better researched, absorbed, and acted upon by government officials.”



The Pentagon’s Defense Science Board was less diplomatic, writing in 2004 that “Muslims do not hate our freedom, but rather, they hate our policies.” Bin Laden himself argued in 2004 that “contrary to what Bush says and claims — that we hate freedom — let him tell us then, why did we not attack Sweden?”



Of course, not every terrorist is motivated by rage at U.S. foreign policy. There are clearly a small number of terrorists who carry out murders for other reasons. It should go without saying that the only viable policy approach toward committed terrorists — no matter their motivation — is to pursue them and capture or kill them in cooperation with foreign intelligence services and, in some cases, with the limited use of American military power. But our strategy should not be solely reactive. There are a vast number of people who may be receptive to bin Ladenism but aren’t yet convinced they should join him. And by far the most effective recruiting tool in al Qaeda’s arsenal is the notion — alarmingly widely accepted in the Muslim world — that America’s actions prove we are out to destroy Islam.



Accordingly, to treat the problem we need to focus more on the question of how we can better affect the marginal terrorist recruit. What makes him or her more or less likely to join the cause? Wouldn’t removing bin Laden’s best recruiting tool be helpful? The other side of the coin is that al Qaeda’s remarkable barbarity has been a public relations disaster in the Islamic world. Very few people — far fewer than support relatively liberal governance — express any desire to be governed by people like al Qaeda. Shibley Telhami, one of the leading pollsters of the Islamic world, testified to Congress in 2005 that al Qaeda’s support in the Arab world stems disproportionately from its opposition to U.S. foreign policy. Of the Arabs in Telhami’s poll expressing support for any of al Qaeda’s aims, only 6 percent supported the group’s objective of creating a Taliban‐​style state.



Al Qaeda can’t sell an affirmative agenda; what it can sell is opposition to U.S. foreign policy. A smart approach to counterterrorism would recognize that fact and avoid providing bin Laden and his comrades with opportunities to pose as the defenders of Islam against a hostile, colossal, anti‐​Islam United States.



Now for the prognosis. It is time to take a deep breath and recognize the strength of our system. Liberal capitalism is the best means for organizing human activity. It provides for the most flourishing, it provides for the most technological innovation, and it has the strength to endure through time. During the Cold War, alarmists warned constantly about the durability of the Soviet system, insisting that it was, in many ways, stronger than our own. They were proved fantastically wrong when the sclerotic Soviet state collapsed in a shambles in 1991. To respond to the band of fanatics we face today with hysteria does not befit a great nation of our size and vitality.



Hollow though it was, Soviet communism was a far more dangerous force than Islamic terrorism. The system that withstood the challenge of communism can similarly survive the threat from Islamic terrorists. As mentioned above, the style of governance that al Qaeda and its affiliates can offer to Muslims around the world is exceedingly unpopular. Earlier in the Bush administration, citizens of Arab countries held surprisingly favorable views of American freedom and the American people, although those figures have declined substantially. What becomes clear from the data, however, is the overwhelmingly negative view of U.S. foreign policy in the Islamic world. Putting our best face forward and emphasizing the positive features of the United States will go a long way to repairing our poor position in the world. As George F. Kennan wrote in his 1993 memoir, the United States must “never lose sight of the principle that the greatest service this country could render to the rest of the world would be to put its own house in order and to make of American civilization an example of decency, humanity and societal success from which others could derive whatever they might find useful to their own purposes.” 



We have lost sight of this principle. But in the months and years to come, we should refocus and take solace in the fact that certain important and basic truths remain unchanged. Our system is strong; bin Laden’s is weak. We are wealthy; al Qaeda is poor. We have greatly influenced the structure of the world order; they can only affect it by provoking reaction. The best thing to do now is to jealously guard our strength, not squander it; to keep and hold our quiet confidence, not panic; and to pursue this new breed of enemy with the prudence and wisdom of a mature nation.



The political scientist Hans Morgenthau wrote in _Politics among Nations_ that “throughout the nation’s history, the national destiny of the United States has been understood in antimilitaristic, libertarian terms.” This fact is linked to the rugged individualism of the American founding and the kernel of libertarianism that lies at the heart of the nation even today. Those who would jettison the antimilitarism would also jettison the libertarianism, compounding the tragedy.



Before his death in 2006, Milton Friedman lamented that his life’s project of limiting government power was “being greatly threatened, unfortunately, by this notion that the U.S. has a mission to promote democracy around the world,” pointing out: “War is a friend of the state… . In time of war, government will take powers and do things that it would not ordinarily do.” It is for precisely that reason that libertarians, more than anyone, should not be friends of war.



 _Justin Logan is associate director of foreign policy studies at the Cato Institute._
"
"“It makes sense” is the first thing to say about the phenomenon being described by psychologists as climate anxiety. Wherever in the world you live, there are very good reasons to feel anxious about the rate of global heating and the lack of adequate action to tackle it by governments, businesses and organisations of all sorts. The predicted consequences are frightening: hotter weather in already inhospitable places, sea-level rises caused by melting ice sheets, and increased disruption of weather systems leading to floods, fires, hurricanes, food and water shortages – with the linked biodiversity crisis another cause for grave concern. Depending on the steps that are taken (or not) over the next decade, a period during which the UN estimates that carbon emissions need to be cut by 7.6% annually if we are to avoid temperature rises above 1.5C, the disruption caused to human societies could be immense. For countries such as Bangladesh, the effects are likely to be devastating.  Given all this, it arguably makes more sense to be anxious than not. And climate anxiety is one way of describing the motivations of every person or organisation that is trying to do something to limit or to mitigate the effects of global heating – whether an individual altering their diet, a charity switching energy supplier, a council setting emissions targets or the Guardian deciding to stop selling advertising space to fossil fuel companies. But, as with all negative emotions, the trick is to distinguish ordinary feelings – what Sigmund Freud famously called “common unhappiness” – from those that are disproportionate, or so intense and prolonged as to be debilitating. While it makes sense to be worried about the climate emergency, becoming overwhelmed is counterproductive. The sound advice from psychologists that actions, however small, can help to alleviate feelings of distress and powerlessness echoes the experiences of activists including Jane Fonda that “the minute you start doing something, the depression goes away”. Not all low moods are readily lifted, however, and warnings of worsening mental health as a result of climate disruptions and hardships should be taken seriously. Already there is cause for concern, with research showing that people who have experienced extreme weather such as floods in the UK are 50% more likely to suffer from problems including depression. Resilience may be a desirable quality, but is much more easily developed by those who are cushioned by income or advantage. Growing demand for psychological support should be met by professionals who are able to distinguish everyday worries from post-traumatic stress or other symptoms. Disasters on the scale of Australia’s recent bush fires and Indonesia’s floods can be expected to produce severe mental as well as physical reactions, particularly in children and other vulnerable groups. In some parts of the world, trauma is already normalised, and when psychologists write of their fear that it could become ubiquitous, policymakers everywhere should take notice. But it’s important to remember that there are reasons to hope, as well as despair. As the environmental scientist Vaclav Smil said last year, “We [humans] are stupid, we are negligent, we are tardy. But on the other hand, we are adaptable, we are smart and even as things are falling apart, we are trying to stitch them together”."
"

A friend from my coffee group sent this about recognizing the signs of a stroke and encouraged me to post it and spread the word. I checked it out to make sure it was not another Internet hoax and I’m happy to report it is valid.
If everyone can remember this simple STR procedure, lives could be saved.
Some background –
During a BBQ, a friend stumbled and took a little fall – she assured everyone that she was fine (they offered to call paramedics) …..she said she had just tripped over a brick because of her new shoes.
They got her cleaned up and got her a new plate of food. While she appeared a bit shaken up, Ingrid went about enjoying herself the rest of the evening.
Ingrid’s husband called later telling everyone that his wife had been  taken to the hospital – (at 6:00 pm Ingrid passed away.) She had suffered a  stroke at the BBQ. Had they known how to identify the signs of a stroke, perhaps Ingrid would be with us today. Some don’t die…. they end up in a  helpless, hopeless condition instead.
A neurologist says that if he can get to a stroke victim within 3 hours he can totally reverse the effects of a stroke… totally . He said the trick was getting a stroke recognized, diagnosed, and then getting the patient medically cared for within 3 hours, which is tough.
RECOGNIZING A STROKE
Remember these ‘3’ steps:  STR. It’s the first three letters of the word STRoke.
Sometimes symptoms of a stroke are difficult to identify. Unfortunately, the lack of situational awareness spells disaster. The stroke victim may suffer severe  brain damage when people nearby fail to recognize the symptoms of a stroke .
Now doctors say a bystander can recognize a stroke by asking three simple
questions:
S * Ask the individual to SMILE.
T * Ask the person to TALK and SPEAK A SIMPLE SENTENCE (Coherently)
       (i.e. It is sunny out today)
R * Ask him or her to RAISE BOTH ARMS.
If he or she has trouble with ANY ONE of these tasks, call 999/911 immediately and describe the symptoms to the dispatcher.
See References: American Stroke Foundation, Stroke Awareness.org
New Sign of a Stroke ——– Stick out Your Tongue
Ask the person to ‘stick’ out his tongue.. If the tongue is ‘crooked’, if it goes to one side or the other , that is also an indication of a stroke.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1962e56',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This spring the Pew Research Center released its eighth annual report on the state of American journalism. “In some ways, new media and old, slowly and sometimes grudgingly, are coming to resemble each other,” the study says. The traditional platforms of the Fourth Estate are changing, and last year, online news consumption outstripped print media for the first time in terms of both advertising revenue and readership. The tipping point has arrived. The trend line is clear. And the Cato Institute, it seems, has been ahead of the curve.



Since 2005 _Cato Unbound_ has given readers access to a state‐​of‐​the‐​art virtual trading floor in the intellectual marketplace. A unique online magazine, it reflects an appreciation of the way ideas are exchanged in the digital age. Every month one of the world’s leading thinkers presents an essay on a topical issue. A panel of distinguished experts responds, each offering his case before challenging and refining the arguments in an ongoing conversation. Readers are then encouraged to join the dialogue by offering their own thoughts through websites, blogs, and letters to the editor. These contributions are pulled together into an easily accessible forum, creating a media product that is virtually distinct within the digital realm.



Yet _Cato Unbound_ is also designed to avoid the pitfalls of its platform. For starters, the site revolves around the value of debate. All too often, the sheer availability of personalized news today allows readers to give in to confirmation bias — to seek out only the information that reinforces their existing beliefs. The internet, by any measure, caters to the obstinate. At _Cato Unbound_ , however, contributors are forced to confront their critics, and the tendency to selectively ignore the opposition is mitigated.



The site also hinges on the importance of perspective. The current news climate is subject to certain kinds of pressure: readers increasingly look for minute‐​by‐​minute updates. Many sites therefore suffer from a lack of depth by becoming preoccupied with instantaneous delivery. _Cato Unbound_ is different. “We try to step back, take a deep breath, and focus on the larger picture,” Jason Kuznicki, the site’s editor, explains.



In the latest issue, “Targeted Killing and the Rule of Law,” the editors ask whether the executive branch can lawfully kill. Lead essayist Ryan Alford, assistant professor at the Ave Maria School of Law, argues that it cannot. In fact, the “presidential death warrant” is so repugnant to our constitutional tradition, he says, that the Founders didn’t even think it necessary to make an explicit statement about the practice. At the time of the Revolution, British kings hadn’t enjoyed such a power for centuries, and it was thought to be the very antithesis of the rule of law. The distinguished panel of legal and historical experts responding to Alford includes John C. Dehn of the U.S. Military Academy at West Point, Gregory McNeal of Pepperdine University, and Carlton Larson of the University of California at Davis.



Other past issues have included



These monthly conversations have received attention from publications like the _New York Times_ , the _Washington Post_ , and _The Economist_. The site has featured a lineup of prominent contributors, including James M. Buchanan, the Nobel laureate and founder of the public choice school of political economy; Richard H. Thaler, professor of economics and behavioral science at the University of Chicago; James R. Flynn, a pioneer in the study of IQ; Clay Shirky, the renowned social media theorist; and Jorge Castañeda, former foreign minister of Mexico. Over the years, this forum has shown a depth of exchange and an accessible format that few other outlets offer.



An idea can be bound between covers, bound by convention, or bound for the dustbin of history. The ideas of _Cato Unbound_ , we hope, are none of the above.
"
"Australia should legislate a target of net zero emissions by 2050, the chief executive of the Business Council of Australia has said. Appearing on Monday night’s Q&A panel, Jennifer Westacott told the audience “we have to do net zero by 2050”.  Asked if that meant Australia should follow other parliaments such as the United Kingdom and legislate for net zero she said, “I think that would be a start.” “I reckon if we could get the two political parties to agree to that and legislate it, we would have made a massive advance in this country because we would know where we’re going,” Westacott said. “For business that does want to take action in this space, that would at least give us a kind of certainty about where are we heading.” Monday’s panel featured no politicians and was focused on how Australia should transition to a carbon neutral economy. Audience members who asked questions included coalminers from areas including Victoria’s Latrobe Valley. Westacott’s comments come as the independent MP Zali Steggall and crossbench MPs push for bipartisan support for a climate change framework bill aimed at helping the transition to a decarbonised economy. It includes a proposal for a net zero emissions target by 2050, a carbon emissions budget, and assessments every five years of national climate change risk. Steggall is calling for government MPs to be given a conscience vote on the bill, which would only succeed if this was granted. The bill will be introduced as a private member’s bill next month. Westacott told Monday night’s panel what Steggall had proposed was “sensible”. “Make no mistake – and she acknowledges this – the how really matters. We have to get the how right because we’ve got to create those new jobs,” she said. The BCA, which represents Australia’s largest companies, is also saying Australia, if possible, should meet its 2030 emissions targets without relying on carryover credits from the Kyoto period. And it repeated its call for the reintroduction of a carbon price to “drive the transition and incentivise investment in low and no-emissions technology”. It is included in a scoping paper seeking views from members, which include Australia’s major banks and mining companies such as BHP and Rio Tinto, as the group reviews and updates its energy and climate policies. The scoping paper says delivering net zero emissions at the lowest economic and social cost will require emissions cuts across all parts of the economy, including not just electricity, but also industrial processes, mining, transport and agriculture. The statement that Australia should effectively stop releasing carbon pollution within three decades comes as debate over climate action is escalating inside the Morrison government. Moderate Liberals declared on Monday the government should not underwrite a new coal-fired power station, and the trade minister, Simon Birmingham, acknowledged that, by signing the Paris agreement, the Coalition had already agreed to a long-term goal of global net zero emissions. It followed the Coalition announcing a $4m feasibility study into a proposed coal-fired power plant at Collinsville, in north Queensland. Last year the business council joined other groups in representing industry, unions, farmers and investors under the Australian Climate Roundtable banner in calling for policies that could put it on a path to net zero emissions. The group has previously been accused of standing in the way of serious proposals to address climate change. In 2018, it described Labor’s promised 2030 target of a 45% cut in emissions below 2005 levels as “economy wrecking”. The paper says the business council has supported strong action for “over a decade”. It says it supports the Paris agreement, a transition to net-zero emissions by 2050, a market-based carbon price and using technology to drive the change and create jobs and industries that “maintain Australia’s competitiveness”. On the government’s plan to use credits from the Kyoto protocol to meet its target under the Paris agreement, the business council said: “If we can meet our emissions reduction targets without carryover credits then we should.” On Monday night, Westacott said a net zero target had been the BCA’s policy for “a long time” but repeatedly stressed that how Australia got there was what mattered. “The how really matters,” she said. “It matters for regions and we have a responsibility to step out how that will actually occur. “Because just saying stuff and then not being able to show how we’re going to do it over what period of time and what cost and what technology and policy settings, what incentives, that will be another set of empty words that Australians will become cynical about.”"
"Like so many other wine country towns dependent on tourism and out-of-town visitors, the California resort community of Guerneville typically experiences a winter downturn. Business owners know to prepare for it. Restaurant owners scale back seasonal staff. Hotels offer discounted rates.  But this winter, no one was prepared for the aftershocks of the second major wildfire to hit Sonoma county in three seasons – one that prompted widespread mandatory evacuations and panic. No one was prepared for the after-effects of weeks of power shutoffs, a preventative measure during high fire weather. Business owners still reeling from the cancelled bookings and loss of business during peak tourism season in the fall are now struggling more than ever to get through the winter. “There just are not enough people coming into town,” said Larry Boeger, the owner of the Timberline at the River restaurant. “We’re closed during the week. We stopped offering brunch. Last winter, we were open every day.” In a region ruled by an industry where image means everything, Guerneville is experiencing an unforeseen economic impact of climate change. The Kincade fire, which burned more than 77,700 acres in Sonoma county in 2019, was miles away from the community of 4,500 along the Russian River, but officials evacuated the region as a precaution, fearing a repeat of the 2017 Tubbs fire that killed 22 people. The lush green hills and pastural vineyards that define the area remained largely untouched, but some fear the deluge of news drove away prospective visitors who pictured a barren wasteland. “It was a different scenario, but the fact that it was another big fire in Sonoma county, all the imagery of 2017 came to light in people’s heads,” said Claudia Vecchio, the president of Sonoma County Tourism. “Between the level of evacuation, which was widespread, and this ‘once again’ kind of mentality, it’s almost a more difficult perception issue than we had back in 2017.” Data on the number of visitors these past few years have been skewed because evacuations and home destruction have affected local hotel stays. But months after the Kincade fire, bookings are down in Guerneville. Boeger and other business owners are seeking out loans to stay afloat through the winter. “I’m looking out the window right now and there’s nothing but empty parking places,” Boeger said. “It scares people to think that their favorite vacation place isn’t safe any more,” said Megan Perkins, a manager with Russian River Vacation Homes. “They Google the area that they’re coming to visit or coming to book, they’re finding the articles about all the things that have gone wrong and how scary things have been. I think that’s impacting the amount of bookings we’re seeing.” The economic impact of the wildfires has come in waves. In preparing for leaner winter months, Sonoma county business owners know they need to build up a nest egg during the last hurrah of the tourism season, the fall harvest. During this time, Pacific Gas and Electric (PG&E) began cutting the power to large swaths of the state in massive power shutoffs that officials have described as California’s “new normal”. The unpredictability of that time period led to a lot of preemptive cancellations. Boeger lost two wedding parties, which would have brought him up to $12,000 a night. “PG&E planned to turn off the electricity the week before Thanksgiving, so we had Thanksgiving reservations canceling, we had Christmas parties canceling,” Boeger said. “People were calling to say, ‘I’m sorry, if I can’t count on doing my daughter’s wedding dinner here, then we’re going to reschedule somewhere else’.” Perkins’ company realized it was too unsafe to have guests staying in rentals without power if there was a chance of wildfire danger. Then once the fire hit and the mandatory evacuation order came down, they had to get everyone out. “We also personally, everybody who works here, had to evacuate,” she said. “We had to help all our guests get out, cancel anyone who was coming in, and this was usually a very lucrative time because fall is gorgeous in this area. We weren’t available to take bookings for the rest of the year and into the beginning of this year.” Elsewhere in Sonoma county, some felt that the slow season was a bad time to take the temperature of the wildfire’s economic toll. David Howard, the owner of Howling Wine Tours in Healdsburg, says he’s received a fair number of bookings for wedding parties in the critical on-season of summer and fall, and that the winter has not been as slow for him as those in Guerneville.  But after three years of losing business during the peak month for wine tours – in 2018, the smoke from the Camp fire in Paradise choked much of the Bay Area and kept everybody indoors – the problems are still there. “We’ve had three years of crushingly bad numbers,” Howard said. “It’s hit everybody in our business.” Tourism is a $2bn industry in the county, according to Sonoma County Tourism, with one in 10 jobs in hospitality. To help protect this, the organization is working on a new marketing push to encourage long-distance travelers and wedding planners to visit in the spring and early summer – a time when wildfire risk is low and power shutoffs unlikely. “People are wondering if Sonoma is reliable place to visit,” Vecchio said. “We’re telling people to choose to come to Sonoma county, but choose to come when there isn’t that sort of uncertainty.” In the meantime, Guerneville holds out hope in making it through this lean winter. “We’re continuing to message our repeat guests and people who have stayed out here before that it is safe, it’s fun, come stay with us again,” Perkins said."
"
Veteran Meteorologist Joe Bastardi of AccuWeather on Al Gore’s 60 minutes interview:
I am absolutely astounded that someone who refuses to publicly debate anyone on this matter and has no training in the field narrated a movie where frames of nuclear explosions were interspersed in a subliminal way in scenes of droughts and flood, among other major gaffes, can say these things and then have them accepted… by anyone.
See the complete writeup here on the AccuWeather Blog
If you wish to write letters to CBS New regarding the issue, see my post on the same subject here.
(h/t Jim Arndt)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea000efca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

One of the classic examples of the failure of politicians to communicate with the citizenry is found in a video of Romanian tyrant Nicolae Ceausescu, giving what turned out to be his last speech to the teeming masses gathered in a square in Bucharest.



Oblivious to the mood of the people, Ceausescu is at his bombastic, self‐​important best until he realizes that the chants from the crowd below are not praise, but something rather to the contrary. The look on his face is, as they say in the MasterCard commercials, priceless.



America is a democratic republic, complete with an excellent Constitution that politicians still feel compelled to acknowledge, if not take seriously. So, the growing communications gap between the average American and the average politician, while worrisome, is not irreparable. Solving it should be a high priority for all involved.



The communication problem involves the accelerating realization on the part of many Americans that the essence of America, namely, a respect for the dignity of the individual, which inherently involves the government leaving the individual alone, has been pretty much forgotten by the politicians in Washington, D.C., the state capitals, and city councils around the nation. Which explains why public employees now make on average 30 percent more than their private sector counterparts — and 70 percent more in benefits. The political class seems to believe they have carte blanche to do as they please. They turn a deaf ear to increasingly vocal expressions of frustration by the American people.



Take, for example, a town hall meeting in Washington state last summer in which a young Marine veteran said to six‐​term Rep. Brian Baird, “Now I heard you say tonight about educating our children, indoctrinating our children, whatever you want to call it.” The congressman denied wanting to indoctrinate, but the young father simply responded, “Stay away from my kids.” Virtually all of the 400 or so people in the hall rose as one in loud applause. It was a Ceasusescu moment. The congressman had no clue the people of his district weren’t interested in the federal government concerning itself with the education of their children.



The politicians simply do not get it. The Declaration of Independence says governments are created to secure our rights to life, liberty, and the pursuit of happiness. In other words, to leave us the hell alone. That is what makes for American exceptionalism, despite President Obama’s claim that all nations are exceptional. No, they are not, not in the way America is.



As I write these words, across my desk comes a press release from Bloomberg telling me that 18- term Rep. Henry Waxman wants Congress to ban the use of smokeless tobacco in Major League Baseball dugouts. This is part of our communications problem. Read my lips, Henry: It is none of Congress’s business if baseball players want to use smokeless tobacco (or any other kind of weed, for that matter). And this is the encouraging thing about the Tea Party movement. It is made up of average Americans who are sick to death of politicians regulating, taxing, controlling, and limiting individual choice.



This bipartisan communications problem is also exemplified by a joint press conference held just before the start of the lamentable 111th Congress by Senate Majority Leader Harry Reid and Minority Leader Mitch McConnell. Said Reid, “Sen. McConnell and I believe … that we are going to work in a bipartisan basis … to solve the problems of the American people.” Whoa! See how simple the communications problem is? They think we sent them to Congress to solve our problems when we actually sent them there to see to it that we are left alone to solve our own problems.



Add to that the fact that many, if not most, of our problems have been created by Congress in the first place and we have the basis for a healthy peaceful revolution. Some 85 percent of Americans like their healthcare, so Congress shoves a government‐​mandated system down our throats. Taxes are way too burdensome, so Congress is contemplating a valueadded tax to add to our burden. We spend billions of dollars on wars in the Middle East for no rational reason. Climate change proves to be a wildly exaggerated issue, yet Congress still plans on raising taxes on energy to solve this non‐​existent problem. The list is long, and the frustration grows daily. Talk about a failure to communicate. According to a recent Pew Research Center Poll, 78 percent of Americans don’t trust the federal government. As Ronald Reagan famously put it, “The nine most terrifying words in the English language are: ‘I’m from the government and I’m here to help.’ ”
"
"Labor’s deputy leader has not ruled out the party supporting new coal developments, saying it would be a decision for the markets despite previously declaring it would be a “good thing” if the thermal coal market collapsed. In a somewhat difficult and at times awkward interview with David Speers on the ABC’s Insiders program, Richard Marles struggled to articulate the difference between Labor’s coal policy and the Coalition’s, stating “coal will continue to play a part within our economy for decades to come”, while demanding the government do more on climate policy.  Marles maintained public funds should not be used to subsidise coal and the market should be allowed to make its own decisions, but if a private company decided to push forward with a mine and gained the necessary approvals Marles said Labor would not stand in its way. “This is a matter for the market,” Marles said on the question of whether Labor proposed new coal projects. “… And the normal environmental approvals should apply. “… The normal process should play out … a Labor government is not going to put a cent into subsidising coal-fired power. And that is the practical question as to whether or not it happens.” Although he has maintained no taxpayer funds should be used to prop up the industry, a year ago Marles was more strident about the future of coal. “The global market for thermal coal has collapsed, and at one level that’s a good thing because what that implies is the world is acting in relation to climate change,” he told Sky News in February 2019. The Morrison government is attempting to balance competing forces within its party room by examining a proposal for a new coal-fired power station in north Queensland – a politically turbulent region for both major parties – while maintaining it is serious about reducing emissions. The government announced on Saturday a $4m feasibility study with Shine Energy into the proposed Collinsville coal-fired power plant. In an interview with Sky News, Dan Tehan attempted to suggest the study wasn’t about the government looking to support a new coal-fired plant, but about “providing cleaner, more efficient, lower emissions technology to provide base load funding into northern Queensland”. The move has raised eyebrows given the private sector’s hesitancy to move forward on coal-fired plants, despite pressure from conservative MPs. The recent Nationals leadership wobbles, which led to the Queensland senator and resources champion Matt Canavan’s return to the backbench, has seen those same MPs begin to agitate within the joint party room for the government to do more to support the coal industry, claiming climate change was not an issue in their electorates. ""Coal will continue to play a part within our economy for decades to come,"" @RichardMarlesMP tells @David_Speers on #Insiders #auspol pic.twitter.com/qnTyKCEdNA Following the May election loss, and the party’s backwards slide in Queensland, Labor has attempted to walk a fine line between its support for coal regions, led by the Hunter Valley MP Joel Fitzgibbon, and its push to do more on climate policy. Yes Adam, come to our regions and explain to our coal miners why you believe that, despite growing demand for their relatively efficient & modernising product in Asia, we can’t have both a cleaner Oz economy & a strong coal export industry @InsidersABC @GuardianAus #jobs pic.twitter.com/wcvPCKJx2F Asked to differentiate between Labor’s policy and what the government was doing on Sunday morning, Marles struggled to draw a distinction, instead returning to what he said were the government’s failures. Asked what the government could be doing and wasn’t, Marles said “you could start by having a proper energy policy”, but pressed on what that should include Marles said it was “a matter for them – they’ve been in power for seven years”. Marles settled on the need for a joint parliamentary approach to climate policy, and said the Labor party would once again look at supporting the national energy guarantee, a proposal that ultimately led to Malcolm Turnbull’s downfall as Liberal party leader. “We have been seeking bipartisanship for a long time in relation to this,” Marles said. “But to get bipartisanship, we actually need to have a side that we can talk to. Right now, we’re watching a whole lot of people having a war with each other inside their party room.”"
"New York is one of many cities whose mythical allure claims that the streets are paved with gold. Sadly, you are more likely to be treading on – or at least wading through – the remains of burgers, hot dogs, sweets, cookies, fries and more unmentionable sources of nutrients. Yet in among all that detritus is an awful lot of energy, a resource that could underpin a complex ecosystem. Food webs are a staple of ecology research, but usually explored in rain forests and coral reefs, ponds and savannahs. However a team at North Carolina State University has recently turned its attention to the much more dangerous terrain of Manhattan to find out if the insects living on and under the streets clear up a significant amount of the food litter – and whether the diversity of species makes any difference. Their results are published in the journal Global Change Biology. The precise relationship between resources and the diversity of flora and fauna in ecosystems has been the subject of intense research ever since the coming of the word “biodiversity” in the late 1980s. Ecologists were challenged to explain the role of species: does it matter how many there are, does the number of species affect the way ecosystems work, what do all these species do for us?  All this activity drives the natural ecosystems which keep us alive. Ecosystems are more productive, efficient and resilient the more species they contain, perhaps because different species carry out complementary roles or, however unwittingly, benefit the activities of others.  The North Carolina team set out to test whether the diversity of invertebrate street life affected the removal of food that had suffered “improper disposal” (a charming politeness runs through the whole study) in the parks and elongated traffic islands of Manhattan. To audit the pavement biodiversity, the team collected insects from among the leaf litter, with additional forays into other areas in search of ants. The rate of food clear-up was measured by putting out potato chips, cookies and hot dogs and seeing how much was left the following day.  Some of the food was protected by wire mesh, others not – so that larger creatures such as rats and pigeons could get in too, to allow for their impact. The precise brands of crisp, cookie and hot dog are detailed, each cut up into more appetising chunks.  This is important, allowing experimental replication with street food around the world. For example in Britain the late-night kebab might be a significant bio-geographical variation. The rate of food clear-up was compared to the overall diversity of invertebrates and the precise mix of species. Sadly, nowhere do the team members outline how they explained any of their activity to passing policemen. The speed with which food was removed proved startling. In the first run of the experiment using small chunks of food, 59% was gone within 24 hours. A second run using larger portions resulted in a 32% loss within a day. Whole cookies and chips … gone, chunks of hot dog … vanished.  The insect life on the traffic islands consumed supplies two to three times faster than the inhabitants of the parks. Life in the fast lane perhaps, or maybe the park life was more used to ice creams and sandwiches. In either locality, hot dogs were preferred to the light snacks.  In total the insects from the medians and traffic islands of two long Manhattan streets – Broadway and West St – could remove the equivalent of 600,000 potato chips per year. This could become a standard measure of invertebrate junk food ecosystem services.   The overall conclusion is that our invertebrate neighbours in the city make a notable contribution to the removal of litter. However the food clear-up was not affected by the diversity of species. More important was the presence of one species of ant, the perfectly named pavement ant, Tetramorium caespitum. Two to three times more food was removed where these particular ants were present. There is something particularly pleasing about it being the pavement ant. Not just the name; this ant is not a native New Yorker, but an immigrant from over a century ago, probably coming from Europe to the Big Apple.  Urban wildlife is often rather overlooked as a sorry mix of second-rate left-over habitats and dodgy aliens. However the city represents a whole new habitat, likely to become ever more widespread; a zoopolis, with a distinct and fascinating ecology. Where the streets are paved with last night’s food, these ants have certainly found their niche."
nan
"There are five trillion pieces of plastic in the world’s oceans, weighing a total of 268,000 tonnes. That’s according to a paper  by an international team of scientists who took a substantial amount of data collected across the globe and merged it with ocean circulation models to come up with the staggering figure.  That sounds like a lot of plastic – it is a lot of plastic – but it isn’t distributed quite as most people would imagine. First of all, the way a Pacific Garbage Patch has been reported by some newspapers is largely a myth. No, you can’t see the garbage patch from space – the “garbage” in question is usually many small particles. We are told how these plastic particles now outnumber the tiny creatures at the bottom of the oceanic food web, and it all needs to be cleaned up as a priority. It is true that in some areas plastics do exceed plankton.  But this generally happens in areas with very few nutrients (oligotrophic) and hence low productivity, and which sit at the centre of large currents which focus debris and create these “patches”. The plastics in the ocean can remain as recognisable objects – fishing nets, plastic bottles, bags – but much of it mechanically breaks down to small particles, many a fraction of a millimetre across, which explains the large numbers involved. Though previous studies found plastic isn’t restricted to ocean gyres, the latest paper is one of the first to bring the research together to produce a global estimate. The major gyres of the oceans are found in the north and south Atlantic, the north and south Pacific, and in the Indian Ocean. These vast rotating vortices trap surface material at their centres, concentrating any debris.  However, the gyres aren’t closed off and, in the several years it takes a piece of plastic to complete a circuit, there is a 20-30% chance of it spinning out into other parts of the ocean. This results in the increasing levels of plastic beyond the gyres.   In 2010 I undertook some research in the Arctic Ocean, north of the Svalbard archipelago, the northernmost permanently inhabited place on earth.  As we were interested in monitoring plastics we went equipped with plankton nets to sample the region’s sea.   We didn’t need the nets. On a remote island, more than 1,000 km away from any moderately sized village or town we collected a large box full of plastic debris from one 10 square metre section of beach, which included the box itself. Our finds included plastic bags and bottles from the US, Canada, the UK, France, the Netherlands and Norway.  The ocean currents had transported the debris thousands of miles, and even managed to wrap several bags around the tusks of resident walruses on the island. This is a global problem and the researchers in the new study consider that there are many thousands of tonnes of fine particles unaccounted for which have either entered the food chain or reside in the ocean sediments.  The figure of 268,940 tonnes is quite low to my mind and can easily be put into perspective: if the material was distributed evenly across the ocean then it equates to three particles in every million tonnes of seawater.  The reality is that the plastic is concentrated in the surface layers (or the sea bed sediments) and is globally on the increase, by definition – it is not breaking down and it is still entering the environment.  More importantly there is nothing we can do with what is already there.   I am often asked why don’t we just filter it out of the oceans?  The figures above should tell you why – it is an impossible task, and even if we could we’d take out all of the plankton as well.  All we can do for now is reduce, recycle and monitor. The big question, which still remains largely unanswered, is: what happens to this material once it enters the food chain?"
"
Share this...FacebookTwitterExpect this new study to be greeted by an angry mob with pitch forks and torches. Results Big Pharma, Bill Gates and social engineering technocrats don’t want to see. 
So picture this: tens of thousands of scientists, doctors and health authorities worldwide spending billions and billions in a frenzied search for new medicines and vaccines – while imposing economy-crippling lock downs – all to combat the COVID 19 virus. Meanwhile, a large part of the solution is likely just sitting right there on the supermarket shelf – to be had for just a few bucks!
Use a mouthwash, stupid!
That may be just the case, believe it or not, according a a recently published study appearing in the British journal FUNCTION titled: “Potential Role of Oral Rinses Targeting the Viral Lipid Envelope in SARS-CoV-2 Infection“.
It may be that simply gargling regularly in fact goes a long way in combating the spread of COVID-19, and letting us do away with the face mask circus we’ve been going through lately.
Promising results

A team of scientists led by Valerie B O’Donnell reviewed known mechanisms of viral lipid membrane disruption by widely available dental mouthwash components that include ethanol, chlorhexidine, cetylpyridinium chloride, hydrogen peroxide, and povidone-iodine and assessed their potential ability to disrupt the SARS-CoV-2 lipid envelope, based on their concentrations.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




 

Figure 1: Breaching the viral envelope. Source: O’Donnell et al 2020. 
Direct evidence: “Potential way to reduce transmission”
The preliminary results are promising enough to warrant serious further investigation, the authors say. Moreover, citing already published research on other enveloped viruses, they conclude that several deserve clinical evaluation.
These studies “directly support the idea that oral rinsing should be considered as a potential way to reduce transmission of SARS-CoV-2,” the authors say.
Mixing science and politics
If this turns out to be so, there are going to be lots of “experts” out there looking awfully silly. But so it is so often when social engineering politics get mixed with science.

Share this...FacebookTwitter "
nan
"

9/29/07 UPDATE: We are still waiting on Mr. Steve Bloom to answer this question: “Why is positive bias imparted in USHCN adjustments?”
He incorrectly asserts that he has been “banned” from this blog. Not true. Once he answers this question, that answer along with whatever else he has to say after that will be posted here. Otherwise we’ll continue to wait.
What say you, Mr. Bloom?
——————————————-
Given what NASA GISS has recently done with posting a change to the data methodology on the heels of an error which was embarrasing to them, (see Raising Walhalla)  I think this review of a relevant paper might bear some examination:
An Introduced Warming Bias in the USHCN Temperature Database Reference
Balling Jr., R.C. and Idso, C.D. 2002. Analysis of adjustments to the United States Historical Climatology Network (USHCN) temperature database. Geophysical Research Letters 10.1029/2002GL014825.
Abstract http://www.agu.org/pubs/crossref/2002/2002GL014825.shtml and the full paper Download file
What was done:
The authors examined and compared trends among six different temperature databases for the coterminous United States over the period 1930-2000 and/or 1979-2000.
What was learned:
For the period 1930-2000, the RAW or unadjusted USHCN time series revealed a linear cooling of 0.05°C per decade that is statistically significant at the 0.05 level of confidence. The FILNET USHCN time series, on the other hand – which contains adjustments to the RAW dataset designed to deal with biases believed to be introduced by variations in time of observation, the changeover to the new Maximum/Minimum Temperature System (MMTS), station history (including other types of instrument adjustments) and an interpolation scheme for estimating missing data from nearby highly-correlated station records – exhibited an insignificant warming of 0.01°C per decade.
Most interestingly, the difference between the two trends (FILNET-RAW) shows “a nearly monotonic, and highly statistically significant, increase of over 0.05°C per decade.” With respect to the 1979-2000 period, the authors say that “even at this relatively short time scale, the difference between the RAW and FILNET trends is highly significant (0.0001 level of confidence).” Over both time periods, they also find that “the trends in the unadjusted temperature records [RAW] are not different from the trends of the independent satellite-based lower-tropospheric temperature record or from the trend of the balloon-based near-surface measurements.”
What it means:
In the words of the authors, the adjustments that are being made to the raw USHCN temperature data “are producing a statistically significant, but spurious, warming trend in the USHCN temperature database.” In fact, they note that “the adjustments to the RAW record result in a significant warming signal in the record that approximates the widely-publicized 0.50°C increase in global temperatures over the past century.” It would thus appear that in this particular case of “data-doctoring,” the cure is worse than the disease. In fact, it would appear that the cure IS the disease.
From the paper: Our analyses of this difference are in complete agreement with Hansen et al. [2001]
and reveal that virtually all of this difference can be traced to the adjustment for the time of observation bias. Hansen et al. [2001] and Karl et al. [1986]
The reviewer notes: “Our prescription for wellness? Withhold the host of medications being given and the patient’s fever will subside.”
Originally from CO2Science


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3ed9891',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe southwestern US was nearly a desert from about 9000 to 5000 years ago, when Holocene peaks in aridity, surface temperature, and wildfire rates occurred. Arctic sea ice was at its lowest extent of the Holocene during these years.

Image Source: Lachniet et al., 2020
A new extensively-referenced study (Lachniet et al., 2020) reviewing many dozens of climate records from across western North America has determined there is nothing unprecedented or even unusual about the modern climate for this region.
CO2 checked in at about 265 ppm during the Early and Middle Holocene, but today’s associated >400 ppm CO2 climate is much cooler and wetter, and there is much more Arctic sea ice present today.
During the Early to Middle Holocene (approximately  9 to 5 thousand years ago) this region could be characterized like this:
·2°C warmer, the warmest of the Holocene


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




·150 meters higher tree lines on mountains, indicating greater warmth
·5-17 m lower lake/pond levels
·“complete desiccation” or desert-like conditions in some areas
·peak wildfires
·“a warmer Arctic, reduced sea ice extent” – the lowest extent of the Holocene

Image Source: Lachniet et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_c46e1d5aedc7d8b197bc83e2f0539a56').on('change', function() {
			  jQuery('#amount_c46e1d5aedc7d8b197bc83e2f0539a56').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe equatorial Pacific is now showing a distinct band of cool surface water developing. Many forecasters have already written the obituary for the now departing El Nino, which pushed global surface temperatures up by some 0.8°C over the last few months.
Developing La Nina
Source: unisys.com
The newly developing La Nina will probably erase much of that in the months ahead. For example, Accu Weather meteorologist Joe Bastardi predicts cooling over the next year or two. But he does say this year’s Arctic ice melt may challenge the record low set in 2007. One more but – he says that after that the ice will recover over the next 2 years and reach normal levels. So all you obsessed global warmists out there, milk it while you can. It’s your last chance (if Joe’s right).
Read the latest Wrap-Up from the Australian Bureau of Meteorology.

Share this...FacebookTwitter "
"

A new report from the International Energy Agency is sparking headlines across the media. “Global carbon dioxide emissions soared to record high in 2012” proclaimed USA Today; The Weather Channel led “Carbon dioxide emissions rose to record high in 2012”; and the Seattle Post‐​Intelligencer added “The world pumped a record amount of carbon dioxide in the atmosphere in 2012.”   
  
  
The figure below (taken from the IEA summary) provides the rest of the story.   
  
  
It shows a breakdown of the change in carbon dioxide emissions from 2011 to 2012 from various regions of the globe.   






  
  
  
Notice that the U.S. is far and away the leader in reducing carbon dioxide (CO2) emissions, while China primarily is responsible for pushing global CO2 emissions higher. In fact, CO2 emissions growth in China more than offsets all the CO2 savings that we have achieved in the U.S.   
  
  
This will happen for the foreseeable future. Domestic actions to reduce carbon dioxide emissions will not produce a decline in the overall atmospheric carbon dioxide concentration. The best we can hope to achieve is to slow the rate of growth of the atmospheric concentration—an effect that we can only achieve until our emissions are reduced to zero. The resulting climate impact is small and transient.   
  
  
And before anyone goes and getting too uppity about the effectiveness of “green” measures in the U.S., the primary reason for the U.S. emissions decline is the result of new technologies _from the fossil fuel industry_ that are leading to cheap coal being displaced by even cheaper natural gas for the generation of electricity. As luck would have it, the chemistry works out that that burning natural gas produces the same amount of energy for only about half of the CO2 emissions that burning coal does.   
  
  
A new report from the U.S. Energy Information Administration estimates that as a result of these new technologies (e.g., hydraulic fracturing and horizontal drilling), globally, the technologically recoverable reserves of natural gas are nearly 50% greater than prior to their development.   
  
  
Currently, the U.S. is the leader in the deployment of these technologies, and the effects are obvious (as seen in the figure above). If and when more countries start to employ such technologies to recover natural gas, perhaps the growth in global carbon dioxide emissions will begin to slow (as compared to current projections).   
  
  
Considering that possibility, along with the new, lower estimates for how sensitive the global average temperature is to carbon dioxide emissions, and the case for alarming climate change (and a carbon tax to try to mitigate it) is fading fast.
"
"A major change is coming to our skies. From next March, pilots will be able to determine their own routes and plan to fly direct from point to point.  Currently, flights must plan to follow explicitly defined corridors. These are rather like roads in the sky, up to 10 miles wide. Under the new scheme, flights will still be subject to air traffic control from Prestwick Centre in the West of Scotland, but pilots will be considerably more free to plan the specific route that they take.  The move by the UK’s main air-traffic control agency, NATS, to switch to what is known as “route free airspace” will initially apply to an area between Skye and the Isle of Man that carries about 450 planes each day.  Route-free airspace will allow aircraft to take shorter and more direct routes. This is expected to bring multiple benefits including better flight efficiency, greater cost-effectiveness, reduced engine running time, reduced fuel consumption and resulting environmental gains.  How it will work in practice is that the pilot or his airline will determine the best flight path using dedicated software and then submit a flight plan and route before take-off or before entering UK airspace. This can be done online or via several other methods including radio.  Route-free airspace has been made possible by technologies such as Automatic Dependent Surveillance-Broadcast (ADS-B). This makes use of satellite positioning to allow aircraft to broadcast their precise location every few seconds so that pilots of other aircraft are able to plan their routes accordingly.  This means that aircraft can make better use of available space and fly closer together. It removes the need for the tightly restricted routes that are used at present. ADS-B is being fitted to an increasing number of aircraft. It is mandatory in Australia. It will become mandatory for most aircraft in Europe from 2017, and from 2020 in the USA. The idea of pilots planning routes without central coordination might raise safety concerns, but in practice, aircraft will still remain under the supervision of air-traffic control. Our fixed routes were indeed originally designed to maintain safety, but they employ decades-old systems that are based on radar and no longer necessary.  You might also think that fixed routes could lead to greater delays as aircraft manoeuvre around each other, but as routes will be actively monitored and predicted up to 25 minutes ahead of time, such issues are actually reduced. Neither will the routes result in a noise nuisance for people on the ground, since route-free airspace only applies to airspace above 25,000 feet (4.8 miles). NATS plans to extend the trial to the rest of Scotland and parts of the North Sea from 2017 and has a long-term strategy to establish all upper airspace in the UK as route-free (making this work in much of England and Wales will admittedly take rather more planning because the airspace is already very congested). This is being promoted as part of the Single European Sky initiative, which aims to modernise Europe’s air traffic control system. It has the target of a 10% reduction in the effect of aircraft on the environment by 2020, against a background of increasing passenger traffic.  The pan-European air-traffic coordination agency Eurocontrol estimates that while the number of flights will have increased by 50% between 2012 and 2035, deploying route-free airspace over central Europe at night and weekends will reduce flight distance by the equivalent of 1.16m km per year. Route-free airspace has already been implemented in Sweden, Portugal and Ireland. It is planned or partially implemented in the rest of Scandinavia, Italy and central and eastern Europe. It also forms part of the Federal Aviation Authority’s NextGen programme in the United States. However, despite the obvious benefits from going route-free, some argue that rising air-traffic volumes is not something that we will be able to support indefinitely. The biggest potential barriers are probably infrastructure on the ground (especially airports themselves, which consume large areas of land) and rising costs of jet fuel.  Particularly around major cities, airspace is already crowded. While improvements to planning and control could make better use of the space, there is only so much to be done before no further aircraft can fit in the sky with a safe separation distance. And though the answer would presumably be to improve road infrastructure and high-speed rail instead, this has its own issues and controversies. And we can’t just rely on route-free airspace to deal with the environmental issues around aircraft. Eurocontrol’s central forecast is that by 2035, total emissions might have fallen slightly despite the expected rise in air traffic, but that assumes that fuel efficiency keeps improving. This is by no means certain.  Air-traffic control therefore needs to focus on other ways of improving pollution, such as optimising taxi routes at airports and flight sequencing. This stems from the fact that short flights are more polluting than long-distance ones because of the impact of take-off and landing compared to the distance travelled. Indeed, it is interesting to note that in contrast with the liberalisation of air routes, there is a possible move towards more centralised control of taxi routes as automated systems for optimising routes are considered.  For the same reason, it is important that new technologies in computational search and mathematical modelling are fully explored and investigated in the context of ensuring that our air-traffic systems and resources are being used as efficiently as possible. So long as we see these moves towards making large amounts of airspace route free as just one contribution to a wider environmentally-focused modernisation of the aviation industry, we should ultimately reach the right destination."
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"

**March 1:** Mexico’s peso crisis is only the latest example of how misguided monetary policy can wreak havoc on economiclife and disrupt society. At a Book Forum, editors James A. Dorn and Roberto Salinas‐​León discussed their new book, _Money and Markets in the Americas_. The book provides a framework for thinking about how to end the monetary chaosthat has plagued Latin America and how to energize the market‐​liberal order that is now emerging in the Americas. SteveHanke, professor of applied economics at Johns Hopkins University; Sir Alan Walters, former economic adviser to MargaretThatcher; and Ed Hudgins, Cato’s director of regulatory studies, provided comments. 



**March 7:** At a Capitol Hill Policy Forum, “It’s Time to Tear Up the Tax Code: The National Retail Sales Tax,“cosponsored with the National Taxpayers Union and Citizens for an Alternative Tax System, Cato’s director of fiscal policystudies Stephen Moore led a discussion on completely replacing the tax code with a retail sales tax. The panel included Rep.Dan Schaefer (R‐​Colo.), Rep. Billy Tauzin (R‐​La.), David Keating of the National Taxpayers Union, David Burton of theArgus Group, and Vic Krohn of Citizens for an Alternative Tax System. 



**March 8:** A debate over whether regulations, like legislation, should have to be signed by the president before they becomelaw was the centerpiece of a Cato Policy Forum on “Ending Regulation As We Know It: Legislative Deregulation inthe Dock.” Professor Marci Hamilton of Cardozo Law School discussed the highly controversial constitutional questionssurrounding legislative delegation, and Professor David Schoenbrod of New York Law School, author of Power withoutResponsibility: How Congress Abuses the People through Delegation, addressed the policy ramifications of executive‐​branchlawmaking. Nadine Strossen, president of the American Civil Liberties Union, explained how civil liberties are curtailed by theabrogation of legislative responsibility. David Hawkins, senior attorney at the National Resources Defense Council, spokeabout how ending the delegation of lawmaking power to the executive branch will affect regulatory practice; and Rep. J. D.Hayworth (R‐​Ariz.) described legislation he has sponsored to require all lawmaking regulations to be affirmatively adopted byCongress and signed into law by the president before they take legal effect. 



**March 18:** Cato hosted a Capitol Hill Policy Briefing on the question, “Is the Immigration Bill in the NationalInterest?” A panel featuring Ben Wattenberg of the American Enterprise Institute, Scott Hoffman of Americans for TaxReform, and Stuart Anderson and Stephen Moore of the Cato Institute addressed the economic, demographic, and politicalimplications of the pending legislation. 



**April 3:** A Cato Book Forum celebrated publication of _Oil, Gas, and Government: The U.S. Experience_. Author RobertL. Bradley Jr., president of the Institute for Energy Research, debunked the “market failure” arguments for oil and gasregulation, including those based on a theory of natural monopoly, predatory pricing, and national security. 



**April 9:** A Policy Forum, “The New Prohibition? Freedom and Tobacco under Siege by the FDA,” asked whether theFood and Drug Administration is acting outside its statutory authority and threatening free speech. Sam Kazman of theCompetitive Enterprise Institute, Larry Pilot of McKenna & Cuneo, L.L.P., Jack Calfee of the American Enterprise Institute,and Matthew Myers of the Coalition on Smoking or Health debated the issue. 



**April 9:** The conflict over Taiwan is the latest in a series of rough spots in the U.S.-Chinese relationship. At a Policy Forumentitled “Tensions in the China Sea,” Ted Galen Carpenter, Cato’s vice president for defense and foreign policy studies;James Przystup, director of the Asian Studies Center at the Heritage Foundation; and Selig Harrison, senior associate at theCarnegie Endowment for International Peace, discussed the steps that Washington could take to avoid a showdown overTaiwan. 



**April 10:** The Honorable Vojt_​ch Cepl, a justice on the Czech Constitutional Court and currently E. L. WiegandDistinguished Visiting Professor of Democratization at Georgetown University, discussed the legal foundations of civil societyat a Roundtable Luncheon with Cato policy staff. Justice Cepl attributed the Czech Republic’s successful transition fromcommunism to capitalism to the combination of a tradition of civil society in Bohemia and Moravia and a carefully implementedpolicy of legal “lustration”: a clean legal break with the collectivist past. 



**April 11:** The inclusion of medical savings accounts (MSAs) in Medicare reform proposals has sparked an intense debateover “adverse selection”–the notion that MSAs will appeal only to the young and healthy, leaving traditional Medicare to servethe elderly and sick. Proponents of MSAs maintain that they will appeal to everybody because they minimize theout‐​of‐​pocket costs of both the healthy and the sick. At a Policy Forum entitled “Medical Savings Accounts and AdverseSelection,” Leonard Burman, tax analyst at the Congressional Budget Office; Edwin Hustead, chair of the MSA WorkGroup of the American Academy of Actuaries; and Peter Ferrara, general counsel and chief economist at Americans for TaxReform, debated the issue. 



**April 17:** Many Americans anticipate that the Federal Communications Commission will be doing less under theTelecommunications Act of 1996. The Clinton administration, however, has proposed increasing the FCC’s budget by almost$47 million. In light of those opposing expectations, Cato organized “The Future of the FCC,” a Policy Forum that askedwhat the administration’s policy bodes for proposals to phase out the FCC altogether. Discussants included Gregory Simon,chief domestic policy adviser to Vice President Al Gore; James Gattuso, vice president of policy research at Citizens for aSound Economy; and Kenneth Robinson, attorney at law. 



**April 17:** While the political tide of regulatory and statutory reform appears to have been stemmed for the time being by theenvironmental lobby, many people still criticize the centralized command‐​and‐​control regulatory structure of mostenvironmental laws. At a Policy Forum entitled “The Politics and Policy of Environmental Protection: Beyond the 104thCongress,” Kenneth Chilton, executive director of the Center for the Study of American Business, and Debra Knopman,director of the Center for Innovation and the Environment at the Progressive Foundation, discussed both what should be doneto reform the status quo and how to do it in the current political climate. 



**April 18:** As the Senate prepares to consider congressional term limits, Sen. John Ashcroft (R‐​Mo.) has launched anunprecedented online petition drive to highlight popular support for that constitutional change. At a Policy Forum entitled“Emerging Technologies and the Fight for Congressional Term Limits,” Senator Ashcroft discussed the role that termlimitation and emerging technologies can play in promoting participation in the democratic process. Introductory remarks wereby Paul Jacob, executive director of U.S. Term Limits. 



**April 23:** Recent Supreme Court opinions and the California Civil Rights Initiative have catapulted affirmative action to centerstage as we enter the political season. Not to be outdone, Congress itself will revisit its 30‐​year‐​old policy of groupentitlements this year as it debates H.R. 2128, the Equal Opportunity Act of 1995, a bill to end preferences in federalprograms, contracting, and employment. At a Capitol Hill Policy Forum entitled “An End to Preferential Treatment?“Cato’s director of constitutional studies Roger Pilon moderated a critical discussion of affirmative action. Panelists includedClint Bolick of the Institute for Justice, Faye Anderson Douglass of the Policy Institute, Linda Chavez of the Center for EqualOpportunity, Rep. Charles Canady (R‐​Fla.), and Rep. Tom Campbell (R‐​Calif.). 



**April 25:** Cato hosted a Roundtable Luncheon with Hisahiko Okazaki, former Japanese ambassador to Saudi Arabia andThailand. Okazaki spoke about U.S.-Japanese relations and East Asian security issues. 



**April 26:** The Institute hosted a City Seminar in New York City that featured a keynote address by John Stossel,investigative reporter for ABC’s 20/20. Other speakers included José Piñera, president of the Center for Pension Reform andcochairman of Cato’s Project on Social Security Privatization, and Cato’s president Edward H. Crane, director of fiscal policystudies Stephen Moore, and director of telecommunications and technologies studies Lawrence Gasman. 
"
"

Did Al Gore really deserve that Oscar for “An Inconvenient Truth”? The Left says yes — only the ideologically disabled or intellectually dishonest deny that the four horsemen of the environmental apocalypse (drought, disease, sea rise, and hurricanes) will soon devastate our fair planet. Reporter William Broad in the _New York Times_ today, however, says not so fast — a backlash is brewing among REAL scientists who are getting sick and tired of bed‐​wetting hysteria surrounding climate change.



The gist of their concern is this: while most (but not all) scientists are willing to accept that industrial emissions are an important driver in the planetary warming we’ve experienced since the late 1970s, they aren’t anywhere near so eager to embrace politically inspired warnings from non‐​scientists about how “the end is near.” Al Gore, according to many of the scientists interviewed by William Broad, is too shrill and too apocalyptic given the scientific evidence. 



Case in point: Al Gore warns in his documentary that sea levels will rise over 20 feet if warming continues. Yeah, well maybe in a thousand years or so if trends continue indefinitely, but the former Vice President leaves that little bit of perspective out of the movie. What might happen during our lives and the lives of our children and grandchildren? A sea rise of 23 inches, max, according to the new report just out from the Intergovernmental Panel on Climate Change. That’s hardly going to flood Manhattan, but acknowledging that would spoil the wonderful special effects visuals offered in the slideshow, now wouldn’t it? 



Gore’s scientific advisors, friends, and admirers defend the documentary and the book that followed by conceding that he may be a bit dodgy here and there, but that he gets the big picture right. That’s ridiculous. The fact that the planet is warming and that industrial emissions might well have something to do with it is not what this debate is ultimately about. This debate is whether we should or should not care. And if the former, how much should we be willing to sacrifice to do something about it? 



To say that Al Gore is to some extent out to lunch on the “should we care” argument but relatively sound on the question about whether we’re warming the planet (at least, if we measure these things by that most holy of metrics, the “scientific consensus” as defined by the IPCC) is akin to saying that the fellow proclaiming that a wrathful God is about to incinerate the planet is contributing to social welfare by usefully pointing out to the unbelievers that there is a God. That bit about God being particularly angry or plotting to destroy the world — Well, that’s a bunch of nonsense, but hey, he got the big picture right.



One of the scientists interviewed in the article — Roger Pielke, Jr. — wrote an essay recently for our own _Regulation_ magazine pointing out that science is inevitably corrupted when politicians decide to effectively delegate policymaking power to those who wear white frocks. So if you want to know why scientists aid and abet this kind of thing, go there. 
"
"

Europe is slowly disarming. For decades the continent could rely on America to fill the gap. No longer. That realization has given France pause. Maybe other European states also will start taking their security responsibilities more seriously.



A new report from the European Union’s Institute for Security Studies acknowledged an unpleasant reality—centuries of the West’s and especially Europe’s “dominance are currently giving way to a more multipolar and less governable world system.” That wouldn’t be such a problem if the change was not combined with diminishing military capabilities, again especially in Europe.





There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



Noted ISS: “Failing to act, therefore, means that a mixture of acute budgetary pressures, lack of investment in research development, and widespread reluctance to make the maintenance of effective armed forces a political priority could cause additional reductions in EU military capacity as well as a potential exodus of the defense industry and a loss of technological leadership.” The Europeans are spending ever less, with “the budget cuts carried out so far have been made without any coordination and consultation among allies.” Moreover, European governments are spending unwisely, emphasizing personnel and land‐​based facilities, for instance.



A continuing reduction in capabilities seems likely if not quite inevitable because Europe no longer faces any serious, let alone existential, threats. Russia is a poor replacement for the Soviet Union. It is impossible to build a plausible scenario for Russian troops threatening Warsaw or Berlin, let alone Paris or London. Moscow still might beat up on its neighboring constituent republics, such as Georgia, but the latter actually started their war.



China might become a peer competitor of America, but it has no European ambitions. Balkan instability is no substitute for potential aggression from whomever. North Africa and the Middle East generate continual geopolitical complications, but getting involved usually creates even greater problems. Even a nuclear Iran—an unpleasant prospect, to be sure—seems unlikely to target Europe. About all that’s left for Europe’s militaries are distant nation building, anti‐​pirate sea patrols, and play‐​acting like a _Weltmacht_.



EU leaders still might talk about creating a continental foreign policy and military, and national politicians still might want armed forces capable of doing more than providing an honor guard for foreign dignitaries, but European peoples exhibit little interest in paying the resulting bill. Spending more efficiently and collaborating more extensively would help, but the continent’s ongoing Euro crisis, recession, and heavy indebtedness all encourage further retrenchment. One visiting NATO official told a private, off‐​the‐​record gathering in Washington, “There is no chance for budget increases, not even for keeping spending levels as they are.” Earlier this year Rasmussen declared, “There is a lower limit on how little we can spend on defense.” But what is it?



This is a prescription for eventual European disarmament, but a slight sign of hope is flickering in France. Although modern French presidents don’t look much like reincarnations of Emperor Napoleon, they are not shrinking violets internationally. Both Presidents Nicolas Sarkozy and Francois Hollande had wars they wanted to fight—Libya and Mali, respectively. However, they both found Paris to be unable to fight without assistance, primarily from America.



Europe’s rising enthusiasm for war is ironic. Observed Philip Stephens in the _Financial Times_ : “Europeans have caught the interventionist bug just as the U.S. has shaken it off.”



However, France’s financial difficulties created pressure for additional cuts in military outlays. The Hollande government recently released its defense review, known as the _livre blanc_. Although the government reduced its rapid‐​deployment forces, it “opted to keep France’s air, ground and sea capabilities, while freezing defense budgets over six years,” noted the _Economist_. Outlays will shrink in real terms and as a percentage of GDP, but “dark talk of the loss of 50,000 jobs proved unfounded. The planned yearly cuts will be smaller than under the previous president, Nicolas Sarkozy. France will maintain its capability for expeditionary warfare, and boost special forces.”



One reason for this is Gallic pride, even ego. President Hollande explained: “France’s destiny is to be a global nation and our duty is to guarantee not only our own security but that of our allies and partners.” In doing so, he added, “France wants to maintain its ability to react alone.” How could it be any other way?



Opposition legislators complained that the proposed force was inadequate for such a role. Vincent Desportes, former director of a military school, told the _New York Times_ that the plan “makes France a really minor actor in coalition operations.” However, a budget increase was inconceivable in today’s economic climate.



The second reason is more significant. Paris apparently realized that if it is going to continue to be a “global nation,” it no longer could expect as much help from across the Pond. As the _livre blanc_ delicately put it, Americans will “prove more selective in their overseas engagements.” This led to one conclusion. Noted the _Economist_ : “One arresting element is the recognition that France may have to step up militarily in the Mediterranean and Africa as America pulls back.”



That requires not just sufficient forces but the right forces. Defense Minister Jean‐​Yves Le Drian called some of his nation’s deficiencies “incomprehensible,” requiring Paris to spend more on aerial refueling and other specialties. Said Le Drian, new investments “seem to me inevitable, like intelligence and special forces.”



This may be a seminal moment for European defense policy. Explained Francois Heisbourg of the Foundation for Strategic Research: “Planning to operate in a world where the Americans will be in only a supporting role changes everything. It is essential that we get the right kit to do it.”



Hallelujah!



It long has been obvious that Washington’s promise to protect prosperous and populous allies created a disincentive for them to do more for their own defense. During the Cold War the Europeans routinely violated their promises to hike military expenditures, even in the face of the numerically superior Red Army. Japan hid behind its pacifist constitution and kept military (“self defense”) outlays below 1 percent of GDP. Since the mid‐​1990s South Korea has skimped on its armed‐​forces budgets while providing the North with $10 billion worth of assistance as part of the Sunshine Policy—even as North Korea threatened to turn Seoul into a “lake of fire.”



A lack of capacity did not stop Britain and France from pushing for war with Libya, though they received only limited support from other European states and had to go to Washington for additional assistance. However, American officials have demonstrated far greater reluctance to join the Syrian civil war. As the U.S. further reduces both capabilities and obligations, even Paris realizes that Washington might say no to its next war proposal.



Which means France must do more than it really wished. But Paris apparently will do what it must.



U.S. policymakers should learn from this experience. Instead of bashing the Europeans, insisting that they spend more when they see no compelling reason to do so, Washington should simply shed the burden of Europe’s defense. Inform America’s long‐​time friends and allies that the cheap ride is over. Then let the Europeans decide how much they want to spend to defend what. And allow them to bear the consequences.



The same goes for the Balkans, Mediterranean, Central Asia and Middle East. Whether the issue is Kosovo, Libya, Georgia or Syria, absent a compelling interest for America military action should be up to Brussels, or Paris, London and Berlin. If they decide not to act, no worries. There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



There’s still substantial room for security cooperation. And Washington obviously could help the Europeans become militarily self‐​sufficient. But the time for a U.S.-dominated alliance is over.



Economists long have told us that incentives matter. France’s behavior proves that they do. When Paris believed that it could rely on Uncle Sucker, the former did one thing. When the French realized that the Yanks really might not be coming, they did something different. Washington needs to send the same message to the rest of its defense dependents.
"
"Wind turbines are a leading source of green energy which could supply 12% of the world’s energy by 2020. But their use is often criticised for its impact on wildlife, particularly birds. Larger birds can collide with turbines and some have even learned to avoid flying near them. Impacts on smaller birds are less well documented as they tend to manoeuvre around turbines and can avoid impacting with them much more easily than larger species. My own research showed that birds associated with farmland, including a range of songbirds, were generally unfazed – their winter distribution didn’t change in the presence of turbines.  But there were also some intriguing patterns in the behaviour of skylarks in early spring. We noticed their numbers were generally lower close to turbines. I wondered then whether the noise emitted by the turbines might be responsible. Much of the evidence for how wind turbines affect birds concerns their distribution patterns around turbines, but we know little about why birds choose to avoid them. The robin, a widespread small bird which lives in rural areas where turbines are common, seemed a perfect candidate to investigate. Robins are an aggressive but popular species in the UK, having recently been voted the nation’s favourite bird. Males are territorial beyond proportion to their diminutive size. Nevertheless, we subjected territorial male robins to one of three treatments – another robin’s song, a robin’s song with wind turbine noise, and wind turbine noise alone – via a sound recording device inside their territory. Robins defending their territory typically respond to an intruder by increasing the proportion of low frequency sounds in their songs. We found that the robins subjected to robin song and wind turbine noise simultaneously had significantly fewer low frequency elements in their songs and so their songs sounded higher pitched. We interpreted this as interference from the wind turbine noise which occurs at low frequencies. It’s suspected that lower frequency noises make the robin singer “sound” bigger and thus reduce the need for more direct physical encounters to defend their territory. But with the low frequency sound emitted by wind turbines drowning them out, there was a suggestion that robins were having to rely more on puffing out their red chest to deter aggressors. That may be why breeding songbirds, such as the skylark, avoid turbines. Recent work has found breeding bird populations such as the Dupont’s lark, a near-threatened species of songbird found in North Africa and Spain, declined in areas with wind turbines. The underlying reason may be, at least partially, that birds avoid noisy habitat that makes communication more difficult. Another study looked at the long-term impact of noise from generators in a forest, and showed how it reduced territory quality for ovenbirds, a common warbler with a complex and beautiful song from the Americas.  


      Read more:
      Wind turbines aren't quite 'apex predators', but the truth is far more interesting


 The impact of wind turbines on birds goes beyond the risk of direct collision or avoidance. Noise emitted from turbines could disrupt their communication and leave them vulnerable. This is particularly troubling when we consider that wind turbines are often located in remote areas, some with high densities of songbirds, such as meadow pipits and skylarks in upland areas of the UK.  Of course, it is important to remember the bigger picture. While wind turbines may harm birds nearby, renewable energy is a vital solution to climate change – perhaps the most pressing threat to biodiversity globally.  Nevertheless, noise pollution from wind turbines should be measured during environmental impact assessments of wind energy projects, to ensure effects on the surrounding wildlife are minimised. That way, the robin and other songbirds might hope for more peaceful Christmases in future."
"
Share this...FacebookTwitterThe prominent German online news magazine  FOCUS  reports that 2010 may set a new NASA high temperature record. The cause of the recent warmth is El Nino. But FOCUS then throws ice-cold water on any warmist dream of an overheating planet, at least for the next few years, and writes that scientists believe: “Womöglich aber sind die warmen Zeiten für unseren Globus bald vorüber”.
In English:
Quite possibly, the warm times for the planet will soon be over.
The FOCUS report looks at three factors, which I present in 3 parts. 
Part 1: La Nina
FOCUS first zeroes in on La Nina, and quotes AccuWeather meteorologist Joe Bastardi:
There are wild cards in the climate system that have changed the previous climate events. Now we’ve got a weak solar cycle and the prospect of increased volcanic activity. Together with a La Nina, it all could be a troublesome triple whammy.
FOCUS also quotes Joe D´Aleo of TV Weather Channel:
We’ll have La Nina conditions before the summer is over, and it will intensify further through the fall and winter. Thus we’ll have cooler temperatures for the next couple of years.
Part 2: Solar Activity
The next big factor is the sun, which has worried a number of scientists over the last couple of years. It refuses to start-up with a new cycle. 2008 had 266 spotless days and 2009 had 261.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




FOCUS writes:
Now some scientists fear the solar slumber could herald in a new Little Ice Age. This period, which extended from the 15th to the 19th century, was characterised by bitter cold winters and cool, wet summers which left grains and crops rotting in the fields.
FOCUS magazine then acknowledges the Maunder and Dalton Minimums, thus indirectly refuting Mann’s version of climate history. The German media is waking up!
FOCUS then quotes Joe D’Aleo:
If the number of spots does not climb over 40 or 50 during the next maximum, which would mean a low level of solar energy, then we have to reckon with much lower temperatures in the coming years.
Part 3: Volcanoes
Volcanoes in Iceland are coming alive. So far the ash clouds have been too small to have any effects on the climate. The real risk, however, is that it may be a foreboding of something much worse to come – the eruption of the mighty neighbouring Katla volcano. Katla has a far more immense chamber of magma. It erupts on average every 70 years and in tandem with Eyjafjalla. The last Katla eruption was in 1918, thus making an eruption overdue.
According to Joe Bastardi:
Katla could be a game-changer. If it erupts and throws ash and sulfur particles into the stratosphere, then the global temperature will plummet.
The triple whammy of La Nina, low solar activity and increased volcanic activity all acting together would certainly put global warming on the back burner for a while. But some scientists, like Prof. Mojib Latif of the University of Kiel, insist that warming will resume once the cooling factors fade off, and that global temperature increases of 5°C by the end of the century cannot be excluded.
In the meantime, get ready for cooling.
Share this...FacebookTwitter "
nan
"With the UK leaving the European Union and eyeing new trade opportunities beyond the EU, it should also be looking for ways to take forward its policies for reducing greenhouse gas emissions. One of the few advantages of Brexit might lie in being able to design policies that haven’t gone down the long and winding road of Brussels’ consensus building. Since the beginning of industrialisation, humanity has released 1.5 trillion tonnes of carbon dioxide into the atmosphere and, as a consequence, global temperatures have risen by 1°C. As the recent IPCC special report reminds us, we must pump out less than another 770 Gigatons to keep the total rise below 1.5°C.  This is not going to be easy. The currently stated ambitions of the world’s nations would actually increase outputs from their current total of just over 40 Gigaton a year to around 55 Gigaton by 2030. At 50 Gigaton a year we blow the 770 Gt budget in just 15 years. So, more must be done, and it must be done immediately. A carbon tax is arguably the most effective way of addressing the emissions problem as it provide a simple framework that everyone understands as well as the regulatory stability that allows businesses to plan ahead. Carbon taxes send a clear price signal and incentivise households and industry to change their behaviour.  Taxes are also superior to top-down regulation such as sector-specific reduction targets or even emissions trading. A carbon tax factors in the cost of CO₂ emissions to production, thereby forcing industry to account for it and reduce emissions. It is “technology-blind”, meaning a carbon tax does not pick a winner in the market and instead leaves it to industry to develop more sustainable production.  In practice, however, carbon taxes haven’t been very popular. Political leaders worry about support for anything called a tax and shy away from it. The recent fuel protests in France are a testimony to that fear. By increasing the cost of energy, taxes also risk increasing fuel poverty. In short, squaring the circle between climate goals and distributional equity is a matter of energy justice. The EU instead opted for an emissions trading system, the ETS, and other countries, including China, have adopted similar schemes. The ETS puts a cap on emissions, forcing polluters to buy carbon permits if they emit more than they can under their allocated quota. Permits are tradeable, thus creating a price signal for carbon. The problem with the EU’s ETS is that, until very recently, it has delivered prices around €5 per ton or less – a far cry from the €45 or more that would be needed for it to be compliant with targets set in the 2015 Paris agreement. The ETS also only covers a few industries, such as electricity generation, that, together, produce only 45% of the EU’s total emissions. So how can the UK enhance its climate leadership and keep citizens and businesses on board? A place for inspiration is Canada, the first nation to implement a carbon-fee and dividend scheme.  Their main idea was to tax carbon emissions and ensure money taken in taxes is given to the public in the form of a dividend paid to households. There is much to celebrate in such a scheme. The initial tax level might be something like £25 per tonne of carbon dioxide and, in the UK, we each produce about six tonnes per year. A rough back-of-the-envelope calculation suggests the tax could generate £150 per person per year.  A recent study estimates that the majority of households will come out rather well on a net basis, getting more back in carbon dividends than they would pay in carbon taxes, should the government roll out the scheme nationwide as planned. As research has shown, redistributing carbon revenues not only helps social equality, it also improves the acceptance of such taxes among taxpayers. It therefore makes it very hard for a future government to reverse the policy if people become attached to payments from the state. Just think how hard it would be for any government to repeal the UK’s winter fuel allowance for over 65s. 


      Read more:
      Electricity bills could rise if Brexit threatens Northern Ireland's unique energy agreement with Ireland


 An effective and socially acceptable carbon fee, in turn, fosters economic competitiveness. The Nordic countries, which pioneered carbon taxes, have become leaders in clean technology. Denmark has some of the world’s lowest unit energy costs thanks to drastically reduced energy use in their economic output. This, among other reasons, effectively shields the country’s industry from energy price shocks. The UK, in many respects already a climate frontrunner thanks to its carbon floor price, stands to learn from the Canadian experience as its post-Brexit low carbon policies take shape. In Europe, the UK is the second-largest polluter, behind only Germany in overall greenhouse gas emissions.  The UK can exert true leadership by designing progressive policies that benefit both people and the climate. If they work, this might set a model across the continent – whether from within or outside the EU."
"

Above: Earth in comparison, size wise to common sunspots
The Christion Science Monitor had a detailed article recently that brought in a surprisng source – NASA GISS – an entity that seems firmly entrenched in the AGW- CO2 theory of climate change. Here are some excerpts from the article:
Researchers say they’ve found puzzling correlations between changes in the sun’s output and weather and climate patterns on Earth. These links appear to rise above the level of misinterpreted data or faulty equipment.
“There are some empirical bits of evidence that show interesting relationships we don’t fully understand,” says Drew Shindell, a researcher at NASA’s Goddard Institute for Space Studies in New York.
For example, he cites a 2001 study in which scientists looked at cloud cover over the United States from 1900 to 1987 and found that average cloud cover increased and decreased in step with the sun’s 11-year sunspot cycle. The most plausible cause, they said: changes in the ultraviolet (UV) light the sun delivers to the stratosphere.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea38acdc3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterA new paper reveals that climate models have failed to take important natural factors, such as the North Atlantic Oscillation, into account in their climate models on which leaders have been basing their policies. 

 A new paper in Nature says NAO not taken adequately into account by climate models. Image: see video (German) here.
Paper in Nature Criticizes NAO Hole: Medium-Term Climate Far More  Predictable Than Climate Models Suggest
By Die kalte Sonne
(German text translated/edited by P. Gosselin)
Can the ups and downs of the climate of the coming years and decades be predicted? A large group of researchers (Smith et al. 2020) affirms this and describes in a Nature article that much more is possible here than current climate models suggest.

Image Source: Smith et al., 2020
North Atlantic climate far more predictable than models imply
Quantifying signals and uncertainties in climate models is essential for the detection, attribution, prediction and projection of climate change1,2,3. Although inter-model agreement is high for large-scale temperature signals, dynamical changes in atmospheric circulation are very uncertain4. This leads to low confidence in regional projections, especially for precipitation, over the coming decades5,6. The chaotic nature of the climate system7,8,9 may also mean that signal uncertainties are largely irreducible. However, climate projections are difficult to verify until further observations become available. Here we assess retrospective climate model predictions of the past six decades and show that decadal variations in North Atlantic winter climate are highly predictable, despite a lack of agreement between individual model simulations and the poor predictive ability of raw model outputs. Crucially, current models underestimate the predictable signal (the predictable fraction of the total variability) of the North Atlantic Oscillation (the leading mode of variability in North Atlantic atmospheric circulation) by an order of magnitude. Consequently, compared to perfect models, 100 times as many ensemble members are needed in current models to extract this signal, and its effects on the climate are underestimated relative to other factors. To address these limitations, we implement a two-stage post-processing technique. We first adjust the variance of the ensemble-mean North Atlantic Oscillation forecast to match the observed variance of the predictable signal. We then select and use only the ensemble members with a North Atlantic Oscillation sufficiently close to the variance-adjusted ensemble-mean forecast North Atlantic Oscillation. This approach greatly improves decadal predictions of winter climate for Europe and eastern North America. Predictions of Atlantic multidecadal variability are also improved, suggesting that the North Atlantic Oscillation is not driven solely by Atlantic multidecadal variability. Our results highlight the need to understand why the signal-to-noise ratio is too small in current climate models10, and the extent to which correcting this model error would reduce uncertainties in regional climate change projections on timescales beyond a decade.
The authors looked at the behavior of the North Atlantic winter climate over the last six decades and found that there is basically good predictability. Yet, current climate models do not make use of this as they underestimate the influence of the North Atlantic Oscillation (NAO) by an entire order of magnitude. The noise is still too high in the climate models, and the real signal is lost.
The paper is so important that even Science commented on it at the end of July 2020
Missed wind patterns are throwing off climate forecasts of rain and storms
Climate scientists can confidently tie global warming to impacts such as sea-level rise and extreme heat. But ask how rising temperatures will affect rainfall and storms, and the answers get a lot shakier. For a long time, researchers chalked the problem up to natural variability in wind patterns—the inherently unpredictable fluctuations of a chaotic atmosphere.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now, however, a new analysis has found that the problem is not with the climate, it’s with the massive computer models designed to forecast its behavior. “The climate is much more predictable than we previously thought,” says Doug Smith, a climate scientist at the United Kingdom’s Met Office who led the 39-person effort published this week in Nature.
[…]
The study, which includes authors from several leading modeling centers, casts doubt on many forecasts of regional climate change, which are crucial for policymaking. It also means efforts to attribute specific weather events to global warming, now much in vogue, are rife with errors. “The whole thing is concerning,” says Isla Simpson, an atmospheric dynamicist and modeler at the National Center for Atmospheric Research, who was not involved in the study. “It could mean we’re not getting future climate projections right.”
You can read the entire commentary at Science.
Forecasts based on weak models
Now medium-term forecasts based on the weak computer models are being put to the test. Unfortunately, these are exactly the same forecasts that have already been used for political planning purposes. Now it is probably dawning on the latter: “the Science is NOT settled”.
Climate snake oil
Even the “attribution game” of extreme weather events, which is so popular in the media, is now being put on the back burner. Not good news for Friederike Otto, who has already been treated as an attribution superstar in the media, as she claimed to be able to calculate with percentage accuracy how much man was involved in a storm, flood or drought.
But wait, the German-language media did not even report on this important new paper…perhaps it did not fit into the climate narrative given from above. Fortunately, there is the Die kalte Sonne climate blog: That’s the first place you’ll hear it.
NAO ignored too long… stirring resistance
By the way, the systematic influence of the NAO on European temperatures has also been described by Lüdecke et al. 2020 a few months earlier. So it was a logical further step by Smith et al. 2020 to actively demand improvements in climate models.
The effect of the NAO has been known for decades. Surprisingly, it is only now that resistance is stirring in the scientific community, since it was actually clear that the models did not reflect these empirically well-documented relationships.


		jQuery(document).ready(function(){
			jQuery('#dd_b898986594013ea7fb3b1091acdc7faf').on('change', function() {
			  jQuery('#amount_b898986594013ea7fb3b1091acdc7faf').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Throughout her decade-long modelling career, Arizona Muse has walked catwalks for Chanel, Prada and Tom Ford, and starred in advertising campaigns for Karl Lagerfeld and Louis Vuitton. Now, however, she is starring in a new video for the climate activist group Extinction Rebellion, challenging viewers to “share, rewear, relove your clothing” and “to rebel against your clothes”. Because, she says to camera, “this season’s must-have is the continuation of life on Earth”. The video, which is released today, before London fashion week, features animals on the verge of extinction, as well as ice caps melting and wildfires raging. It is the latest attempt by the fashion wing of Extinction Rebellion to challenge an industry that, as Muse puts it, “is in crisis”.  Every year, global emissions from textile production outweigh the carbon footprint of international flights and shipping combined, according to the Ellen MacArthur Foundation. Between 80 and 100 billion pieces of clothing are made, with £140 million worth of clothing estimated to go into landfill in the UK alone. The fashion industry is also contributing to deforestation and water scarcity. The new video focuses on consumer responsibility, with Muse explaining: “When we are buying clothes we are making choices about the future of the planet. We’re talking about how you and I are all contributing to climate change.”  This week in London, the fashion industry is flying in to show off the designs they have for us next season. But all I can think of is what the seasons of the future might look like. #rebel #rewear #actnow @extinctionrebellion Director: Heydon Prowse Producers: Sarah Creswell & Sara Arnold DOP: Toby Lloyd A post shared by  Arizona Muse (@arizona_muse) on Feb 10, 2020 at 9:20am PST She’s keen not to make the video about shaming, however. Speaking over the phone, Muse readily admits to not having a fully sustainable wardrobe herself, and sometimes takes to Instagram to rank her outfits’ green credentials. Today her jumper (regenerated cashmere), jeans (not organic) and shoes (not vegetable tan leather) score her one out of three. The 31-year-old British-American model, whom Anna Wintour once described as the “new face of American fashion”, is hoping the video reaches “anyone who buys clothes … and anyone who makes clothes. Because clothes are a really big part of our problem today, we are making them in a way that is completely destructive to the environment and societies and people, and we need to seriously rethink this in a very rapid way.” Muse first became interested in sustainable fashion five years ago after realising she knew nothing about where the clothes she was modelling came from or how they were made. In the early days of her sustainable fashion journey, she recalled sitting at fashion events and knowing she had “maybe one or two minutes to talk about sustainability before my seat partner would turn in the other direction”. Now, she says, “people are actually interested”. Educating herself, via documentaries and reports from the Intergovernmental Panel on Climate Change, she became “fascinated by the way the whole supply chain works.” Yet she is not immune to criticism for being a part of the industry that she is calling out. She has been asked previously how she feels about modelling for brands that are not sustainable. Her answer, she says, is simple: “I feel so lucky that I have an access point to these brands. I get to go and meet the designers and CEOs, and guess what I do all day long when I’m on set? I’m chit-chatting about sustainable fashion.” During London fashion week last September, XR staged protests and put on a funeral to grieve for the industry. For many, it seemed a radical move, but there are examples of fashion weeks revolutionising, without that kind of external pressure, in the name of climate crisis. Earlier last year the Swedish Fashion Council announced it would cancel Stockholm fashion week to explore more sustainable options, and last month Copenhagen fashion week announced “radical” new sustainability goals. This time around, XR is planning more rebellion. “XR has an amazing ability to keep the climate crisis looming large in the imagination of the public and to get the right people talking,” says Muse. “I am so hopeful for fashion’s future and I don’t believe that fashion week should be cancelled. But it definitely needs some massive changes. During fashion week, there is an immense pressure on supply chains that leads to more emissions, and there is so much waste – from extravagant set designs to small things like the thousands of old coat hangers that take centuries to break down, giving off terrible toxins and endangering wildlife. We also need to ensure that more is done to highlight the amazing work of those brands making clothes that don’t harm the planet or the people who make them.” Beyond fashion week, Muse is hopeful that the industry can change for the better. “I see greenwashing, but I also see companies hiding behind the big excuse that ‘we can’t move that quickly, we have so many things to consider’. I think they’re considering margins over the health and wellbeing of the planet and society and I’d really like to see that ending.” Across the industry, steps are being taken to try to limit fashion’s detrimental impacts on the planet. For instance, in August last year, 32 companies representing about 150 brands signed the Fashion Pact at the G7 summit promising to cut greenhouse gas emissions to zero by 2050. In September, the luxury conglomerate Kering announced a bid to become carbon neutral. Even so, there have been criticisms of such efforts and Muse is wary of the current focus: “Brands are setting targets – by 2050, we’re going to be carbon neutral, say. I think it would be much more effective to say: ‘Starting right now we’re going to do everything we can to lower our emissions.’” A future target she says, “especially one that doesn’t have a precise action plan of how to get there, a roadmap, actually lets you relax. I don’t think that’s going to get us where we need to go – I think we need to feel urgency.”"
"There is a touch of the Dickensian about the urban pigeon, often seen hobbling about on gnarled stumps, pecking at trash. The mongrel mix of grey and brown plumage on feral pigeons adds to the dowdy look, the occasional iridescent flash on neck feathers too obvious, too cheap. Dickens himself wrote of the pigeons of Spitalfields in London, associated with the poorest hovels: The pigeon hutches and pigeon traps on the tops of poor dwelling.  Our local shopping mall entrance regularly echoes to the wheeling scream of a recorded peregrine falcon, played purposefully to scare away the pigeons. Urban animal life, whether domestic or feral, has often been lumped in with the socially excluded – beggars, drunks and revellers and the like. Meanwhile other pigeons provoke intense concern. The pink pigeon of Mauritius is a heartwarming example of what we can do to protect endangered species. In 1975 only ten birds lingered at just one site. It was listed by the IUCN as Critically Endangered in 1994 – “possibly the most threatened bird in the world”. Today there are around 400 in the wild of Mauritius.  This pigeon has some distinct advantages. It is pink, it lives on an exotic island famous for the Dodo and it has had some big-name backers – notably author Gerald Durrell, who brought some into captivity in 1976 to breed and started a programme of releasing them back into the wild – along with intensive habitat management. As a result, the pink pigeon’s numbers and range have markedly increased, with wild-bred young now turning up. Pink pigeons in Mauritius have turned the corner. Meanwhile their urban grey brethren remain every bit the unloved city mob. The urban pigeon is a great example of the inheritors of the Earth described by ecologist Chris Thomas – species that do well because of us, thriving in the world we have created. Theirs is a biodiversity of cityscapes, a zoopolis deserving of our respect. Not least because the rats, racoons and pigeons of our cities are so like us – at home in concrete landscapes and on diets of processed food. I am not against the effort expended on the pink pigeon at all. They are cute and exotic, two prime criteria for conservation – but the urban pigeon deserves our respect too. The great shame is that other members of the pigeon family are also at risk, species that were once commonplace, which can hardly be said of the pink pigeon.  


      Read more:
      Where are all the dead pigeons?


 Take the turtle dove, for example, which belongs to the same family as pigeons. In the UK, the turtle dove population has declined by more than 95% in barely two decades. This once common farmland dove is a bird of high summer – its call is a sleepy drone on the hottest days. It is the dove of poetry and ballads, the Phoenix’s lover in a Shakespeare sonnet. We understandably focus so much effort on rare species while we don’t notice that the commonplace is in sharp decline. Thankfully, the turtle dove is now the focus of its own conservation projects. The urban pigeon needs no such help, although they have fallen from grace since the days when pigeon fancying was a widespread hobby, which even had Charles Darwin hooked. Pigeons in all their domestic variety feature large in Darwin’s book, The Variation of Animals and Plants Under Domestication. Darwin installed a pigeon breeding loft in his home and gathered invaluable insights into how traits are inherited through reproduction by studying their breeding. Pigeons are likely to be more qualified muses for Darwin’s theories on evolution by natural selection than the commonly cited Galapagos Island finches. 


      Read more:
      Humans not entirely at fault for passenger pigeon extinction


 The story lends a distinct air of erudite science and respectability to the gentlemen pigeon fanciers of Victorian times. Not so any more – pigeon fancying is now a peripheral pastime of a lost world of old men tending their pigeon crees in tucked away allotments and backyards. Turtle doves and pink pigeons will always arouse our sympathy, while the dodo and passenger pigeon – once the most numerous bird in North America before dying out in 1914 – stand as accusing witnesses. Meanwhile the feral pigeon battles on, hobbling, chased by kids, tormented by mall managers, swallowed whole by pelicans. You can’t help but admire them."
"
Looks like its support is splintering:
Without widespread corporate support, passage of the bill – already a long shot at best – becomes even more unlikely this year. President Bush remains opposed. House Democrats have been slow to act.
From CNN Money.
According to the WSJ even Hillary and McCain are likely to stay away from it. Voting to increase your local energy prices due to a flawed cap and trade carbon tax scheme which will create 5 new government bureaucracies is never a good thing for somebody trying to get elected.
Even with chances of passage dwindling, write your senator to tell them how you feel about it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ed9440e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"There are many complex reasons why people decide not to accept the science of climate change. The doubters range from the conspiracy theorist to the sceptical scientist, or from the paid lobbyist to the raving lunatic.   Climate scientists, myself included, and other academics have strived to understand this reluctance. We wonder why so many people are unable to accept a seemingly straight-forward pollution problem. And we struggle to see why climate change debates have inspired such vitriol. These questions are important. In a world increasingly dominated by science and technology, it is essential to understand why people accept certain types of science but not others.    In short, it seems when it comes to climate change, it is not about the science but all about the politics. Back in the late 1980s and early 1990s differing views on climate science were put down to how people viewed nature: was it benign or malevolent? In 1995 leading risk expert John Adams suggested there were four myths of nature, which he represented as a ball on different shaped landscapes.   Different personality types can be matched on to these different views, producing very different opinions about the environment. Climate change deniers would map on to number one, Greenpeace number two, while most scientists would be number three. These views are influenced by an individual’s own belief system, personal agenda (either financial or political), or whatever is expedient to believe at the time.     However, this work on risk perception was ignored by mainstream science because science up to now operates on what is called the knowledge deficit model.  This suggests that people do not accept the science because there is not enough evidence; therefore more needs to be gathered.     Scientists operate in exactly this way, and they assume wrongly the rest of the world is equally rational and logical.  It explains why over the past 35 years a huge amount of work gone into investigating climate change – even though, despite many thousands of pages of IPCC reports, the weight of evidence argument does not seem to work with everyone.    At first failure of the knowledge deficit model was blamed on the fact that people simply did not understand science, perhaps due to a lack of education. This was exacerbated as scientists from the late 1990s onwards started to be drawn into discussions about whether people believed or did not believe in climate change. The use of the word “belief” is important here, as it was a direct jump from the American-led argument between the science of evolution and the belief in creation.  But we know that science is not a belief system. You cannot decide that you believe in penicillin or the principles of flight while at the same time disbelieve humans evolved from apes or that greenhouse gases can cause climate change. This is because science is an expert trust-based system that is underpinned by rational methodology that moves forward by using detailed observation and experimentation to constantly test ideas and theories.  It does not provide us with convenient yes/no answers to complex scientific questions, however much the media portrayal of scientific evidence would like the general public to “believe” this to be true. However, many who deny climate change is an issue are extremely intelligent, eloquent and rational. They would not see the debate as one about belief and they would see themselves above the influence of the media. So if the lack of acceptance of the science of climate change is neither due to a lack of knowledge, nor due to a misunderstanding of science, what is causing it?    Recent work has refocused on understanding people’s perceptions and how they are shared, and as climate denial authority George Marshall suggests these ideas can take on a life of their own, leaving the individual behind.  Colleagues at Yale University developed this further by using the views of nature shown above to define different groups of people and their views on climate change. They found that political views are the main predictor of the acceptance of climate change as a real phenomenon.  This is because climate change challenges the Anglo-American neoliberal view that is held so dear by mainstream economists and politicians. Climate change is a massive pollution issue that shows the markets have failed and it requires governments to act collectively to regulate industry and business.  In stark contrast neoliberalism is about free markets, minimal state intervention, strong property rights and individualism. It also purports to provide a market-based solution via “trickle down” enabling everyone to become wealthier. But calculations suggest to bring the incomes of the very poorest people in the world up to just $1.25 per day would require at least a 15 times increase in global GDP. This means huge increases in consumption, resource use and of course, carbon emissions.   So in many cases the discussion of the science of climate change has nothing to do with the science and is all about the political views of the objectors. Many perceive climate change as a challenge to the very theories that have dominated global economics for the last 35 years, and the lifestyles that it has provided in developed, Anglophone countries.  Hence, is it any wonder that many people prefer climate change denial to having to face the prospect of building a new political (and socio-economic) system, which allows collective action and greater equality?  I am well aware of the abuse I will receive because of this article.  But it is essential for people, including scientists, to recognise that it is the politics and not the science that drives many people to deny climate change.  This does mean, however, that no amount of discussing the “weight of scientific evidence” for climate change will ever change the views of those who are politically or ideologically motivated. Hence I am very sorry but I will not be responding to comments posted concerning the science of climate change but I am happy to engage in discussion on the motivations of denial."
"
Rounding out a review of California weather stations this week we visit Gilroy, CA, the garlic capital. This COOP station has an MMTS temperature sensor on a pole just a few feet from a concrete slab. We’ve seen a lot of that lately. But look closely – roasted garlic anyone?

Photo from NWS, San Franciso/Monterey CA
While it’s likely the BBQ grill is not used daily, one has to wonder just how much bias it’s proximity imparts into the temperature record. This station  COOP number is 04-3417 as is part of NOAA’s “A” network which reports climate to NCDC. It is located at the Fire station in Gilroy, seen below. Notice that is is also near a large parking lot and major intersection downtown. So much for NOAA’s 100 foot rule for station siting.


Click on the picture for a larger interactive view
The recently released paper from LaDochy et al. showed that “urban” stations warmed at a rate of 0.20°C per decade while the “non-urban” stations warmed only 0.08°C per decade, with the lack of attention to the measuring environment such as we see here, is it any wonder?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea26beeda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Earlier I wrote up an essay on the NOAA climate station at Cordova, AK.
Click thumbnail at left for a larger image. 
  
This station was directly next to the village diesel power plant. That station also happens to be part of the NASA GISS surface temperature record used for climate research. The problem is the proximity to nearby human caused heat sources, which may not be accurately adjusted for in the record. Of course the real issue is that if the stations were properly setup and maintained by NOAA, paying attention to their own 100 foot rule, such potential bias would not be an issue. Today I’d like to show you a few other NOAA climate stations in Alaska.
Click thumbnails below for larger images.
Thanks to John Papineau for these photographs






English Bay – note the MMTS temperature sensor within about 1 foot of the building.No cold winter nights for this sensor!


 



Moose Pass – note the concrete structure which is a fish hatchery
NCDC record says: HATCHERY, OUTSIDE & 3 MI NW OF PO AT MOOSE PASS, AK


 




Susitna Landing – note proximity to building this was installed on May 21st, 2003

NCDC record says: FLAT GRAVEL AREA NEAR CONFLUENCE OF KASHWITNA AND SUSITNA RIVERS. How would a researcher know about the building proximity from this?




 


  
  
Seward 19N – note proximity to building
NCDC Record says: OBSERVERS HOME, OUTSIDE & 19.5 MI N OF PO AT SEWARD, AK Again, how would a researcher know about the building proximity?





 





Seward #2 – note proximity to street and shading issues. You can see the station location in Google Earth.




 



Tutka Bay – note proximity to building and weathering of old Stevenson Screen shelter.


 


As I’ve been saying, the MMTS temperature sensor and it’s cable is systematically forcing measurements closer to human influences. They problem clearly is not unique to the continental United States as these photos from Alaska demonstrate.
In all of Alaska’s open wilderness, are these truly representative of the climate? It seems that every station is close to the small packets of towns and villages that dot Alaska, and necessarily so, since a human observer is required to read and record the thermometer.
Surely though, a better job at station siting could have been done.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea15855be',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This was forwarded to me by Russ Steele over at NCWatch. Titled: Climate Change – Is CO2 the cause? 
It’s a compelling presentation by Bob Carter of the facts of climate change to a recent public forum in Australia.  He is a research professor in the Marine Geophysical Laboratory at James Cook University, Australia. He’s featuring a “Count the torpedos” element.
My www.surfacestations.org project is highlighted in Part 4. It is gratifying to see my work used by others,
Part1
Part2
Part3
Part4


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3616c34',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I had just finished reading a speech by George W. Bush in which he calls for creating a Compassion Capital Fund whereby the federal government would finance a wide variety of local social initiatives that “worked” when I received a letter from Milton and Rose Friedman asking me to give a speech at the upcoming Vancouver Mont Pelerin Society meeting making the optimistic case for the future of liberty. I didn’t know whether to laugh or to cry. After all, when the leading candidate of the so‐​called market‐​oriented party in the U.S. thinks that federal bureaucrats could or should engage in such activities, one despairs for the future of liberty. But one also doesn’t turn down Milton and Rose Friedman, so here I am.



Which is fine, because the truth be known, I genuinely am optimistic about the future for liberty in the world and, specifically in the United States of America. I apologize in advance, by the way, to our non-U.S. members and guests for the mostly provincial nature of my comments, but I think there is generally a universal applicability to the points I’ll be making. To begin with, what do we mean by liberty? To this audience the definition won’t be as contentious as it is for most of society, because as Hayek points out in the beginning of _The Constitution of Liberty_ , Abraham Lincoln said, “The world has never had a good definition of the word liberty … We all declare for liberty: but in using the same word, we do not mean the same thing.” Which is true, of course, but Hayek typically cuts quickly to the chase, saying we should seek “that condition of men in which coercion of some by others is reduced as much as possible in society.” He contrasts liberty to slavery by saying that liberty involves “the possibility of a person’s acting according to his own decisions and plans, in contrast to the position of one who was irrevocably subject to the will of another.”



By that standard the march of freedom that Milton described on Monday has indeed been impressive. And Milton said something else in his talk that is equally important in terms of assessing the prospects for liberty. He talked about the overwhelming support for socialism and the “tiny pockets of people who believed in freedom” back in 1947 when our Society’s founders met on Mont Pelerin. Everything must be put in perspective. My friend and one of the great champions of liberty in the Twentieth Century, the late Julian Simon, was keenly aware of this fact. Whenever a left‐​wing environmentalist would point to some trend starting in, say, 1985, the first thing Julian would ask was, Why 1985? Why not 1975 or 1875?



In a debate with environmentalist Hazel Henderson in 1996 Julian was confronted with a with a chart purporting to show the decline in pollution levels in London since the passage of London’s Clean Air Act of 1956. In his rebuttal, Julian produced his own chart showing the smoke levels in London dating back to the 1800s and, as reported in a great _Wired_ magazine profile of Simon, “the line from the 1920s on showed a constant and uniform downward slope. ‘If you look at all the data,’ Julian said, ‘you can’t tell that there was a clean‐​air act at any point.’ ”



And so in projecting the prospects for liberty, it’s probably useful to stand back a bit from the Compassion Capital Funds of 1999 and put things a bit more in perspective. In 1772 when there were 775,000,000 people inhabiting the world, it is estimated that only about 33,000,000 of them lived under relatively free governments. Some 95 percent of humanity lived lives described by historian Arthur Young as “miserable slaves of despotic tyrants.” As late as 1848, according to Julian Simon, the share of serfs among the population of Austria was about 72 percent, and in Hungary about 50 percent.



By another measure, worldwide per capita income in 1800 was $100; by 1900 is was about $500; next year it will be about $5000 and by the end of the next century some estimates put it to be in excess of $40,000, or higher than the average Western income of today. It could, of course, turn out to be much greater than even that.



In addition, any long term assessment of human liberty has to take into account the collapse of communism. Hundreds of millions of people today are free from the yoke of communist totalitarianism under which they labored just a decade or so ago. The change has been dramatic, even in Russia, despite all its difficulties. In those nations that have really moved toward capitalism, the past decade has been nothing short of exhilarating. As _Business Week_ noted a couple of months ago, for instance, “Poland has enjoyed brisk economic growth for most of the decade because it chose radical reform, and despite the pain, stuck with it.”



The only remaining communist country of any consequence is China which, as Milton has pointed out, for all it’s human rights failings is nevertheless clearly headed in a capitalist direction. The Associated Press distributed a photo recently of a protester in Tiananmen Square sporting an umbrella painted with the slogan “Privatize. Give all state property to the people.” He was arrested, to be sure, but when such subversive ideas are alive in the land, the end is near for the thugs in Beijing. 



And not all of the Chinese leaders in Beijing are thugs. A couple of years ago in Shanghai Jose Pinera and I met with an individual from Beijing who has been charged with responsibility for creating a public pension system in China. His name is Sun Jianyong and as he approached us in the lobby of the Peace Hotel, Jose remarked, “But you are much younger than I had anticipated.” To which Sun, who appeared to be in his late thirties, replied, “But you were only thirty when you privatized Social Security in Chile.” At the meeting Sun convinced us that he was a great admirer of the Chilean system because of the higher income at retirement, the economic boost from increased savings, and, he said, because it gave people the dignity of not depending on the state for their retirement income. Some communist.



Speaking of the collapse of communism, I think one of the clear indicators of the fact that liberty has the long‐​term momentum today is what Vaclav Klaus and John O’Sullivan talked about on Monday. The so‐​called Third Way. Because, believe me, Bill Clinton, Tony Blair and those other European politicians wouldn’t be adopting that phrase — the Third Way — if socialism wasn’t as thoroughly discredited as it is. They are leftists who are trying desperately to hide that fact from the voters. To a large degree they’ve succeeded. But such deceit won’t be successful over the long haul as it becomes increasingly evident that whatever they call themselves, they always end up promoting more state intrusion into civil society. Bill “The era of big government is over” Clinton, for instance, offered no fewer than 95 new or expanded federal interventions in his January State of the Union Address. The Third Way politicians are trying to sugar‐​coat statism in the rhetoric of free markets and reinventing government, but in the Information Age they will sooner or later — sooner, probably — be exposed for the frauds that they are.



I mentioned the interest in China in setting up a private, individually capitalized pension system. There is, of course, tremendous interest in doing so in the United States also, in large part, because of the work of the Cato Institute, the Heritage Foundation and the National Center for Policy Analysis, each of which, in turn, is indebted to the incredible work of the international Pied Piper of pension reform, our good friend Jose Pinera. Even if we live in an era of Bill Clinton and George W. Bush in the U.S., the fact is that by any objective standard, classical liberal ideas are making remarkable progress in the national policy debate there. Privatizing Social Security is supported by two‐​thirds of the population in the United States, and if you talk to people under 50, it’s nearly unanimous. Men and women, Republicans, Democrats and Independents, union workers, blacks, whites, Asians, and Hispanics all overwhelming favor junking Social Security, the centerpiece of the New Deal. When asked if when a system is set up to allow the purchase of stocks and bonds government or individual workers should be allowed to invest the funds, by a margin of nearly five to one, Americans say individuals should be allowed to invest on their own. They also say the present government‐​run pay‐​as‐​you‐​go system is riskier than the market. This is all from a Zogby International poll we commissioned recently that will be released in a week or so. In looking over those poll results, by the way, I was reminded of a poll from the Pew Research Center last year that asked government officials this question: “Do Americans know enough about issues to form wise opinions about what should be done?” Here are the results. Among members of Congress 31 percent said yes, 47 percent said no. Among Presidential appointees, 13 percent said yes, and 77 percent said no. Civil servants also are disdainful of the American people, with 14 percent saying the public can form wise decisions and a whopping 81 percent saying no, they can’t. This huge gulf between the political class and the people in the U.S., it seems to me, is another cause for optimism.



Getting back to Social Security, it’s true that neither political party has had the courage to call for complete privatization today, but that’s what the people want and it may well turn out to be a decisive issue in next year’s presidential campaign. Dr. Pinera, who’s working with Cato in our efforts in that regard, has already succeeded in bringing some form of privatization to pension systems in no less than eight Latin American countries and, most recently, Poland. To achieve such a thing in the U.S. would not only dramatically change our political dynamics in a very favorable direction from a classical liberal perspective, I believe it would also put tremendous pressure on the European Union and Japan to follow suit. They cannot compete with the U.S. or survive forever with public pension systems that feature unfunded liabilities of two or three hundred percent of GDP.



There are other significant policy gains evident in the U.S. today — which is not to say that we’ve won them — but that progress is clearly being made. Today the education monopoly is under attack as never before. The teachers unions are in rapid retreat, throwing Charter Schools at the snarling masses in the hopes of placating them before they tear down the walls of their monopoly. Ten years ago, not to mention when Milton Friedman first wrote about school vouchers, the unions were impervious to criticism. 



In the area health care, there is a growing understanding that it’s the third party payer system, whether government or first dollar insurance coverage, that’s to blame for bureaucratized and expensive health care in America. Hillary Clinton’s effort to sell essentially the Canadian system as the model for the U.S. broke down when it became common knowledge that people in this country travel south when they have serious health problems, despite the deficiencies of the U.S. system. There is a serious effort underway now to expand Medical Savings Accounts and, indeed, to separate health insurance from employment through equal tax treatment, something a growing number of major corporations in the U.S. now favor. All of this undermines efforts to socialize medicine in the United States.



In other policy fronts, the welfare establishment has never really recovered from the assault on its hegemony by Charles Murray’s _Losing Ground_ and today lives with the reality that welfare is no longer a federal entitlement. Most people clearly understand the counterproductive nature of the dole and are determined to hold their fellow citizens responsible for their own actions, as they did prior to the advent of the paternalistic Great Society programs of the Sixties.



There is also a growing consensus that scrapping the 9000‐​plus page IRS code in the U.S. would be a good thing to do. Tax simplification is something all politicians now must at least pay lip service to. At Cato we frequently have forums on the flat tax or replacing the income tax altogether, not with a VAT, but with a retail sales tax. It is virtually impossible to get a politician or even someone from the IRS to defend the current system at these events. Radical simplification of the tax code not only would be good economically, but it would end the patronizing policies of politicians who now use the tax code to socially engineer citizen behavior. It would also create some taxpayer solidarity in the movement to sharply lower taxation in America. We are making progress in this area, including creating a consensus to abolish both the capital gains tax and the death tax.



Further, trade policy has clearly been on a positive trend in the U.S. for decades. Free traders have won the intellectual battle. The United States today has lower tariffs, as measured by the ratio of tariff income to the value of imports, than at any time in our history. In 1929 with the Smoot‐​Hawley tariff, that number stood at nearly 60 percent. Today it is less than 4 percent. Furthermore, trade and foreign investment income as a percentage of GDP in the U.S. is at an all‐​time high of 30 percent, when as recently as three decades ago it was only 15 percent of GDP. Internationally, a large number of countries ranging from Chile, to Mexico, Argentina, Australia, New Zealand, the transition countries of Central and Eastern Europe and even, to a certain degree, India, are following suit. As, indeed, they must if they’re to survive in the new global economy.



One other positive development in the United States has been a series of court decisions that may portend the end of a very sorry history of jurisprudence in the U.S. dating back to 1937 when Franklin Roosevelt threaten to pack the Supreme Court unless it agreed to ignore its clear constitutional responsibilities and capitulate to FDR’s grand social schemes. Thomas Jefferson once said that “The natural progress of things is for government to gain ground and for liberty to yield.” Kind of an early Public Choice analysis. What Jefferson and most of the American Founders understood so well was that there is an inherent built‐​in bias for the state to expand. The statist imperative, if you will. Without some kind of institutional constraints — in the case of the United States, the Constitution — the majoritarian instinct in a democracy would naturally lead to the tendrils of the state reaching into every corner of civil society.



As they pretty much have since 1937. But, as I say, that all may be changing. The father of the Constitution, James Madison, said that the courts were to be the “bulwark of our liberties” against the inevitable majoritarian onslaught from the two political branches of the national government. In recent years the federal courts have once again started defending property rights, have been firm in support of free speech rights, have challenged Congress not to delegate its power to unelected bureaucrats, and even have resurrected the essence of the Constitution, the Doctrine of Enumerated Powers, whereby if the power is not specifically delegated to the national government it is reserved to the states or to the people. A renaissance of respect for the Constitution, which seems to be taking place in the States, is imperative if the prospects for liberty are to be as positive as they should be.



So, in conventional terms, the prospects for liberty are, if you stand back far enough, pretty bright. But as the people in this room are well aware, there are other forces at work which augur even more brightly for a global future with far less political society and far more civil society. I speak, of course, of the Information Age and the two most dramatic things it brings to society: widespread, diversified and instantaneous access to knowledge, and on the financial side of the ledger, what economist Richard McKenzie accurately calls “quicksilver capital” — the ability of capital to move anywhere in the world with the click of a mouse. An ancillary benefit of the financial revolution that is occurring, and to which Milton briefly referred, is what our colleague Richard Rahn refers to as “the end of money.” There are brochures on his book of the same name at the Cato table outside and I highly recommend that you order a copy of the book, which must be giving central bankers around the world severe cases of heartburn.



At the Cato Institute we prefer to discuss the political battle, that is, man’s relationship to the state, in terms of civil society versus political society, rather than liberal versus conservative or even libertarian. In a civil society you make the choices about your life — how to spend your money, where to send your children to school and so forth — in a political society, based as it is on coercion, somebody else — a politician or a bureaucrat — makes those decisions. The goal, it seems to us, should be to minimize the role of political society consistent with the protection of our individual liberties.



Political society, of course, has historically derived its power from three main sources: Geographical territory, which is to say land; control of the flow and nature of information because knowledge is power; and control over capital flows and the value of a nation’s currency. The Information Age is eating away at those three sources of power just as surely as the sun rises in the East.



Geographical territory and natural resources, as Hong Kong let anyone who was paying attention know decades ago, become increasingly irrelevant with the advent of the new Global Economy made possible by the information revolution in knowledge and finance. Indeed, the computer‐​challenged Soviet Union ended up finding geographical territory a liability in its contest with the information‐​rich West. Let me read to you from a book that’s been on the _New York Times_ bestseller list for four months now. It’s called _The Lexus and the Olive Tree_ and it’s written by the _Times_ ’ chief foreign correspondent, Thomas Friedman. Friedman, I should say, unlike our Friedmans, is a liberal in the bad sense of the word — an Al Gore Democrat. But the first half of the book is really terrific. He refers in the following quote to the “Golden Straitjacket,” by which he means that in order to benefit from the new global economy, nations must play by certain rules. Here’s what he writes:



“To fit into the Golden Straightjacket a country must either adopt, or be seen as moving toward, the following golden rules: making the private sector the primary engine of its economic growth, maintaining a low rate of inflation and price stability, shrinking the size of its state bureaucracy, maintaining as close to a balanced budget as possible, if not a surplus, eliminating or lowering tariffs on imported goods, removing restrictions on foreign investment, getting rid of quotas and domestic monopolies, increasing exports, privatizing state‐​owned industries and utilities, deregulating capital markets, making its currency convertible, opening its industries, stock, and bond markets to direct foreign ownership and investment, deregulating its economy to promote as much domestic competition as possible, eliminating government corruption, subsidies and kickbacks as much as possible, opening its banking and telecommunications systems to private ownership and competition, and allowing its citizens to choose from an array of competing pension options and foreign‐​run pension and mutual funds.…As your country puts on the Golden Straightjacket,” he writes, “two things tend to happen: your economy grows and your politics shrinks.”



Not bad for a liberal Democrat, is it? Before you Americans decide to vote for Al Gore, however, I should point out that the second half of _The Lexus and the Olive Tree_ is truly awful — full of mush‐​minded environmentalism, bleeding‐​heart calls for more funding for the IMF and World Bank, and more taxing of the rich. So, he doesn’t really get it, but Friedman’s analysis of the nature of the new global economy is brilliant. So brilliant, in fact, that I think much of the analysis came directly from Walter Wriston’s wonderful 1991 book, _The Twilight of Sovereignty_. That book, written in anticipation of the Internet, has to be one of the most thoughtful, prescient books of all time. Walt Wriston simply sees things the rest of us can’t.



In it he writes, “Intellectual capital is becoming relatively more important than physical capital. Indeed, the new source of wealth is not material, it is information, knowledge applied to work to create value. The pursuit of wealth is now largely the pursuit of information.” And in competition with the private sector today, government can’t possibly keep up in the pursuit of information. Individuals are being empowered irrespective of borders; irrespective of what politicians have done throughout the sorry history of government domination of society, which is happily coming to an end: The twilight of sovereignty, as Walter puts it.



And as Walter knows so well, one of the great sources of power for the state has been its ability to control capital flows by regulating major financial institutions, among other means. But one of the great aspects of the information revolution has been disintermediation — the decreasing need for the middleman, for major institutions — and the increasing ability of people to deal with one another directly, anywhere in the globe. Consider, for instance, the fact that in 1997 the singer David Bowie raised $55 million in capital based on his projected royalties. The ability of capital markets to securitize virtually any future income flow, combined with the ability of companies to set up operations virtually anywhere on the globe, means that developing nations can expect explosive growth in the next century and that IMF and World Bank bureaucrats can start looking for honest work.



Richard Rahn writes in his book, “The world’s people will be neither truly prosperous nor free unless governments retreat from their seemingly never‐​ending desire to control the production and use of money.” He then goes on to persuasively demonstrate that they have no choice but to give up that control. Private, digital, encrypted money is already a reality and it will become the norm early in the next century. It is likely that those nations that wish to preserve their sovereignty in the future will do so only in a superficial sense, and then only by pursuing policies of very low taxation and free and open trade.



We live in interesting times. When the Agricultural Age turned into the Industrial Age virtually no one was aware of what was happening. But as the Industrial Age turns into the Information Age, by virtue of the age it’s turning into, virtually everyone is aware of it. It’s estimated that by the end of the year 2000 some 100 million Americans will be plugged into the Internet. Some even suggest that the Internet may come to Canada. _Wired_ magazine has dubbed those individuals who participate on the Net, Netizens. In a classic article from 1997 in _Wired_ , Jon Katz wrote, “The Digital Nation constitutes a new social class. Its citizens are young, educated, affluent. They inhabit wired institutions and industries — universities, computer and telecom companies, Wall Street and financial outfits, the media.…and some of their common values are clear: they tend to be libertarian, materialistic, tolerant, rational, technologically adept, disconnected from conventional political organizations — like the Republican or Democratic parties — and from narrow labels like liberal or conservative.…The digital young, from Silicon Valley entrepreneurs to college students, have a nearly universal contempt for government’s ability to work; they think it’s wasteful and clueless. On the Net, government is rarely seen as an instrument of positive change or social good. Politicians are assumed to be manipulative or ill‐​informed, unable to affect reform or find solutions, forced to lie to survive.”



Katz went on to suggest that this Netizen community will fuse technology with politics in such a manner as to advance civil society. I think he’s right. The twilight of sovereignty means the dawning of a new age of liberty and the empowerment of individual choice. The world is moving toward pluralism, capitalism, and civil society. It will take time, but it likely will happen. But it will happen because as the world community grows, as we get to know one another and work with one another around the globe, independent of the political process, civil society will flourish. Increasingly, it will be groups like the Mont Pelerin Society, and not political parties, that lead the way. I’m reminded of that famous quote from the French politician Alexandre Ledru‐​Rollin, who was quoted while among the mob during the Paris revolt of 1848 as saying, “There go the people. I must follow them, for I am their leader.”



Politicians and political society are not the answer. The Mont Pelerin Society, uncountable other voluntary organizations and civil society are the answer. Thus, let me conclude with the same plea my colleague Bill Niskanen made at last year’s meeting in Washington, D.C. We do live in the information age today. There clearly was a time in 1947 and through the Fifties when the idea of secret, off‐​the‐​record talks at MPS made sense, given the intellectual climate of the time. But my goodness, our members win Nobel Prizes now. We should be celebrating the fact that classical liberals from dozens of nations attend these events. We should invite the media in and drop the conceit that we have something to hide, or something for which the outside world is somehow not worthy. The future of liberty does indeed look bright, but it won’t happen automatically. It will require leadership and openness. I trust we can all work together toward that goal. Thank you very much.


"
"
Share this...FacebookTwitter


Pachauri CCX advisory board member

EIKE, the European Institute for Climate and Energy, a sponsor of the 4th ICCC in Chicago, has dug up a some information on the Chicago Climate Exchange CCX, where Maurice Strong and a host of other influential leftists are among its board members.

When it comes to cap & trade, i.e. emissions trading, we’re talking serious money here. According to Dr. Richard Sandor, an economist and CCX architect, cap-and-trade represents a $10 trillion per-year market.
Recognizing the enormous profit potential, Al Gore’s Generation Investment Management (GIM)  purchased a 10 percent stake in CCX and became the company’s fifth largest co-owner. Yet all this coziness shouldn’t surprise anyone.
But taking a closer look at the CCX Advisory Board, we find among the likes of Joe Kennedy, Ed Begly, Thomas Lovejoy – Rajendra Pachauri. It’s known that Pachauri has huge investments in carbon markets, but this is ridiculous. Talk about a scam. Yet another gate to add to the long list.
Conflict of interest? Nahhhh
Update: FYI, CCX Directors here and External Advisory Board members here.
Share this...FacebookTwitter "
"
There’s a story at the Telegraph UK from Christopher Booker where he states “…the latest US satellite figures showing temperatures having fallen since 1998, declining in 2007 to a 1983 level…”
Wanting to make sure that there was some data to reference this claim for my readers, I’ve presented some graphs of satellite microwave sounder data below.
MSU data are produced by Remote Sensing Systems and sponsored by the NOAA Climate and Global Change Program. Data are available at www.remss.com 
Below are the trend graphs for the data since 1979. Note that these graphs are multi channel, which represent different microwave sounder wavelength channels from the spacecraft. These channels represent different measured levels of the atmosphere.
Channel. TLT – Lower Troposphere
 
Channel TMT – Middle Troposphere

Channel TTS – Troposphere and Stratosphere combined

Channel TLS – Lower Stratosphere

Above: Global, monthly time series of brightness temperature anomaly for channels TLT, TMT, TTS, and TLS.
All matter emits microwave radiation that varies with its temperature, among other factors. Microwave sensors on weather satellites can take more than 60,000 temperature measurements of oxygen in the atmosphere, from the surface to about 10 km (6 mi) altitude.
NOAA and it’s affiliated researchers have compiled almost three decades of data showing how atmospheric temperature has behaved over the entire globe.  At UAH (University of Alabama, Huntsville) where Dr. John Christy and Dr. Roy Spencer have been keeping watch on this trend for some time as well, they have tabular data online should you care to plot it. Here is an ongoing history of the data. You can see some of their other work here.
For Channel TLT (Lower Troposphere) and Channel TMT (Middle Troposphere), the anomaly time series is dominated by ENSO events and slow tropospheric warming. The three primary El Niños during the past 20 years are clearly evident as peaks in the time series occurring during 1982-83, 1987-88, and 1997-98, with the most recent one being the largest. Channel TLS (Lower Stratosphere) is dominated by stratospheric cooling, punctuated by dramatic warming events caused by the eruptions of El Chichón (1982) and Mt Pinatubo (1991). Channel TTS (Troposphere + Stratosphere combined) shows a mixture of both effects.
Temperatures in the lower troposphere (for non weather geeks, that is the portion of the atmosphere where we live) have shown a series of ups and downs since 1979, mostly in a ±0.4oC band, with negligible trends over that period. This contrasts with the near surface temperature record that shows a warming during the same period of time. The graph below is from Wikipedia.

Note in the TLT graph above, the strong 1997-98 El Niño event caused significant lower tropospheric warming in late 1997, and record warmth in February 1998 as evidenced by the spikes shown in the TLT, TMT, and TTS graphs above.
Satellite measurements of the lower stratosphere (TLS) reveal two marked warm periods (as much as 1.5oC warmer), caused by sulfuric acid aerosols deposited in this layer by the eruptions of El Chichón in 1982 and Pinatubo in 1991.
These two warm periods are concurrent with a strong cooling trend over the 19-year period that has been attributed to ozone depletion in the lower stratosphere. In 1997, record low stratospheric temperatures were recorded.
On the TLT graph, for the years 1998 to present, there appears to be a slight downward trend in lower stratospheric temperature, and this is what I believe Christopher Booker is referencing in his article in the Telegraph.  Note that there have been other downward trends in the nearly 30 year measurement history, but the overall trend in the TLT, TMT, and TTS channels has been positive, so a short downward trend doesn’t necessarily prove anything. The TLS channel shows a negative trend, and along with the ozone depletion factor, indicates that we aren’t getting much heat transport from the troposphere into the stratosphere.
The real question is whether this small downturn in the tropospheric temperature trend is a short term anomaly, or something indicative of a longer term event. Only time will tell.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2a961b0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
While I was on my week long road trip to survey weather stations and visit the National Climatic Data Center in Asheville, NC last week, I encountered lots of signs. Restaurant signs, road signs, signs from above, you name it. I think I must have passed 100 Bojangles restaurants and/or road signs. Bojangles is a popular southern chicken and biscuits restaurant. One though, really got my attention.

But please read on for the real “mother of all signs” I encountered.

Then there was Cracker Barrel and Waffle House…

Cracker barrel has an interesting marketing slogan on their supply trucks:

Yeah, I drove some of those….
On good advice from my readers, I avoided every one of these I saw:

This place is the southern version of Bob’s Big Boy and Frisch’s, good eats and a great breakfast bar. I didn’t try the wine though.

There were some other restaurant signs that I didn’t quite understand…

And there were some signs that I often wished I had for use when moderating this blog:

Then there were some signs that really spoke to the mission I was on:

And then there were others that I encountered that didn’t have a hint of southern hospitality at all…

I saw a lot of these at gas pumps, and given how I feel about biofuels, I drove to the next stations where I didn’t have to burn food to finish my trip:

I had mentioned that after surveying too many weather stations at sewage treatment plants that I needed a long shower, but when I saw this while I briefly toyed with the idea, I just didn’t see how it would change anything. I’d just be trading one smell for another.

There was one sign though that left a lasting impression on me, and it requires just a little bit of explanation.
When I was driving in Northwestern North Carolina, I went through many small towns and country roads that had small churches, I must have passed 200 during my trip. One thing I noticed is that pastors in these towns tend to try to out do each other with sermon topics on their front signs. I’d drive into a little town, and I’d see one sign advertising salvation, the next would have salvation plus breakfast, the next might have salvation, confession, a quote from the Bible, and a spaghetti feed, while the fourth might just have a zinger that would put all the others to shame.
This was one of those:

Now a caveat, the mountain road I was traveling on when I saw this had no shoulder and there was a semi truck right behind me. I looked for a place to pull over and turn around, and didn’t see one coming up, I couldn’t even pull over to let the truck pass. Five miles later I was still stuck and gave up on the idea.
Today I Google image searched to see if I could find an image where I could recreate the message, and found this neat web site called the Church Sign Generator, the output of which you see above. The sign and message was real, and didn’t look much different from this example except the top had something about Sunday’s service which I didn’t include because I didn’t get a good look at it.
We should all take this message home with us and live it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9cc70b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Just two days after sunspot 983 was reported, it has now disappeared. They just aren’t sticking around like they used to. This is yet another indication of the bottomed out solar minima we are in.
It will be very interesting to see if the cycle 24 predictions by Hathway at NASA for an even stronger cycle will materialize.

Though there does seem to be more discussion of a weak cycle 24 than a strong one as of late. Personally, I think this graph of Average Planetary magnetic index (Ap) is quite telling in the step that occurred in 2005. From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below:

click for a larger image
What is most interesting about the Geomagnetic Average Planetary Index graph above is what happened around October 2005. Notice the sharp drop in the magnetic index and the continuance at low levels.
From this story on space.com where they talk about the opposing views solar scientists have for cycle 24 they offer some opinions. NOAA Space Environment Center scientist Douglas Biesecker, who chaired the panel, said in a statement:
 […] despite the panel’s division on the Sun cycle’s intensity, all members have a high confidence that the season will begin in March 2008.
We shall soon see if they are correct, March starts this Saturday.
Nature will truly be the final arbiter of this argument.
UPDATE: Jeff C writes
I thought you might find this chart interesting.  Since sunspot cycles overlap and there is no clear start/stop, the “start” of the new solar cycle is usually defined as the smoothed sunspot minimum between cycles (as opposed to the appearance of the first reversed-polarity spot). Although different definitions are sometimes used, this seems to be the most common and accepted variation.
The enclosed chart shows the transition from cycle 22 to cycle 23 back in 1996.  It is interesting how the first new cycle sunspots appeared over a year before the commonly accepted May 1996 start date of the new
cycle.
I’m unsure of the cycle start date definition used by Douglas Biesecker, but if it is the commonly accepted definition, he will be way off.  It will be interesting to see if they claim the appearance of a few reversed cycle sunspots count as a “start”.  If so, then cycle 23 actually started back in March 1995 and is 13 years old.

Click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0b64fad',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 A new paper published  by the Astronomical Society of Australia titled:
Does a Spin–Orbit Coupling Between the Sun and the Jovian Planets Govern the  Solar Cycle?
contains a warning about earthly climate change not immediately obvious from the abstract:

Based on our claim that changes in the Sun’s equatorial rotation rate are  synchronized with changes in the Sun’s orbital motion about the barycentre, we  propose that the mean period for the Sun’s meridional flow is set by a Synodic  resonance between the flow period (~22.3 yr), the overall 178.7-yr repetition  period for the solar orbital motion, and the 19.86-yr synodic period of Jupiter  and Saturn.
According to an interview with Andrew Bolt, of the Australian Newspaper, Herald Sun, Ian Wilson, one of the authors explained:
It supports the contention that the level of activity on the Sun will  significantly diminish sometime in the next decade and remain low for about 20 –  30 years. On each occasion that the Sun has done this in the past the World’s  mean temperature has dropped by ~ 1 – 2 C. 
###
Hmmm, I’m not sold on this idea. This is a lot like what Dr. Theodor Landscheidt proposes. I have a little bit of trouble understanding how the “mass at a distance” gravitational effects of Jupiter and Saturn could have much effect on the solar dynamo.

I’m sure both my readers, and Dr. Leif Svalgaard, who regularly monitors this blog, will have something to add to provide additional insight. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e396d70',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Preparations for this year’s climate summit in Glasgow are being overshadowed by a bitter row between the UK and Scottish governments over a key building near the venue. UK government sources have accused Scottish ministers of refusing to hand over a building the Scottish government wants to use as its base for the COP 26 climate talks in November.  Scottish ministers say they booked the Glasgow science centre, a publicly funded venue on the opposite bank of the Clyde from the main conference site, only after the UK government said it would be outside the summit security zone last November. The dispute deepened after Claire O’Neill, the former minister who was sacked as the summit’s chair by Boris Johnson last week, alleged the two governments “were in an extraordinary state of standoff” over the event. O’Neill told BBC Radio 4’s Today programme that she had been told the Scottish government had “behaved disgracefully” by booking buildings on the summit site – a claim rejected by Scottish government officials and independent sources. In a letter to the prime minister published by the FT, O’Neill said that Johnson was even considering moving the event to an English city because of “ballooning costs”. That claim was rejected by a Whitehall source who said there was “zero chance” of it being relocated. O’Neill told Today that the “playground politics, the yah boo of this, has to stop”. She suggested to Johnson that Nicola Sturgeon, the first minister of Scotland, be given a key role at the summit which “the prime minister heartily and saltily rebutted.” Spurred on by O’Neill’s disclosure, Sturgeon wrote to Johnson on Tuesday afternoon asking him to allow Scotland’s environment and climate secretary, Roseanna Cunningham, to take part in climate change ministerial meetings that he has promised to chair in London. Johnson is very likely to reject that request. At the Conservative’s conference in October, he told a raucous Scottish Tory fringe event that he did not want Sturgeon “anywhere near” the climate summit. The spat deepened after a Whitehall source said that the Scottish government had failed to reply to a formal request from Michael Gove in mid-January to relinquish use of the centre and hand it to the climate summit secretariat. A spokesman for Sturgeon insisted her government was willing to discuss how the centre could be used. Ministers in Edinburgh were open to the idea of sharing it, he said, but refused to confirm that they would vacate the building entirely. “It’s a mischaracterisation of our position to suggest we’ve been behaving in anything other than a responsible or cooperative way over the organisation of COP 26. That’s the entire basis on which we are taking things forward. It really shouldn’t be a party-political spat,” he said. Other sources said that the Foreign Office had considered using the science centre last year but failed to book it in time. The event is expected to be the largest gathering of foreign leaders, diplomats and dignitaries ever hosted by the UK, dwarfing previous global summits. Up to 30,000 people are expected to attend, including 200 world leaders. Police Scotland’s chief constable said last month that the force estimated policing the event could cost more than £200m, increasing tensions between the two administrations. The Scottish government said on Tuesday the UK government should foot the entire bill – a position rejected by the UK government. The Whitehall source said there were well-understood processes for setting and sharing costs in situations like this. The UK government believed last year the main Scottish Events Campus (SEC), which includes the SEC Hydro venue, the Scottish exhibition centre, the Armadillo theatre and the Crown Plaza hotel, would be enough. They now argue the summit should expand across the Clyde to include the science centre, which sits opposite BBC Scotland’s headquarters at Pacific Quay. Environment and climate justice campaigners also approached the science centre to use it as a base for their alternative summit but were told the Scottish government, which is the centre’s largest funder, had already taken it."
nan
"

As we approach the end of the year, it is appropriate to give you an update on Cato. Fortunately, Cato is doing very well and has positive momentum on all fronts, although there are always opportunities for improvement.



Commentators on both the left and the right are discussing the “libertarian moment.” While it’s obvious we do not have the current president’s ear, libertarian ideas are being taken very seriously, in part as the result of 37 years of Cato scholarship. There is a rapidly rising libertarian student movement where Cato has played a critical role in training future student leaders and providing the intellectual ammunition for students to challenge their left‐​leaning professors.



The Cato brand has also been rising. As a number of our fellow think tanks have become visibly politically partisan, we have maintained a reputation for objectivity in a politically charged environment. Of course, since we have plenty of disagreements with both Republicans and Democrats, this is relatively simple to do.



Cato’s financial position is strong, with our second year of significant increases in contributions. Thank you! However, our budget is tiny compared to the billions of dollars in resources controlled by the statists. We have many opportunities to use additional funding to productively communicate the libertarian message. As we approach the end of the year, I hope you will put Cato high on your list for financial support so we can maintain and accelerate the libertarian momentum.



The list of Cato’s recent accomplishments is impressive. Largely because of a great effort by Cato’s Michael Cannon, the court battle to undo Obamacare continues, with two recent victories in an uphill fight. Cato’s Constitutional Studies team’s record was 10–1 for amicus briefs filed at the Supreme Court this past session and 15–3 for the previous session. This is the result of over 30 years of effort to persuade the Court to limit Congress to its constitutionally enumerated powers.



Since 1999 Cato has been discussing the danger of police militarization. Finally, the public is becoming aware of this issue and its threat to innocent citizens (bring back Barney Fife). The Herbert A. Stiefel Center for Trade Policy Studies has started an invitational dinner program for business and government leaders to discuss trade liberalization and its many benefits. Participants have included Eric Schmidt, executive chairman of Google; Russ Girling, president and CEO of TransCanada Corporation; Greg Page, executive chairman of Cargill; and former U.S. trade representative Susan Schwab.



Harvard’s Jeff Miron, who has joined Cato as director of economic studies, has commissioned a series of economic briefs from top scholars. The scholars are typically not libertarian, but their research supports libertarian policy positions. The willingness of these high‐​level scholars to publish under the Cato brand is very encouraging.



We also took our ideas into the world’s hot spots this year: we cosponsored a conference in Ukraine with approximately 500 policymakers, businessmen, and journalists, as well as a student conference for freedom in Venezuela.



In the 12 months ending in August, Cato’s scholars were mentioned in 5,622 news stories, published 849 opeds, and made 442 major broadcast appearances. We published approximately 100 academically credible articles; held 30 conferences, 38 policy forums, 41 book forums, and 14 Capitol Hill briefings; and testified 18 times before congressional committees. Over 1,300 students participated in various Cato educational programs and our scholars made many presentations to student organizations such as Students for Liberty, Young Americans for Liberty, and the Federalist Society.



We just launched the Cato Center for Monetary and Financial Alternatives, which is the first serious challenge to the Federal Reserve in its 100‐​year history. The quality of scholars and business leaders who have agreed to participate is extraordinary and reflects the increasing concern by even mainstream economists about both the Fed’s monetary and regulatory policies.



The Cato Center for the Study of Science is growing rapidly to take on the increasingly politicized climatechange movement. Climate change is the religion of the Progressives and as their models have continued to fail they have become more irrational in defending their detached‐​from‐​reality positions.



We have launched a campaign to help independent thinkers truly grasp that big government is failing across the board. We are in the process of hiring two civilliberties scholars to take on the NSA, the IRS, and the many other agencies and policies (including the drug war) that are a serious threat to your individual liberties.



We are a voice of reason in an increasingly politicized and irrational environment. Cato is committed to creating a free and prosperous society based on the principles of individual liberty, free markets, limited government, and peace — a noble endeavor. Your support makes our work possible.
"
"
A guest post by David Smith


Recently I completed my tenth survey for Surfacestations.org. These surveys  are fun, almost like treasure hunts where the clues are good but not always  great, thus requiring some ingenuity. Also, the surveyor gets to see areas which  may otherwise never be visited. And, they’re for a good cause.
While I found no “poster-child” poor quality sites I did observe an array of  siting problems. Some thermometers were near the drip-lines of trees, some next  to buildings, one was near a concrete patio, one at a sewage plant, several sat  above poorly-drained soil and so forth.
These conditions are less than ideal, obviously. Perhaps more importantly,  these conditions can change over time. Trees and shrubs grow and die, ground  cover changes, concrete is added (and tends to darken over time), drainage may  improve or deteriorate, fences and other construction are added or removed, and  so forth. Each of these can subtly change the local temperature, a situation  which is especially important if one is looking for changes of a fraction of a  degree.
To what extent do these imperfections affect local temperature?  Well, we  really don’t know (or if anyone knows they’re not talking!).
So, to make a small and imperfect step in that direction, I’m running a few  local experiments. My goal is to examine, at least qualitatively, how local  microclimate factors like trees and concrete affect temperature. As you’ll see,  my methods are too crude to allow fractions of a degree determinations but I  should be able to quantify the magnitudes of the impacts of trees, concrete,  etc. Or at least that is my goal.
First, my instruments:

I’m using several temperature detector/recorders (”USB1″) like the gray  object shown in the photo. These electronic devices measure and log  the temperature to the nearest degree F and allow sampling on various schedules.  I use 30-minute sampling.
Note: Interested readers can buy these at:
http://www.weathershop.com/USB1_temperature_logger.htm
At this point I’m testing the hardware and developing my experimental  plan. But, I have made a few (literally) backyard tests and I’d like to share  one of those. This is to help illustrate the approach and, I hope, stimulate  helpful comments from other readers.
This initial run (sort of a beta test) was made in my backyard. It involved  two extremes. One is near my garage, above a dark-soil flower bed and landscape  bricks. This is near a wooden deck and walkway gravel. This spot gets direct  sunlight about 50% of the day.
The second extreme is deep shade, beneath low-tree (crepe myrtle) cover and  above thick, semi-tropical shrubbery.This is about twenty feet from sunlight. A  photo of the backyard is below, with red boxes marking the two locations:

I also use the temperature readings from an airport/airbase located four  miles west of my house. This airport provides professional-grade open-field  temperature readings which should reasonably approximate regional ambient  conditions.
A representative backyard temperature time series is below:

This shows pretty good agreement between the deep-shade max/min and the local  airport open-field max/min, which frankly surprised me. I’d expected the  deep-shade readings to show less variability (lower highs and higher lows).
More importantly is the contrast between #1 (sunlight and plant beds) and #2  (deep shade). The #1 spot stayed 5 to 10F hotter at midday than #2 (deep  shade) less than 50 feet away (and, as a matter of fact, #1 was 5 to 10 F warmer  than the high-quality nearby airport).
Why does this matter? well, suppose a co-op station had slowly drifted, over  several decades, from open-field conditions to those found at site #1. What  would that do to the apparent trend?  That’s an important question which is at  the heart of the surfacestation effort.
This backyard demonstration involved convoluted conditions. There is little  chance to untangle the relative contributions of so many variables (bricks,  soil, tomato plants, trees, etc). So, my plan is to reduce the number of  variables in the tests such that we might be able to make broad conclusions  about the relative impacts of trees, concrete, drainage and other factors which  may change over time.
This should be fun! Suggestions welcome.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fa983f4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"



Earlier this week, the _New York Post_ published articles containing information about alleged emails between Hunter Biden, the son of Democratic presidential nominee Joe Biden, and employees at Chinese and Ukrainian energy firms. Twitter and Facebook both took steps to limit the spread of the articles, prompting accusations of “election interference.” Prominent Republican lawmakers took to social media to condemn Twitter’s and Facebook’s decisions. These accusations and condemnations reveal a misunderstanding of policy that could result in dramatic changes to online speech.



According to Twitter, the company restricted access to the _New York Post_ ’s articles because it violated the company’s policies against spreading personal and private information (such as email addresses and phone numbers) and hacked materials. Twitter cited the same policy when it prohibited users from sharing 269GB of leaked police files. Twitter users who click on links to the two _Post_ articles face a click‐​though “this link may be unsafe” warning. The articles in question include such information in images of the leaked emails. Those accusing Twitter of a double standard because the company allows users to share the recent _New York Times_ article based on the president’s leaked tax documents neglect the fact that the _New York Times_ did not publish images of the documents. Although consistent with Twitter’s policies, the decision to block the spread of the _Post_ ’s articles on Twitter absent an explanation or context was criticized by Twitter CEO Jack Dorsey.



According to a Facebook spokesperson, Facebook’s decision to restrict the spread of the _Post_ ’s Hunter Biden articles is “part of [Facebook’s] standard process to reduce the spread of misinformation.” Compared to Twitter’s response, Facebook’s was less clear.



Whatever one thinks about Twitter’s and Facebook’s decisions in this case the decisions were legal and consistent with Section 230 of the Communications Decency Act. Much of the online commentary surrounding restrictions on the _New York Post_ (head over to #Section230 on Twitter to take a look for yourself) makes reference to a non‐​existent “publisher” v. “platform” distinction in the law.



In brief, Section 230 states that interactive computer services (such as Twitter, the _New York Time_ ’s comments section, Amazon, etc.) cannot — with some very limited exceptions — be considered the publisher of the vast majority of third‐​party content. Twitter is not the publisher of your tweets, but it is the publisher of its own content, such as the warning that appears when users click on the two _New York Post_ article links. Section 230 applies to “platforms” and “publishers,” and does not prevent social media sites from fact‐​checking, removing, or limiting access to links.



Some “Big Tech” critics decided not to focus on Section 230 and instead focus on election interference. The conservative outlet The Federalist issued a statement making this claim, as did many others. According to those making the “election interference” claim, the _New York Post_ articles are embarrassing to Joe Biden, and Twitter’s and Facebook’s actions constitute a pro‐​Biden interference in the 2020 presidential election. Conservative pundits are not the only ones making this kind of claim. Senator Joshua Hawley (R-MO) wrote to Dorsey asking him to appear at a hearing titled “Digital Platforms and Election Interference.” Sen. Ted Cruz (R-TX)  wrote to Dorsey accusing Twitter of trying to influence the upcoming election. Later he accused Twitter of election interference and supported the Senate Judiciary Committee issuing a subpoena to Dorsey, which is expected to happen this coming Tuesday.  
  
It is one thing for conservative pundits to accuse a private company of interfering in an election. In today’s political climate it is expected. What should send chills down the spine of everyone who values the freedom of speech and the freedom of association is the sight of two of the most powerful politicians in the country making the same accusation and insisting that Twitter’s CEO appear before a hearing and hand over documents related to how Twitter conducts its business.  
  
To portray how Twitter and Facebook handled the _New York Post_ articles as “election interference” has significant implications. Twitter and Facebook limited access to an article that is potentially embarrassing to a political candidate. If such actions can be considered “election interference,” should every content moderation action by a private company taken against any politician or candidate be considered interference? If _The Wall Street Journal_ rejects an op‐​ed written by the Green Party’s presidential candidate is not that also “election interference”? When a music hall owner decides to allow the Trump campaign, but not the Biden campaign, to host a rally is that not “election interference”?  
  
“Election interference” is a term that ought to mean something useful. Unfortunately, conservative commentators seem intent on warping the term so that it means little more than, “moderating content.”



So‐​called “Big Tech” and content moderation will continue to make headlines next year regardless of who wins the presidential election next month. While conservative commentators and activists are convinced that “Big Tech” is engaged in an anti‐​conservative crusade, they should consider that the political left has its own complaints. Bipartisan anger towards Big Tech could result in Section 230 reform or other legislation that puts the freedom of speech and freedom of association at risk. As lawmakers continue to criticize the most prominent social media companies we should remember that attempts to regulate online speech could have disastrous consequences.
"
"

The purpose of this report is to provide a framework for doing research on the problem of bias in science, especially bias induced by Federal funding of research. In recent years the issue of bias in science has come under increasing scrutiny, including within the scientific community. Much of this scrutiny is focused on the potential for bias induced by the commercial funding of research. However, relatively little attention has been given to the potential role of Federal funding in fostering bias. The research question is clear: does biased funding skew research in a preferred direction, one that supports an agency mission, policy or paradigm?



Federal agencies spend many billion dollars a year on scientific research. Most of this is directly tied to the funding agency mission and existing policies. The issue is whether these financial ties lead to bias in favor of the existing policies, as well as to promoting new policies. Is the government buying science or support?



 **Our working definition of “funding‐​induced bias” is any scientific activity where the prospect of funding influences the result in a way that benefits the funder.**



While the basic concept of funding‐​induced bias is simple, the potential forms that this bias might take are far from simple. Science is a complex social system and funding is a major driver. In order to facilitate research into Federal funding and bias it is necessary to isolate specific kinds of bias. Thus the framework presented here is a taxonomy of funding‐​induced bias.



For the purposes of future research the concept of funding‐​induced bias is analyzed in the following ways:



1) the practices in science where bias can occur,  
2) how agency policy can create bias,  
3) the level at which bias is fostered, and  
4) indicators of bias.



Fifteen different scientific practices are analyzed, ranging from the budgeting and funding for research to the publishing and communication of results. For each of the fifteen practices there is a snapshot of the existing research literature on bias, plus a brief discussion of the directions that new research might take in looking into funding‐​induced bias. The potential for quantifying the extent of bias is also addressed.



In order to provide examples along the way there is a special focus on climate change. Federal policies on climate change and federal funding of climate research are both extensive. The linkage between these policies and research has become a major topic of discussion, including numerous allegations of bias.



The research framework provided here applies to the study of all funding‐​induced bias in science, not just to climate change science. The linkages between Federal policy and federally funded science are extensive and not well understood. Moreover, these linkages have policy implications, especially if they are inducing bias in scientific research. However, policy is not our topic here. Rather we are addressing the needed research that might lead to new policies.



In this report we are mostly concerned with individual types of funding induced bias. But there is an intrinsic sequence to the various biases we have identified and this raises the possibility of cascading amplification. By amplification we mean one biased activity is followed by another, such that the first bias is increased.   
  
A simple, and perhaps common, example of amplification might be when the hype in a press release is exaggerated in a news story. Let’s say the press release overstates the importance of the research result, but with some qualification. The news story then reports the result as a great breakthrough, far more strongly than the press release, ignoring the latter’s qualifications. In this way the original bias has been amplified. 



Cascading amplification when one biased activity is followed by multiple instances of amplification. Using our example, suppose a single biased press release generates many different news stories, which vie with one another for exaggeration. This one‐​to‐​many amplification is properly termed a cascade.



Moreover, there is the possibility of cascading amplification on a very large scale and over multiple biased stages. Here is an example of how it might work.



1) An agency receives biased funding for research from Congress.



2) They issue multiple biased Requests for Proposals (RFPs), and



3) multiple biased projects are selected for each RFP.



4) Many projects produce multiple biased articles, press releases, etc, 



5) many of these articles and releases generate multiple biased news stories, and



6) the resulting amplified bias is communicated to the public on a large scale.



One can see how in this instance a single funding activity, the agency budget, might eventually lead to hundreds or thousands of hyperbolic news stories. This would be a very large scale cascading amplification of funding‐​induced bias.



 _Climate Change Examples_



In the climate change debate there have been allegations of bias at each of the stages described above. Taken together this suggests the possibility that just such a large scale amplifying cascade has occurred or is occurring. Systematic research is needed to determine if this is actually the case. 



The notion of cascading systemic bias, induced by government funding, does not appear to have been studied much. This may be a big gap in research on science. Moreover, if this sort of bias is indeed widespread then there are serious implications for new policies, both at the Federal level and within the scientific community itself.
"
"Barclays is facing a fresh revolt from the UK’s most powerful investor group amid mounting concerns over its role as the biggest European financier of fossil fuel companies. The Investor Forum, which holds £18.5tn in assets and represents Britain’s largest investors, is understood to be pressing Barclays to adopt stricter policies on climate change before the bank’s annual shareholder meeting in May.  The bank is already facing criticism from a separate group of 11 pension and investment funds managing more than £130bn of assets. Spearheaded by the campaign group ShareAction, they have filed a resolution calling for Barclays to set clear targets to phase out services to energy companies that fail to align with Paris climate goals. But the reported intervention of the Investor Forum, whose members include all the big names in the UK fund management industry, will intensify the pressure on the bank. The Investor Forum declined to comment. Barclays is being targeted because of its leading role in funding fossil fuel projects. A recent study found that its lending and underwriting to carbon-intensive companies and projects between 2015 and 2018 totalled $85bn (£64bn). The sum is more than any other British or European bank, and prompted campaign group ShareAction to coordinate the resolution tabled for May’s AGM. Barclays said: “We continue to engage with ShareAction and other stakeholders on this issue and will make a further statement at the appropriate time.” Privately, the bank believes the campaign against it is unfair, pointing to figures that show that while it remains a significant banker to the fossil fuel industry, its business with the companies most aggressively expanding in the sector fell sharply, from $13.1bn in 2016 to $5.2bn in 2018. It has also facilitated £27.3bn in “green” bonds and renewable financing. Unlike some European banks, Barclays has not ruled out funding projects or companies involved in coal or tar sands, regarded as particularly polluting as they require large amounts of energy to extract. Barclays has faced protests in its branches from Greenpeace campaigners over its involvement in tar sands, focusing on the pipelines being built across Canada and the US to bring oil to market from Alberta’s tar sands. If the AGM resolution is approved by shareholders, the bank will have to publish a plan to phase out the provision of financial services to energy companies that are not meeting Paris climate goals.  It is understood that since the pension and investment funds filed their resolution, the Barclays chairman, Nigel Higgins, has engaged with institutional investors. The Financial Times suggests the bank is considering putting forward its own climate change plan to stave off the investor revolt. Barclays has a way to go before it meets the standards set by some other European financial giants. The insurer Axa said in 2017 that it was divesting from 25 tar sands companies as well as the three large pipelines needed to deliver their oil to market. BNP Paribas, one of the biggest banks in France, has also pledged to stop financing companies whose main activity is extracting oil and gas from shale deposits or tar sands. 1. JP Morgan Chase $195.6bn 2. Wells Fargo $151.6bn 3. Citi $129.5bn 4. Bank of America $106.7bn 5. RBC (Canada) $100.5bn 6. Barclays $85.2bn 7. MUFG (Japan) $80.1bn 8. TD (Toronto Dominion) $74.1bn 9. ScotiaBank $69.6bn 10. Mizuho (Japan) $67.7bn Source: Fossil Fuel Finance Report Card 2019"
"The unintended consequences of our agricultural food system – polluted air and water, dead zones in coastal seas, soil erosion – have profound implications for human health and the environment. So more sustainable agricultural practices are needed as soon as possible.  Some farmers have turned to less chemically-intensive techniques to reduce the negative impact of agriculture, such as organic farming, which has been shown to outperform conventional farming by many standards of environmental sustainability. The question is whether we can meet these environmental standards and still meet the demand for food, which is predicted to rise substantially in the next 50 years. In our new study, published in Proceedings of the Royal Society B, we found that organic farming systems, when done right, come close to matching the productivity of conventional systems.  Designing a single experiment that could possibly represent the huge variation in crops, weather and soil necessary to get a complete answer is impossible. Instead, we examined the many specific studies that have already been conducted and combined their results – a meta-analysis. We compiled studies from across the globe that compared organic and conventional yields over three decades, representing more than 1,000 comparisons of 52 crop species from 38 countries. This isn’t the first time researchers have attempted to answer this question, but previous studies have had conflicting results. Combining studies carried out by different scientists for different reasons is a big challenge. Depending on what data is included and how it is handled, answers can vary substantially. Many previous studies found organic yields were 8-25% lower than conventional systems. Another study found that organic farming outperformed conventional in developing countries. In revisiting this question, we used the most extensive dataset to date and methods that try to account for the complexity of the data. We found that although organic crop yields are about 19% lower than conventional yields, certain management practises appear to significantly reduce this gap. In fact, planting multiple different crops at the same time (polyculture) and planting a sequence of crops (crop rotation) on an organic farm cut the difference in yield in half. Interestingly, both these practices are based on techniques that mimic natural systems, and have been practised for thousands of years. Our study strongly suggests that we can develop highly productive organic farming methods if we mimic nature by creating ecologically diverse farms that draw strength from natural interactions between species. Crop rotation and polycultures are known to improve soil health and reduce pest pressure. Because these practices add diversity to the landscape they also support biodiversity, so they may improve yields while also protecting the environment. We also found that for some crops such as oats, tomatoes and apples there were no differences in yield between organic and industrial farming at all. The largest yield gaps were found in two cereal crops, wheat and barley. However, since the agricultural Green Revolution in the mid-20th century, improving the yields of cereals grown using conventional, industrial agriculture has received a huge amount of research and funding – far more than organic agriculture. Little wonder, then, that we see a large difference in yields. For example, some seeds are specifically bred to work well in the nutrient-rich, pest-free conditions found in conventional farms due to the heavy use of fertilisers and pesticides, so they may underperform in organic farms. But if we invested in organic agricultural research and development we’d no doubt see a large increase in the yield too. We also found evidence that the yield gap estimate we and others have calculated is likely an overestimate. We found evidence of bias in the studies we compiled, which favoured the reporting of higher conventional yields relative to organic. This can arise for several reasons: the studies can favour specific crops or practices so that the results are unrepresentative, or introduce bias during the selection of results to be published. It’s impossible to know the origins of the bias, but it’s necessary to acknowledge the effect it will have on yield estimates. It’s important to remember that simply growing more food is not enough to address the twin crises of hunger and obesity. Current global food production already greatly exceeds what is needed to feed the world’s population, yet social, political, and economic factors prevent many people from living well-fed, healthy lives. A focus solely on increased yields will not solve the problem of world hunger.  To put the yield gap into context, the world’s food waste alone is 30-40% of food production per year. If food waste were cut by half, this would more than compensate for the difference in yield from converting to organic agriculture, as well as greatly reducing the environmental impact of agriculture."
"
Share this...FacebookTwitterI know I shouldn’t go here, but the temptation is just too strong. I’ll let the readers make up their own minds. Here are some links to read. But do research more.UPDATE 1: Oh! Oh! Drudge has got Gore as the big headline.
1. Serious accusations
2. 3 reasons not to believe the accusations 
3. $540 massage(?) at ritzy hotel
4. Gore assault NYT
5. FULL POLICE REPORT
This eventually will boil down to the question: Can we really trust Al Gore? We saw how far he took the level of propaganda in his AIT film, which was carefully crafted to pull at the heart-strings and to mislead viewers. It was slammed by a British High Court. His jet-setting, mansion-buying lifestyle is in complete contradiction to what he preaches. He constantly ducks debate and keeps his head in the sand.
Not long ago he separated from his wife, indicating possible breach of trust in the relationship. He once claimed to have invented the internet. Just how believable is this guy?
My personal opinion is that Gore is as great a fraud as one will ever find, and he’s living high on the hog because of it. But that’s just my opinion, which is based on the so many words that have come out of his mouth and on his actions.
Indeed this has the potential to be much bigger than Climategate.
“But what does that have to do with the science,” one may ask? Gore is not a scientist, but he is a big messenger who has a message he wants (demands) everybody to believe. And it so happens that he has a huge interest in that message.
Look for the media to build a massive bulwark around Gore, and to come out blasting with everything they’ve got.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterEchoing the determination of NASA scientists, a new study suggests the natural variability in cloud cover allowing more solar radiation to be absorbed by the Earth’s oceans drove the 2014-2020 global warming.
NASA scientists (Loeb et al., 2018) used satellite data to assess the 2014-2017 warming was driven by a +0.83 W/m² shortwave forcing due to the downward trend in cloud cover.

Image Source: Loeb et al., 2018
A new study (Ollila, 2020) affirms this analysis and suggests the 2018-2020 temperature changes can also be explained by shortwave cloud forcing.
“…the pause was over at the end of 2014, and the major cause was not the anthropogenic forcing, but it was the SW [shortwave] radiation forcing”
“Decreases in low cloud cover were the primary driver of the decrease in reflected SW…”

Image Source: Ollila, 2020
Share this...FacebookTwitter "
"The woman who toppled Tony Abbott in Warringah at the last election on a platform of climate change action now has the whole parliament in her sights as she seeks bipartisan support for a climate change framework bill aimed at transitioning Australia to a decarbonised economy. Zali Steggall – along with her fellow crossbenchers Rebekah Sharkie, Helen Haines and Andrew Wilkie – will release the climate change national framework for adaption and mitigation bill on Monday, ahead of its introduction to the parliament in March.  Steggall and the crossbench have begun a conscience vote campaign online and within their communities. They hope to win over enough government MPs to see the bill, which has been modelled on existing legislation in the UK, New Zealand and Ireland, pass in Australia. However, the crossbench faces an uphill battle, with Scott Morrison declaring just last week he would not be “bullied’ into more action on climate change.  Steggall has previously called on the self-styled “modern Liberals” to support the legislation, which she said became imperative following the summer of unprecedented bushfires and resulting hazardous air quality that left communities reeling. With the government’s party room once again at war over climate policy, Steggall said it was time to let individual MPs speak for their communities rather than toe a party line. “The bill will be circulated to all MPs as well as business, environmental and relevant stakeholder groups on Monday,” she said. “It is time to take the party politics out of climate policy. It is a matter of principle that we should all be committed to a safer future. I am urging for a conscience vote when I present the bill on March 23 as a private member’s bill. Now is the time for a rational approach to climate change.” The crossbench group, working with climate action organisations, has already launched petitions calling on MPs to be allowed a conscience vote on the legislation once it is introduced. Without a conscience vote, the bill is doomed to fail, with the government holding the numbers in the lower chamber. Steggall said the events of the summer, on top of the climate impacts Australia was already suffering through, should be enough to prompt MPs to follow their conscience and vote on behalf of their constituents. “This bill is a sensible and bipartisan approach to safeguarding Australia’s future against the impacts of climate change,” she said. “The devastating fires that ripped through Australia over summer; the drought; and our deteriorating air pollution have shown how the impacts of climate change are a real threat to our way of life.” Dave Sharma in Wentworth, Tim Wilson in Goldstein and Jason Falinski in Mackellar, as well as Brisbane’s Trevor Evans and North Sydney MP Trent Zimmerman are being targeted as potential allies. Newcomer Katie Allen, who won the seat of Higgins at the last election, and Bennelong MP John Alexander, who have both urged their government to take more action on climate policy in recent weeks, are also being urged to vote according to their electorate’s wishes. Steggall has previously warned of voter backlash if moderate Liberals ignore their wishes on climate action. “I think they have to be mindful of their electorates feeling disenfranchised if they aren’t voting in accordance with their majority wishes,” Steggall told the Guardian last month. “The Liberal party is the party of the free vote – I am not asking them to do something they have never done before, and I think crossing the floor to vote for a climate act is something they need to do to represent their constituents. “If you choose to ignore the amount of people in your electorate [who want stronger climate action] … you do so at your peril.” Steggall and the crossbench have kept much of the bill under wraps, but have said they are aiming for a statutory long-term target of net zero emissions by 2050, as well as a climate change risk assessment for all sectors. The group wants the government to focus on a national adaption schedule for Australia’s industries, based on what the science has revealed in regards to impacts of climate change. To ensure accountability, the group wants to follow the UK’s lead and establish some sort of climate change authority, which would act independently of the government, and report back on the progress each year. Labor’s deputy leader, Richard Marles, said the opposition was looking to work with the government on a bipartisan climate policy. “We have been seeking bipartisanship for a long time in relation to this,” he told the ABC on Sunday. “But to get bipartisanship, we actually need to have a side that we can talk to. Right now, we’re watching a whole lot of people having a war with each other inside their party room … and that’s preventing the conservatives in this country even coming to the table to have a discussion about this.”"
"All eyes are on Brazil following the re-election of Dilma Rousseff as president after an eventful campaign in which the strongly pro-environment candidate Marina Silva was squarely defeated.  Now, the country’s green credentials are seriously at risk. In a new report in the journal Science, researchers from Brazil and the UK (including myself) highlight the danger of new plans to allow mining and dams in protected areas and indigenous lands.  Congressional debates to approve or reject proposed legislation will decide if Brazil will retain its hard-won status as what The Economist calls “the world leader in reducing environmental degradation”. The new government is at a crossroads: either maintain the integrity and long-term future of its globally significant ecosystems or favour industrial interests by allowing 10% of even strictly protected areas to be mined. While the proposals include mitigation measures (protecting land elsewhere) these are unrealistic and also inadequate because they fail to account for the indirect impacts of mines. Developing mines and hydropower dams in protected areas would represent a reversal for Brazilian law-makers and a body blow to environmental agencies, credited with drastically reducing Amazonian deforestation over the past decade. Mega-projects have mega-impacts and in the Amazon, mining and damming go hand in hand. Mining is energy intensive and is one of the underlying reasons for Brazil developing dozens of large hydropower plants in Amazonia. Hydroelectric dams can harm both society and the environment. For example, I was alarmed to see how severe flooding in Rondônia state this year led to economic paralysis and the spread of water-borne diseases in towns and countryside along the Madeira River. The flooding of the Madeira in both Brazil and upriver in Bolivia was suspected to have been caused by the recently completed Jirau hydropower dam. Under current plans, very few protected areas will remain free from the influence of hydroelectric dams.  Mining projects such as the enormous Carajás iron ore mine in eastern Amazonia, powered by construction of the controversial Tucuruí dam in the 1970s, are only the tip of the iceberg. Mineral extraction in Brazil is poised to expand into what were previously considered no-go areas for industrial development.  Our research found that in the Amazon alone 34,117km2 of strictly protected areas and 281,443km2 of indigenous lands are in areas of registered mining interest. Forget football fields, this is an area larger than the whole of the UK.  The direct impacts of mines and dams are eclipsed by the indirect effects, as thousands of workers follow mega-development projects into protected areas. Rapid population growth in service towns causes urban areas, roads and farmland to expand into surrounding forests. By 2000, Brazil had created the world’s largest protected area network, covering an enormous 2.2m km2 (an area the size of Greenland). These parks have been highly effective. For example, by reducing deforestation rates to only 10-15% of those in surrounding areas, Brazil’s protected areas contribute to mitigating future climate change.  The beneficiaries of climate mitigation range from farmers in the south of Brazil who depend on Amazonia for their rainfall, to the poorest people in developing countries who stand to bear the brunt of global warming, sea-level rise and extreme climatic events.  Brazil’s protected areas go far beyond just saving the forests themselves and support traditional peoples, including rubber-tappers and Brazil-nut harvesters. In addition, indigenous lands provide a safe space to maintain the traditions and cultures of the country’s 305 indigenous ethnic groups, including 69 uncontacted groups. Get-rich-quick mining is not a new threat to Brazil’s unique ecosystems. I have witnessed the decade-long struggle of a strictly protected area, the Jari Ecological Station in Pará, to remove illegal gold-mining from within its borders. However, it is harrowing to now see 71% of the park (an area larger than Greater London) being under official consideration for mining operations. Even if “only” 10% of the park is used for mining, indirect effects will change it for ever.  Relevant federal departments need adequate resources to ensure government decisions are made democratically and with reliable impact assessments. Chronic under-staffing in Brazil’s protected areas means that many lack basic information on baseline environmental conditions and diversity of plants and animals. Buffer areas designed to protect parks from external threats are put at risk, and a lack of staffing and data puts ICMBio (the agency responsible for protecting parks) in a weak position from which to assess the potential impacts of dams or mining on the integrity of a park.  Brazil’s population is growing and increasingly wealthy, which means higher demands for energy and food. Some difficult decisions will have to be made.  However, environmental impact assessments and mitigation measures (in some cases impossible to achieve and in most cases not implemented anyway) surrounding proposed mega-projects have fallen short of international best practice and largely ignore indirect impacts. I hope that Brazil reasserts its status as a leader of green development and does not legislate against her national treasures."
"A colleague at the UN’s Food and Agriculture Organization (FAO) tells a terrifying story about the desert locust. In 2005 she visited farmers in Niger as they prepared to harvest their crops. Just hours later, a swarm of locusts swept through the area and destroyed everything. One month later, truckloads of families were forced to leave their homes because they had nothing to eat. A year before that the UN had launched an appeal for $9m (£6.9m) to help Niger and neighbouring countries control the locusts. The response was slow, and six months later the amount required in the appeal had reached $100m. The maths was simple: the locusts were faster than the international response. History is now in danger of repeating itself. But on a much bigger scale. The worst outbreak of desert locusts in decades is currently underway in the Horn of Africa. It is the biggest of its kind in 25 years for Ethiopia and Somalia – and the worst Kenya has seen for 70 years. The impacts of the outbreak in these countries are particularly acute as pastures and crops are being wiped out in communities that were already facing food shortages. As we write, the swarms have just crossed into Uganda and Tanzania, and moved within 50km (31 miles) of South Sudan. Djibouti and Eritrea are also affected. And Oman, Saudi Arabia, Sudan, Yemen, and Pakistan are fighting their own serious infestations. The desert locust is considered the world’s most destructive migratory pest. A single locust can travel 150km and eat its own weight in food – about two grams – each day. A swarm the size of New York City can consume the same amount of food in one day as the total population of New York, New Jersey and Pennsylvania. What we are seeing in East Africa today is unlike anything we’ve seen in a very long time. Its destructive potential is enormous, and it’s taking place in a region where farmers need every gram of food to feed themselves and their families. Most of the countries hardest hit are those where millions of people are already vulnerable or in serious humanitarian need, as they endure the impact of violence, drought, and floods. We have acted quickly to respond to this upsurge. Local and national governments in East Africa are leading the response, and our respective offices are working closely together to keep this outbreak under control. The UN’s Office for the Coordination of Humanitarian Affairs has released $10 million from its Central Emergency Relief Fund to fund a huge scale-up in aerial operations to manage the outbreak. The FAO is urgently seeking $76 million from donors and other organisations to help the affected countries fight the outbreak. The amount required is likely to increase as the locusts spread. But the window to contain this crisis is closing fast. We only have until the beginning of March to bring this infestation under control as that is when the rain and planting season begins. The swarms are highly mobile; the terrain often difficult; the logistical challenges immense. But left unchecked – and with expected additional rains – locust numbers in East Africa could increase 500 times by June. We must act now to avoid a full-blown catastrophe. And we will. At the same time, we need to pay attention to a bigger picture. This is not the first time the Greater Horn of Africa has seen locust upsurges approach this scale, but the current situation is the largest in decades. This is linked to climate change. Warmer seas mean more cyclones, generating the perfect breeding conditions for locusts. Together, we express deep solidarity with the people and communities affected. And we call on the international community to respond with speed and generosity to control the infestation while we still have the chance. • Qu Dongyu is director-general of the UN Food and Agriculture Organization; Mark Lowcock is the UN under-secretary-general for Humanitarian Affairs and Emergency Relief Coordinator."
"

Alengthy new papal encyclical is being rolled out today. A version of _Laudato Sii_ , or “Be Praised”—thought by most observers to be final, though the Vatican said otherwise—was leaked on Monday. It is a highly political discussion of the theology of the environment.



In fact, Pope Francis addresses not just fellow Catholics but “every person who inhabits this planet,” with whom he proposes “to enter into discussion… regarding our common home.” Climate change is high on his list. With the UN pushing a new agreement for December, Christiana Figueres, head of the UN Climate Change Secretariat, exulted that the encyclical “is going to have a major impact.”



It’s a difficult document to critique, especially since the release was in Italian, so the English versions circulating are poor online translations. Nevertheless, _Laudato Sii_ mixes heartfelt concern for the status of the environment and man’s connection with the world around him with an often limited or confused understanding of the problem of pollution and meaning of markets. The document also wanders widely, connecting most every human endeavor, from drug consumption in affluent societies, architecture, overcoming the “barriers of selfishness,” and cultural homogenization to the environment. Indeed, contended the Pontiff, “The disappearance of a culture can be as serious as or more than the disappearance of an animal or plant species.”





The new papal encyclical understands man and religion, but not economics and politics.



Moreover, the document mixes the indubitable and the dubious. Pope Francis rightly worries about the quality of life, “extreme consumerism,” and meaning in people’s lives. He also highlights God’s concern “for the poor and abandoned,” evident throughout Christian Scripture. As for the environment, he noted, “to insist in saying that the human being is the image of God should not make us forget that every creature has a function and nothing is superfluous.”



Despite his commitment to ecological values, the Holy Father acknowledges that “a return to nature cannot be at the expense of freedom and the responsibility of the human being, that is the part of the world tasked with cultivating its ability to protect and develop their potential.” He also rejects “deification of the earth, which would deprive us of the call to collaborate with it and protect its fragility.”



Nevertheless, humanity’s responsibility for the environment is complex and the Pope discusses ecological values in the context of economic development and care for the poor. How to creatively transform but at the same time gently preserve the natural world is not easy. Unfortunately, in its policy prescriptions _Laudato Sii_ sounds like it was written by an advocate, largely ignoring countervailing arguments. The resulting factual and philosophical shortcomings undercut the larger and more profound theological discussion.



For instance, the encyclical begins by referring to “the deteriorating global environment.” In fact, “the environment” is not a single thing. There are a host of environmental issues which vary dramatically across continent, country, region, and locality. The Pope warns about “the depletion of natural resources,” yet most resources, such as oil, have been growing relatively more abundant. This reality doesn’t negate the Pope’s insistence that “destruction of the human environment is something very serious,” but affects the application of his injunction.



Worse, the document complains much of capitalism, one of the “correct models of growth that seem incapable of guaranteeing respect for the environment,” as well as property rights, which, in the Pope’s view, allow selfish individuals to act in their individual rather than the public’s interest. In fact, no system guarantees respect for ecological values. Communism was the worst: the state controlled everything and the party demanded industrialization. Analysts referred to “ecocide” in the Soviet Union and Eastern Bloc. China looks little better.



In contrast, capitalism provides the resources and technology to improve environmental protection. Indeed, the Holy Father acknowledges that “science and technology are a wonderful product of human creativity that is a gift from God.” Of course, such advances offer no panacea, he warns. Nevertheless, wealthier societies produce more efficiently. They deploy better tools to cope with ecological ills. They are freer and allow people to demand political change.



Indeed, prices in a marketplace operate as signals. _Laudato Sii_ complains that disproportionate consumption steals “from poor nations and future generations,” and that “the rate of consumption waste and degradation of the environment has passed the possibilities of the planet, in such a way that the current lifestyle, being unsustainable, may only result in disaster.” No evidence of this claim is provided. In fact, rising resource prices encourage people to use less, producers to find more, manufacturers to operate more efficiently, and entrepreneurs to create substitutes. Claims that humanity was running out of resources and destroying the ecology go back centuries and so far have been proved wrong.



Markets also do a good job of comparing the costs and benefits of different means to achieve a common end. Costs matter, and not just to big corporations, as the encyclical suggests. For the poor environmental protection can be an unaffordable luxury. _Laudato Sii_ well describes the importance of work, but jobs are not created, like the earth, _ex nihilo_. The more regulatory dictates, higher energy prices, greater supply costs, and more, the fewer the jobs and the lower the salaries.



When it comes to solving specific problems, markets can be quite helpful. For instance, the document complains about water shortages and then criticizes the “tendency to privatize this scarce resource.” Yet monopoly public utilities are renowned for providing poor service at high cost. The poor’s lack of “access to drinking water” has much more to do with Third World poverty and government incompetence than privatization.



Most curious is _Laudato Sii_ ’s almost angry attack on emissions credits, which “can give rise to a new form of speculation and would not help to reduce the global emission of polluting gases.” Yet well‐​designed tradeable permits, like emission taxes, encourage those who can control emissions at the least cost to do so the most. This is not an assault “on the solidarity of all peoples,” as the document proclaims, but a means to most help those who have the least to give.



Moreover, markets and property rights are the most important means to provide people with what the Pontiff calls “a dignified life through work.” Even Marx acknowledged that capitalism raised the mass of humanity out of immiserating poverty. Commercial society destroyed the foundations of aristocratic oppression.



Alas, the document offers a confused, misguided criticism of mechanization and economies of scale. It was the new “machines” replacing jobs, decried by the encyclical, which created vast new economic and social opportunities. The Gutenberg Press put scribes out of business, while computers transformed whole industries.



 _Laudato Sii_ asserts the “principle of subordination of private property to the destination” and the “social function of any form of private property.” Property rights may not be absolute, but the legal right to land is most important for those who lack wealth and influence. The lack of such rights in the kleptocratic systems in Latin America with which the Holy Father is so familiar hampered the entrepreneurial poor, a phenomenon highlighted by Hernando de Soto.



Property rights also create incentives for environmental stewardship. Garrett Hardin famously wrote about the “tragedy of the commons,” how public ownership naturally leads to environmental degradation. Ownership vests both costs and benefits with a sole decision‐​maker who can be held responsible. Where the public “owns” land no one effectively does so.



That’s why many countries have created quota systems, creating a form of defined development right, to govern ocean fishing. In the case of the Amazon rain forest, mentioned by the Pontiff, indigenous peoples lacked formal legal rights to the land they used. The problem is _lack of_ property rights.



Most environmental problems occur because of what economists call externalities—costs and benefits that fall on others. For instance, Pope Francis speaks of “the obligation of polluters to take responsibility economically” and “assess the environmental impact of each work or project.” Without an appropriate legal regime, industry could spew emissions far and wide, the antithesis of property rights. The real environmental issue is over where to draw the line, which requires balancing complex interests: prosperity, liberty, ecology. _Laudato Sii_ seems to assume the correct outcome in every case is more of the latter.



Indeed, the encyclical lacks any sense of the flawed nature of government. The Pope is disappointed that environmental efforts “are often frustrated not only by the refusal of the powerful, but also by the lack of interest of the other.” However, public choice economists diagnosed this problem decades ago: concentrated benefits, diffuse costs. In this case environmental organizations are as prone as corporations to push their narrow preferences on everyone else.



This reality raises doubts about the Pope’s endorsement of the “precautionary principle,” which in practice would hold virtually every beneficial human innovation hostage to interest groups dedicated to the status quo. Equally dubious is the encyclical’s endorsement of the “essential development of international institutions stronger and effectively organized… with the power to sanction.” There is no reason to believe that ever more distant, unaccountable bureaucracies will operate for the common good rather than at the behest of whatever interests, corporate, labor, activist or other, wielding the greatest influence. In fact, the encyclical complains of the failure of global conferences due to “too many special interests.”



Politics also is far more open to the Holy Father’s complaint about “the earth’s resources” being “plundered due to ways … too tied to the immediate result.” The price of property incorporates perceived future value. Ruin it and you lose that value. Politicians’ decision‐​making time frame usually is years, often months. If opening up sensitive land for development will win a few votes in the next election, why worry about future generations, which don’t vote? Those in politics may talk the talk, but in practice they are no less selfish and neglectful than those in business, and respond to far more destructive incentives.



The encyclical includes a confusing discussion of trade and globalization. External debt “has become an instrument of control.” True, yet local political leaders borrowed much and wasted the proceeds. The document also claims that trade relations forbid “access to ownership of property and resources to meet [people’s] vital needs.” However, trade mandates and forbids nothing. Rather, it provides opportunities which, like industrialization, might not offer an easy path for the poor, but which usually are better than the alternatives. That is why polls consistently show the greatest support for globalization in the poorest nations.



 _Laudato Sii_ also argues for redefining progress, contending that “diversification of a production more innovative and with less environmental impact, can be very profitable.” If true, it will happen without legal mandate.



Yet the Pope argues that it is not sufficient to care for nature while enjoying financial profits, or practicing “environmental conservation with progress.” Without evidence the encyclical contends that this will only mean “a small delay in the disaster.” However, past doomsayers consistently have been proved wrong. The Pontiff certainly is right to question “technological and economic development that does not leave a better world and quality of life.” However, compare the lives of the average person, and especially poor person, today with a century ago and a century before that. The world and quality of life are dramatically better. The Holy Father should encourage people to ask, “How much is enough?” But it is important that those living in comfort in the industrialized West do not try to answer for those living in the impoverished Third World.



Although the Vatican often is treated as an independent state, its comparative advantage is not legislation. Yet at one point the encyclical discusses “household waste and commercial, demolition debris, clinical waste, electronic or industrial waste.” Also noted is the “special challenge” presented by “marine debris and the protection of marine areas.” Later the document asserts the importance of education on “how to avoid the use of plastic material or paper,” “cooking only what you can eat reasonably,” and turning off “unnecessary lights.” Indeed, there is a lengthy but not entirely fruitful discussion of urban planning, highlighted by the professed need to improve urban transportation.



The discussion of climate change is similarly specific but partisan. For instance, the encyclical takes an almost panicked view of the problem, even though it closes out the chapter noting the Church’s obligation to “listen and promote debate honest among scientists, respecting the diversity of opinion.” _Laudato Sii_ also blames extreme weather events on climate change while admitting in the same sentence “that we cannot attribute a cause scientifically determined for each particular phenomenon.”



The fact that there likely will be more warming does not mean it will be catastrophic. In fact, models failed to accurately predict past behavior, peer‐​reviewed research increasingly suggests warming toward the lower range, and it is impossible to accurately predict events a few years, let alone a century hence (virtually no one saw the Shale gas/​oil revolution coming, for instance). This argues against making draconian, expensive changes that may make little sense soon after they are implemented. Rather, it would be better to adapt to particular problems as they arise rather than to attempt to hold down temperatures by radically rolling back energy use. The resources saved are needed to meet many other human needs.



In contrast, the Pontiff truly is acting in his unrivaled role as spiritual leader when he advocates a personal, social, and spiritual transformation in how people relate to the environment. He promotes an “ecological spirituality that arises from convictions of our faith” and advocates human freedom being “put at the service of another kind of progress, healthier, more human, more social and more integral.” The Pope’s proposed “ecological conversion” should spark much discussion, since his application of basic Christian principles is plausible, if not necessarily convincing. Is it really true that the same principle of “brotherly love” requires “us to love and accept the wind, the sun or the clouds”? Nevertheless, throughout history too many Christians probably have practiced dominion and slighted stewardship. Even this untutored Protestant can appreciate the claim that “the Eucharist is even light source and motivation for our environmental concerns and directs us to be guardians of all creation.”



Moreover, Pope Francis warns that “we cannot think that the political agendas or the force of law is enough to avoid behaviors that affect the environment,” since the culture itself is corrupt. He contends: “if we feel intimately united with all that exists, sobriety and care will arise spontaneously.” Quite true. It is committed individuals who form the “innumerable variety of associations advocating on behalf of the environment,” cited by _Laudato Sii_ , and whose reformed buying behavior can “change the behavior of firms, forcing them to consider the environmental impact and production patterns.” Aligning desire and incentive is the best way to achieve what the Pontiff’s objective.



The Pope expresses the triumph of hope over experience in his call on politicians to act responsibly and the public to participate knowledgeably. Just as one should avoid “a magical concept of the market,” so should we beware the same for politics. Problems will not be magically resolved by investing politicians and bureaucrats with vast new powers.



Larger themes point through the encyclical, which warns that “the market alone does not ensure human development and full social inclusion.” The Gospel, unlike the market, reaches the empty hearts which the Pope sees. We must “not give up asking us questions about the purposes and on the sense of everything.”



The Vatican is not well‐​positioned to assess environmental problems and develop policy solutions. Rather, the Pontiff’s duty is much more fundamental: “the great wealth of Christian spirituality, generated by twenty centuries of personal and communal experiences, is a magnificent contribution to make the effort to renew humanity.” We desperately need such a renewal. Hopefully _Laudato Sii_ , despite its practical shortcomings, will advance the larger and more important theological mission.
"
"It has been five years since an earthquake hit the Italian city of L’Aquila leaving 309 people dead. In the aftermath one public official and six earthquake scientists were charged with multiple counts of manslaughter. Each defendant was sentenced to six years in jail.  It is commonly believed the scientists were condemned for failing to predict the earthquake but, in truth, the case was about communicating risks to a vulnerable population. The defendants were accused by the prosecution of giving “inexact, incomplete and contradictory information”. One of L’Aquila’s citizens succinctly articulated the position of many survivors:  We all know that the earthquake could not be predicted, and that evacuation was not an option. All we wanted was clearer information on risks in order to make our choices. The appeal process has been ongoing but as of November 10 charges have been dropped against the scientists involved. Conspicuously, the convicted Italian official had his sentence reduced but still faces two years in prison.  Six days prior to the earthquake scientists met with Bernardo De Bernardinis, then deputy director of Italy’s Civil Protection Agency. A local laboratory technician had been making dubious predictions of an impending large earthquake. Meanwhile, smaller tremors were being experienced in the region.  The meeting was called with intentions to reassure the public. The scientists correctly emphasised to De Bernardinis that the precise timing of major earthquakes could not be known. They were careful not to rule out the possibility of a major earthquake any time.  Following their meeting De Bernardinis publicly stated: “The scientific community tells us there is no danger, because there is an ongoing discharge of energy. The situation looks favourable.” None of the scientists made an effort to correct Bernardinis’s imprecise statements.  Public officials clearly felt pressure to reassure. The overriding wish to calm citizen’s fears created a situation where risks were downplayed and scientific uncertainty was emphasised at the expense of responsible warnings.  There are obvious lessons to be gleaned from L’Aquila: clear science communication has real consequences for public safety. Political officials have a responsibility to communicate risks to the public without sacrificing the science to politically palatable messages.  However we need not endorse the L’Aquila judgements in order to consider alternative circumstances where legal penalties might be appropriately applied.  Take the purposefully organised campaigns of disinformation over lead poisoning, asbestos or tobacco, for instance. Such campaigns, generally orchestrated by vested interest groups, show a reckless disregard for public safety. Now a related campaign seeks to undermine the public’s understanding of climate science. It is easy to feel sympathy for an Italian official who seems to have been motivated to reassure rather than mislead. Despite his failure to convey an accurate but tempered assessment of the dangers it remains true that large earthquakes really are unforeseeable in the short term. I hope his ongoing appeal process is ultimately successful.  But what do we make of politicians who doggedly deny the overwhelming evidence for human-caused global warming after receiving large sums of campaign money from the fossil fuel industry? They too emphasise scientific uncertainty at the expense of responsible warnings.  This is clearly irresponsible and dangerous. The devastation to human life resulting from unchecked climate change is magnitudes greater than the tragedy in L’Aquila and it will get worse still. We may be uncomfortable with the Italian courts, but consider the alternative: is a system which allows politicians to openly receive large sums of money from fossil fuel interests, while dismissing the greatest crisis humanity has yet faced, any better at serving the public’s interests?  We should reconsider political frameworks that allow large sums of money to shape politician’s understanding of climate science if we want to avoid cases like L'Aquila in the future."
"Gol-e-Zard Cave lies in the shadow of Mount Damavand, which at more than 5,000 metres dominates the landscape of northern Iran. In this cave, stalagmites and stalactites are growing slowly over millennia and preserve in them clues about past climate events. Changes in stalagmite chemistry from this cave have now linked the collapse of the Akkadian Empire to climate changes more than 4,000 years ago. Akkadia was the world’s first empire. It was established in Mesopotamia around 4,300 years ago after its ruler, Sargon of Akkad, united a series of independent city states. Akkadian influence spanned along the Tigris and Euphrates rivers from what is now southern Iraq, through to Syria and Turkey. The north-south extent of the empire meant that it covered regions with different climates, ranging from fertile lands in the north which were highly dependent on rainfall (one of Asia’s “bread baskets”), to the irrigation-fed alluvial plains to the south.  It appears that the empire became increasingly dependent on the productivity of the northern lands and used the grains sourced from this region to feed the army and redistribute the food supplies to key supporters. Then, about a century after its formation, the Akkadian Empire suddenly collapsed, followed by mass migration and conflicts. The anguish of the era is perfectly captured in the ancient Curse of Akkad text, which describes a period of turmoil with water and food shortages:  … the large arable tracts yielded no grain, the inundated fields yielded no fish, the irrigated orchards yielded no syrup or wine, the thick clouds did not rain. The reason for this collapse is still debated by historians, archaeologists and scientists. One of the most prominent views, championed by Yale archaeologist Harvey Weiss (who built on earlier ideas by Ellsworth Huntington), is that it was caused by an abrupt onset of drought conditions which severely affected the productive northern regions of the empire.  Weiss and his colleagues discovered evidence in northern Syria that this once prosperous region was suddenly abandoned around 4,200 years ago, as indicated by a lack of pottery and other archaeological remains. Instead, the rich soils of earlier periods were replaced by large amounts of wind-blown dust and sand, suggesting the onset of drought conditions. Subsequently, marine cores from the Gulf of Oman and the Red Sea which linked the input of dust into the sea to distant sources in Mesopotamia, provided further evidence of a regional drought at the time. Many other researchers viewed Weiss’s interpretation with scepticism, however. Some argued, for example, that the archaeological and marine evidence was not accurate enough to demonstrate a robust correlation between drought and societal change in Mesopotamia.  Now, stalagmite data from Iran sheds new light on the controversy. In a study published in the journal PNAS, led by Oxford palaeoclimatologist Stacy Carolin, colleagues and I provide a very well dated and high resolution record of dust activity between 5,200 and 3,700 years ago. And cave dust from Iran can tell us a surprising amount about climate history elsewhere. Gol-e-Zard Cave might be several hundred miles to the east of the former Akkadian Empire, but it is directly downwind. As a result, around 90% of the region’s dust originates in the deserts of Syria and Iraq.   That desert dust has a higher concentration of magnesium than the local limestone which forms most of Gol-e-Zard’s stalagmites (the ones which grow upwards from the cave floor). Therefore, the amount of magnesium in the Gol-e-Zard stalagmites can be used as an indicator of dustiness at the surface, with higher magnesium concentrations indicating dustier periods, and by extension drier conditions.  The stalagmites have the additional advantage that they can be dated very precisely using uranium-thorium chronology. Combining these methods, our new study provides a detailed history of dustiness in the area, and identifies two major drought periods which started 4,510 and 4,260 years ago, and lasted 110 and 290 years respectively. The latter event occurs precisely at the time of the Akkadian Empire’s collapse and provides a strong argument that climate change was at least in part responsible. The collapse was followed by mass migration from north to south which was met with resistance by the local populations. A 180km wall – the “Repeller of the Amorites” – was even built between the Tigris and Euphrates in an effort to control immigration, not unlike some strategies proposed today. The stories of abrupt climate change in the Middle East therefore echo over millennia to the present day."
"While listening this week to the Guardian Australia editor, Lenore Taylor, on the Full Story podcast speaking with sadness and exasperation about the past 30 years of climate change policy, I thought about good intentions and perfection. We often hear that “the road to hell is paved with good intentions”. It’s one of those sayings that, when you really think about it, is just a squib.  The reality is these good intentions that are paving our way to hell are not so much good as ignorant, and quite often (such as with much of Indigenous policy) outright racist. They are really considered good only by those seeking to excuse the action in the first place. And while good people might occasionally do wrong with actual good intentions, there is a much higher strike rate of people with bad intentions doing bad things. That the road to hell may be paved with good intentions means we should ensure our good intentions are not ignorant or biased; and it sure as heck does not give an excuse to those with bad intent. We certainly have seen a plethora of bad intent with respect to climate-change policy since 1990. Similarly, you could probably fill a long list of quotes by people over the past decade or so who have suggested of climate change policy that “we should not make the perfect the enemy of the good”. And yet rather than this suggesting we need to compromise, what it has come to mean is that we should excuse policy that is bad, because it is not perfect. If we are so worried about the perfect being the enemy of the good, wouldn’t we at least see some evidence of someone in the major parties actually suggesting a policy that could be described as perfect? The science of climate change tells us we need to reduce emissions and the sooner we do it the less the impact will be. And yet rather than see any “perfect” policy with this aim, instead we get supposedly good policy accompanied with caveats – talk of the need for transitional fuels such as gas or that a coalmine is fine – hey, let’s not be perfect! (And please don’t not argue about just how good something has to be before perfect becomes its enemy). So bad has this become that the carbon price instituted by the Gillard government is now considered some perfect policy too far beyond our political grasp. Why are we so lacking in ambition? And given the other side have an ambition fuelled by bad intentions, it might be worthwhile when trying to compromise to ensure negotiation starts from a position rather closer to perfect than just “good”. Because let us be honest: the government is paving the way to climate change hell with bad intentions. The government is replete with climate-change deniers who intend to block and retard any action to reduce emissions. They will do this through obfuscation and outright misinformation – such as the lies about the impact of the ALP’s electric car policy during the last election, or the current lies about the bushfires. On Wednesday, Peter Dutton told ABC’s Afternoon Briefing that “obviously, as we’ve all pointed out, we’re experiencing hotter weather, longer summers, but did the bushfires start in some of these regions because of climate change? No. It started because somebody lit a match. There are 250 people, as I understand it, or more that have been charged with arson. That’s not climate change.” It’s also not the truth. The prime minister has equally betrayed his bad intentions on the issue. He told the media on Thursday that, “I’m not going to allow a confined, narrow debate when it comes to understanding what it means to live in the climate we’re going to live in. It’s not just about emissions reduction. That’s important. But it’s also about resilience and it’s also about adaptation” His intent is clearly to actually narrow the debate by excluding as much as possible any discussion of emissions, and instead to focus on building dams as though that is some saviour for a country affected by climate change and by greater periods of drought. Scott Morrison – and other members of his cabinet – have no good intentions when they suggest Australia is doing well by meeting and beating out Kyoto commitments. Those commitments are frauds. The Howard government purposefully ensured Australia alone could include land use because the Kyoto base year of 1990 involved a spectacular level of land clearing, as does 2005, the base year for our Paris agreement. Saying we will meet and beat our targets is like bragging that you are meeting your target of drinking less beer by comparing how many glasses you drink per day now to what you drank on the day of your 21st birthday party. Last week Morrison told the National Press Club the United States were doing great because of its increase in gas-produced electricity and that “between 2005 and 2017 US emissions fell by about 13 per cent, that’s just a click over what we have achieved, which is 12.8 per cent by the way.” What he failed to note was those drops occurred prior to the election of Trump and its withdrawal from the Paris agreement and that it is not on target to meet what were the US’s Paris targets. His suggestion that Australia is nearly doing just as well is also one of bad faith. Yes, from 2005 to 2017 we reduced our emissions by 12.8%, just below that of the US, but only if we include land use. If we exclude that very dodgy measure, the US’s emissions still fell by 12%, but Australia’s actually rose by 6.2%. We are not doing our bit, and you would only argue we are if your intention was to ensure that good policy is not merely blocked but bad policy is pushed. There has been some talk this week about new Greens leader Adam Bandt’s call for a “Green New Deal” and whether or not Australia should adopt such a US-style political term. To be honest I’m not all that fussed about the marketing, so long as the policy has large ambition. We need some good intentions and we need to aim for perfection. The challenges and forces against action on climate change are large and powerful. As the past 30 years have shown us, being content to argue for a “good” third- or fourth-best policy is no way to win this fight, and neither is allowing those with bad intentions to tell us that what they want is good."
"Some great news at last, as China and the US announce a secretly negotiated deal to reduce their carbon emissions. After years of seeming to get nowhere at all it looks like we have the beginnings of meaningful commitments. If the rest of the world can fall in line with the combined targets of China, the US and EU, and if between us all we can enforce them, we would actually have progress. Not success, but for the first time we would have better-than-nothing global progress on climate change. But just before we all relax, lets get things into perspective. Global emissions have been on a mathematically predictable exponential trajectory for at least 160 years.  Cumulative CO2 emissions (broadly speaking that’s what determines the temperature change) continue to double every 39 years. Nothing that anyone has done to date has succeeded in making even the faintest detectable change in that.  To be blunt, our species has so far not demonstrated any ability whatsoever to influence global emissions growth through deliberate action on climate change. Savings in one place have simply popped up elsewhere. And if we stay on our age-old trajectory we will shoot through the likely threshold of two degrees in the mid-2040s.  By that I mean that by about 2045 we will pass the point at which we will probably experience more than a 2°C rise even if no-one anywhere in the world ever again set fire to any coal, oil or gas. And, roughly speaking, 39 years after that we will crash through the 4°C threshold which humans would be very likely to find extremely unpleasant.  Of course we don’t really know all that much about what level of temperature change will cause us what level of suffering and death. We don’t understand the climate discontinuities that we might trigger, and we don’t know how good we will be at adapting to change and we don’t know how good we will be at preserving world order if things get tough.  The mainstream consensus is that 2°C entails significant risk of something nasty happening while 4°C is probably very nasty indeed. No one knows for sure. What we need is a global constraint on greenhouse gases. And it needs to be rapid enough to  keep temperatures as close to 2°C rise as possible. This much, thankfully, seems to be uncontested these days among people who talk any sense on climate change. So how far do the latest US and China pledges take us? If (and it’s still a big “if”) the world falls quickly in line with the US (27% cuts by 2025), China (peak by 2030 – by which time their emissions could be enormous) and EU (40% cut by 2030) announcements we will come off the exponential curve but still fly through the 2℃ threshold and well beyond. Coming off the curve would be a huge achievement but not nearly enough.  So when I say we might actually stand a chance of getting somewhere, I don’t mean that things are looking rosy. But I do mean this gives me real hope, as big players are talking the right language at last.  All we need now is more of the same – and to make sure the words turn into enforced action. That will be enormously challenging but it is radically more hopeful position than the situation we have been in in which sticky plasters have been proposed, no amount of which could help.  We need the rest of the world to come into the fold with similar commitments, so we get a leak-proof deal on leaving fuel in the ground. Any countries that don’t participate will probably end up growing their emissions to undo efforts made elsewhere, because that is how the system dynamics work to negate piecemeal actions. Binding targets need tightening up for everyone, beyond what is currently on the table, to take us a lot closer to topping out at 2°C. The deal needs enforcing. This is going to be tough, remember that the exponential global emissions curve has proved incredibly resilient to date. All the greenhouse gases need to be properly included in the plan. We need to head off a global dash for biofuels which will undoubtedly be at the expense of feeding the world’s poorest if left to market forces. Some smart and robust agreements are going to be needed on land use for biofuels. While all this is being put in place we can start investing in the technologies we will urgently need – redirecting the money we have been channelling into fossil fuel research and development. To sum up, the announcement is very encouraging. There may still be a long way to go yet and we all need to push hard for next year’s Paris talks to put it all in place – but it is starting to look as if it might actually be worth the effort."
"
Share this...FacebookTwitterWow! Arctic sea ice is still at levels we had back in mid-February. It’s at the highest level for this date in nine years. http://www.ijis.iarc.uaf.edu/en/home/seaice_extent.htm
Normally there’s about a 1 million sq. km decrease from mid Feb to late April. This year – nada!
Share this...FacebookTwitter "
"

Click image for movie – note download is large 2.4MB
A guest post by Michael Ronayne
Note: Mike has created a movie (solar_cycle_23-24_sunspots.gif large (2.4MB) animated GIF) that shows how the cycle 23 forecast has progressed through time. Given that NASA’s David Hathaway recently commented on SpaceWeather that we are still seeing Cycle 23 spots, this seemed like a good time to post Mike’s effort.
The Space Weather Prediction Center (SWPC) at http://www.swpc.noaa.gov/ issues weekly reports on solar activity know as Preliminary Report and Forecast (PRF) of Solar Geophysical Data or “The Weekly”. Generally on the week following the end of the month a monthly summary is issued which includes graphics for the past month.
In the summary is the “ISES Solar Cycle Sunspot Number Progression” graphic which shows past, present and predicted average sunspot numbers by month. SWPC maintains a compressed archive of all weekly PRD reports in PDF format since 1996 which is available here.
Individual weekly reports for 2007 are available here  and current reports for 2008 are available here .
The most current graphic is always here.
All of “The Weekly” reports were inspected to identify the monthly summaries and determine the quality of the “ISES Solar Cycle Sunspot Number Progression” graphic contained therein. It was determined that the graphs prior to April 30, 2003 were in a significantly different format, had quality control problems and skipped months, therefore only graphs from April 30, 2003 to present were used.
Using Adobe Acrobat Professional the “ISES Solar Cycle Sunspot Number Progression” graphics was extracted from each of “The Weekly” PDF reports as oversized TIFF graphics to preserve resolution. The standard publication size for the graphic was 720×550 pixels but the aspect ratio for some of the graphs was not preserved within the PDF document. When the oversized TIFF graphic were resized to 720×550 without preserving the aspect ratio within the PDF the original 720×550 graphic was recovered in all cases. The 720×550 TIFF graphic was then converted to a GIF graphic for use in the animation sequence.
While extracting the “ISES Solar Cycle Sunspot Number Progression” graphs it was found that January 31, 2008 monthly summary had not been generated, a fact which SWPC confirmed in response to an Email inquiry. The February 29, 2008 graphic was hand edited at the pixel level to recreate the missing month and is identified in the animation sequence “proxy200801.gif”. The remaining graphics are all identified by the PRF document number.
The Advanced GIF Animator program was used to create the animation sequence. With the exception of January 31, 2008 all of the frames are prefixed by PRF9999 when 9999 is the document number of the original PDF report from which the graphic was extracted.
When the animated frames were inspected in sequence it was found that there was a discontinuity between July 31, 2006 (PRF1510), August 31, 2004 (PRF1514) and the September 30, 2004 (PRF1520) frames. The causes of the discontinuities were:

Data was retroactively changed on the August 31, 2004 frame.
The August 31, 2004 data point was not plotted on the August 31, 2004 frame.

These three frames were not altered or correct in anyway and are displayed as published. This technique is very good at identifying data discontinuity problems.
Excluding the problems noted above the reconstructed graphic went very well and there was no discernible flicker between frames indicating that the PDF extraction process was near prefect. With the exception of the problem about August 31, 2004 and the missing monthly summary for January 31, 2008 the SWPC product has been amazingly consistent since April 30, 2003.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea00d55cf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A big cat is apparently lurking in the countryside near Disneyland, Paris. After reports of a tiger on the loose, 200 police and military officers backed up by a helicopter, and a special “wolfcatcher”, were called in to look for the animal. Local authorities have now confirmed the mystery creature is not actually a tiger. The tiger story was always unlikely. Local zoos and a passing circus have denied knowledge of any animal escapes and the photograph of the “tiger” was unconvincing, typical of such sightings: low light level, on the brow of a hill and no scalable objects within view.   The animal does appear to be a cat, but the person who reported it did not report a tiger but a lynx.  Could this be a case of media exaggeration?  After all lynx are native to France and not a threat to humans, whereas tigers… If it is a lynx and a wild one then this in itself is newsworthy as this species is only, normally, to be found a few hundred kilometres from Paris. If it proves to be a more exotic big cat then its origins are of great interest.  It could be a “pet” from a private collection.  Although, I suspect just as in the UK few people in France are able to meet the legal requirements for maintaining such animals in captivity.  In the UK up until 1976 it was possible to keep animals such as tigers in your back garden, but the Dangerous Wild Animals Act put a stop to this. Since the 1950s there have been reports of big cats on the loose in the UK, a country where badgers count as big and scary. In the 1990s the fabled “Beast of Bodmin Moor” regularly made headlines. And over the years a number of cats such as pumas and lynx have been captured in the UK.   But these animals usually show signs that they were escapees from captivity.  Scientific evidence supporting big cats living wild in the UK has not really been forthcoming.  Most evidence is photographic and suffers from the same problems as the “Paris tiger” image.   Big cats can leave footprints, pugmarks, but they are most likely to be left when the ground is soft, after rain, and this can distort their size as the cat sinks into the mud.  One of the problems with big cats in the UK is that it tends to be investigated by amateurs who want to find a big cat and so their objectivity can be questioned.   A supposed photograph of the Paris tiger’s pugmark is clearly that of a dog: you can see the claws at the end of the toes.  All cats aside from cheetahs retract their claws when walking – something dogs cannot do. I worked at the Edinburgh Zoo for four years back in the mid 1990s and we never had an animal escape of any significance.  By this I mean nothing dangerous.  We did have a few animals go over the wall for a few minutes, but they always returned of their own accord.   The most memorable incident involved a group of cotton-top tamarins. These tiny monkeys could go where they wished within the zoo and one day they decided to visit a neighbouring hospital.  We know this because the hospital rang the zoo saying that one of their patients, who was taking strong pain killing medication, complained that he had seen gremlins at his bedroom window. Zoo animal escapes are rarer than we think.  We need to remember that there are around 10,000 zoos in the world housing about 4m animals and crucially the escape of a zoo animal is big news, especially if it is dangerous.  We live in a world of near misses. Think about crossing the road: everyday cars miss us by seconds but when one hits someone it becomes news.  It is thus that we perceive roads as dangerous and clearly if we crossed them without due care and attention they are dangerous.  Zoos build their animal enclosures with care and attention to avoid animal escapes. Of course there have been cases where zoo animals have escaped and injured or killed people such as in the San Francisco Zoo where a tiger killed a visitor and attacked two others in 2007.  An investigation into the case implicated those attacked in provoking the escape.  There has even been a case where a wild tiger entered Nandankana zoo in India to mate with the resident tigress.  Thus, we cannot assume animals escape because they don’t like their home.   At the Belo Horizonte Zoo in Brazil a female tamandua, a tree-living anteater, used to escape her enclosure into the surrounding wild habitat to find a mate and get pregnant.  Then she would return to the zoo to have her baby with all of the creature comforts that a zoo provides.  She did this several times. I for one will be surprised if the cat near Paris turns out to be a tiger: as a conservation biologist I will be delighted if it proves to be a wild lynx but unsurprised if it turns out to be a domestic cat."
"
Share this...FacebookTwitter
‘Die Welt’ science editor Axel Bojanowski (right) comments “This is how climate change is forged”. He had 4 leading scientists confirm the above hockey stick chart was dubious. Image: Die Welt , Source:: DeWikiMan/commons.wikimedia.org/CC BY-SA 4.0; Bojanowski/private
ZDF weather moderator Özden Terli under fire: Did he deceive viewers on historical climate with sleight of hand?
By Die kalte Sonne
(German text translated/edited by P. Gosselin)
Disinformation at ZDF German public broadcasting?
ZDF meteorologist Özden Terli has been a topic at this blog several times. Now ‘Die Welt’ science editor Axel Bojanowski  has taken a closer look at a weather report, particularly a chart that Terli presented July 24, 2020.
The article is behind a paywall, but is worth the money. The centerpiece is the chart from the Potsdam Institute for Climate Impact Research (PIK). It depicts a hockey stick temperature curve that is supposed to convey drama. Excerpt from the article in Die Welt:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The weather report of the “Heute Journal” on ZDF on July 24th presented something that was supposed to be sensational. Moderator Özden Terli presented a graph that allegedly showed the course of the global average temperature since the Ice Age. At first ‘the warming was very slow,’ Terli explained, ‘and then it was stable for a long time’. Suddenly, however, since around 1900, the temperature “jumped up”.
The graph showed an almost vertical red line, the peak of which far exceeded all temperatures since the Ice Age. “This jump is quite enormous,” Terli said.
Millions of viewers were shown that the present day would be warmer than all the rest of human civilization. The discovery could only be a scientific sensation, or a hoax.”
Bojanowski interviewed several experts on the subject and their reactions speak volumes because the graph cleverly mixed two things together, namely smoothed averages from the past with measured current values. Fluctuations in the past can thus no longer be seen. The experts’ verdict was unanimous: it is dubious. Bojanowski analyzes:
The graph was a sleight of hand: the steep red line at the end was not comparable to the data in the previous period. It showed annually measured average temperatures on Earth since the end of the 19th century. However, no such precise records exist for earlier times. Most of the time in the history of civilization can only be shown with average values.
Often there is only one temperature value for hundreds of years, or data has been “smoothed”, i.e. only its average value is shown – short-term warming or cooling is not shown. In order to make the period from industrialization to the present day (the steep red line) comparable with the data for the rest of the time, it should therefore only be shown as a dot showing the average temperature from 1900 to the present day – the red line would only be an inconspicuous dot.”
And because Bojanowski has been there before, the subject of drought can be brought up again. In a weather report Terli attributed this to changes in the climate system. But without going into other sources. Perhaps the weatherman would then have noticed that annual precipitation in Germany has increased over the past 120 years. … It’s astonishing that the head of ZDF Wetter, Katka Horneffer, allows Özden Terli to include such inserts in the weather report again and again.
Share this...FacebookTwitter "
"
The National Weather Service office in San Diego, CA operates a cooperative observer network of weather stations, as do all NWS offices. The station in Coronado, CA, is particularly interesting since it is located on the roof of the Fire Station there.
Given that the MMTS sensor shown below is only about 2 feet above the tar and pea gravel roof, which is known to be a hot environment during the day, and a source for re-radiated heat at night, you have to wonder: “What were they thinking?”

Photo from NWS San Diego, click photo for larger image
The NOAA’s credit, this station is not part of the USHCN climate station network, but still, of what possible value could an air temperature measurement just 2 feet off the rooftop be to anyone?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2b7ab04',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 **Introduction**  
President Obama’s major trade initiatives, the Trans‐​Pacific Partnership, the Transatlantic Trade and Investment Partnership, and obtaining fast‐​track trade negotiating authority from Congress, have run into a buzz saw of opposition, which has derailed prospects for U.S. trade liberalization for the time being.



What began as the usual objections from the usual suspects—labor unions blaming trade for manufacturing decline and job loss; environmental groups blaming trade for climate change; anti‐​globalization activists sparing the developing world from development–has grown into a populist backlash against the TPP, which is portrayed as a secretive, corporatist plot to circumvent democratic processes and usurp national sovereignty. The nascent TTIP negotiations have been smeared with a similar taint.



Characterizations of the TPP as a scheme to boost the fortunes of tobacco, oil and gas, banking, and pharmaceutical companies at the expense of worker protections, the environment, public health, and food and product safety have gone viral. And without so much as a single public repudiation of these claims by the president those perceptions are sticking.



As is true of most populist causes, buried beneath the enabling mythology and hyperbole are some kernels of truth. One such truth, which this paper seeks to distill from the vacuous, anti‐​capitalist hyperventilation surrounding the trade agenda, is that the so‐​called Investor‐​State Dispute Settlement (ISDS) mechanism, which enables foreign investors to sue host governments in third‐​party arbitration tribunals for treatment that allegedly fails to meet certain standards and that results in a loss of asset values, is an unnecessary, unreasonable, and unwise provision to include in trade agreements. Although detractors may not know it by name, ISDS is a significant reason why trade agreements engender so much antipathy. Yet, ISDS is not even essential to the task of freeing trade. So why burden the effort by carrying needless baggage?



Purging both the TPP and the TTIP of ISDS makes sense economically and politically, would assuage legitimate concerns about those negotiations, splinter the opposition to liberalization, and pave the way for freer trade.



 **What’s Troubling the Trade Agenda?**  
President Obama has failed to make an affirmative case for his trade agenda, and his disinterest in rebutting the flood of damaging portrayals of the TPP has permitted germinating dissent to metastasize into a problem much worse. Meanwhile, the nature of trade, the nature of protectionism, and the substance of trade agreements have changed with the proliferation of cross‐​border investment and transnational supply chains. As companies establish operations in foreign markets, where they are engaged in direct and more intense competition with incumbent firms, concerns about protectionism are no longer confined to the border.



Protectionism manifests in more subtle ways today. Accordingly, ensuring nondiscrimination against imports, foreign investment, and the operations of foreign companies requires rules that sometimes burrow into areas that were once the exclusive purview of domestic legislatures and regulators. Agreements nowadays include provisions affecting domestic intellectual property laws, environmental and labor standards, data flow and storage requirements, banking regulations, and food and product safety requirements, to name some.



The interplay of domestic governance and trade agreement obligations has raised questions about jurisdiction, sovereignty, and the separation of powers. That some perceive the TPP as secretive has heightened sensitivity about its objectives and implications. So, instead of seeing negotiations on “regulatory coherence” as a commonsense way for businesses to reduce the costs of compliance without compromising public health or safety objectives, some suspect it is a path to gutting compliance obligations altogether. Others see regulatory harmonization as another step toward global governance. Efforts to include provisions extending protection of patents, copyrights, and other forms of intellectual property are perceived by some as attempts to impose through treaty what was unachievable through domestic processes. Negotiations of rules that would help ensure that financial‐​sector regulations are promulgated in manners that are nondiscriminatory are painted as attempts to weaken domestic safeguards against recurrence of a financial meltdown.



The hallmarks of tighter global economic integration—cross-border investment, transnational supply chains, and intensifying competition—have created tension between the imperative of domestic sovereignty and the growing demand for rules to guard against protectionism and discrimination. One area where this debate has gotten especially heated is tobacco regulation.



Anti‐​tobacco advocates have been demanding a “carve out” provision, which would excuse lawmakers and domestic agencies from their obligations to craft and enforce tobacco regulations in manners that do not discriminate against imports. The rationale for the safe‐​harbor provision is that tobacco poses special known risks to public health and human safety and that trade obligations should not interfere with the capacity to regulate such a dangerous product. Recently, 42 of the 50 U.S. state attorneys general signed a letter insisting that such a safe‐​harbor provision is essential to protecting public health.1



But as trade experts have explained, there is nothing about the TPP or any other trade agreement that impedes a government’s capacity to protect human life or health. Trade agreements do not prohibit regulating. They merely require that such measures be based on sound science and that discrimination against similar products on the basis of national origin be avoided. The states can ban cigarettes, for example, but not cigarettes “from Indonesia.” As Cato trade policy analyst Simon Lester puts it: “Although there may be valid concerns about some of the more recent additions to trade and investment agreements … the core of these rules constrains domestic regulation only to the extent that such regulation discriminates against imports and does not preclude legitimate domestic policymaking.“2



But if one listens closely to the arguments of anti‐​tobacco advocates (or reads the letter from the 42 attorneys general), what most oppose is the possibility of tobacco companies suing the U.S. government in third‐​party tribunals. Creating a tobacco carve out would reiterate a right that governments already possess and would do nothing to safeguard against suits by tobacco companies—or any other companies.



The real ire of anti‐​tobacco advocates is the Investor‐​State Dispute Settlement mechanism.



 **What is Investor‐​State Dispute Settlement?**  
The ostensible purpose of ISDS is to ensure that foreign investors—usually multinational corporations (MNCs)—are protected against host government actions or policies that fail to meet certain standards of treatment and that cause the investor economic harm. The ISDS confers special legal privileges on foreign‐​invested companies, including the right to sue host governments in third‐​party arbitration tribunals for failing to meet those standards.



Investor‐​State Dispute Settlement dates back to the era following World War II, when previous European colonies were achieving independence and seeking to attract Western investment. It was borne as an expedient to overcome concerns about expropriation by new governments lacking experience with property rights and the rule of law.3 But ISDS procedures were rarely used. In fact, from the inception of ISDS in 1959 through 2002, the number of known ISDS claims worldwide stood at fewer than 100.4 However, during the 10 years between 2003 and 2012, the cumulative total increased to 514 cases.5 In 2012, claimants initiated 58 ISDS cases worldwide, which was the greatest number of initiations in any year, surpassing the previous record set in 2011.6



Provisions for ISDS are included in the 41 U.S. bilateral investment treaties in effect, as well as most of the U.S bilateral trade agreements.7 American TPP and TTIP negotiators are seeking ISDS rules in those agreements. Proponents argue that ISDS provides assurances against unfair treatment from host governments, strengthens the rule of law, and helps bring otherwise reluctant investors to capital‐​hungry jurisdictions. But looking more closely, ISDS arguably weakens the rule of law, forces the public to subsidize the risk of MNC investment abroad, and effectively encourages outsourcing.



 **Eight Good Reasons to Drop ISDS from TPP and TTIP**  
There are practical, economic, legal, and political reasons to expunge ISDS from current trade negotiations.



First, _ISDS is overkill_. Governments are competing to attract productive investment to keep their citizens employed and their economies growing. Accordingly, it is imperative to maintain smart, transparent, predictable policies that are administered fairly and nondiscriminatorily. Asset expropriation or other forms of shabby treatment of foreign companies is not likely to be rewarded by new investment.



Of course, that doesn’t guarantee that policies will never go astray. Sometimes they will. But investment is a risky proposition. Foreign investment is usually more risky. But that doesn’t necessitate the creation of institutions to protect MNCs from the consequences of their business decisions. Multinational companies are among the most successful and sophisticated companies in the world. They are quite capable of evaluating risk and determining whether the expected returns cover that risk. Although MNCs may want assurances, they don’t need them.



Multinational companies can mitigate their own risk by purchasing private insurance policies. Alternatively, they can condition investment on the host government’s agreeing to other protections, contractually. Whether the host agrees would be influenced by the supply of potential investors and the strings they would attach.



Second, _ISDS socializes the risk of foreign direct investment_. When other governments oppose, but ultimately concede to, U.S. demands for ISDS provisions, they may be less willing to agree to other reforms, such as greater market access, that would benefit other U.S. interests. That is an externality or a cost borne by those who don’t benefit from that cost being incurred. In this regard, ISDS is a subsidy for MNCs and a tax on everyone else. Taking the argument one step further, ISDS not only subsidizes MNCs, but particular kinds of MNCs. What may be too risky an investment proposition without ISDS for Company A is not necessarily too risky for Company B. By reducing the risk of investing abroad, then, ISDS is a subsidy for more risk‐​averse companies. It is a subsidy for Company A and a tax on Company B.



Third, ISDS encourages “discretionary” outsourcing. In the global competition to attract investment from the world’s best companies, the United States has some enormous advantages. For many decades, the United States has been the world’s premier destination for foreign direct investment. But in recent years, the United States has been slipping in a number of important investment‐​location decision criteria and, accordingly, its share of global foreign direct investment has declined from 39 percent in 1999 to 17 percent in 2011.8



While ISDS may benefit U.S. companies looking to invest abroad, it neutralizes what was once a big U.S. advantage in the competition to attract investment. Respect for property rights and the rule of law have been relative U.S. strengths, but ISDS mitigates those U.S. advantages. Access to ISDS could be the decisive factor in a company’s decision to invest in a research center in Brazil, instead of the United States. Why should U.S. policy reflect greater concern for the operations of U.S. companies abroad than for the operations of U.S. and foreign companies in the United States? Why should ISDS effectively subsidize outsourcing, and not insourcing?



To be sure, success abroad and success at home are closely correlated. Companies must be able to invest abroad to compete there, and the success of those foreign affiliates tends to be reflected in the performance of the parent companies at home.9 But there is a crucial distinction between “discretionary” and “nondiscretionary” outsourcing.



“Discretionary” outsourcing is investment that goes abroad, but doesn’t really have to. It is investment in activities that could be performed competitively in the United States, but is chased away by policies that make U.S. investment relatively more expensive. “Nondiscretionary” outsourcing is investment in activities that requires a foreign presence.



While we should not denigrate, punish, or tax foreign outsourcing, neither should we subsidize it, and ISDS subsidizes discretionary outsourcing.



Fourth, ISDS exceeds “national treatment” obligations, extending special privileges to foreign corporations. An important pillar of trade agreements is the concept of “national treatment,” which says that imports and foreign companies will be afforded treatment no different from that afforded domestic products and companies. The principle is a commitment to nondiscrimination. But ISDS turns national treatment on its head, giving privileges to foreign companies that are not available to domestic companies. If a U.S. natural gas company believes that the value of its assets has suffered on account of a new subsidy for solar panel producers, judicial recourse is available in the U.S. court system only. But for foreign companies, ISDS provides an additional adjudicatory option.



This inequality of treatment seems to run afoul of the investment provisions in the Baucus‐​Hatch‐​Camp legislation (to extend fast‐​track trade promotion authority to the president), which state that the principal U.S. negotiating objectives regarding foreign investment are to: “[R]educe or eliminate artificial or trade distorting barriers to foreign investment, while ensuring that foreign investors in the United States are not accorded greater substantive rights with respect to investment protections than United States investors in the United States…“10



Foreign investors having recourse to the U.S. legal system and then, if that produces unsatisfactory results, to third‐​party ISDS procedures arguably constitutes greater substantive rights for them than for domestic investors, whose options are confined to the U.S. legal system.



Fifth, _U.S. laws and regulations will be exposed to ISDS challenges_ with increasing frequency. The number of cases is on the rise. Most claims have been brought against developing countries—with Argentina, Venezuela, and Ecuador leading the pack—but the United States is the eighth‐​largest target, having been the subject of 15 claims over the years.11



As the percentage of global Fortune 500 companies domiciled outside the United States continues to increase, U.S. laws and regulations are likely to come under greater scrutiny. The specter of foreign companies prevailing in challenges of U.S. laws outside the U.S. legal system would frustrate further the task of selling trade to a skeptical public and would reward trade critics who have been warning of just such an outcome for many years.



Investor‐​State Dispute Settlement raises concerns about domestic sovereignty. Among recent cases highlighting these tensions is a suit brought by Philip Morris, Inc. against the Australian government for a law requiring that cigarettes be sold in plain packaging. Philip Morris claims that the requirement deprives it of its property (trademarks, logos, and labels), which is important for brand recognition and without which its revenues will decrease. Philip Morris may have a legitimate claim, but the optics will not be favorable for trade agreements if the company prevails.



Meanwhile, growing concerns in Europe about the vulnerabilities of environmental and public‐​safety laws to challenges by foreign corporations—sparked, in part, by a case brought by a Swedish energy company against Germany for its decision to abandon nuclear power—have led the EU to suspend ISDS negotiations in the TTIP for a period of three months, as it collects and evaluates public comments and reconsiders its position. Realistically, it is difficult to conceive of any benefits to including ISDS provisions in the TTIP, given the advanced legal systems in the United States and Europe, unless the wave of the economic future is expected to arrive in a tsunami of international litigation.



Sixth, _ISDS is ripe for exploitation by creative lawyers_. There is a lot of latitude for interpretation of what constitutes “fair and equitable” treatment of foreign investment, given the vagueness of the terms and the uneven jurisprudence. Thus, ISDS lends itself to the creativity of lawyers willing to forage for evidence of discrimination in the arcana of the world’s laws and regulations. Among the complaints worldwide in 2012 were challenges related to “revocations of licenses, breaches of investment contracts, irregularities in public tenders, changes to domestic regulatory frameworks, withdrawals of previously granted subsidies, direct expropriations of investments, tax measures and others.“12



Meanwhile, some agreements are attempting to expand the definition of a breach of the obligation of host governments to provide fair and equitable treatment to include: “targeted discrimination on manifestly wrongful grounds, such as gender, race or religious belief.” This attempt to broaden the scope for complaints—included in the EU‐​Canada trade agreement—should be a cause for concern.13



Seventh, _ISDS reinforces the myth that trade primarily benefits large corporations_. A persistent myth that has proven hard to dispel permanently is that trade benefits primarily large corporations at the expense of small businesses, workers, taxpayers, public health, and the environment. The fact is that trade is the ultimate trustbuster, ensuring greater competition that prevents companies from taking advantage of consumers. Lower‐​income Americans stand to benefit the most from trade liberalization, as the preponderance of U.S. protectionism affects products and services to which lower‐​income Americans devote higher proportions of their budgets.



But by granting special legal privileges to multinational corporations, ISDS reinforces that myth and is a lightning rod for opposition to trade liberalization. It is effectively a subsidy that mitigates risk for U.S. multinational corporations and enables foreign MNCs to circumvent U.S. courts when lodging complaints about U.S. policies. Ultimately, ISDS is unimportant to the task of trade liberalization and its inclusion in trade agreements only strengthens trade’s opposition.



Eighth, _dropping ISDS would improve U.S. trade negotiating objectives_ , as well as prospects for attaining them. Recently, a group of business associations joined in a statement of opposition to the requests for a tobacco carve‐​out provision, arguing that it would be superfluous and set a dangerous precedent that would undermine the settled view that governments are already entitled to regulate in the interest of protecting human life or health.14 Given their concern for the rule of law and the traditions of the trading system, the statement’s signatory organizations should be amenable to a compromise that would include purging ISDS from the TPP and the TTIP in exchange for a denial of the carve‐​out language.



Such a deal would assuage thoughtful critics of the trade agenda, who do not oppose trade, but who believe trade agreements should be more modest and balanced. Meanwhile, what now appears to be an angry mob protesting trade generally will be thinned out, exposing the unsubstantiated arguments of the professional protectionists who benefit by impeding Americans’ freedom to trade.



 **Conclusion**  
For practical, economic, legal, and political reasons, ISDS subverts prospects for U.S. trade liberalization. Yet it is tangential, at best, to the task of freeing trade. Any benefits to availing MNCs to third‐​party adjudication are all but totally overwhelmed by the additional costs. In the proverbial airplane that is down one engine and losing altitude, throwing ISDS out of the cargo hold to reduce unnecessary weight is the best solution.



At this point, it remains unclear whether the president is genuinely committed to doing what it will take to advance his trade agenda. But should he convince himself of the efficacy and righteousness of freeing trade and become interested in putting the necessary pieces together to bridge political divides, jettisoning ISDS and explaining how doing so liberates us from legitimate concern that corporations will run roughshod over domestic laws could go a long way toward selling these agreements to the public.



 **Notes**  
1\. Inside U.S. Trade’s World Trade Online, “Attorneys General From 42 States Call For Tobacco Carveout In Trade Deals,” February 6, 2014, www​.insid​e​trade​.com.  
2\. Simon Lester, “Free Trade and Tobacco: Thank You for Not Smoking (Foreign) Cigarettes,” Cato Free Trade Bulletin no. 49, August 15, 2012.  
3\. For a good history of ISDS, see Simon Lester, “Liberalization or Litigation? Time to Rethink the International Investment Regime,” Cato Policy Analysis no. 730, July 8, 2013.  
4\. United Nations Conference on Trade and Development (UNCTAD), “Recent Developments in Investor‐​State Dispute Settlement (ISDS),” IIA Issue Notes No. 1, May 2013.  
5\. Ibid   
6\. Ibid.  
7\. Shayerah Ilias Akhtar and Martin A. Weiss, “U.S. International Investment Agreements: Issues for Congress,” Congressional Research Service, April 29, 2013.  
8\. Daniel Ikenson, “Reversing Worrisome Trends: How to Attract and Retain Investment in a Competitive Global Economy,” Cato Policy Analysis no. 735, August 22, 2013.  
9\. Ibid., Figures 4, 5, 6, 7, 8, and 9; Matthew J. Slaughter, “American Companies and Global Supply Networks: Driving U.S. Economic Growth and Jobs by Connecting with the World,” Business Roundtable, the United States Council for International Business and the United States Council Foundation, January 2013.  
10\. United States Senate, Bipartisan Congressional Trade Priorities Act of 2014.  
11\. United Nations Conference on Trade and Development. To date, investors have not prevailed in any of their complaints against the United States.  
12\. United Nations Conference on Trade and Development.  
13\. Simon Lester, “An Equal Protection Clause for Investment,” International Economic Law and Policy Blog, February 6, 2014.  
14\. U.S. Chamber of Commerce, et al., “Joint Statement on Proposal to Modify Longstanding “General Exception” Provision and Dispute‐​Settlement Provision in the Trans‐​Pacific Partnership (TPP) Negotiations,” February 2014, http://​www​.amcham​.com​.au/​D​o​w​n​l​o​a​d​s​/​9​9​a​9​e​9​a​3​-​1​e​1​7​-​4​1​8​0​-​a​6​a​6​-​7​2​d​d​3​8​0​a65ca….
"
"
Share this...FacebookTwitterGerman skeptic site Die Kalte Sonne here debunks a recent alarmist article appearing in Spiegel aimed at shocking its readers. The reality, it turns out, is not shocking at all.
Greenland ice doomed?
According to Spiegel, the Greenland ice sheet is already doomed (that is unless we skip the usual democratic process and just act immediately).
Spiegel claims Greenland “glaciers are continuously losing huge masses of ice” and that the system there is “dramatically off balance”. The leftist Hamburg-based weekly reported:
The melting of the glaciers on Greenland has apparently passed the point of no return. Even if the global rise in temperature were to stop immediately, the ice sheet would continue to retreat, report researchers led by Michalea King of Ohio State University report in the journal “Communications Earth and Environment“.
Read more at Spiegel

4°C warmer 11,000 years ago
But Die kalte Sonne wondered if this were really so, and needed only 2 mouse clicks to find a recent temperature reconstruction for Greenland’s past (Lecavalier et al. 2017, pdf here). The paper’s Figure 4a  shows the temperatures, with the temperature of 1950 at the far right which in paleo-climatology is always meant as “present”.
 

Thus, 11,000 years ago, it was up to 4°C warmer than in 1950 over long periods of thousands of years, and today the warming has been about 1°C since then. Since we can see an ice sheet of 2,850,000 km³ (that is roughly Gt) today, the “point of no return” cannot have been exceeded 10,000 years ago. How does the heading then come about? We take a look at the associated work by King et al. 2020:
Dynamic ice loss from the Greenland Ice Sheet driven by sustained glacier retreat


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Greenland Ice Sheet is losing mass at accelerated rates in the 21st century, making it the largest single contributor to rising sea levels. Faster flow of outlet glaciers has substantially contributed to this loss, with the cause of speedup, and potential for future change, uncertain. Here we combine more than three decades of remotely sensed observational products of outlet glacier velocity, elevation, and front position changes over the full ice sheet. We compare decadal variability in discharge and calving front position and find that increased glacier discharge was due almost entirely to the retreat of glacier fronts, rather than inland ice sheet processes, with a remarkably consistent speedup of 4–5% per km of retreat across the ice sheet. We show that widespread retreat between 2000 and 2005 resulted in a step-increase in discharge and a switch to a new dynamic state of sustained mass loss that would persist even under a decline in surface melt.
Is there any talk of an irreversible end of the Greenland ice sheet? From the abstract:
We show that widespread retreat between 2000 and 2005 resulted in a step-increase in discharge and a switch to a new dynamic state of sustained mass loss that would persist even under a decline in surface melt.“
The authors see an acceleration in melting towards the ocean in the period 2000-2005, with not enough snowfall to compensate for the losses. They find a loss of about 500 Gt/year.
Only 0.15% of total ice mass
Unfortunately, they do not address the highly accurate gravity measurements with satellites in their paper. These data show a linear mass loss of only 275 Gt/year between 2003 and 2019 (with a gap in 2017 and 2018 due to a satellite change), so that in 17 years about 4200 Gt were lost, which is 0.15% of the total sheet.
What exactly do they say about the future?
Ultimately, predictions of future change will require improved understanding of the ice/ocean boundary and controls on glacier calving.“
Low-fact propaganda
This is much more cautious than what is being served up to us as “doomed” with the usual “overconfidence”. A look into the past is enough to unmask the media scream for what it is: low-fact propaganda.
Also read here at Ice Age Now.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLast year Germany’s Potsdam Institute (PIK) boasted that it had a superior El Niño one-year forecasting model, claiming 80% certainty. Today, a year later, its forecast emerges totally wrong and the prestigious institute is left humiliated. 
Hat-tip: Snowfan
In 2019, Germany’s Potsdam Climate Institute (PIK) boasted that it had a superior El Niño forecasting model, claiming one year in advance and with 80% certainty, there would be an El Niño event late in 2020 (upper curve is just an El Niño illustration). But the PIK model forecast flopped totally. The opposite has in fact emerged. Chart source: BOM (with additions).
One year ago, together with researchers of the Justus Liebig University Giessen (JLU), and Bar-Ilan University in Ramat Gan in Israel, Germany’s alarmist yet highly regarded Potsdam Institute for Climate Research (PIK) boldly declared in a press release there would “probably be another ‘El Niño’ by the end of 2020.”
PIK even boasted forecast model superiority
The PIK November 2019 press release bragged that its team of researchers had developed a new, far better model – which they said was capable of forecasting a late 2020 El Niño event a year in advance: “The prediction models commonly used do not yet see any signs of this,” the PIK press release wrote.
The PIK press release then called the early forecasting model approach “groundbreaking”, claiming it was based on a “novel algorithm” developed by its team. Their forecast relied “on a network analysis of air temperatures in the Pacific region and which correctly predicted the last two ‘El Niño’ events more than a year in advance.”
The results were even published in a journal: https://arxiv.org/abs/1910.14642


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Conventional methods are unable to make a reliable ‘El Niño’ forecast more than six months in advance. With our method, we have roughly doubled the previous warning time,” stressed JLU physicist Armin Bunde, who initiated the development of the algorithm together with his former PhD student Josef Ludescher.
John Schellnhuber: “80% certainty”…”pretty significant”
Prof. Hans-Joachim (John) Schellnhuber, Director Emeritus of PIK, explained: “This clever combination of measured data and mathematics gives us unique insights – and we make these available to the people affected.” He pointed out that, of course, the prediction method did not offer one hundred percent certainty: “The probability of ‘El Niño’ coming in 2020 is around 80 percent. But that’s pretty significant.”
The 20% uncertainty ends up humiliating PIK physicists
Using data from the past and with the help of of their algorithm, the PIK scientists said El Niño events could then be “accurately predicted the year before”.
Today, one year later, in November 2020, we see that the opposite is in fact occurring, see chart above. Now the equatorial Pacific is entering a La Niña event instead of the almost certain El Niño claimed earlier by the now embarrassed PIK researchers.
Can’t even get one climate component over a single year right
The PIK’s “high certainty” forecast misses totally and so underscores the risks and pitfalls of being overconfident when it comes to still poorly understood complex systems.
And if scientists struggle predicting just one single regional component of the entire climate for just one year, then imagine what the reliability of their complete climate system predictions going out decades has to be. GIGO!


		jQuery(document).ready(function(){
			jQuery('#dd_6a44e38d298891ff14bbe070b492c213').on('change', function() {
			  jQuery('#amount_6a44e38d298891ff14bbe070b492c213').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Die kalte Sonne

Atlantic region near Iceland has cooled over the past 120 years. Image: NASA (public domain)
(German text translated by P. Gosselin)
There are areas of the world that stubbornly resist “global warming”. These include an oceanic region near Iceland where sea surface temperatures have cooled by almost 1°C in the last 120 years.
Allan & Allan 2019 have examined the “cold blob” more closely and suspect that the summer ice melt will cause cold melt water to flow into the ocean, which will then lead to the winter cold of the sea area.
The researchers disagree with the model by Stefan Rahmstorf from Potsdam, who suggested a weakening of the Gulf Stream as the cause of the “cold blob”.
Here’s the abstract of Allan & Allan 2019:
Seasonal Changes in the North Atlantic Cold Anomaly: The Influence of Cold Surface Waters From Coastal Greenland and Warming Trends Associated With Variations in Subarctic Sea Ice Cover
Worldwide sea surface temperatures (SST) have increased on average by about 1 °C since 1900 with the exception of a region of the North Atlantic subpolar gyre near 50°N which has cooled by up to 0.9 °C over the same period, generating the negative feature on temperature anomaly maps which has been colloquially described by Rahmstorf et al. (2015, https://doi.org/10.1038/nclimate2554) as the “cold blob” (abbreviated here CB). This unique long‐term surface cooling trend is most evident in February, but in August net warming is observed even at CB epicenter and the CB itself is reduced to a mere “warming hole.” These seasonal changes in the intensity of the CB are the product of two separate factors: (1) a long‐term winter cooling specific for the CB region which appears to be associated with cooling of Greenland coastal waters in autumn, plausibly linked to summer meltwater from icebergs and sea ice and (2) summer warming effects which derive from (a) dramatic reduction in summer sea ice cover in the sub‐Arctic over the last 30 years that allows enhanced absorption of sunlight by the new open water in summer and (b) an unusual period of increased summer sub‐Arctic ice cover in the early twentieth century, which lowers the SST baseline measured from 1900, thus increasing the calculated linear rate of change of SST with time. Both of these effects could contribute to the observed Arctic amplification of warming.”


		jQuery(document).ready(function(){
			jQuery('#dd_432284bd7a4adeca2d29179312ac45d3').on('change', function() {
			  jQuery('#amount_432284bd7a4adeca2d29179312ac45d3').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"Since the 1980s, increasingly frequent and intense heatwaves have contributed to more deaths than any other extreme weather event. The fingerprints of extreme events and climate change are widespread in the natural world, where populations are showing stress responses. A common fingerprint of a warmer world is a range shift, where the distribution of a species moves to higher altitudes or migrates toward the poles. A review of several hundred studies found an average shift of 17km poleward, and 11 metres upslope, every decade. However, if temperature changes are too intense or lead species to geographic dead ends, local extinctions occur in the heat. In 2003, 80% of relevant studies found the fingerprints were seen among species, from grasses to trees and molluscs to mammals. Some migrated, some changed colour, some altered their bodies and some shifted their life cycle timings. A recent review of more than 100 studies found 8-50% of all species will be threatened by climate change as a result.  Currently, we have a disturbingly limited knowledge of which biological traits are sensitive to climate change and therefore responsible for local extinctions. However, a potential candidate is male reproduction, because a range of medical and agricultural studies in warm blooded animals have shown that male infertility happens during heat stress. However, until recently this had rarely been explored outside fruit flies in cold blooded animals. This is despite the fact that ectotherms – organisms that rely on heat in their environment to maintain a suitable body temperature – comprise most of biodiversity. Astonishingly, nearly 25% of all species are thought to be a beetle. The red flour beetle (Tribolium castaneum) is a useful ectotherm for large experiments on reproduction, as they can go from egg to adult in a month at 30°C. Females can store male sperm in specialised organs called spermathecae and they only need to keep 4% of a single ejaculate to enable them to produce offspring for up to 150 days. To look at the impact of heatwaves on reproduction, beetles were exposed to either standard control conditions or five-day heatwave temperatures, which were 5°C to 7°C above their preferred temperature. Afterwards, beetles mated and a variety of experiments looked for damage to their reproductive success, sperm form and function, and offspring quality. We found that 42°C heatwave temperatures halved the number of offspring males could produce relative to 30°C, with some males failing to produce any and mature sperm in female storage also experiencing damage from heatwaves. However, the reproductive output of pairs where only the females endured a five-day heatwave event was similar in all temperatures. The decline was likely due to a combination of males becoming worse at mating, less sperm being transferred, less sperm transferred being alive, less sperm being kept in the females’ spermathecae and more sperm being damaged and infertile. Two results were particularly concerning. These beetles, and many cold-blooded animals, can live for years and are likely to see multiple heatwaves. When we exposed males to two heatwave events, ten days apart, their offspring production was less than 1% of that of unheated males.  


      Read more:
      Wildlife winners and losers in Britain's summer heatwave


 This suggests that successive heatwaves can compound the damage of previous ones. The damage to offspring longevity and male fertility was another effect which was compounded over successive generations, and could lead to spiralling population declines. Knowing what aspects of biology higher temperatures could compromise is essential to understanding how climate change affects nature. Hopefully, this new knowledge can help predict which species are most likely to be vulnerable, allowing conservationists to prepare for the trouble ahead."
"Antimicrobial resistance (AMR) has been framed as one of the biggest threats to humanity in the 21st century. By 2050, more humans could die because of AMR than cancer. But despite alarming concerns from the early 1960s and warnings that the issue of antimicrobial resistance could cross barriers between animal species, the problems of antimicrobial use in animal farming have for long been ignored by policy makers and the food industry. Yet when the World Health Organisation (WHO) officially declared in 2001 that antimicrobial resistance was a “global problem” for both humans and animals, the link between animal farming and human health could no longer be ignored. Since then, significant public investments and political actions have been taken around the world to limit the use of antimicrobials in animal farming and raise awareness of the risks and problems related to the medicalisation of animal production.  Unsurprisingly, though, drugs are still massively consumed in animal farming and food production systems worldwide. Drugs have been instrumental to the modernisation of agriculture and are fundamental to the production of abundant and cheap food. In animal farming, drugs not only improve the health and growth of farmed animals, but are also part of the strategies employed by the food industry in order to produce more food for more people.  Public debates on the use of antimicrobials in animal farming have emerged at a time when vast epidemics of untreatable drug-resistant infections have emerged in humans because of antimicrobial resistance. Antibiotics that were once used to save lives against sepsis and were first introduced to help Allied soldiers fight the Nazis, soon became our worst enemies. In fact, the cost of new epidemics of drug-resistant infections has cast doubt on the benefit of using antimicrobials in animal farming in the global north, where countries spend vast amounts treating drug-resistant infections in their populations – the US spends US$34 billion a year on doing this. Contrary to what most newspaper headlines seem to suggest, antimicrobials are not the only drugs excessively used in animal farming. The intensive use of anthelmintics – a group of antiparasitic drugs – to prevent or regain production losses from multiple parasitic worms, has also progressively led to an increase in parasitic worm (helminth) resistance to available drugs in animal production. However, unlike antimicrobials, the crisis of this resistance in humans has been limited to the global south and neglected tropical diseases. Given the invisibility of their implications in the global north, anthelmintics are still massively consumed in animal farming. Since the beginning of the debate on drug resistance, many groups and voices, especially urban elites, have spoken about the medicalisation of animal farming, each offering its perspective and putting forward their preferred solutions: stopping animal farming in specific countries, regulating the use of antibiotics in farming, developing new antibiotics, vaccines and diagnostic tests, promoting biomedical research, developing lab-grown meat, educating veterinarians, and educating farmers. Because of their portrayed “ignorance”, farmers became stigmatised and their practices framed as part of the problem of drug resistance in the city. Associations, decision makers, the pharmaceutical industry, the food industry, experts and health professionals all called on farmers to stop using drugs in farmed animals, pushing the rhetoric towards the idea of an end to animal farming. Talking with livestock farmers across the UK, my recent research reveals that decisions made by farmers on the use of drugs in animal farming are not governed by ignorance. Instead, they are shaped by what farmers consider as appropriate practices for maintaining the well-being of their livestock and the tradition of rural professions – besides more mundane aspects that help farmers ensuring the sustainability of their businesses.  In fact, the use of drugs by farmers is situated within a larger context of animal production systems and food demand, which themselves contribute to the emergence of animal diseases, the medicalisation of animal farming and drug resistance. Urban elites often see farmers as “technicians” and the farming community as a ground for applying what is considered the best practices for improving animal health, welfare and the sustainability of the food industry. This denial of rural agency has been happening for some time and has led to an increasingly visible social divide between urban and rural. This social dislocation has recently been demonstrated in France with the gilets jaunes (yellow vests) movement. Sadly, urban elites tend to disregard the fact that farmers are the ones closest to the land and their livestock, and have everything to lose by not taking care of it. If public authorities are serious about tackling drug resistance and supporting animal well-being, sustainable food production and local economies, when it comes to farming, farmers should be the first we listen to."
"Bumblebees are in drastic decline across Europe and North America owing to hotter and more frequent extremes in temperatures, scientists say. A study suggests the likelihood of a bumblebee population surviving in any given place has declined by 30% in the course of a single human generation. The researchers say the rates of decline appear to be “consistent with a mass extinction”.  Peter Soroye, a PhD student at the University of Ottawa and the study’s lead author, said: “We found that populations were disappearing in areas where the temperatures had gotten hotter. If declines continue at this pace, many of these species could vanish forever within a few decades.” The team used data collected over a 115-year period on 66 bumblebee species across North America and Europe to develop a model simulating “climate chaos” scenarios. They were able to see how bumblebee populations had changed over the years by comparing where the insects were now to where they used to be. Dr Tim Newbold, of University College London’s Centre for Biodiversity & Environment Research, said: “We were surprised by how much climate change has already caused bumblebee declines. Our findings suggest that much larger declines are likely if climate change accelerates in the coming years, showing that we need substantial efforts to reduce climate change if we are to preserve bumblebee diversity.” Bumblebees play a key role in pollinating crops such as tomatoes, squash and berries. The researchers say their methods could be used to predict extinction risk and identify areas where conservation actions are needed. Prof Jeremy Kerr, of the University of Ottawa and the study’s senior author, said: “This work also holds out hope by implying ways that we might take the sting out of climate change for these and other organisms by maintaining habitats that offer shelter, like trees, shrubs or slopes, that could let bumblebees get out of the heat. “Ultimately, we must address climate change itself and every action we take to reduce emissions will help.” The research is published in the journal Science."
nan
"

Mount Kilamanjaro – Tanzania, Africa – still snowy. Photo by Neil Modie, January 2008
Last week, I broke the story of a press release issued by NOAA where they publish an opinion smashing any link between hurricanes and global warming saying that “There  is nothing in the U.S. hurricane damage record  that indicates global warming has caused a  significant increase in destruction along our coasts.”
Many readers may recall that Al Gore used hurricanes prominently in An Inconvenient Truth, and mentions hurricane Katrina specifically. Gore claims that increased hurricane activity is caused by global warming.
Last week, when the NOAA press release came out smashing any link between hurricanes and global warming, I wrote to my local newspaper editor, David Little, and said to him “Do you care to bet that AP and Reuters won’t run this story?” He responded: “I hope they do, it seems newsworthy to me.”
Well here is is, 4 days later, not a peep.
A Google search of news stories for “NOAA increased hurricane” (keywords of the press release) reveals a tiny handful of stories about the press release. Could you imagine though if the story said the reverse?  What if NOAA claimed they had established a definitive link between global warming and hurricanes. Oh my, the humanity of it all! Gloom, doom, death, destruction, angst, and demands for action on Kyoto. If it bleeds it leads. Compare to all the stories still circulating about hurricane Katrina and global warming.
Here is another story about a point from Gore’s AIT hit parade; Mount Kilimanjaro. Mr. Gore asserted that the disappearance of snow on Mount Kilimanjaro in East Africa was expressly attributable to global warming; “Within the decade, there will be no more snows of Kilimanjaro.” That was in 2005 in his movie An Inconvenient Truth.
Deforestation seems to be causing Mount Kilimanjaro’s shrinking glacier. Researchers think deforestation of the mountain’s foothills is the most likely culprit. Without the forests’ evapotranspiration of humidity into the air, previously moisture-laden winds blowing across those forests now blow drier. The summit, no longer replenished with water from those winds, started shrinking. Studies show the ice is evaporating through a process called sublimation. You can witness this effect at home, have you ever noticed that ice cubes left in your freezer tend to shrink with time?
Last year, a British Court ruled Gore’s point about Kilimanjaro not to be true.
So when a news story crossed my desk today that said: “Mount Kilimanjaro: On Africa’s roof, still crowned with snow” I had to wonder, will we see this one covered in the main stream media? Or maybe those beacons of truth over at Real Climate will make a note of it?
Don’t hold your breath. But, at least the New York Times travel section covered it. It seems more of a touristy thing to have snow on Kilimanjaro than a scientific issue of truth I suppose.
UPDATE: Kate over at SDA created a collage over time showing the snow of Mt. Kilimanjaro:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1177354',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
BUMPED for visibility. Originally published on 6/24. Bumped on 6/28 and again on 6/30
This poll will gauge reader perception to the issue that Dr. Hansen of NASA has recently raised that I cover in my post here. One vote per computer, and please spread this permalink to the poll far and wide to get a good mix of input across the blogosphere.

Click on a dot, then click the little yellow vote icon. Poll closed.
I will run this poll 1 week until next Wednesday at 9AM PST, at which time it will close. The results will be submitted to a member of the U.S. Senate for distribution, NASA’s director, and will also be mailed to Dr. Hansen at NASA GISS.
You can subscribe to the results of this poll by RSS. Simply copy the link below into your RSS reader.
http://polldaddy.com/pollRSS.aspx?id=49940E93EC30ACAF
NOTE: A couple of Pro-Hansen sites have staged a “crash party” for this poll. This has accounted for a huge increase in the votes for the first question overnight. This sometimes happens with online polls when agenda driven activists decide to skew it, which is the biggest weakness of online polls.
Addendum: Some other sites that are not Pro Hansen have also now linked to this poll, so I suppose it is becoming a battle between opposing views now. Agenda driven activists on both sides are at work now. 
Update 7/1 It appears that about 8000 votes were added for question 1 overnight. -Anthony
Update 7/2 9 AM PST Poll is closed, more here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e1960fd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Question: Why does a major grocery store chain need a “comprehensive policy addressing climate change”?
Answer: They don’t.

The Atlanta Business Chronicle reports that one of the nations oldest and largest grocery firms, Kroger Inc., based in Cincinnati, OH rejected a shareholder proposal which called for the company to develop a comprehensive policy addressing climate change.
Having shopped at many a Kroger store myself, I’m glad I won’t be bombared with climate change messages while I shop. I really don’t need to know what the carbon footprint is on a can of soup or a head of lettuce.
Cincinnati-based Kroger (NYSE: KR) operates more than 2,400 supermarkets and multidepartment stores in 31 states.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e935c4a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Atlassian co-founder Mike Cannon-Brookes has thrown his support behind a climate action bill proposed by the independent Zali Steggall and has urged the major parties to put down the cudgels and support it. And the Australian Energy Council, representing major electricity and gas businesses, said the Steggall bill deserved to be seriously considered as it had the potential to deliver certainty and a path forward for the national economy.  Cannon-Brookes said on Tuesday the Steggall proposal, unveiled this week, was “a smart bill, and the exact type of action we need to change Australia’s international reputation on climate”. The bill includes a proposal for a net zero emissions target by 2050, a carbon emissions budget, and assessments every five years of national climate change risk. The MP has called on the major parties to bring the bill to the floor and allow a conscience vote. Cannon-Brookes said the proposal contained all the elements of a viable settlement to the climate wars. “The legislated 2050 target and five-year increments are precisely what is required, and the bill deserves bipartisan support.” Cannon-Brookes has been vocal in support of climate action in Australia, and has supported independents through Climate 200 – an environmental lobby group also supported by Simon Holmes à Court. But he has not played a hands-on role in drafting the current proposal. Sarah McNamara, the Australian Energy Council’s chief executive, said Steggall’s bill was a considered attempt to find a workable framework and had the potential to move the country beyond the climate policy impasse. “We would encourage that it be carefully considered and calmly assessed,” she said. “It offers the potential for much-needed certainty and a clear path forward not just for the energy industry, but for the Australian economy. For our part we will be consulting with our members in the coming weeks to assess the bill in detail.” It follows a declaration by the Business Council of Australia that Australia should work to achieve net zero emissions by 2050. Independent MP Zali Steggall – along with her fellow crossbenchers Rebekah Sharkie, Helen Haines and Andrew Wilkie – is seeking bipartisan support for a climate change framework bill aimed at transitioning Australia to a decarbonised economy.  This is what's in it. • Aims to limit global warming to well below 2C, pursuing efforts to limit it to 1.5C above pre-industrial temperatures. • Net zero carbon emissions target by 2050. • To achieve the target, the minister creates an emissions budget. • Establishes a Climate Change Commission to prepare a national climate change risk assessment every five years. • The commission is made up of a chair, Australia's chief scientist and five other members – who must have experience in either climate science, business, agriculture, environmental management, energy, transport or regional development. * The assessments cover the risks climate change poses to the economy, society, agriculture, environment and ecology. • In response, the minister creates a national adaptation plan. • The commission provides the minister with yearly adaptation plan progress reports. • Decisions made under the Act must be based on the best available science. • It abolishes the current Climate Change Authority, with the commission to take its place. While the BCA has at times in its history has been riven on climate action, and has actively stymied progress, the organisation’s chief executive Jennifer Westacott told the ABC on Monday night the time had now come to deliver policy certainty. “I reckon if we could get the two political parties to agree to that and legislate it, we would have made a massive advance in this country because we would know where we’re going,” Westacott said. While the BCA in 2018 described Labor’s more ambitious 2030 emissions reduction target as “economy wrecking”, in 2019 it joined other groups in representing industry, unions, farmers and investors under the Australian Climate Roundtable banner in calling for policies that could put it on a path to net zero emissions. On Monday night, Westacott characterised the Steggall proposal as “sensible”. Steggall’s bill will not be brought on for debate unless either the government or Labor supports it reaching the floor of the House. The government has not yet made a decision but it is unlikely to support it. On Tuesday morning, the Labor leader Anthony Albanese said it was highly unlikely the bill would be voted on “because that’s what happens with private member’s bills in the House of Representatives, unless the government agrees to allocate time for the bill, it will not be voted on”. Albanese said the proposal was very well intentioned, and he “respected” Steggall for bringing it forward, but told the ABC “we are unlikely to have a conscience vote on climate change. What we’ll do is support action on climate change.” The Labor leader said the opposition would commit to a long-term emissions reduction target “very soon” and, referencing an internal split within the Coalition about taxpayer backing for new coal plants, said: “I don’t think there is a place for new coal-fired power plants in Australia. Full stop.” On Sunday, Labor’s deputy leader Richard Marles, in a particularly awkward interview, did not rule out the party supporting new coal developments, saying it would be a decision for the markets despite previously declaring it would be a “good thing” if the thermal coal market collapsed."
"Stink bug. As names go it is a PR disaster, each of the words alone hardly endearing, and, in combination, wholly off-putting. Which is a shame because stink bugs have a perky charm, a distinctive style and, for an insect, a surprising concern for their offspring.  The trouble is that a new member of their clan is on its way to the UK: the marmorated stink bug, Hyalomorpha halys. The press is on the case, combining a distrust of anything arriving from the continent to live in the UK with dire warnings of damaged apples and nasty smells. Stink bugs deserve a better name and they have one: shieldbugs. This is the much more widely used name in the UK for the Pentatomidae, Scutelleridae and allied families of the order of insects called the Heteroptera, or true bugs. All of them have glands producing nasty smelling defensive secretions, but they are also united by many much more likeable characteristics. The UK has an enchanting selection of native species, such as the forget-me-not shieldbug, ornate shieldbug and parent shieldbug. A much more enticing set of names, hinting at their often striking form and behaviour.  Most are less than a centimetre long, either neatly oval or akin to a medieval knight’s triangular shield. Some such as the hawthorn and red-legged shieldbugs have sharply angled front corners to their thorax, giving a hint of 1980s shoulder pads.  Many are brightly coloured, black and white or with bold yellow or red.  Even the more conventionally dowdy brown or green species often sport zebra striped antennae and are edged with a black and white chequered border. The marmorated stink bug goes in for this chequered style too: marmorated refers to a marbled-effect pattern. The “marbled shieldbug”. That sounds much better. The marmorated stink bug often feeds on soft fruit which it probes using a long proboscis to suck the juices.  If fruit are attacked by large numbers of bugs the puncture wounds disfigure the crops and they are no longer marketable. Some bugs also have the potential to transmit crop diseases.  On the other hand this group of bugs includes many species that show devoted parental care for their eggs and newly hatched young. The adults stay close to batches of eggs, often squatting over them, to fend off predators. When the young hatch they cluster together much like a proud gaggle of primary school children on their first outing. What provokes the greatest ire, however, is the smell. Glands between the first and second pair of legs release a foul odour if the bugs are attacked. The stink from a marmorated bug is like a nasty version of coriander and very persistent.  Over the past decade the bugs have spread from their original home in China, Japan and Korea to become established as an invasive species in the US, where they are considered a major agricultural pest. They come into houses to hibernate and beleaguered home-owners who unwittingly try to dispose of them are left regretting their actions as the frightened bugs let rip with those armpit stink glands  I’ve had a soft spot for shieldbugs ever since receiving a letter from a member of public wondering if I could identify the strange insect she had found.  Inside the letter, immaculately displayed where the sorting machine had squished it flat, was a birch shieldbug, laid out with a precision to gladden the heart of the most painstaking curator.  While many dead insects shrivel or curl, the shieldbug’s tough design holds its shape, the curves and fins reminiscent of American 1950s automobiles.  The live bugs are even better. They tend to walk slightly high on their front legs lending an inquisitive air. They do not scuttle or jump but instead the legs on either side alternate back and forth much like a wind-up toy. Everything about them is slightly retro, a steampunk insect. The bug is likely to be on its way to the UK. A few stink bugs have been found in passenger luggage from the US but the real invasion threat is cross channel, part of a trend for continental insects to establish in the UK in recent years. Many have gone unremarked outside of specialist insect newsletters but others have attracted wider attention such as the willow emerald and small red eyed damselflies. This summer saw a widespread scatter of the scarce tortoiseshell butterfly, recorded only once before in the UK.  We like damselflies and butterflies. Less lovely is the spread of the bluetongue virus, a livestock disease that seems to have arrived with midges. What all these cases have in common is continental, warmth-loving species expanding their ranges north-westwards across Europe and hopping over to the UK. The marmorated stink bug is a good flyer. It is spreading. Maybe we should start calling it the marbled shieldbug and wait to see if it is quite the nuisance that its press suggests."
nan
"
Share this...FacebookTwitterMerkel No Longer Backs World Climate Treaty
That’s the headline announcing a report in the upcoming issue of Der Spiegel. German Chancellor Angela Merkel is retreating from the objective of reducing global CO2 emissions through a binding global treaty. http://www.spiegel.de/spiegel/vorab/0,1518,691013,00.html. 
…Merkel wants to avoid another debacle for Germany and Europe in the UN climate negotiations.
According to Der Spiegel, the climate conference that began in Petersberg near Bonn on Sunday shifts the focus to climate protection measures that can lead to measurable results without a binding treaty.
Federal Minister of the Environment Norbert Röttgen of Merkel’s CDU party told SPIEGEL of the new approach:
It’s not about giving up on the 2°C target; rather it is about finding new ways to reach it. At Petersburg  we want to create a new level on which we not only want to reach CO2 targets from the top,  but also to start projects from the bottom that lead to measurable results.
This includes protection of forests and more concrete cooperation in the transfer of environmentally friendly technologies.
Make no mistake about it, without the cooperation of Germany any global binding treaty mandating CO2 reductions becomes extremely unlikely, and that sends a clear signal that global Cap & Trade is all but dead. The US Senate can (and should) now kill Cap & Trade for good.
H/t: Rudolf Kipp at http://www.science-skeptical.de/
UPDATE…read the entire Der Spiegel article here in English (h/t Brian H): http://www.spiegel.de/international/germany/0,1518,691194,00.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNaomi Orsekes
My how the times have changed. Climategate, and all the other gates surrounding it, have turned things inside-out. The science is far from settled, as many of us have long suspected. The ranks of sceptic scientists are swelling, public opinion has swung; even the Royal Society has adopted a new position on climate science –  by George, there might be more to it than CO2 molecules after all!  The mainstream media is slowly coming around, too.
Yet, others refuse to hear it. 
Here’s a Youtube clip of Naomi Oreskes’ Truth About Denial presentation in 2007. Some of you may have watched it already. That presentation is in two parts.
Part 1: The Truth Part (CO2 drives global warming, there’s a consensus, science is settled).
Part 2: The Denial Part (There’s a disinformation campaign out there, denying it all).
Okay, that was back in 2007. Back then global warming science looked convincing, and so maybe such a position was plausible.
But here’s Oreskes in March 2010  in a presentation called the Merchants of Doubt, which is pretty much the same as her 2007 Truth About Denial. Despite all the new revelations, scandals and shifting scientific viewpoints, Oreskes continues to play the same music.  In the 2010 presentation she continues to ask (paraphrasing):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




How can there be so much scepticism in the public when there’s consensus among scientists?  Where does all the public doubt come from?
And answers by claiming it all stems from a tiny few merchants of doubt, who she describes as:
…a small but powerful group of people aided and abetted by well-funded think-tanks and a compliant mass media…not for money, but in defense of an ideology of laissez-faire governance, opposition to gevernment regulation in all forms.
Yes, ladies and gentlemen, Oreskes still believes, despite all the new revelations we’ve seen over the last few months, that all the scepticism and denialism out there today is still coming from the same sinister merchants of doubt. You’d think she’d would step back for a minute and re-evaluate her position. No chance.  Instead her reaction is to drive her head yet further into the sand.
Oreskes claims to be a science historian. My question is: Will she wake up and start a new chapter for the science history books? Or will she continue repeating her fairy tales? Don’t hold your CO2 breath.
UPDATE: Yesteday this post appeared at position No. 6 when one googled “Naomi Oreskes’ Denial”. Today it has dropped off to No. 17.
Share this...FacebookTwitter "
"Well-meaning celebrities and MPs recently published a letter in the Guardian, calling for a ban on trophy hunting imports into the UK. To the novice conservationist, this surely sounds like a good thing, right? After all, trophy hunting kills animals so how could it possibly be good for conservation?  Unfortunately, these arguments are, at best, ill-informed and, at worst, they divert attention from the most pressing causes of biodiversity loss. The Guardian letter states that, over the last decade, hundreds of trophies have been imported into the UK. The CITES trade database lists the number of trade-restricted wildlife products entering and leaving a country. A quick perusal of it shows that, between 2008-2017, the UK imported more than 800 CITES-listed trophy products, averaging fewer than 100 trophies imported per year. To put this into perspective, more than 100 elephants are thought to be illegally poached every two days for their ivory, meaning the elephants killed for trophies and imported into the UK are an infinitesimally small number compared to the massive threat of poaching.  Poaching for the illegal ivory trade is not the same thing as legal trophy hunting and, while every death counts towards the decline of a species, we must not forget that trophy hunting helps reduce the greatest threat to terrestrial mammals: habitat loss. Don’t get me wrong, trophy hunting is morally repugnant. I cannot understand why anyone would want to kill an animal for fun – just as I can’t understand why anyone with other dining options would eat an animal, as we don’t need meat to survive.  Ethically, it makes sense to ban trophy hunting imports if the goal is to provide the greatest good for the greatest number of animals. But there are issues with this line of reasoning. Habitat loss, where land is converted for human use, remains the biggest driver of wildlife declines. Hunting reserves retain natural land for the benefit of trophy species like zebras and impalas, as well as a whole host of other biodiversity, such as birds, plants, insects and small mammals.  


      Read more:
      Trophy hunting is not poaching and can help conserve wildlife


 In effect, trophy animals become martyrs killed so other wild animals can benefit from the ever-dwindling resource of land. By diverting attention from these more pressing causes of wildlife decline by focusing on banning trophy imports, we may be left patting ourselves on the back and thinking that we’ve done our part for conservation and can all go home. I wish conservation needed such an easy fix, but sadly that is not the case. If these animal-loving politicians and celebrities are serious about conserving wildlife, they may be more effective focusing their energy towards the much bigger issues of agricultural expansion, the illegal wildlife trade and the ever-expanding threat of climate change.  If we addressed these more drastic problems we would likely save far more animals from untimely death while ensuring we have wildlife populations for generations to come. 


      Read more:
      How we arrived at a $1 billion annual price tag to save Africa's lions


 If we really want to “bend the curve” on biodiversity loss, we may do better byreducing our meat intake, hold politicians accountable for the UK’s climate change targets and reduce our overall consumption of goods. Like the signatories of the Guardian’s letter, I too want to protect the world’s wildlife. But let’s not kid ourselves into thinking red herrings like banning UK trophy hunting imports will be the silver bullet needed for addressing the sixth mass extinction. Knee-jerk reactions, while imbued with noble intents, will not save lions, elephants and rhinos."
"
Share this...FacebookTwitterScientists continue to publish papers revealing no unusual climate trends for the last several centuries in many regions of the world.
Despite the 135 ppm increase in CO2 concentration (275 ppm to 410 ppm) since the 1700s, a new 250-year temperature (precipitation) reconstruction (Peng et al., 2020) shows there has been no net warming in Central Asia since 1766. Two other reconstructions from this region also show no warming trend in recent centuries.

Image Source: Peng et al., 2020
Earlier this year we highlighted a new study that indicated France was up to 7°C warmer than today about 7800 years ago after cooling by 3°C in the last 200 years.
Another new study (Esper et al., 2020) suggests there has been no net warming in Spain since 1350 A.D.
The years that spanned 1474-1606 A.D. scored 7 of the 10 warmest years in the record. In contrast, there has been only 1 warmest year (1961) and 4 of the 10 coldest years since 1880.
The 2 warmest 30-year (climate) periods occurred in the decades surrounding the ~1530s and ~1820s.
The authors record a “striking” and abrupt (within decades) 1°C warming trend during the late 1700s to early 1800s that exceeds any temperature change in the modern record.

Image Source: Esper et al., 2020
Share this...FacebookTwitter "
"Representatives of almost all the countries on the planet are gathering in Katowice, Poland, for the 24th Conference of the Parties (COP24) of the UN Framework Convention on Climate Change (UNFCCC). They will set the course for action on climate change by discussing the implementation plan for the 2015 Paris Agreement which aims to coordinate international effort to halt warming at 1.5°C. The COPs receive significant media attention and, sometimes, even notable public interest. They take place every year as an opportunity for countries to collectively assess progress on dealing with climate change. In 2018 the negotiations kick off barely two months after a report by the UN’s Intergovernmental Panel on Climate Change (IPCC) warned that the international community only has a 12-year window to drastically reduce greenhouse gas emissions. Clearly, 24 years after the first COP there is a deep disconnect between how urgently the world needs effective climate policy and the pace of discussing global mechanisms on how to abate greenhouse gas emissions.  The first COP meetings held in the 1990s led to the creation of the Kyoto Protocol in 1997, which set binding emissions targets for developed countries over two “commitment periods” (2008-2012 and 2013-2020). However, the Kyoto agreement failed as the US did not ratify it and because several inconclusive conferences followed its implementation. COP15 in Copenhagen in 2009 also failed to yield any agreement on binding commitments for the second commitment period. A few major countries agreed to a short accord recognising the need to limit global temperature rises to 2°C, but there were no substantial guidelines on how to do so.  Similarly, COP19 in Warsaw four years later did not finalise any binding treaty. It only recognised “a flexible ruling” on differentiated responsibilities and loss and damage. In Warsaw, the international community failed to take essential steps for the future. Some even think that the 2013 conference cast some doubt on the capacity of the Polish government to successfully lead COP24 in 2018. Against this backdrop, COP21 in Paris in 2015 appeared to generate the most optimistic outcome in two decades of international climate negotiations. In Paris, the world leaders agreed on a general action plan that legally binds countries to have their progress tracked by technical experts.  The countries who signed up also agreed on a “global stocktake” – a process for reviewing collective progress towards achieving the long-term goals of the agreement. However, lots of details about the Paris Agreement still have to be nailed down. This is precisely what the international community seeks to do this December in Poland. The major objective for COP24 is to agree upon the so-called Paris “rulebook” – the details of how nations should implement the Paris Agreement and report their progress. Three major areas of political discussion will receive most attention: finance, emission targets, and the role of “big” states. Finance In 2015, richer countries pledged US$100 billion a year by 2020 for poorer nations to mitigate the effects of climate change. However, the climate funding is still about US$20 billion short. COP24 delegates will need to discuss in more detail on when the rest of the money will be generated before committing to the rulebook.  Perhaps even more importantly, rules for where that money comes from, and particularly whether international loans are acceptable, still have to be agreed on. Because finance is closely linked to issues of justice and fairness in the international system, it is unlikely that this discussion will lead to more generous levels of climate aid – although there is space for improvement, and some past conferences have actually provided small but significant advances on this front. Emission targets COP24 also needs to set some form of flexible yet comparable rules that will govern the Paris Agreement. One groundbreaking feature of the Paris Agreement is that all parties agreed to commit to national contributions to climate action. In other words, the agreement is based on a bottom-up process in which countries largely determine their own contributions, and then act upon them. This COP may settle on some basic strategies for verifying climate actions, but it is very unlikely that the international community will agree on any mechanisms for delivering sanctions to states that do not meet their targets, because of the high sensitivity towards financial costs for non-abatement. The role of ‘big’ states Finally, while “small” countries will have an important role to play at the negotiations as usual, there are several question marks around the large countries that need to bear a lot of the efforts to curb greenhouse gas emissions.  It will not help that President Donald Trump, who intends to withdraw the US from the Paris Agreement, decided in 2017 to cancel climate funding for poor nations. The US position at COP24 will also affect China and India, which are likely to continue disagreeing with rich countries on some fundamental issues. Additionally, the domestic politics of Russia and Brazil point to more uncertainty for cooperation. The urgency to reach key milestones in the Paris Agreement and deal with climate change puts a lot of high expectations on COP24. Unfortunately, many challenges stand ahead of international climate cooperation.  Approaching the negotiations with the right level of reason and determination will be critical to manage expectations and avoid any media “hysteria”, as
media coverage can hurt the climate talks by shifting attention from the policy issues to unproductive discussions of whether climate change is influenced by humans.  For a credible and valid rulebook, we need frank conversations about energy transition and compensating the “losers” of climate policies, such as people working in high-emission sectors.  There might be the opportunity to do so in Katowice, an industrial hub and coal-mining city. We will see if this COP will highlight the necessary transition from fossil fuel industry to renewable solutions as the negotiations unravel."
"
Share this...FacebookTwitterHow is it that a settled science keeps finding things never expected?
For example, the HIAPER Pole-to-Pole Observations (HIPPO) mission was launched in January 2009 and will make a series of five flights over three years covering more than 24,000 miles to sample the atmosphere in some of the most inaccessible regions of the world. Read HIPPO background here.
The goal of the mission is the first-ever, global, real-time sampling of carbon dioxide and other greenhouse gases across a wide range of altitudes in the atmosphere, from pole-to-pole.
Professor Mark Zondlo of Princeton University has taken measurements of water vapour in the atmosphere, from 14 km high to just above the sea ice, using a vertical cavity surface mini laser hydrometer.
Watch Zondlo video here.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here are some of Professor Zondlo’s observations so far:
We don’t really know how clouds are formed. Water vapour impacts the climate more than any other gas.
What we are finding is surprising. Large plumes of water vapour exist in areas we never expected to find them.
Learning how this fits into the puzzle is crucial for predicting climate and making smart policy decisions.
What does that mean? It means the climate models used so far were nothing more than junk, thus the same applies for their predictions. They completely neglected the water vapour factor (and who knows what other factors).
Climate forecasting is best left to real forecasters, and not tainted modelers.
Share this...FacebookTwitter "
"
The survey project continues to move forward, even in these cold and snowy winter months. I’m pleased to announce that we have just passed the 500 mark for surveyed stations. Now with 41.1% of the network surveyed comprising 502 stations surveyed so far, that leaves 719 to go out of 1221 stations nationwide.
Some stations have recently become catalysts for larger investigations, such as the station in Lampasas TX, done by Julie K. Stacy which has brought out questions from a number of other bloggers. This prompted a review of stations previously surveyed, such as Cedarville, CA, which then prompted a larger investigation in the satellite city nightlights methodology used by NASA GISS. A whole new avenue of exploration has now opened up not just for US stations, but worldwide thanks to new features of Google Earth.
You never know where curiosity and serendipity will lead you. Thanks to Atmoz for starting the ball rolling. I also want to thank Barry Wise and Gary Boden, our early volunteers, whose help on this project has been indispensable.
Recently, this project got a significant endorsement from Dr. Roger Pielke of the University of Colorado in Boulder in his weblog. I and all the volunteers appreciate the recognition.
Here is the latest breakdown of USHCN stations that have been surveyed, and their site quality ratings:


We could really use some help this spring and summer in the following states:
Kansas, Nebraska, Arkansas, Alabama, Illinois, Idaho, Kentucky, Tennessee, Missouri, Mississippi, North Dakota,  South Dakota, Oklahoma, Texas.
If you think that you can help with this project by surveying a station near you, please visit the www.surfacestations.org website and sign up. We’ll provide instructions and help on locating stations in need of surveying.
You may also wish to consider signing up for the national flower and foiliage survey to help track climate change which is prominently mentioned on Dr. Roger Pielke’s weblog.  You can double your fun!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea10c3fc4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

You know, for as much as we humans think we really have control over our planet, nature tends to remind us from time to time that we are just flyspecks in the vastness of space and energy. Take for example the amount of energy we get from the sun: 174.0 PetaWatts – (10^15 watts) which is the total power received by the Earth from the Sun. Now compare that to this news item.
From Slashdot: Astronomers are still speculating as to what could have caused an abnormally strong five millisecond burst to be detected six years ago when it completely saturated their recording equipment. From the article: ‘The burst was so bright that at the time it was first recorded it was dismissed as man-made radio interference. It put out a huge amount of power (10^33 Joules), equivalent to a large (2000MW) power station running for two billion billion years.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3705f38',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Over the past few weeks Clover Hogan has found herself crying during the day and waking up at night gripped by panic. The 20-year-old, who now lives in London, grew up in Queensland, Australia, cheekbyjowl with the country’s wildlife, fishing frogs out of the toilet and dodging snakes hanging from the ceiling. The bushfires ravaging her homeland over the past few weeks have taken their toll. “I’ve found myself bursting into tears … just seeing the absolutely harrowing images of what’s happening in Australia – it is overwhelming and terrifying.”  Hogan said her lowest point came when she heard about the death of half a billion animals incinerated as the fires swept through the bush. “That was the moment where I felt my heart cleave into two pieces. I felt absolutely distraught.” The physical impact of the climate crisis is impossible to ignore, but experts are becoming increasingly concerned about another, less obvious consequence of the escalating emergency – the strain it is putting on people’s mental wellbeing, especially the young. Psychologists warn that the impact can be debilitating for the growing number of people overwhelmed by the scientific reality of ecological breakdown and for those who have lived through traumatic climate events, often on the climate frontline in the global south.. Until two years ago Dr Patrick Kennedy-Williams, a clinical psychologist from Oxford, had spent his career treating common mental health difficulties including anxiety, depression and trauma. Then something new started to happen. Climate scientists and researchers working in Oxford began to approach him asking for help. “These were people who were essentially facing a barrage of negative information and downward trends in their work … and the more they engaged with the issue, the more they realised what needed to be done – and the more they felt that was bigger than their capacity to enact meaningful change,” he said. “The consequences of this can be pretty dire – anxiety, burnout and a sort of professional paralysis.” Kennedy-Williams began to research the topic and realised it was not just scientists and researchers who were suffering. “There is a huge need among parents, for instance, who are asking for support on how to talk to their kids about this.” When Kennedy-Williams began focusing on young people he assumed most would be older teenagers or at least have started secondary school. But he soon discovered worrying levels of environment-related stress and anxiety in much younger children. “What I was most surprised by is how young the awareness and anxiety starts. My own daughter was just six when she came to me and said: ‘Daddy, are we winning the war against climate change?’ and I was just flummoxed by that question in the moment. It really showed me the importance as a parent of being prepared for the conversation, so we can respond in a helpful way.” He says there is no way to completely shield young people from the reality of the climate crisis, and argues that would be counterproductive even if it were possible. Rather, parents should talk to their children about their concerns and help them feel empowered to take action – however small – that can make a difference. A key moment for Kennedy-Williams came with the realisation that tackling “climate anxiety” and tackling the climate crisis were intrinsically linked. “The positive thing from our perspective as psychologists is that we soon realised the cure to climate anxiety is the same as the cure for climate change – action. It is about getting out and doing something that helps. “Record and celebrate the changes you make. Nobody is too small. Make connections with other people and at the same time realise that you are not going to cure this problem on your own. This isn’t all on you and it’s not sustainable to be working on solving climate change 24/7.” This certainly resonates with Hogan, who has set up Force of Nature, an initiative aimed at helping young people realise their potential to create change. Hogan’s group aims to target people aged 11-24 with a crash course in the climate crisis that helps them navigate their anxiety and realise their potential to get involved, take action and make a stand. “This is only the beginning,” said Hogan. “We’re going to see massive, massive widespread climate crisis in every country around the world, so it’s about developing the emotional resilience to carry on, but in a way that ignites really dramatic individual initiative.” Beyond climate anxiety – the fear that the current system is pushing the Earth beyond its ecological limits – experts are also warning of a sharp rise in trauma caused by the experience of climate-related disasters. In the global south, increasingly intense storms, wildfires, droughts and heatwaves have left their mark not just physically but also on the mental wellbeing of millions of people. For Elizabeth Wathuti, a climate activist from Kenya, her experience of climate anxiety is not so much about the future but what is happening now. “People in African countries experience eco-anxiety differently because climate change for us is about the impacts that we are already experiencing now and the possibilities of the situation getting worse,” she said. She works with young people through the Green Generation Initiative she founded and sees the effects of eco-anxiety first-hand. A common worry she hears among students is: “We won’t die of old age, we’ll die from climate change.” Extreme climate events can create poverty, which exacerbates mental health problems, and Wathuti says she has seen stress, depression and alcohol and drug abuse as some of the side-effects of climate anxiety and trauma in her country.  Even in the UK, a recent study by the Environment Agency found that people who experience extreme weather such as storms or flooding are 50% more likely to suffer from mental health problems, including stress and depression, for years afterwards. More than 1,000 clinical psychologists have signed an open letter highlighting the impact of the crisis on people’s wellbeing and predicting “acute trauma on a global scale in response to extreme weather events, forced migration and conflict”. Kaaren Knight, a clinical psychologist who coordinated the letter, said: “The physical impacts related to extreme weather, food shortages and conflict are intertwined with the additional burden of mental health impacts and it is these psychologists are particularly concerned about.”  She added that fear and trauma “significantly reduced psychological wellbeing”, particularly in children. “This is of huge concern to us and needs to be part of the conversation when we talk about climate breakdown.” One of the high-profile signatories of the letter, Prof Mike Wang, the chair of the Association of Clinical Psychologists UK, said: “Inaction and complacency are the privileges of yesterday … Psychologists are ready and willing to help countries protect the health and wellbeing of their citizens given the inevitable social and psychological consequences of climate change.” This rallying of the psychological profession around the climate crisis has led to experts around the world forming groups to research and treat the growing number of people caught up in the unfolding crisis, attempting to help them move from fear and paralysis towards action. But even for those who are following this advice, the scale of the emergency is taking its toll. Kennedy Williams – who has set up his own group, Climate Psychologists, specialising in climate anxiety – said he and his colleagues were not immune from the psychological impacts of the crisis. “This is such a universal thing that [we] have all been through our own set of climate-related grief and despair, and we talk about riding the wave between hope and despair … it is absolutely as real for us as it is for anyone else.” Remember that you do not need to be a climate expert It’s OK to explore learning together. If your child asks a question you can’t answer immediately, respond by saying: “What a great question. Let me look into that so I can answer it properly.” Try to validate, rather than minimise, children’s emotions If children express anxiety, it’s much better to say: “It’s OK to feel worried. Here is what we can do about it,” than to say: “Don’t worry. It’s all fine.” But always try to support this emotion with suggestions for positive action. Negative information hits harder Bad or threatening facts tend to resonate more strongly – and therefore stick in the mind. So try to balance one piece of negative news with three pieces of positive news. Have some examples of good climate-related news ready – for example, successful conservation projects. For younger children, keep it local and tangible Suggest litter picks and school events. For teenagers, encourage them to stay connected at a wider level – help them write to their MP, take part in protests and join local communities and campaigns. Set practical goals as a family and follow through Record and celebrate your climate successes together (even a piece of paper on the fridge door). Reinforce the message that small actions can make a big difference."
"What do Beyonce, Hitler, David Attenborough, Darth Vader and GoldenPalace.com all have in common?  They all have species named after them.  In the case of Beyonce it is an Australian horse fly whose striking golden behind apparently inspired the scientists to give this species the scientific name Scaptia beyonceae. Most species do not have such frivolous scientific names.  Last week a new species of frog was described from New York City.  It has been named Rana kauffeldi, in honour of the American herpetologist Carl Kauffeld who in the 1930s predicted a new species of leopard frog would be found on the east coast of the US. What’s in a name? And why don’t scientists simply number species?  The scientific name is not an arbitrary label, well at least not the first part, which tells us the genus of the species.  From knowing this we can start to understand the evolutionary relatedness between species.  For example, chimpanzees and bonobos both come from the genus Pan; whereas humans are from Homo.  Thus, as a scientist I know that chimpanzees and bonobos are closer to each other than they are to humans.  Scientific names according to the naming rules for species must be unique and show evolutionary relatedness; that is, relate to the importance of common ancestor species. The reason we need scientific names and not just common names is to permit scientists to precisely identify the species they are investigating.  Returning to our New Yorker Rana kauffeldi, there are 15 species of leopard frog and in many countries common names are generic or vary by region.   I have spent many years studying titi monkeys in Brazil of which there are more than 20 species, but in Minas Gerais where I study them they are all referred to in Portuguese as guigó.  Once I was giving a talk at the University of São Paulo in Brazil about my titi monkey research and I noticed a look of puzzlement on the audience’s faces until I showed a slide of my study animal when the audience collectively shouted out sauá. If I had use the monkey’s scientific name I would have avoided ten minutes of bemused expressions. The second part of a scientific name is chosen by whoever first described the species in a scientific journal, and this is where opportunity lies.  Many species names refer to physical characteristics, such as Artibeus hirsutus for the hairy fruit eating bat, its geographic location Ovis canadensis for bighorn sheep or after an appropriate eminent scientist such as Rhinoderma darwinii for Darwin’s frog.  A successful taxonomist may sell the rights to a person or a company to award a species its name. The Golden Palace titi monkey (Callicebus aureipalatii) is a case in point.  This online gambling site in 2004 paid US$650,000 to name the species and the funds were used for the conservation of the monkey’s habitat in Bolivia.  There are now websites where you can bid to name species.  Some people have criticised this approach to raising funds for species conservation as being vulgar and too commercial, liking it to how sponsorship has taken over sport. When I was a child there was “The FA Cup”, these days it is the “The FA Cup with Budweiser” – a change that has generated a lot of money for football.  Yet such sporting examples are ephemeral: the FA Cup will not always belong to the same sponsor. Scientific names are permanent however, and can only be changed in accordance with the rules of the International Code of Zoological Nomenclature.  Thus, while it may be extremely unpalatable to think there is a beetle, which in 1937 was named  Anophthalmus hitleri – the rules do not permit a name change. There has been alarm that some companies, which do considerable environmental damage, might use species naming as a form of greenwashing.  What I would like to suggest here is that rather than paying a one-off fee to name a species companies would need to pay into an environmental endowment fund. Thus, the impact of their funds would be positive for the environment in the long-term. Each year approximately 15,000 new species are given a formal scientific name, creating lots of sponsorship opportunities.  Of course companies will prefer to sponsor charismatic species such as monkeys, dolphins or parrots.  While such species aren’t as common as new insects, there should be enough to go around.  In Brazil, a new primate species is discovered on average once a year.   Since funds would be to protect the sponsored species’ habitat this will result in the protection of the non-cute species in that habitat.  Thus, sponsored animals would become what we conservation biologists call umbrella species, inadvertently sheltering others in their habitat."
"

Back in December, Senate Democrats, with President Obama’s backing, attempted to prohibit anyone on the federal government’s terrorism watchlist from purchasing a firearm.   
  
  
At the time, I criticized the proposal for its lack of process and its inevitable inefficacy at reducing gun crime or terrorism.   
  
  
Yesterday, Senate Democrats launched a filibuster in order to push for the resurrection of the failed “No‐​Guns List.”   
  
  
The substance of their plan has not changed, and my earlier criticism still stands:   




How does a person prove they are not a terrorist? It’s virtually impossible. A no‐​flyer doesn’t receive the evidence against them or a hearing before being placed on the list. They are not allowed to confront their accuser. Even getting the government to acknowledge that a person is on the list may require lengthy and expensive litigation. A person on the no‐​fly list may not even know they are on the list until they’re refused service at the airport. A person on the broader terror watch list has no means of finding out. The system is devoid of anything resembling due process, a flaw _The New York Times_ condemned as being intolerable in a free and democratic society and over which the American Civil Liberties Union is currently suing the Obama administration. The no‐​fly listing procedure has already been declared unconstitutional by at least one federal judge.   
  
  
Including too many people on the list is inevitable. Nobody wants to explain, after a terrorist attack, why the attacker wasn’t in the database. And that overly inclusive quality has manifested itself in absurd ways already. Just a few examples of no‐​fly denials: the late Democratic Massachusetts Sen. Ted Kennedy, congressman and civil rights hero John Lewis, dozens of people named Robert Johnson, members of the U.S. military and federal air marshals.   
  
  
The potential for false positives and mistaken identities is not just accepted as collateral damage by these no‐​gun list proposals; it is the entire point. Anyone who has actually been convicted or is currently charged with terrorism‐​related crimes is already prohibited from purchasing a firearm under federal law. The people adversely affected by this proposal will inevitably be people against whom the government lacks sufficient evidence to charge.   
  
  
The fact that a person hasn’t been adjudicated as dangerous doesn’t preclude them from committing violence, of course. But just how much discretion should the president have in abolishing constitutional rights without charge or trial?



What _has_ changed is the political climate in the interim.   
  
  
The No‐​Guns List appears to have picked up some powerful allies on the right. Presumptive Republican presidential nominee Donald Trump has expressed support for the idea, and is apparently lobbying the National Rifle Association to come along with him.   
  
  
The GOP and the NRA are generally regarded as the two primary bulwarks against misguided gun control proposals. Adding their weight to this particular gun control proposal would bolster its legislative prospects immensely.   
  
  
Even if, as some supporters have urged, the law requires hearings before a watchlisted person can be denied the right to bear arms, important questions remain. What exactly does the state need to prove in order to take someone’s 2nd Amendment rights away? What is the burden of proof? Will judges allow the use of secret evidence, citing state secrecy concerns for refusing to disclose it? Will the individual be entitled to legal representation? Can he call and cross‐​examine witnesses? Can he appeal the ruling? Can he publicly discuss his case?   
  
  
And those are just the legal concerns. There are also pragmatic issues. What information does the FBI convey to the gun seller when someone on the list is denied? Is the gun seller told that he’s got a terror suspect standing in his store? What if the person actually is an aspiring terrorist under government surveillance? Doesn’t this process inevitably tip him off? Would finding out that he’s on the government’s radar only encourage an aspiring terrorist to act quicker? Would it compromise legitimate surveillance operations?   
  
  
The Boston bombers didn’t need guns. Nor did Timothy McVeigh or the 9/11 hijackers. Giving terror suspects a sure‐​fire way to figure out whether they’re being surveilled seems like a large price to pay for what may be a non‐​existent benefit.   
  
  
Omar Mateen passed background checks. He passed training requirements. He had access to weapons as a security guard. He wasn’t even on the terrorism watchlist. Nothing in this proposal, and nothing in any of the other gun control proposals this tragedy has spawned, would have kept firearms out of Omar Mateen’s hands. The only way his rampage could have been prevented was for someone to kill him first. Unfortunately, laws that deny even sober people the right to carry weapons in establishments that serve alcohol meant that the law‐​abiding victims were sitting ducks.   
  
  
Knee jerk reactions to horrible tragedies have proven to be a poor basis for good public policy. We have institutions like due process precisely for times when emotions threaten to overrun safeguards that are just as important for protecting the innocent as the guilty.   
  
  
It’s hard to imagine a graver violation of the spirit of the 2nd Amendment than a law allowing the President to declare anyone an enemy of the state without so much as a charge and subsequently bar them from exercising their 2nd Amendment rights. But Republicans, lured from their stalwart support of gun rights by fears of terrorism, and Democrats, lured from their stalwart support of civil rights by their zeal for gun control, combined with an election cycle that has been defined by appeals to fear may be creating a perfect storm and a severe threat to liberty.   
  
  
P.S. Two tweets this morning from sitting Congressmen highlight the divide.   
  
  
Democratic Senator and gun control advocate Joe Manchin doesn’t inspire confidence when he says things like “due process is killing us.”   




> .@Sen_JoeManchin: Due process is what's killing us right now https://t.co/OTf9LnxHXZ
> 
> — Morning Joe (@Morning_Joe) June 16, 2016



.@Sen_JoeManchin: Due process is what's killing us right now https://t.co/OTf9LnxHXZ



Luckily, not everyone in Congress agrees.   




> Amazing that U.S. senators would filibuster in favor of using secret lists, like some authoritarian regime, to deny rights w/o due process.
> 
> — Justin Amash (@justinamash) June 16, 2016



Amazing that U.S. senators would filibuster in favor of using secret lists, like some authoritarian regime, to deny rights w/o due process.
"
"

The recession of 2007–2009 knocked the wind out of state government budgets. Yet, as revenues have risen steadily in recent years, some governors have pursued reforms to reduce tax burdens on families and make their states more competitive. Other governors have used rising revenues to expand programs. In their biennial survey, **“Fiscal Policy Report Card on America’s Governors 2014” (White Paper)** , Nicole Kaeding, a Cato budget analyst, and Chris Edwards, director of tax policy studies at the Institute, use statistical data to grade the governors on their taxing and spending records. “Reading the report card and other works by the institute may change some minds,” according to Forbes​.com. “But more importantly, it broadens the debate over the role of fiscal policy in particular and government more generally.” Four governors were awarded an “A” on this report card: Pat McCrory of North Carolina, Sam Brownback of Kansas, Paul Le‐ Page of Maine, and Mike Pence of Indiana. Eight governors were awarded an “F”: Mark Dayton of Minnesota, John Kitzhaber of Oregon, Jack Markell of Delaware, Jay Inslee of Washington, Pat Quinn of Illinois, Deval Patrick of Massachusetts, John Hickenlooper of Colorado, and Jerry Brown of California. “With the economy currently growing, governors and legislatures are having few problems balancing their budgets in the short run, but the states face major budget challenges down the road,” the authors write. At the same time, global economic competition is making it imperative that states improve their investment climates.



 **IS PRESCHOOL EFFECTIVE?**  
Demands for universal preschool programs have now become commonplace, reinforced by President Obama’s call for “highquality preschool for all” in 2013. Yet as David J. Armor, professor emeritus at George Mason University, points out in **“The Evidence on Universal Preschool” (Policy Analysis no. 760)** , any program that could cost state and federal taxpayers $50 billion per year warrants a closer look at the evidence on its effectiveness. This paper reviews the major evaluations of preschool programs, including both traditional programs such as Head Start and those considered high quality. As it turns out, these evaluations do not paint a generally positive picture. “The most methodologically rigorous evaluations find that the academic benefits of preschool programs are quite modest, and these gains fade after children enter elementary school,” Armor writes. This is the case for Head Start, Early Head Start, and also for the “high‐​quality” Tennessee preschool program. Two other high‐​quality programs have been evaluated using a rigorous experimental design, and have been shown to have significant academic and social benefits, including long‐​term benefits. These are the Abecedarian and Perry Preschool programs. However, the groups studied were very small, they came from single communities several decades ago, and both programs were far more intensive than the programs being contemplated today. Armor concludes, “Before policymakers consider huge expenditures to expand preschool, especially by making it universal, much more research is needed to demonstrate true effectiveness.”



 **GOOD INTENTIONS, IMPOVERISHED RESULTS**  
Over the last half century, federal and state governments have spent more than $19 trillion fighting poverty. But what have we really accomplished? In **“War on Poverty Turns 50: Are We Winning Yet?” (Policy Analysis no. 761)** , Michael Tanner, a Cato senior fellow, and Charles Hughes, a research associate at the Institute, argue that, although far from conclusive, the evidence suggests that we have successfully reduced many of the deprivations of material poverty. However, these efforts were more successful among socioeconomically stable groups such as the elderly than low‐​income groups facing other social problems. “Moreover, other factors like the passage of the Civil Rights Act, the expansion of economic opportunities to African Americans and women, increased private charity, and general economic growth may all have played a role in whatever poverty reduction occurred,” the authors write. Nevertheless, even if the War on Poverty achieved some initial success, the programs it spawned have long since reached a point of diminishing returns. In recent years we have spent more and more money on more and more programs, while realizing few, if any, additional gains. We may have made the lives of the poor less uncomfortable, but we have failed to truly lift people out of poverty. This should serve as an object lesson for policymakers today. “Good intentions are not enough,” Tanner and Hughes conclude.



 **WORK DISINCENTIVES**  
The Social Security Disability Insurance (SSDI) program faces imminent insolvency. Annual expenditures totaled $143 billion in 2013, but program receipts amounted to $111 billion—a shortfall that is projected to continue indefinitely. In **“SSDI Reform: Promoting Gainful Employment while Preserving Economic Security”(Policy Analysis no. 762)** , Jagadeesh Gokhale, senior fellow at the Cato Institute, points out that, according to the Social Security Trustees, the program’s trust fund will be fully depleted in 2016, compelling either a large benefit cut or a large tax hike. Neither option will be politically popular. Regardless of the program’s insolvency, SSDI creates substantial work disincentives, causing many with medical impairments who could work to withdraw from the labor force and apply for SSDI. Gokhale advocates a change in the structure of SSDI’s benefit payments to those admitted to the program. Shifting benefits at the margin toward paying beneficiaries to work rather than to remain out of the work force would encourage beneficiaries with residual capacities to return to work. “That shift would serve as a backstop to reduce the economic loss from wrongful allowances of applicants into SSDI,” Gokhale writes. “Such a switch in benefit design can be accomplished without compromising benefit eligibility for those who cannot work.” In this analysis, he explains how to implement such a change to SSDI’s benefit structure and the advantages that would accrue from it.



 **DISTORTING TRADE**  
The use of antidumping measures to protect certain domestic industries may be the most widely abused trade policy instrument worldwide,” writes K. William Watson, trade policy analyst at the Cato Institute. In **“Will Nonmarket Economy Methodology Go Quietly into the Night? U.S. Antidumping Policy toward China after 2016” (Policy Analysis no. 763)** , Watson argues that U.S. authorities reserve their most punitive and abusive practices for goods from China. In those cases, the United States sets antidumping duties using what is called nonmarket economy (NME) methodology. The practice gives license to the U.S. Department of Commerce to ignore Chinese producers’ cost and price data and to turn, instead, to estimates for those data that are punitive and unrealistic. Current WTO rules permit the United States to maintain this discriminatory approach, but that condition will expire in December 2016. Absent a major change in the mindset of U.S. trade officials with respect to Chinese treatment in antidumping proceedings, it is unlikely that the United States will bring its policy into compliance. Watson presents some of the alternative scenarios that might unfold as the expiration date approaches. “The policy that would best serve a strong U.S. trade agenda and the American public is to end NME treatment of China by no later than December 2016,” he concludes. Nondiscriminatory treatment of Chinese imports would bring U.S. trade policy into compliance with WTO rules while reducing the distorting effect of antidumping measures on the U.S. economy.
"
"In the UK it is illegal to deliberately kill or injure red squirrels, disturb them while they are using a nest, or destroy their nests. Yet, although the 1981 Wildlife and Countryside Act provides these protections, there is a legal anomaly in England and Wales – one that can potentially undermine the conservation of the red squirrel, along with every other rare and endangered forest plant or animal species. Although rare woodland species are protected, the habitat they dwell in is generally not. Timber harvesting requires a licence – although there are some very limited exceptions where this permission is not needed, for example due to public safety, or where small volumes of wood are being cut. But under the 1967 Forestry Act, applications in England and Wales cannot be refused for “the purpose of conserving or enhancing” flora or fauna (though they can be refused for this purpose in Scotland). Nor can licence conditions be imposed for this reason. No matter how rare, how vulnerable or how much effort has gone into the regional conservation of a species, there are no exceptions to this. A timber felling licence does not sweep aside the legal protection that animals such as the red squirrel have – and a precautionary approach is advisable when felling in woodlands containing this species. Nevertheless, the possession of a felling licence opens a loophole because the wildlife legislation protecting the red squirrel provides the defence of “incidental result of an otherwise lawful operation”. So, with a licence in hand, woodlands containing this threatened species can be clearfelled because tree harvesting is a lawful operation.  The solution is clearly to amend the Forestry Act to better align timber harvesting and wildlife protection laws. Harmonising UK forestry legislation would allow for better timing, methods and patterns of tree harvesting to be guaranteed in habitats containing any rare species. Additionally, while licensing authorities currently can only assess each felling licence application in isolation, legislative change would enable the cumulative impact of granting a licence to be considered in relation to felling that had previously been approved. This stops management of rare woodland species on specific sites being at the mercy of timber prices and market economics. Commercially managed forests provide jobs and produce valuable products. As the modernised laws in Scotland show, the forest industry operates quite successfully where timber harvesting licence applications can consider wildlife impacts. Amendment in England and Wales would deliver similar integration.  Consequently, the ethical credentials of the timber harvesting industry would be strengthened. In an age where consumers want confidence that timber products they purchase have not destroyed wildlife populations, this is essential. It is already commonplace for products made of UK-sourced wood to have the Forest Stewardship Council (FSC) logo. The FSC signifies the wood is from sustainable sources managed with a high regard for wildlife conservation. So amendment of the 1967 Forestry Act would give greater consumer confidence in supply chains and also reinforce the credibility of the global FSC forest certification scheme itself.  Since the 1980s, the forestry sector has increasingly balanced commercial, societal and environmental imperatives. Consequently there will be times, should the law change, when refusal of a logging license to conserve biodiversity is an unavoidable trade off. Here it is important to stress that the forest industry receives state grants to support crop establishment and protection. The taxpayer therefore has a right to ensure that forests are managed sympathetically for wildlife. We should not forget that commercial plantations can be vitally important for wildlife and without them many species would be much rarer. On the other hand, some felling will inevitably still be licensed even though operations will adversely affect individual animals of a protected species through habitat loss or alteration. Although such decisions may be unpopular with local people, it is common for wildlife management strategies to focus on population level conservation targets rather than at the individual animal level. I believe an amendment to the Forestry Act is overdue. Regulatory change will empower authorities with the legal tools to achieve a better balance between often competing forest management objectives. It will benefit wildlife and the UK timber industry too."
nan
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
There has been a lot of discussion lately about the accuracy of measuring Sea Surface Temperatures prompted by a new study from Phil Jones from the University of East Anglia and Director of UEA’s Climatic Research Unit. The measurement issue for sea surface temperatures that Dr. Jones is studying was recently showcased in an article in the UK Independent.
I’m going to present the article here first, and then we’ll talk about how sea surface temperatures have been measured, and what sorts of issues the changes between cloth buckets, metal buckets, and engine inlets actually entails.
At first glance, I see this issue raised by Phil Jones as not being well thought through, and ignoring the measurement environment actuality, instead focusing on the change in bucket types as being “absolute”. I think it has a lot of grey area, and a lot of potential errors that haven’t been considered. I’ll cover those in the next part, but for now please read the article and let me know what you think.
Case against climate change discredited by study
By Steve Connor, Science Editor
Thursday,  29 May 2008

A difference in the way British and American ships measured the temperature  of the ocean during the 1940s may explain why the world appeared to undergo a  period of sudden cooling immediately after the Second World War.
 Scientists believe they can now explain an anomaly in the global temperature  record for the twentieth century, which has been used by climate change skeptics  to undermine the link between rising temperatures and increases in atmospheric  carbon dioxide.
The record for sea-surface temperatures shows a sudden fall after 1945, which  appeared to go against the general trend for rising global average temperatures  during the past century.
Skeptics have argued it supports the idea that rising temperatures have more  to do with increased solar activity – sunspots – than increasing levels of  man-made carbon dioxide exacerbating the greenhouse effect.
However, an international team of scientists has investigated the raw data  from the period. They found a sudden increase from 1945 onwards in the  proportion of global measurements taken by British ships relative to American  ships.
The scientists point out that the British measurements were taken by throwing  canvas buckets over the side and hauling water up to the deck for temperatures  to be measured by immersing a thermometer for several minutes, which would  result in a slightly cooler record because of evaporation from the bucket.
The preferred American method was to take the temperature of the water sucked  in by intake pipes to cool the ships’ engines. Those records would be slightly  warmer than the actual temperature of the sea because of the heat from the ship,  the scientists said.
Taking into account the difference in the way of measuring sea-surface  temperatures, and the sudden increase in the proportion of British ships taking  the measurements after the war, the result was an artificial lowering of the  global average temperature by about 0.2C, said Professor Phil Jones of the  University of East Anglia in Norwich.
“It occurred in the period of the 1940s when the number of observations of  sea-surface temperature were markedly fewer than either before or after that  period and most of the measurements were made by British and American ships.  This made the apparent anomaly more pronounced,” Professor Jones said.
The study, published in the journal Nature, found that the global average  temperatures in the late 1940s stayed roughly the same rather than falling.  David Thompson of Colorado State University, the team’s leader, said a drop was,  in effect, an artifact rather than a real observation.
“I was surprised to see the drop so clearly in the filtered data, and working  in partnership with others, realized it couldn’t be natural,” Dr Thompson  said.
Although the initial drop was significant, it did not last. By the 1960s,  many other nations began taking ship-borne measurements of ocean temperature, minimizing the discrepancy.
Professor Jones said that the study lends support to the idea that a period  of global cooling occurred later during the mid-twentieth century as a result of  sulphate aerosols being released during the 1950s with the rise of industrial  output. These sulphates tended to cut sunlight, counteracting global warming  caused by rising carbon dioxide.
“This finding supports the sulphates argument, because it was bit hard to  explain how they could cause the period of cooling from 1945, when industrial  production was still relatively low,” Professor Jones said.
A similar problem could be occurring now with the move from ship-borne  measurements to those from unmanned buoys, which tend to produce slightly lower  records. This could explain why global average temperatures in recent years have leveled off.

FYI: According to the American Meteorological Society:
bucket thermometer—A water-temperature thermometer provided with an  insulated container around the bulb.




It is lowered into the sea on a line until it has had time to reach the temperature of the surface water, then withdrawn and  read. The insulated water surrounding the bulb preserves the water reading and  is also available as a salinity sample. 






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f033b91',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Mining and heavy industry companies, including BHP and Alcoa, have again been allowed to lift their greenhouse gas emissions without penalty under a climate change policy that the Australian government promised would prevent national pollution increasing. Under changes posted online on Thursday, BHP coal and iron ore mines in Western Australia and Queensland, Alcoa’s Portland aluminium smelter in Victoria and a Boggabri coalmine in New South Wales were each given the green light to emit more under the scheme known as the “safeguard mechanism”.  Of those made public, the allowed increases ranged between 3% to 33% above previous emissions limits. Not all the increases were published. Two of three BHP mines moved from annual to multi-year emissions limits, which means they promised to emit less over the next two years to make up for excess emissions last year. The increases were signed off despite the safeguard mechanism’s promise to limit emissions from big polluters to ensure they do not just cancel out cuts paid for by taxpayers through the Coalition’s main climate policy, the emissions reduction fund. Under the scheme, every industrial facility across the country that emits more than 100,000 tonnes a year was set a pollution limit, known as a baseline, based on either its historic emissions or an independent forecast of future emissions. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on how much they expect to emit per unit of production. The Australian Conservation Foundation found increases approved over the past two years alone allowed more than 7m tonnes of potential extra emissions each year – about 1.3% of annual national carbon pollution. The increases allowed under Thursday’s changes, including those from a new Jemena gas pipeline between Tennant Creek and Mount Isa, are at least 236,658 tonnes a year. Bret Harper, director of research with energy and carbon consultants RepuTex, said it showed the safeguard mechanism was a “pretty ineffective policy”. “The bottom line is there is no accountability for any emissions increases that do occur,” he said. Emissions from electricity generation have reduced and the drought has triggered a drop from agriculture, but cuts in those sectors have been effectively cancelled out by increases, mostly from big industry. Emissions have increased from the liquified natural gas (LNG) industry, mining sector and transport, in particular. Emma Herd, chief executive of the Investor Group on Climate Change, said national climate policies did not send a strong enough market signal to unlock the private investment needed to put Australia on a path to zero emissions. She said it should take the opportunity to strengthen the safeguards mechanism and other emissions reduction policies after a review led by businessman Grant King and through the development of a long-term emissions reduction strategy. “Unless we provide tighter emissions pathways to net-zero for large emitters it’s hard to see how we can reduce pollution in line with our overarching commitments under the Paris agreement,” Herd said. The Coalition has changed how it describes the safeguard mechanism over time. In 2016, the then environment minister, Greg Hunt, said it would ensure emissions cuts contracted through the emissions reduction fund were not offset by significant increases above business-as-usual levels elsewhere in the economy. But a government climate policy document released before last year’s election said the mechanism required Australia’s largest emitters to “measure, report and manage” their emissions, not that it would limit pollution. An analysis by RepuTex found the government regulator had approved a 32% increase in how much large industrial facilities were allowed to emit under the scheme. While not every company emitted up to their limit, the most recent data, for 2017-18, showed emissions from large industry were up 12% since 2015. Some companies that have exceeded their limit have been expected to buy carbon credits to offset the additional emissions, or pay a penalty. Over the first two years of the scheme companies paid for cuts equivalent to 707,625 tonnes of emissions. The government denies the scheme is a form of carbon pricing. The minister for emissions reduction, Angus Taylor, did not respond to a request for comment before publication."
"It seems new action to tackle plastic pollution is announced every week, from the 5p plastic bag charge to governments debating a tax on plastic packaging. Businesses are also showing their green credentials as major supermarkets pledge to reduce plastic packaging alongside some multinational companies. With such serious steps, it looks like our problem with plastic will soon be fixed.  Before we get too excited though, other recent news stories include billions of dollars being invested in new plastics refineries and plastics being found everywhere, including in our soil. It’s estimated that 4.8–12.7m metric tonnes of plastic enters the ocean from land-based sources annually. That’s everything from toothbrushes to microplastics worn off vehicle tyres. The plastics found in the ocean come from every country in the world and if we are to tackle it we need a worldwide solution.  Like COP24 for climate change, an international summit for plastic pollution could achieve just that. We do have some international laws that attempt to tackle plastic pollution. The UN Convention on the Law of the Sea contains a commitment to “prevent, reduce and control pollution from land-based sources” which covers plastics. More recently, the Honolulu Strategy was agreed in 2011 to help tackle marine debris coming from land-based activities. If these commitments were to be fully met then our plastic problem would be vastly reduced. One issue is that these obligations depend on plastic being recognised as harmful to humans or marine life. Plastic has long been considered a wonder material, which makes modern life possible. Like other “wonderful inventions” such as the ozone-eating CFCs, it is only as plastic has started to accumulate in the world that we have realised it is a problem. A second issue is that each country has responded to this problem in different ways. Kenya, for example, has adopted legislation banning single use plastic bags, while the UK has added a charge to their use.  Current proposals to tackle plastics focus on increasing recycling. It is worth remembering though that only around 11% of plastic is currently recycled around the world. If we are to rely on recycling as a means to tackle plastic pollution we need to rapidly increase recycling in almost every country.   An increase in recycling to the extent needed can’t happen overnight. We’d need effective and accessible recycling facilities and public education. Both would need huge investments of time and resources across the world.  A treaty may be one way of coordinating such action and sharing knowledge about how best to improve recycling. Countries already share knowledge about how they meet some treaty obligations through reports to a governing body on climate change, a similar approach could be taken in a plastics treaty. Another measure being used is taxation. The assumption is that if we make plastics more expensive then either less will be used or alternative materials will replace them. Deposit return schemes are also suggested as a way to “nudge” producer and consumer behaviour. These types of measures do not always, however, prompt the desired response.  Sometimes, for example, costs are simply passed on to consumers. It is also difficult to apply these measures in emerging economies which lack the same regulatory bodies and infrastructure to monitor these measures, so other approaches may be needed. Governments have faced the question of how to tackle a pervasive pollutant produced by all countries before and the answer was to adopt a treaty for a rapid and coordinated response. The best known example is the Ozone Convention which was adopted in 1985 to reduce chemicals used in refrigeration and aerosols which damaged the ozone layer. Like subsequent treaties addressing other harmful chemicals, such as the POPs Convention, the Ozone Convention tackled the most harmful first and was designed to enable alternatives to be introduced. Alternatives to harmful plastics do already exist – current plastics are largely derived from oil and so do not easily degrade.  Alternative plastics are being developed from prawn shells and from plants such as seaweed which will degrade more easily. World leaders have called for action on plastics. It’s time to follow through with a “plastics convention”, containing binding commitments to phase out and prevent future plastic pollution. A plastics convention could ban oil-based plastics in a similar way to the ban on ozone-eating chemicals. Single use bags and straws could be phased out almost immediately under a global treaty, with other plastics addressed over a longer time frame. Those used in medical surgery may take decades to phase out, but support could be provided to industry to develop bioplastics, or other alternatives to plastics. A treaty could also address gaps in the current law. There is, for example, no provision for cleaning up the plastics already in the ocean. A new treaty could provide for a clean up fund to address these “legacy” plastics. The fund could be supported through contributions from importers and exporters of plastics, as already happens with importers and exporters of oil who pay into a fund to address harm from oil spills, or through a tax on oil-based plastics products. The public are clearly supportive of action to tackle plastic pollution and alternative materials are being developed that could replace oil-based plastics. A treaty negotiated by the world’s governments would allow us to take coordinated action against oil-based plastics."
"

 **Presented by Daniel T. Griswold at the James and Margaret Tseng Loe Chinese Studies Center Conference, St. Vincent College, PA, on November 6, 2002.**



Let me confess up front that I am not a China expert. But one cannot talk about international trade and globalization for even a few minutes without addressing China. We are all students of China now. Today China has become one of the world’s major trading nations, and it is destined to grow more influential in the years ahead. 



My remarks today will address four aspects of the topic of China and international trade, what we might call “Chairman Dan’s Four Theses”: The Re‐​emergence of China as a Trading Nation; U.S.-China Commercial Relations Today; Answering the Critics of Normal Trade Relations; and Tilling the Soil for Human Rights.



If we were to travel back six or seven centuries, we would enter a world where China was the most advanced economy on earth and the most and dynamic force in Asian trade. China organized a professional Navy in 1232, with treadmill‐​operated paddle‐​wheelers and catapults that launched heavy stones. 



Marco Polo testified to the vigor of China’s international trade during his visits in the late 13th century. The commercial city of Hangzhou had 1 million residents by then, including a merchant class and uprooted refugees. The city embraced relative freedom, change, and travel, and was open to Arab and Hindu learning. The citizens of Hangzhou had a saying: “Vegetables from the east, water from the west, wood from the south, and rice from the north.” 



In those days, Chinese plied the Indian Ocean with fleets of ocean going merchant junks, 100 feet long and 25 feet wide, carrying 120 tons of cargo and 60 crew. Those ships visited Indonesia, Ceylon, and the west coast of India. By the 13th century, the Chinese had developed dry docks and gunpowder bombs–300 years before those were seen in the West. 



Beginning under Emperor Zhu Di, the Chinese launched seven official naval expeditions between 1405 and 1431 to Indonesia, India, Arabia, and East Africa. The expeditions were lead by the eunuch officer Zheng He. These “Treasure ships” were the largest in the world, 400 feet long by 160 feet wide (vs. 85 feet long for the Santa Maria). The ships were multi‐​decked, with nine masts and sails of red silk traversed with laths of bamboo for more durability and precise steering. Each ship carried hundreds of sailors and had 15 or more watertight compartments, and 60 cabins. At 7,800 tons of displacement, they were the largest ships in the world until those of the British Navy after 1800. In all, China built 250 such ships as part of a major shipbuilding program that would have been unimaginable in Europe at the time. 



The Treasure Ships were sent on huge trade missions. The first, in 1405, consisted of a fleet of 317 ships with 28,000 Chinese. What a sight that must have been! On a mission to Hormuz, in the Persian Gulf, Chinese traded porcelains and silks in exchange for sapphires, rubies, oriental topaz, pearls, coral beads, amber, woolens, and carpets, along with lions, leopards, and Arabian horses. But these were not market‐​opening missions, but more diplomatic in nature, a showing of the flag. No attempt was made to establish bases for trade or military objectives. The missions were very costly for the Chinese government and not profitable in a commercial sense. 



The bureaucratic mandarins, who detested commerce, soon prevailed over the rival eunuchs. At its peak in early 1400s, the great Ming navy consisted of 3,500 ships, but the number fell in half by 1440, and rapidly diminished after that. With the death of Zhu Zhanji in 1435, the new emperor recalled the fleets. In 1477, one of leading eunuchs called for writings of Zheng He to stimulate interest in naval expeditions, but the vice president of the ministry of war ordered them destroyed, calling them “deceitful exaggerations of bizarre things far removed from the testimony of people’s eyes and ears.” By 1500 it was a capital crime to build a ship with more than two masts. In 1525 coastal authorities were enjoined to destroy all ocean‐​going vessels, and in 1551 it was declared a crime to set sail in a multi‐​masted ship. 



In 1400, China was in every way superior to West: in technology, living standards, and global influence. But the country became enveloped in a smug self‐​sufficiency, cultural and economic inwardness, a closed and centralized political system, and an anti‐​commercial culture. In the 15th century, China turned its back on the world economy. It even abandoned naval defenses. Its highly educated elite was uninterested in Western technology and military potential. A British mission in 1793 brought 600 cases of presents, including chronometers, telescopes, a planetarium, chemical and metal products. Chinese officials rebuffed the foreigners, asserting that “there is nothing we lack‐​we have never set much store on strange or ingenious objects, nor do we want any more of your country’s manufactures.” 



So for more than 500 years, from the 15th to the 20th century, China’s economy slipped further behind the rest of the world. As late as 1820, the gross domestic product of China was still 30 percent higher than the total GDP of Western Europe and its settlements, but it was only one‐​twelfth the size by 1950. The Chinese economy was not open in the 19th century despite trade treaties and Western encroachment. Its trade was conducted in self‐​contained trade zones with little impact on the rest of China. The share of exports in China’s GDP was only 1.2 percent in 1913 at the height of pre‐​war globalization in the West. The Taiping Rebellion in the mid‐​19th century and World War II, Civil War, and communist convulsions in 20th devastated China’s economy. 



The economic reforms that began in late 1970s reversed 500 years of history. China’s trade with the rest of the world has grown from only $20 billion at the beginning of the reforms to more than $500 billion in 2001. China is now the world’s sixth largest exporter of goods and also the world’s sixth largest importer. In the past decade, China has reduced its average tariff from 43 percent to 15 percent, and those barriers will fall further as it implements the agreement it signed to join to World Trade Organization. So much for China being a closed economy! 



I believe the re‐​emergence of China as a trading nation is one of the most important and far‐​reaching developments in the last, oh, half millennium or so. After 500 years on the sidelines, China has rejoined the global economy.



Since China began to unilaterally open its market, the people of China and the United States have enjoyed a growing and mutually beneficial trade relationship. From practically nothing in 1980, two‐​way U.S.-China trade grew to more than $120 billion in 2001. China today is America’s fourth largest trading partner. In 2001, Americans imported $102 billion worth of goods from China while we exported $19 billion‐​leaving a bilateral trade deficit with China of $83 billion. 



Since 1980, the United States has allowed Chinese products to enter the U.S. market at the same tariff rates applied to our other trading partners. But the extension of so‐​called normal trade relations to China was always conditioned on the president granting a waiver to the Jackson‐​Vanik amendment (a relic of the Cold War that conditions trade with communist countries on their emigration policies). Each year congressional opponents of trade with China would try in vain to override the waiver, and in 2000 Congress made normal trade relations permanent to clear the way for China’s entry into the World Trade Organization. 



So what in the world do we buy from China? It’s a running joke with my kids that we cannot go to the store without buying something–clothing, toys, household goods–made in China. Three‐​quarters of what Americans import from China are toys and other miscellaneous manufactured goods: footwear‐​1 billion pairs of shoes a year‐​furniture, lighting fixtures, office machines, household electronics, electrical appliances, and clothing. Wal‐​Mart alone will import an estimated $12 billion worth of goods from China in 2002. Those goods mean lower prices, more choice, and more real income for American families. 



On a much smaller scale, China buys American‐​made aircraft, telecommunications equipment, scientific instruments, oil seeds and fruits, electrical machinery and appliances, data processing machines, and fertilizers. 



Why do we run such a large bilateral trade deficit with China? We are the world’s number one consumer society and China has become the world’s workshop for consumer goods, so it should be no surprise that we have become China’s best customer. On the other hand, we are the world’s leading high‐​end manufacturer, while China remains a relatively poor country. In sum, we are more willing and able to buy what the people of China make than they are willing or able to buy what we make. 



Despite warnings, the United States is not dangerously “dependent” on trade with China. Our imports to and exports from China remain a small fraction of our total trade. If anything, China is more dependent on trade with the United States than vice versa. Both our imports and exports with China are less than 10 percent of our total trade, but 38 percent of China’s exports go to the United States. If our trade relations were disrupted, by an outbreak of protectionism or a hot or cold war, both countries would suffer economically but China would suffer more. 



And despite the warning that U.S. factories will soon lock up and move to China, American investment in the mainland remains modest. At the end of 2001, American companies owned $7 billion worth of direct manufacturing investment in China. That is less than 2 percent of the total stock of U.S. manufacturing FDI abroad, and far less than the $35 billion in manufacturing investment American companies own in the tiny Netherlands, population 15 million. Annual outflows of manufacturing investment to China remain a tiny fraction of what American companies invest domestically in the U.S economy, and what the rest of the world invests in China. 



Criticism of U.S. trade with China takes two basic forms: that our trade with China, and by this the critics invariably mean what we import from China, threatens our national security, and that it threatens our economy. 



Let’s examine the national security argument first. The most legitimate concern about trade and national security is what we export to China. The U.S. government wields extensive powers to block exports to China of sensitive military and so‐​called dual‐​use technology‐​and the government should use that power when necessary. We should not be selling cutting‐​edge military technology to China that could then be sold to our enemies or turned against us in any way. But that is not what really bothers the critics of trade with China. What they object to are imports from China. They believe in a simple, what I would call a simplistic, formula that says: When we buy goods from China, China becomes richer, and the richer China becomes, the more it can fund its military to threaten American security. 



That was the conclusion this summer of the U.S.-China Security Review Commission. The commission was established by Congress in 2000 when it approved permanent normal trade relations. In its first annual report, the commission warns that, through our trade and investment ties with China, “we are strengthening a country that could challenge us economically, politically, and militarily.” 



“If China becomes rich but not free,” the commission warns, “the United States may face a wealthy, powerful nation that could be hostile toward our democratic values, to us, and in direct competition with us for influence in Asia and beyond.” 



The commission’s national security critique is fundamentally flawed, for at least two major reasons. First, while trade with the United States has been important for China’s development, it has not been the most important factor. Far more important has been China’s own internal liberalization, starting with its farm sector in the late 1970s, and then expanding to the privatization of its state‐​owned sector, repeal of price controls, and the unilateral opening of its economy to foreign competition. If the U.S. market were far less open to Chinese goods than it actually is, China would still have grown rapidly in the last 20 years, although not quite as rapidly as it actually has. 



Second, even if it were possible, through changes in U.S. trade policy, to put the brakes on China’s economic growth, would we even want to? From a humanitarian point of view, a dramatic slowdown in China’s growth would cause hardship for hundreds of millions of families and condemn millions of children to lives of perpetual poverty without hope for further education and upward mobility. And from a foreign policy point of view, a still‐​poor, stagnant, and frustrated China may be more unstable and hostile to American interests than a China that is advancing economically. In fact, a policy of disengagement from China could be self‐​fulfilling, creating the very enemy its proponents claim to be protecting us from. In sum, it would be cynical and foolish to stake our national security on a policy designed to keep 1 billion people isolated and poor. 



The other major criticism of trade with China is that is threatens America’s economy. Here the critics believe in an equally simplistic formula that says: Every widget we import from China means one less widget we make ourselves, which means a weaker U.S. economy and a potentially dangerous dependence on foreign widgets. And here too the argument against trade with China is fundamentally flawed. 



First, the types of goods we import from China are not important for the U.S. military. Recall the list of top imports from China: toys, shoes, clothing, office machines, household appliances and household electronics. American soldiers may be buying those goods at the local Wal‐​Mart or PX, but they are not being procured by the Pentagon. The China Security Commission warns that the U.S. steel industry may be jeopardized by Chinese imports, but the Commerce Department has already investigated the national security impact of steel imports and found no connection. 



Second, imports from China do not weaken the U.S. economy, cause unemployment, or threaten our industrial base. Imports strengthen our economy by raising real wages for families, providing lower‐​cost inputs for business, and spurring innovation and higher productivity through competition. Like technology, trade does cause certain industries to decline, thus eliminating some jobs, but it also creates new opportunities for wealth and job creation. In an economy with a reasonably flexible labor market, jobs eliminated by technology and trade will be fully offset by the creation of new jobs. 



A blatant example of overblown rhetoric about the trade deficit and jobs occurred on the eve of the vote on permanent normal trade relations in May 2000, during a segment of the NewsHour with Jim Lehrer on PBS. In summing up why the House should reject permanent normal trade relations with China in a vote the next day, AFL-CIO executive Richard Trumka asserted:



No one is saying isolate China. That’s the smoke screen they blow out because they don’t have the facts. Look, we have a $70 billion trade deficit with China. The U.S. International Trade Commission came out with a study yesterday [Monday, May 22] saying, if you give them permanent NTR status, two things will happen: We’ll lose one million jobs, and the trade deficit will increase.



Trumka’s sweeping claim offers a textbook example of how opponents of trade liberalization abuse trade deficit figures to serve their agenda. In fact, the U.S. International Trade Commission had issued no such study that week on trade with China. The commission’s most recent study on the impact of China PNTR had been released in August 1999, almost a year earlier, and it contained no estimate of job gains or losses. 



The actual source of the figure of one million jobs lost was a paper released the week before by the Economic Policy Institute, a union‐​aligned, non‐​profit organization. The EPI had used numbers from the 1999 USITC study to extrapolate an estimate of future bilateral trade deficits with China. It then crunched the hypothetical trade deficit numbers to estimate a total loss of almost 900,000 jobs during the next decade if Congress were to approve PNTR with China. But the EPI estimate of job losses was based on three whoppingly false assumptions. 



One serious error of the EPI study was to misapply the USITC’s estimates for the growth in China trade. The USITC study only offered a one‐​year, static estimate of the impact of Chinese tariff liberalization on the U.S. trade deficit. The ITC study didn’t even attempt to estimate the number of American jobs that would be created or eliminated by the further opening of the Chinese market. 



The EPI’s second crucial error was then to assume that rising imports from China automatically mean lost jobs in the U.S. economy. But rising imports need not and typically do not translate into a net loss of jobs. In fact, the growth of real goods imports and manufacturing output tend to be positively correlated. That is, as manufacturing output rises in the United States so too do imports of goods, adjusted for price changes. As with so many other economic indicators, the same economic expansion that spurs manufacturing output also attracts more imports and enlarges the trade deficit. 



Trade critics such as EPI wrongly assume that every import from China displaces domestic production, eliminating jobs in the economy. In reality, much of what we import from China, such as toys, shoes, and clothing, substitutes for imports from other low‐​wage producers. Another sizeable portion of our imports consists of intermediate inputs, which are then assembled into U.S.-made products by American manufacturers. That helps to explain why there is no correlation between rising manufacturing imports from China and falling manufacturing output. 



A third critical error of the EPI study was to consider the bilateral trade balance with China in isolation. While a change in trade policy can affect a particular bilateral deficit, the increased bilateral deficit tends to be offset by changes in other bilateral balances. The ITC study confirms this. The USITC estimated that China’s lower tariffs would cause America’s overall trade deficit to shrink slightly. Although America’s bilateral deficit with China would increase within the USITC’s limited model, our trade balance with other countries would “improve” enough to more than offset the increased deficit with China. The USITC estimated that America’s total exports would growth by $1.9 billion while imports would grow by $1.1 billion, decreasing the overall U.S. trade deficit by $0.8 billion. If you believe EPI’s own faulty methodology, the smaller overall U.S. trade deficit caused by China’s lower tariffs should lead to an increase in U.S. jobs, not a decrease. 



Trade with China is about more than jobs and incomes. Around the world, trade and the development it has spurred have created a more hospitable climate for civil and political freedoms. The economic openness of globalization allows citizens greater access to technology and ideas through fax machines, satellite dishes, mobile telephones, Internet access, and face‐​to‐​face meetings with people from other countries. Rising incomes and economic freedom help to nurture a more educated and politically aware middle class. People who are economically free over time come to want and expect to exercise political and civil liberty as well. Catholic social thinker Michael Novak identified this as the “Wedge Theory”:



Capitalist practices, runs the theory, bring contact with the ideas and practices of the free societies, generate the economic growth that gives political confidence to a rising middle class, and raise up successful business leaders who come to represent a political alternative to military or party leaders. In short, capitalist firms wedge a democratic camel’s nose under the authoritarian tent.



The interplay of economic openness and political and civil freedom is admittedly complex, and the question of causation remains unsettled, but the two phenomena are clearly linked in the real world. In the past 25 years, as an expanding share of the world has turned away from centralized economic controls and toward a more open global market, political and civil freedoms have also spread. Since 1975, the share of the world’s governments classified by Freedom House as democracies has risen sharply, especially since the late 1980s when globalization began to gather steam. Many of those new democracies are low‐ and middle‐​income countries that have simultaneously liberalized and opened their economies. 



When we compare countries according to their economic openness and their degree of political and civil freedom, the connection becomes even more evident. People who live in countries relatively open to international trade and investment are far more likely to enjoy full political and civil liberties than those who live in countries that are relatively closed. Among the top two quintiles of nations ranked according to their economic openness, 90 percent are rated “Free” by Freedom House and not a single one is rated “Not Free.” In the bottom quintile of openness (i.e. those with the most closed economies), fewer than 20 percent are rated “Free” and more than half are rated “Not Free.” In other words, countries that maintain a relatively open economy are more than four times more likely to be free of political and civil oppression than countries that remain closed. 



Recent decades have witnessed dramatic examples of how economic freedom and openness till the soil for civil and political reform. Twenty years ago, both South Korea and Taiwan were military dictatorships without free elections or full civil liberties. Today, thanks in part to economic growth and globalization, both are thriving democracies where citizens enjoy the full range of civil liberties and where opposition parties have won elections against long‐​time ruling parties. In Mexico, more than a decade of economic and trade reforms helped lay the foundation for the historic July 2, 2000, election of the opposition candidate Vicente Fox, ending 71 years of one‐​party rule by the PRI. Internal economic reforms and the North American Free Trade Agreement helped to undermine the dominance of the PRI over Mexican political life. Alejandro Junco, publisher of the opposition newspaper Reforma, noted after the PRI’s historic defeat, “As the years have passed, and with international mechanisms like NAFTA, the government doesn’t control the newsprint, they don’t have the monopoly on telecommunications, there’s a consciousness among citizens that the president can’t control everybody.” 



While genuine political reform has been absent so far in China, and dissent is still brutally suppressed, economic reform and globalization give reason to hope for political reforms. After two decades of reform and rapid growth, an expanding middle class is experiencing for the first time the independence of home ownership, travel abroad, and cooperation with others in economic enterprise free of government control. The number of telephone lines, mobile phones, and Internet users has risen exponentially in the past decade. Tens of thousands of Chinese students are studying abroad each year. 



China’s economic reforms have opened the door for religious witnessing. More than 100 Western missionary organizations are active in China. Those organizations have distributed millions of Chinese language Bibles in China. Thousands of Christian workers who are tent‐​making as English teachers and in other occupations are able to minister to the growing body of believers in China. All this would have been unthinkable 25 years ago when China was still isolated from the global economy. 



All this must be good news for individual freedom in China, and a growing problem for the government. A recent study by the Chinese Communist Party’s influential Central Organization Department noted with concern that “as the economic standing of the affluent stratum has increased, so too has its desire for greater political standing.” The study concluded that such a development would have a “profound impact on social and political life” in China. 



Globalization and economic development do not guarantee political reform in China or anywhere else, but the track record of economic engagement is far more promising than the failed record of sanctions and economic isolation. Four decades of an almost total U.S. embargo against Cuba have yet to soften Fidel Castro’s totalitarian rule. Sanctions against Burma (a.k.a. Myanmar) have only worsened the condition of the very people we are trying to help without bringing any progress toward democracy and freedom. The folly of imposing trade sanctions in the name of promoting human rights abroad is that it deprives people in the target countries of the technological tools and economic opportunities that can help to free them from tyranny. 



For the past two decades, globalization, human rights and democracy have been marching forward together, haltingly, not always and everywhere in step, but in a way that unmistakably shows they are interconnected. By encouraging more trade and market liberalization in China, we not only help to raise growth rates and incomes, promote higher standards, and feed, clothe and house the poor; we also spread political and civil freedoms. 



President Bush, in The National Security Strategy of the United States of America document released in 2002, wrote that, “Chinese leaders are discovering that economic freedom is the only source of national wealth. In time, they will find that social and political freedom is the only source of national greatness.” Opponents of trade with China see the rising incomes and falling poverty of hundreds of millions of people as a threat to our security and well‐​being. Instead, we should see China’s rising prosperity as an immediate blessing for mankind. And we should understand that trade offers the best hope that China will one day join the community of nations that are free and democratic just as it now seeks to join those that are open and prosperous.
"
"
Below is an opinion from the Washington Times by Geophysicist David Denning, to which I’ve added photos and links to the events he’s written about.
 
A caveat: I caution the reader that annual weather does not equate to long term climate. Yes we have had a number of record cold events in 2007, and the winter in the Northern Hemisphere is already shaping up to be colder than normal. But fortunes of weather can turn on a dime. I’d also point out that we are in a solar minimum right now and predictions of solar cycle 24’s peak range from it being very low (colder) to very high (warmer). The next few years will be telling.

Snow in Buenos Aires, July 9th, 2007



Year of global cooling
By David Deming
December 19, 2007


South America this year experienced one of its coldest winters in decades. In Buenos Aires, snow fell for the first time since the year 1918. Dozens of homeless people died from exposure. In Peru, 200 people died from the cold and thousands more became infected with respiratory diseases. Crops failed, livestock perished, and the Peruvian government declared a state of emergency.

Unexpected bitter cold swept the entire Southern Hemisphere in 2007. Johannesburg, South Africa, had the first significant snowfall in 26 years. Australia experienced the coldest June ever. In north-eastern Australia, the city of Townsville underwent the longest period of continuously cold weather since 1941. In New Zealand, the weather turned so cold that vineyards were endangered.

Last January, $1.42 billion worth of California produce was lost to a devastating five-day freeze. Thousands of agricultural employees were thrown out of work. At the supermarket, citrus prices soared. In the wake of the freeze, California Gov. Arnold Schwarzenegger asked President Bush to issue a disaster declaration for affected counties. A few months earlier, Mr. Schwarzenegger had enthusiastically signed the California Global Warming Solutions Act of 2006, a law designed to cool the climate. California Sen. Barbara Boxer continues to push for similar legislation in the U.S. Senate.

In April, a killing freeze destroyed 95 percent of South Carolina’s peach crop, and 90 percent of North Carolina’s apple harvest. At Charlotte, N.C., a record low temperature of 21 degrees Fahrenheit on April 8 was the coldest ever recorded for April, breaking a record set in 1923. On June 8, Denver recorded a new low of 31 degrees Fahrenheit. Denver’s temperature records extend back to 1872.

Recent weeks have seen the return of unusually cold conditions to the Northern Hemisphere. On Dec. 7, St. Cloud, Minn., set a new record low of minus 15 degrees Fahrenheit. On the same date, record low temperatures were also recorded in Pennsylvania and Ohio.

Extreme cold weather is occurring worldwide. On Dec. 4, in Seoul, Korea, the temperature was a record minus 5 degrees Celsius. Nov. 24, in Meacham, Ore., the minimum temperature was 12 degrees Fahrenheit colder than the previous record low set in 1952. The Canadian government warns that this winter is likely to be the coldest in 15 years.

… If you think any of the preceding facts can falsify global warming, you’re hopelessly naive. Nothing creates cognitive dissonance in the mind of a true believer. In 2005, a Canadian Greenpeace representative explained ‘global warming can mean colder, it can mean drier, it can mean wetter.’ In other words, all weather variations are evidence for global warming. I can’t make this stuff up.’
Note from Anthony: That’s what I call CYA forecasting 😉
No; but others can, and do. However, maybe at long last the penny is dropping. The New Statesman, no less, this week publishes a piece by sensible David Whitehouse which says flatly:… The fact is that the global temperature of 2007 is statistically the same as 2006 as well as every year since 2001. Global warming has, temporarily or permanently, ceased. Temperatures across the world are not increasing as they should according to the fundamental theory behind global warming – the greenhouse effect. Something else is happening and it is vital that we find out what or else we may spend hundreds of billions of pounds needlessly.

… For the past decade the world has not warmed. Global warming has stopped. It’s not a viewpoint or a sceptic’s inaccuracy. It’s an observational fact…. So we are led to the conclusion that either the hypothesis of carbon dioxide induced global warming holds but its effects are being modified in what seems to be an improbable though not impossible way, or, and this really is heresy according to some, the working hypothesis does not stand the test of data.

It was a pity that the delegates at Bali didn’t discuss this or that the recent IPCC Synthesis report did not look in more detail at this recent warming standstill.A pity indeed, that the entire western ruling class has been taken in by this scam.But now the cavalry appears at last to have arrived. According to this story, a US Senate report documents the opinion of hundreds of prominent scientists from around the world who say global warming and cooling is a cycle of nature and cannot legitimately be connected to man’s activities.The report compiled observations from more than 400 prominent scientists from more than two dozen nations who have voiced objections to the so-called ‘consensus’ on ‘man-made global warming.’ Many of the scientists are current or former participants in the United Nation’s Intergovernmental Panel on Climate Change, whose present officials, along with former Vice President Al Gore, have asserted a definite connection.

The new report comes from the Senate Environment and Public Works Committee’s office of the GOP ranking member, and cites the hundreds of opinions issued just in 2007 that global warming and man’s activities are unrelated. [My emphasis]…‘Many scientists from around the world have dubbed 2007 as the year man-made global warming fears “bite the dust”’, the introduction said. And there probably would be many more scientists making such statements, were it not for the fear of retaliation from those aboard the global-warming-is-caused-by-SUVs bandwagon, the report said.And it details some of this intimidation.
Looks like man-made global warming theory is melting away faster than you can say Al Gore. A lot of reputations are now going to disappear along with it: all those who were part of the famous ‘consensus’ (not).Those people should never be taken seriously again.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1f56227',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Evidence is rapidly accumulating that ocean acidification and elevated temperatures will have catastrophic consequences for marine organisms and ecosystems. In fact, it is something we are already witnessing. Coral reefs are bleaching, while snails and other calcifying marine organisms struggle to build their shells, scales and skeletons and juvenile marine animals even struggle to navigate to suitable habitats. Yet many primary producers, including seaweeds, are predicted to thrive in the acidic oceans of the future – as they use CO₂ from the seawater to produce energy by photosynthesis.  Humans have eaten seaweeds for tens of thousands of years and today the diets of billions of people, especially in Asia, are based on cultivated seaweeds. However, while future ocean conditions may improve the yield of farmed seaweeds, we do not know how the nutritional content of seaweeds will be affected by climate change. To investigate this, we recently looked into how the iodine content of seaweeds will be affected by future climate change scenarios.  Seaweeds are one of the best natural sources of iodine, and this essential mineral is used by the body to make thyroid hormones. But both too much and too little iodine can change the way the body’s thyroid gland works. If climate change were to affect the amount of iodine in seaweed, humans – and other animals – who rely on it as a staple part of their diet may suffer serious health problems. For this recently published study, we simulated current and future ocean acidification conditions in laboratory and outdoors settings. To conduct the outdoor experiments, we enclosed seawater in  cages made of very small mesh polythene nets so that environmental conditions such as CO₂ and temperature could be manipulated and responses monitored, while all other environmental conditions remained the same as the natural environment. We used three kelp species – Saccharina japonica, Undaria pinnatifida, and Macrocystis pyrifera – as well as the coastal seaweeds Ulva pertusa, Ulva intestinalis, Gracilaria lemaneiformis and Gracilaria chouae, for the research. With the exception of M. pyrifera, these seaweeds are widely consumed by humans across the world – for instance, in sushi, soups and in the Welsh delicacy laverbread. M. pyrifera was selected as it is a preferred food source of marine invertebrates, such as sea urchins and abalone, which are harvested by the fishing industry. In ocean acidification research like this, oceanographers monitor the partial pressure of CO₂ in seawater. This figure reflects the amount of dissolved CO₂, which is measured as parts per million (or µatm) and is an indicator of how acidic the oceans are. The Intergovernmental Panel on Climate Change predicts that future CO₂ in the oceans will more than double by the year 2100 – rising from current levels of 400 µatm to 1,000 µatm – if no mitigating action is taken against climate change.  We created these future ocean acidification conditions by blowing CO₂ bubbles into the seawater, and measuring the µatm. We then grew seaweeds in eight climate scenarios in the lab and two climate scenarios in the field. These ranged from current levels of CO₂ and temperature to future ocean acidification and elevated temperature scenarios.  We found that seaweeds grown in conditions which followed future ocean acidification predictions accumulated more iodine than seaweeds grown in present-day conditions. However, in the scenarios we tested, elevated temperature was not as important as ocean acidification in causing iodine accumulation in seaweeds. This means that while we expect the yield of a very important food crop to increase under future climate change, levels of iodine will also increase, affecting human nutrition. We also traced elevated iodine content from seaweeds to their consumers. Natural consumers of seaweeds such as fish and shellfish are also a rich dietary source of iodine for humans. Using an outdoor feeding experiment, we examined the effect of consuming seaweeds under future ocean acidification conditions on the edible shellfish, abalone (Haliotis discus). We found that iodine concentrations increased in shellfish tissue after eating seaweeds with elevated iodine concentration. In addition, we saw that the concentration of thyroid hormones in the shellfish tissue decreased. This provides evidence that ocean acidification impacts the quality of seafood by changing the concentrations of an essential mineral with consequences for consumers. There is a risk that as the world’s climate continues to change, people who eat seaweed as a staple part of their diet may consume too much iodine, which can lead to a wide range of health problems. Since seaweeds and shellfish underpin the nutrition of billions of humans around the world it is essential to understand how the iodine content of seafood will change under global climate change. This information can for instance be used by the World Health Organisation to provide recommendations on appropriate levels of seaweeds consumption to maintain a sufficient daily iodine intake."
"There are at least 268,000 tonnes of plastic floating around in the oceans, according to new research by a global team of scientists.  The world generates 288m tonnes of plastic worldwide each year, just a little more than the annual vegetable crop, yet using current methods only 0.1% of it is found at sea. The new research illustrates as much as anything, how little we know about the fate of plastic waste in the ocean once we have thrown it “away”. Most obviously, this discarded plastic exists as the unsightly debris we see washed ashore on our beaches. These large chunks of plastic are bad news for sea creatures which aren’t used to them. Turtles, for instance, consume plastic bags, mistaking them for jellyfish. In Hawaii’s outer islands the Laysan albatross feeds material skimmed from the sea surface to its chicks. Although adults can regurgitate ingested plastic, their chicks cannot. Young albatrosses are often found dead with stomachs full of bottle tops, lighters and other plastic debris, having starved to death.   But these big, visible impacts may just be the tip of the iceberg. Smaller plastic chunks less than 2.5mm across – broken down bits of larger debris – are ubiquitous in zooplankton samples from the eastern Pacific. In some regions of the central Pacific there is now six times as much plankton-sized plastic are there is plankton. Plankton-eating birds, fish and whales have a tough time telling the two apart, often mistaking this plastic – especially tan coloured particles – for krill. However, even this doesn’t quite tell the whole story. For technical reasons Eriksen and his team weren’t able to consider the very smallest particles – but these may be the most harmful of all. We’re talking here about tiny lumps of 0.5mm across or considerably less, usually invisible to the naked eye, which often originate in cosmetics or drugs containing nanoparticles or microbeads. Such nanoparticles matter as they are similar size to the smallest forms of plankton (pico and nano plankton) which are the most abundant plankton group and biggest contributors in terms of biomass and contribution to primary production. There’s a lot going on when you zoom right in. We don’t yet know precisely how plastic nanoparticles interact with marine fauna but we do know that they can be absorbed at the level of individual cells. And what’s worse is they’re very efficient carriers of organic molecules such as estradiol, the drug used for birth control and IVF that finds it way through our sewage system into the sea.  Indeed, this efficiency is one of the reasons nanoparticles are being explored for drug delivery – they’re a great way to get the right medicine absorbed into the right cells.  Therefore it isn’t just the plastic itself that should concern us. We need to look at what it’s carrying, as substances clinging to nanoparticles of plastic could badly damage marine ecosystems.  Nasty endocrine disrupting chemicals can be concentrated a million times more than background levels on the surfaces of plastic particles. These can then be ingested by organisms and the chemicals absorbed leading to disruption of the reproductive process – some species such as bivalve mussels have even seen males turned into females. Floating chunks of plastic can also be colonised by organisms including potential bacterial pathogens such as cholera, and marine insect sea skaters which need a hard surface to lay their eggs on – plastic in the sea increases their numbers and range. The fact that floating plastic debris is novel and persists for longer than most natural flotsam could make them ideal vehicles for the introduction of invasive species with potentially devastating consequences. Plastic pollution of the marine environment is the Cinderella of global issues, garnering less attention than its ugly sisters climate change, acidification, fisheries, invasive species or food waste but it has links to them all and merits greater attention by the scientific community."
"

 **Presented by Alan Reynolds at “Deregulation in the Global Market Place** : **Challenges for Japan and the United States in the 21stCentury** **”  
A Keidanren‐​Cato Institute Symposium, Tokyo, April 6, 1998 .**



American economists have been giving policy advice to Japan since the Shoup tax reform commission of 1949. Even then, the advice was not always helpful. 



U.S. government officials now tell Japan that larger budget deficits are the key to boosting economic growth and stock prices. Yet the same officials claim that the U.S. economy and stock market have benefitted from much _smaller_ budget deficits. 



When looking at tax policy as a separate policy tool, we must get beyond Keynesian “macroeconomics,” and the related bad habit of ascribing magical properties to government borrowing. A few years ago, prominent macroeconomists in the U.S. and Japan were absolutely confident that American budget deficits had pushed interest rates and the dollar up, causing trade deficits. Today, much larger budget deficits in Japan are associated with the lowest interest rates in world history, a falling yen, and rising trade surpluses. 



A theory that can predict anything can also predict nothing. Fiscal macroeconomics is an unsolvable mystery, something best relegated to a museum. Even the dogma that budget deficits “stimulate” demand (nominal GDP) is just another archaic theory with no shred of evidence.(1) Budget deficits are a consequence of slow economic growth, not a cause of rapid growth. Yet the elusive theory of “fiscal stimulus” still allows politicians to claim that endless public works schemes are a free lunch, rather than a dubious debt service burden on taxpayers.(2)



Japan’s nominal GDP growth surged in the late 1980s because the Bank of Japan was buying a lot of securities (see the Appendix: “Money Matters”). Nominal GDP growth slowed after 1991–92 because the monetary base shrank. Growth of aggregate demand depends on monetary policy, not government borrowing. Whether or not supply can keep up with demand, however, _is_ affected by the _micro_ economic structure, including tax rates. 



Any serious look at policy must be based on microeconomics — all the messy but vital structural details about disincentives and distortions. An improvement in tax policy is _not_ measured by how much revenue is lost, but by how much efficiency and prosperity is gained. 



To describe tax policy in macroeconomic terms — solely by its immediate effect on the budget — leads to paradoxical conclusions. Many people suspect that the April 1997 increase in Japan’s value‐​added tax contributed to recession.(3) If that is correct, this “tax increase” will surely result in _less_ tax revenue, not more. A macroeconomist may later observe such a loss of revenue and conclude that tax policy has become “stimulative” — the opposite of what actually happened. 



Conversely, if a reduction in the highest, most damaging tax rates results in slightly more rapid economic growth, then it may also result in _more_ tax revenue. It is the impact on the economy that matters. What happens to the budget deficit mainly depends on what happens to the economy, not the other way around. 



An _efficient_ tax system is one that _raises revenue with the least possible damage to economic progress._ With such a system, tax revenues naturally _grow_ as the economy expands. 



The _marginal_ tax burden in Japan is unusually high, yet average tax revenues are relatively low. Aside from Social Security, revenues have been _falling_ since 1990, when many new taxes were added. High tax _rates_ and weak revenues are symptoms of an _inefficient_ tax system. 



******Figure 1** shows that it is _not_ unusual for countries with high tax rates (such as Japan) to collect very little revenue, even in _static_ terms — as a percentage of GDP, rather than real revenue growth over time. These figures are for the _national_ income tax on individuals (although local taxes are very important in Sweden, Canada, Japan and others). The only countries with high tax rates that collect much revenue are those in which the _lowest_ tax rate is also high, such as the 19% rate in Germany (although even a 25% minimum tax does not work in Turkey). Tax rates above 35–40% never seem to yield much revenue, anywhere. 



As the table indicates, the ratio of tax revenues to GDP can be a misleading measure of the actual burden of taxes. Some taxes may be extremely destructive yet yield very little revenue.(4) Just as “prohibitive” tariffs yield no revenue, extremely high tax rates can also be prohibitive, or nearly so, because they (1) seriously discourage the taxed activity, (2) lead to rampant tax avoidance and evasion, and (3) foster capital flight and a “brain drain.” 



A tax system that suffocates growth of GDP may result in a rising ratio of revenues to GDP even though revenues are growing very slowly in real terms. This is the situation of many Continental European countries. Real tax revenues grow very slowly, but the economy grows even more slowly, so revenues rise as a percentage of stagnant GDP. Yet such governments have little room to increase spending in real terms because revenues have stopped growing in real terms. 



Conversely, revenues may appear quite low relative to GDP and yet be growing very rapidly in real terms, because the tax climate is conducive to rapid growth of real GDP. Hong Kong’s tax revenues ( _excluding_ land sales) grew by 17.8% a year from 1984 to 1996 — nearly three times as fast as the 7.1% pace of U.S. revenue growth. Yet the ratio of taxes to GDP remains low in Hong Kong because real GDP has increased almost as rapidly as real tax receipts. Government consumption in Hong Kong has long increased by 6% a year, in real terms. 



In short, tax policy is _not_ properly described by revenue losses alone, nor even by revenues as a percentage of GDP. We need to look at _marginal_ tax rates on the returns to additional capital, including human capital. 



**The Shoup Mission**



Rather than rely too heavily on economic theory, or on foreign advice, it is often useful for a country to reexamine its own history (and that of its neighbors) to see which policies were followed by prosperity and which were not.()



In the late 1940s, the American Occupation had imposed brutal income tax rates on Japan, as high as 86% on income above Â¥5 million. This was a central part of a severe austerity program, _not_ a plan to promote economic growth. As Edwin Reischauer pointed out at the time, “Steeply graduated income taxes and inheritance taxes have been adopted to prevent in the future the accumulation of … concentrations of wealth.” But taxes designed to punish additions to _income_ must also punish additions to _output_ — economic growth. So, Japan set out to free itself from the oppressive Occupation tax regime. 



In late 1950, following a similar policy coup in Germany, Japan’s highest individual income tax rate was slashed to 55% from 86%. From 1950 to 1974, Japan cut taxes _every year_ (except 1960) often by greatly increasing the income thresholds at which the higher tax rates applied, or by enlarging deductions and exemptions. The taxable income needed to fall into a 60% tax bracket was raised to 3 million yen by 1953, for example, compared with only 300,000 in 1949. The Shoup Commission’s net worth tax was also abolished in 1953. The sting of high tax rates was further neutralized by exemptions for interest income and capital gains, deductions from corporate and individual taxes on dividends, a deduction for earnings, and various other holes in the tax base, legitimate and otherwise.(6)



Some deductions were far from neutral, and therefore less desirable than lower tax rates would have been. Yet the continual tax reductions from 1950 to 1974 accomplished two things. First, they greatly reduced effective marginal tax rates. Second, they moved the system a long way toward what is sometimes called a “consumed income tax” or “expenditure tax” — that is, a system that taxes income only once, regardless of whether the income is saved or devoted to immediate consumption.(7)



American economists were extremely critical of both accomplishments. In 1958, a member of the Shoup Commission, Jerome Cohen, described the tax cuts as “foolhardy from an economic point of view.”(8) In 1964, Martin Bronfenbrenner complained that Japan’s reductions of the highest tax rates amounted to “a retreat from the _equitable_ policies of the American Occupation.”(9)



Tax reduction was considered “foolhardy” precisely _because_ it fostered rapid economic growth. American economists were convinced that Japan would always be plagued with what Bronfenbrenner called “endemic Japanese balance of payments problems.” Failing to distinguish between tax rates (which were falling) and tax revenues (which were soaring), the U.S. critics argued on Keynesian grounds that Japan’s taxes had to be kept high to _suppress_ economic growth. Otherwise, Japan’s trade deficit would supposedly reach “currency crisis” proportions. For the same reason, many U.S. economists were sharply critical of Japan’s deep tariff cuts in the 1960s (e.g., by 1975, the effective tariff on autos fell from 40% to 10%, and the tariff on televisions from 30% to 5%). 



Bronfenbrenner’s remark about Occupation tax rates being more “equitable” was also in the spirit of the times. From about 1938 to 1962, U.S. “public finance” economists were much more concerned about using taxes to prevent people from becoming rich than about any adverse effects on economic growth. An intellectual godfather of the Shoup Commission, Henry Simons, knew perfectly well that “every gain [in] better distribution will be accompanied by some loss in production.”(10) But Simons and his followers assigned a low priority to production, believing that income leveling was the primary goal of tax policy.(11)



In the U.S., the postwar emergence of Keynesian macroeconomics destroyed any lingering anxieties about the impact of high tax rates on economic growth. Early postwar predictions of chronic “underconsumption” and “secular stagnation” meant that _saving was considered a terrible thing_ , and therefore an excellent target for confiscatory taxation. After those dire predictions proved false, slow growth was then said to be something that could easily be fixed by printing more government bonds (called “fiscal policy”) or more money (“monetary policy”). Attention to tax incentives disappeared until the Kennedy Administration, when it was revived largely because of the embarrassing success of falling tax rates and rising income thresholds in Japan and Germany. 



The point of all this history is to emphasize that promoting economic growth in Japan was certainly _not_ the primary goal of the Shoup Commission. Yet the Shoup Commission continues to influence influential Japanese tax specialists who were trained in this early postwar American tradition. 



**1975–87: Bracket Creep and Public Works**



In 1971–73, the U.S. devalued the dollar, pursued an inflationary money policy, and tried to disguise the consequences with price controls. When the dollar goes down, commodities priced in dollars go up. By the fall of 1972, commodity futures prices had already begun to soar, with oil a relative laggard. The end result was the stagflationary swamp of 1974. Japan was an innocent victim. 



Although the event was temporary, it had a lasting impact on Japan’s economic _policies._ Japan imposed a corporate surtax in April of 1972 and stopped cutting individual tax rates after 1974, thus allowing inflation, aging and economic growth to push more and more taxpayers into higher tax brackets. There was a major shift in emphasis away from fostering _private_ investment toward “social overhead capital” and “infrastructure improvement.”(12) In the quaint language that economists used at the time, endless bracket creep would supposedly generate so much revenue that public works spending would be needed to prevent “fiscal drag.” 



**Figure 2** illustrates the main reason why seemingly high marginal tax rates of 40–70% had not done much damage before 1974. The income thresholds for the highest tax brackets were repeatedly increased by huge amounts, much more than enough to keep far ahead of inflation and rising real incomes. 



From 1975 to 1984, by contrast, there were no adjustments of tax thresholds at a time when inflation averaged more than 6% a year. The resulting bracket creep appeared to generate a lot of revenue, but this was partly money illusion. It was not until 1979 that real, inflation‐​adjusted revenue was again as high as it had been in 1974. Revenues did rise as a percentage of GDP, but that would not have happened if real GDP had not slowed. Whatever its long‐​term effect on revenues, bracket creep certainly generated more and more tax distortions and disincentives. 



In 1984, there was a modest increase in the threshold for the 60% rate, but also an increase in the corporate tax rate. In fiscal 1987, the 60% rate was reduced to 55% and the 65–70% rates to 60%. That was temporarily helpful, contributing to a brief spurt of growth in the economy and in tax receipts. 



In April 1989, national tax rates above 50% were ended. But a provision that kept total national and local taxes from exceeding 78% had been eliminated in 1987, leaving combined national and local tax rates as high as 76% even in 1992. The value‐​added tax was also introduced in 1989, raising marginal rates by three percentage points at all income levels. 



**Figure 2** shows that even as recently as 1993, the 40% national tax rate still applied to the same _nominal_ income at which a 42% rate had applied back in 1974. _If tax thresholds had merely been adjusted for consumer price inflation from 1974 to 1993, not for real wage gains, they should have more than doubled in terms of yen_! The latest threshold adjustment finally came close to making up for past inflation with respect to the 50% bracket, but is still inadequate at the 40% rate. All things considered, there has been surprisingly little relief from the highest, most destructive tax rates on individual income, despite appearances to the contrary. 



Japan has been quite unique in this respect. Between 1979 and 1986, many countries had cut their highest income tax rates in half — including the United States, United Kingdom, South Korea, and Singapore. And no country that cut tax rates in half suffered any sustained revenue loss, even as a percentage of GDP. It is not even true that lowering the highest U.S. tax rate from 50% to 28–33% in 1986 was financed by eliminating deductions. Fewer itemized deductions were replaced with a larger standard deduction, but total deductions were not any smaller. Taxable incomes reported by higher‐​income people were “surprisingly” strong after tax rates came down, and labor force participation increased sharply among their spouses. 



**Social Security**



As bracket creep was pushing middle‐​aged Japanese professionals and mid‐​level managers into tax brackets once intended for the very rich, the burden of “contributions” for Social Security pensions also doubled from 6.2% in 1970 to 12.4% in 1986, and to 14.3% more recently.(13) The entire Social Security tax burden, including public health insurance, reached 25.5% of payroll by 1996. Most of this must be added to _marginal_ tax rates on labor income because, unlike the U.S., there is no maximum contribution to eliminate the _marginal_ burden at high incomes. It is the deadly combination of income tax, payroll tax and VAT that matters, at the margin. 



The fact that employees can deduct Social Security taxes from their income tax base is modestly helpful (only employers can deduct payroll taxes in the U.S.). But neutral taxation of savings requires that if contributions are deductible when going into a pension fund, then the funds must be fully taxable when they are later withdrawn. This is the principle behind the original Individual Retirement Account in the U.S. 



If contributions to public or private pensions were _not_ deductible, then income going into a pension fund would have already been taxed before it was saved. In that case, withdrawals from the pensions should _not_ be taxed a second time, when the money is taken out. This is the principle behind the new Roth retirement accounts in the U.S. 



Either of these _neutral_ treatments of saving would be equivalent to a “consumed income” tax, if applied uniformly without restrictions (such labels are misleading, however, since _all_ taxes fall on the individual owners of factors of production). Such a policy had long been advocated by British economists (Mill, Marshall, Pigou, Kaldor), and by Irving Fisher of Yale. It is in marked contrast with the Haig‐​Simons concept of “comprehensive” income taxation that inspired Shoup Commission plans. When older economists, in the Haig‐​Simons tradition, speak of “broadening the tax base,” they often mean defining income in ways that ensure that savings will be taxed several times. Some even refer to corporate depreciation as a “tax preference,” as though the cost of plant and equipment was not a cost at all, but taxable income. Today, younger U.S. specialists in public economics have moved away from these Haig‐​Simons ideas that were so fashionable fifty years ago. Most favor neutral tax treatment of saved income, and prefer immediate expensing of plant and equipment. 



Japan’s system of fully deducting contributions to (public) pensions _but not treating_ pensions as taxable income is not consistent with neutral treatment of savings. To deduct contributions and also exempt withdrawals is _too generous_ toward this specific form of savings (Social Security). Yet Japan’s tax policy after 1989 moved back toward multiple levels of taxes on all other forms of savings — capital gains, dividends and interest income. A policy of tax neutrality for _all_ savings, or at least for all _retirement_ savings, would greatly ease the fiscal strains expected from an aging population. Neutrality requires taxing income from public pensions exactly like any other income (because contributions were deducted), even if pretax benefits have to be increased to sweeten the deal.(14) Applying that same principle to _private_ pensions (deductible going into a pension plan and taxable coming out, or vice‐​versa) would encourage greater use of private retirement plans and thus ease the excessive dependence on Social Security pensions. This would be a start toward partial or full “privatization” of pensions — something that should never be a state _monopoly._



Because incomes generally rise with seniority, tax burdens tend to be extremely heavy at ages 45–57, then fall sharply at older ages due to the dubious exemption of pension income. Since Japan is aging fast, the combination of superhigh taxes on older workers and none on pensions must push many people into early retirement, whether they like it or not. 



**Different Policies; Different Results**



Before 1975, tax policy greatly reduced effective marginal tax rates and eased the multiple taxation of saved income. Economic growth in Japan ( **Figure 3** ) averaged 9.6% a year from 1952 to 1973. 



From 1975 to 1987, “bracket creep” and higher Social Security taxes reversed much of the previous progress on marginal tax rates. Economic growth slowed to 4.3% from 1975 to 1991. 



After 1989, tax policy also _reversed_ much of the previous progress toward neutral treatment of savings. Tax rates on new capital investments increased (at the level of individual investors). Economic growth slowed to 1.2% from 1992 to 1997. To continue blaming this on the “oil shocks” of the 1970s, as many do, is no longer plausible.(15) Oil has been very cheap for more than a dozen years. 



A large increase in the marginal cost of oil can indeed make production of many goods unprofitable. Growth stalls. An increase in the marginal cost of labor or capital can have the same depressing effect. An increase in the marginal cost of government is no different. It reduces the (after‐​tax) return on investments in physical and human capital — the primary source of economic growth. 



Although the dramatic slowdown of economic growth after 1974 coincided with an equally dramatic change in tax and spending policies, that change in economic policies is rarely blamed for the change in economic results. From 1989 to 1992, Japan added more taxes on sales, land, capital gains, dividends and interest. Yet the dramatic deterioration in economic growth after 1990 is rarely blamed, even in part, on those simultaneous changes in tax policy. The sole exception was the increased VAT of April 1997. Even in that case, however, complaints that this particular tax increase hurt the economy are not often translated into the logical conclusion that rolling‐​back such a counterproductive tax increase must likewise _help_ the economy. 



**Figure 4** shows OECD estimates of _average_ effective tax rates on capital and labor in the U.S. and Japan.(16) Before 1985, Japan had much lower tax rates on capital than the U.S. did. Since then, that situation has been reversed — _Japan is now more hostile to capital_. Little wonder that Japan’s domestic investment is weak, and capital flows out. 



It is often said that because Japan’s savings exceed domestic investment, the answer is to use tax policy to reduce savings (adding and increasing the VAT on consumption was therefore an odd choice, even from this strangely contractionary point of view). A much better alternative is to roll back some of the recent tax impediments to domestic investment. 



_Average_ tax rates on labor were still relatively low in Japan over the 1985–95 period as a whole. But the opposite is true of _marginal_ tax rates on highly skilled salaried people, which are extremely high in Japan. As a practical matter, the complaint that salaried people are more heavily taxed than small business owners, farmers and politicians can only be ameliorated by reducing the _marginal_ burden on salaries. 



Rising marginal tax rates on labor and capital are certainly not Japan’s _only_ problem. Monetary policy also made an unsettling shift between extremes between 1988 and 1991. Excessive regulation of finance and commerce is another unnecessary obstacle to economic progress. But the likelihood that changes in the _microeconomic incentives_ of tax policy may account for a large part of Japan’s longer‐​term economic slowdown deserves more serious attention than it has been getting. This is _not_ about tax revenues (which are terribly weak). It is about _incentives._



**“Tax Reform” Took a Wrong Turn**



Some Japanese tax specialists really believe the Shoup Commission designed a system that was “pure” in some theological sense. It does not work in practice, but it looks beautiful in theory. Such students of Shoup missionaries must therefore regard the _annual tax cuts_ of 1950–74 as a horrible mistake — a deviation from the “comprehensive” (yet arbitrary) Haig‐​Simons definition of taxable income. Hiromitsu Ishi, for example, recently urged that _“reform should achieve a return to the Shoup proposals.”_ Indeed, that is exactly the direction that Japan has taken since 1988–89 — heading back toward Occupation austerity policies in the name of “tax reform.” 



The Shoup Commission advocated a value‐​added tax, and urged that income taxes be applied to dividends, interest and capital gains. From 1989 to 1992, “tax reform” in Japan was almost _defined_ as adding a value‐​added tax and adding income taxes on dividends, interest and capital gains. There was also a curious fascination with the _number_ of tax brackets (which is irrelevant), but too little attention among academic economists to the uncompetitive _height_ of marginal tax rates, and to the declining real income thresholds at which they were applied.(17)



 **Figure 4** showed that Japan’s taxes on capital have become much heavier since all these “tax reforms” were adopted. In one respect, this may seem paradoxical. From 1988 to 1990, the statutory tax on (retained) corporate earnings was reduced from 42% to 37.5%, at the national level. But the corporate tax on earnings paid out as dividends was simultaneously increased from 32% to 37.5%. The net effect left the effective corporate tax virtually unchanged.(18) Although the _national_ corporate tax rate of 37.5% does not appear to be much higher than the U.S. rate, local taxes on corporate profits are much higher in Japan. 



The main reason that Japan’s tax on capital increased, however, was the new taxes on interest income and realized capital gains of individual investors. After April 1988, individuals who supplied debt capital to businesses (but not to housing) were subjected to a new 20% withholding tax on interest income. A year later, individuals who supplied equity capital to corporations were also subjected to national and local taxes of 26% on capital gains. Although these taxes are collected from individuals, the increased tax wedge raises the cost of capital to businesses. 



**Punishing Efficient Uses of Land**



The 1992 land tax is another new tax on capital, but one designed to achieve a specific purpose. Ishi describes the land tax as “an attempt to seek an effective policy with respect to reducing land prices.” Unfortunately, the Bank of Japan had the same goal. If inflicting windfall losses on landowners is a legitimate policy objective, then these policies were extremely effective. 



Land had been a very important asset behind Japan’s stocks, so deflating land prices deflated stock prices too. The addition of taxes on stockholder capital gains was also bound to be capitalized in lower stock prices. Land and stock were collateral behind many loans, so the goal of “reducing land prices” continues to have unpleasant repercussions on banks and credit. 



The actual problem is that capital gains taxes on land _sales_ can be _extremely_ high, while property taxes on land _ownership_ are very low, particularly for farms. The net effect is a powerful “lock in” effect that discourages property sales, and discourages efficient use of a valuable resource. Adding another tax on large business land holdings did nothing to fix this problem, because it continued to impose the lowest tax on the least efficient uses of land (urban farmland) while adding a discriminatory tax on the most efficient uses (big business). The real problem continues to be the prohibitive capital gains tax. Henry George made an overly enthusiastic theoretical case for taxing the _appreciation_ of unimproved land. But the only practical way of doing that is to tax _realized_ capital gains land _transactions._ Since any such _transactions_ tax is easily avoided by _not selling_ , changing land ownership toward more efficient uses (a process _facilitated_ by “speculation”) is thwarted by confiscatory taxes on this particular source of capital gain. Capital gains on land _transactions_ should surely not be taxed at a higher rate than gains on financial claims to land (which is largely what many corporate stocks represent). Also, higher capital gains tax rates on assets held for short periods do not make economic sense for _any_ asset, except perhaps as a very crude way of indexing for inflation. In short, the capital gains tax on land is far too high, and the new land tax is much less efficient than raising the broader property tax. 



******High Rates, Low Revenues**



There are usually two obstacles to reducing excessive tax rates. One is the belief that marginal tax rates of 50% or more actually raise substantial revenue. We have indicated before (in **Figure 1** ) that high individual income tax rates are normally associated with relatively _low_ tax revenues, and with slow growth of revenues. We also noted that several countries which slashed their highest tax rates to 28–40% in the 1980s suffered no revenue losses.



Despite our earlier warnings about expressing tax revenues as a percentage of GDP (rather than as real growth over time), **Figure 5** shows that revenues from Japan’s Social Security tax increased from 6% of GDP in 1975 to 10.4% in 1995. In the same period, revenues from all other taxes increased more modestly, from 14.9% to 18.1% of GDP. In fact, revenues were no higher in 1995 than they had been in 1980, despite 15 years of bracket creep, a new VAT, and new taxes on interest income, capital gains, and land. 





**Figure 6** takes a closer look at the nineties. It shows that receipts from individual income taxes declined by 22% from 1990 to 1995. Corporate tax receipts fell even more, thanks to weak profits. 



In the nineties, Social Security has been the _only_ significant source of added revenue. All other sources of tax revenue dropped to 18.1% of GDP in 1994–95, down from 22.2% in 1990. That actually understates the problem, because there was also very little growth of GDP. 



The prolonged downward trend of income tax revenues after 1990 was certainly not due to any “tax cuts” during these years. On the contrary, 1989 is when the biggest effort to _increase_ taxes began. 



Receipts from the VAT have been almost flat and rather small — about 1.5% of GDP. It is a common mistake to regard revenues from a value‐​added tax as a net addition to total revenues. Any variety of sales tax must _reduce sales_ , and therefore reduce personal and corporate incomes that would otherwise have been generated in producing and selling goods and services that are no longer marketable (because the VAT has raised their prices). Whether the increased revenues from VAT exceed the lost revenues from incom ****e taxes is an empirical question.



 **Figure 7** switches to a long‐​run picture of revenues from both income taxes (corporate and individual) and the VAT in real terms, adjusted for inflation. Together, Figures 5 to 7 hammer home a simple point: _Japan’s total tax revenues have declined in real terms ever since the VAT and various investor taxes were introduced._ Without the increase in Social Security tax, which depends on it remaining attractive for employers to employ and for workers to work, the budget would be in much more serious shape than it is. All these ambitious new taxes have been killing revenues. 



If taxes were measured solely by their revenue yield, as macroeconomists tend to do, then Japan’s numerous and painful new taxes would look like a “tax cut.” Such falling revenues from rising taxes might be a run of bad luck. But Canada’s experience suggests otherwise. After Canada added a VAT in 1990, revenues from all taxes virtually stopped growing. Revenues rose by 19% from 1990 to 1995 (only 1.2% if measured in U.S. dollars), compared with a 52% rise from 1985 to 1990, and 59% from 1980 to 1985. 



Japan’s subtraction‐​method VAT (with exemptions for small firms, and for some important services) is easy to evade. Such a tax will never raise much money. Higher rates will just produce more evasion, and that evasion will result in greatly reduced _income and Social Security_ tax receipts. It would be easier and more effective to allow prefectural and municipal governments to adopt simple _retail sales taxes_ , preferably at rates of their own choosing, in exchange for lower local income tax rates. 



When a large package of new taxes is followed by eight years of economic stagnation and falling government revenues, it is time to consider the possibility that some of those tax policies were a mistake. There is nothing wrong with admitting mistakes and then fixing them. 



**Income Leveling**



Aside from revenue anxieties, the other major obstacle to any constructive policy change, in both the U.S. and Japan, is the stubborn notion that tax policy can and should “redistribute income.” 



Taxes do not redistribute income; they just _reduce_ income. If less real income is produced, there is less to distribute. 



High marginal tax rates on capital must make capital more scarce and therefore _more valuable_. Pretax returns to capital rise to adjust for the tax (in a world of mobile capital, after‐​tax returns cannot possibly remain below the global norm). With less capital per worker, however, there will be less real output per worker, and therefore less real income per worker. Labor thus ends up bearing the burden of taxes ostensibly aimed at affluent owners of capital.(19)



A steeply progressive tax on _labor_ income is mainly a tax on the returns to investments in higher education. This tax makes human capital more scarce than otherwise, and therefore more valuable. Salaries of highly skilled people will command a larger premium (or they will emigrate until that happens). Consumers end up paying this tax, because they must pay more for the artificially scarce services of highly skilled professionals and managers, directly or indirectly. 



Distribution tables, which purport to show how various income groups will fare under different tax policies, involve hopelessly static, zero‐​sum, partial equilibrium incidence assumptions.(20) The first step toward meaningful tax reform is to discard these meaningless statistical exercises. 



**Minimal First Steps**



It would be wonderful to see Japan embrace some sort of fundamental tax reform, perhaps borrowing ideas from Hong Kong or Singapore, but this might take more time than the situation can afford. In the meantime, there are many very constructive steps that could be taken immediately. 



1\. The highest individual income tax rate should not exceed 40% at the national level. The threshold for the 30% rate should be raised too. 



2\. The effective corporate tax should also be reduced to no more than 40% (including the enterprise tax) through some combination of lower rates and more rapid depreciation. 



3\. The VAT could be rolled‐​back to 3% for _at least_ two years (announcing a return to the 5% rate would shift buying plans forward in time, risking another slump after the increase took effect). 



4\. The securities transactions tax should be abolished _immediately_. Phasing it out would provide a risky incentive to delay stock transactions until 1999.(21)



There is much more to do, of course, and many possible ways by which to do it. The Japan Research Institute, among others, has offered several worthy suggestions.(22) The essential point is that marginal tax rates on capital and human capital are much too high in Japan, sapping the entrepreneurial vitality of the economy. The highest tax rates do the most damage to the economy in return for the least revenue. There is little risk in being bold, and great danger in being too timid. 



Economic growth requires more and better capital, including human capital. All taxes fall on individual suppliers of labor and capital, including taxes ostensibly levied on corporations or consumption. Even consumption taxes are really production taxes. Taxes on a company’s stockholders, workers and consumers hurt business, and taxes on business hurt stockholders, workers and consumers. Excessive tax rates on capital hurt labor by reducing investment and therefore slowing the growth or real output and income per hour of work. Demoralizing tax rates on labor likewise hurt capital by raising reservation wages, shortening lifetime work hours, and reducing the intensity and quality of work. 



If Japan continues to embrace the tax and spending policies of Continental Europe and Scandanavia, nobody should be surprised if economic performance becomes as disappointing as it has been in those areas. Without more vigorous economic growth, Japan’s future budget problems could become far more difficult. Philosophers are free to debate “equity” all they like. But the serious question to ask about the structure of tax incentives is the question that was at the top of Japan’s list in the 1950s: “ _How will this tax proposal help economic growth_?” An economy that is taxed into oblivion will not help anyone — not the poor, and not even the politicians. 



* * * * *   
**Appendix: Money Matters**  
  




Macroeconomics is concerned with the short‐​term growth of aggregate demand, or _nominal_ GDP. Microeconomics is concerned with longer‐​term incentives to expand aggregate supply, or _real_ GDP. That distinction led to the 1976 phrase “supply‐​side” economics — meaning the application of microeconomics to macroeconomic problems. The “demand side” was not neglected, however, but was properly assigned to the central bank. 



The Bank of Japan pursued a very expansionary monetary policy during the so‐​called “bubble” period, and an extremely restrictive monetary policy since 1991. From 1987 to 1989, reserve money grew by 11.5% a year, and broad money (M2+CDs) by 10.9%. **As Figure 8** shows, this expansive monetary policy financed growth of _nominal_ GDP of more than 7% a year from 1988 through 1990. 



In 1991–92, the monetary base was actually _reduced_ by 2.8% a year, which was quite remarkable. Broad money then grew at only a 1.2% rate. Growth of nominal GDP slowed to 2.8% in 1992 and to less than 1% from 1993 to 1995. Consumer prices have been _falling_ lately, aside from the one‐​time effect of the increased VAT in April 1997. 



The conventional wisdom is that monetary policy is impotent in Japan — merely “pushing on a string” — because nominal interest rates are very low. That is exactly what was said about the Federal Reserve from 1930 to 1933. It was dangerously wrong then, and still is. Interest rates are high in Turkey because the central bank prints too much money. Interest rates are low in Japan for the opposite reason. 



In a deflationary situation, shaky banks naturally want to hold more reserves, and people want to hoard more currency. The central bank has to accommodate those liquidity demands before additional bank reserves and currency can have any “reflationary” effect. If the central bank fails to convert enough securities into cash, through the discount window and open market purchases, then people have to liquidate assets and inventories to get cash. To stop such a deflation, the Bank of Japan merely has to purchases as many domestic or foreign securities as necessary, and discount freely (i.e., without rationing access to the discount window) at a penalty rate. 









**NOTES**



1\. William Niskanen writes of “the residual Keynesian perspective of many older economists, based on a theory — without evidence — that government deficits increase total demand.” — “Myths About the 1980s,” _The Wall Street Journal_ , November 5, 1996. 



2\. In his 1852 critique of Louis Napoleon’s “hot house” economics, even Karl Marx understood that “public works increase the obligations of the people in respect of taxes.” Indeed, debt service now accounts for 22% of Japan’s national budget, despite the lowest interest rates in world history. 



3\. When the United Kingdom first introduced a 10% VAT in April 1973, this was promptly followed by two full years of recession in 1974–75. The VAT was increased to 15% in June 1979, followed by another two years of recession in 1980–81. 



4\. “The problem is that analysts use tax revenue, not tax rates … When households evade a high tax, they drive down tax revenue, and so the high‐​tax experiment is less noticeable in the data.” — William Easterly’s comment in _Brookings Papers on Economic Activity_ , 1995:2, p. 421. 



5\. See Alan Reynolds, “Tax Cuts Will Restore the Tigers’ Roar,” _The Wall Street Journal_ , March 17, 1998. 



Also, Reinhard B. Koester & Roger C. Kormendi, ” Taxation, Aggregate Activity and Economic Growth: Cross‐​Country Evidence on Some Supply‐​Side Hypotheses” _Economic Inquiry_ , July 1989, pp. 367–86. 



6\. “By 1956 the total number of special [tax] measures exceeded fifty because the government was very active in the promotion of economic growth through tax devices.” Keimei Kaizuka, “The Tax System and Economic Development in Japan,” in Richard A Musgrave, Ching‐​huei Chang & John Riew, eds., _Taxation and Economic Development Among Pacific Asian Countries_ , Westview Press, 1994, p.55 



7\. “In 1950, a drastic change was enforced to make the income tax less progressive than it was under the influence of the Shoup proposals.… The treatment of savings or investment income [became] almost the same as that which would be applied to all savings under and expenditure tax.… However, this hybrid developed spontaneously, without any special attempt to avoid the double taxation of savings.” Hiromitsu Ishi, _The Japanese Tax System,_ Clarendon Press, 1993, pp. 86 & 97\. 



8\. Jerome B. Cohen, _Japan’s Postwar Economy_ , Indiana University Press, 1958, p. 107. 



9\. Martin Bronfenbrenner, “Economic Miracles and Japan’s Income‐​Doubling Plan” in William W. Lockwood, ed., _The State and Economic Enterprise in Japan_ , Princeton University Press, 1964, pp. 536n & 551–52. A more perceptive essay by Hugh Patrick of Yale University attributed Japan’s vigorous growth to continual “tax rate cuts,” but even Professor Patrick saw Japan as careening “from one balance of payments crisis to another.” 



10\. Henry C. Simons, selection from _Personal Income Taxation_ (1938) reprinted in H.C. Harlan, ed., _Readings in Economics and Politics_ , Oxford University, 1961, p. 303. Simons’ influential 1948 book, _A Positive Program for Laissez Faire,_ also advocated breaking‐​up large enterprises, which was almost adopted in extreme form by Occupation officials until a properly alarmed U.S. Congress stopped them. 



11\. Referring to Shoup, Vickrey, Pechman and others, Musgrave captured the zeal with which that former generation of tax missionaries spread their gospel: “The comprehensive income tax base thus became the banner of tax reform in the United States, designed to . . provide a global base on which progressive rates could be assessed… This movement … provided the focus of analysis and delight for a generation of tax economists in the United States.” R. A. Musgrave, “A Brief History of Fiscal Doctrine,” in A. Auerbach & M. Feldstein, eds., _Handbook of Public Economics_ , Elsevier Science, 1985, Vol. 1, p. 22. 



12\. Government investment, including “the pump‐​priming function of public finance,” was the main theme of former Prime Minister Kakuei Tanaka, _Building A New Japan_ , The Simul Press, 1972, p. 207. 



13\. Yukio Noguchi, “Tax Reform Debates in Japan,” in Michael J. Boskin & Charles E. McClure, eds., _World Tax Reform_ , International Center for Economic Growth, 1990, p. 114. 



14\. “A scheme that subjects the old to global income taxation would be superior to simply using the consumption tax.” — Maria S. Gochoco, comment on Yukio Noguchi, “Aging of Population, Social Security and Tax Reform,” in Taktoshi Ito & Anne O. Krueger, eds., _The Political Economy of Tax Reform_ , University of Chicago, 1992, p. 232, 



15\. Ishi, _op. cit_., p. 54: “What are the main causes for the sharp rise of fiscal deficits since 1973? … First, there was a conspicuous slowdown of Japanese economic growth cause by the two oil crises.” 



16\. Willi Leibfritz, John Thornton & Alexandra Bibbee, “Taxation and Economic Performance,” OECD Working Paper No. 176, 1997, p. 50. 



17\. Unlike leading groups of academic and think tank economists, who advocated leaving the top _national_ tax rate at 50–60% (while also taxing much more of investment income), Japan’s Committee for Economic Development, a business group, made a relatively bold proposal in January 1986 that the _combined_ national and local income tax rate (then 78%) should not exceed 50%. Even the major trade unions advocated reducing progression. M. Homma, T. Maeda & K. Hashimoto, “Japan,” in Joseph A. Pechman, ed., Comparative Tax Systems, Tax Analysts, 1987, pp. 429–32. 



18\. Toshiaki Tachibanaki & Tatsuya Kikutani, “Japan” in Dale W. Jorgenson & Ralph Landau, eds., _Tax Reform and the Cost of Capital_ , Brookings Institution, 1993, p. 262. 



19\. This modern “general equilibrium” analysis of tax incidence is often associated with Joseph Stiglitz, _Economics of The Public Sector_ , Norton, 1988, pp.430–32. But it was well understood by classical, pre‐​Keynesian economists. Harvard’s Sumner H. Slichter, _Modern Economic Society_ , Henry Holt, 1929, p. 743: “To the extent that a tax on capital retards the increase in the supply of capital, it enables capitalists to obtain a higher return on their funds and falls, therefore on others [consumers and wage earners].” 



20\. David F. Bradford, ed., _Distributional Analysis of Tax Policy_ , American Enterprise Institute, 1995. 



21\. There is no legitimate reason to restrict corporate share repurchases, and therefore no reason for making the easing of such restrictions temporary, as has been proposed. Share repurchases create capital gains, which will generate tax revenue and strengthen loan collateral. If the reason for the restriction on repurchases has been to foster dividend payouts, that would be better accomplished by restoring some relief from double‐​taxation of dividends. 



22\. _Japan Research Quarterly_ , Winter 1997/98. 
"
"
With apologies to Robert Duvall in Apocalypse now-
Kilgore: Smell that? You smell that?
Lance: What?
Kilgore: Sewage, son. Nothing in the world smells like that.
[kneels]
Kilgore: I love the smell of sewage in the morning.  The smell, you know that rotten eggs smell… Smells like… victory. Someday this war’s gonna end…

USHCN at Tullahoma, TN Wastewater Treatment Plant – Visible light

USHCN at Tullahoma, TN Wastewater Treatment Plant – Infra red
You know it seems like every morning this week that I prepare to start my day’s worth of surveys, I find that I’m going to visit another USHCN climate station of record at a sewage treatment plant. And so is the case today, my last day of surveys. I’m gonna take a loooong shower when I get home.
I know you all want to hear more about NCDC and USHCN2, and I’ll get into those details next week, but for now, another sewage treatment plant beckons.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fc997a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAlso the Danish Meteorological Institute (DMI) projects a sturdy Arctic sea ice extent for this July, meaning no falling summer ice extent trend since 2007! The climate alarms are being muffled. 
Snowfan here gives us the latest on global mean temperature and Arctic sea ice.
After the year’s low in June 2020, with an anomaly of +0.48°C from the 1981-2010 WMO climate mean, the global 2-meter temperatures (black line) depicted below shows the July 16, 2020 analysis and forecast up to July 23.

Source: here
Both the anomalies of the global 2-meter temperatures from the WMO mean (black line) and especially the temperatures on the SH (blue line) continue to fall, with the deviations on the SH repeatedly falling below the zero line. This also pulls the global temperatures (black line) down to near zero in the forecast.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But also the temperatures in the Northern Hemisphere (red line) have had a falling trend since the end of February 2020, having since reached a new annual low. There has been no more global warming since 2016. Note: With 81% of the sea surface, the southern hemisphere has the largest energy storage on earth.
Surprise DMI projection
Arctic sea ice trend growth in July 2020?

Source: DMI
A surprising DMI forecast was issued on July 14, 2020 which projects strong growth of Arctic sea ice areas for July 2020. If this expert forecast is correct, it would mean there’s been a strongly positive summer trend since 2007 – instead of the ridiculous Al Gore complete meltdown.


		jQuery(document).ready(function(){
			jQuery('#dd_9c06936df01f592ca3b87782180bf27a').on('change', function() {
			  jQuery('#amount_9c06936df01f592ca3b87782180bf27a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Economic interests are set to play an increasingly important role in shaping development in the Arctic. Yet prominent members of the mining industry, familiar with the economic and reputational perils of impatient investment, remain cautious. They can – and should – play a pivotal role in guiding responsible industrial activity in the region.  Corporate development of the Arctic appears to be a foregone conclusion and this is reflected by the development of major transnational agreements. For example, with Arctic shipping projected to increase massively in the coming years, the UN Maritime Safety Committee (MSC) will adopt the Polar Code, international guidelines for the safety of ships operating in polar waters.  On the back of such recognition, countries are making longer term economic commitments. China, for instance, has just agreed to purchase oil and gas from the Russian Arctic over the next decades, while at the same time having secured stakes in Russian oil platforms in the region.  The Arctic is rich in oil, gas, and metals such as nickel, copper, gold, uranium, or tungsten. It even has large diamond reserves. Rapidly shrinking sea ice exposes new shipping routes through the Arctic Ocean that will save time and money for companies moving goods from Asia to Europe, while also providing new opportunities for tourism and fishing.  Mining companies have a big opportunity here, and some of the planet’s northernmost mines are already making an impact. Alaska’s Red Dog mine is one of the world’s biggest producers of zinc and lead, whereas Greenland’s Ilimaussaq complex is estimated to meet a quarter of global demand for rare earth elements, critical components in a wide range of electronic devices, over the next 50 years. With Arctic mining in its infancy, many non-Arctic states, including China and the UK, are lining up to invest in future projects.  In spite of these riches, mining companies, particularly the mega-multinationals such as BHP Billiton or Rio Tinto, remain cautious about the Arctic. This is partly because mining companies have had their fingers burnt by high-profile environmental accidents, such as the 2006 lead poisoning of the Australian town of Esperance or the discharge of over two billion tons of untreated mine waste, over nearly three decades, into the Ok Tedi river in Papua New Guinea. Mining accidents have also killed or disabled workers. For example, around 170 miners are killed each year in the South American gold industry. Expensive mining projects built in a hurry have hit the industry’s reputation. Yet these companies can only avoid the Arctic for so long. Climate change coupled with the global decline of ore quality has already made life difficult for extractive industries, which find themselves having to operate in increasingly remote environments where they encounter heightened competition with local inhabitants for water and energy. In addition, most major mining companies are part of the International Council on Mining & Metals (ICMM), founded in 2001 with the specific aim of addressing sustainable development challenges. As members, major mining companies commit to a set of principles designed to maintain sustainable development standards. This arrangement is unique for large-scale industries; for example, the equivalent global oil and gas industry association for environmental and social issues (IPIECA) allows any company to join regardless of environmental performance.  As the Arctic opens up for business, mining has taken an important step towards a leadership position in private-sector environmental stewardship. At the recent Arctic Circle Assembly in Iceland, ICMM president Anthony Hodge urged other industries to follow mining’s lead in endorsing full sustainability perspectives in the Arctic, taking into account the well-being of the region’s people and the broader natural environment.  Admiral Robert Papp, the newly appointed US Special Representative for the Arctic, believes social media has completely changed how the US government interacts with remote communities in Alaska, increasing pressure for accountability by establishing a transparent two-way dialogue. It’s a lot harder to put a zinc mine next to a village in the middle of nowhere if the residents are able to tweet their concerns. Modern communication has also pushed industry transparency and disclosure policies to the top of the agenda. This bottom-up approach to whistleblowing has contributed to rapidly improved standards of accountability among extractive companies, suggested within the broader Global Reporting Initiative (GRI) and the more recent industry-specific Initiative for Responsible Mining Assurance (IRMA). Operating in the Arctic is challenging, whatever industry you’re in. But mining firms have already made many of the mistakes, and learnt many of the lessons, that lie ahead of the oil, gas and shipping industries. Before these mistakes are repeated by others, mining representatives must step up to facilitate the sensible economic development of Earth’s northernmost latitudes."
"
One of the strangest things I’ve learned in the past year about the US Historical Climatological Network is the propensity for placement of weather stations at sewage treatment plants.
The reason of course has to do with putting a thermometer at a facility that is staffed 7 days a week. That thermomter must be manually read once a day and the readings transcribed into a logbook. Waste Water Treatment Plants (WWTP’s) fit that requirement (as they have an operator on duty, often 24/7) but they themselves are their own mini islands of waste heat and humidity, especially in winter and overnight. Yet, a significant portion of the US climate data comes from these locations.
Some have grassy areas where a climate monitoring station could be placed, such as the one in Morrison, IL, and you’d think they would place it there, away from the sewage tanks. Unfortunately, no.

Click for a larger image, additional photos available here at surfacestations.org
My sincere thanks to volunteer surveyor Scott Finegan for these photos.
The Stevenson Screen housing the thermometer is about 5 feet away from each tank, while the concrete building in the background is some 50 feet away. You’d think that they could have placed the station a little further away. Again, as we’ve seen time and time again, the placement is not often about the best location, it is about convenience for the observer.
The GISS graph of temperature over the station history shows a fairly strong warming trend from about 1980 to the present. The question is, how much of that is from increased throughput of the sewage treatment plant responding to population growth, and how much of it is climate change?

Click for source graph from NASA GISS
According to NCDC’s Multi Metadata System database, this station has been at this location since at least 1948, even though a lat/lon accuracy update makes it appear to have been relocated in 1997, it has in fact been at this location all that time.
A nearby station, 25km away, Clinton, IA, is also in the GISS database and shows less of a trend during the same period:

Separating a climate change signal from the waste heat (and increasing effluent volume of the WWTP due to population growth) may not be a simple matter to disentangle. Since each WWTP has different conditions, coming up with a blanket correction would not be easy. Therefore, since the USA is highly oversampled spatially with weather stations that report daily data which can used for climate, it would be prudent in my opinion, to remove stations like this from the climatic database since the data produced by USHCN stations at WWTP’s may not be truly representative of climate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f3e77b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn the early 1900s, the globally-averaged distribution of calculated surface temperature estimates ranged between 14 and 15°C. For 1991-2018, HadCRUT, Berkeley, and NASA GISS also estimate today’s global temperature is about 14.5°C.
Scientists estimating Earth’s surface temperature has been an ongoing pursuit since the early 19th century.
A new study (Kramm et al., 2020) suggests the generally agreed-upon global temperature from 1877 to 1913 from dozens of calculated results was about 14.4°C.
Problematically, HadCRUT, Berkley, and NASA GISS also indicate the 1991-2018 had a global surface temperature of about 14.5°C.
This would suggest there has been “no change in the globally averaged near-surface temperature over the past 100 years”.

Image Source: Kramm et al., 2020
Share this...FacebookTwitter "
"

Representatives of some 160 nations are gathered in Kyoto, Japan, this week to negotiate an international treaty to control emissions of greenhouse gases. While the summit has all the trappings of a substantive event, the gathering in Kyoto is more an exercise in public relations than in serious statecraft. Hot air, rather than cold reason, will dominate the conference and political sleight of hand will be its product. 



That’s because, according to the International Panel on Climate Change (the United Nation’s body of experts devoted to the study of global warming and known as the IPCC), even the most aggressive and costly proposals on the table this week would shave only a fraction of a degree off temperature increases projected by computer models for the year 2100. To achieve those emissions reductions, each American would have to pay an additional $1,000-$3,000 annually in higher energy prices. Such proposals would, according to Yale economist and sometime Clinton adviser William Nordhaus, take us back to the days of semipermanent energy crises like those of the 1970s. 



Both greenhouse alarmists and skeptics agree that actually preventing the global warming projected by computer models would require the world to reduce carbon dioxide emissions by 60–80 percent. Only by virtually abandoning the use of oil, gas and coal could we achieve such reductions. President Clinton’s assurances about “free lunch” climate change policies notwithstanding, nobody is proposing any real policy to prevent climate change because no one wants to usher in a permanent global depression. The idea, then, is to get the world to commit to a slow‐​motion control policy, one that would ease us into higher energy costs, a reordered industrial world and, as National Public Radio reporter Richard Harris puts it, “a whole new society” structured around less energy use.



But if the computer models are correct about global climate change (the data thus far are inconclusive), what choice do we have? Isn’t it prudent to hedge our bets with a control strategy now in order to avoid far more costly economic crash planning later? Well, no. All indications are that the “cure” for global warming is far worse than the “disease” of rising temperatures.



First, only about 2 percent of America’s economy is sensitive to weather conditions. No matter how ruinous climate change might be, it couldn’t possibly have a serious long‐​term impact on the United States. Even the most alarmist projections of ocean rise (about 3 feet or so) are trivial. If Amsterdam could figure out a way to hold back an even larger sea rise hundreds of years ago, it’s clear that a wealthier and more technologically advanced United States could counter a 3‐​foot rise. Foreign aid to help poorer countries adopt would be far less expensive than control policies.



Second, it’s not altogether clear that a warmer world would be a less habitable world. A temperature rise of 4.5 degrees Fahrenheit (the median computer‐​predicted result of a doubling of atmospheric carbon dioxide in the next 100 years) was exactly what occurred about a thousand years ago (A.D. 850 — 1300) in a period climatologists refer to as “the little climate optimum” (note that they don’t refer to it as “the little climate hell”). The result? A longer growing season, rapid economic development, a minor cultural renaissance, an expansion of fertile crop and forestland and a decrease in mortality rates. Since the data indicate that the small amount of warming we have detected over the last 100 years has largely been confined to winter evenings in the far northern latitudes, we have every reason — both empirical and theoretical — to believe that warming would be a benign, not a deleterious, event.



There are still open questions about how much if anything man has had to do with the slight amount of warming detected over the past 100 years and how much warming might eventually occur (the IPCC estimates range from insignificant to moderately significant). The IPCC report itself states that it will be another decade or so before scientists will know for certain. So why not wait? Nature magazine reported last year that waiting 20 years for better scientific information before acting will only cost us .36 degree Fahrenheit, at worst, over the next 100 years. 



In the face of this kind of uncertainty, the best “insurance policy” we could buy is one that increases the amount of wealth at society’s disposal to handle whatever problems might occur in the decades to come. Impoverishing society today to avoid a very uncertain problem tomorrow would harm, not help, future generations.
"
"
Share this...FacebookTwitterThe German transformation to green energies will fail due to wind power
By Prof. Fritz Vahrenholt, Die kalte Sonne

Not going to work, says German energy expert Prof. Fritz Vahrenholt
(German text translated/edited/subheadings by P. Gosselin)
The goals of the German transition to green energies are simple in terms of energy policy:
1. phase-out nuclear energy by 2022,
2. phase-out coal by 2035,
3. phase-out oil and gas in parallel and completely by 2050.
The energy needed for electricity, heat, mobility and industrial processes in climate-neutral Germany will then have to be supplied by wind and solar energy and a few percent by hydropower and biomass. This is at least according to the plans of the German government, which are supported by all major social players.
Is this realistic?
Today, wind and photovoltaics supply slightly less than 30% of the 600 terawatt hours of electricity (1 terawatt hour Twh is 1 billion kilowatt hours Kwh). Today 126 Twh is supplied by wind energy and 46 Twh by photovoltaics. For 600 TWh, the same mix would need 439 Twh of wind and 161 Twh of solar. For the sake of simplicity, let us assume that this amount of electricity should be generated by the largest wind turbines, namely 5 megawatt turbines positioned 1000 m apart. With an annual efficiency of 25%, a turbine produces an average of 5 MW x 0.25 x 8760 (hours) = 10,950 Mwh = 0.01095 Twh. For 439 Twh we would need 40,000 such turbines. To accomplish this, an area of 200 km x 200 km (40,000 sq km) would be required.
Too much unneeded surplus power
But we still would not reach the goal. Wind energy is produced when the wind blows, and not necessarily when the consumer needs it. With a power supply in Germany based solely on volatile sources, 36% of the electricity generated annually can be consumed directly (source: Dr. Ahlborn). The rest is surplus electricity that has to be stored. For economic reasons, storage in hydrogen alone is the best option here. For this purpose, a gigantic number of electrolysis plants would have to be installed.
Huge area required for electricity from wind turbines
However, it is completely uneconomical to dimension the capacity according to the extreme peaks of strong wind events. Therefore, about 12% of the wind energy has to be regulated. This now leaves 52% of the electricity generated that can be stored in hydrogen. Electrolysis of hydrogen, storage/methanization and conversion back into electricity leaves only 15.6% of the 52%. The entire conversion chain generates a loss of 2/3 of the electricity used. 36% plus 15.6% result in about 50% of the generated wind power being usable. Thus, we need twice as many turbines. The area for the 80,000 wind turbines becomes 80,000 km², which corresponds to an area of 283 km x 283 km (80,089 sq. km).
Now add the demand from transport and heating…
But we remain very far from the finish line. Up to now we have only covered the electricity demand with 2 x 439 Twh, but without supplying the demand from transport and heating. Also with demand from transport (today 600 Twh) and heat (today 1200 Twh) we have storage and conversion losses when the necessary electricity is generated by wind and solar. Here we only consider wind for this, because with photovoltaics, the annual efficiency of 10% full load hours is significantly lower and the land consumption is many times higher. This makes our calculation extremely conservative.
Devastating lack of efficiency


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Assuming that the transport sector can actually be powered by battery vehicles, which is justifiably doubtful, converting cargo transport, maritime transport or air cargo transport over to electricity is already adventurous. Instead, synthetic fuels would have to be used.
And here as well the electricity calculation is devastating. As Dr. Detlef Ahlborn was able to show, the Frankfurt airport alone consumes 14.7 million liters of kerosene per day (before Corona), which comes out 4.3 million tons annually. 4.3 million tons of kerosene correspond to an energy value of 47 Twh. If one wanted to synthesize kerosene from electricity with the help of hydrogen (assumed efficiency 50%), 100 Twh of electricity would be needed. Just for the Frankfurt airport alone, this comes out to being as much as the German wind energy industry currently produces (126 Twh).
Minimum 900 Twh for heating and transport
Next we conservatively assume that all passenger transport also can be powered with electricity and that only a quarter of the amount of the 600 Twh of energy consumed today (since electric cars are more efficient by this factor) is needed. However, we also want to drive a car when there is no wind, and as explained above, most of this electricity has to be put through the chain of hydrogen, storage, and re-electrification, thus doubling the input electricity to 300 Twh.
We further assume that the current demand of 1200 Twh for heating can be reduced to a quarter through electrification (heat pump) so that here too, due to the necessary intermediate storage of wind power via hydrogen, the necessary doubling of wind energy leads to 600 Twh. If synthetic gas from wind power, hydrogen, is used directly, the yield is even worse because the efficiency of the heat pump is not applicable. Transport and heat therefore in the best case lead to a wind power demand of 900 Twh. This results in an area requirement of another 80,000 km², thus we are up to 160,000 km² of area needed by wind turbines (approx. half the area of Germany).
Another 600 Twh for heavy industry
But we still haven’t reached the ultimate target because the most difficult part is still unsolved. Emissions from the steel, chemical and cement industries (10% of CO2 emissions) require 600 Twh, according to industry estimates (www.in4climate.nrw). This is easy to understand if one remembers the above example of Frankfurt Airport. And plastics, pharmaceuticals, insulating materials, paints, varnishes, adhesives, detergents and cleaning agents may then only be produced using CO2 plus hydrogen. The replacement of industrial CO2 emissions thus leads to a further 55,000 km² area for wind turbines, so now we are up to 215,000 km² – much more than half of Germany’s total area.
2/3 of Germany would end up plastered with wind turbines
Two thirds of Germany would now be outfitted with 200-meter tall rotating wind turbines at a distance of 1000m, no matter if there is a city, a river or a highway, a forest, a lake or a nature reserve.
Can we and policymakers imagine such a Germany?
Environmental catastrophe, obstinate policymakers
If you wish to know which effects wind power plants in large numbers have on the extinction of birds of prey, bats, the decline of insects already today, then read it in our book Unerwuenschte Wahrheiten (Unwanted Truths). There you’ll find the hidden fact that wind farms lead to a considerable warming in their area of influence of about 0.5 ° Celsius because the rotating blades compensate for the strong temperature gradient at night and shovel warmer air back to the ground. Numerous studies have shown that the soil in the windparks has dried up considerably.
10-fold higher electricity prices
But politicians refuse to discuss the environmental incompatibility of a massive expansion of wind power plants. Recently the German Bundestag decided that the so-called legal, suspensive effect of objection and action for rescission is no longer applicable to lawsuits against turbines taller than 50 meters. In this way, Germany can be now turned into a single giant wind park without all the annoying objection.
It is almost superfluous to point out that we are talking about astronomical costs. Electrolysis and power-to-gas plants cannot be operated free of charge.
From today’s point of view, one has to expect a tenfold higher electricity price. Any person can imagine the consequences for jobs and prosperity.
Prof. Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_1a0c8b531431dcb96168154efc24df57').on('change', function() {
			  jQuery('#amount_1a0c8b531431dcb96168154efc24df57').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterUPDATE: Here’s an even earlier version of the NASA GISS data plot of Hokitika station before all the data altering began (hat-tip Iggie). It shows temperatures had been COOLING:

Source: NASA
By Kirye
and Pierre Gosselin
We continue to hear warming horror stories coming from the island of New Zealand, and the socialist by sales pitch how “climate change is the biggest challenge of our time“.
Yet this doesn’t seem to be the case in New Zealand. For example, we learned from Electroverse here that the Pacific island country “just recorded its coldest June temperature in 5 years”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moreover, when we examine NASA GISS data, we uncover where all the warming rumors come from: alterations to the recorded historical data.
The following chart shows the data from Hokitika Aerodome, going back more 130 years (It’s the only NASA station that has data going back over 100 years). Plotted are the unadjusted Version 3 data and the Version 4 unadjusted:

Data source: NASA GISS V3 and V4
The old data set showed no warming, until NASA GISS went back and rewrote it Orwellian style, and made up a warming trend and called it Version 4.
Tony Heller also reported earlier: “NASA  didn’t like the fact that New Zealand wasn’t warming, so they simply changed the data.”
In summary: There really hasn’t been that much change at many locations around the globe. In fact the real changes are taking place in the NASA GISS datasets.


		jQuery(document).ready(function(){
			jQuery('#dd_2092e503990bc1bcda4ec06e5b01627d').on('change', function() {
			  jQuery('#amount_2092e503990bc1bcda4ec06e5b01627d').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

As this year draws to a close, I think back about what I’ve accomplished on this blog in the last year, and it occurs to me that I have a lot of people to thank. It truly has been a team effort in a lot of ways, with many people contributing from many different angles to help make the work I’m doing a possibility.
First and foremost, I’d like to thank Steve Thompson of Assemblyman Rick Keene’s office. It was his mention in an email to me “that Russ Steele and I ought to get together” that started me on the path to study climate change from the data gathering aspect. Of course Russ and I had similar ideas, but we just didn’t know about each other, and knowing that there’s somebody else nearby that thought like I did whom I could converse with, was really a boost. Of course on the political front, I should also thank the local activist group “Esplande League”, because if they hadn’t worked so hard to keep me from being re-elected to the local school board, I never would have had the time to pursue this research.
I owe Russ Steele a lot, not only for the many stations he’s surveyed, and for the encouragement and support, but also for introducing my work to many people, including Steve McIntyre of Climate Audit. It wasn’t until Steve took notice that things really began to take off. Steve has been most gracious in helping to promote my work and for offering me the ability to co-author on his blog.
And there are many others, I can think of the many volunteers on surfacestations.org that have contributed many ideas, data sorting and spreadsheet macros that saved me time and effort, and made the project’s data analysis better. Gary Boden, Chris Dunn, Joel McDade, John Goetz, Barry Wise, and Eric Gamberg have all made significant contributions to the project via surveyed stations and or improvements to the survey process and analysis.
Super surveyor Don Kostuch, has been traveling the country and surveys new stations every week. He is leader of the station surveyors not only in terms of quantity, but of quality too. His surveys are always carefully done. 15 year old Kristen Byrnes and her dad have surveyed almost all of New England single handedly.
One volunteer, Arthur Edelstein, I owe a great deal to because he did some significant data capture and collation that I wouldn’t have been able to do myself in the fraction of time that he did it in.
I owe Dr. Roger Pielke Sr. a debt of gratitude for his faith in my work and his encouragement, along with his assistant, Dallas Staley, who has pulled many an obscure request for data or publications out of nowhere, even after hours.
Then there’s all the other blogs and newspaper authors out there that have promoted what I’m doing.  Joe D’Aleo of ICECAP comes to mind, and does Barry Hearn and Steve Milloy of JS for publishing my “How not to measure temperature” series, and Kate from Small Dead Animals for being a regular traffic driver.  There’s Evan Jones, who is my most prolific and enthusiast commenter, along with regulars George M., Papertiger, Larry Sheldon, and Stan Needham. Let’s not forget Steven Mosher and Jeez, for putting up with my silly rants at dinner with Mac at AGU. Jeez also footed the dinner bill, and so deserves double thanks.
Local blogger Lon Glazner deserves a nod for blogging some early support and for some mental stimulus on thermometers that got me fired up last spring.
In the newspaper realm, Ryan Olson of the local Chico Enterprise Record, not only for the stories he’s done, but for putting up with my complaints about Moveable Type and helping me migrate to WordPress where I’ve been able to make a better product.  I thank Bill Steigerwald of the Pittsburgh Tribune whose article launched me into national attention. And finally, Evan, who did a really balanced and fair article even though I feared the worst.
Then there’s the 300 plus volunteers for www.surfacestations.org Thank you each and every one.
I owe you all a debt of gratitude. Thank you. If I’ve missed anyone, don’t be shy about speaking up.
There’s a few that deserved coal this year, but I’ll leave them nameless.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1ae9c6f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"At the COP24 conference in Poland, countries are aiming to finalise the implementation plan for the 2015 Paris Agreement. The task has extra gravity in the wake of the recent IPCC report declaring that we have just 12 years to take the action needed to limit global warming to that infamous 1.5ᵒC target. Although the conference itself is open to selected state representatives only, many see the week as an opportunity to influence and define the climate action agenda for the coming year, with protests planned outside the conference halls. A crucial role of environmental activists is to shift the public discourse around climate change and to put pressure on state representatives to act boldly. COP24 offers a rare platform on which to drive a step change in the position of governments on climate change. However, many environmental movements in Europe are not offering the critical analysis and radical narratives needed to achieve a halt to climate change. 


      Read more:
      Extinction Rebellion : I'm an academic embracing direct action to stop climate change


 By now most people agree that greenhouse gas emissions (including CO₂)  are the proximate driver of climate change, and that climate change is not only a future problem, but is already causing significant environmental and social problems across the world. Further, the trend in global CO₂ emissions still appears to be increasing, driven largely by consumption in advanced and emerging economies. Economic growth measures the increase in the amount of goods and services produced by an economy over time, and it has historically been tightly coupled to CO₂ emissions. Decoupling these two factors is not impossible, and indeed many leading academics argue that the power of human ingenuity will solve the climate crisis. However, this is certainly unlikely in the timescales needed to tackle climate change in a just and equitable way. Practically, what this means is that as long as economic growth continues to expand rapidly and indefinitely, so too will the quantity of CO₂ in the atmosphere and the associated environmental and social impacts. To address climate change, therefore, we must address the root cause of this planetary ailment: the ideology of growth first, growth always. By moving away from growth-oriented societies in Europe and other advanced economies, towards ones that prioritise environmental and social health, we stand the slimmest chance of solving our climate crisis, while still allowing the poorest economies globally to meet their economic needs. Recent environmental movements demanding action on climate change, like the Extinction Rebellion in the UK and the Ende Gelande Alliance in Germany, don’t seem to take a clear stance on the role of economic growth in driving climate change. They don’t identify our unwavering commitment to the dogma of infinite economic growth as the driving force behind climate change, and as the reason that our efforts thus far have been impotent to stop the growing tidal wave of CO₂ emissions. In the UK, the Extinction Rebellion has captured the public’s attention and gathered widespread support and media coverage over the past few weeks, with their outraged cries for government action.  However, their demands are broad and unspecific, asking for “net zero [carbon emissions] by 2025”. They make no mention of how the UK government might achieve this, but link to other websites which offer potential routes for reaching this target. The sites suggested by the Extinction Rebellion promote ideas such as green growth and a green new deal. These ideas are founded on the premise that we can achieve both continually high rates of economic growth and reduce our impact on the planet. Sadly, the evidence (and a dash of common sense) tells us that this is not yet happening, and is unlikely to be possible in the near future. So what should groups like Extinction Rebellion do? It would currently be considered political madness to advocate for policies that might unintentionally, or intentionally, limit economic growth. Unfortunately, however, without a wider critique of the toxic relationship between climate change and economic growth, governments will be almost powerless to achieve any net zero targets they set. At COP24 environmental movements have an opportunity to use their platform to highlight the relationship between economic growth and environmental impact, and even to discuss radical alternative futures that are not dependent on a growth-based economy. Importantly, this doesn’t have to be considered a sacrifice. The relationship between economic growth and happiness in wealthy economies is at best complicated, and at worst nonexistent. This demonstrates the possibility of finding paths to climate stability that do not diminish our quality of life. By identifying the root cause of climate change, and our inability to address it, these groups can go further than demanding action. They can change public mindsets, put pressure on national governments and point to a shared way forward. Here, we have our best shot at limiting the damage of climate change in a meaningful and timely way."
"Australia “needs action on several fronts” following a catastrophic bushfire season, including leading international efforts against climate change and cutting emissions beyond the electricity sector, Julie Bishop has said. Bishop, the incoming chancellor of the Australian National University, made the comments to Guardian Australia, offering to put the university’s disaster management experts and more than 300 climate scientists at the federal government’s disposal to provide “evidence-based” responses to the bushfires.  The former foreign minister and deputy Liberal leader started the ANU role in January but has postponed an event marking her appointment as the university responds to the outbreak of coronavirus and extreme weather including bushfires in the Australian Capital Territory and damage from a hailstorm. Bishop said that “clearly there have been some missteps” in the federal government’s handling of the bushfires, adding she is “certain the government will reflect on recent events and learn from any missteps”. “I as chancellor … have offered to assist the federal government in its bushfire recovery response,” she said. Bishop said the university was preparing to provide expertise in disaster recovery, public health, biodiversity, engineering, and climate science. “ANU has climate change expertise, we have expertise in bushfire recovery, 300 climate scientists across seven colleges and 25 schools. “We have the climate change institute, headed up by Mark Howden who’s on the intergovernmental climate change panel. “As a national university we have a responsibility to the Australian community to deliver evidence-based solutions … to the myriad consequences from these catastrophic events.” The government has come under fire for its handling of climate change because the prime minister, Scott Morrison, has argued Australia contributes just 1.3% of global emissions, while lobbying to lower global ambition by including carryover credits in 2030 Paris targets. Liberal senator Jim Molan said on Monday his stance on climate change was not based on evidence, while the new deputy Nationals leader, David Littleproud, has explained he had professed doubt – but now accepts – climate science because he is not “gifted academically” and lacks a scientific background. Bishop praised Britain for being “ambitious and a leading voice in calling for greater standards to deal with climate change” after Boris Johnson urged major economies to go carbon neutral by 2050. “I’ve always been of the view that Australia, as a leading industrialised and developed nation, with one of the best standards of living in the world, needs to be a leader in the international response to climate change,” she said. “We have a responsibility. The extreme weather events, the horrific fires, that take such a terrible toll on our communities and wildlife place us at the frontline of the impact – and so Australia has a direct interest in leading international debate on this topic. “I certainly look forward to our government taking on that role.” Asked how Morrison should balance Nationals and Liberal conservatives wanting more coal-fired power with moderate Liberals’ calls to do more to fight global heating, Bishop responded that Morrison “doesn’t need my advice”. “We need action on several fronts: disaster response, including the resourcing of emergency fire services; climate change mitigation [and] adaptation; and reducing our own emissions is obviously vital – but we have to focus on all areas, not just the electricity sector,” she said. Despite Morrison claiming the government has set out how it will reduce emissions by 26% to 28% by 2030, critics have noted it is yet to set vehicle emissions standards or a plan to reduce emissions in the transport and agriculture sectors. Bishop backed the Morrison government on its handling of the coronavirus, despite criticism from Universities Australia that Australian Border Force had overreacted by reportedly detaining Chinese students returning to Australia after the travel ban. Bishop said she assumed all measures were taken after “consultation with national and international health bodies”. ANU, like many other universities, has offered flexible arrangements to help its students affected by virus-related travel bans including online delivery of courses, intensive courses and deferral without penalty. Bishop said she was “not aware of any evidence” that Australia’s relationship with China had slowed evacuations from Wuhan or had suffered as a result of travel bans. Bishop listed among other priorities for the ANU to equip students and graduates for the fourth industrial revolution and disruption of traditional work, and to gain greater international recognition for the university. Since retiring from politics at the 2019 election Bishop has also taken a role on the board of foreign aid contractor Palladium which she insists is not a breach of ministerial standards, which ban ministers lobbying on “any matters on which they have had official dealings” in the previous 18 months."
"

Last summer, progressive legal scholar Simon Lazarus offered a commentary on the shifting landscape of the U.S. legal establishment.



“For decades, and as recently as Barack Obama’s first year in the White House, libertarians were marginalized within the conservative pantheon,” he wrote in the _New Republic_. “Now they rival, and in important areas threaten to displace social conservatives and big‐​government conservatives.”



This “upheaval” — which Lazarus concluded was “potentially seismic” — has had a discernible impact on more than just the conservative movement. Since taking office in 2009, President Obama has suffered a string of losses before the Supreme Court, racking up 20 unanimous high court defeats over the last five‐​and‐​a‐​half years. “The fact that his track record is as bad as it is in the Supreme Court,” Sen. Mike Lee (R-UT) recently said, “is yet another indication of the fact that we’ve got a president who is playing fast and loose with the Constitution.”



The Cato Institute has been at the center of this reversal. In another successful term at the marble palace at One First Street NE, the Center for Constitutional Studies went 10–1 in cases where it filed amicus briefs. This is on the heels of the Institute’s 15–3 record last term. Notably, the solicitor general’s office most recently went 11–9 on the year. “Perhaps the government would be better served following our lead on constitutional interpretation, advocating positions that reinforce our founding document’s role in securing and protecting individual liberty,” Ilya Shapiro, senior fellow at the Institute, wrote in response. Cato was the only organization in the country to file on the winning side of this term’s three highest‐​profile 5–4 cases.



The year’s most highly anticipated case was _Burwell v. Hobby Lobby_ , in which a familyowned business filed suit challenging the Affordable Care Act’s contraceptive mandate, citing religious objections. The Court ultimately sided with Hobby Lobby. After the ruling, Cato’s vice president for legal affairs Roger Pilon identified the core issue in the case. “Religious liberty is treated today as an ‘exception’ to the general power of government to rule — captured, indeed, in the very title of the statute on which the Hobby Lobby decision rests: the Religious Freedom Restoration Act,” he wrote on Cato’s blog. “That Congress had to act to try to restore religious freedom — to carve out a space for it in a world of ubiquitous, omnipresent government — speaks volumes.”



In _Harris v. Quinn_ , the Court ruled that government does not have the power to force public employees to associate with a labor union. At issue was an Illinois law claiming that home‐​care workers were public employees, ostensibly for one purpose: collective bargaining. The forced unionization of homebased workers has spread to nearly a dozen states, providing a substantial number of new workers — and dues — to the labor movement. “[This] decision will slow, and perhaps eventually end, that flow of funds, as workers decide they can represent their own interests and would prefer to keep their earnings for themselves and their families,” wrote Cato adjunct scholar Andrew M. Grossman.



Finally, the Court issued its latest blockbuster ruling on campaign finance with _McCutcheon v. FEC_ , striking down the “aggregate” contribution limits on how much money any one person can contribute to election campaigns. As Chief Justice John Roberts wrote for the majority, “If the First Amendment protects flag burning, funeral protests, and Nazi parades — despite the profound offense such spectacles cause — it surely protects political campaign speech despite popular opposition.” Shapiro, for his part, put a finer point on the decision. “In a truly free society, people should be able to give whatever they want to whomever they choose, including candidates for public office,” he wrote.



In one sense, these developments indicate the weight of Cato’s work today. But that impact is the cumulative effect of more than 30 years of intellectual debate. Over that period, the Institute’s mission has been to change the climate of ideas to one more conducive to a government of delegated, enumerated, and limited powers. Since 1989 the Center for Constitutional Studies has been a critical institution in that pursuit. As such, we may find that we are now approaching the Court’s libertarian moment.
"
"Climate change, deforestation, widespread pollution and the sixth mass extinction of biodiversity all define living in our world today – an era that has come to be known as “the Anthropocene”. These crises are underpinned by production and consumption which greatly exceeds global ecological limits, but blame is far from evenly shared. The world’s 42 wealthiest people own as much as the poorest 3.7 billion, and they generate far greater environmental impacts. Some have therefore proposed using the term “Capitalocene” to describe this era of ecological devastation and growing inequality, reflecting capitalism’s logic of endless growth and the accumulation of wealth in fewer pockets. As social inequality and ecological breakdown escalate, steady change may no longer be enough to avoid civilisational collapse. Environmentalists cannot rely on timid appeals to power any longer. I’ve had the pleasure of getting to know radical environmentalists from numerous groups throughout my doctoral research. I’m especially interested in uncovering their worldviews – how they diagnose the root causes of ecological decline and what motivates them to engage in often high-risk interventions on behalf of the natural world and other species. They reject human superiority and separateness from other species. They blame such views, in addition to capitalism and endless economic growth, for the dire state of modern ecosystems. Many follow a burning desire for a more viable and inclusive future for all. Notable radical green groups include Earth First!, Extinction Rebellion, the Hambacher forest occupation, and Sea Shepherd. Early Earth First! activists in the US sat in trees and dismantled tractors to prevent old-growth forests from being felled. For years, Sea Shepherd vessels successfully intervened and protected countless whales from Japanese whalers in the Southern Ocean. However, last year they ended their anti-whaling campaign due to, among other things, advancements in military grade technology by the Japanese whaling industry. Activists have occupied the ancient Hambach forest in Western Germany for a remarkable six years in an ongoing effort to keep coal giant RWE at bay. Many were violently evicted by police recently. Traditional environmental organisations like the WWF tend to focus on making industrial capitalism more sustainable rather than questioning capitalism itself. The radical green movement was born in response to the perceived inability of these mainstream environmental organisations to curb ecological decline. They advocate direct action in the form of civil disobedience, blockades, tree-sits, and even the dismantling of machinery for halting ecological destruction. Criminalising and repressing non-violent activists could fatally delay an effective response to climate change. In the UK, anti-fracking activists were arrested recently after blocking a convoy delivering equipment to the Preston New Road fracking site in Lancashire. They were initially given excessive prison sentences but were eventually released. Political theorist Steve Vanderheiden referred to such incidents in his 2005 article on the “Green Scare”. The “Green Scare” at its height in the mid-2000s saw the US government mount full-scale persecution of environmental activists. The FBI classed radical environmental groups such as the Earth Liberation Front as the nation’s lead domestic terrorist threat, even though it never targeted living beings.  Even the legal definition of “terrorism” was altered to include property destruction. This sought to target radical greens and their attacks against ecologically harmful infrastructure. Lengthy prison sentences and fines befell “eco-terrorists” caught engaging in direct action deemed threatening to economic interests. These are desperate times. We’ve lost a staggering 60% of monitored vertebrate life within just 40 years. Climate change will endanger millions through disease, extreme weather, starvation, and rising seas. Occupying trees or blockading a road to a fracking site is clearly justified resistance during times of widespread injustice. These are the ideas that environmental protectors are attempting to bring to the forefront. As George Monbiot noted, a “hopeless realism” in the form of piecemeal “tinkering around the edges” has led us to our present predicament. Similar approaches simply won’t fix the mess. Radical responses – direct action and mass political mobilising – might be our only hope for building the better world that is still within our reach."
"A cat is, of course, a cat. Lions are cats too, as are leopards, lynxes and so on – the “Felidae” family contains 41 species in total. But what about other closely related species such as hyenas or mongooses? These animals are not in the cat family: they are cat-like “Feliformia”, but are in their own separate families. So why are some species grouped together in the same families and others separated into different families? It might surprise you to learn that there is no general answer to this question, despite the fact that we now know a lot about evolutionary relationships for groups like mammals. Science has moved on and so should the way we classify life on earth. The science of “taxonomy” categorises species (such as Homo sapiens, in the case of humans) into broader groups such as orders (for example primates) or kingdoms (for example Animalia). Current approaches date back to 18th century Swedish biologist Carl Linnaeus. Linnaeus saw all living things as creations of god and sorted them into hierarchical groups according to how similar or different he perceived them to be. Evolution hadn’t even been theorised in Linnaeus’s lifetime. These days, we have a huge amount of DNA and fossil data to map out how, and when, one species branched out from another. Modern taxonomists therefore aim to base their decisions on evolutionary relationships, but the process remains subjective and there has been no attempt to standardise practises across all species on earth. Taxonomic groups such as birds and mammals represent “classes” under current classification systems, which are then subdivided into orders, families and genera. Our research uses the latest evolutionary trees for birds and mammals to demonstrate that current taxonomic classifications are highly inconsistent. [ ](http://www.onezoom.org/tetrapods.htm?view=1&signs=1&common=1&polytomy=3&ltype=2&hltype=2&font=helvetica&colour=1&init=1&taxa=Felidae&url=https://theconversation.com/evolutionary-evidence-shows-its-time-to-revise-how-we-classify-life-on-earth-33161&name=The%20Conversation&logo=http://emilynicholson.files.wordpress.com/2013/05/conversation-full-logo-cbaac7752ab98f2473e3fd769fa885a6.png&text=link%20to%20The%20Conversation) To resolve this issue, we can use evolutionary trees directly in order to consistently create taxonomic ranks. We applied a technique known as “temporal banding” to the bird and mammal trees, producing new classifications that reduce amount of evolutionary divergence within groups to a minimum. Under these new schemes, 70% of bird groups and 61% of mammal groups need to be revised. Biologists have generally determined the major taxonomic orders fairly consistently – we found that the big groups, such as parrots, hummingbirds & swifts, rabbits & hares, opossums and so on, have been made in a fairly constant manner. But classification can zoom in much further than this – there are 372 species of parrot, for instance, grouped into 86 genera. These more specific groupings are sometimes not much better than if they had been defined at random. Our study considered relationships within taxonomic groups that scientists use on a daily basis. This isn’t just a debate for scientists though, as these classifications have an important impact on what species we choose to study and how we communicate our observations of the natural world.  The New Zealand rockwren (Xenicus gilviventris) provides an excellent example of this. These are fairly unique species, not closely related to other species of wren, and are of conservation concern. When we classified bird species in a consistent manner, New Zealand rockwrens became their own taxonomic order, highlighting their evolutionary uniqueness to everyone. In another example, the dog family (Canidae) and the cat family (Felidae) currently have similar numbers of species but, under our standardised system, the cat family is expanded to include civets, hyenas, mongooses, fossas, and other relatives. As a result the new cat family contains four times more species than the dog family, which remained unchanged.  Since these new families are defined on a consistent basis, they tell us something about the evolution of these groups: cats have diversified far more than dogs over a similar time period. An example from the birds sees the owls, which are currently in the order Strigiformes, split in to two new orders: barn owls and true owls. These two groups are too distantly related to be lumped together. Such grouping by evolutionary divergence is controversial and many taxonomists will still feel that classifications should be focused on physical characteristics – what we call morphological similarity. However, this focus on what animals look like just adds inconsistency. A classification system based on morphology makes sense in theory, but in practise it leads to a high level of subjectivity. It is hard to imagine an objective approach based on morphology that could be applied across the entirety of life on earth. How could someone evaluate the physical difference between a bacterium and an animal? We are currently undergoing a revolution in DNA technology and our understanding of the tree of life is improving quickly. Our study demonstrates an approach that can consistently incorporate this information into the way we classify and view the natural world."
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"
Surfacestations.org volunteer surveyor Russ Steele brings us this gem of a climate monitoring station from Panguitch, UT. I’ve seen stations over asphalt, such as the University of Arizona station in Tucson, but this one has a special feature; they made a concrete traffic island especially for the station so that it wouldn’t get collided with by nearby parked vehicles. How’s that for diligence? The station mount was set right into the concrete. So much for the 100 foot rule away from asphalt, concrete and buildings issued by NOAA

Click image for larger version
The station was recently closed, and the instruments and wooden portion of the shelter put into storage, which is why you don’t see any Stevenson Screen shelter in the picture above, only the mount. Since it’s permanently set into the concrete, they couldn’t easily remove it. Not well thought out, I’d say.
The GISS temperature plot has an offset just before the year 2000, care to bet when the concrete for the traffic island was poured?



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea2c58e84',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt didn’t receive much a attention in 2015, but a comprehensive Nature journal study of 0-2000 A.D. global sea surface temperatures shows 1) climate changes occurred more than twice as fast during the Little Ice Age (LIA) than since 1800, 2) the entire first millennium was >1 standard deviation (s.d. unit) warmer than today, and 3) 1800-2000 ocean changes amounted to just 0.08 of a s.d. unit per century.

Adapted Image Source: McGregor et al., 2015
There are several reasons to question the presentation of data in  McGregor et al., 2015. – a global-scale reconstruction of sea surface temperatures.
The myriad authors decided not to clearly depict actual temperature changes in their reconstruction, preferring instead to “reimagine” temperatures as standard deviation units.
The graphical presentation of “standardized SST [sea surface temperature] s.d. units” abruptly and curiously stops in 1900. This unexplained truncation was used despite mentioning in the body of the paper that the 1900-2000 period had a “statistically significant” warming trend of (just) 0.08 s.d. units/century – half of the century-scale changes during 1200-1400 and 1400-1600 (0.17 and 0.18 s.d. units/century, respectively). Perhaps the insignificance of the post-1900 uptick wasn’t considered helpful to the AGW (anthropogenic global warming) narrative.
The graphs depicting no remarkable modern global ocean temperature changes (shown below), such as the ones with flatline trends from the 1860s to 2000, are buried in the supplemental information for the paper, making the data and graphs less accessible. One would think that the lack of any remarkable or anomalous global temperature changes occurring during modern times would deserve some scientific attention.
Finally, this study shows the Roman Warm Period and Medieval Warm Period were globally warmer than today at the ocean surface (with some location and timing differences). It also affirms the LIA was “globally coherent.” The authors even identify the mechanism for “robust” LIA cooling: “high frequency explosive volcanism” with centennial-scale impacts.
At least the latter point made its way into the paper’s abstract…rather than hidden or buried.

Image Source: McGregor et al., 2015 and supplementary data for the paper
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHat-tip: Die kalte Sonne
According to a new study, the expansion of offshore wind energy planned to date could lead to less electricity actually being produced at higher costs because, according to current planning, wind farms are taking the wind away from each other.
The researchers from the Technical University of Denmark in Roskilde and the Max Planck Institute for Biogeochemistry in Jena, Germany have investigated the topic. The study entitled “Making the Most of Offshore Wind” was commissioned by the Agora Energiewende and Agora Verkehrswende think tanks.
The report looks at that the question whether energy models used today by wind farm planners and investors can adequately capture the interaction effects between turbines stemming from very large areas covered with offshore wind farms at high installed capacity density.
Among the study’s key findings:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Offshore wind power needs sufficient space, as the full load operating time may otherwise shrink
from currently around 4,000 hours per year to between 3,000 and 3,300 hours. The more turbines
are installed in a region, the less efficient offshore wind production becomes due to a lack of wind
recovery. If Germany were to install 50 to 70 GW solely in the German Bight, the number of full-load
hours achieved by offshore wind farms would decrease considerably.”
Countries on the North and Baltic Seas should cooperate with a view to maximizing the wind yield
and full-load hours of their offshore wind farms. In order to maximize the efficiency and potential of
offshore wind, the planning and development of wind farms – as well as broader maritime spatial
planning – should be intelligently coordinated across national borders. This finding is relevant to
both the North and Baltic Seas. In addition, floating offshore wind farms could enable the creative
integration of deep waters into wind farm planning.”

Chart source: Study: “Making the Most of Offshore Wind“, Agora Energiewende and Agora Verkehrswende.
More unexpected costs, inefficiency
In a nutshell: a central pillar of the German and European transition to green energies threatens to become even more inefficient and more expensive than planned.


		jQuery(document).ready(function(){
			jQuery('#dd_75682c91c950f13c05f6eed99c42cb66').on('change', function() {
			  jQuery('#amount_75682c91c950f13c05f6eed99c42cb66').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I haven’t been following the debate over Sen. Dodd’s financial overhaul closely enough to have an opinion on the overall package, but Mike Masnick flags one aspect of the legislation that seems really troubling. Bob Litan explains:   




Under existing law, startup companies can raise money easily and quickly from “accredited investors” — individuals with substantial wealth or income. There is no need for the companies or the investors to gain approval from any state or regulatory official.   
  
  
All of this would change if Section 926 of the Dodd bill is included in any final reform legislation. That section would require, for the first time, companies seeking angel investment to make a filing with the Securities and Exchange Commission, which would have 120 days to review it. This would both raise the cost of seeking angels and delay the ability of companies to benefit from their funding.   
  
  
The negative impact of the SEC filing requirement would be aggravated by the proposed doubling of the net worth or income thresholds required for investors to be “accredited.” 



It’s hard to overstate how important a favorable regulatory climate is to the success of startups. Some of the most important startups have been founded by 20‐​somethings without the resources to hire lawyers or navigate regulatory bureaucracies. And startups frequently find themselves within weeks of insolvency before they have a big breakthrough. Having a crucial round of funding delayed by four months can be the difference between success and failure. If this description of the bill is accurate (and I have no reason to doubt that it is), this provision would be very bad for the future of high‐​tech innovation in the United States.
"
"
I found this over on Jerry Pournelle’s Chaos Manor. It seemed fitting given the discussion as of late. 
Some say the world will end in fire,
Some say in ice.
From what I’ve tasted of desire
I hold with those who favor fire.
But if it had to perish twice,
I think I know enough of hate
To say that for destruction ice
Is also great
And would suffice.
– Robert Frost


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0dde577',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Americans have a saying: “you break it, you own it”. In the world of climate diplomacy, the maxim is “you host it, you own it”. In Copenhagen in 2009, the Danes owned a disastrous climate event that broke up in rancour. In Paris in 2015, the French owned the successful agreement. The 2020 UN climate change conference, known as COP26, which will be taking place in Glasgow in November, could be Copenhagen or Paris, or somewhere in between. Ownership and responsibility will firmly rest with the government. The events of the past few days have proved beyond doubt, however, that we cannot leave it to the government alone. We all need to take ownership of this event.  In case you are not convinced, and feeling a bit inert, let me persuade you that this really matters: 2020 was the year nominated in the Paris agreement for when the world had to narrow the gap between its noble aspirations to limit global warming to 1.5C and its country-by-country commitments, which currently add up to, at best, 3C of warming. Given we only have a decade to decisively turn things around, we just cannot allow Glasgow to fail. This is a massive moment and the government has so far simply been unable to get to grips with the importance and the complexity of these negotiations, as illustrated by the fiasco over the appointment and sacking of Claire O’Neill, and the rather public – and so far fruitless – search for a president for COP26. However, it’s not just the government that needs to step up. Success or failure will in part depend on how much it is made to care about the issue. That depends on all of us. If the government thinks it can slink out of Glasgow without too much notice being taken, it may well decide other priorities are more important. We cannot let that happen. That means in practice, first, that the government must feel the pressure to act here at home. This summit is about persuading other countries to ratchet up their ambition. To succeed, we need to ratchet up ours and quickly. Britain has committed to net-zero carbon emissions by 2050 but currently our 2030 target does not reflect even that ambition. As we seek to persuade others to do more in the short term, we need a 2030 target of significantly greater ambition that puts us on track for net-zero by 2050. This more ambitious target must be accompanied by action in every sector, and a budget on 11 March that focuses on this issue, and includes a proper 10-year plan to decarbonise homes over the coming decade, while cutting bills and creating jobs. Second, crucial to the success of Glasgow is a strong alliance with the EU. Before Paris it was the US-China axis that made the crucial breakthrough. This time, with the US off the pitch under President Trump, the best hope is that the EU, which is responsible for 10% of global emissions, and China can act in concert to raise the level of ambition. Third, we have to recognise that every lever of government must be part of making this agreement happen, particularly around how public and private finance supports or thwarts the transition. That includes mobilising public and private finance for developing countries, ending international government support for fossil fuel extraction and building on the appointment of Mark Carney as the COP special envoy to ensure proper regulation of financial and corporate institutions around climate risk. Fourth, over and above actions by national governments, Glasgow must power forward coalitions of states, cities, businesses and civil society around the path to net-zero, including the phasing out of coal, and petrol and diesel vehicles, divestment from fossil fuels, and addressing deforestation. There are millions of people who care deeply about this issue in the UK and beyond. Extinction Rebellion and the pupil climate strikers have changed the terms of the debate over the past year. It’s up to all of us, including our brilliant NGOs, to ensure this moment sees the biggest mobilisation on the climate emergency in British history. You may wonder what the point of mobilisation is if the geopolitical forces are so badly aligned. The answer is there is no other option. The only way we will get change on this issue is showing government that there is a big political coalition that believes this really matters. Positive momentum from Glasgow is essential. We all need to step up. We all need to own it. Our future, and that of our children and grandchildren, is in the balance. Only by acting now can we prevent future disaster. • Ed Miliband is Labour MP for Doncaster North and a former climate change secretary"
"
As I mentioned in my post here about one of the satellite data sets (RSS) that showed a marked cooling globally in 2008, La Niña and PDO seem to be drivers of this change. Here is Joe D’Aleo’s take on it below. – Anthony
By Joseph D’Aleo, CCM ICECAP
Evidence is growing this La Niña will be a longer term event. Most similar important La Niñas are often multi year events (1949-1951,1954-1956, 1961-63, 1970-1972, 1973-1976, 1998-2001). Though the easternmost Pacific near South America has warmed at the surface as the seasonal weakening of the tropical easterlies led to weakened upwelling, it is still cold beneath. Below you can see the latest depth-section of ocean temperatures (top) and anomalies (bottom). Temperature are in degree Celsius. Note the large reservoir of subsurface anomalously cold water (up to 4 degrees C) in the eastern tropical Pacific at 50 to 100 meters.

 Also see the latest CPC depicted ocean heat content in the tropical Pacific. This shows the heat content remains at near maximum deficit levels.
 
These suggest as the easterlies increase again, cooling will return to the east Pacific and La Niña will persist at least well into 2008. The Pacific Decadal Oscillation (PDO) has dropped strongly negative (latest value from NCEP is -1.54 STD). This decline may represent another Great Pacific Climate Shift as the PDO warm and cold phases tend last 25 to 30 years and the last change , to a warm Pacific, occurred in 1976. See more in this pdf here.  If indeed the PDO shift is the real deal, we might expect more La Niñas and fewer weaker El Niños over the next few decades with a net tendency for cooling. Add to that a quieter sun and eventually a cooling Atlantic, and you have a recipe for global cooling.
However, this has its own drawbacks, La Ninas bring more drought and summer heat waves, landfalling hurricanes, large tornado outbreaks, spring floods, winter snows and cold outbreaks than their more famous counterpart, El Niño, which has dominated during the warm PDO era. A while back, Stan Changnon did an interesting analysis which I reported on recently here that suggests the era we have gone through since the late 1970s with dominant El Niños was unusually benign with more benefits than damages and will be looked on as the golden era, a modern climate optimum. Even if all this is correct, you might expect the media and enviro-alarmists ‘evidence’ we are affecting our climate to morph from warming and ice melt to the climate extremes characteristic of La Niñas.
See full pdf here. 
<!– –>


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0c25407',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Two weeks of international climate negotiations in Lima, Peru, are over, with an agreement pulled out of the bag at the eleventh hour. While Lima has been seen by many as a mere curtain-raiser to talks in Paris in a year’s time, when a new deal needs to reached to replace the Kyoto protocol, it will have an impact beyond this. Lima has reinforced the familiar battle ground between the developed and developing world, and it has seen the re-emergence of a key concept: climate justice. The idea of equity is at the heart of this – the question of how to ensure any UN-backed emissions deal is fair and that those countries that caused the problem do the most to clean it up. This had largely been ignored at previous summits but at Lima it was once again a big talking point. “If equity is in, we are out.” Those were the reported words of Todd Stern, the US chief negotiator, on the eve of the last day of Durban talks back in 2011, when the foundations for a new global agreement were laid. Stern was reacting to the clamour from developing countries that rich, developed nations should take the lead in making emission cuts under the principle of “common but differentiated responsibility and capability”, given their historical responsibility for climate change and their enhanced technological capabilities. While some observers were alarmed by Stern’s position, his words were a fair, if vulgar, rendition of the mind-set that is quite pervasive among many developed countries. Rich nations tend to prefer to wave aside or at least make light their moral responsibility to tackle climate change, while appealing for concerted action by “all parties”.  Pragmatism, realism, and “we are in this together” are some of the phrases used by developed countries as they try to duck their responsibility and cajole developing nations to instead step up their own climate actions. It was to this effect that many Western countries lined up behind the US in Durban.  Eventually all references to equity, justice and common but differentiated responsibility were expunged from the text. It was a short-lived victory.  Events in Lima over the past two weeks have overwhelmingly demonstrated the utter futility of developed countries’ schemes to diminish issues of equity and justice, let alone sidestep them altogether.  In virtually all the key issues and categories under discussion – countries’ mitigation contributions, states’ adaptation commitments, the remit of the loss and damage, and climate finance, among others – equity and differentiation have stood out as sticking points. For example, the G77 group of developing countries said that the principle of equity must guide all negotiations and long-term actions. Showing their heightened distrust in the progress, developing countries even requested that texts should be displayed on the big screen in real time while negotiating to enhance transparency. The harshest word for developed countries, however, came from Bolivian president Evo Morales, who referred to industrialised nations that have appropriated more than their own fair share of global atmospheric space as “thieves” that must be made to pay back what they have stolen. Of course, none of this implies that developing countries should be given an easy ride in negotiating the 2015 climate agreement, or that there are easy approaches to finding a “just” climate agreement. Climate justice is a deeply contested concept, open to multiple interpretations, recommending diverse and sometimes conflicting policy. For example, there are plausible justice-based arguments for allocating carbon emissions quota on individual (per capita) and on national (per country) basis.  However it appears that the Stern approach to international climate politics, seemingly without morality, is beginning to lose ground. If Lima has taught us anything, it is that humanity badly needs a dose of international respect if we are to avoid climate chaos. The brazen scheme to expunge equity from previous climate agreements by the US and its backers only served to further erode the mutual trust sorely needed to make compromises. Morality might be a dirty word in some states’ foreign policy handbooks. But call it what you like, the world needs to find its guiding principles quickly, and developing countries want rich nations to pay for what they’ve broken."
nan
"
Share this...FacebookTwitterThe German Readers Edition reports that 3 leading scientists, among them alarmist Stefan Rahmstorf, are calling on Rajendra Pachauri to step down as Chairman of the IPCC because of management errors and the recent attacks on the IPCC and climate science.According to Stefan Rahmstorf’s blog Klimalounge:
I’m not calling for an end of Pachauri, but I could certainly imagine a better Chairman because in my view, among other reasons, he reacted in an unfortunate manner with respect to the media attacks on the IPCC. The role of the Chairman is not to decide the contents of the report (he should not get involved with our work). Rather he ought to well represent the IPCC externally.
Calls for Pachauri’s resignation are nothing new. In February director emeritus of the Max Planck Institute for Meteorology in Hamburg, Prof. Hartmut Graßl, told the Frankfurter Rundschau newspaper that Pachauri should clear the table and leave the job in other hands.
Hans von Storch, director of the GKSS coastal research center in Geesthacht, Germany, said the IPCC director was a burden because he permitted sloppiness in the reviews and checks of the 2007 climate report.
Readers Edition quotes the current issue of zeo2 titled: The zeo2 Climate Summit, which states:
“Pachauri should throw in the towel.”
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEvidence that temperature swings of ±17°C occurred during the end-Triassic mass extinction event imply that CO2 would have needed to increase 8- to 1,024-fold (3 to 10 doublings) to have induced that magnitude of temperature change. It didn’t.
Evidence from a new study (Petryshyn et al., 2020) suggests “repeated” temperature swings of 16-17°C occurred in Cotham Marble (CM, southwest United Kingdom) at the end of the Triassic epoch, when the worst extinction event of the last 500 million years occurred.
However, Petryshyn and colleagues acknowledge they “cannot resolve millennial-scale increases in temperatures in the region, implying that, at least locally, the initial extinction is not attributable to extreme warming.”
But even if the end-Triassic mass extinction (ETME) could be attributable to extreme warming, CO2 would be “implausible” as a mechanism.
According to models, CO2 increases up to 8 times the pre-industrial baseline (280 ppm) could only increase sea surface temperatures 5.4°C at most (Petryshyn et al., 2020). CO2 concentrations would need to increase up to 1,024-fold to elicit temperature changes reaching 16 or 17°C.
Therefore, “the initial onset of the biodiversity crisis may necessitate another mechanism.”

Image Source: Petryshyn et al., 2020
Share this...FacebookTwitter "
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
One of the great things about our current state of technology is the nearly instant reporting we can get from remote sensing platforms. Thanks to  Dr. Roy Spencer & Dr. Danny Braswell, GHCC at the University of Alabama, Hunsville, we can watch global temperatures of the lower troposphere in near real-time at this page:
http://discover.itsc.uah.edu/amsutemps/
According to UAH: Daily averaged temperatures of the Earth are measured by the AMSU flying on the NOAA-15 satellite. The satellite passes over most points on the Earth twice per day, at about 7:30 am and 7:30 pm local time. The AMSU measures the average temperature of the atmosphere in different layers from the surface up to about 135,000 feet or 41 kilometers. During global warming, the atmosphere near the surface is supposed to warm at least as fast as the surface warms, while the upper layers are supposed to cool much faster than the surface warms.
But as I understand it, the lower troposphere is supposed to be closely coupled to CO2 induced forcings. As we’ve seen from comparison to surface data sets such as HadCRUT, the UAH MSU lower troposphere tracks fairly well with surface temps.
You can learn more about how the Advanced Microwave Sounder Unit on NOAA-15 works and what coverage it has here at my post on it the instrument.
According to the UAH data For 2008, we are averaging about .4 to .5 degrees C cooler than last year. See the graph and click it for a larger one:

Click for larger graph
This tracks with some of the anecdotal eveidence we’ve been seeing in the weather in the northen hemisphere this spring, with late snowfalls, late frosts, and below normal temperatures. The northern latitude areas such as Canada have been very slow to have a spring season.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ee7768e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"An experiment in liberalising power markets has been underway in the UK since the 1980s and three phases can be identified. The first ran from around 1989 to 1999, beginning with the privatisation of the generating industry and grid and ending by giving customers the freedom to shop around for their supplier.   When liberalisation proper started in 1999 the system was well supplied – even oversupplied – with generating capacity. So the next stage was one of cut-throat competition for market share. This led to a collapse in the wholesale price of power. Another decade on and new problems emerged as power plants approached the end of their lives. Competitive markets are efficient at getting best value out of existing infrastructure but much less so at deciding when and how it should be replaced – especially when there is uncertainty about how much business coal and gas-fired plants will secure in an era where there are green energy targets to meet.  The problem is that renewable power is not reliable enough to supply all demand, and is often least reliable in the middle of winter when most power is needed – the UK needs about three times as much electricity on a late January afternoon as on an early July morning. This means that to have security of supply, someone needs to build coal/gas/nuclear plants that offer enough reliable capacity to meet peak demand while knowing that for much of the year they won’t have a market at all.  On top of this, the economics of building new carbon-free power are very different from carbon-emitting power. Coal and gas-fired plants tend to be quicker to build and have relatively low capital costs – certainly in the case of the combined cycle gas turbine (CCGT) – but they are expensive to run. Markets prefer this: investors get their money back quite quickly and if the gas price surges, consumers have little choice but to pay the higher power prices that result.   By contrast nuclear power and renewables are expensive and slower to build though they use little or no fuel. Private investors find their capital tied up for longer periods of time without a cash flow. If the project runs over cost and time, as has been seen recently with a number of offshore wind farms, CCGT companies can pick up the business by having a new plant up and running in a couple of years. So governments in the UK and elsewhere have faced real challenges. Do they stick to the market mantra, knowing that to do so they will need to transfer large portions of the risk associated with nuclear and renewables onto the consumer to prevent all investment going into CCGT? Or do they unequivocally renationalise the responsibility for plant mix (while still supporting a competitive market in operation)? So far the answer has been the former. To persuade companies to build renewables, they are not expected to bear the costs of the hugely expanded grid necessary to support their output. When there is too much wind or solar being produced, threatening melting the wires or blowing electronic equipment, renewable generators also get paid to shut their plants down, a benefit not extended to any other players. On top of these enormous hidden subsidies, they also get guaranteed wholesale prices through the “Contracts for Difference” system – a subsidy from which nuclear benefit too.   Crunch point may be coming. Mitigating climate change is coming under threat from an alliance of Big Green and Big Sceptic. Both broadly agree that global warming is probably happening (Big Sceptic less enthusiastically than Big Green but there are very few who do not accept that carbon dioxide is a greenhouse gas). But in practice both argue that the costs of mitigating it are too high –- Big Sceptic focuses on the financial costs, while Big Green frets about nuclear’s environmental costs. (The complete lack of any criticism of increased greenhouse gas emissions in Japan and Germany as they shun nuclear power for purely political reasons is highly illuminating.)  If the fight against greenhouse gas emissions is abandoned under this twin attack from Nigel Lawson and the Greens, all bets are off. And don’t expect an explicit decision so much as a failure to change at the rate needed to meet the very long-term carbon reduction targets. A fracking revolution in Europe like the US one could then reduce dependence on Russia and Iran enough to make a second dash for gas (dwarfing the first) look acceptable.  On the other hand, if the concerns about security of supply and carbon emissions persist then nuclear power is in effect the only source which is both reliable and low carbon. (Two others come close – large dam hydro, which is not quite secure, and biofuels, which are not quite low carbon.)   This means that at least as far as the irreducible 20,000MW of power demand that exists throughout the year is concerned, nuclear is the obvious choice on economic grounds when all costs are concerned. It retains considerable support among British people – even more than before Fukushima, as people realise that in unimaginably stressful circumstance even 1970s nuclear technology did not release enough material to cause detectable health problems. The inevitable waste legacy from the experimental days in the decade or two after fission was discovered will be expensive to resolve but new plants have learned those lessons and volumes of waste will be much lower. Two questions remain though – can the industry deliver to time and cost, and will the government abandon its attempts to persuade investors to carry out public policy at private sector rates of return and instead resume responsibility for the plant mix, allowing it to be carried out at public sector rates of return and slashing the cost to consumers? A yes to both would revolutionise nuclear power’s prospects and return it to its position as the only major technology that can be brought on line quickly. At this uncertain stage, the UK government’s deal over Hinkley Point C and its preliminary agreements over two other new nuclear builds are the only sensible course of action.   To get an alternative viewpoint on nuclear power, now read this piece."
"In recent years, there seems to have been a rise in the extreme weather all over the world from terrible flooding in Bangladesh and Pakistan, the record cold snap in North America, to one of the wettest winters on record in the UK. Extreme events are very difficult to tackle, and in some cases there is little we can do, other than increase our preparedness and our recovery response. However there is one thing we can do in response to smaller scale, more common events such as flooding from intensive rain showers. As winter closes in on, it’s worth looking at some of the ways we can better manage excess water. People have tackled floods for centuries, but modern urban development has thrown up a set of new challenges. The more we develop the landscape, the more rainwater stays on the surface rather than sinking into the soil. That means water gets onto the roads and into drains more quickly, bypassing some aspects of the natural hydrological cycle. Rooftop plants (including green-roofs and roofgardens) along with rainwater collectors and rain gardens can help slow things down and spread the impact of heavy rain out over a longer period. The idea is to replace some of the trees, grass, hollows and wetlands that have been lost to concrete, and so mimic a more natural flow of water. One approach that aims to manage rainwater more naturally is known as Sustainable Urban Drainage Systems (SUDS). SUDS are particularly useful in helping to manage small but frequent floods from rainfall, just as the un-urbanised landscape would. The system has three main aims: to catch and slow down the flow of water; to improve the quality of water by capturing and treating the pollutants it contains; and to benefit the local community by providing a green space that people can enjoy and where wildlife can flourish. Managing water where it lands – at its source – is one of the most effective ways of reducing runoff. We can do this by creating green roofs and raingardens, for example, as well as by designing other mechanisms that slow down the flow of rainwater from our roads, roofs and driveways. These features are often linked to ponds, wetlands and temporary storage areas (called basins) which can hold and treat more water from a larger area. Individual roof gardens, water collectors or raingardens may seem small-scale, but if many of these features are installed in an urban area and they also link up to a larger pond or wetland they can have a huge impact on slowing the flow, cleaning water of some of the pollutants it carries and also providing habitat, attractive settings and helping to recharge our streams and groundwater supplies in a more natural way. Sustainable urban drainage is already being used to great effect in Portland, Oregon, where 3,500 trees have saved the city US$63m in pipe replacement, and in Malmö Sweden, where 6km of water channels and ten retention pools helped an area that previously experienced chronic flooding. In Dunfermline, Scotland, sustainable drainage is being added to a greenfield pre-development. Sometimes it rains so heavily that no sensible drainage system could handle the flow. Where serious and sustained flooding is caused by unprecedented rain – as happened in the UK last winter – it has to be said that these small-scale systems are less effective. Sustainable drainage measures would probably have helped initially, but no form of water management could have accommodated rainfall on that scale.  Record-breaking rainfalls combined with high tides and high water tables presented us with a perfect combination of conditions that any traditional engineering would have been hard pressed to tackle. In these cases, we need to think about how to be more resilient to floods – how we can be more prepared, reduce the impact of the flood, and recover more quickly. Researchers in the UK have been working on this for some time now, and in some cases such approaches have become law – in Scotland, for instance, all new developments since 2006 must have sustainable drainage.  SUDS can deliver sustainable solutions to our urban water management problems. They can give us healthier urban catchments, more livable neighbourhoods, and cleaner rivers and streams. And who doesn’t love a rooftop garden?"
"

 **A Week at Cato University**



While many people took their summer vacations getting away from it all, more than 200 professionals, business people, college students, and retirees spent a week in early August exploring the ideas of liberty at the Cato University Summer Seminar. Held August 1–7 at Rancho Bernardo Inn, about 20 miles from San Diego, the program featured lectures and discussions on American history, law, economics, psychology, philosophy, and public policy.



In announcing the August seminar, Cato University director Tom G. Palmer said, “This program gives you the chance to recapture the intense intellectual atmosphere of your college days, in a climate where the lecturers and other participants share your fundamental ideas about freedom and justice. The schedule of lectures and discussions is designed to impart a great deal of information and analysis and encourage spirited discussion about the implications of the basic ideas.”



The faculty included some of the country’s most spirited and brightest defenders of liberty. Alan Charles Kors, professor of history at the University of Pennsylvania and coauthor of _The Shadow University_ , discussed the roots of liberty and the state of academia today. Randy Barnett, professor of law at Boston University and author of _The Structure of Liberty_ , gave a sneak preview of his forthcoming book during his talk “The Constitutional Presumption of Liberty.” (Excerpts of Barnett’s remarks are available on the September edition of CatoAudio.) Don Boudreaux, president of the Foundation for Economic Education, discussed the economics of law and the illogic of politics. Historian Paula Baker of the University of Pittsburgh discussed the growth of the American welfare state and American liberty in the 19th and 20th centuries. Guest lecturers included psychologist Nathaniel Branden, author of _Taking Responsibility_ , who examined liberty and responsibility from a psychological viewpoint, and philosopher Christina Hoff Sommers, author of _Who Stole Feminism?_ who discussed the way America has become “the republic of feelings.” Cato’s Edward H. Crane, David Boaz, Ted Galen Carpenter, Robert A. Levy, and Tom Palmer also spoke.



The attendees heaped glowing praise on the summer seminar. “The Cato University Summer Seminar was one of the most intellectually exciting times of my life,” said Kyle Larsen of Valrico, Florida. “All of the speakers were engaging and entertaining, and the friendships I have built with some of the fellow participants have far outlasted the week of the conference.”



“Excellent organization and production,” said Lyn Weingarten of Austin, Texas. “Talks were the right length with plenty of time for clarification and discussion.”



“Overall we enjoyed the week immensely, felt uplifted and educated, and are mulling over how much we can increase our annual Cato donation,” said David and Shirley Gilbreath of St. George, Utah.



Scholarships from the Opportunity Foundation allowed 40 students to participate in the program.



The Cato University program also includes a separate 12‐​month home‐​study course that uses audiotapes, books, and an integrated study guide. Another week long seminar and a weekend seminar will be held in 2000. More information about Cato University is available on the Cato University Web site.



 _This article originally appeared in the November/​December 1999 edition of_ Cato Policy Report.



 _This article originally appeared in the November/​December 1999 edition of_ Cato Policy Report.
"
"Community group members and public health professionals have fought back tears while calling on the New South Wales government to drop “anti-climate” legislation that would limit planning authorities’ ability to block fossil fuel developments. Several witnesses became emotional while giving evidence to a parliamentary hearing into the proposed laws, which are designed to stop planning authorities from rejecting or imposing conditions on projects based on their impacts overseas, including overseas emissions.  It is in part focused on scope 3 emissions, which are emissions that occur after coal or gas is sold into the market and burned. Because Australia exports much of these resources, many of these emissions occur overseas. The NSW Minerals Council lobbied the state government in 2019 to change laws that require the Independent Planning Commission to consider these emissions when assessing a project. It followed the NSW land and environment court’s rejection of the Rocky Hill coalmine in February, which cited the impact the mine would have on climate change, including through the burning of coal in other countries, at a time when “a rapid and deep decrease” in global emissions was urgently needed. Two other decisions by the IPC also cited the impact of climate change in their reasons for imposing conditions on a mine or rejecting it entirely. But the amendments now before the NSW parliament were drafted before the unprecedented fires that have affected much of the country. On Thursday, multiple speakers choked back tears as they spoke of the impact the bill would have at a time when Australians were living the reality of the climate crisis. “What I find so insulting, so wilfully ignorant that it leaves me white hot with anger, is that this government and some within its departments are so easily bullied by the fossil fuel lobby to even think of introducing an anti-climate bill while Australia is burning,” Julie Lyford told the hearing. Lyford is the president of Groundswell Gloucester, which argued in the Rocky Hill case that the mine would have a detrimental impact on climate change and the social fabric of the town. “The proponents of this bill are climate criminals. They will be held to account one day if this bill goes through,” she said. Dianne Montague, a member of Groundswell Gloucester, told the hearing rural communities disproportionately bore the impacts of mining and were now also “the ones bearing the burden of the impacts of climate change”. She said the community had been living with smoke from nearby fires for months. “Depression is everywhere. Everybody you talk to is depressed because of what’s happening,” she said. Doctors told the hearing that the bill was a step backward in planning for health risks from fires and other extreme events associated with climate change. Ingrid Johnston, a senior policy officer at the Public Health Association of Australia, told the hearing that the effects of global heating crossed borders. To demonstrate this, she spoke of putting a gas mask on her 11-year-old son because of severe smoke that settled over Canberra in January from fires burning in NSW. “The last couple of months have demonstrated all too clearly that bushfire smoke doesn’t respect borders. Just ask people in the west coast of New Zealand,” she said. Other speakers identified major concerns with the wording of the bill, telling the committee it could have unintended consequences. This included the NSW Minerals Council, which has been pushing for these changes. The council’s policy director, Andrew Abbey, said the proposed amendments could potentially lead to a greater risk that developments would be refused outright. “In the sense that if a project couldn’t get over the line because you can’t impose a condition on it related to scope 3 or downstream emissions the view could be formed that you subsequently don’t approve the project,” he told the hearing. Both the council and the CFMMEU said they supported the “intent” of the bill but were not sure if it achieved its stated purpose. And the Environmental Defenders Office, which acted for Groundswell Gloucester in the Rocky Hill Case, said the bill could limit planning authorities’ ability to consider all emissions associated with projects, including emissions that occur in Australia. “In terms of legal drafting, this is a mess,” the principal lawyer Elaine Johnson said after the hearing. “It’s almost impossible to determine how it would be applied.”"
nan
"





There will be a story featuring Al Gore and his climate views on CBS 60 minutes this weekend.  Normally I don’t pay much heed to this program, but Gore is publicly calling those who question the science “…almost like the ones who still believe that the moon landing was staged in a movie lot in Arizona and those who believe the world is flat…”.
To me, a person who has at one time been fully engaged in the belief that CO2 was indeed the root cause of the global warming problem, I find Gore’s statements insulting. In 1990 after hearing what James Hansen and others had to say, I helped to arrange a national education campaign for TV meteorologists nationwide (ironically with CBS’s help) on the value of planting trees to combat the CO2 issue. I later changed my thinking when I learned more about the science involved and found it to be lacking.

I’ve never made a call to action on media reporting before on this blog, but this cannot go unchallenged.
The press release from CBS on the upcoming story on Gore is below. You can visit the CBS website here and post comments:
http://www.cbsnews.com/stories/2008/03/27/60minutes/main3974389.shtml
See the video clip here
But let’s also let the producer, Richard Bonin,  know (via their communications contact) what you think about it, as I did when Scott Pelley aired a whole hour long special telling us Antarctica was melting. They did no follow up.

Kevin Tedesco KEV@cbsnews.com
Director, CBS News Communications (”60 Minutes”)



That email is listed on the CBS website, so it is fair to send comments to it. In fact, here is a contact list they have on their website where you can comment about this story. I feel it is important to respond and to spread the word to others. While I have not seen the video segment, let us hope that it has some semblance of balance, because the press release certainly does not.






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea031f862',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A dramatically-named “weather bomb” exploded across the UK in the past week, bringing winds gusting up to 144mph on outlying islands. But despite the cool name these “bombs” are more common than you might think. The UK’s forecasting agency the Met Office defines a weather bomb as an intense low-pressure system with an atmospheric pressure in its core that drops at least 24 millibars in 24 hours. Their charts indeed show an impressive low-pressure system tracking slowly eastwards between southern Greenland and Iceland during 8 and 9 December.  The more precise meteorological term is “explosive cyclogenesis”. It occurs when a low-pressure system is subject to sudden, explosive intensification, typically after an injection of cold, dry air from the stratosphere. This doesn’t normally happen: the stratosphere lies above an altitude of 10 to 15 kilometres and the air up there tends to stay there. This air from above bumps into the warmer, wetter air below which has the effect of reinvigorating and spinning up the low-pressure system. As air in the centre of the system rises ever more rapidly, the ground-level air pressure drops sharply. More air rushes in to fill the space, meaning very strong winds. Bombs are likely to be triggered by changes in narrow, fast-flowing currents of air high above the Earth’s surface known as jet streams. The North Atlantic polar front jet stream is a key player in an ever-present but varying meteorological battleground where warm, moist tropical air masses meet cold polar air from further north. This week the jet stream has been very vigorous with core speeds of up to 200 mph, about twice as fast as normal. These “weather bombs” are not unusual features of the UK’s autumns and winters. The British Isles do after all lie in the path of strong prevailing westerly winds and are often near the confluence of tropical and polar air masses.  Take last winter, for example. In the UK this was one of the stormiest seasons on records going back for well over a century, after a series of more than half a dozen major storm systems ran across the country in quick succession. Indeed during one of these storms Stornoway on the Isle of Lewis in Scotland’s Outer Hebrides saw one of the lowest sea-level air pressures on record.  Then there was the October 1987 storm that caused 18 deaths, £300m worth of damage (at 1987 prices), felled 15 million trees and was responsible for the first major blackout in London since the Blitz. That storm was caused by another weather bomb, in the Bay of Biscay off the west coast of France. The barometric pressure change of 25.5 millibars in three hours – indicative of exceptionally strong winds – recorded at a navy base on the south coast of England was easily the greatest on record for the British Isles. But 1987 wasn’t the UK’s worst storm ever. The November 1703 storm, famous today for the author Daniel Defoe’s pioneering journalistic account, left an unprecedented trail of destruction in its wake. It arrived with minimal warning and killed somewhere in the region of 10,000 people.  So the recent low-pressure system – the bomb – has certainly led to some spectacularly high waves and wind speeds but it is not an untypical intense winter low pressure system; given its position in the extreme northern Atlantic around a favoured spot where cyclones form, it does not seem very unusual from a climatological perspective.  Famously, the Met Office largely failed to forecast the 1987 storm but these days it is much better at making short-term weather predictions of up to a week ahead. Provided we listen to the Met’s severe weather warning system  I don’t think we should be unduly scared by “weather bombs”. In a maritime climate they are just a normal part of the autumn and winter weather experience.  Of course climate change could mean more such bombs in future – after all, it seems to be making many weather extremes more common. However, these storms are formed and controlled by the polar jet stream and models of the climate over the next century aren’t too clear on what will happen to the jet stream. Even reconstructions of past storms based on wind speed and pressure records show lots of fluctuations but no clear, consistent trend. Unlike steadily rising global temperatures, we can’t say for sure that these storms will increase. The IPCC cites “substantial uncertainty and thus low confidence” in projecting changes in North Atlantic storms. We simply don’t yet know whether global warming will necessarily mean more storms and “weather bombs” over the British Isles."
"

Some things are sacred to scientists: Facts, data, quantitative analysis, and _Nature_ magazine, long recognized as the world’s most prestigious science periodical. 



Lately, many have begun to wonder if Jayson Blair has a new job as their science editor. On page 616 of the April 8 issue, _Nature_ published an article using a technique that they said, on page 593 _of the same issue,_ was “oversold”, was inappropriately influencing policymakers, and was “misunderstood by those in search of immediate results.” 



The technique is called “regional climate modeling,” which attempts to simulate the effects of global warming over areas the size of, say, the United States. 



As reported by Quirin Schiermeier, scientists at a Lund, Sweden climate conference, “admitted privately that the immediate benefits of regional climate modeling have been oversold in exercises such as the Clinton administration’s US regional climate assessment, which sought to evaluate the impact of climate change on each part of the country.” 



Then, 23 pages later, _Nature_ published an alarming and completely misleading article predicting the melting of the entire Greenland ice cap in 1,000 years, thanks to pernicious human economic activity, i.e., global warming, using a regional climate projection. 



The lower 48 states comprise 2 percent of the globe. Schiermeier reported that the consensus of scientists is that climate models on such a small scale are inappropriate for policy purposes. Greenland covers 0.4 percent of the planet. If the models are no good over the U.S., they’re worse over Greenland. Yet the authors “conclude that the Greenland ice‐​sheet is likely to be eliminated by anthropogenic climate change unless much more substantial emission reductions are made than those envisaged by the IPCC [a United Nations Panel].” 



The Greenland paper, by Jonathan Gregory and two others, was profoundly misleading, offering any climate alarmist an incredible sound bite attributable to our most prestigious science publication. 



The first paragraph states: “The Greenland ice‐​sheet would melt…if the annual average temperature in Greenland increases by more than 3°C [5.4°F]. This could raise global average sea‐​level by 7 meters [23 feet] over a period of 1,000 years or more.” 



Guaranteed, that quote will be on _Hardball_ on May 28, the day that the non‐​science fiction global warming flick, _The Day After Tomorrow_ comes out. It’s ironclad. After all, it’s from _Nature._



And it’s also deceptive. It’s not a warming of 5.4°F that causes the massive meltdown. Instead, it’s an annual warming of an impossible 14°F. Given the way greenhouse warming splits between summer and winter, this implies an outlandish 30°F change in the winter, fueled by a world that would have to be producing carbon dioxide at a rate far beyond anything remotely possible. It is the most extreme scenario in a pack of outlandish future emission scenarios that the U.N. cooked up a few years ago. They actually call them “storylines,” which is appropriate, since they make little sense. 



For example, one of the major storylines assumes that people increasingly favor personal wealth over environmental protection, which is absurd. The richer a nation is, the richer a city is, or the more affluent a neighborhood is, the more it protects its environment. 



How did the first paragraph get by the editors at _Nature?_ Either they weren’t looking or they thought it was OK. Take your pick. 



It’s not the first time, either. Just as scientists “admitted privately” that the models don’t work, so have prestigious environmental journalists told me privately that they are concerned about _Nature’_ s handling of global warming stories, both in terms of increasingly shoddy reviews and timing clearly designed to influence policy. No one has forgotten that in 1996 _Nature_ featured a paper, right before the most important U.N. conference leading to the Kyoto protocol, “proving” that models forecasting disastrous warming were right. The paper was subsequently found to have used data selectively to generate its dire result. 



Note to _Nature:_ Even journalists, normally your friends on global warming, are getting suspicious. 



The Greenland paper is truly an exercise in virtual reality. The threshold for melting is based upon a uniform annual temperature rise. But, every scientist knows that greenhouse effect warming is much greater in winter, when the authors say “no melting takes place.” When they account for this (not reporting how they did so), fully one‐​third of their scenarios fall below the melt threshold. Only when they assume what is patently untrue do almost all the scenarios result in a net melting, and only the most extreme, illogical ones completely melt things. 



This is nothing but tragic, junk science, published by what is (formerly?) the most prestigious science periodical in the world. There’s been a lot of hype‐​much of it from scientists themselves‐​over global warming, but nothing as sacrilegious as this, in such a sacred place. 
"
"
Share this...FacebookTwitterPrior to the transition from the last ice age to the current interglacial climate, when CO2 levels still lingered below 250 ppm, the relative sea levels in southern Greenland were “at least ∼32 m above present.”
Relative sea levels have undergone a series of major changes since the last glacial maximum, when global sea levels were 120 meters below today’s.
Sea levels rose at rates of up to 60 or 70 millmeters per year (6 to 7 meters per century, Tanabe, 2020) from about 12,000 to 8,000 years ago. Most of the globe experienced sea level high stands of 2 or 3 meters above present between about 7,000 to 5,000 years ago (King et al., 2020, Lopes et al., 2020, Martins et al., 2020).
But a new study (Steffen et al., 2020) proposes relative sea levels instead peaked at 32 meters above today’s levels in Nanotalik (southern Greenland) during the latter stages of the last ice age (13,800 years ago).

Image Source: Steffen et al., 2020

Image Source: Tanabe, 2020

Image Source: King et al., 2020

Image Source: Lopes et al., 2020

Image Source: Martins et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_0e946332d5df286d97dcf4a1c7d65536').on('change', function() {
			  jQuery('#amount_0e946332d5df286d97dcf4a1c7d65536').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Matt Ridley’s new book, _The Rational Optimist: How Prosperity Evolves_ , is garnering rave reviews. Ridley, science writer and popularizer of evolutionary psychology, shows how it was trade and specialization of labor–and the resulting massive growth in technological sophistication–that hauled humanity from its impoverished past to its comparatively rich present. These trends will continue, he argues, and will solve many of today’s most pressing problems, from the spread of disease to the threat of climate change.   
  
  
The Cato Institute has now presented three different looks at the book, with a review in the _Cato Journal_ , another in _Regulation_ , and an event at Cato with Matt Ridley himself.   
  
  
**Powell on Ridley**   
  
  
My colleague Aaron Powell published the first Cato review of _The Rational Optimist_ with a piece in the Fall 2010 edition of the _Cato Journal_ (pdf). 



What _The Rational Optimist_ makes clear, in perspicuous prose and enchanting storytelling, is that, just as biological evolution populated the world with the wondrous variety of life, exchange allowed one of those species to achieve a wondrous standard of living that will only improve and become more uniform as we trade and invent.



Powell doesn’t find the book flawless, however. He identifies two problems that weaken Ridley’s argument, the first dealing with the “circular and unconvincing” nature of his claim that trade caused our human ancestors to achieve humanity. The second concern is broader. Powell writes, 



It would be easy to get the impression Ridley is Pollyannaish. If nuclear annihilation, super flus, and starvation are nothing to be worried about, what possibly could be? Unfortunately, Ridley’s response to this critique is less convincing than it could be, for he fails to adequately draw a line between when an anticipated disaster is real and when it’s just pessimism writ large.



 **Henderson on Ridley**   
  
  
David R. Henderson reviews the book in the latest issue of _Regulation_ (pdf). Like Powell, Henderson enthusiastically endorses the style and substance of Ridley’s book, though without identifying the weaknesses highlighted in the former review. His only point of contention with _The Rational Optimist_ is a “jarring misstatement” regarding trade and value. Henderson writes, 



Given the important role of trade in Ridley’s theory, and given his obvious understanding of trade, it is surprising that he makes a jarring misstatement: “For barter to work,” he writes, “two individuals do not need to offer things of equal value. Trade is often unequal, but still benefits both sides.” The correct statement is: “For barter or trade to work, individuals _must_ offer things of _unequal_ value.” If I valued what I give up the same as what I get in return, there would be no point in trading. Trading is _always_ an exchange of unequal values.



Henderson goes on to defend Ridley against the negative appraisal his book received in the _New York Times_. That review, written by famous foreign‐​aid critic William Easterly, attacked _The Rational Optimist_ for its take on Africa and for failing to “confront[] honestly all the doubts about the ‘free market.’ ” “Really?” Henderson responds. “All the doubts? I do not know if such a book could be written with the requisite amount of evidence and have under 3,000 pages.”   
  
  
**Ridley on Ridley**   
  
  
And then, of course, there’s the source himself. In May, Ridley spoke at a Cato Institute book forum about _The Rational Optimist_. He discussed the core arguments of his book and concluded (optimistically) that technology and trade have now made it possible to stop trying to keep the world from getting worse, and instead focus on making it better.   
  
  
As with all Cato events, full video and audio are available for download on www​.cato​.org. Or watch it right here:   

"
"

The main political conflict in recent years is between experts or elites and non‐​experts. For lack of a better word, the non‐​experts are called populists. Their complaints have been specific: Elites and experts are arrogant, they have different values, they condescend in annoying ways, they ignore the sometimes legitimate concerns of populists, among others. Experts say that they should be listened to because they’re more knowledgeable. We see it in debates on every issue from climate change to trade, immigration, and everything in between.



The COVID-19 pandemic exposes another criticism of experts: They lie with noble intentions. And the consequences of those noble lies are quite negative.



A recent _New York Times_ __ op‐​ed by Zeynep Tufekci exposes the danger of noble lies when it comes to limiting the transmission of COVID-19. She details the claims by public officials and health experts that masks don’t limit transmission. She wrote:



Many health experts, no doubt motivated by the sensible and urgent aim of preserving the remaining masks for health care workers, started telling people that they didn’t need masks or that they wouldn’t know how to wear them.



Those claims were simply untrue. Yes, healthcare workers need masks, but masks (even those that are homemade) also reduce transmission outside of hospitals and clinics. Sick people who wear masks reduce their likelihood of transmitting the virus and healthy people who wear them reduce their likelihood of becoming infected. Tufecki pointed out the obvious contradiction: If masks don’t work, why do healthcare workers need them?



Noble lies are those knowingly propagated by elites or experts to advance a bigger agenda. I can’t think of a single noble lie that has led to better outcomes and most have done more harm than good. The arguments against mass use of face masks were noble lies intended for the good reason of attempting to reduce the mass consumption of face masks to conserve them for healthcare workers. However, they backfired quickly. Ultimately, that failure will cause even more harm down the line.



One source of harm is how social enforcers of new anti‐​COVID‐​19 norms respond. Enforcing these norms through pressure to not gather in large crowds, proper hand hygiene, to maintain social distance, and to stop shaking hands is positive. Those social enforcement mechanisms work best when everybody is basically on the same page about what works but they follow the norms to varying degrees. But if lots of people don’t trust the advice and they disagree about proper methods to limit the transmission of the disease because they’ve been misled by noble lies, social pressure will be contradictory and less effective at altering behavior.



Health experts, epidemiologists, medical researchers, scientists, and other experts have knowledge and experience that is valuable in containing COVID-19 and eventually wiping it out. They will eventually discover a vaccine and treatments that will benefit all of us. But without widespread trust in them, their jobs will be harder. Noble lies will reduce that trust and make it less likely that people will heed their advice and warnings. If some percentage of their guidance is a lie and we all know that they are sometimes lying, people will be less likely to listen or will cherry‐​pick which advice to follow. People will be more likely to consume snake oil, listen to grifters, and fall back on prejudices or other biases that will end up hurting themselves and others. And this will all happen rapidly in the current media market where information is cheap and available at a cost near zero, as it currently is.



Even worse, the noble lie does serious damage to expert culture as one noble lie can justify more lies that are increasingly less noble. Experts will justify less‐​noble lies on the precedence of previous lies that were nobler with no natural limiting principle. And they judge the nobility of the lie by the intent of the liar, which is a dangerous trap. This cycle can only destroy expert credibility.



A common justification for the noble lie is that people aren’t taking the current COVID-19 crisis seriously enough, so experts are justified in trying to “scare people straight” with a lie. The major problem, if the goal is to change other people’s behavior with additional information, is that they won’t be scared straight as soon as the lie is known. Thus, the noble lie will backfire.



Scaring people straight works better when scary truths are revealed rather than when lies are peddled. Emily Oster, economist and author of two superb books on pregnancy risks and raising young children, points out this problem in another area of medicine: alcohol consumption by pregnant mothers. She highlights a report published by the American Academy of Pediatrics with the headline finding that “no amount of alcohol should be considered safe in pregnancy.” Oster points out that the report itself contradicts that statement. She further details to another problem:



Reasonable people can differ, but when we lump together all levels of drinking—without really clearly focusing on what we should be concerned about—we risk losing sight of the groups that actually need help.



Heavy drinking during pregnancy is a big risk, but exaggerating it means that the public can lose sight of the people most negatively affected. Perhaps some pregnant mothers can’t limit themselves to a small amount of alcohol so, for them, the better advice is not to drink at all, but that does not translate into a warning that no expecting mothers should imbibe ever. Exaggeration could mute the actual message: Drinking a lot while pregnant can do serious, permanent harm to your baby.



Experts and elites are more trusted when they tell the truth and expose non‐​obvious tradeoffs. Every action has tradeoffs, even those that are obviously a net‐​benefit. For instance, arguing in favor of lockdowns, quarantines, and travel restrictions while acknowledging that those actions will severely disrupt economic activities and lead to other, different health problems and early deaths. Those extra health problems and deaths may be worth it, but being open about that tradeoff and making the case honestly is the best that experts can do.



This doesn’t mean that experts should consider every non‐​expert objection and weigh them equally when considering a response. Anti‐​vaxxers can be safely ignored during the COVID-19 crisis, for instance. But it does mean that experts need to present the facts honestly and openly. Populists may not believe them, but it’s better to make an honest case for an action that isn’t believed than it is to make a dishonest case that is later exposed as the long‐​term costs in lost credibility are high. The present value of trust in experts is too valuable to be squandered on an ephemeral change in behavior bought at the expense of a lie.



As a libertarian, my preference is for as few government rules and regulations as required to build and maintain a free, peaceful, and prosperous society. In the areas where rules and regulations are necessary, they should be well‐​considered and guided by experts who understand the issue that is being regulated. There should also be consequences for making errors and rewards for being correct. Trust in those experts is fragile in even the best of times, but crucial for widespread popular acceptance which is necessary for the enforcement of any new policy. When some experts commit noble lies, it damages their credibility and limits the extent of their wiser (compared to non‐​experts) recommendations.



Tufekci ended her piece with this prescient warning:



Research shows that during disasters, people can show strikingly altruistic behavior, but interventions by authorities can backfire if they fuel mistrust or treat the public as an adversary rather than people who will step up if treated with respect. Given that even homemade masks may work better than no masks, wearing them might be something to direct people to do while they stay at home more, as we all should.



Experts should commit themselves publicly to always telling the truth and to banish the noble lie from public debate. By limiting the transmission of noble lies, hopefully we can do something to limit the spread of COVID-19.
"
"Mushroom, pineapple and algae: it sounds like the topping for a rather unusual pizza. In fact, they could be the crucial ingredients in the wardrobe of the future as growing numbers of designers try to create fashion that doesn’t harm the environment. Examine a garment’s care label and you may find that it was made out of pineapple stalks or cactus leaves, or a tote bag was woven with thread made from banana trees. From mushroom leather to algae T-shirts, the search is on for alternative materials with smaller carbon footprints. And the latest result are carbon-negative clothes made with algae that absorb carbon dioxide from the air.  “Fashion is part of the problem but it’s also part of the solution,” said Nina Marenzi, founder and director of the Sustainable Angle, a not-for-profit organisation which promotes green textiles at its annual Future Fabric Expo. “We begin with materials and making them sustainable, and if fashion supply chains can change, then we start to address that.” The New York designer Charlotte McCurdy has made a see-through bioplastic mac using algae – specifically algae powder used in vegan food products. She worked with glass casters to find a way to heat the algae and cool it in a controlled fashion to make it transparent. The material is carbon-negative because the algae draw carbon out of the atmosphere, meaning the coat acts as a carbon sink. “Follow the carbon – where did it come from?” she said. “Has it come from carbon taken out of the atmosphere millions of years ago and put in the ground? We talk a lot about what happens to materials after we use them, but not where they come from in the first place.” Post Carbon Lab is using the same principle with another algae prototype – clothes that photosynthesise. The start-up in London has created photosynthesis coating, a layer of living algae on the fabric of garments that absorb carbon dioxide and emit oxygen, turning the carbon into sugar. One large T-shirt – nearly a square metre of material – generates about as much oxygen as a six-year-old oak tree, according to the co-founder Dian-Jen Lin. The start-up has been working with designers and industry to translate its photosynthesis coating into a marketable product, and Lin said it could be used in shoes, backpacks, curtains, pillow cases, umbrellas and building canopies. The care instructions were rather different to normal clothes, she said. Wearing algae was not without its perils. “You can’t put it into your dark wardrobe. It needs light and carbon dioxide, so you have to put it in a well-ventilated area, like the back of your chair.” Washing machines would harm the algae, so “it’s handwash only – you have to be a bit careful. I wouldn’t recommend this coating for your underwear but maybe for a windbreaker or a jacket.” Lin and her co-founder Hannes Hulstaert are testing the limits of the coating, which she says can be applied to almost any garments, either as a full coating or a print. “But it might change colour if it’s really upset, if it didn’t like the light or temperature,” Lin said. “Most of the organisms are in the green shade. In the healthy state they are dark brownish green, orangeish green. When it’s unhappy it might turn yellow, orange, brown, purple or white or transparent.” However, it seems remarkably resilient. “We’ve had samples for three years which have come back to life,” Lin said. Other textiles include Piñatex, made from pineapple leaves and used by Hugo Boss and H&M, and Mycotex, a substance grown from mushrooms. Cactus is the next plant-based leather to emerge, the creation of Desserto, a Mexican company that makes leather from leaves. The challenges facing the fashion industry in its quest to become greener are huge. The UK throws about 300,000 tonnes of clothes into landfill each year, and some studies suggest global textile production creates 1.2bn tonnes of carbon dioxide a year – more than airlines and shipping combined."
"

The nation’s 76 million stockholders have “internalized their new role as capitalists,” causing public opinion to favor investor‐​friendly policies over government programs, says Richard Nadler in “The Rise of Worker Capitalism” (Policy Analysis no. 359). Forty‐​three percent of U.S. households own stocks or stock mutual funds, a 126 percent increase in shareholding over the last 15 years. The rate of increase was particularly steep among laborers and farmers (106 percent), householders 34 years old or younger (64 percent), and families with incomes under $25,000 (80.4 percent). As wage earners become owners of capital, Nadler finds, they increasingly favor policies that reduce taxes on savings and distrust government “investments” such as Social Security. “Congress should enact policies that expand worker ownership and financial self‐​sufficiency,” Nadler concludes, pointing out the importance of spreading wealth to even larger segments of the population by expanding individual retirement accounts and 401(k) plans and instituting individually owned Social Security accounts.



 **Tennessee—Still the Volunteer State?**



In “The Case against a Tennessee Income Tax” (Cato Briefing Paper no. 53), Stephen Moore and Richard Vedder argue that insti‐​tuting an income tax in Tennessee would reduce growth and job creation and would be the most economically destructive way to close the state’s budget shortfall. The study was released the day before the state legislature was to begin debating Gov. Don Sundquist’s proposed 3.75 percent state income tax. Tennessee is currently one of only nine states without an income tax. Moore, director of fiscal policy studies at Cato, and Richard Vedder, an economics professor at Ohio University, contend that “Tennessee’s structural deficit problems are a result of a huge growth in state expendi‐​tures, not insufficient revenues.” Of the options available for closing the state budget deficit, estimated to be between $300 million and $500 million, an income tax “would likely be the single most economically harmful. Tennessee derives large economic benefits from not having an income tax, and it should not forfeit those benefits,” the authors conclude.



 **An Agenda for the WTO**



Supporters of free trade should abandon the reciprocity model of negotiations and instead pursue a course of coordinated unilateralism, in which the benefits of open markets at home and abroad are clearly recognized, write the authors of “Seattle and Beyond: A WTO Agenda for the New Millennium” (Trade Policy Analysis no. 8). Brink Lindsey, director of Cato’s Center for Trade Policy Studies; Daniel Griswold, associate director of the center; Mark Groombridge, research fellow; and Aaron Lukas, trade policy analyst, argue that the new WTO round should be seen as a “ ‘bottom‐​up’ process in which countries liberalize, not merely to gain ‘concessions’ from other countries, but primarily to reap the economic rewards of their own liberalization.” Free traders, the authors maintain, “should focus on getting the available gains as quickly as possible and fend off efforts to clog and corrupt the agenda with illiberal initiatives.”



 **Iraqi Threat Overblown**



The U.S. policy of attempting to remove Saddam Hussein from power will be difficult, could be counterproductive, and might throw Iraq into a civil war, argues defense analyst David Isenberg in “Imperial Overreach: Washington’s Dubious Strategy to Overthrow Saddam Hussein” (Policy Analysis no. 360). The author contends that the Iraq Liberation Act of 1998, which states that the United States will aid efforts to overthrow Saddam and promote democracy, is flawed because it does not offer a realistic way of dealing with the Iraqi leader. Isenberg believes that the threat of Saddam is “overblown,” pointing out that Saddam’s army has already been decimated by war and sanctions. “Saddam may be odious, but his regime does not pose a major threat to America’s security.” Isenberg argues that a more realistic policy would be to lift general economic sanctions in exchange for international weapons inspections and to continue a selective embargo on military weaponry.



 **Not‐​So‐​Smart Growth**



The campaign to eliminate urban “sprawl” and replace it with “smart growth” has been financed with federal tax dollars, note the authors of a new Cato study, “Smart Growth at the Federal Trough: EPA’s Financing of the Anti‐​Sprawl Movement” (Policy Analysis no. 361). The federal government, via grants from the Environmental Protection Agency to nonprofit organizations, has been covertly supplying funds and technical support to anti‐​automobile, anti‐​suburb groups. Peter Samuel, editor of _Toll Roads Newsletter_ and a consultant on EPA policies for the George C. Marshall Institute, and Randal O’Toole, executive director of the Thoreau Institute and an adjunct scholar at the Cato Institute, argue that “EPA’s campaign fundamentally subverts not only the Tenth Amendment but the very concept of democracy itself.”



 **Social Security Is Still a Bad Deal**



The current Social Security system would not pay higher rates of return and benefits than a privatized system of personal retire‐​ment accounts, writes Peter J. Ferrara in “Social Security Is Still a Hopelessly Bad Deal for Today’s Workers” (Social Security Paper no. 18). The analysis refutes a recent study by John Mueller for the National Committee to Preserve Social Security and Medicare. Ferrara, chief economist and general counsel with Americans for Tax Reform and senior fellow at Cato, points out that Mueller’s findings are contradicted by a broad range of analysts, institutions, and leaders, including President Clinton, Harvard economics professor Martin Feldstein, the Heritage Foundation, the World Bank, and the 1994–95 Social Security Advisory Council.



 **The Imperial Presidency**



Modern presidents have moved beyond their constitutional duty of seeing “that the Laws be faithfully executed” and have instead been usurping vast lawmaking powers reserved to Congress or the states, argue attorneys William J. Olson and Alan Woll in “Executive Orders and National Emergencies: How Presidents Have Come to ‘Run the Country’ by Usurping Legislative Power” (Policy Analysis no. 358). The authors note that, during the recent presidential scandals, many people called for the investigations to end “so that the president could get back to ‘the business of running the country.’ ” How did we get to a point, the authors ask, “where so many Americans think of government as embodied in the president and then liken him to a man running a business?” The answer rests, in part, “with the growth of presidential rule through executive order and national emergency,” according to the authors. Congress has delegated more and more power to the executive branch, aiding and abetting the expansion of presidential power, the authors note. The courts have acted in just two cases––in 1952 and 1996––to restrain the executive branch. The good news, the authors point out, is that the nation’s governors have just forced President Clinton to rewrite a federalism executive order; and now there are two proposals in Congress that seek to limit presidential lawmaking.



 **Clinton’s Pyrrhic Victory in Kosovo**



The Clinton administration’s policy in Kosovo has habitually failed to meet its objectives and will continue to entangle the United States in multi‐​billion‐​dollar, open‐​ended peacekeeping operations, writes Christopher Layne, a visiting scholar at the Center for International Studies at the University of Southern California. In “Faulty Justifications and Ominous Prospects: NATO’s ‘Victory’ in Kosovo” (Policy Analysis no. 357), Layne writes that the administration “stumbled into war and blundered its way to ‘victory.’ ” Layne says that President Clinton’s claim of victory “rings hollow”: NATO’s intervention not only killed many innocent civilians in Yugoslavia; it also caused serious economic and social disruptions throughout the Balkans and greatly strengthened the position of the extremist Kosovo Liberation Army. Layne warns that the war continues to have negative policy repercussions. “The war with Yugoslavia has had important geopolitical effects that reverberate far beyond the Balkans. Clinton’s Kosovo policy has had portentous consequences for America’s relations with its great‐​power rivals, Russia and China, and its great‐​power allies, the West European nations.”



 **Cut Global Warming Program**



Congress should eliminate funding for a $1.4 billion global warming program, argues Jerry Taylor, Cato’s director of natural resource studies, in “Energy Efficiency: No Silver Bullet for Global Warming” (Policy Analysis no. 356). The Climate Change Technology Initiative, being pushed by the Clinton administration as a way to combat global warming, is a “sham,” and a “repackaging of failed programs” that do nothing to significantly reduce global temperatures, he writes. The program—an amalgam of tax credits, research and development, product labeling and awareness programs, demonstration projects, and subsidies and regulations to increase energy efficiency and the economic attractiveness of renewable energy—is “built on economic ignorance and political symbolism,” Taylor writes.



 **Protocols on Biological Weapons Ineffective**



The protocols proposed for the Biological Toxins and Weapons Convention would do little to stop the spread of bioweapons and could compromise valued U.S. secrets and critical data used for defense against biological weapons, writes Eric R. Taylor of the University of Louisiana at Lafayette in “Strengthening the Biological Weapons Conventions: Illusory Benefits and Nasty Side Effects” (Policy Analysis no. 355). Taylor writes that proposed protocols render inspections “useless” in demonstrating either compliance with or violation of the convention. According to Taylor, U.S. pharmaceutical development, which relies heavily on the very technology that is also critical to bioweapons research and development, would be especially hurt by the new protocols. “The future of the people’s right to be secure in their possessions and personal effects is placed in peril by the Biological Toxins and Weapons Convention protocols,” he writes. “Although an attack with biological weapons on the United States would be dangerous, an assault on U.S. constitutional rights in an effort to strengthen an international convention has little hope of stopping the spread of those weapons.”



 **Repeal the Community Reinvestment Act**



The Community Reinvestment Act should be repealed, writes economist George J. Benston in “The Community Reinvestment Act: Looking for Discrimination That Isn’t There” (Policy Analysis no. 354). Originally intended to deal with “redlining”—the alleged refusal of banks to lend to residents of poorer urban areas inhabited by racial minorities—the two‐​decade‐​old CRA is an expensive way to deal with a problem that may not exist, the study finds. Benston reports that qualified applicants, regardless of their address, do not suffer unwarranted discrimination in lending. “Researchers using the best available data find very little discernible home‐​mortgage lending discrimination based on area, race, sex, or ethnic origin,” writes Benston, the John H. Harland Professor of Finance, Accounting, and Economics at Emory University.



 **Cradle‐​to‐​Grave Taxation**



The federal gift and estate tax, better known as the “death tax,” is clearly a failure from an economic standpoint, but “the biggest problem with the death tax is a moral one,” writes law professor Edward J. McCaffery in “Grave Robbers: The Moral Case against the Death Tax” (Policy Analysis no. 353). He notes that the tax’s economic shortcomings are well‐​known. It “raises barely over 1 percent of total federal tax revenues,” and “for every dollar raised from the tax, roughly another dollar is lost because of avoidance, compliance, administrative, and enforcement costs.” But it is the moral impact that is most objectionable, according to McCaffery. The tax “rewards a ‘die‐​broke’ ethic, whereby the wealthy spend down their wealth on lavish consumption, and discourages economically and socially beneficial intergenerational saving.” McCaffery, a professor in the University of Southern California Law School, finds that the death tax rewards those who don’t work, don’t save, and spend all of their wealth.



 _This article originally appeared in the January/​February 2000 edition of_ Cato Policy Report.
"
nan
"India Block makes many valuable points in her critique of the disastrous standard of accelerated development in UK cities (Who wants cities of ugly new-builds?, Journal, 5 February), but I was exasperated by her failure to mention the non-negotiable bottom line for rejection of this model, especially given the juxtaposition of her article with Steve Bell’s cartoon above it: the climate emergency. As we know, new construction is responsible for 40% of carbon emissions, and operational building emissions account for 28%; speculative development and large-scale “urban regeneration” is complicit in the catastrophic trajectory of global heating and the collapse of ecosystems.  Architects have been developing innovative and radical approaches to address the burgeoning environmental crisis for decades – mostly ignored by unenlightened clients pleading poverty. More recently, schools of architecture in universities, and also cities across the world, are declaring a climate emergency, taking the lead where national governments and powerful corporations have failed to. We have to completely shift our understanding of the measures that are needed to tackle the crisis we face over the next decade, among which the tired concept of “sustainability” seems scarcely serviceable. It is time for city leadership and society as a whole to turn its back on new development as a principle of urban growth and commit to the model of the ecologically “smart” city based on husbanding and reuse of existing resources: the gold standard “world-class city”. For these new-builds are not just ugly on the surface, they are destroying the ecosystem that sustains us on this planet.Dr Clare MelhuishDirector, UCL Urban Laboratory; principal research fellow, Bartlett Faculty of the Built Environment, University College London"
nan
"
Share this...FacebookTwitterGermany’s Bundestag moves to enact a higher CO2 tax 
The “Corona pandemic”, despite the ever falling death rate, has given governments cover to enact draconian regulation and lockdowns, thus allowing their even wildest power wet dreams to turn into reality.
Just a year ago much of what we are seeing today was considered unimaginable. Yet, here we are.
Never before have modern “democratic” governments enacted such extreme lockdown and government intrusion measures like those we have seen in the current “Corona crisis”. And like real junkies, they need more.
There are other government crackdown opportunities left out there, among them the “climate crisis” – the Big Kahuna when it comes to government regulation, takeover and control. It’s not for nothing they’ve frittered away hundreds of billions propping up this fake crisis.
Germany is already seizing the opportunity, having pledged to ban internal combustion engines soon, modify human eating habits and restricting a host of other amenities we once took for granted. Soon these amenities will be redefined as privileges, and they will be easily available only to the wealthy and elite.
The latest is energy and heat.
Higher CO2 tax decided
The German media, e.g. NTV public broadcasting, have reported that the Bundestag has just decided on a higher CO2 tax beginning already next year, January 1st.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“It is intended to make fossil fuels less attractive. This means that fuel, heating oil and gas, among other things, will cost more,” reports  German public broadcaster NTV. “The CO2 price will be 25 euro per ton from the new year on. The levy acts like a tax and is to climb gradually to 55 euros by 2025.”
That also means higher gasoline and diesel fuel prices, which will make transportation more expensive.
Already German electricity is the most expensive in Europe.

But not to worry: Industries with particularly high energy requirements that are also in global competition will be relieved of the costs. And of course, the rich and elitists will also keep their cushy red carpet lives – so that they can continue effectively doing their important work – while the rest of us are forced to move out into the cold mud.
In the end it’ll be lower income workers and households left struggling with the higher prices for everything. Even heat will become a luxury.
And so continues the cycle of political demise 
And once governments get total control and surveillance over citizens across the world, they’ll try to tell us all just how much better things have become as a result, like the old communists used to do with their state controlled media. Of course, life in reality will become much worse, but we’ll be asked to pretend that it isn’t.
We all know what follows next: Revolution.


		jQuery(document).ready(function(){
			jQuery('#dd_4223c6fa640da5fd71799ecd0745c810').on('change', function() {
			  jQuery('#amount_4223c6fa640da5fd71799ecd0745c810').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Is it possible to address environmental problems, such as pollution, without resorting to the traditional regulatory approach? In the new issue of the _Cato Journal_ , economists Geoffrey Black, D. Allen Dalton, Samia Islam, and Aaron Batteen offer one prominent example of allowing the market to work. By examining the New York City Watershed Memorandum of Agreement (MOA), the authors demonstrate how large‐​scale externalities can be successfully internalized with minimal state intervention.



In 1997, New York City entered into an agreement in which it assigned the area’s watershed communities the property rights to continue developing, despite the fact that some of those activities degraded the city’s drinking water. This assignment placed the burden of water quality on the city. Once responsibility was established, the government opted to buy lands that were contributing to water quality degradation, instead of building a multibillion‐​dollar filtration system. In turn, the residents and landowners upstream were compensated for development restrictions incurred from the agreement. In short, the New York City Watershed MOA is the first of its kind. “The negotiations forged a new method for dealing with externalities showing how … solutions could be facilitated and used in wide‐​reaching economic conflicts,” the authors write.



In September 2012, seven weeks before the presidential election, a Congressional Research Service (CRS) study claimed that there is no evidence that changes in top marginal tax rates have had any impact on U.S. economic growth since World War II. The mainstream media, politicians, and political groups favoring higher taxes on the wealthy widely cited the study as evidence against Mitt Romney’s economic program and in favor of President Obama’s plan to raise top marginal rates. In “Marginal Tax Rates and U.S. Growth,” economists Jason E. Taylor and Jerry L. Taylor revisit the CRS analysis and pinpoint its fatal flaw. “Our results are consistent with what economists have long understood: that a tradeoff exists between income redistribution and economic growth,” they conclude.



Despite pronounced differences in medical financing arrangements, the United States and other countries throughout the Organization for Economic Cooperation and Development (OECD) have witnessed a tremendous growth in health care costs over the last several decades. In “The Medical Care Cost Ratchet,” scholars Andrew Foy, Christopher Sciamanna, Mark Kozak, and Edward J. Filippone explain that health care spending increases over time as new technologies that confer only modest clinical benefits are incorporated into the traditional standard of care. They argue, furthermore, that encouraging individuals to economize on nonemergent health care decisions would help bend the cost curve over time. “Reform efforts,” they conclude, “should focus on rejuvenating market forces that have been systematically suppressed.”



Forecasts of future economic activity underlie any budget revenue projection. However, public choice models of political decisionmaking suggest that government agencies such as the Congressional Budget Office (CBO) and Office of Management and Budget (OMB) face pressures that are likely to result in systematically biased forecasts— whereby, for instance, rosy growth forecasts are rewarded and underforecasted growth penalized. In his article, economist Robert Krol finds that while the CBO, consistent with the private‐​sector forecast, has a downward bias, the OMB estimates indicate a significant upward bias—which is “interpreted to mean executive branch political pressure influences the forecast.” Other contributors include Thomas L. Hogan and William J. Luther on “The Explicit Costs of Government Deposit Insurance,” Paul H. Rubin on “Pathological Altruism and Pathological Regulation,” and Paul Ballonoff with “A Fresh Look at Climate Change.”



The Winter 2014 issue also features reviews of books on the importance of Ayn Rand’s ideas, a theoretical framework for understanding the financial crisis, and the famous bet between Julian Simon and Paul Ehrlich.
"
"People are too afraid to return to the village so they are sleeping in the forest or have left altogether. They have lost their stored grain and all their belongings. I don’t know how they will get by. These are the words of Riana*, a young woman from Bevoahazo, a tiny village in the eastern rainforests of Madagascar. Bevoahazo sits on the edge of Ranomafana National Park in a UNESCO world heritage site teeming with endangered and endemic species. Security in the area has been deteriorating over the last few years but things have escalated recently. On November 24, 50 men raided the village stealing stores of rice – vital food reserves for local people who are mostly subsistence farmers – and injuring anyone who tried to defend their property. A few days later the local police chief, Heritiana Emilson Rambeloson, who had come to the area with a small team to investigate, was shot dead. I spent two years living in Bevoahazo in the early 2000s while researching the sustainability of crayfish harvesting. I have spoken to friends from the village who are are currently staying in the nearby town of Ranomafana for safety, and researchers in the area to get a better understanding of what is happening.  Patricia Wright, a professor of anthropology, has spent more than 30 years working in Ranomafana. She directs the Centre Valbio, an internationally renowned conservation research centre situated on the edge of the forest. She said: The security situation is at crisis point. This is leading to real human suffering in one of the most important places for biodiversity on the planet. The [murdered policeman] was smart, dedicated to his job and was interested in wildlife and the importance of the forest. A genuine friend. We will miss him. The recent death comes just months after a member of Valbio staff was killed by bandits. Jean François Xavier Razafindraibe was killed when armed men raided his village close to the park entrance in June 2018. Ranomafana National Park was established by the Malagasy government to protect its globally important biodiversity. As part of the Forests of Atsinanana it is home to a number of critically endangered endemic lemurs such as the golden bamboo lemur and the black-and-white ruffed lemur. Ranomafana is a popular tourist spot in Madagascar with stunning scenery, rare wildlife and the friendly, sleepy town nearby. So far the insecurity hasn’t influenced tourism. As Wright says:  The bandits steer clear of tourists, but the villagers are living a life of fear. Miners panning for gold illegally in the forest interior are a source of the insecurity. This has been an ongoing issue for many years but has become much more difficult for the park authorities to control. The miners pollute rivers, clear the rare swamp forest and hunt endangered wildlife for meat. The situation is complicated. Armed cattle thieves known as dahalo are causing havoc in many areas of Madagascar. A recent estimate suggests they have caused 4,000 deaths in the last five years alone. In 2017, the mayor of the neighbouring town of Ambalakindresy, Elysé Arsène Ratsimbazafy, was shot dead in what is widely believed to have been a hit. He had run for election on a platform of ridding the town of the bandits and had cooperated with efforts to get the miners expelled from the national park interior. Mar Cabeza, a professor of biology at the University of Helsinki, returned from the area a few days ago. She said: The gold mining has escalated in recent years and differs greatly from previous subsistence-related threats. The widespread fear has negatively affected both research and conservation management. One of Cabeza’s PhD students, Marketta Vuola, was meant to conduct research in the attacked villages recently, but was warned of the danger and moved to another village. Vuola told me  News spread fast, with all villages in the region being afraid. We spent last night hiding, with our day packs ready to escape to the forest. There has been a robust response to the recent series of attacks. The district quickly sent reinforcements of 80 police. This will hopefully reassure the local population, allowing people to return to their village, and will reduce the immediate threat. 


      Read more:
      Animals are victims of human conflict, so can conservation help build peace in warzones?


 This reassurance is essential as my old friend Koto* told me over the phone:  People need to be able to get back home to tend their crops; if they can’t do this they will suffer even more. However the rise in insecurity reflects a wider problem of respect for the rule of law in Madagascar. Jonah Ratsimbazafy, a professor of paleontology at the University of Antananarivo in Madagascar, said: If you focus on what is happening, then you will lose your hope for Madagascar. We must focus on the solutions. Good governance is crucial in order to develop the economy of Madagascar and for saving the irreplaceable biodiversity. Madagascar will elect a new president on December 19. People in Bevoahazo, and throughout Madagascar, are hoping that the new government can bring the change so desperately needed. *Names changed to protect identities."
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"

**May 1:** In the name of “universal service,” the Telecommunications Act of 1996 empowers the Federal CommunicationsCommission to create public entitlements to advanced telecommunications services. At a Policy Forum on “UniversalService: Socializing the Telecommunications Infrastructure,” Milton Mueller, professor of communications at RutgersUniversity; Wayne Leighton, senior economist at Citizens for a Sound Economy; and Lawrence Gasman, director oftelecommunications and technology studies at the Cato Institute, addressed the question: Can mandated and subsidizedtelecommunications serve consumers better than competition? 



**May 8:** Auctions are heralded as the most beneficial means of allocating the airwaves that have been set aside for advancedtelevision services (ATV), including high‐​definition television. But if federal regulators insist that broadcasters be subject topublic‐​interest controls, must they offer broadcasters free ATV spectrum as a quid pro quo? That question was the focus of“Beyond Budgetary Concerns: A Free‐​Market Perspective on ATV Spectrum Auctions,” a Policy Forum featuringTom Hazlett, visiting scholar at the American Enterprise Institute; James Gattuso, vice president for policy research at Citizensfor a Sound Economy; and Bob Okun, vice president of NBC. 



**May 14:** Cato hosted a delegation from the Hungarian Embassy at a Roundtable Luncheon. The discussion with Cato staffand policy analysts centered on the Hungarian perspective on European security issues, expanding NATO, and U.S. troops inHungary. 



**May 15:** During a Policy Forum titled “Red Resurgence or Revitalized Reform? Russia’s Political Future,” SusanEisenhower, chairman of the Center for Post‐​Soviet Studies; Dmitry F. Mikheyev, senior fellow at the Hudson Institute; andAriel Cohen, senior policy analyst at the Heritage Foundation, discussed the prospects and implications of a possiblecommunist victory in Russia’s June election. 



**May 16:** Although natural gas deregulation is generally considered an economic success pregnant with valuable lessons forother industries, a web of regulatory oversight still surrounds the industry. Jerry Ellig of the Center for Market Processes andJoseph Kalt of Harvard University appeared at a Cato Book Forum to discuss their new book, _New Horizons in NaturalGas Deregulation_ , a collection of papers originally presented at a 1995 Cato conference. Ellig and Kalt reviewed the pastfailure of natural gas regulation, the lessons of regulatory reform, and how further deregulation should proceed in the 1990s. 



**May 22:** As chairman of the U.S. House of Representatives’ Task Force on Privatization, Rep. Scott Klug (R‐​Wis.) is aleading proponent of privatization in the 104th Congress. At a Policy Forum titled “Privatization: New Zealand’s SuccessStory,” Klug discussed his fact‐​finding mission to New Zealand and the success of that country’s privatization program. Inmeetings with railroad executives and sheep and dairy farmers living without subsidies, Klug learned lessons that Americansshould heed. 



**May 23:** The Cato Institute held its 14th Annual Monetary Conference, “The Future of Money in the Information Age.“The full‐​day conference addressed the technological viability and economic implications of digital currency, or “E-money.“Speakers included Scott Cook, chairman of Intuit, manufacturer of the popular business software “Quicken”; Rep. MichaelCastle (R‐​Del.), chairman of the House Subcommittee on Domestic and International Monetary Policy; and Jerry L. Jordan,president and CEO of the Federal Reserve Bank of Cleveland. For the first time, a Cato event was carried live via interactivetelevideo to nine sites around the country as well as broadcast on the Internet. Over 175 people attended the event inWashington, D.C., while others participated in New York, San Francisco, Chicago, Silicon Valley, and other places. 



**May 28:** As telephone services in New Zealand were deregulated, bureaucrats took a more “hands‐​off” approach than in theUnited States. Should U.S. regulators take a similarly minimalist approach, allowing even the terms of interconnection to be setby private negotiations? That was the topic of discussion during “New Zealand Telephone Deregulation: What Lessonsfor the United States?” a Policy Forum featuring Milton Mueller of Rutgers University, Jeff Rohlfs of Strategic PolicyResearch, and Joseph Farrell of the Federal Communications Commission. 



**May 29:** This year’s decertification of Colombia and the confirmation of Gen. Barry McCaffrey as “Drug Czar” suggest astepped‐​up effort in the war on drugs. With that in mind, Cato asked, “Does the International Drug War Make Sense?“at a Policy Forum featuring Robert Gelbard, assistant secretary of state for international narcotics and law enforcement affairs,and Kevin Jack Riley, author of _Snow Job? The War against International Cocaine Trafficking._ Gelbard reviewed thelogic of Washington’s international narcotics control strategies and explained how the United States plans to significantlyreduce the flow of drugs across its borders. Riley questioned the supply‐​side campaign, examined its impact on drug‐​sourcecountries, and assessed its prospects for success. 



**June 6:** Cato held its midyear Board of Directors Meeting. Board members set the Institute’s course and were brought upto date on Cato’s policy activities, progress in fundraising, and fiscal standing. 



**June 17:** In Seattle Cato hosted a City Seminar, “Leviathan and the New Millennium: An Agenda for Real Reform,“that featured a keynote address by Lawrence Kudlow, economic counsel at Laffer Advisors, Inc., and a panelist on CNBC’s _Strictly Business._ Other speakers included José Piñera, co‐​chairman of Cato’s Project on Social Security Privatization,Edward H. Crane, president of the Cato Institute, Stephen Moore, Cato’s director of fiscal policy studies, and MichaelTanner, Cato’s director of health and welfare studies. 



**June 18:** America’s security commitments abroad remain largely unchanged despite the end of the Cold War. Nowhere is thatmore evident than on the Korean peninsula, where a commitment of nearly 40,000 U.S. troops, costing billions of dollars ayear, threatens to draw the United States into any conflict that might erupt in Northeast Asia. Cato senior fellow DougBandow appeared at a Book Forum to discuss his new Cato book, _Tripwire: Korea and U.S. Foreign Policy in aChanged World._ Bandow argued that it is time to phase out the American military commitment to South Korea, which hastwice the population of North Korea and an economy 18 times as large as that of the North. That step would free the UnitedStates of an obsolete obligation and give South Korea responsibility for its own security. 



**June 19:** Cato hosted a City Seminar in San Francisco on “Leviathan and the New Millennium: An Agenda for RealReform.” The keynote address was given by Ward Connerly, a member of the Board of Regents of the University ofCalifornia and chairman of the California Civil Rights Initiative, which would outlaw racial quotas in state policy. Otherspeakers included José Piñera, co‐​chairman of Cato’s Project on Social Security Privatization and Edward H. Crane, StephenMoore, and Michael Tanner of the Cato Institute. 



**June 21:** In the postcommunist era Russia and Eastern Europe have implemented systems of parental choice in educationsimilar to the U.S. voucher concept. At a Book Forum for _Educational Freedom in Eastern Europe,_ author Charles L.Glenn, professor of education at Boston University, discussed his survey of educational reforms in 10 East European countriesand the lessons that America might learn. Denis P. Doyle of the Heritage Foundation commented. 



**June 26:** Living standards and rates of growth differ dramatically around the world. Robert J. Barro, Robert C. WaggonerProfessor of Economics at Harvard University and author of _Getting It Right: Markets and Choices in a Free Society,_ spoke at a Book Forum on what accounts for those disparities. He discussed the relationships among material progress anddemocracy, domestic institutions, and government policies and concluded that the rule of law has enormous explanatory poweras a factor in economic growth and that governments should provide markets with a stable framework of rules and then get outof the way. 



**June 27:** Sixty years ago the New Deal Supreme Court began unraveling the Constitution of limited government byreinterpreting first the general welfare clause and then the commerce clause. Recently, scholars and the Court have begun toreexamine the commerce clause jurisprudence that gave us the modern regulatory state, but little has been done with thejurisprudence of the general welfare clause that gave us the modern redistributive state. At a Book Forum, Leonard R.Sorenson, professor of history at Assumption College, discussed _Madison on the “General Welfare” of America,_ his newbook that provides a detailed refutation of scholars on whom members of today’s Court were schooled. Comments wereprovided by Judge Douglas H. Ginsburg of the U.S. Court of Appeals for the District of Columbia Circuit. 



**June 28:** International negotiations addressing the issue of global climate change have resumed in Geneva, and a recent reportfrom the Intergovernmental Panel on Climate Change (IPCC) has introduced a sense of urgency to those negotiations. Doesthat report really justify immediate governmental action to address global warming, or is it just another example of scientificsensationalism? At a recent Policy Forum, “The New IPCC Report: Scientific Consensus of Scientific Meltdown?“William O’Keefe of the Global Climate Coalition argued that the scientific “finds” of the report have been heavily anddisingenuously edited by political activists. Patrick Michaels, climatologist at the University of Virginia, similarly maintained thatthe report is so riddled with basic scientific errors as to be a completely unreliable guide for policymaking. 
"
"

The “SnoMote Remote Controlled Weather Station”
At first, I though this must be a joke. But, it is not. They call it “an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.” It started life as the Ski-Doo® RC Snowmobile which is 28″ long, and runs for 30 minutes on a charge.
But in a press release from Georgia Tech on May 27th, seen below, it is clear that this is real, true, fully federally funded NASA science project. You can buy one here from Hammacher Schlemmer for $79.95 Ooops, sold out, looks like Georgia Tech bought them out.
My question is, when one of these gets stuck in a crack or crevasse, or simply runs out of power prematurely, do they just leave it there for the polar bears to play with or do they send the lowliest science intern out on the ice to fetch it back, lest it remain to pollute the sea and/or sea ice with it’s Lead or Nickel Cadmium rechargeable batteries?
UAV’s have already been used in the arctic.

Robots go where scientists fear to tread








SnoMote, an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.
Click here for more information.





ATLANTA ( May 27, 2008 ) — Scientists are diligently working to understand how and why the world’s ice shelves are melting. While most of the data they need (temperatures, wind speed, humidity, radiation) can be obtained by satellite, it isn’t as accurate as good old-fashioned, on-site measurement and static ground-based weather stations don’t allow scientists to collect info from as many locations as they’d like.
And unfortunately, the locations in question are volatile ice sheets, possibly cracking, shifting and filling with water — not exactly a safe environment for scientists.
To help scientists collect the more detailed data they need without risking scientists’ safety, researchers at the Georgia Institute of Technology, working with Pennsylvania State University, have created specially designed robots called SnoMotes to traverse these potentially dangerous ice environments. The SnoMotes work as a team, autonomously collaborating among themselves to cover all the necessary ground to gather assigned scientific measurements. Data gathered by the Snomotes could give scientists a better understanding of the important dynamics that influence the stability of ice sheets.








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





“In order to say with certainty how climate change affects the world’s ice, scientists need accurate data points to validate their climate models,” said Ayanna Howard, lead on the project and an associate professor in the School of Electrical and Computer Engineering at Georgia Tech. “Our goal was to create rovers that could gather more accurate data to help scientists create better climate models. It’s definitely science-driven robotics.”
Howard unveiled the SnoMotes at the IEEE International Conference on Robotics and Automation (ICRA) in Pasadena on May 23. The SnoMotes will also be part of an exhibit at the Chicago Museum of Science and Industry in June. The research was funded by a grant from NASA’s Advanced Information Systems Technology (AIST) Program.
Howard, who previously worked with rovers at NASA’s Jet Propulsion Laboratory, is working with Magnus Egerstedt, an associate professor in the School of Electrical and Computer Engineering, and Derrick Lampkin, an assistant professor in the Department of Geography at Penn State who studies ice sheets and how changes in climate contribute to changes in these large ice masses. Lampkin currently takes ice sheet measurements with satellite data and ground-based weather stations, but would prefer to use the more accurate data possible with the simultaneous ground measurements that efficient rovers can provide.
“The changing mass of Greenland and Antarctica represents the largest unknown in predictions of global sea-level rise over the coming decades. Given the substantial impact these structures can have on future sea levels, improved monitoring of the ice sheet mass balance is of vital concern,” Lampkin said. “We’re developing a scale-adaptable, autonomous, mobile climate monitoring network capable of capturing a range of vital meteorological measurements that will be employed to augment the existing network and capture multi-scale processes under-sampled by current, stationary systems.”








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





The SnoMotes are autonomous robots and are not remote-controlled. They use cameras and sensors to navigate their environment. Though current prototype models don’t include a full range of sensors, the robots will eventually be equipped with all the sensors and instruments needed to take measurements specified by the scientist.
While Howard’s team works on versatile robots with the mobility and Artificial Intelligence (A.I.) skills to complete missions, Lampkin’s team will be creating a sensor package for later versions of Howard’s rovers.
Here’s how the SnoMotes will work when they’re ready for their glacial missions: The scientist will select a location for investigation and decide on a safe “base camp” from which to release the SnoMotes. The SnoMotes will then be programmed with their assigned coverage area and requested measurements. The researcher will monitor the SnoMotes’ progress and even reassign locations and data collection remotely from the camp as necessary.
When Howard’s research team first set out to build a rover designed to capture environmental data from the field, it took a few tries to come up with an effectively hearty design. The group’s first rover was delicate and ineffective. But after an initial failure, they decided to move on to something designed for consistent abuse — a toy. Instead of building yet another expensive prototype, Howard instead opted to start with a sturdy kit snowmobile, already primed for snow conditions and designed for heavy use by a child.
Howard’s group then installed a camera and all necessary computing and sensor equipment inside the 2-foot-long, 1-foot-wide snowmobile. The result was a sturdy but inexpensive rover.
By using existing kits and adding a few extras like sensors, circuits, A.I. and a camera, the team was able to create an expendable rover that wouldn’t break a research team’s bank if it were lost during an experiment, Howard said. Similar rovers under development at other universities are much more expensive, and the cost of sending several units to canvas an area would likely be cost-prohibitive for most researchers, she added.
The first phase of the project is focused primarily on testing the mobility and communications capabilities of the SnoMote rovers. Later versions of the rovers will include a more developed sensor package and larger rovers.
The team has created three working SnoMote models so far, but as many SnoMotes as necessary can work together on a mission, Howard said.
The SnoMote represents two key innovations in rovers: a new method of location and work allocation communication between robots and maneuvering in ice conditions.
Once placed on site, the robots place themselves at strategic locations to make sure all the assigned ground is covered. Howard and her team are testing two different methods that allow the robots to decide amongst themselves which positions they will take to get all the necessary measurements.
The first is an “auction” system that lets the robots “bid” on a desired location, based on their proximity to the location (as they move) and how well their instruments are working or whether they have the necessary instrument (one may have a damaged wind sensor or another may have low battery power).
The second method is more mathematical, fixing the robots to certain positions in a net of sorts that is then stretched to fit the targeted location. Magnus Egerstedt is working with Howard on this work allocation method.
In addition to location assignments, another key innovation of the SnoMote is its ability to find its way in snow conditions. While most rovers can use rocks or other landmarks to guide their movement, snow conditions present an added challenge by restricting topography and color (everything is white) from its guidance systems.
For snow conditions, one of Howard’s students discovered that the lines formed by snow banks could serve as markers to help the SnoMote track distance traveled, speed and direction. The SnoMote could also navigate via GPS if snow bank visuals aren’t available.
While the SnoMotes are expected to pass their first real field test in Alaska next month, a heartier, more cold-resistant version will be needed for the Antarctic and other well below zero climates, Howard said. These new rovers would include a heater to keep circuitry warm enough to function and sturdy plastic exterior that wouldn’t become brittle in extreme cold.
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f135bf6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Notice all the horrendous news about our environment? That’s a sure sign that the UN is about to throw another mega‐​gabfest where global leaders will shake their heads and shake down the U.S. for monies that Congress will wisely refuse to fork over.



Two weekends from now, the UN is holding its “Rio+20 Earth Summit,” the largest meeting in the history of an organization that pretty much does nothing but stage meetings. The 1992 Rio Summit produced the Framework Convention on Climate Change, which was the basis for the completely failed Kyoto Protocol on global warming. It also spawned Agenda 21, a document which outlined in great detail its plans to punish and pillage producer nations and transmit their wealth to the world’s great kleptocracies.



Rio 1992 was also the basis for 19 annual “Conferences of the Parties” to the Framework Convention, all of which succeeded in doing exactly nothing measurable about climate change. The most famous of these, after Kyoto in 1997, was in Copenhagen in December, 2009.



President Obama flew there, fresh with an “Endangerment Finding” from carbon dioxide hot off of his EPA’s presses. Because it was obvious that the Senate wouldn’t touch cap‐​and‐​trade, he needed something credible in order to goad the world into a new treaty to replace the dead Kyoto agreement. Despite being treated pretty roughly by Brazil, South Africa, China and India, he declared victory — with no specifics — as the meeting drew to a close. Obama couldn’t answer many questions, though, as he had to hightail it back to Washington to beat the first of that winter’s three blizzards. He didn’t, and the image of Air Force One landing in a blinding snowstorm will forever be the icon of the Copenhagen fiasco.



The great “success” of Copenhagen was an agreement that all the participants would submit plans detailing how they would reduce their dreaded greenhouse gas emissions in six weeks. Two weeks before that deadline, Yvo de Boer, Executive Secretary of the Framework Convention, announced that, never mind, we didn’t mean it, we don’t need your silly plans, and then he resigned.



Rio+20 is intended to go beyond all this. Failure is not an option, it is guaranteed.



While the agenda has yet to be finalized at this late date, it’s more of the same hand‐​wringing gloom and doom followed by more of the same outstretched hands. Not surprisingly, the same fault lines that have continually plagued the UN’s are emerging. Poor nations want our money. Europe agrees with this but they’re fresh out. Our Congress wants to be re‐​elected and won’t cooperate. India and China plead for special treatment.



But the list is longer than ever. In addition to climate change, we now have to remediate biodiversity loss, poverty, acid oceans (no such thing), poverty, “unsustainable consumption” (honest!), poverty, the right to food, poverty, and the “right to an adequate standard of living.” If much of this sounds like the wish list of your indolent teenager, that’s about right.



My academic pals are doing their level best to flog for the UN. Just this week, and, according to the _Christian Science Monitor_ , “timed for the Rio meeting,” _Nature_ published a remarkable screed by a team of twenty scientists forecasting the end of the world as we know it (literally) caused largely by increasing human population.



(Hint: a policy‐​driven piece authored by more than ten people, accompanied by a breathless press release, and published before a UN summit is known as a “petition.”)



If this sounds anything like the Club of Rome’s sophomoric 1972 “Limits to Growth,” it is. That forecast of the end of the world as we knew it by 2000 obviously failed, using the advanced methodology of the day (harmonic analysis and multiple regressions). The new paper by Anthony Barnosky uses a “fold bifurcation with hysteresis.” That’s impressive to all the UN delegates, most of whom avoided math and science in order to boss around mathematicians and scientists.



Actually, it really means a lagged discontinuous function, something you can find in honors Algebra II.



The 1972 and 2012 ends‐​of‐​the‐​world are simply the same shtick with the same tactics and objectives, namely abuse of authority to give authority to a global bureaucracy. Between then and now there have been literally dozens of such silly screeds. They obviously didn’t or won’t work, just like the 1992 Earth Summit and Rio+20.



If these people were serious about greenhouse gases and hot air, they would meet online. But they are not, not after 20 consecutive failures.
"
"

Got a person on your christmas list that is fully deserving but can’t find that lump of coal at K-Mart at the last minute? Thanks to the good folks at Free Carbon Offsets, you too can join the ranks of the carbon purified. Just visit: www.freecarbonoffsets.com  and you can print your own Carbon Offset Certificate suitable for a stocking stuffer for the most deserving person on your Christmas list.

Merry Christmas Everyone! 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1e71ffe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAccording to MSNBC, the Katla volcano in Iceland is about to blow her top – hat tip Joe Bastardi Joe Bastardi blog. Katla is the big sis of Eyjafjallajökull;I mean the really big sis. And according to an initial research paper by the University College of London Institute for Risk and Disaster Reduction:
Analysis of the seismic energy released around Katla over the last decade or so is interpreted as providing evidence of a rising … intrusive magma body on the western flank of the volcano.
and
 We conclude that given the high frequency of Katla activity, an eruption in the short term is a strong possibility.
A Katla eruption could be an order of magnitude greater than Eyjafjallajökull and possibly emit significant quantities of ash and sulphur particles into the stratosphere. Read the part about volcanoes at FOCUS warming will end. If that happens, then it’s a game-changer.
And look for a lot of uneasy alarmists to use it as a back door out of an increasingly embarrassing situation.
Share this...FacebookTwitter "
"
La Nina and Pacific Decadal Oscillation Cool the Pacific 

Click here to view full image (228 kb) 

 “The shift in the PDO can have significant implications for global climate, affecting Pacific and Atlantic hurricane activity, droughts and flooding around the Pacific basin, the productivity of marine ecosystems, and global land temperature patterns. ” – NASA JPL

       
A cool-water anomaly known as La Niña occupied the tropical Pacific Ocean throughout 2007 and early 2008. In April 2008, scientists at NASA’s Jet Propulsion Laboratory announced that while the La Niña was weakening, the Pacific Decadal Oscillation—a larger-scale, slower-cycling ocean pattern—had shifted to its cool phase. 
This image shows the sea surface temperature anomaly in the Pacific Ocean from April 14–21, 2008. The anomaly compares the recent temperatures measured by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) on NASA’s Aqua satellite with an average of data collected by the NOAA Pathfinder satellites from 1985–1997. Places where the Pacific was cooler than normal are blue, places where temperatures were average are white, and places where the ocean was warmer than normal are red.
The cool water anomaly in the center of the image shows the lingering effect of the year-old La Niña. However, the much broader area of cooler-than-average water off the coast of North America from Alaska (top center) to the equator is a classic feature of the cool phase of the Pacific Decadal Oscillation (PDO). The cool waters wrap in a horseshoe shape around a core of warmer-than-average water. (In the warm phase, the pattern is reversed).
See the entire story here:
http://earthobservatory.nasa.gov/Newsroom/NewImages/images.php3?img_id=18012
See the PRESS RELEASE from JPL here:
http://www.jpl.nasa.gov/news/news.cfm?release=2008-066
Look out California agriculture. The wine industry, fruits and nut growers will be hit with a shorter growing season and more threats of frost, among other things.
Recently in Nevada County, much of their grape crop was wiped out. From The Union in Nevada County (h/t Russ Steele)
Nevada County’s agricultural commissioner will seek disaster relief from the state after tens of thousands of dollars worth of crops were ruined from last week’s freezing temperatures.
Orchard trees, wine grapes and pastures were hardest hit, Pylman said. The commissioner is compiling a report of damages that he will send to the state Office of Emergency Services in coming weeks.
“Growers don’t have anything to harvest. That’s a disaster in my mind,” Pylman said.
 
In Paradise, CA, Noble Orchards reports damage to their Apple crop from recent colder weather, as well as reports of issue with vineyards in the Paradise ridge area suffering from frost damage recently.
Here is a short history of PDO phase shifts:
In 1905, PDO switched to a warm phase.
In 1946, PDO switched to a cool phase.
In 1977, PDO switched to a warm phase.
California agriculture has ridden a wave of success on that PDO warm phase since 1977, experiencing unprecedented growth. Now that PDO is shifting to a cooler phase, areas that supported crops during the warm phase may no longer be able to do so.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f7d202f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’ve spent a lot of time on this blog showing how badly maintained and situated the stations in the USHCN network are. And rightly so, the majority of them have issues. But, finding the good ones is actually more important, because they are the ones that hold the true unpolluted temperature signal. Unfortunately, the “good ones” are few and far between.
But when one comes along that is a real gem, it deserves to be highlighted. I present the USHCN climate station of record for Tucumcari New Mexico, COOP ID # 299156, located at the Agricultural Experiment station about 3 miles outside of the edge of town.
I “had” (he just moved to St. Louis) a nephew who lived in Tucumcari, and he just happened to be friends with the director of the experiment farm. Before my nephew left they both helped me get this survey done.

Click picture for additional images
Surfacestations.org image gallery link
This station has several advantages:

Length of continuous record – going back to at least 1946 at this location, possibly to 1905 but NCDC MMS metadata stops at 1946.
Length of continuous instrumentation – using mercury max/min thermometers
Length of continuous data record – there doesn’t appear to be any missing years
Lack of encroachment – 3 miles from the northeast edge of town, little development, little UHI. Tucumcari is well off the beaten path of development. Population actually declined 12% in recent years.
Good siting – the station rates a CRN2 due to distant trees and sun angle, and one small asphalt road 70 meters away.

See the station survey report here (PDF) You can also make out the station on Google Earth using this link. After opening Google Earth, zoom in and the fenced outline and screen will be visible.
Eyeballing, you can see that the temperature data trend for Tucumcari is slightly positive over the last century, about 0.5°C, but there is a “bump” in 2000, which brings it to about 0.9°C. This same bump appears in neighboring stations such as in San Jon (33km away) and in Boys Ranch (135km away). There is nothing in the metadata location or equipment record to suggest a reason for the bump. So, either the bump is naturally occurring, or there is something we don’t know about that changed in the local environment, or we have another data set splicing error like the GISS Y2K debacle from last year.

Click for larger graph from NASA GISTEMP
I plotted the data provided by GISS (which you can find here) to show the effect of the “bump” at year 2000 on the overall trend:

Click for larger graph
Here is the data plot after the GISS homogeneity adjustment, I’ve hue shifted my saved version to red to help keep the graphs visually separate:

Click for larger graph from NASA GISTEMP
And here is the overlay of the USHCN data from GISTEMP and the data from the GISTEMP homogenization process:

In this case, the GISTEMP homogenization code appears to do what would be reasonably expected; reduce temperatures in the present to account for population growth and UHI. I’ve pointed out more than a few times that the GISTEMP homogenization adjustment often becomes flawed for truly rural sites like this when there are large cities within the 250km up to 1200km (depending on process) adjustment zone that Hansen uses, that have accelerating UHI trends. Due to these cities, often the past of a rural station gets adjusted cooler, resulting in an increased temperature trend, such as what happens at Cedarville, CA. Hopefully we’ll have a detailed analysis of that adjustment from John Goetz soon.
If you look at this list, you’ll see that there are a lot of rural stations within 250km. Tucumcari has the advantage of being truly in the middle of nowhere when it comes to other big cities. The closest big cities are Amarillo and Lubbock, but as I understand the algorithm used, when they are near the edge of the 250 km zone, their weighted value decreases.
In this case though, the GISTEMP homogeneity adjustment doesn’t take Tucumcari’s declining population into account, it only uses nightlights, and while the population may dwindle, town infrastructure usually doesn’t; streetlights counted around the station likely remain.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e2be9d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Given the deluge of coverage, comment and analysis of the UK’s chaotic exit from the EU, it’s tempting to conclude that the exhausting subject of Brexit has been talked into the ground and analysed to death.   But that is far from true. Take the issue of the Irish border. Many keenly understand that creating a hard border between Northern Ireland and the Republic of Ireland would threaten the fragile peace that has held for two decades since 1998’s Good Friday Agreement.  Theresa May’s proposed withdrawal agreement – which goes to a parliamentary vote on December 11 – endeavours to avoid a hard border if the UK and EU cannot reach an alternative agreement, by temporarily aligning Northern Ireland with some EU single market rules, and the UK as a whole with EU customs rules. Before the Good Friday Agreement, the border was heavily fortified and crossings were tightly controlled. Its post-1998 incarnation is more than just an improvement in practical terms; it has come to symbolise progressive peace and openness in a place marked by violence and militarised division. The border’s uncertain future in light of Brexit has attracted a tremendous amount of attention – and rightly so, given its enormous importance. But Brexit’s cyclops eye struggles to look meaningfully at more than one dimension of an issue at a time, and other areas that are crucial to people’s everyday lives are being neglected. One key issue concerns the potentially damaging impact of Irish border divisions on Ireland’s all-island energy market. Northern Ireland and the Republic of Ireland operate an Integrated Single Electricity Market (ISEM), which covers most energy generation on the island. This bespoke market is regulated by a special committee composed of northern and southern regulators, and overseen by a Single Electricity Market Operator (SEMO). This groundbreaking set up was created in 2007 by the then Northern Ireland secretary, Peter Hain, during a period of direct Westminster rule, after devolution had collapsed. Working in tandem with the Irish government, Hain’s team drew the northern and southern electricity markets together into a single all-island market, with most of the island’s electricity bought and sold through one overall market. A two-party legislative framework locked the ISEM in place on either side of the border, supported by a “memorandum of understanding” between the two governments.  Energy is a devolved matter under the UK’s constitutional arrangements, and when devolution was restored in 2007 the reins were passed to the Stormont administration, which continued to operate the electricity market jointly with the Irish government. The Democratic Unionist Party’s alleged mismanagement of a renewable heat incentive scheme has since collapsed the devolved institutions once more, and so the Northern Irish elements of the ISEM are currently being overseen by Belfast civil servants until the situation can be resolved.   The ISEM improved on the conditions that preceded it, increasing competition and dampening consumer prices. It has also bolstered north-south energy security, and has had positive effects on energy efficiency and integrating renewable energies. But Brexit is currently pulling this UK-Ireland innovation in opposite directions. A target model has been issued by the EU that requires member states such as the Republic of Ireland to strive for greater energy integration, whereas the Northern Irish/UK momentum is disengaging from the EU initiative as a consequence of Brexit. Meabh Cormacain, manager of the Northern Ireland Renewables Industry Group (NIRIG),  offered insightful comment on this crucial issue, suggesting that the Irish border might function as an “electric fence”, cutting the ISEM market in two if Brexit is not carefully managed. Consumers on both sides will bear the brunt of any diplomatic failure because it will likely increase their energy costs over time. It has been pointed out that these difficulties might be solved if Northern Ireland continues to operate as a distinct zone within the UK that remains largely subject to EU law in this area post-Brexit, meaning Northern Ireland would continue to be in regulatory alignment with the republic. May’s withdrawal agreement by and large adopts this approach. While sticking with this type of collaborative approach clearly solves problems, I suggested in a recent lecture to the Irish energy industry that it seems undemocratic to subject Northern Irish citizens to significant EU energy laws while depriving them of the democratic mechanisms to influence those laws – bearing in mind Northern Ireland will have left the EU along with the rest of the UK. At any rate, the overall principle is clear: the ISEM is a novel, bespoke energy market specific to the island of Ireland, and so its preservation and maintenance will require a similarly novel, bespoke set of agreed UK-EU arrangements for the post-Brexit period. May’s proposals may achieve this in practice, but it seems unlikely that the UK parliament will vote in favour of her deal as a whole. The Irish and UK governments support the protection and continuation of the ISEM, as does the European Commission. With the Brexit clock ticking down to the make-or-break parliamentary vote in December, the sooner the parties responsible can agree on the specific policies that will underpin arrangements for the future, the better. Markets need certainty – and so do the people of Northern Ireland."
"

There’s nothing wrong with your computer monitor, do not attempt to adjust the picture.
Normal blogging will resume shortly.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea39aedd2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The Cato Institute is committed to holding seminars throughout the country each year, on the basic premise that important questions of policy and governance should not be confined to Washington. At the Policy Perspectives 2012 event in New York City in March, Steve Forbes, chairman and editor‐​in‐​chief of Forbes Media, focused on the economic headwinds holding the country back. The true source of prosperity, he said, is the free market — and these barriers are standing in the way. Mary Anastasia O’Grady, an editorial board member of the _Wall Street Journal_ , detailed the intellectual roots of underdevelopment in Latin America — and, more importantly, the lessons they hold for economic growth in the United States.



 **Steve Forbes** : We all know, even though the economy is doing better this year, that we are still witnessing a punk performance. It’s like being on a superhighway and getting the automobile to go from 20 miles an hour to 40. It should be going 70 or 75 — and, when no one is looking, 125. But something is inhibiting us. I’d like to take a few moments to touch upon the major headwinds that are pushing us back, at a time when we should be moving forward rapidly.



The first, of course, is monetary policy — possibly the most boring subject in the world. The best way to think about monetary policy is to continue the above metaphor. Let’s say you have an automobile, a magnificent vehicle in its own right. If you don’t have sufficient fuel for that car, you’ll stall. With too much fuel, you’ll flood the engine. Only with the right amount do you have the chance to move forward.



The same is true of an economy, even one that has all of the basic strengths. If you don’t supply enough money to meet the organic needs of the marketplace, you’ll stall. If you print too much money, you get the economic equivalent of a flooded engine. The Federal Reserve has been on a bender recently, printing excess money ever since the early part of the last decade. We never would have had a housing bubble if the Fed had not provided the juice for it. Money, of course, is not created by government. It is generated by individuals who make transactions in the marketplace.



Money is simply a facilitator of those transactions. Before money, we had to barter — which means that if I wanted to sell an ad in _Forbes_ , I might have to accept a herd of goats as payment. If I then wanted to turn around and purchase iPads for our writers, I’d have to be prepared for the Apple storeowner to demand sheep rather than goats, which would then take me to a sheep herder, who may prefer red wine to my white wine … . and so on. Simply put, commerce becomes cumbersome.



With money, however, it’s much easier to create capital, invest in the future, and bring that future into the present. The key, however, is that money has to be stable in value, like ounces in a pound or inches in a foot. Can you imagine what would happen if Washington did to the hour what it does to the dollar? Sixty minutes in an hour one day, 42 minutes the next, 77 the following … you would soon need hedges, derivatives, and futures to figure out how many hours you’re working. If you hire someone for $15 an hour, you’d have to specify if that’s a New York hour, an Illinois hour, or a Bangladesh hour. The key is simplicity.



The way to do this is to re‐​link the dollar to gold. For all its imperfections, the question is: What works better, gold or politicians? Without a stable currency measure, people invest in existing hard assets — which results in capital flight from this country because people are not willing to take risks. Equity prices are supposed to reflect the present value of discounted future income flows. But if you don’t know what the income flows are denominated in, that depresses values today. Stability works. Make money a fixed measure of value. Period.



Another headwind that has been holding us back, which Cato has been at the forefront of, is taxes. What the mandarins in Washington don’t understand is that taxes are a price and a burden. A tax on income is the price you pay for working. A tax on capital gains is the price you pay for taking risks.



A tax on profits is the price you pay for success. And the idea is a simple one: when you lower the price of good things like productive work, risk taking, and success, you get more of them. When you raise the price, you get less. Yet the political class keeps trying to raise taxes to solve their own spending problem.



The current tax code, though, goes way beyond raising revenue. It is ultimately a source of power and manipulation, and the biggest source of lobbying in Washington. Last year, we spent 6.5 billion hours filling out tax forms — the equivalent of three million full‐​time jobs. The code has been changed more than 14,000 times since 1986 and, no matter what your faith, it is beyond redemption. The only thing to do with this monster is drive a stake through its heart, bury it, and hope it never rises. We need to start all over.



I am personally in favor of a flat tax: a single rate, with generous exemptions for adults and children, and no federal income tax on the first $46,000 for a family of four. Beyond that, the rate should be 17 percent with no taxes on savings or death. We should be able to leave this world unmolested by the IRS — or, as the Founders would say, no taxation without respiration. The same rules should apply on the corporate side. If you do that, the dollar is as good as gold.



The next barrier to growth, not surprisingly, is spending. Of course, it’s not just that public spending is wasteful. It’s ultimately a source of power. What the government does is, it takes resources from you, puts it through the sausage factory, takes a cut, and then spits it out to politically anointed recipients. That’s not stimulus; it’s stagnation. One reason the current president doesn’t like achieving real reductions in spending is because he knows, given his background, that it is a source of power.



The more resources you control, the more power you have at the center. John Maynard Keynes, after all, said it doesn’t matter whether you dig a hole and fill it up again — it doesn’t matter where the money goes — as long as you have that power. It’s about power, which is why they love it when the government goes from 20 percent of GDP to 25 percent. The more the better.



Regulations are another form of taxation, a burden on the economy. The cost of complying with regulations each year in this country is $1.75 trillion. The regulatory state, in other words, is bigger than every economy in the world except for the top three or four. We do that to ourselves with regulations.



James Madison, the father of the Constitution, wrote in _Federalist 51_ : “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.” We need sensible rules for the road. We need speed limits in school zones, for instance — but this is very different from the government telling you what, where, and when to drive.



C. Northcote Parkinson was a British naval historian and author of the bestseller _Parkinson’s Law_. Back in the 1920s, he noticed that the British Navy was sharply downsized after World War I, when they thought they weren’t going to have any more wars to fight. They had far fewer ships, far fewer crews, and far fewer dock workers. But what Parkinson noticed in particular was that the Admiralty, which ran the navy, was bigger than it was when the war ended! And he concluded that organizations grow — like weeds — until somebody stops them, no matter what may be the work at hand.



The same is true of all organizations. They all, if you leave them alone, lose sight of why they were originally created. But private markets serve as a check on this tendency. If you expand to fill the allotted time, without meeting the needs and wants of the market, you fail. Why, then, is the FDA still testing based on a model from 60 years ago — a period during which there have been countless major advances in medicine? Why are they letting countless cancer patients lead shortened lives by refusing to approve medications? It’s a power play, in a sea of cumbersome rules, with no incentive to streamline the process.



Ronald Reagan was right. He said that if you want to change minds in Washington, the best way to do it is through the heat of public opinion. It is not enough to have a change at the top and get a few new faces on Capitol Hill. Ideas matter — and we need to make the case that free enterprise works. Markets create social trust. Government destroys it. In the real world, even if you lust for money, you don’t get it unless you provide for others — and, without you knowing it, that creates circles of cooperation. Markets force you to look toward the future. That’s what the Cato Institute understands, and that’s the mentality that we must encourage to get others to understand as well.



 **MARY ANASTASIA O’GRADY** : Many of you are no doubt wondering what Latin America could possibly teach the United States — what with our muscular Constitution, open markets, limits on federal power, and independent central bank. (No snickering, please.) I was once like you. But, in the last few years, I have seen a number of frightening parallels between this country and our neighbors to the south. To be clear, those parallels did not begin with this president, but they have certainly become more pronounced under the current administration.



The fashionable explanation for Latin American underdevelopment blames corruption, lack of education, poor infrastructure, and — my personal favorite — a shortage of money. But these things are symptoms of bad policies, which I sum up as the Three Ps of poverty: populism, protectionism, and prohibition. Our challenges are, how do we keep politicians from turning us into government dependents? How do we keep markets open? How do we change drug laws in a way that prevents organized crime from replacing democratic institutions? Yet, I’m increasingly convinced that, just as corruption and poor infrastructure are byproducts of the Three Ps, so are the Three Ps byproducts of something else. The source of our economic troubles — in both Latin America and here — is, I believe, much more fundamental.



Consider two simple observations. First, to borrow a fundamental principle of the Cato Institute, ideas matter. To be more specific, those ideas that prevail in society as legitimate are what matter. And second, without entrepreneurship, it is impossible for a society to achieve prosperity.



Looking beyond the immediate policy challenges in Latin America, it becomes clear that it is the ideas of academia — and of intellectuals more broadly — that have played the most important role in undermining the entrepreneurial spirit in Latin American over the past century. Ideas that are hostile to entrepreneurship are not only part of the popular culture, they are embedded in the basic institutions of these countries.



At their core, these ideas hold that profits are morally suspect and private property is not justified, and it is these ideas that strike directly at the heart of prosperity for hundreds of millions of Latin Americans. How did this happen? As John Maynard Keynes wrote, “The ideas of economists and political philosophers, both when they are right and when they are wrong, are more powerful than is commonly understood. Indeed the world is ruled by little else.



Practical men who believe themselves to be quite exempt from any intellectual influence are usually the slaves of some defunct economist. Madmen in authority” — we won’t mention names — “who hear voices in the air are distilling their frenzy from some academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas.” This is a truism that Latin Americans did not understand until it was too late — and it is how we too will lose if we don’t emphasize the moral case for the market. Latin Americans, of course, have no problem being entrepreneurial.



Immigrants to the United States have a long history of starting their own businesses once they’ve landed. So why don’t they display these same skills at home? I submit to you that it is because the dominant ideas in the region over the last century have been hostile to entrepreneurship.



In a new book entitled _Redeemers: Ideas and Power in Latin America_ , the Mexican historian Enrique Krauze profiles 12 individuals who he believes represent the major political ideas in the region from the middle of the 19th century through the 20th.



He starts with José Martí and ends with Hugo Chávez — and along the way he includes profiles on Eva Perón, Che Guevara, Octavio Paz, Gabriel Garcia Márquez, and Bishop Samuel Ruiz, among others. These individuals, Krauze argues, were the ones who sowed the dominant political ideas over that time period. And those ideas focused on hostility toward individualism. Collectivism, economic equality, and the socialization of risk were the chosen themes of political philosophy — and it was the dissemination of these ideas that molded the norms and values of their respective countries. Not one name listed here, by the way, is an entrepreneur. I should add that Krauze also includes Mario Vargas Llosa in the group. He is not a collectivist but he is the exception to the rule.



The power of ideas was well understood among intellectuals on the left throughout the 20th century. They made it their business to get control of academia, and they succeeded. Take for example Venezuela, where the left got total control of the universities, and in the classroom a new narrative emerged. It gave the moral high ground to the state and denounced the market as immoral. Venezuela is reaping the fruits of that indoctrination today. Millions of Latin American students around the region have been marinated in that same stew. This view — that government redistribution is the source of justice and the market is greedy and wrought with failure — has had a profound effect on the political and economic climate in the region.



Today, the ideas of Che Guevara and Eva Perón have been discredited. Modern socialists — those who reject communism and fascism but favor some other form of collectivism — do not attack private enterprise head on. That would be suicidal because the market has created so much prosperity. They therefore emphasize not the wealth of nations, but the immorality of inequality. This, for socialists, is the soft underbelly of the market.



In societies where the morality of the market is understood, vigorously defended, and imparted to young minds, the ethics of collectivism doesn’t do very well. But Latin America shows what can happen when the market is not defended. Even in a society that has made economic gains by adopting free‐​market policies, if the population is not convinced of the legitimacy of the market, it will attempt to destroy what it has achieved.



Take Chile, where since last year students have been running wild in the streets, making all kinds of demands from their government, and accusing those who don’t give in of immorality. The tragedy is that the country’s establishment — including the president — has not been able to put up a strong defense. This is in Chile, the one place in the region that actually reduced poverty significantly. We should be thankful for scholars like José Piñera for carrying the torch of liberty in Chile. But the fact remains that while Chileans are beneficiaries of the market system, they don’t seem convinced of the morality of private property — and of differing outcomes.



Outside of Chile, things are even worse. In most of the region, the idea that equality is the highest goal was handed down from the ivory towers and enshrined in the constitutions themselves. Latin American constitutions are hundreds of pages long. They have objectives like guaranteed national development, the eradication of poverty, and the protection of cultural heritage. The 1988 Brazilian constitution offers constitutional rights for everything from health to education. It guarantees minimum salaries, yearend bonuses, and vacation pay. The section dedicated to sports specifies that “the government shall encourage leisure as a form of social promotion.”



Of course, who can object when the goal is to make the poor child more equal to the wealthy entrepreneur? The problem with a constitution that guarantees equality of outcome is that it cannot protect individual rights. It gives the government not only the power, but the obligation to use coercion toward that end. The fundamental problem with Latin development is this lack of liberty, which emanates from constitutional mandates that intrude on every aspect of human action.



What I’m describing originates with the intellectual class, of course, but many of these bad ideas in Latin America gained influence because the business class supported them. The 1961 Venezuelan constitution was, by most accounts, a fairly sound document. But factions, as James Madison would have called them, began to pick it apart. The business community played a key role.



Venezuelan journalist Carlos Ball described the process like this: “Many in the business community did not rebel against the growing state intrusion because they saw it was easier to convince one cabinet minister than a market of consumers. I’ll never forget watching Venezuelan businessmen cheering the nationalization of foreign oil companies, not realizing that the politicians would soon come after them with more controls, regulations, and taxes.”



The lesson is that when the state seizes the moral high ground in matters of personal decisions, there is no end to the steps that it will take to restrain liberty in the name of social justice. Our neighbors to the south have demonstrated it. You may think this can’t happen in the United States of America. Unfortunately, I am nowhere near as convinced.
"
"My wildlife friends and I often talk about what species we would bring back from extinction. I am torn between the dodo and the thylacine, also known as the Tasmanian tiger. This was once a speculative, sci-fi debate but not anymore. Ever since Dolly the sheep was cloned, conservation biologists have muted the idea and the process of de-extinction – bringing back dead species – is coming closer to reality.  De-extinction can be achieved by one of two means: selective breeding or cloning.  In the selective breeding method we try to re-create extinct species such as the aurochs (extinct large cattle from Europe and Asia) by looking for their surviving genes among existing cattle and breeding animals to favour these genes. Then you compare the genome of the resulting animals with that for aurochs until you have what is genetically an auroch.   The second method essentially involves finding the DNA of an extinct species and inserting it into a recipient egg cell and recipient animal – the cloning process. This second process is limited to species that have gone extinct more recently (hundreds of years) because you need to find intact DNA, so I am afraid there will be no Jurassic Park. We would also need to find DNA from several different individuals otherwise we would end up with problems due to inbreeding such as those seen in white tigers. Unsurprisingly the “instant fix” of cloning has received more interest as it would not depend on many generations of captive breeding. A wide range of species have been suggested for cloned de-extinction from the dodo to the woolly mammoth.  Initially, I liked the idea. I’d love to see a dodo in a zoo or even better to see wild woolly mammoths on an ecotourism trip to the steppes of Siberia. But such meddling raises a host of questions. For instance, an African elephant would be the obvious recipient for woolly mammoth DNA. But as mammals learn a considerable part of their behaviour from their parents and peers – are we not just creating an elephant in mammoth’s clothing? It would, therefore, seem our resurrected animals would need some kind of training to survive in the wild, which may not be unlike the survival training reintroduced zoo animals already receive. If a species was successfully reintroduced and its population grew to previous levels it would have a major ecological impact. The animals which have occupied its ecological space may find themselves squeezed out. Governments would, rightly, be very cautious about the reintroduction of such animals. Given the limited money available for wildlife conservation it’s not clear that the expense of bringing back the dodo makes sense. A simple utilitarianism would suggest not; the cost of resurrecting the dodo could be used to save many other living species from extinction. For example, it now appears that a cloning approach may be the only solution to save the northern white rhinoceros from extinction – there are now only five individuals left. However society, thankfully, does not always run according to such utilitarian analyses. So perhaps the dodo will have its day – even if that is just living in a zoo. It may behave like a farmyard chicken but it would still be a powerful symbol for species conservation: I suspect some zoos would be shedding their giant pandas to go into dodos. But what kind of symbol would a living dodo be? It can no longer be the symbol of extinction; the Rubber Dodo Award for people who have contributed most to species extinction would need to be renamed. It would be testimony to how far science has come and how far science can take us.   But this sense of scientific wonder isn’t always helpful. A living dodo would give out the wrong message to society and politicians – we can destroy anything we like and scientists will eventually find a way to fix it. This seems, for example, to be the hope with climate change. As a species I think we need to accept responsibility for what we have done to this planet and not have blind faith that in the future scientists will fix all of our mistakes.  We need to live with our mistakes and learn from them. It is for this reason I am not wishing for de-extinction."
nan
"

From Slashdot.org The Wall Street Journal has a sobering piece describing the research of
medical scholar John Ioannidis, who showed that in many peer-reviewed research
papers ‘most
published research findings are wrong.’ The article continues: ‘These flawed
findings, for the most part, stem not from fraud or formal misconduct, but from
more mundane misbehavior: miscalculation, poor study design or self-serving data
analysis. […] To root out mistakes, scientists rely on each other to be
vigilant. Even so, findings too rarely are checked by others or independently
replicated. Retractions, while more common, are still relatively infrequent.
Findings that have been refuted can linger in the scientific literature for
years to be cited unwittingly by other researchers, compounding the errors.’


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3ddae1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I was forwarded a slide show presentation done by Thomas Lowell et al of the University of Cincinnati titled: Organic Remains from the Istorvet Ice Cap, Liverpool Land, East Greenland: A Record of Late Holocene Climate Change
It was presented last week at AGU’s Greenland Climate Change Past and Present session. It has some very interesting data in it. In summary it has a report on occurrence of subfossil organic remains, with organics recovered in locations presently void of plant growth.

Picture of Istorvet organic remnants at edge of glacier melt.
The preliminary conclusion from the data collected in the field work is that presently the small ice caps at high latitudes in Greenland are retracting to locations where they were at 1000 years ago.  The presence of subfossil vegetation was found within 280 vertical meters of ice cap summit and where comparable modern assemblages do not exist. The implication seems to be that there were warmer periods in these areas prior to today, warm enough for plant growth.
According to the study, the organic material in Liverpool Land radiocarbon dates from 400 to 1015 AD. It is interesting to note that the Vikings settled in Greenland around 974 AD and the study indicates that ice cap expansion began around 1015 AD.

While the UC team that did the field work still has more work to do to reconstruct temperatures from this data, the study lends support to the idea that Greenland’s climate was warmer approximately 1000 years ago. One of the organic samples recovered at another location was dated to 910BC. This makes one wonder just how often shifts in Greenland’s climate occurs.
More study is needed, but this is certainly interesting. You can view the abstract here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea222cbf0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I was initially concerned that my stats were down this month, then I remembered that April has 30 days and March has 31.

Of course there’s that nice spring weather, and I recall that TV station ratings suffer a drop during the spring since people are digging out from their winter igloos. But even though I broke even, there was a nice surprise at the end of the month, WordPress put me as the top “hawt” post, even if only for awhile:

It was a nice way to end the month, thanks to all my readers for your help in getting me to NCDC @ Asheville and for the continued patronage!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f61a903',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This from www.Spaceweather.com
As January comes to an end, sky watchers in Scandinavia are recovering from a veritable storm of nacreous clouds. After mid-month, hardly a night went by without someone spotting the phenomenon. “It was incredible! They were all over the sky,” says Morton Ross of Oslo, Norway. This picture, taken by Ross on Jan. 25th, shows a typical apparition:

Also known as “Mother of Pearl” clouds, nacreous clouds are peppered with tiny ice crystals that blaze with iridescent color when struck by light from the setting sun. It is these crystals that make nacreous clouds so rare: they require exceptionally low temperatures of minus 85 Celsius (-120 F) to form. Icy nacreous clouds float 9 to 16 miles high, curling and uncurling hypnotically as they are modulated by atmospheric gravity waves.
For much of January, these clouds rolled across the Arctic circle with puzzling regularity. Why the sudden abundance? Is the show over? No one knows. Stay tuned for February!
For more, see the 2008 Nacreous Cloud Gallery For the science behind nacreous clouds, please see this entry in Atmospheric Optics.
As for temperatures at high latitudes, its -35°F in Saskatoon at the surface this morning, so there’s a chance we’ll see more nacreous clouds in days ahead.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea13cef97',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMany news outlets have reported today that humans hunted and killed off the large methane-emitting mammoths 13,000 years ago, thus causing global cooling. This report has been published in Nature by some University of New Mexico scientists. Again man, armed with nothing but spears and arrows, may have been the culprit in significant climate change. Read here for example. Here, the Telegraph reports:
But by 11,500 years ago, around 80% of these big mammals had vanished forever.
Their disappearance, accounting for more than 114 lost species, came within 1,000 years of the arrival of humans in the New World.
So the mammoth killing ended at about 11,500 years ago? Wouldn’t that mean that it should have gotten colder from then on as a result? Let’s take a look at the temperatures over the last 20,000 year or so.

This graphic is taken from: oceanworld.tamu.edu.
At about 11,500 years ago the Ice Age ended! With all those big inefficient herbivores disappearing and the millions of tons of methane along with it, wouldn’t the hypothesis suggest that the Ice Age would have deepened, and not ended? Using AGW logic, less methane means more cooling, which leads to more ice, which then leads to more albedo, more cooling, more ice…you know an irreversible tipping point into a permanent ice age. But the opposite happened!
To me this stinks of more junk science by a journal desperate to rescue a science that’s quickly going the way of the mammoths: to extinction. This will be debunked in a matter of days.
Update: The following graphic from Jeff Masters Weather Underground shows the temperature for the last 100,000 years. Note all the spikes during the period. Why are the Younger Dryas caused by man and all the other dips not? Clearly the graphic shows that climate is always changing, often wildly. Belching mammoths were not the drivers.

Share this...FacebookTwitter "
"Congratulations for your thorough analysis of the environmental impacts of High Speed 2 (Will HS2 help the climate – or will it make things worse?, 3 February; Barn owls, bats, butterflies, birds – all now at risk, 3 February). The escalating costs of HS2 call into question the way the UK designs and evaluates big infrastructure projects. We should take much more account of the impact on development and land values, especially if the uplift were shared with investors. Spending £3bn on land acquisition to date is surely wasteful. But little can be done without reintroducing regional and strategic planning. Before we turn our backs on Europe, we should learn from how the French succeeded in growing their towns and economies while keeping down car use and pollution. The crucial difference is not going for one grand national project or another, but joining up investment in development and infrastructure at a city region level, as a comparison of Lyons or Lille with Leeds would show.  So let’s start by revisiting attempts to come up with regional or sub-regional plans and ask how much of the costs could be recovered from the uplift in land values. The late Sir Peter Hall proposed a relatively economic way of improving rail services, later named HS3. This would use a series of improvements, such as better signalling and junctions, not one big project, which is almost bound to go wrong. Such a policy would produce faster results, using public investment to secure multiple benefits, not just faster long-distance journeys.Dr Nicholas FalkExecutive director, The Urbed Trust • At last the necessity for HS2 is being steered away from the need to get from London to Birmingham a bit quicker (The case for, 4 February; The case against, 4 February). The essential issue is capacity. If the public wants fewer heavy trucks on the roads then additional rail is the only answer. There are virtually no spare paths between London and the Midlands on the west coast mainline: £8bn-£10bn was spent in 2008-10 upgrading that line, but there is now nothing spare. A new line is needed, so why would you build a slow one? The data is regularly published in Modern Railways, but it is apparent that politicians and their advisers don’t read the industry journals. In all the furore about rising costs, politicians never ask why it is four times more expensive per mile compared with the LGV Est line in France. Perhaps land prices are lower in France, and each new LGV line has been dovetailed into existing termini in Paris, which might explain some of the lower costs. But really, four times as much here? Who’s making a killing? Finally, why are the arguments presented as either we spend on HS2 or upgrade areas in other parts of the country? Why can’t it be both?Robert AshleyLondon • Hurrah! At last you have hit the nail on the head and made the clear and coherent case for HS2 (Editorial, 6 February). Taken together with enhancement and integration of local networks to ensure maximum access to HS2, there is little doubt that the new line will more than pay for itself and stimulate development across the Midlands and north. There’s already been digging aplenty in Birmingham. More please, Mr Johnson, and soon.Roy Boffy Sutton Coldfield, West Midlands • Here are some other ways to spend £106bn: 15 tram systems like Manchester Metrolink £42bn, free bus services for everyone for 10 years £30bn, free ebike for every adult £34bn. Well-funded local transport for everyone is what we require.Edward FieldingShipley, West Yorkshire  • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"When I dropped my kids off at childcare just a few weeks ago, the air was so dangerous that the warning on my phone showed someone in a gas mask. As I stood on the burned-out property of Nick Hopkins in Malua Bay last month, he summed it up perfectly, feeling two parts shattered and three parts angry.  Australians are angry and anxious because the government clearly doesn’t have the climate emergency under control and has no plan to get it under control. But people are also angry and anxious because the basics of life are no longer guaranteed. Study hard and do Tafe or university and you get underemployed in an insecure job with low pay. You get a job and then find you can’t afford a house because the government has rigged the housing market against you. We have a climate and environment emergency, an inequality crisis and a jobs crisis, and the government’s only answer is: “get used to it, because it is the new normal”. Well, I refuse to adapt to kids wearing gas masks. I refuse to accept a world where people put off having kids because they are feeling so insecure about their jobs and their future. I refuse to accept people living below the poverty line in a country as wealthy as ours. And I refuse to accept the dismal standard of this rotten government led by Scotty from marketing, a man whose love of coal has helped drive the climate crisis that has made these fires worse, and a Labor so-called opposition that celebrates coal in the middle of bushfires and votes with the Liberals to give tax cuts to millionaires. We are a smart and wealthy country and if we have the guts to take on the big corporations and the weak politicians they have in their pocket, we can solve these crises. That is why we need a Green New Deal. A Green New Deal is a government-led plan of investment and action to build a clean economy and a caring society. The two elements of a Green New Deal – government taking the lead to create new jobs and industries, and universal services to ensure no one is left behind – are the values I have been fighting for my whole adult life. I joined the Labor party at high school, but left in university because the ALP started making education so expensive and putting people in debt. As a lawyer, I fought big corporations on behalf of clothing outworkers and represented firefighters as well as coal workers dealing with privatisation. And in my seat of Melbourne we have brought people together, from public housing tenants to young families with a mortgage. That’s what a Green New Deal will do too, because it solves the big challenges our country faces and provides a hopeful vision for the future that the whole country can support. While this has been a summer of complete devastation for our country, I remain hopeful that a better, fairer, safer world is possible. By fighting for a Green New Deal, we can not only address the challenges we face today, we also have the best ever opportunity to build a more just future. Throughout history, change has come when everyday people stand up and make their voices heard. The conservatives will do everything in their power to avoid tackling the climate emergency and the jobs and inequality crisis, so it’s up to us to shape the country we want to see. To build a future for our kids and grandkids and the generations to come after us. Change is possible. In 2010, with the Greens in balance of power, we delivered climate legislation and the carbon price, bringing down pollution for the first time in Australian history. With a Green New Deal, we can deliver a manufacturing renaissance, turning Australia into a renewable energy superpower exporting our clean energy to the world. At the same time, as Ross Garnaut has proposed, we can process our resources and minerals in Australia and attract new business investment because of our abundance of solar energy. And while the Greens were able to achieve dental into Medicare for 3.4 million children, with a Green New Deal we can ensure all dental is fully included under Medicare. We can make public schools genuinely free, rather than parents being lobbed with hundreds of dollars of additional fees every year. This is the change I believe our country needs. This is the country I know is possible and this is the change I’ll be fighting for. I hope all Australians will join us in this fight. • Adam Bandt is the leader of the Australian Greens"
nan
nan
"

The additional power that is being granted to experts under the Obama administration is indeed striking. The administration has appointed “czars” to bring expertise to bear outside of the traditional cabinet positions. Congress has enacted sweeping legislation in health care and finance, and Democratic leaders have equally ambitious agendas that envision placing greater trust in experts to manage energy and the environment, education and human capital, and transportation and communications infrastructure.



However, equally striking is the failure of such experts. They failed to prevent the financial crisis, they failed to stimulate the economy to create jobs, they have failed in Massachusetts to hold down the cost of health care, and sometimes they have failed to prevent terrorist attacks that instead had to be thwarted by ordinary civilians.



Ironically, whenever government experts fail, their instinctive reaction is to ask for more power and more resources. Instead, we need to step back and recognize that what we are seeing is not the vindication of Keynes, but the vindication of Hayek. That is, decentralized knowledge is becoming increasingly important, and that in turn makes centralized power increasingly anomalous.



 **THE AGE OF THE EXPERT**



Populists often make the mistake of bashing experts, claiming that the “common man” has just as much knowledge as the trained specialist. However, trained professionals really do have superior knowledge in their areas of expertise, and it is dangerous to pretend otherwise.



I have faith in experts. Every time I go to the store, I am showing faith in the experts who design, manufacture, and ship products.



Every time I use the services of an accountant, an attorney, or a dentist, I am showing faith in their expertise. Every time I donate to a charity, I am showing faith in the expertise of the organization to use my contributions effectively.



In fact, I would say that our dependence on experts has never been greater. It might seem romantic to live without experts and instead to rely solely on your own instinct and know‐​how, but such a life would be primitive.



Expertise becomes problematic when it is linked to power. First, it creates a problem for democratic governance. The elected officials who are accountable to voters lack the competence to make well‐​informed decisions. And, the experts to whom legislators cede authority are unelected. The citizens who are affected by the decisions of these experts have no input into their selection, evaluation, or removal.



A second problem with linking expertise to power is that it diminishes the diversity and competitive pressure faced by the experts.



A key difference between experts in the private sector and experts in the government sector is that the latter have monopoly power, ultimately backed by force. The power of government experts is concentrated and unchecked (or at best checked very poorly), whereas the power of experts in the private sector is constrained by competition and checked by choice. Private organizations have to satisfy the needs of their constituents in order to survive. Ultimately, private experts have to respect the dignity of the individual, because the individual has the freedom to ignore the expert.



These problems with linking expertise with power can be illustrated by specific issues. In each case, elected officials want results. They turn to experts who promise results. The experts cannot deliver. So the experts must ask for more power.



 **JOB CREATION**



With the unemployment rate close to 10 percent, there is a cry for the government to “create jobs.” But the issue of job creation illustrates the increasingly decentralized nature of the necessary knowledge.



A job is created when the skills of a worker match the needs of an employer. I like to illustrate this idea using an imaginary game in which you draw from two decks of cards, one of which contains workers and one of which contains occupations. For example, suppose that you drew “Arnold Kling” from the deck of workers and you drew “fisherman” from the deck of occupations. That would not be a good match, because my productivity as a fisherman would be zero.



You could do worse — my marginal product as an oral surgeon would be negative. However, you could do better if you were to draw an occupation card that said “financial modeler” or “economics teacher.” One hundred years ago, if you had played this game, you had a good chance of finding a match just by picking randomly. Most jobs required manual labor, and for most people manual labor was the most productive use of their working hours.



Today’s work force is more highly educated and more differentiated. As a result, the task of creating jobs requires much more knowledge than it did in the past. A New Deal program like the Public Works Administration or the Civilian Conservation Corps would not have much appeal for a recent law school graduate or laid‐​off financial professional.



Production today is more roundabout than it was 50 years ago. Only a minority of the labor force is engaged in activities that directly create output. Instead, a typical worker today is producing what George Mason University economist Garett Jones calls “organizational capital.” This includes management information systems, internal training, marketing communications, risk management, and other functions that make businesses more effective.



When production was less roundabout, there was a tight relationship between output and employment. When a firm needed to produce more stuff, it hired more workers.



Today, additional demand can often be satisfied with little or no additional employment.



Conversely, the decision to hire depends on how management evaluates the potential gain from adding new capabilities against the risks of carrying additional costs. The looser relationship between output and employment is implicit in the phrase “jobless recovery.” So how does the economy create jobs? There is a sense in which nobody knows the answer. In his essay, “I, Pencil,” Leonard Read famously wrote that not a single person on the face of this earth knows how to make a pencil. Pencils emerge from a complex, decentralized process. The same is true of jobs.



What the issue of job creation illustrates is the problem of treating government experts as responsible for a problem that cannot be solved by a single person or a single organization.



Economic activity consists of patterns of trade and specialization. The creation of these patterns is a process too complex and subtle for government experts to be able to manage.



The issue also illustrates the way hubris drives out true expertise. The vast majority of economists would say that we have very little idea how much employment is created by additional government spending. However, the economists who receive the most media attention and who obtain the most powerful positions in Washington are those who claim to have the most precise knowledge of “multipliers.”



 **HEALTH CARE**



Despite the many pages contained in the health care legislation that Congress enacted, the health care system that will result is for the most part to be determined. The design and implementation of health care reform was delegated to unelected bureaucrats, as was done in Massachusetts.



In Massachusetts, the promises of propo‐​nents have proven false, and the predictions of skeptics have been borne out. Costs have not been contained; they have shot up. Emergency room visits have not been curtailed; they have increased. The mandate to purchase health insurance has not removed the problem of adverse selection and moral hazard; instead, thousands of residents have chosen to obtain insurance when sick and drop it when healthy. The officials responsible for administering the Massachusetts health care system are no longer talking about sophisticated ways of making health care more efficient.



Instead, they are turning to the crude tactic of imposing price controls.



Once again, we have legislators putting unrealistic demands on experts. This results in the selection of experts with the greatest hubris, shutting out experts who appreciate the difficulty of the problem. When the selected experts find that their plans go awry, they take out their frustrations by resorting to more authoritarian methods of control.



 **THE SECURITY APPARATUS**



In July 2010, the Washington Post ran a series of stories on the size and complexity of the national security apparatus that has developed in response to the terrorist attacks of September 11, 2001. Yet with all this manpower and budget, we still have incidents like the Christmas bomber, a would‐​be terrorist who was stopped by citizens.



There are an infinite number of potential terrorist threats. In response, one could devise an infinite number of agencies and policies. There is little or no scope for anyone to question the relationship between costs and benefits.



More than 10 years ago, scientist and author David Brin wrote The Transparent Society, a book that anticipated the problems of surveillance and terrorism in the context of technological advance. Brin advocated making surveillance tools accessible to ordinary citizens. As counterintuitive and potentially disturbing as this sounds, Brin argued that it is better than the alternative, which is giving surveillance tools to government experts only. The latter approach threatens liberty without providing security. Unfortunately, that is the approach that the United States government has adopted, and it has grown out of control.



 **ENERGY AND ENVIRONMENT**



The Department of Energy has decided that it has the expertise to select specific energy projects, such as the electric car that is being developed by Fisker Automotive of California, the recipient of a $500 million loan guarantee. In theory, if the economic prospects for this electric car were good enough, venture capitalists would be willing to risk money on its development. Now, with a loan guarantee, private investors enjoy only the potential gains while taxpayers bear the risk. Many citizens who would never have considered investing in this electric car company are now partners in the venture, except that we have only the downside and no upside.



The officials who are putting taxpayer money at risk may or may not have better expertise than venture capitalists who put their partners’ money at risk. What the officials certainly have is more power.



The threat of climate change, like the threat of terrorism, can be characterized in such a way as to justify an unlimited attempt at expert control. Regardless of whether experts really can accurately measure, predict, and explain climate change, some will be tempted to exercise power as if their analysis were precise and certain.



 **FINANCIAL REGULATION**



The financial crisis spawned demands for new regulatory powers. However, the crisis itself clearly resulted from the misuse of regulatory power in the first place. It was government policy that attempted to promote home “ownership” by encouraging lending with little or no money down to speculators and inexperienced borrowers. It was government capital regulations that steered banks toward AAA‐​rated securities, with no need to investigate the true underlying risks. It was the view of leading regulators at the Federal Reserve and the International Monetary Fund in 2005 and 2006 that the financial system had become adept at managing and distributing risk. The regulators were not powerless to stop the risky behavior; instead, they were convinced that they had everything under control.



If the regulatory experts could not prevent the financial crisis of 2008, the most reasonable inference to make is that financial crises cannot be prevented. There is no such thing as a financial system that is “too regulated to fail.” The recent Dodd‐​Frank legislation gives broad new discretionary powers to regulators.



Many of the important rules, such as bank capital regulations, are left up to the experts. The decision to use new authority to break up or take over risky financial institutions is discretionary.



Unfortunately, the resolution of troubled financial institutions requires rules rather than discretion. With discretion, there is a problem of time inconsistency. No matter how loudly the regulators proclaim that they will not bail out failing institutions, history shows that when a crisis comes the officials in charge would rather do a bailout than face the uncertainty associated with shutting an institution down. Large failing banks will only be closed if there are strict rules in place that tie the regulators’ hands to make bailouts impossible.



Discretionary resolution authority is authority that will never be used. Banks and their counterparties know this, and they will behave accordingly.



 **THE KNOWLEDGE-POWER DISCREPANCY**



As Hayek pointed out, knowledge that is important in the economy is dispersed. Consumers understand their own wants and business managers understand their technological opportunities and constraints to a greater degree than they can articulate and to a far greater degree than experts can understand and absorb.



When knowledge is dispersed but power is concentrated, I call this the knowledgepower discrepancy. Such discrepancies can arise in large firms, where CEOs can fail to appreciate the significance of what is known by some of their subordinates. I would view the mistakes made by AIG, BP, Freddie Mac, Fannie Mae, and other well‐​known companies as illustrations of this knowledge‐​power discrepancy in practice.



With government experts, the knowledge‐ power discrepancy is particularly acute.



As we have seen, the expectations placed on government experts tend to be unrealistically high. This selects for experts with unusual hubris. The authority of the state gives government experts a dangerous level of power.



And the absence of market discipline gives any errors that these experts make an opportunity to accumulate and compound almost without limit.



In recent decades, this knowledge‐​power discrepancy has gotten worse. Knowledge has grown more dispersed, while government power has become more concentrated.



The economy today is much more complex than it was just a few decades ago. There are many more types of goods and services.



Consumers who once were conceived as a mass market now have sorted into an everexpanding array of niches. In the 1960s, most households had one television, which was usually tuned to one of just three major networks. Today, some households have many televisions, with each family member watching a different channel. Some people still watch major networks, but many others instead focus on particular interests served by specialty cable channels. Still others watch very little TV at all.



This increased diversity of consumer tastes in a world of tremendous variety makes the problem of aggregating consumer preferences more difficult. It becomes harder for government experts to determine which policies are in consumers’ interests. For example, is a national broadband initiative going to give consumers access to something they have been denied or something that they do not want? The advances of science are leaving us with problems that are more complex. As fewer Americans die of heart ailments or cancer in their fifties and sixties, more of our health care spending goes to treat patients with multiple ailments in their eighties and nineties. Given the complexity of each individual case, it seems odd that health care reformers believe that government can effectively set quality standards for doctors.



In business, performance evaluation of professionals is undertaken by other professionals who are in the same work group, observing their workers directly, and who understand the context in which the professionals are working. Even then, performance evaluation and compensation‐​setting are challenging tasks. In health care, proponents of government “quality management” propose to evaluate the decision‐​making of professionals and adjust their compensation on the basis of long‐​distance reports. Taking into account the knowledge‐​power discrepancy, this notion of quality management from afar is utterly implausible.



Financial transactions have gotten extremely complex. Some critics blame the use of quantitative risk models and derivative securities.



However, removing these tools would not remove financial risk, and in many respects could make it more troublesome.



One consequence of modern finance is that it exacerbates the knowledge‐​power discrepancy.



It is as futile for financial regulators to try to track down all sources of risk as it is for security agencies to try to keep track of all possible terrorist threats.



How can we deal with the knowledgepower discrepancy in government? It would be great if we could solve the problem by increasing the knowledge of government experts. Unfortunately, all experts are fallible.



If anything, expert knowledge has become more difficult for any one individual to obtain and synthesize. Analysts of the scientific process have documented a large increase in collaborative work, including papers with multiple authors and patent filings by groups and organizations. Scientists tend to be older when they make their key discoveries than was the case in the first half of the 20th century.



When he was an executive at Sun Microsystems, Bill Joy said, “No matter who you are, the smartest people work for someone else.” Joy’s Law of Management applies to government at least as much as to business. There is no way to collect all forms of expertise in a single place.



Instead, the way to address the knowledge‐ power discrepancy is to reduce the concentration of power. We should try to resist the temptation to give power to government experts, and instead allow experts in business and nonprofit institutions to grope toward solutions to problems.



 **LIVING IN A COMPLEX WORLD**



To summarize: We live in an increasingly complex world. We depend on experts more than ever. Yet experts are prone to failure, and there are no perfect experts.



Given the complexity of the world, it is tempting to combine expertise with power, by having government delegate power to experts. However, concentration of power makes our society more brittle, because the mistakes made by government experts propagate widely and are difficult to correct.



It is unlikely that we will be able to greatly improve the quality of government experts.



Instead, if we wish to reduce the knowledgepower discrepancy, we need to be willing to allow private‐​sector experts to grope toward solutions to problems, rather than place unwarranted faith in experts backed by the power of the state.
"
"
Share this...FacebookTwitterCORRECTION: The chart below is misleading. The ARD has not changed its color scheme. The chart for 2009 shows the 3-day forecast, whose color coding according to ARD in fact has not changed.  The 2019 is the forecast chart for the day, and this 1-day chart uses a different color code for temperature. It too has not changed since 2009. The third chart, as already mentioned, is from another drama source: website: wetter.de. Hat-tip: reader Taylor Martin
=======================================
WARNING: Watching too much German public television will make you hysterical, and pretty stupid (if you’re already leaning in that direction)
Sometimes you really wonder: can people be fooled really so easily?
German ARD public television network apparently thinks so, and has since gone off the deep end with the hype, drama and disinformation they add to their reports. I guess they think it’s working.
At Facebook one person illustrated this very well by showing the evolution of weather charts used by ARD German television in 2009, 2019 and this year. If you were to rate numerically the level of redness and fiery imagery and plot it,  you’d get a real hockey stick trend. Check out the forecast chart evolution below:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Hat-tip: Axel Robert Göhring at Facebook
In 2009, a nice hot summer day way forecast using pleasant color scheme of yellow and green. Even the “Samstag” (Saturday) forecast of highs from 19-25°C used the same green color scheme as it did for showing 34-36°C.
But as the years went by, Germans refused to panic enough about warming, and so by 2019 everything charts had to look hot and unbearable – even for forecasts of 20°C, see middle chart. Even temperatures in the 70sF look warm and toasty.
Then last year came doomsday prophet Greta with her inferno-visions and messages of hell. This year, weather forecast charts of highs in the 70s! to upper 80s are exploded in size, and made to look like explosion-like inferno images taken from beyond the gates of Hell. Just a note: I’m not sure if the 2020 chart is from ARD or another station, but you get the idea of what’s going on.
Also read here.


		jQuery(document).ready(function(){
			jQuery('#dd_579b3b87884f076d22b60daba2b95d2e').on('change', function() {
			  jQuery('#amount_579b3b87884f076d22b60daba2b95d2e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
According to wire reports, temperatures reached their lowest point in 30 years, reaching to -2°C in the capital, Riyadh, and to -6°C in mountainous regions blanketed by snow.  At least 10 people have died in the country as a weather system driven South from Siberia sent temperatures plummeting. Below are some pictures of snow from that region.
  
click for larger images
Apparently its gotten so bad (or they just aren’t prepared to deal with it) that King Saud ordered that government assistance should be given in the affected areas, which witnessed sub-zero temperatures this week.

I had to laugh at the photo above and the caption:  “Saudi Arabians are used to getting stuck in the sand, but snow is a new challenge for many.” It almosts seems Pythonesque.
Meanwhile, many roads were flooded by heavy rains in the nearby country of Dubai, which attracts sun-hungry tourists with its year-round blue skies. Roofs in some luxury hotels and office blocks were leaking water and several schools asked parents to keep their children home on Wednesday. It’s hard to imagine getting a “rain day” in the middle east.

While I’m enjoying pointing out these uncommon phenomena, I’d also point out that even though both the northern and southern hemispheres have both seen some record cold events in the past 6 months, that doesn’t necessarily equate to “climate change”. Still, something seems afoot as we are seeing more and more events like this. Maybe the massive La Niña now stretching across the Pacific ocean has something to do with this.
Oh but wait…there’s more!

Snow was seen yesterday atop Maui’s Mount Haleakala  – see story
Yeah, somethings up.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea124d90b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The latest UN climate talks, known as COP24, have just concluded. The supposed story this time was one of a grinding victory by the EU and developing nations over recalcitrant petro-states – Russia, the US, Kuwait and Saudi Arabia. These four, condemned as “climate villains” over the past week, worked to block the adoption of a critical IPCC report that detailed how woefully inadequate current international action was for limiting future climate change to 1.5C. Building on a previous COP in Paris in 2015, this meeting focused on writing the “rulebook” for the Paris Agreement, setting out how emissions will be measured, reported and verified. Absent at COP24 was any real discussion of how efforts to cut emissions would be increased, or targets raised from their current low level. This will be discussed at another meeting – another COP – in 2020. You could be forgiven for thinking this COP (short for Conference Of the Parties to the UN climate agreements) was no different to any of the previous COPs. As usual, there were a set of villains who were “holding up progress”. There was another scientific report spelling out how little time we have and how bad climate change will be if nothing changes. There was rancorous debate on technicalities, a sideshow debate around carbon markets, and no action on what to actually do. So far, so normal. Throughout its history very little has actually been achieved at the COP.  As things stand, we are still heading for 3℃ or more of global warming. We do not have 12 years to “do something” about it as the IPCC insists. Increasing numbers of commentators, journalists, scientists and environmentalists are breaking ranks from the “hopeful”, to argue that not only is far too little being done too late, but that dangerous climate change is already here.  Kevin Anderson of the Tyndall Centre for Climate Change Research, has consistently criticised IPCC reports for magical thinking, for assuming that at some point in the near future technology will be both invented and rolled out on a mass scale that will suck carbon dioxide from the atmosphere (so-called negative emission technologies). At the moment, there are none that are close to being ready to be mass produced. Take these out of the most recent IPCC report and instead of 12 years to stop dangerous climate change we have just three. Given all this, it could be tempting to blame the state of things on the climate villains – who doesn’t want to blame authoritarian or outright fascist government leaders for the world’s problems? But the problem isn’t bad leaders, but the entire system itself. The reality of climate change is that we need a radically different economic and political system if we are to limit future warming and ensure adaptation is fair and just. The COP reveals the limits of using nation states as the basis for action. Wedded to geopolitical realities and economic competition, states have not changed their behaviour to match the demands of climate science. In many ways it is unrealistic and naive to demand they do so. After all, they are not, as sometimes imagined, ships under the command of a single captain, able to direct the nation one way or another, but rather, complex assemblages where a huge number of actors and interested parties compete for wealth, power, access and influence.  Let’s be clear about what must be demanded of nation states: not some kind of minor adjustment or new zero-cost policy, but the end of economic growth. It would require legislating for de-growth, something that could be considered, after a decade of economic austerity, as electoral suicide. Legislating for de-growth is the right government policy, but the wrong approach. If the nation state is the wrong climate change actor, then the national economy is also the wrong perpetrator. Yet this is what every plan to combat climate change focuses on: national emissions. But this focus hides massive inequities within national populations and, more importantly, obscures both who is responsible for carbon emissions and who has the power to arrest them. It is really important that we – that is, the vast majority of humanity who will or already are suffering the effects of dangerous climate change – move past “national action plans” and start to take action immediately against two groups largely responsible for climate change. They are the 100 or so corporations responsible for 71% of global carbon emissions and the wealthiest 10% of the global population responsible for 50% of consumption emissions. To put the latter in perspective, if this 10% reduced their consumption to the level of the average European that would produce a 30% cut in global emissions.  Focusing on the wealthy and their corporations would enable us to bring about an immediate cut in carbon emissions. But it would also form part of a just transition, ensuring that the majority of the world’s population do not have to pay for climate policy, a conflict we have already seen on the streets of Paris in recent weeks in the yellow vests movement. 


      Read more:
      Gilets jaunes: why the French working poor are demanding Emmanuel Macron's resignation


 As we hurtle into 2019, we need to immediately shift to actions against the ultra-wealthy and the uber-powerful. It is long past time for changing how we talk about climate change. At some point we will need social movements capable of changing everything, but right now we need to relentlessly focus our actions on that small group of people profiting off the destruction of the world, and not wait in vain on governments to do it for us."
"

It was a term more notable for the remarkable amount of unanimity than its blockbuster decisions. Nevertheless, the 2010–2011 term of the United States Supreme Court provided plenty of judicial fodder to provoke a day’s worth of discussion at the Cato Institute’s Annual Constitution Day Conference. This year marked the 10th, and as always, it coincided with the release of the new _Cato Supreme Court Review_.



The conference, “The Supreme Court: Past and Prologue: A Look at the October 2010 and October 2011 Terms,” featured leading legal experts discussing the most pertinent cases of the last term and what we can expect in the near future from the Supreme Court.



David Post, law professor at Temple University and a member of the _Review_ ’s editorial board, examined _Brown v. Entertainment Merchants Association_ — teasing out the First Amendment’s “doctrinal oddities” in a provocative essay on the so‐​called violent video games dispute. “The case presents a fascinating snapshot of the state of [free speech] in the early years of the 21st century,” Post said, “and contains enough peculiarities … to keep law professors and their students busy for years to come.”



The _Review_ includes additional articles on the First Amendment — by far the highest‐​profile issue of the term — with case analyses on funeral protests, campaign financing, and much more. John Eastman, law professor and former dean at Chapman University, addressed _Bond v. United States_ — “your typical sordid tale of adultery, toxic chemicals, and federalism,” as _Review_ editor‐​in‐​chief Ilya Shapiro described it. The case involved a defendant being brought up, not on charges of assault, but on violating an international chemical‐​weapons treaty. The judicial lesson? “Don’t mess with the husband of someone who works in a chemical lab!” Eastman quipped.



The day closed with the annual B. Kenneth Simon Lecture, during which a distinguished legal scholar presents a paper that will be included in the following year’s _Cato Supreme Court Review_. This September, the Hon. Alex Kozinski — chief judge on the U.S. Court of Appeals for the Ninth Circuit — discussed the influence of new technologies on emerging expectations of privacy (See page 9). While he acknowledged the public’s willingness to trade certain boundaries for convenience, the story is much more complex than that. “I think it’s fair to say that privacy is not dead as an ideal,” he argued.



The last decade has seen a reanimation of many such ideals. In the foreword to the inaugural volume, Roger Pilon, Cato’s vice president for legal affairs, expanded on the purpose of the _Cato Supreme Court Review_. “We will examine the Court’s decisions and upcoming cases in the light cast by the nation’s first principles — liberty and limited government — as articulated in the Declaration of Independence and secured by the Constitution,” he wrote. The mission, modestly laid out, was to play a part in changing the climate of ideas to one more conducive to a constitutional government of delegated, enumerated, and thus limited powers. The Center for Constitutional Studies, established in 1989, has been a critical institution in making that vision a reality.



“In short, the Court cannot roll back Leviathan on its own,” Pilon says, “but it can put a brake on it and chip away at its substance” — or, perhaps, lack thereof.
"
"

On Oct.10, Tom Daschle (D-SD), Senate Majority Leader and potential presidential candidate, ordered his own Energy and Natural Resources Committee to stop working on a National Energy Policy bill. He did this because a majority of that committee now favors drilling for a modest amount of oil in a tiny area of the remote Arctic National Wildlife Refuge (ANWR). Further, he has substitute‐​legislation in mind that goes beyond ANWR, calling for drastic changes in our energy structure.



Hopefully, Daschle will reconsider his actions on the energy bill. Although ANWR can supply only a small fraction of our energy, it is a political no‐​brainer. Who’s going to vote against domestic oil when we are at war? The emerging energy bill also encouraged the development of technologies to further reduce the already small atmospheric impact of coal combustion, promoting the increased use of this domestically abundant fuel.



But do we need any energy legislation? Coal is cheap and plentiful, and it is regulated via the Clean Air Act Amendments of 1990. If there’s the political will, go ahead and re‐​open those (though expect some opposition here). Drilling in the ANWR doesn’t require 30 pages of text; a few sentences will do.



However logical it might be to “do nothing here,” that’s not the way D.C. works. Instead, Daschle would like to substitute another bill, S.556, for large portions of the current energy legislation. The substitute‐​bill is sponsored by Jim Jeffords (?-VT). Instead of drilling in ANWR and developing cleaner coal technology, the bill looks a lot like the old Kyoto Protocol on global warming, which has wisely been rejected by President Bush because it is 1) expensive, and 2) scientifically indefensible. Jeffords’ bill mandates that we reduce emissions of carbon dioxide from power plants to 1990 levels by January 2007. Our national emissions are currently about 15 percent above 1990 levels. As we continue on our merry economic way, those emissions will go 20 percent above 1990 levels by 2007. To meet this law would therefore require a major energy reduction in a nation at peace, let alone in one at war.



Now, about 55 percent of the nation’s electricity is produced by the combustion of coal. Jeffords’ target could be met if somehow every coal‐​fired turbine was converted to natural gas. But we do not have the infrastructure to move that much gas, and, further, S.556 mandates “policies that would reduce the rate of growth of natural gas consumption” Without coal or natural gas, there’s only one significant source of power production left: nuclear. Does anyone seriously think that the same radical greens that pressured Daschle into all this will allow him to push nuclear power?



It gets worse. The substitute‐​bill also requires that 90 percent of mercury emissions be removed from power generation by 2007. There is only one way to do that: Stop burning coal. If this bill is passed, it will be against the law to use coal to produce appreciable amounts of electricity. That is mandated for a nation that has hundreds of years of coal supply, and depends upon the rest of the world for 60 percent of its oil. That is mandated for a nation that is currently fighting a war using oil‐​powered technology.



So what S.556 scuttles is more than just ANWR drilling, it is our domestic energy hole card. The result of the substitute‐​bill is that the country will become bereft of power. Jeffords’ bill makes energy prohibitively expensive even as it mandates an impossible result. All of this when there’s a war on.



How could such folly evolve? It all goes back to “genus: Extreme environmentalism, species: global warming.” You’d think this critter would at least go into hibernation given the current problems in the world. But instead it is flying stealthily through Congress while the public concentrates on the more important matters at hand.



How much additional global warming “gain” do we get for the Jeffords pain? We ran the United Nations’ own computer model, assuming their dire “storylines” (their word) for global warming and development. The amount of global warming that Jeffords’ bill prevents in the next 50 years is 0.04ºF. No one will be able to measure this against the natural variability of climate.



Do we need omnibus energy legislation when a few sentences will do? Worse, do we want to substitute a bill that will increase energy prices, have no demonstrable effect on climate, and outlaw an inexhaustible domestic source of energy?



If anybody has noticed that we are at war, it must be Sen. Daschle. Maybe it’s time to be a bit more conservative about domestic energy policy. There will be plenty of time to debate things like global warming after we win a victory that is much more assured by domestic energy security at this precarious moment in time.
"
"The UK is showing a “lack of coherence” in its leadership of vital UN climate crisis talks this year and giving the damaging impression that the talks are not a high priority, one of the world’s leading voices on the climate crisis has said. Mary Robinson, a former UN climate envoy and Ireland’s first female president, also said the perception that major British politicians, including the ex-prime minister David Cameron and former foreign secretary William Hague, were unwilling to take on the role of leading the COP 26 summit was damaging.  “It is not helpful that we are getting the impression that in the UK no one wants the job. I mean, come on! The UK asked for this, they pitched for this responsibility, they must carry it forward.” “This needs to be an overarching priority, that needs to come across. I do not see a coherent drive for [the summit] in the UK. “The UK’s handling of COP 26 has not become coherent enough for the UN even to be able to support them,” she added. Robinson, the former UN high commissioner for human rights who chairs the Elders group of former world statespeople, served twice as a UN climate envoy and campaigns for climate justice. Her exasperation reflects a growing disquiet among some leading international experts over the UK government’s leadership of the vital COP 26 summit, which remains without a figurehead after a tumultuous week. Last Friday, the former energy minister Claire O’Neill was abruptly sacked as president of the negotiations, the key role alongside the UN in bringing countries together to agree tougher limits on greenhouse gases. Since then, O’Neill has unleashed vitriolic attacks on Boris Johnson, alleging a lack of interest in the climate crisis, while Johnson’s failure to set out a clear vision of how the UK will get a deal at COP 26 has also been criticised. “It’s a huge pity this is happening,” said Yvo de Boer, the UN’s top climate official from 2006-2010, who began the push for climate action that resulted in the 2015 Paris agreement. “The president of the COP really needs to be the force for stability in this process. You need someone in place who is reaching out to key countries to smooth this process.” The sacking of O’Neill reminded him of “a very bad time in my life”, he said, when during the Copenhagen COP in 2009 the Danish prime minister removed the serving COP president, Denmark’s environment minister Connie Hedegaard, and installed himself to lead the process. That was one of the factors that doomed the Copenhagen summit to an acrimonious and chaotic end. In this case, the sacking has happened sooner – meaning “this is not irreparable”, said de Boer. “But if it goes on, I think it will be damaging.” Johnson must also do more to show the UK’s leadership in cutting emissions, beyond the promise to reach net zero by 2050, he said. “It’s very important that the UK has domestic policies in place that allow them to inspire confidence in other countries,” said de Boer. One long-time observer of COPs, who asked not to be named, said Johnson’s handling of the launch, at which he failed to set out clear stepping stones to a successful outcome for COP 26 or domestic policies to reduce emissions, had hurt his standing among world leaders on the issue. “There was nothing there, it was a mess,” they said. “It made me angry.” However, there is still strong support for the UK’s COP 26 presidency and a willingness among many leading figures to assist Johnson and the government in building the coalition of countries needed. At COP 26, governments are being asked to come forward with tougher targets to cut greenhouse gas emissions, as the current targets are too weak to meet the goal of the Paris agreement in limiting global heating to 2C above pre-industrial levels. “COP 26 needs to send a signal, to give guidance to governments and businesses [to encourage them to cut carbon],” said Fatih Birol, executive director of the International Energy Agency, and one of the most respected voices globally on climate issues. “I am sure that the UK will be a good manager of the COP.” He said the UK had the diplomatic heft to forge the coalition of countries needed, even without a COP president currently. “I know the UK civil servants [working on climate], they have top notch climate experts, some of the best in the world. I have full confidence in them.” But he called for the government to show more urgency. “Building a team needs to happen immediately, and putting out a clear vision for COP 26. You need leadership, the engagement of all the key parties, including developing countries, and all other stakeholders [including business and civil society].” “The prime minister is really critical – his leadership will be very important to a successful outcome,” he added. Chile, which holds the current presidency of the UN climate process, also offered its support. Carolina Schmidt, who presided over last December’s Madrid climate talks, said: “Since the announcement that the UK will take on the presidency of COP 26, we have met representatives of the UK government on many occasions – including since the COP 25 meeting in Madrid. We look forward to continuing our close collaboration with the incoming COP 26 presidency, as well as other key partners, throughout 2020. This is a crucial year of climate action for humanity as a whole. We have a great challenge ahead: the climate crisis is a reality that all countries must tackle together, and we will stand firmly with our international partners to do so.” Richard Kinley, former deputy head of the UN’s climate secretariat, said there was still time for the UK to recover. “Ideally, you would have started earlier, but politicians in the UK were preoccupied [with Brexit],” he said. “It is late but it is not too late for things to begin, but a lot needs to be done fairly quickly to build momentum for these talks. You need to get mobilised.” As well as the lack of a president, there is a widespread sense that the UK has got off to a slow start in the all-out diplomatic push that is needed to bring reluctant countries together. “The UK diplomatic machine really needs to be running at full speed now,” said de Boer. “They need to be going to other capitals, finding out the key sticking points, and what needs to happen to get a high level of ambition. That’s only going to happen if you understand what the red lines are for all parties.” Some developing countries said they were awaiting more details from the UK, including the identity of the new president. “Without a doubt it would be good that sooner rather than later we will know who our primary interlocutor will be for COP 26,” said a representative of one key group of countries. Saleemul Huq, director of the International Centre for Climate Change and Development, added: “Every day that the prime minister does not have a COP 26 president in place is valuable time being lost”"
"Following the sacking of Claire O’Neill as president of the forthcoming climate change talks in Glasgow, and her subsequent criticism of Boris Johnson (COP 26: Cameron was asked by PM to take over – but he said no, 5 February), I would suggest that the only UK politician of sufficient integrity, dedication and expert knowledge on the subject of climate change to take over is Caroline Lucas MP.Rose HarvieDumbarton • I wonder what the government will do to replace the huge tax receipts lost when petrol and diesel vehicles are banned. Answers on the back of an electricity bill, please.Ian MetcalfePerth • No, Larry Elliott is not the only economist who believes the UK to be better off (in the long term) outside the EU (Letters, 3 February). One of the others is Prof Bill Mitchell, of Newcastle University, Australia. Prof Mitchell is speaking in London on 20 February, so why not come along and ask him to explain his viewpoint?Mike EllwoodAbingdon, Oxfordshire • May I, a 95-year-old, second-generation Guardian reader, join the marmalade marathon (Letters, 5 February). This year I have made 6lb of grapefruit marmalade and 20lb of Seville orange. I hope to live to enjoy it all!Joy NalpanisReading, Berkshire • I have never made marmalade. But I still do create jams/chutneys for my friends. To keep them on their toes, I also write the labels. “Rosepetal and Earwig” was a favourite!Yvonne MalikWray, Lancashire • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
nan
"A row over lamps is emerging as a first major test of the EU’s commitment to its much-vaunted European Green Deal and the bloc’s target of carbon neutrality by the middle of the century. A debate over the continued use of mercury in fluorescent lighting has split the 27 member states with Germany’s industrial interests being pitted against the environmental concerns of Sweden, according to leaked correspondence.  The European commission is being asked by Germany, the Netherlands, Hungary, Poland and the Czech Republic to continue to allow manufacturers to use mercury in light bulbs despite the potential damage to the environment and human health. The three main companies in the sector, General Electric, Philips and Osram, are major employers, particularly of German and Hungarian workers. But critics say that an exemption granted to the lighting industry in 2011 from a general ban on mercury use under the Restriction of Hazardous Substances directive is no longer justifiable. Sweden, Finland and Bulgaria, among others, say the successful argument nine years ago that there was not a readily available alternative to mercury in the manufacture of fluorescent lamps is defunct. Mercury-free LED light bulbs were said to produce significantly poorer levels of lighting, but it is now claimed that the technology has sufficiently moved on. Brussels’ next move is being billed as a test of its stated commitment to fighting the climate crisis. “Sweden’s main concern is that there are no legal grounds for renewing an exemption for the use in questions,” a letter from the Swedish government to the commission stated on 22 January. “Today there are economically viable substitutes available for most of the mercury-containing light sources.” In 2017, the EU signed the Minamata convention obliging it to reduce the use of toxic materials such as mercury. The Swedish government argued in its letter that Brussels risked being in breach of its legal obligations. “Sweden also doubts that renewing the exemptions for mercury in question would be in line with the goal on climate neutrality and the ambitions on chemicals of the green deal recently launched by the commission”, its government added, citing the 40.9m metric tonnes of CO2 emissions savings that would be delivered by phasing mercury out by September 2021. The European commission president, Ursula von der Leyen, said in December that the premise of the so-called European Green Deal targeting a “climate-neutral continent” by 2050 was that the “old growth model that is based on fossil fuels and pollution is out of date”. She described it as Europe’s “man on the moon moment” as she laid out 50 policies to revamp EU rules and regulations. Discussions among the member states, the commission and stake-holders on mercury-bearing lamps are due to continue next week."
"

**_Study one of first to look at little‐​known success story_**



Report: Drug Decriminalization Works in Portugal 

In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 

On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 

With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 

Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 

Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.



In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 



On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 



With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 



Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 



Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.
"
"
Share this...FacebookTwitterData from NASA point to a powerful Pacific La Nina event in the works, and so with it could bring a considerable drop in the mean global surface temperature in 2021. 
According to the latest report issued by the Australian Bureau of Meteorology (BOM), the La Niña conditions continue in the tropical Pacific: “International climate models suggest it is likely to continue at least through February 2021.”

Peak La Niña conditions expected in January, 2021. Chart source: BOM. 
Central and eastern tropical Pacific Ocean sea surface temperatures (SSTs) are at La Niña levels, and remain similar compared to two weeks ago, reports the BOM. “Models continue to suggest some possibility that central and eastern tropical Pacific SSTs could briefly reach levels similar to 2010–12, with the peak most likely in December 2020 or January 2021.”
The BOM uses the ACCESS–S model for generating its forecasts.
NASA: temperature deviation to -3°C


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Mean while Snowfan here reports that NASA prognoses are in fact expecting an “unusually powerful La Niña development, with cooler than normal surface anomalies extending into the summer of 2021 and which could become an extended year-long event, which also occurred in 2010 – 2012.

Source: BOM NASA/GMAO-ENSO-Prognosen
“November 2020 sees an unusually strong La Niña in the equatorial Pacific with temperature deviations down to below -3°C and with unusually long duration until NH summer 2021,” Snowfan writes. “If the current NASA forecast is correct, a multi-year La Niña could develop into 2022, just like in 2010-2012.”
Strong and long La Niña events cool the Earth by several tenths of a degree Celsius with a time lag. The overall global cooling occurring since 2016 will therefore continue at least until 2021 and could even last until 2022.
Fuel for future bush fires
The BOM notes that La Niña events “typically enhance spring rainfall in northern, central and eastern Australia” and that during a La Niña summer, “above average rainfall is also typical for much of eastern Australia, but particularly eastern Queensland”. But such good news comes with a price: Rainfall means more vegetation growth, which in turn will lead to much more fuel ffor uture bush fires when drought conditions return, as they always inevitably do.


		jQuery(document).ready(function(){
			jQuery('#dd_30ad4b406255c528e276dfb072615ea2').on('change', function() {
			  jQuery('#amount_30ad4b406255c528e276dfb072615ea2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
I’m working on porting over to a new blogging platform. So there may be a delay in new content here. I’m trying out some ideas and themes, and it is looking promising.
When it’s all done, the URL will be announced. Stay tuned. Thanks to all who gave me feedback.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3aa00f2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Today I obtained the paper: LaDochy, S., R. Medina, and W. Patzert. 2007. Recent California climate variability: spatial and temporal patterns in temperature trends. Climate Research, 33, 159-169 You can download the paper in PDF format in its entirety here: ca_climate_variability_ladochy.pdf
I’ll post more on this paper later, but I wanted to make it available for everyone to read beforehand.
This paper references my good friend and colleague, Jim Goodridge, former California State Climatologist in its bibliography. As you may recall, I posted on Jim’s work here a couple of months ago. One of the maps that Jim has prepared, seen below, closely matches the mapped results from the LaDochy et al paper.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea24130c6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman libertarian site Die Freie Welt reports that German Extinction Rebellion activists protested against short-haul flights at several German airports earlier this week.
Lübeck / Munich
In Lübeck, 10 people tried to glue themselves to the runway with superglue, but the police were able to prevent the action. In Munich, people chained themselves to luggage trolleys.Even political figures took part, for example Lorenz Gösta Beutin, climate policy spokesman of Die Linke (The Left)  accompanied the protests in Lübeck, reports Freie Welt. “100 to 150 additional demonstrators protested outside the airport building.”
Düsseldorf
In Dusseldorf, one person even managed to get through security and onto a plane for Hamburg. The protester claimed to be a climate saver and shouted: “Please stop this plane. I am not prepared to fly. I want to get off. I am not prepared to sit down again.”
Many annoyed passengers reacted angrily. One passenger shouted, “Hey you arse—, sit back down”.  The activist claimed to represent everyone and that the issue was not about him.
“The earth is getting hot”, he declared, “Our mother nature is going to hell.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to witnesses, the activist tried to continue reading his speech, which he had written down, and stammered about “the end of the dear world”. The pilot was forced to to return the aircraft to the parking position so that the passenger could get off.
Berlin
In Berlin another activist made her way onto a plane and demanded passengers “disembark and stay on the ground” in order to keep domestic German flights from heating up the planet. “We are all threatened by an ecological collapse.”

BREAKING: Flugzeug in #Berlin #Tegel durch #ExtinctionRebellion Aktivisti gestört!!!Die sich täglich verschärfenden #Klimakrise weiter durch innerdeutsche Flüge anzuheizen ist Wahnsinn – Zeit auszusteigen und am Boden zu bleiben!#LuftBlock #ActNow #RebelForLife #stayGrounded pic.twitter.com/fevcjIAlse
— Extinction Rebellion Berlin 🌍 (@XRBerlin) August 17, 2020

The Freie Welt refuted the claims made by the Düsseldorf protestor, writing:

The figures actually say something completely different: domestic flights within Germany are responsible for just 0.3 percent of Germany’s CO2 emissions. In contrast, car traffic alone accounts for 20.8 percent. But you can imagine how motorists will react if activists stick themselves to the autobahn with super glue.”


		jQuery(document).ready(function(){
			jQuery('#dd_8a0034c297c24f880c76cad0fcfcb63f').on('change', function() {
			  jQuery('#amount_8a0034c297c24f880c76cad0fcfcb63f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"

“In December 1919, Carlo ‘Charles’ Ponzi approached a group of friends and acquaintances in Boston with a new investment opportunity,” Cato senior fellow Michael Tanner writes. What followed has become immortalized as one of the most infamous investment scams in history. In **“Social Security, Ponzi Schemes, and the Need for Reform”** (Policy Analysis no. 689), Tanner considers recent calls comparing this fraudulent operation with the current U.S. social insurance program. The two programs, he says, have several similarities. Social Security, for instance, “does not actually save or invest any of a participant’s payments” — relying instead on inflows from future contributors to finance the system. This, in turn, provides “a windfall to the first participants, but declining returns to subsequent joiners” — also similar in operation to a Ponzi scheme. Finally, Social Security is “a system that worked well when demographics were favorable,” yet it’s “facing insolvency as the ratio of recipients to contributors increases.” Despite these similarities, there is in the end one crucial distinction between the two. “Social Security is not a Ponzi scheme,” Tanner concludes, “because Charles Ponzi didn’t have a gun.” As such, the debate over epithets obscures a much deeper issue: Social Security is unable to pay promised benefits with current levels of taxation. “In short,” Tanner writes, “the program is facing insolvency without fundamental reform.”



 **Drug Violence Flaring in Mexico**  
In December 2006, President Felipe CalderÃ³n of Mexico launched a military‐​led offensive against his country’s increasingly violent narcotics trade. In **“Undermining Mexico’s Dangerous Drug Cartels”** (Policy Analysis no. 688), Cato senior fellow Ted Galen Carpenter argues that this campaign is not simply ineffective: “It is a futile, utopian crusade that has produced an array of ugly, bloody side effects,” he writes. Many are now questioning whether Mexico is on its way to becoming a “failed state.” While Carpenter determines that these fears are overblown, he nevertheless acknowledges that “the overall trend is troubling.” By the same token, he notes that the extent of a spillover of violence and corruption into the United States has been limited — yet the possibility of turf battles becoming proxy wars is “a harbinger of deterioration of the security situation on our southern border.” By examining several alternatives to the current approach, Carpenter finds that one stands out above the rest. “The most feasible and effective strategy to counter the mounting turmoil in Mexico is to drastically reduce the potential revenue flows to the trafficking organizations,” he writes. This hinges on abandoning the prohibitionist model in favor of full legalization. “The fire of drug‐​related violence is flaring to an alarming extent in Mexico,” he concludes. Restricting the damage will require swift action, “before that fire consumes our neighbor’s home and threatens our own.”



 **War and the Practice of Politics**  
Article I of the United States Constitution vests the power to “declare War” in Congress, leaving to the executive the power to “repel sudden attacks.” But in the years since the Cold War, the practice of initiating limited conflicts has blurred these constitutional distinctions. In **“Congress Surrenders the War Powers: Libya, the United Nations, and the Constitution”** (Policy Analysis no. 687), John Samples, director of Cato’s Center for Representative Government, examines a number of smaller wars — namely, those in Bosnia, Somalia, Kosovo, Iraq, and most recently, Libya — and reaches several conclusions. First, the president has arrogated “largely unfettered powers” to launch wars that are “half‐​made” — conflicts he feels “are essential to fight and yet beyond constitutional propriety.” Second, while sometimes critical, Congress tends to defer to presidential command when these wars are both brief and popular. Third, Congress’s active “investigations and criticisms can affect the conduct of a limited war but not its inception.” On the other hand, while the public is often skeptical that limited conflicts are worth the cost, their “desire for congressional authorization of such wars goes unfulfilled.” Finally, Samples finds that, in Libya in particular, an incremental transfer of these powers to international institutions — also known as weak internationalism — ” contravenes values central to American republicanism.” As such, he concludes, “law becomes over time a function of, not a constraint on, the practice of politics.”



 **The Ivory Tower’s Burden**  
“If you follow higher education — or just live near a college or university — you’ve probably heard the complaint: government keeps axing higher education funding,” writes Neal McCluskey, associate director of Cato’s Center for Educational Freedom, in **“How Much Ivory Does This Tower Need? What We Spend on, and Get from, Higher Education”** (Policy Analysis no. 686). The problem is that there is little evidence to support this claim. While most analysts rely on public funding as a share of overall school revenues, McCluskey examines the burden of postsecondary education borne by taxpayers — the most direct measure of public support — and one that is “typically ignored in anecdote‐​driven media stories.” What do these numbers suggest? “No matter how you slice it, the burden of funding the Ivory Tower has grown heavier on the backs of taxpaying citizens,” he writes. In fact, the burden on the individual taxpayer has risen from $426 in 1995 to $532 in 2010, a 25‐​percent increase. But this is only part of the higher‐​education story. The real question is whether human capital has expanded along with this increased investment. McCluskey finds that the increased flow of dollars has “underwritten poor academic results, rampant price inflation, and considerable college inefficiencies.” “The money taken from taxpayers,” he concludes, “to ‘invest’ in higher education has been on the rise, and it appears to be hurting both taxpayers individually and society as a whole.”



 **Malpractice Caps Hurt Patients**  
Supporters of capping court awards for medical malpractice argue that such caps will make health care more affordable. But is this necessarily the case? In **“Could Mandatory Caps on Medical Malpractice Damages Harm Consumers?”** (Policy Analysis no. 685), economist Shirley Svorny of California State University, an adjunct scholar at the Cato Institute, says that it may not be so simple. In reviewing the structure of the medical liability insurance industry, Svorny begins by offering a key insight. “The decades‐​old conventional wisdom holds that medical malpractice insurers rarely adjust premiums to reflect an individual physician’s risk,” she writes. This assumption, however, is misplaced. As Svorny illustrates, the industry has developed a complex, “interdependent system of physician evaluation, penalties, and oversight” — all of which is based upon the threat of legal liability for negligence. Patients, in turn, derive protections from this oversight. In short, she writes, “the evidence presented here shows that physicians pay the price for putting patients at risk.” Svorny draws on interviews with underwriters and brokers, published sources, and an extensive analysis of state insurance company rate filings to make her case — showing that premiums “act as signals that steer physicians toward higher‐​quality care.” As such, the implication is clear. “Capping court awards, all else equal, will reduce the resources allocated to medical professional liability underwriting and oversight,” she argues, “and make many patients worse off.” The study generated a lively online discussion at the Manhattan Institute’s PointofLaw website.



 **Lessons from Deepwater Horizon**  
On April 20, 2010, an explosion on the Deepwater Horizon offshore drilling unit led to the largest accidental oil spill in the history of the petroleum industry. What lessons have emerged in the year since the well has been declared “effectively dead”? Richard L. Gordon, professor emeritus of mineral economics at Pennsylvania State University and an adjunct scholar at the Cato Institute, argues in **“The Gulf Oil Spill: Lessons for Public Policy”** (Policy Analysis no. 684) that the resulting political backlash uncovers longstanding issues with the attempt to regulate commercial activities. “The underlying problem is a mythology that holds that public lands are precious resources needing careful government management,” he writes. This isn’t the case. By examining the political response — particularly the Waxman‐​Markey bill — he underscores the real issue. “The failure was in fact due to the impotence of the very policy initiatives that the Obama administration wishes to expand,” he writes. Gordon carefully deconstructs the “tangential campaigns” against foreign oil imports, oil consumption, and climate change — making it clear that “the only thing these concerns have in common is their invalidity.” The ideal solution, he contends, is privatization of federal lands. In the interim, Gordon demonstrates that the Gulf oil spill reflects the problems associated not only with our command‐​andcontrol energy strategy, but with government oversight in general. “The real lesson of the oil spill,” he concludes, “is the familiar point that bad policies beget bad consequences.”
"
"A mystery bidder recently paid US$50,000 to name Dallas zoo’s latest baby giraffe.  As we humans know, your name significantly affects your life.  It has been shown to influence your career choice, a phenomenon called nominative determinism (think of Sarah Blizzard the weather presenter), or whether you will get that job interview in the first place.  Because without even meeting you, people form opinions about you based on your name.  My wife likes to remind me she married me because she will be forever Young. We can’t yet conclusively say whether other species of animals have names. Of course, many can be identified by sight or smell, but these are not something the animal actively emits to announce its presence. Until now dolphins are the only other known species with an auditory label. They have what we call signature whistles, which they use to identify themselves and seem to function to co-ordinate the movement of their group.   It’s easy to see why zoos name their attractions – celebrity animals such as Jumbo the elephant who lived in London zoo in the 1860s have become legend. But what of us scientists, who are supposed to ignore such matters in favour of objective reality, should we name our subjects? In most experiments, scientists must identify their subjects as individuals.  Technically, we should identify our animals in a neutral manner; for example, by using numbers to avoid inducing bias into our experiments.  But what about lucky number 7, jinxed number 13, or 666 the number of the beast? Zoologists studying mammals in the field or in captivity may have numbers for their subjects but they often have “pet” names for their animals. It would seem humans cannot resist giving names to certain types of animals.   Over the years I have set up or been involved with a number of research projects where we have had to identify our animals. I have always started out with the good intention of using neutral identifiers, but my postgraduate students/fieldworkers – usually the ones who really spend time with the animals – are the name-givers. Mammals are by far the most frequently named group of animals.  Others just tend to get stuck with a number unless the species is particularly large or an individual has a notable trait such as One-Eye the fish. Primates are virtually always given names. Some wild primates have become famous and it is hard to imagine this could happen if they did not have a name.  Anyone who is fascinated by chimpanzees knows of Flo studied by Jane Goodall who had her obituary published in the Sunday Times.  Another of Goodall’s subjects was called Satan and she was criticised for this name, as it could bias her interpretation of his behaviour. Many primatologists name their animals using the first letter in a name to indicate relatedness.  Thus, my first habituated to humans group of titi monkeys was known as group D and contained Desbotado (faded), Diana and Diego.  When I asked my postgraduate students why they chose the name Diana, not a common name in Portuguese, they said, “because she looks like a princess”. I always tell my postgrads to savour their field time because all too quickly they could end up stuck at a desk like me.  However, my situation is lightened by my primatology students with their stories from the field, which have an almost soap opera-like quality to them – of who did what to whom.  And of course without names none of this would make sense. Giving an animal a name helps people to relate to them.   But the danger of anthropomorphism – ascribing human qualities to animals – rears its ugly head. Can we objectively evaluate the behaviour of a monkey that we think looks like a princess? Or a naughty individual we have affectionately called Asbo? Animal subjects are frequently named by human observers in honour of their academic mentors. Swinging around the rainforests of Brazil is a northern muriqui called Robert Young.  Such anthropomorphising of certain species is difficult to stop. From a scientific perspective we need to be aware of it and take steps to eliminate any biases which it might induce in our research.   For example, we could have one set of people who record the behaviour of the animals with a video camera and a second set of people, who do not know the animals, who collect their behaviour from the video.  This is what we scientists call a blind experimental design. Naming your experimental subjects is a double-edged sword: it seems to spark fascination in researchers but it may cloud their judgement of their subjects’ behaviour."
"
Recently I had some of my readers comment that they thought that The Weather Channel and USA Today (which uses TWC graphics) temperature maps seemed to look “hotter”. They suspected that the colors had changed. I tend to watch such things since my own company (IntelliWeather) produces similar maps.
I searched Google images for some saved older TWC maps, but found none. So I can’t be absolutely sure they have or have not changed.  But looking at the color scheme, nothing sticks out in my recollection of the temperature map colors.
But I decided that it would be an interesting exercise to compare USA national temperature maps from the commonly used services today. I saved national CURRENT temperature isotherms/gradient maps from around 03Z (11PM Eastern Time) tonight. All were generated within about an hour of each other.
What I found was surprising. Here they are in alphabetical order:
Intellicast: (probably the ugliest national temp map I’ve ever seen)



IntelliWeather:

NOAA-NWS:

Unisys:

Weather Central:

Weather Channel:

WeatherForYou:


Weather Underground:

A couple of notes on the graphics: The Weather Channel does not show their color key, nor does IntelliCast. From experience it appears the with the exception of the IntelliWeather map, all maps have fixed color schemes. The IntelliWeather map uses a sliding scale of color based on the max and min temps presented in the data. Also, I tried to include AccuWeather, but could not locate a current national temperature map from that company. They had everything else but that.
UPDATE: I decided that even though AccuWeather did not have a CURRENT temperature map, the color and color key on their HIGH TEMPERATURE FORECAST map would suffice for this comparison, since it a similar range of temperatures presented, from (50’s to 90’s) so here it is:

Note the color scale and where the perceived “cooler” colors start on the AccuWeather map.
So what do you think?
Is it just me or does there appear to be a warm bias in the color temperature presentation of the majority of providers shown here? Just an FYI, I designed my color scheme for the IntelliWeather Map in 2001, well before I started blogging, so please no suggestions that I skewed this comparison with my own map color scheme.
Along those lines, I’ll point out that the color choices are usually done either by a meteorologist, or a graphic artist/programmer or both. Usually the color scheme is the result of the input from a couple people. In my case, myself and my graphic artist made the choice. In places like TWC or AccuWeather, the choice may be made initially by one or two then approved by a larger group.
The point I’m trying to make is that each map represents the color and temperature perception of the presenting organization, as I don’t know of any “standard” for map colors used for air temperature presentation. Having said that, somebody will probably put one in front of me that I’ve never known about. 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e8352aa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"As all good Monty Python fans know, water technologies feature large in the legacy of benefits left by Roman civilisation. But while aqueducts, sewers and baths retain an obvious presence in the landscape and in the archaeological record, the Romans’ largest and most important water achievement may have been “virtual”. The Romans developed networks of trade and food supply that enabled them to escape local water constraints, in a way that is explained in a new study in the journal Hydrology and Earth System Sciences. Fertile regions such as southern Spain or Italy’s Po valley would grow lots of food and ship it back to Rome or to the drier outposts of the Empire.  Embedded within this is a what geographers call a virtual water trade – an indirect way of shifting this precious resource from wetter, less populated areas to those regions with more people or a less consistent climate. The map below shows this in action. The amount of virtual water imports (a) and exports (b) in different parts of the Empire are illustrated by the size of the circles. The numbers express this in tonnes of grain. Rome is by far the largest water importer, followed by Alexandria and Memphis in Egypt, and Ephesus and Antioch in modern-day Turkey. Spain and Egypt were the biggest exporters. All ships lead to Rome The paper’s primary author, Brian Dermody at the University of Utrecht, suggests this sophisticated water economy ultimately contributed to its own downfall as it enabled urban populations to boom beyond sustainable levels.  Does this sound uncomfortably familiar? In the next 30 years we are facing a critical combination of inter-related stresses on the core resources that keep our civilisation running. As it happens, the Romans gave us a word for that too – the “food-water-energy nexus” (from the Latin nectere, to bind together).  So are we doomed to the same fate as the Romans? As its name suggests, the nexus recognises that different resources are intimately interconnected. We need water for drinking, washing and for industry; but we also need it to grow food, and around 70% of global fresh water supply is used for farming. As populations grow and become more wealthy, demand for food will increase, placing pressure on domestic water supply and industrial output. Economic growth and technological developments increase energy use, driving additional demand for water in power station cooling and other uses in energy generation. The rise in shale gas extraction provides a stark illustration of this: irrespective of the many other ethical and political issues surrounding fracking, it is its thirst for water (used to force the hydrocarbons out of the rocks) that may prove the key limitation. After all, 38% of the world’s shale gas resources are located in areas of extremely high water stress or arid conditions. In the UK, plans for fracking in major regions such as the Severn catchment could place untenable pressure on water use for farming and domestic supply. In all this complexity, the mega-issue of climate change arguably plays only an aggravating role. Intensive farming is degrading soil, its primary resource base, up to 100 times faster than the rates at which it is formed. Non-renewable fuel and mineral resources are becoming increasingly scarce and more costly to recover. And renewable, drinkable water supplies are under often extreme threat.  Solving climate change will not in itself solve the problems of the food-water-energy nexus; in fact it should be apparent that our effective response to climate change is deeply entwined with a sustainable untangling of the nexus. Like the Romans, the “modern” response to the emergent limitations of the food-water-energy nexus was economic. Global trade through the 20th century allowed us to circumvent local or regional resource limitations, stimulating unprecedented population growth along with rising wealth and living standards.  Many countries could not hope to maintain their current consumption of food and resources if they were forced become self-reliant on resources available within their territory – in the current economic and technological conditions, at least.  This makes the global economy vulnerable to regional problems. Look at this year’s escalation of tension between Russia and the West, for instance. Sanctions imposed by both sides have affected international trade in wheat and other crops, leading to supply shortages or gluts in some places and the destabilisation of farming economies and farmer incomes in others – and has raised the threat of disruption to transnational energy supplies. Again, the challenges of the nexus – and our vulnerability to those changes – transcend the background threat of climate change. So, faced with challenges which appear strikingly similar, what can our postmodern, self-aware civilisation do to avoid the fate of the Romans? We cannot stop the nexus any more than we can prevent the climate change that will result from our current levels of greenhouse gas emission. Business as usual is clearly not an option. In the absence of a magic bullet (or something much worse, an environmental disaster or collapse), resilience is the key.  One advantage we have over the Romans is information; we can learn from precedent. We can see what is over the horizon and make a judgement on how it may impact our lives and livelihoods. The challenge, unfortunately, remains how to stimulate people and politicians to change in response to those dangers. However human nature means we are as ready to listen to soothsayers as scientists and, in that respect, we and the Romans are still very much the same."
"

Click image for a live interactive view of the National Climatic Data Center in Asheville, NC
Today started off terrible. I slipped in the bathtub last night at the hotel, and strained a back muscle and was so sore that just getting dressed and into the car was a chore. As a result, I was late getting to NCDC this morning. I’ve been popping Aleves today. Fortunately, they had slack built in so the day got started cheerfully with a review of the new Climate Reference Network with the principal scientists. It was a super meeting and I took many notes, I’ll have much to share later.
Next came a briefing on “Climate Science” from Tom Peterson, but I’m afraid I stole his thunder a little bit when I announced that I had already seen his presentation, which included an analysis of the Marysville USHCN Station.  See the powerpoint he presented here:aapg-san-antonio-peterson
Then came a personal tour of the Asheville CRN station by Dr. Bruce Baker. In addition to taking visible light photos, I also took matching IR photos from many angles. Bruce and his team were quite impressed with the IR camera I use, and he says he plans to buy a couple in use for siting surveys. He also plans to post the IR photos I took today on the CRN site to show how well the design and siting is free of IR influences.

I’ll have much more on all of this but I still have 8 more stations to survey plus an unexpected customer detour service call Friday to WDNN-TV in Dalton, GA which has some trouble with our weather display system there. So stay tuned for more details on the visit and questions that were asked and answered.
But the big news came with Dr. Baker providing me with a press release (new today) to post here for you all to see. CRN is getting completed and USHCN modernization is starting:
NOAA today announced it will install the last nine of the 114 stations as part of its new, high-tech climate monitoring network. The stations track national average changes in temperature and precipitation trends. The U.S. Climate Reference Network (CRN) is on schedule to activate these final stations by the end of the summer.
NOAA also is modernizing 1,000 stations in the Historical Climatology Network (HCN), a regional system of ground-based observing sites that collect climate, weather and water measurements. NOAA’s goal is to have both networks work in tandem to feed consistently accurate, high-quality data to scientists studying climate trends.
See the full press release here:
 press_release_042408_climatereferencenetwork
What this means: No more adjusted data, the raw data from CRN and from HCN-M is the real data and will be pristine, assuming the network is maintained. No more torturous gyrations of FILNET, SHAP, and TOBS. The downside is that a track record needs to be built up, the older data is also going to be revised with USHCN2 algorithms soon, and I’ll touch on that later.
One thing that Debra Braun said to me today in the meeting hit home: “our funding had been cut for the last two years, and we were unable to move forward until this year”. This made me think that perhaps some of the focus the surfacestations.org project brought to illuminating the deplorable condition of the network may have helped a little bit in convincing some legislators that it was time to get serious about allocating funding to complete the CRN and fix the USHCN. A little public embarrassment of the USHCN provided by all of us that have contributed to surfacestations.org may have helped. I’d sure like to think so.
I want to extend my heartfelt thanks to Dr. Baker, Debra Braun, Grant Goodge, and the entire CRN science team, plus Jeff Arnfield, and Steven Del Greco for answering all my questions and taking such careful time with me. Additionally I wish to thank Dr. Karl, and Assistant Director Sharon LeDuc for hearing my concerns  and offering ideas.
Everyone there at NCDC made me feel welcome and appreciated.
Most importantly, I want to thank you, my loyal readers and volunteers, because without your help, the trip and presentation I made would not be possible.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fd87aba',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The recent confidence in shale gas was likely premature, according to several new reports published in the US. In particular a study from the University of Texas claims the US boom will tail off by 2020 and not keep going to 2040 as previous less thorough analyses have predicted. To anyone who has been closely following the industry in recent years, this difference in predictions will not be surprising, of course.  In 2013 the US Energy Information Administration (EIA) already noted that Norway’s assessment of its shale gas potential went from 83 trillion cubic feet (tcf) (2011) to zero (2013) due to results obtained from test wells in the alum shale, and how Poland’s estimates went from 44tcf to 9tcf due to stricter application of requirements for successful shale formations. But at that time the EIA did not comment as strongly or publicly on similar concerns about the accuracy of the US shale data.  Likewise concern about overestimates of shale potential is becoming louder in Britain, which is at a much earlier stage in terms of shale gas exploration but has a similarly enthusiastic government. Last month scientists from the UK Energy Research Council suggested that promises by ministers about greater energy security and lower energy prices through shale gas were premature and unlikely to be deliverable. Additionally there are concerns over whether investment in shale gas is still profitable, while numerous potentially costly environmental concerns have not yet been dispelled either.  What then are the reasons for these unreliable calculations? And why do governments promote shale gas with such conviction when it is surrounded by such uncertainty? One possible factor behind inaccurate judgement of shale gas potential is that both official organisations like the EIA and industry specialists rarely release the data behind their forecasts.  The terminology is also surely to blame: there are resources, and then there are reserves. While this is clear to experts, the distinction is not made consistently in the media. This prompted a recent note by the UK Parliamentary Office for Science and Technology urging the government to address the variance.  Experts distinguish between total resources, potentially/technically recoverable resources, and reserves. The reserves is the amount that is economically recoverable, and is normally a fraction of the total resources. Even then, the North American experience demonstrates that well productivity is highly variable.  When UK prime minister David Cameron announced that Britain was to go “all out for shale,” there was no more detailed information available for the UK than the size of the total resource. Reserves could still be anywhere between substantial and zero.  One cannot help but be reminded of the nuclear energy discussion from the 1980s. Less than a month before the Chernobyl accident, The Economist described the technology as being “as safe as a chocolate factory” – and has since castigated itself for its remark. Today, shale gas is lauded to be both an economical solution and environmentally amenable –- before there is ample evidence for either claim.  Given election cycles, politics is naturally drawn to short-term solutions. The fact that shale has brought US gas prices to lower levels than were probably imaginable a decade ago makes it attractive to this mindset. Lobbying interests will be part of the picture too – it does not surprise that countries with major petroleum companies would push the shale gas/oil agenda.  The idea of tapping into a new petroleum resource also sits well with the underlying and largely uncontested objective of faster and continued economic growth. Equally, the idea of technological innovation fixing future problems and therefore allowing the status quo to continue is too good to pass up.  There are many things about alternatives such as renewables or conserving energy that are less comfortable or popular. When you read recent UK energy strategy White Papers, despite much talk of climate change prevention and a need for renewable energy, growth and revenue are more linked to fossil fuels and the traditional energy industry. Shale gas fits the bill.  I don’t categorically deny the possibility of continued revenue and profitability in the shale gas business. My point is that it should be treated as just that: a possibility, not a promise. In an era where the need for more transparency is a statement that appears in nearly all policy proposals, energy politics should be no different. What is needed is clear and factual communication between experts, policymakers and the public about the opportunities and drawbacks. Environmental costs should also be part of those considerations. As these latest US reports remind us, this has to include an accurate account of what is known about it and what uncertainties remain."
"
Share this...FacebookTwitter      While world governments bedwet over a fantasized climate catastrophe taking place 100 years out, mankind could be facing a potential catastrophic food shortage. A worthhile read (see link below).
A fungus threatens 20% of the world's food supply.
      The disease is Ug99, a virulent strain of black stem rust fungus (Puccinia graminis), discovered in Uganda in 1999, threatens the world’s wheat supply. Read the scary details here: http://www.wired.com/magazine/2010/02/ff_ug99_fungus/all/1
      Wheat provides 20% of all calories consumed by humans. According to Nobel laureate Norman Borlaug, father of the Green Revolution:
This thing has immense potential for social and human destruction.
      According to wired.com, the fungus attacks the stem of the wheat plant, causing it to wither and die.
Stem rust is the polio of agriculture, a plague that was brought under control nearly half a century ago as part of the celebrated Green Revolution. After years of trial and error, scientists managed to breed wheat that contained genes capable of repelling the assaults of Puccinia graminis, the formal name of the fungus.
But now it’s clear: The triumph didn’t last.
      The new fungus has spread from Africa and into the Middle East. It would only take a  traveller with a single spore on his shirt to transport it to the USA and Canada.
The pathogen makes its presence known to humans through crimson pustules on the plant’s stems and leaves. When those pustules burst, millions of spores flare out in search of fresh hosts.
      It goes to show that nature has a bag full of nasty tricks, and there’s nothing you can do to stop her. All you can do is adapt, hopefully quickly enough. But if you waste your time trying to appease her, and don’t invest your resources wisely in adapting, you’ll get eliminated.
Share this...FacebookTwitter "
"It’s early January and freezing cold in New York when I meet Jenny Offill to talk about her new novel, Weather – an innocuous title for something that feels less innocuous every day. A couple of weeks earlier, the temperature was warm and spring-like. These fluctuations in the weather, and the warming trends they reveal, are increasingly unsettling reminders of the climate crisis, and they form the backbone of Offill’s latest novel, the follow-up to 2014’s bestselling Dept. of Speculation. Weather follows Lizzie, a university librarian, who responds to the emails sent in to “Hell or High Water”, a climate-focused podcast hosted by her former academic mentor. The job opens Lizzie’s eyes to the crisis and the myriad ways different people respond to it, from “dreary” environmentalists obsessed with composting toilets to “end-timers” eager to embrace the Rapture. Amid a growing sense of her own responsibility to the planet and fear for the future, Lizzie struggles to balance her responsibilities as a wife, mother, sister, daughter and friend.  “I became interested in why I wasn’t more interested,” Offill says, considering the question of why she chose to focus a novel around a subject many people find too vast and frightening to contemplate. In other words, she was curious how it was possible to be intellectually aware of an unfolding disaster without feeling emotionally connected or moved to action. The novel follows Lizzie as she moves from “a state of twilight knowing” to a more conscious awareness of the crisis. At its core, the story asks: what happens after we start to pay attention? When writers and artists have tackled the climate crisis, they’ve tended to come at it obliquely, through imagined dystopias. Even the “zombie apocalypse” that Offill says crops up on doomsday-prepper websites is somehow more comprehensible than the ravages of climate breakdown. “‘Apocalypse’ is one of those words, like ‘fascism’, that immediately feels like an overstatement,” Offill says. “But it was hard to see those pictures coming out of Australia in the last few weeks, or hear that a billion animals may have died, and not feel like that is what’s happening.” Offill’s novel might best be called “pre-apocalyptic”. There are no large-scale emergencies, no heroic survivors; there is no overt horror. Instead, the novel belongs to the everyday world of contemporary gentrifying Brooklyn. As with Dept. of Speculation, the narrative is crafted from short, resonant passages, rarely longer than three or four sentences, that read like diary jottings, conversation fragments, jokes, quotations, trivia or poetry. Their lightness is deceptive. In the gaps between, there’s “the ambient dread of feeling something is coming or happening, but that you can’t understand its dimensions,” she says. That dread is increased in the wake of the 2016 election, which features, indirectly, in the novel’s plot. “I don’t think they’re unrelated,” Offill says, of climate anxieties and Trumpian politics. “I think the spectre of climate change is leading to some of this tightening of us-against-them, lifeboat-ethics feelings.” Strikingly, it’s the election that elicits a grim, panicked response from Lizzie’s husband, Ben, who has been unmoved by her climate worries; he wonders, fleetingly, if they ought to get a gun. Offill is fascinated by the way that these distances can open up, suddenly or slowly, between people within the intimate confines of a family. In Dept. of Speculation, the protagonist finds herself isolated by the intense experience of early motherhood, depression, and the struggle to complete her second novel. The contours of her life reflect Offill’s own, also a married writer with one daughter who has supported herself by teaching and ghostwriting, and who took 15 years to publish the follow-up to her 1999 debut novel, Last Things. But she has resisted the “autofiction” label, which she thinks is applied too readily to women writers, minimising the craft that goes into their fiction. In the course of writing Dept. of Speculation, Offill jettisoned another, more conventionally structured novel, experimented with writing poetry, taught fiction, and eventually hit on the style, stripped to the bone, that would make the novel so powerful – so “joyously demanding”, as Roxane Gay put it in her review for the New York Times. Despite Offill’s belief that the book’s fragmented form would mainly excite other writers – Ocean Vuong, Sheila Heti and Jia Tolentino are among her fans – it found an unexpectedly wide audience, drawn to its offbeat yet honest depiction of the clash between motherhood and creativity. Dept. of Speculation’s unnamed protagonist was a writer who declared her intention to become an “art monster”, the kind of artist who makes a ruthless priority of her creative life. Yet she finds it impossible. The domestic and emotional worlds of motherhood and marriage – not to mention the pressure to make a living – exert a far stronger pull than she anticipated. That character and Lizzie are not the same person, Offill stresses, but she sees a continuity between them in the struggle to balance their domestic roles with their larger ambitions. “To me, the through-line is caretaking,” she says – the work women do to care for children and other family members, often without pay or even the recognition that it is work at all, leading to a kind of burnout that makes it difficult to pay attention to the wider world. “That exhaustion was something I wanted to channel through Lizzie.” It’s no accident that Sylvia, the host of the podcast that draws Lizzie towards an awareness of the climate emergency, is a superstar academic with no children. Her approach to the crisis is shaped by her social class and the freedom of movement it brings. When Lizzie accompanies Sylvia to a conference in Silicon Valley, the well-heeled attendees want to know where the safest place will be to escape the apocalypse – not for themselves, they insist, but for their children. Of course, as Lizzie is well aware, most people don’t have the freedom to choose where to go if their family is in danger. “There’s a lot of people in the world right now who have terrifyingly little control over how to keep their children safe,” Offill observes. When Lizzie worries about the future of her young son, Sylvia’s only advice is to become really rich. Lizzie herself is neither a climate refugee nor a tech titan, but rather “smack in the middle”, Offill says. She has enough support and security to be able to think about the world beyond her immediate situation, but not enough to protect everyone she loves. And like many women entering middle age, her responsibilities keep extending beyond her husband and son to her brother, a recovering addict, her religious mother, and her brother’s wife and baby. She gives too much of herself to them, she chides herself, listening to their problems and offering her advice for hours on end (her husband drily, or sourly, points out that if only she were a real shrink, they’d be rich). Her impoverished mother drives around town giving socks to the homeless, which gives Offill the opportunity to show us a different kind of caring and social responsibility at work. “They’ve done all these studies to show that, proportionally, the people who make the least money give the most to charity,” she points out. “I think what Bezos just gave to charity” – he announced that Amazon would donate $690,000 (£527,000) to the Australian wildfire relief fund – “was something like the equivalent of if I gave one 18th of a penny in terms of proportion of my income.” These bits of information pepper Offill’s conversation as they do the book, connecting ideas and hinting at the research underpinning its allusive style. But while the studies she consulted provided useful reference material, Offill says that her reading began as a way to understand her own reaction to the climate crisis, or lack of it. One book that she cites as formative is criminologist Stanley Cohen’s States of Denial: Knowing About Atrocities and Suffering (“so, big sell at a party”). It was in Cohen’s work that she found the idea of “twilight knowing”, a state hovering between denial and knowledge, as a name for where Lizzie finds herself at the beginning of the book. Her reading widened to climate science, mythology, psychology and sociology, in an effort to understand how people have responded to disasters at different points in history. To confront a looming apocalypse through reading, she admits, was “deeply silly, and also the only thing I knew to do”. By making her protagonist a university librarian, Offill was able to create a character who shared her own instinct to look for answers in books, and to indulge in imagining “a little bit of an unlived life”, she says. Yet Lizzie’s job involves far more human than literary interaction, with an array of lost souls including a pale adjunct professor (“I worry he is selling his plasma again,” Lizzie says) and an eternal graduate student. Offill, who has taught at several different colleges, is familiar with this highly educated but precarious class, who have “social capital and not so much other capital”. It certainly doesn’t escape Lizzie’s notice that her skills and knowledge would be of vanishingly little practical use in a doomsday scenario. Yet libraries also represent, for Offill, a bastion of hope. “As the social fabric of our society has continued to unravel, libraries have become one of the last places where you don’t have to buy anything and yet you are welcome,” she says. “It’s like a tiny little vision of utopia.” She mentions the current interest in experimenting with libraries of tools and household appliances as evidence of this utopian instinct at work. It’s surprising, given the subject matter, how much fun Weather is, both to read and discuss, and also how darkly funny. “I’m always trying to figure out when you can puncture some of the self-righteousness,” Offill says. Even where it seems warranted, relentless seriousness about serious subjects can keep people from facing and fighting them. “They feel undone by the earnestness, by the sense that you have to have your own house completely together before you’re able to join a bigger movement.” Lizzie’s tone is often earnest, sometimes self-righteous, but usually a blend of deadpan and terrified.  One joke is something called the “obligatory note of hope”. In the story, it’s a sardonic label for what Sylvia feels she has to include in every article and speech about the climate crisis, if she wants to keep her audience. After the end of the book, the phrase reappears as a URL, which Offill is excited to explain will direct readers to a website that reaches beyond the novel to offer some pathways to activism (right now, it leads to a landing page with a picture of a goat in an abandoned library). The site highlights the work of three environment-focused organisations – the Sunrise Movement, the Transition movement (which emphasises grassroots local initiatives) and Extinction Rebellion – as well as sharing stories of people who have worked for change. A further section, “Tips for Trying Times”, sounds the most distinctively Offill-like, taking the research that didn’t make it into the book and turning it into a resource that is both practical and encouraging. It includes “what people did when their movements didn’t seem like they were making any headway” and “what does the Swedish government say to do if crisis or war comes?” Offill is adamant that the site is purely a resource, and doesn’t feature any links to buy her books. It’s simply an effort to connect with other people, to stick to a promise she made while writing Weather, that she would undertake more in-person activism. “I no longer felt like I could opt out. I no longer felt like it wasn’t my fight,” she says. That means moving far beyond the individualistic ambitions of the art monster, and the comforts of books and intellectual isolation. “Out of the library, into the streets. But, boy, is it a nerve-racking place to be!” • Weather by Jenny Offill is published by Granta (RRP £12.99). To order a copy go to guardianbookshop.com"
"This is an article from Head to Head, a series in which academics from different disciplines chew over current debates. Let us know what else you’d like covered – all questions welcome. Details of how to contact us are at the end of the article. Rob Bellamy: 2018 has been a year of unprecedented weather extremes around the world. From the hottest temperatures ever recorded in Japan to the largest wildfire in the history of California, the frequency and intensity of such events have been made much more likely by human-induced climate change. They form part of a longer-term trend – observed in the past and projected into the future – that may soon make nations desperate enough to consider engineering the world’s climate deliberately in order to counteract the risks of climate change.  Indeed, the spectre of climate engineering hung heavily over the recent United Nations climate conference in Katowice, COP24, having featured in several side events as negotiators agreed on how to implement the landmark 2015 Paris Agreement, but left many worried that it does not go far enough. Matt Watson: Climate engineering – or geoengineering – is the purposeful intervention into the climate system to reduce the worst side effects of climate change. There are two broad types of engineering, greenhouse gas removal (GGR) and solar radiation management (or SRM). GGR focuses on removing anthropogenically emitted gases from the atmosphere, directly reducing the greenhouse effect. SRM, meanwhile, is the label given to a diverse mix of large-scale technology ideas for reflecting sunlight away from the Earth, thereby cooling it. RB: It’s increasingly looking like we may have to rely on a combination of such technologies in facing climate change. The authors of the recent IPCC report concluded that it is possible to limit global warming to no more than 1.5°C, but every single one of the pathways they envisaged that are consistent with this goal require the use of greenhouse gas removal, often on a vast scale. While these technologies vary in their levels of maturity, none are ready to be deployed yet – either for technical or social reasons or both. If efforts to reduce greenhouse gas emissions by transitioning away from fossil fuels fail, or greenhouse gas removal technologies are not researched and deployed quickly enough, faster-acting SRM ideas may be needed to avoid so-called “climate emergencies”. SRM ideas include installing mirrors in Earth’s orbit, growing crops that have been genetically modified to make them lighter, painting urban areas white, spraying clouds with salt to make them brighter, and paving mirrors over desert areas – all to reflect sunlight away. But by far the best known idea – and that which has, rightly or wrongly, received the most attention by natural and social scientists alike – is injecting reflective particles, such as sulphate aerosols, into the stratosphere, otherwise known as “stratospheric aerosol injection” or SAI. MW: Despite researching it, I do not feel particularly positive about SRM (very few people do). But our direction of travel is towards a world where climate change will have significant impacts, particularly on those most vulnerable. If you accept the scientific evidence, it’s hard to argue against options that might reduce those impacts, no matter how extreme they appear. Do you remember the film 127 Hours? It tells the (true) story of a young climber who, pinned under a boulder in the middle of nowhere, eventually ends up amputating his arm, without anaesthetic, with a pen knife. In the end, he had little choice. Circumstances dictate decisions. So if you believe climate change is going to be severe, you have no option but to research the options (I am not advocating deployment) as broadly as possible. Because there may well come a point in the future where it would be immoral not to intervene. SRM using stratospheric aerosols has many potential issues but does have a comparison in nature – active volcanism – which can partially inform us about the scientific challenges, such as the dynamic response of the stratosphere. Very little research is currently being conducted, due to a challenging funding landscape. What is being done is at small scale (financially), is linked to other, more benign ideas, or is privately funded. This is hardly ideal. RB: But SAI is a particularly divisive idea for a reason. For example, as well as threatening to disrupt regional weather patterns, it, and the related idea of brightening clouds at sea, would require regular “top-ups” to maintain cooling effects. Because of this, both methods would suffer from the risk of a “termination effect”: where any cessation of cooling would result in a sudden rise in global temperature in line with the level of greenhouse gases in the atmosphere. If we hadn’t been reducing our greenhouse gas emissions in the background, this could be a very sharp rise indeed. 


      Read more:
      Time is running out on climate change, but geoengineering has dangers of its own


 Such ideas also raise concerns about governance. What if one powerful actor – be it a nation or a wealthy individual – could change the global climate at a whim? And even if there were an international programme, how could meaningful consent be obtained from those who would be affected by the technology? That’s everybody on Earth. What if some nations were harmed by the aerosol injections of others? Attributing liability would be greatly contentious in a world where you can no longer disentangle natural from artificial.  And who could be trusted to deliver such a programme? Your experience with the SPICE (Stratospheric Particle Injection for Climate Engineering) project shows that people are wary of private interests. There, it was concerns about a patent application that in part led to the scientists calling off a test of delivery hardware for SAI that would have seen the injection of water 1km above the ground via a pipe and tethered balloon. MW: The technological risks, while vitally important, are not insurmountable. While non-trivial, there are existing technologies that could deliver material to the stratosphere.  Most researchers agree that the socio-political risks, such as you outline, outweigh the technological risks. One researcher remarked at a Royal Society meeting, in 2010: “We know that governments have failed to combat climate change, what are the chances of them safely implementing a less-optimal solution?”. This is a hard question to answer well. But in my experience, opponents to research never consider the risk of not researching these ideas.  The SPICE project is an example where scientists and engineers took the decision to call off part of an experiment. Despite what was reported, we did this of our own volition. It annoyed me greatly when others, including those who purported to provide oversight, claimed victory for the experiment not going ahead. This belies the amount of soul searching we undertook. I’m proud of the decisions we made, essentially unsupported, and in most people’s eyes it has added to scientists’ credibility. RB: Some people are also worried that the promise of large-scale climate engineering technologies might delay or distract us from reducing greenhouse gas emissions – a “moral hazard”. But this remains to be seen. There are good reasons to think that the promise (or threat) of SRM might even galvanise efforts to reduce greenhouse gas emissions. MW: Yes, I think it’s at least as likely that the threat of SAI would prompt “positive” behaviour, towards a sustainable, greener future, than a “negative” behaviour pattern where we assume technology, currently imaginary, will solve our problems (in fact our grandchildren’s problems, in 50 years time). RB: That said, the risks of a moral hazard may not be the same for all climate engineering ideas, or even all SRM ideas. It’s a shame that the specific idea of stratospheric aerosol injection is so frequently conflated with its parent category of SRM and climate engineering more generally. This leads people to tar all climate engineering ideas with the same brush, which is to the detriment of many other ideas that have so far raised relatively fewer societal concerns, such as more reflective settlements or grasslands on the SRM side of things, or virtually the entire category of greenhouse gas removal ideas. So we risk throwing the baby out with the bathwater. MW: I agree with this – somewhat. It’s certainly true all techniques should be given the same amount of scrutiny based on evidence. Some techniques, however, often look benign but aren’t. Modifying crops to make them more reflective, brightening clouds, even planting trees all have potentially profound impacts at scale. I disagree a little in as much as we simply don’t know enough yet to say which technologies have the potential to reduce the impacts of climate change safely. This means we do need to be thinking about all of these ideas, but objectively.  Anyone that passionately backs a particular technology concerns me. If it could be conclusively proven that SAI did more harm than good, then we should stop researching it. All serious researchers in SAI would accept that outcome, and many are actively looking for showstoppers. RB: I agree. But at present there is very little demand for research into SRM from governments and wider society. This needs to be addressed. And we need broad societal involvement in defining the tools – and terms – of such research, and indeed in tackling climate change more broadly. 


      Read more:
      Why you need to get involved in the geoengineering debate – now


 MW: Some people think that we should just be getting on with engineering the climate, whereas others feel even the idea of it should not even be discussed or researched. Most academics value governance, as a mechanism that allows freedom to explore ideas safely and there are very few serious researchers, if any, who push back against this.  A challenge, of course, is who governs the governors. There are strong feelings on both sides – scientists either must, or cannot, govern their own research, depending on your viewpoint. Personally, I’d like to see a broad, international body set up with the power to govern climate engineering research, especially when conducting outdoor experiments. And I think the hurdles to conducting these experiments should consider both the environmental and social impact, but should not be an impediment to safe, thoughtful research. RB: There are more proposed frameworks for governance than you can shake a stick at. But there are two major problems with them. The first is that most of those frameworks treat all SRM ideas as though they were stratospheric aerosol injection, and call for international regulation. That might be fine for those technologies with risks that cross national boundaries, but for ideas like reflective settlements and grasslands, such heavy handed governance might not make sense. Such governance is also at odds with the bottom-up architecture of the Paris Agreement, which states that countries will make nationally determined efforts to tackle climate change.  Which leads us to the second problem: these frameworks have almost exclusively arisen from a very narrow set of viewpoints – either those of natural or social scientists. What we really need now is broad societal participation in defining what governance itself should look like. MW: Yes. There are so many questions that need to be addressed. Who pays for delivery and development and, critically, any consequences? How is the global south enfranchised - they are least responsible, most vulnerable and, given current geopolitical frameworks, unlikely to have a strong say. What does climate engineering mean for our relationship with nature: will anything ever be “natural” again (whatever that is)?  All these questions must be considered against the situation where we continue to emit CO₂ and extant risks from climate change increase. That climate engineering is sub-optimal to a pristine, sustainably managed planet is hard to argue against. But we don’t live in such a world. And when considered against a +3°C world, I’d suggest the opposite is highly likely to be true. If there’s a specific topic or question you’d like experts from different disciplines to discuss, you can:"
"
Share this...FacebookTwitterA new temperature reconstruction indicates today’s sea surface temperatures are colder than all but a few millennia out of the last 156,000 years.
A Southern Ocean site analyzed in a new study (Ghadi et al., 2020) has averaged 1-2°C during glacials and 4°C during interglacials. Today, with a 410 ppm CO2 concentration, this location has again plummeted to glacial/ice age levels (2°C).
The site was 2°C warmer than now when CO2 concentrations were 180 ppm about 20,000 years ago, or during the peak of the last ice age. During the Early Holocene (10,000 to 8,000 years ago), summer sea surface temperatures were also 2°C warmer than today.
There is no indication that CO2 concentration changes are in any way correlated with temperature changes throughout this entire 156,000-year epoch.

Image Source: Ghadi et al., 2020
Share this...FacebookTwitter "
"

More indicators of a colder than normal winter continuing in the northern hemisphere.
From the London Telegraph:
Britain is enduring its most miserable Easter for 25 years as Arctic winds sweep in, bringing snow, hail and sleet.
Easter Sunday temperatures could drop to as low as -3C at night with a band of snow and sleet forecast to move down from the North. The bad weather is most likely to affect the Midlands but snow could even reach London, forecasters said.
From the Sofia news agency:
Bulgaria Meets Vernal Equinox With Snow, Sun Gleams
From This is London:
It’s Bad Friday: Britain braced for worst Easter weather in 25 years as country is battered by gales and sleet.
From the Stars and Stripes:
Snow hits Germany military bases with more possible for Easter.
From CTV.ca
‘Spring’ weather nasty for Eastern Canada
Also from CTV.ca
Six more weeks of winter, top weatherman forecasts
From KDKA-TV:
Snow Advisory In Effect For Parts Of Western Pa
From RedOrbit:
Nebraskans and Iowans heading east for the Easter weekend were experiencing flight delays or snow-covered roads today, and the troubles could continue into Saturday.
From the Detroit Free Press:
Heavy snow across Michigan and points west meant increasing cancellations and delays at Metro Airport today, with things getting worse as snow piled up.
From swissinfo.ch
The Easter break has started with heavy snowfall and strong winds in Switzerland, causing some disruption to traffic.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea08f2db6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
